[{"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 4, "deletions": 79, "changes": 83, "file_content_changes": "@@ -55,8 +55,8 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n // -----\n \n-// TODO: Pending on the support of isSplat constant\n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+// TODO: masked load with vectorization is pending on TODO\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: masked_load_const_other\n   func @masked_load_const_other(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n@@ -94,7 +94,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n     %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.getelementptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n \n     // Load 4 elements from vector0\n     // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n@@ -125,81 +125,6 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n-module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n-  // CHECK-LABEL: kernel__Pfp32_Pfp32_Pfp32_i32__3c256_vec4\n-  func @kernel__Pfp32_Pfp32_Pfp32_i32__3c256_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n-    %c256_i32 = arith.constant 256 : i32\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n-    %1 = arith.muli %0, %c256_i32 : i32\n-    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n-    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n-    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n-    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %6 = tt.getelementptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n-    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.getelementptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n-\n-    // Load 4 elements from A with single one vectorized load instruction\n-    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n-\n-    // Load 4 elements from B with single one vectorized load instruction\n-    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n-\n-    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n-    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %13 = tt.getelementptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n-\n-    // Store 4 elements to global with single one vectorized store instruction\n-    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n-    tt.store %13, %11 : tensor<256xf32, #blocked0>\n-    return\n-  }\n-}\n-\n-// -----\n-\n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n-module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK-LABEL: kernel__Pfp32_Pfp32_Pfp32_i32__3c256_vec8\n-  func @kernel__Pfp32_Pfp32_Pfp32_i32__3c256_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n-    %c256_i32 = arith.constant 256 : i32\n-    %0 = tt.get_program_id {axis = 0 : i32} : i32\n-    %1 = arith.muli %0, %c256_i32 : i32\n-    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n-    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n-    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n-    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %6 = tt.getelementptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n-    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.getelementptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n-\n-    // Load 8 elements from A with two vectorized load instruction\n-    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n-    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n-\n-    // Load 8 elements from B with two vectorized load instruction\n-    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n-    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n-\n-    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n-    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %13 = tt.getelementptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n-\n-    // Store 8 elements to global with two vectorized store instruction\n-    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n-    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n-    tt.store %13, %11 : tensor<256xf32, #blocked0>\n-    return\n-  }\n-}\n-\n-// -----\n-\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_vec4\n@@ -751,4 +676,4 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n     return\n   }\n-}\n+}\n\\ No newline at end of file"}]