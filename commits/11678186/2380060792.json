[{"filename": "include/triton/codegen/analysis/liveness.h", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -1,12 +1,14 @@\n #ifndef TDL_INCLUDE_IR_CODEGEN_LIVENESS_H\n #define TDL_INCLUDE_IR_CODEGEN_LIVENESS_H\n \n-#include <map>\n-#include <set>\n-#include <vector>\n #include \"triton/codegen/analysis/layout.h\"\n #include \"triton/tools/graph.h\"\n \n+#include \"llvm/ADT/MapVector.h\"\n+\n+#include <set>\n+#include <vector>\n+\n namespace triton{\n \n namespace ir{\n@@ -42,14 +44,14 @@ struct segment {\n \n class liveness {\n private:\n-  typedef std::map<shared_layout*, segment>    intervals_map_t;\n+  typedef llvm::MapVector<shared_layout*, segment>    intervals_map_t;\n \n public:\n   // constructor\n   liveness(layouts *l): layouts_(l){ }\n   // accessors\n   const intervals_map_t& get()  const { return intervals_; }\n-  segment get(shared_layout* v)  const { return intervals_.at(v); }\n+  segment get(shared_layout* v)  const { return intervals_.lookup(v); }\n   // run\n   void run(ir::module &mod);\n "}, {"filename": "include/triton/codegen/selection/generator.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -177,6 +177,7 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n   void visit_sqrt_inst(ir::sqrt_inst*);\n   Value* shfl_sync(Value* acc, int32_t i);\n   void visit_reduce1d_inst(ir::reduce_inst*, std::function<Value*(Value*,Value*)>, Value*);\n+  void visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral);\n   void visit_reducend_inst(ir::reduce_inst*, std::function<Value*(Value*,Value*)>, Value*);\n   void visit_reduce_inst(ir::reduce_inst*);\n   void visit_select_inst(ir::select_inst*);"}, {"filename": "include/triton/tools/graph.h", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "file_content_changes": "@@ -3,8 +3,9 @@\n #ifndef _TRITON_TOOLS_THREAD_GRAPH_H_\n #define _TRITON_TOOLS_THREAD_GRAPH_H_\n \n+#include \"llvm/ADT/SetVector.h\"\n+\n #include <map>\n-#include <set>\n #include <vector>\n #include <iostream>\n \n@@ -13,21 +14,21 @@ namespace tools{\n \n template<class node_t>\n class graph {\n-  typedef std::map<node_t, std::set<node_t>> edges_t;\n+  typedef std::map<node_t, llvm::SetVector<node_t>> edges_t;\n \n public:\n   typedef std::map<size_t, std::vector<node_t>> cmap_t;\n   typedef std::map<node_t, size_t> nmap_t;\n \n private:\n-  void connected_components_impl(node_t x, std::set<node_t> &nodes,\n+  void connected_components_impl(node_t x, llvm::SetVector<node_t> &nodes,\n                                  nmap_t* nmap, cmap_t* cmap, int id) const {\n     if(nmap)\n       (*nmap)[x] = id;\n     if(cmap)\n       (*cmap)[id].push_back(x);\n-    if(nodes.find(x) != nodes.end()) {\n-      nodes.erase(x);\n+    if (nodes.count(x)) {\n+      nodes.remove(x);\n       for(const node_t &y: edges_.at(x))\n         connected_components_impl(y, nodes, nmap, cmap, id);\n     }\n@@ -39,7 +40,7 @@ class graph {\n       cmap->clear();\n     if(nmap)\n       nmap->clear();\n-    std::set<node_t> nodes = nodes_;\n+    llvm::SetVector<node_t> nodes = nodes_;\n     unsigned id = 0;\n     while(!nodes.empty()){\n       connected_components_impl(*nodes.begin(), nodes, nmap, cmap, id++);\n@@ -59,7 +60,7 @@ class graph {\n   }\n \n private:\n-  std::set<node_t> nodes_;\n+  llvm::SetVector<node_t> nodes_;\n   edges_t edges_;\n };\n "}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 56, "deletions": 46, "changes": 102, "file_content_changes": "@@ -2311,60 +2311,69 @@ inline Value* generator::shfl_sync(Value* acc, int32_t i){\n }\n \n /**\n- * \\brief Code Generation for `reduce` (1D case)\n+ * \\brief Code Generation for `reduce` (ND case)\n  */\n-void generator::visit_reduce1d_inst(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral) {\n-  std::map<indices_t, Value*> partial;\n+void generator::visit_reducend_inst_fast(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral){\n+  //\n   ir::value *arg = x->get_operand(0);\n+  analysis::scanline_layout* layout = layouts_->get(arg)->to_scanline();\n+  std::vector<unsigned> shapes = layout->get_shape();\n+  std::vector<int> order = layout->get_order();\n+  unsigned mts = layout->mts(order[0]);\n+  unsigned nts = layout->nts(order[0]);\n+  unsigned col_per_thread = shapes[order[0]] / mts;\n+  auto idxs = idxs_.at(arg);\n+  size_t n_elts = idxs.size();\n+  //\n   Type *ret_ty = cvt(x->get_type()->get_scalar_ty());\n-  Value *acc = nullptr;\n-\n-  // reduce within thread\n-  for(indices_t idx: idxs_.at(arg)){\n-    Value *val = vals_[arg][idx];\n-    acc = !acc ? val : do_acc(acc, val);\n-  }\n-  // reduce within wrap\n-  for(int i = 16; i > 0; i >>= 1)\n-    acc = do_acc(acc, shfl_sync(acc, i));\n-  // pointers\n   unsigned addr_space = shmem_->getType()->getPointerAddressSpace();\n   Value *base = bit_cast(shmem_, ptr_ty(ret_ty, addr_space));\n   Value* thread = tgt_->get_local_id(mod_, *builder_, 0);\n   Value* warp = udiv(thread, i32(32));\n   Value* lane = urem(thread, i32(32));\n-  // store warp result in shared memory\n-  add_barrier();\n-  store(neutral, gep(base, lane));\n-  add_barrier();\n-  store(acc, gep(base, warp));\n-  add_barrier();\n-\n-  // reduce across warps\n-  Value *cond = icmp_eq(warp, i32(0));\n-  Instruction *barrier = add_barrier();\n-  builder_->SetInsertPoint(barrier->getParent());\n-  Instruction* dummy = builder_->CreateRet(nullptr);\n-  Instruction *term = llvm::SplitBlockAndInsertIfThen(cond, barrier, false);\n-  dummy->removeFromParent();\n-  builder_->SetInsertPoint(term);\n-  Value* ret = load(gep(base, thread));\n-  for(int i = (num_warps_+1)/2; i > 0; i >>= 1){\n-    Value *current = shfl_sync(ret, i);\n-    ret = do_acc(ret, current);\n-  }\n-  store(ret, gep(base, thread));\n-\n-  // store first warp done\n-  builder_->SetInsertPoint(barrier->getParent());\n-  ret = load(base);\n-  for(indices_t idx: idxs_.at(x))\n-    vals_[x][idx] = ret;\n+  size_t warps_per_inner = std::max<int>(mts/32, 1);\n+  Value* warp_i = udiv(warp, i32(warps_per_inner));\n+  unsigned row_per_thread = std::max<int>(32/mts, 1);\n+\n+  for(size_t i = 0; i < n_elts/col_per_thread; i++){\n+    Value* acc;\n+    // reduce within thread\n+    for(size_t j = 0; j < col_per_thread; j++){\n+      Value* val = vals_[arg][idxs[i*col_per_thread + j]];\n+      acc = (j == 0) ? val : do_acc(acc, val);\n+    }\n+    // reduce within warp\n+    for(int k = std::min<int>(mts, 32)/2 ; k > 0; k >>= 1)\n+      acc = do_acc(acc, shfl_sync(acc, k));\n+    // store warp result in shared memory\n+    Value* ret = acc;\n+    if(mts >= 32){\n+      add_barrier();\n+      store(neutral, gep(base, lane));\n+      add_barrier();\n+      store(acc, gep(base, warp));\n+      add_barrier();\n+      // reduce across warps\n+      Value *cond = icmp_eq(warp, i32(0));\n+      Instruction *barrier = add_barrier();\n+      builder_->SetInsertPoint(barrier->getParent());\n+      Instruction* dummy = builder_->CreateRet(nullptr);\n+      Instruction *term = llvm::SplitBlockAndInsertIfThen(cond, barrier, false);\n+      dummy->removeFromParent();\n+      builder_->SetInsertPoint(term);\n+      ret = load(gep(base, thread));\n+      for(int k = (mts/32)/2; k > 0; k >>= 1){\n+        Value *current = shfl_sync(ret, k);\n+        ret = do_acc(ret, current);\n+      }\n+      store(ret, gep(base, thread));\n+      builder_->SetInsertPoint(barrier->getParent());\n+      ret = load(gep(base, warp));\n+    }\n+    vals_[x][idxs_[x][i]] = ret;\n+  }\n }\n \n-/**\n- * \\brief Code Generation for `reduce` (ND case)\n- */\n void generator::visit_reducend_inst(ir::reduce_inst* x, std::function<Value*(Value*,Value*)> do_acc, Value *neutral) {\n   ir::value *arg = x->get_operand(0);\n   Type *ty = cvt(x->get_type()->get_scalar_ty());\n@@ -2462,8 +2471,9 @@ void generator::visit_reduce_inst(ir::reduce_inst* x) {\n     default: throw std::runtime_error(\"unreachable\");\n   }\n   ir::value *arg = x->get_operand(0);\n-  if(arg->get_type()->get_tile_rank() == 1)\n-    visit_reduce1d_inst(x, do_acc, neutral);\n+  analysis::scanline_layout* scanline = layouts_->get(x->get_operand(0))->to_scanline();\n+  if(scanline && scanline->get_order()[0] == x->get_axis())\n+    visit_reducend_inst_fast(x, do_acc, neutral);\n   else\n     visit_reducend_inst(x, do_acc, neutral);\n }"}, {"filename": "python/bench/bench_blocksparse.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -40,7 +40,7 @@ def bench_matmul(M, N, K, block, layout_mode, op_mode, AT, BT, dtype, provider,\n     # create op\n     tflops = lambda ms: num_flops / ms * 1e3\n     if provider == 'triton':\n-        op = triton.ops.blocksparse.matmul(layout, block, op_mode, trans_a=AT, trans_b=BT)\n+        op = triton.ops.blocksparse.matmul(layout, block, op_mode, device=\"cuda\", trans_a=AT, trans_b=BT)\n         # inputs\n         a = triton.testing.sparsify_tensor(a, layout, block) if op_mode == 'dsd' else a\n         b = triton.testing.sparsify_tensor(b, layout, block) if op_mode == 'dds' else b\n@@ -83,7 +83,7 @@ def bench_softmax(M, N, block, layout_mode, dtype, provider, warmup=10, rep=50):\n     a = torch.randn((Z, H, M, N), dtype=dtype, device='cuda')\n     if provider == 'triton':\n         a = triton.testing.sparsify_tensor(a, layout, block)\n-        op = triton.ops.blocksparse.softmax(layout, block)\n+        op = triton.ops.blocksparse.softmax(layout, block, device=\"cuda\")\n         gbps = lambda ms: (2 * a.numel() * a.element_size() * 1e-9) / (ms * 1e-3)\n         mean_ms, min_ms, max_ms = triton.testing.do_bench(lambda: op(a), warmup=warmup, rep=rep)\n         return gbps(mean_ms), gbps(min_ms), gbps(max_ms)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 43, "deletions": 4, "changes": 47, "file_content_changes": "@@ -514,9 +514,41 @@ def kernel(X, Z):\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n \n+def test_atomic_cas():\n+    # 1. make sure that atomic_cas changes the original value (Lock)\n+    @triton.jit\n+    def change_value(Lock):\n+        tl.atomic_cas(Lock, 0, 1)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    change_value[(1,)](Lock)\n+\n+    assert(Lock[0] == 1)\n+\n+    # 2. only one block enters the critical section\n+    @triton.jit\n+    def serialized_add(data, Lock):\n+        ptrs = data + tl.arange(0, 128)\n+        while tl.atomic_cas(Lock, 0, 1) == 1:\n+            pass\n+\n+        tl.store(ptrs, tl.load(ptrs) + 1.0)\n+\n+        # release lock\n+        tl.atomic_xchg(Lock, 0)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+    ref = torch.full((128,), 64.0)\n+    serialized_add[(64,)](data, Lock)\n+    triton.testing.assert_almost_equal(data, ref)\n+\n+\n # ---------------\n # test cast\n # ---------------\n+\n+\n @pytest.mark.parametrize(\"dtype_x, dtype_z, bitcast\", [\n     (dtype_x, dtype_z, False)\n     for dtype_x in dtypes\n@@ -644,7 +676,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     )\n     assert torch.all(\n         torch.logical_not(mismatch)\n-    ), f\"{f16_input[mismatch]=} {f16_output[mismatch]=} {abs_error[mismatch]=} {min_error[mismatch]=}\"\n+    ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n \n \n # ---------------\n@@ -676,9 +708,16 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n \n-@pytest.mark.parametrize(\"dtype_str, shape, axis\", [\n-    (dtype, (1, 1024), 1) for dtype in ['float32', 'uint32']\n-])\n+reduce_configs1 = [\n+    (dtype, (1, 1024), axis) for dtype in ['float32', 'uint32']\n+    for axis in [1]\n+]\n+reduce_configs2 = [\n+    ('float32', shape, 1) for shape in [(2, 32), (4, 128), (32, 64), (64, 128), (128, 256), (32, 1024)]\n+]\n+\n+\n+@pytest.mark.parametrize(\"dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n def test_reduce2d(dtype_str, shape, axis, device='cuda'):\n     # triton kernel\n     @triton.jit"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 45, "deletions": 26, "changes": 71, "file_content_changes": "@@ -11,6 +11,7 @@\n import sys\n import tempfile\n import textwrap\n+import threading\n import time\n import warnings\n from typing import Dict, Set, Tuple, Union\n@@ -22,12 +23,17 @@\n import triton._C.libtriton.triton as _triton\n from .tools.disasm import extract\n \n+try:\n+    from torch._C import _cuda_getCurrentRawStream as get_cuda_stream\n+except ImportError:\n+    get_cuda_stream = lambda dev_idx: torch.cuda.current_stream(dev_idx).cuda_stream\n+\n \n def current_cuda_stream(device_idx=0):\n     # Torch's torch.cuda.current_stream() is slow. We provide this\n     # function to give the user an opportunity to monkey-patch their\n     # own faster current stream lookup.\n-    return torch.cuda.current_stream().cuda_stream\n+    return get_cuda_stream(device_idx)\n \n \n def mangle_ty(ty):\n@@ -921,6 +927,7 @@ def pow2_divisor(N):\n \n     def __init__(self, fn):\n         self.fn = fn\n+        self.cache_key = {}\n \n     def add_to_cache(self, key, wargs, device_idx, num_warps, num_stages):\n         tensor_idxs = [i for i, arg in enumerate(wargs) if hasattr(arg, 'data_ptr')]\n@@ -962,12 +969,11 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n         #         assert arg.is_cuda, \"All tensors must be on GPU!\"\n         # set device (i.e., make sure torch has the context initialized)\n         device = torch.cuda.current_device()\n-        torch.cuda.set_device(device)\n-        # query compute capability\n-        cc = torch.cuda.get_device_capability(device)\n-        cc = str(cc[0]) + '-' + str(cc[1])\n-        cache_key = self.fn.cache_key + cc\n-        # query current stream\n+        if device not in self.cache_key:\n+            cc = torch.cuda.get_device_capability(device)\n+            cc = str(cc[0]) + '-' + str(cc[1])\n+            self.cache_key[device] = self.fn.cache_key + cc\n+        cache_key = self.cache_key[device]\n         stream = current_cuda_stream(device)\n         return _triton.runtime.launch(wargs, self.fn.do_not_specialize, cache_key, self.fn.arg_names,\n                                       device, stream, self.fn.bin_cache, num_warps, num_stages, self.add_to_cache,\n@@ -1070,27 +1076,40 @@ def __call__(self, *args, **kwargs):\n         return self.kernel(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n \n \n-@functools.lru_cache()\n+_version_key_lock = threading.Lock()\n+_version_key = None\n+\n+\n def version_key():\n-    import pkgutil\n-    contents = []\n-    # frontend\n-    with open(triton.code_gen.__file__, \"rb\") as f:\n-        contents += [hashlib.md5(f.read()).hexdigest()]\n-    # backend\n-    with open(triton._C.libtriton.__file__, \"rb\") as f:\n-        contents += [hashlib.md5(f.read()).hexdigest()]\n-    # language\n-    language_path = os.path.join(*triton.__path__, 'language')\n-    for lib in pkgutil.iter_modules([language_path]):\n-        with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n+    global _version_key\n+\n+    if _version_key is not None:\n+        return _version_key\n+\n+    with _version_key_lock:\n+        if _version_key is not None:\n+            return _version_key\n+\n+        import pkgutil\n+        contents = []\n+        # frontend\n+        with open(triton.code_gen.__file__, \"rb\") as f:\n             contents += [hashlib.md5(f.read()).hexdigest()]\n-    # ptxas version\n-    try:\n-        ptxas_version = hashlib.md5(subprocess.check_output([\"ptxas\", \"--version\"])).hexdigest()\n-    except Exception:\n-        ptxas_version = ''\n-    return '-'.join(triton.__version__) + '-' + ptxas_version + '-' + '-'.join(contents)\n+        # backend\n+        with open(triton._C.libtriton.__file__, \"rb\") as f:\n+            contents += [hashlib.md5(f.read()).hexdigest()]\n+        # language\n+        language_path = os.path.join(*triton.__path__, 'language')\n+        for lib in pkgutil.iter_modules([language_path]):\n+            with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n+                contents += [hashlib.md5(f.read()).hexdigest()]\n+        # ptxas version\n+        try:\n+            ptxas_version = hashlib.md5(subprocess.check_output([\"ptxas\", \"--version\"])).hexdigest()\n+        except Exception:\n+            ptxas_version = ''\n+        _version_key = '-'.join(triton.__version__) + '-' + ptxas_version + '-' + '-'.join(contents)\n+        return _version_key\n \n \n class DependenciesFinder(ast.NodeVisitor):"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -806,7 +806,7 @@ def _decorator(func):\n @_add_atomic_docstr(\"compare-and-swap\")\n def atomic_cas(pointer, cmp, val, _builder=None):\n     cmp = _to_tensor(cmp, _builder)\n-    val = _to_tensor(cmp, _builder)\n+    val = _to_tensor(val, _builder)\n     return semantic.atomic_cas(pointer, cmp, val, _builder)\n \n "}, {"filename": "python/triton/tools/compare_asm.py", "status": "added", "additions": 76, "deletions": 0, "changes": 76, "file_content_changes": "@@ -0,0 +1,76 @@\n+'''\n+Compare cached triton kernels in 2 directories.\n+\n+example:\n+python compare_asm.py --dir0=triton-works/ --dir1=triton-fails/ --asm=ttir \\\n+       --diff-out0=diff-works.ll --diff-out1=diff-fails.ll\n+'''\n+import argparse\n+import os\n+import pickle\n+\n+parser = argparse.ArgumentParser(description=\"unpickle\")\n+parser.add_argument('--dir0', dest='dir0', required=True,\n+                    help=\"Triton cache dir 0\")\n+parser.add_argument('--dir1', dest='dir1', required=True,\n+                    help=\"Triton cache dir 1\")\n+parser.add_argument('--asm', dest='asm',\n+                    choices=['ttir', 'llir', 'ptx', 'cubin'], required=True)\n+parser.add_argument('--early-stop', dest='early_stop', action='store_true',\n+                    help=\"Stop after first diff\")\n+parser.set_defaults(early_stop=True)\n+parser.add_argument('--diff-out0', dest='diff_out0', required=True,\n+                    help=\"output file path for kernels in dir0\")\n+parser.add_argument('--diff-out1', dest='diff_out1', required=True,\n+                    help=\"output file path for kernels in dir1\")\n+args = parser.parse_args()\n+dir0 = args.dir0\n+dir1 = args.dir1\n+asm = args.asm\n+\n+dir0_files = {}\n+dir1_files = {}\n+for root, _, files in os.walk(dir0):\n+    for file in files:\n+        if not file.endswith('.lock'):\n+            path = os.path.join(root, file)\n+            with open(path, 'rb') as f:\n+                loaded_file = pickle.load(f)\n+                bin = loaded_file['binary']\n+                key = loaded_file['key']\n+                info = key.split('-')[-3:]  # num_warps, num_stages, signature\n+                dict_key = bin.name + '-'.join(info)\n+                dir0_files[dict_key] = bin.asm\n+\n+for root, _, files in os.walk(dir1):\n+    for file in files:\n+        if not file.endswith('.lock'):\n+            path = os.path.join(root, file)\n+            with open(path, 'rb') as f:\n+                loaded_file = pickle.load(f)\n+                bin = loaded_file['binary']\n+                key = loaded_file['key']\n+                info = key.split('-')[-3:]  # num_warps, num_stages, signature\n+                dict_key = bin.name + '-'.join(info)\n+                dir1_files[dict_key] = bin.asm\n+\n+diff_keys = []\n+for key in dir0_files:\n+    asm0 = dir0_files[key]\n+    if key not in dir1_files:\n+        continue\n+    asm1 = dir1_files[key]\n+    if asm0[asm] != asm1[asm]:\n+        diff_keys.append(key)\n+\n+if args.early_stops:\n+    diff_keys = diff_keys[:1]\n+if diff_keys:\n+    with open(args.diff_out0, 'w') as f0, open(args.diff_out1, 'w') as f1:\n+        for key in diff_keys:\n+            f0.write(f'{asm} mismatch at {key}')\n+            f0.write(dir0_files[key][asm])\n+            f0.write('\\n')\n+            f1.write(f'{asm} mismatch at {key}')\n+            f1.write(dir1_files[key][asm])\n+            f1.write('\\n')"}]