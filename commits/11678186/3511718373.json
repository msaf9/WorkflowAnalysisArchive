[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -74,11 +74,9 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n   let builders = [\n     AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n                      \"ArrayRef<int64_t>\":$shape,\n+                     \"ArrayRef<unsigned>\":$order,\n                      \"Type\":$eltTy), [{\n         auto mmaEnc = dotOpEnc.getParent().dyn_cast<MmaEncodingAttr>();\n-        // Only support row major for now\n-        // TODO(Keren): check why column major code crashes\n-        SmallVector<unsigned> order = {1, 0};\n \n         if(!mmaEnc)\n           return $_get(context, 1, 1, 1, order);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -464,21 +464,6 @@ struct SharedMemoryObject {\n     }\n   }\n \n-  // XXX(Keren): a special allocator for 3d tensors. It's a workaround for\n-  // now since we don't have a correct way to encoding 3d tensors in the\n-  // pipeline pass.\n-  SharedMemoryObject(Value base, ArrayRef<int64_t> shape, Location loc,\n-                     ConversionPatternRewriter &rewriter)\n-      : base(base) {\n-    auto stride = 1;\n-    for (auto dim : llvm::reverse(shape)) {\n-      strides.emplace_back(i32_val(stride));\n-      offsets.emplace_back(i32_val(0));\n-      stride *= dim;\n-    }\n-    strides = llvm::to_vector<4>(llvm::reverse(strides));\n-  }\n-\n   SmallVector<Value> getElems() const {\n     SmallVector<Value> elems;\n     elems.push_back(base);\n@@ -2255,8 +2240,18 @@ struct AllocTensorOpConversion\n         getTypeConverter()->convertType(resultTy.getElementType());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n     smemBase = bitcast(smemBase, elemPtrTy);\n+    auto order = resultTy.getEncoding().cast<SharedEncodingAttr>().getOrder();\n+    // workaround for 3D tensors\n+    // TODO: We need to modify the pipeline pass to give a proper shared encoding to 3D tensors\n+    SmallVector<unsigned> newOrder;\n+    if (resultTy.getShape().size() == 3) \n+      newOrder = {1 + order[0], 1 + order[1], 0};\n+    else\n+      newOrder = SmallVector<unsigned>(order.begin(), order.end());\n+\n+    \n     auto smemObj =\n-        SharedMemoryObject(smemBase, resultTy.getShape(), loc, rewriter);\n+        SharedMemoryObject(smemBase, resultTy.getShape(), newOrder, loc, rewriter);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n     rewriter.replaceOp(op, retVal);\n     return success();\n@@ -2306,6 +2301,10 @@ struct ExtractSliceOpConversion\n         strideVals.emplace_back(smemObj.strides[i]);\n       }\n     }\n+\n+    // llvm::outs() << \"extract slice\\n\";\n+    // llvm::outs() << strideVals[0] << \" \" << smemObj.strides[1] << \"\\n\";\n+    // llvm::outs() << strideVals[1] << \" \" << smemObj.strides[2] << \"\\n\";\n     auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n     auto resTy = op.getType().dyn_cast<RankedTensorType>();\n@@ -3266,8 +3265,8 @@ class MMA16816SmemLoader {\n     cMatShape = matShape[order[0]];\n     sMatShape = matShape[order[1]];\n \n-    cStride = smemStrides[1];\n-    sStride = smemStrides[0];\n+    cStride = smemStrides[order[0]];\n+    sStride = smemStrides[order[1]];\n \n     // rule: k must be the fast-changing axis.\n     needTrans = kOrder != order[0];\n@@ -6207,6 +6206,7 @@ class ConvertTritonGPUToLLVM\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::SharedEncodingAttr::get(mod.getContext(), dstDotOp,\n                                                  srcType.getShape(),\n+                                                 getOrder(srcBlocked),\n                                                  srcType.getElementType()));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -201,7 +201,9 @@ LogicalResult LoopPipeliner::initialize() {\n                                              ty.getShape().end());\n             bufferShape.insert(bufferShape.begin(), numStages);\n             auto sharedEnc = ttg::SharedEncodingAttr::get(\n-                ty.getContext(), dotOpEnc, ty.getShape(), ty.getElementType());\n+                ty.getContext(), dotOpEnc, ty.getShape(),\n+                triton::gpu::getOrder(ty.getEncoding()),\n+                ty.getElementType());\n             loadsBufferType[loadOp] = RankedTensorType::get(\n                 bufferShape, ty.getElementType(), sharedEnc);\n           }"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -186,9 +186,9 @@ def get_proper_err(a, b, golden):\n     [128, 256, 128, 4, 128, 256, 32, False, False],\n     [256, 128, 64, 4, 256, 128, 16, False, False],\n     [128, 64, 128, 4, 128, 64, 32, False, False],\n-    # TODO[goostavz]: fix these cases\n-    #[128, 64, 128, 4, 128, 64, 32, True, False],\n-    #[128, 64, 128, 4, 128, 64, 32, False, True],\n+    # trans\n+    [128, 64, 128, 4, 128, 64, 32, True, False],\n+    [128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n     if (TRANS_A):"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -882,14 +882,14 @@ def ttir_to_ttgir(mod, num_warps, num_stages):\n     pm.enable_debug()\n     # Convert blocked layout to mma layout for dot ops so that pipeline\n     # can get shared memory swizzled correctly.\n+    pm.add_coalesce_pass()\n     pm.add_triton_gpu_combine_pass()\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     # Prefetch must be done after pipeline pass because pipeline pass\n     # extracts slices from the original tensor.\n     pm.add_tritongpu_prefetch_pass()\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n-    pm.add_coalesce_pass()\n     pm.add_triton_gpu_combine_pass()\n     pm.add_licm_pass()\n     pm.add_triton_gpu_combine_pass()"}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -34,7 +34,7 @@ TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n \n   // create element type\n   Type eltType = IntegerType::get(&ctx, params.typeWidth);\n-  auto layout = SharedEncodingAttr::get(&ctx, encoding, params.shape, eltType);\n+  auto layout = SharedEncodingAttr::get(&ctx, encoding, params.shape, {1, 0}, eltType);\n \n   ASSERT_EQ(layout.getVec(), params.refSwizzle.vec);\n   ASSERT_EQ(layout.getPerPhase(), params.refSwizzle.perPhase);"}]