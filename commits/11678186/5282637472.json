[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 43, "deletions": 5, "changes": 48, "file_content_changes": "@@ -25,7 +25,7 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"gfx908\"]]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n           else\n             echo '::set-output name=matrix::[\"ubuntu-latest\"]'\n           fi\n@@ -53,6 +53,11 @@ jobs:\n         run: |\n           echo \"BACKEND=ROCM\" >> \"${GITHUB_ENV}\"\n \n+      - name: Set XPU ENV\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'arc770')}}\n+        run: |\n+          echo \"BACKEND=XPU\" >> \"${GITHUB_ENV}\"\n+\n       - name: Clear cache\n         run: |\n           rm -rf ~/.triton\n@@ -62,13 +67,22 @@ jobs:\n           echo \"PATH=${HOME}/.local/bin:${PATH}\" >> \"${GITHUB_ENV}\"\n \n       - name: Check pre-commit\n-        if: ${{ matrix.runner != 'macos-10.15' }}\n+        if: ${{ matrix.runner != 'macos-10.15' && (matrix.runner[1] != 'arc770') }}\n         run: |\n           python3 -m pip install --upgrade pre-commit\n           python3 -m pre_commit run --all-files\n \n+      - name: Check pre-commit arc770\n+        if: ${{ matrix.runner != 'macos-10.15' && (matrix.runner[1] == 'arc770') }}\n+        run: |\n+          source ${HOME}/triton_vars.sh\n+          source ${HOME}/miniconda3/bin/activate\n+          conda activate triton-xpu-ci\n+          python3 -m pip install --upgrade pre-commit\n+          python3 -m pre_commit run --all-files\n+\n       - name: Install Triton\n-        if: ${{ env.BACKEND != 'ROCM'}}\n+        if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n           cd python\n           python3 -m pip install --upgrade pip\n@@ -84,8 +98,23 @@ jobs:\n           python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n+      - name: Install Triton on XPU\n+        if: ${{ env.BACKEND == 'XPU'}}\n+        run: |\n+          source ${HOME}/triton_vars.sh\n+          source ${HOME}/miniconda3/bin/activate\n+          conda activate triton-xpu-ci\n+          git submodule update --init --recursive\n+          cd python\n+          python3 -m pip install --upgrade pip\n+          python3 -m pip install cmake==3.24\n+          export TRITON_CODEGEN_INTEL_XPU_BACKEND=1\n+          python3 -m pip uninstall -y triton\n+          python3 setup.py build\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n+\n       - name: Run lit tests\n-        if: ${{ env.BACKEND != 'ROCM'}}\n+        if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n           python3 -m pip install lit\n           cd python\n@@ -115,7 +144,7 @@ jobs:\n           path: ~/.triton/artifacts.tar.gz\n \n       - name: Run CXX unittests\n-        if: ${{ env.BACKEND != 'ROCM'}}\n+        if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n           cd python\n           cd \"build/$(ls build | grep -i cmake)\"\n@@ -127,6 +156,15 @@ jobs:\n           cd python/test/unit/language\n           python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n \n+      - name: Run python tests on XPU\n+        if: ${{ env.BACKEND == 'XPU'}}\n+        run: |\n+          source ${HOME}/triton_vars.sh\n+          source ${HOME}/miniconda3/bin/activate\n+          conda activate triton-xpu-ci\n+          cd python/test/backend/third_party_backends\n+          python3 -m pytest --capture=tee-sys -rfs --verbose --backend xpu\n+\n       - name: Regression tests\n         if: ${{ contains(matrix.runner, 'A100') }}\n         run: |"}, {"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -16,7 +16,11 @@ jobs:\n     steps:\n \n       - name: Checkout\n-        uses: actions/checkout@v2\n+        uses: actions/checkout@v3\n+\n+      - name: Install Azure CLI\n+        run: |\n+          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n \n       - name: Azure login\n         uses: azure/login@v1\n@@ -46,8 +50,8 @@ jobs:\n           export CIBW_MANYLINUX_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n           #export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n           export CIBW_BEFORE_BUILD=\"pip install cmake;\"\n-          export CIBW_SKIP=\"{cp,pp}35-*\"\n-          export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64 cp3*-musllinux_x86_64\"\n+          export CIBW_SKIP=\"{cp,pp}{35,36}-*\"\n+          export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n           python3 -m cibuildwheel python --output-dir wheelhouse\n \n       - name: Publish wheels to Azure DevOps"}, {"filename": ".gitmodules", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+[submodule \"third_party/intel_xpu_backend\"]\n+\tpath = third_party/intel_xpu_backend\n+\turl = http://github.com/intel/intel-xpu-backend-for-triton"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -22,6 +22,7 @@ endif()\n # Options\n option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n+set(TRITON_CODEGEN_BACKENDS \"\" CACHE STRING \"Enable different codegen backends\")\n \n # Ensure Python3 vars are set correctly\n # used conditionally in this file and by lit tests\n@@ -263,6 +264,14 @@ if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n   target_link_libraries(triton ${CUTLASS_LIBRARIES} ${PYTHON_LDFLAGS})\n endif()\n \n+list(LENGTH TRITON_CODEGEN_BACKENDS CODEGEN_BACKENDS_LEN)\n+if (${CODEGEN_BACKENDS_LEN} GREATER 0)\n+  set(PYTHON_THIRD_PARTY_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/triton/third_party)\n+  foreach(CODEGEN_BACKEND ${TRITON_CODEGEN_BACKENDS})\n+    add_subdirectory(third_party/${CODEGEN_BACKEND})\n+  endforeach()\n+endif()\n+\n add_subdirectory(test)\n \n add_subdirectory(unittest)"}, {"filename": "README.md", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -25,12 +25,12 @@ You can install the latest stable release of Triton from pip:\n ```bash\n pip install triton\n ```\n-Binary wheels are available for CPython 3.6-3.11 and PyPy 3.7-3.9.\n+Binary wheels are available for CPython 3.8-3.11 and PyPy 3.8-3.9.\n \n And the latest nightly release:\n \n ```bash\n-pip install -U --pre triton\n+pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly\n ```\n \n # Install from source"}, {"filename": "docs/getting-started/installation.rst", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -12,13 +12,13 @@ You can install the latest stable release of Triton from pip:\n \n       pip install triton\n \n-Binary wheels are available for CPython 3.6-3.9 and PyPy 3.6-3.7.\n+Binary wheels are available for CPython 3.8-3.11 and PyPy 3.8-3.9.\n \n And the latest nightly release:\n \n .. code-block:: bash\n \n-      pip install -U --pre triton\n+      pip install -U --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly\n \n \n -----------"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "file_content_changes": "@@ -69,9 +69,11 @@ bool supportMMA(triton::DotOp op, int version);\n \n bool supportMMA(Value value, int version);\n \n-Type getElementType(Value value);\n+bool isSingleValue(Value value);\n \n-std::string getValueOperandName(Value value, AsmState &state);\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n+\n+Type getElementType(Value value);\n \n template <typename T_OUT, typename T_IN>\n inline SmallVector<T_OUT> convertType(ArrayRef<T_IN> in) {\n@@ -120,10 +122,6 @@ template <typename T> T nextPowOf2(T n) {\n   return n + 1;\n }\n \n-bool isSingleValue(Value value);\n-\n-bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n-\n /// Multi-root DAG topological sort.\n /// Performs a topological sort of the Operation in the `toSort` SetVector.\n /// Returns a topologically sorted SetVector."}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -75,13 +75,11 @@ SmallVector<unsigned> getOrder(Attribute layout);\n \n bool isaDistributedLayout(Attribute layout);\n \n-bool expensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n+bool isSharedEncoding(Value value);\n \n } // namespace gpu\n } // namespace triton\n \n-bool isSharedEncoding(Value value);\n-\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "renamed", "additions": 11, "deletions": 5, "changes": 16, "file_content_changes": "@@ -1,9 +1,13 @@\n-#ifndef TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n-#define TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#ifndef TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#define TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"llvm/ADT/MapVector.h\"\n \n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n namespace mlir {\n \n LogicalResult fixupLoops(ModuleOp mod);\n@@ -12,9 +16,11 @@ LogicalResult fixupLoops(ModuleOp mod);\n LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n                              Attribute &ret);\n \n-bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+\n+bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n \n-bool expensiveToRemat(Operation *op, Attribute &targetEncoding);\n+bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n // skipInit is True when we only consider the operands of the initOp but\n // not the initOp itself.\n@@ -36,4 +42,4 @@ LogicalResult canMoveOutOfLoop(BlockArgument arg,\n \n } // namespace mlir\n \n-#endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#endif // TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "include/triton/Target/AMDGCN/AMDGCNTranslation.h", "status": "removed", "additions": 0, "deletions": 19, "changes": 19, "file_content_changes": "@@ -1,19 +0,0 @@\n-#ifndef TRITON_TARGET_AMDGCNTRANSLATION_H\n-#define TRITON_TARGET_AMDGCNTRANSLATION_H\n-\n-#include <string>\n-#include <tuple>\n-\n-namespace llvm {\n-class Module;\n-} // namespace llvm\n-\n-namespace triton {\n-\n-// Translate LLVM IR to AMDGCN code.\n-std::tuple<std::string, std::string>\n-translateLLVMIRToAMDGCN(llvm::Module &module, std::string cc);\n-\n-} // namespace triton\n-\n-#endif"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -38,7 +38,7 @@ void SharedMemoryAliasAnalysis::visitOperation(\n       // insert_slice %src into %dst[%offsets]\n       aliasInfo = AliasInfo(operands[1]->getValue());\n       pessimistic = false;\n-    } else if (isSharedEncoding(result)) {\n+    } else if (triton::gpu::isSharedEncoding(result)) {\n       aliasInfo.insert(result);\n       pessimistic = false;\n     }"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -151,7 +151,7 @@ class AllocationAnalysis {\n     }\n \n     for (Value result : op->getResults()) {\n-      if (isSharedEncoding(result)) {\n+      if (triton::gpu::isSharedEncoding(result)) {\n         // Bytes could be a different value once we support padding or other\n         // allocation policies.\n         auto tensorType = result.getType().dyn_cast<RankedTensorType>();"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 3, "deletions": 10, "changes": 13, "file_content_changes": "@@ -60,9 +60,9 @@ SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n \n   auto argLayout = getSrcLayout();\n   auto argLayoutMma = argLayout.dyn_cast<triton::gpu::MmaEncodingAttr>();\n-  if (argLayoutMma && argLayoutMma.getVersionMajor() == 2 &&\n-      triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n-    return {{1, 1}, {1, 1}};\n+  // if (argLayoutMma && argLayoutMma.getVersionMajor() == 2 &&\n+  //     triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n+  //   return {{1, 1}, {1, 1}};\n \n   /// shared memory block0\n   smemShapes[0] = convertType<unsigned>(getSrcShape());\n@@ -163,13 +163,6 @@ Type getElementType(Value value) {\n   return type;\n }\n \n-std::string getValueOperandName(Value value, AsmState &state) {\n-  std::string opName;\n-  llvm::raw_string_ostream ss(opName);\n-  value.printAsOperand(ss, state);\n-  return opName;\n-}\n-\n bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy) {\n   // dot_op<opIdx=0, parent=#mma> = #mma\n   // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 13, "deletions": 24, "changes": 37, "file_content_changes": "@@ -7,6 +7,7 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.cpp.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n \n using namespace mlir;\n@@ -354,22 +355,6 @@ bool isaDistributedLayout(Attribute layout) {\n          layout.isa<SliceEncodingAttr>();\n }\n \n-bool expensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n-  // If the new elements per thread is less than the old one, we will need to do\n-  // convert encoding that goes through shared memory anyway. So we consider it\n-  // as expensive.\n-  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n-  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n-  auto shape = tensorTy.getShape();\n-  auto elemTy = tensorTy.getElementType();\n-  auto newTotalElemsPerThread =\n-      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n-  return newTotalElemsPerThread < totalElemsPerThread;\n-}\n-\n-} // namespace gpu\n-} // namespace triton\n-\n bool isSharedEncoding(Value value) {\n   auto type = value.getType();\n   if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n@@ -379,6 +364,9 @@ bool isSharedEncoding(Value value) {\n   return false;\n }\n \n+} // namespace gpu\n+} // namespace triton\n+\n } // namespace mlir\n \n static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,\n@@ -1142,7 +1130,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   if (auto cat = dyn_cast<triton::CatOp>(arg)) {\n     auto encoding =\n         op->getResult(0).getType().cast<RankedTensorType>().getEncoding();\n-    if (triton::gpu::expensiveCat(cat, encoding))\n+    if (isExpensiveCat(cat, encoding))\n       return mlir::failure();\n     rewriter.replaceOpWithNewOp<triton::CatOp>(op, op->getResult(0).getType(),\n                                                cat.getOperands());\n@@ -1151,7 +1139,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n   auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n   if (alloc_tensor) {\n-    if (!isSharedEncoding(op->getResult(0))) {\n+    if (!triton::gpu::isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n     }\n     rewriter.replaceOpWithNewOp<triton::gpu::AllocTensorOp>(\n@@ -1161,7 +1149,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(insert_slice(x), type2) -> insert_slice(cvt(x, type2))\n   auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n   if (insert_slice) {\n-    if (!isSharedEncoding(op->getResult(0))) {\n+    if (!triton::gpu::isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n     }\n     auto newType = op->getResult(0).getType().cast<RankedTensorType>();\n@@ -1183,7 +1171,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n   auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n   if (extract_slice) {\n-    if (!isSharedEncoding(op->getResult(0))) {\n+    if (!triton::gpu::isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n     }\n     auto origType =\n@@ -1213,12 +1201,13 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(cvt(x, type1), type2) -> cvt(x, type2)\n   if (llvm::isa<triton::gpu::ConvertLayoutOp>(arg)) {\n     if (arg->getOperand(0).getDefiningOp() &&\n-        !isSharedEncoding(arg->getOperand(0)) &&\n-        isSharedEncoding(op.getOperand()) &&\n-        !isSharedEncoding(op.getResult())) {\n+        !triton::gpu::isSharedEncoding(arg->getOperand(0)) &&\n+        triton::gpu::isSharedEncoding(op.getOperand()) &&\n+        !triton::gpu::isSharedEncoding(op.getResult())) {\n       return mlir::failure();\n     }\n-    if (isSharedEncoding(op.getOperand()) && isSharedEncoding(op.getResult())) {\n+    if (triton::gpu::isSharedEncoding(op.getOperand()) &&\n+        triton::gpu::isSharedEncoding(op.getResult())) {\n       return mlir::failure();\n     }\n     auto srcType = op.getOperand().getType().cast<RankedTensorType>();"}, {"filename": "lib/Dialect/TritonGPU/IR/Traits.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -7,7 +7,7 @@ mlir::OpTrait::impl::verifyResultsAreSharedEncoding(Operation *op) {\n     return failure();\n \n   for (auto result : op->getResults())\n-    if (!isSharedEncoding(result))\n+    if (!triton::gpu::isSharedEncoding(result))\n       return op->emitOpError() << \"requires all results to be shared encoding\";\n \n   return success();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -47,7 +47,7 @@ SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n   auto filter = [&dotOp](Operation *op) {\n     return op->getParentRegion() == dotOp->getParentRegion();\n   };\n-  auto slices = mlir::getSlice(dotOp, filter);\n+  auto slices = mlir::getSlice(dotOp, {filter});\n   for (Operation *op : slices)\n     if (isa<triton::DotOp>(op) && (op != dotOp))\n       return {(unsigned)numWarps, 1};\n@@ -113,8 +113,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n     if (versionMajor == 1) {\n       SetVector<Operation *> aBwdSlices, bBwdSlices;\n       auto isCvt = [](Operation *op) { return isa<ConvertLayoutOp>(op); };\n-      getBackwardSlice(a, &aBwdSlices, isCvt);\n-      getBackwardSlice(b, &bBwdSlices, isCvt);\n+      getBackwardSlice(a, &aBwdSlices, {isCvt});\n+      getBackwardSlice(b, &bBwdSlices, {isCvt});\n       // get the source of the first conversion found in slices\n       auto getCvtArgOrder = [](Operation *op) {\n         return cast<ConvertLayoutOp>(op)"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,11 +1,11 @@\n-#include \"Utility.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include <memory>\n \n using namespace mlir;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,3 @@\n-#include \"Utility.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n@@ -8,6 +7,7 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/MapVector.h\"\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -172,7 +172,7 @@ LogicalResult Prefetcher::initialize() {\n         break;\n       rets.push_back(op->getOperand(0));\n       if (auto cvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(op))\n-        if (isSharedEncoding(cvt.getOperand())) {\n+        if (triton::gpu::isSharedEncoding(cvt.getOperand())) {\n           foundConvertFromShared = true;\n           break;\n         }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 7, "deletions": 8, "changes": 15, "file_content_changes": "@@ -1,4 +1,3 @@\n-#include \"Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n@@ -16,6 +15,7 @@\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n \n #include <memory>\n \n@@ -74,9 +74,8 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n           dstType.getShape(), dstType.getElementType(), dstParentMma);\n       auto tmp = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           convert.getLoc(), tmpType, convert.getOperand());\n-      auto newConvert = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          convert.getLoc(), dstType, tmp);\n-      rewriter.replaceOp(op, {newConvert});\n+      rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(op, dstType,\n+                                                                tmp);\n       return mlir::success();\n     }\n     return mlir::failure();\n@@ -353,13 +352,13 @@ class RematerializeForward : public mlir::RewritePattern {\n              !(isa<triton::ReduceOp>(op) &&\n                !op->getResult(0).getType().isa<RankedTensorType>());\n     };\n-    mlir::getForwardSlice(cvt.getResult(), &cvtSlices, filter);\n+    mlir::getForwardSlice(cvt.getResult(), &cvtSlices, {filter});\n     if (cvtSlices.empty())\n       return failure();\n \n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensiveToRemat(op, srcEncoding))\n+      if (isExpensiveToRemat(op, srcEncoding))\n         return failure();\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n@@ -408,8 +407,8 @@ class RematerializeBackward : public mlir::RewritePattern {\n     if (!op)\n       return mlir::failure();\n     // we don't want to rematerialize any conversion to/from shared\n-    if (isSharedEncoding(cvt->getResults()[0]) ||\n-        isSharedEncoding(cvt->getOperand(0)))\n+    if (triton::gpu::isSharedEncoding(cvt->getResults()[0]) ||\n+        triton::gpu::isSharedEncoding(cvt->getOperand(0)))\n       return mlir::failure();\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accommodate fused attention"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 26, "deletions": 16, "changes": 42, "file_content_changes": "@@ -1,10 +1,10 @@\n-#include \"Utility.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n \n namespace mlir {\n \n@@ -88,32 +88,42 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n   return success();\n }\n \n-bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n+bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   // Case 1: A size 1 tensor is not expensive since all threads will load the\n   // same\n   if (isSingleValue(op->getOperand(0)))\n     return false;\n   // Case 2: Tensor of pointers has more threads than elements\n   // we can presume a high hit-rate that makes it cheap to load\n   auto ptrType = op->getOperand(0).getType().cast<RankedTensorType>();\n-  IntegerAttr numWarps =\n-      op->getParentOfType<ModuleOp>()->getAttrOfType<IntegerAttr>(\n-          \"triton_gpu.num-warps\");\n-  if (numWarps) {\n-    int sizePerThread = triton::gpu::getTotalElemsPerThread(ptrType);\n-    if (ptrType.getNumElements() < numWarps.getInt() * 32)\n-      return false;\n-  }\n+  auto mod = op->getParentOfType<ModuleOp>();\n+  int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+  int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n+  if (ptrType.getNumElements() < numWarps * threadsPerWarp)\n+    return false;\n   return true;\n }\n \n-bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n+bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n+  // If the new elements per thread is less than the old one, we will need to do\n+  // convert encoding that goes through shared memory anyway. So we consider it\n+  // as expensive.\n+  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n+  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n+  auto shape = tensorTy.getShape();\n+  auto elemTy = tensorTy.getElementType();\n+  auto newTotalElemsPerThread =\n+      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n+  return newTotalElemsPerThread < totalElemsPerThread;\n+}\n+\n+bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n-    return expensiveLoadOrStore(op, targetEncoding);\n+    return isExpensiveLoadOrStore(op, targetEncoding);\n   if (isa<triton::CatOp>(op))\n-    return triton::gpu::expensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n@@ -126,7 +136,7 @@ bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n \n bool canFoldConversion(Operation *op, Attribute &targetEncoding) {\n   if (isa<triton::CatOp>(op))\n-    return !triton::gpu::expensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return !isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n              triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }\n@@ -148,7 +158,7 @@ int simulateBackwardRematerialization(\n     queue.pop_back();\n     // If the current operation is expensive to rematerialize,\n     // we stop everything\n-    if (expensiveToRemat(currOp, currLayout))\n+    if (isExpensiveToRemat(currOp, currLayout))\n       break;\n     // A conversion will be removed here (i.e. transferred to operands)\n     numCvts -= 1;"}, {"filename": "python/setup.py", "status": "modified", "additions": 18, "deletions": 4, "changes": 22, "file_content_changes": "@@ -31,6 +31,17 @@ def get_build_type():\n         # TODO: change to release when stable enough\n         return \"TritonRelBuildWithAsserts\"\n \n+\n+def get_codegen_backends():\n+    backends = []\n+    env_prefix = \"TRITON_CODEGEN_\"\n+    for name, _ in os.environ.items():\n+        if name.startswith(env_prefix) and check_env_flag(name):\n+            assert name.count(env_prefix) <= 1\n+            backends.append(name.replace(env_prefix, '').lower())\n+    return backends\n+\n+\n # --- third party packages -----\n \n \n@@ -210,6 +221,11 @@ def build_extension(self, ext):\n         cfg = get_build_type()\n         build_args = [\"--config\", cfg]\n \n+        codegen_backends = get_codegen_backends()\n+        if len(codegen_backends) > 0:\n+            all_codegen_backends = ';'.join(codegen_backends)\n+            cmake_args += [\"-DTRITON_CODEGEN_BACKENDS=\" + all_codegen_backends]\n+\n         if platform.system() == \"Windows\":\n             cmake_args += [f\"-DCMAKE_RUNTIME_OUTPUT_DIRECTORY_{cfg.upper()}={extdir}\"]\n             if sys.maxsize > 2**32:\n@@ -249,16 +265,14 @@ def build_extension(self, ext):\n         \"triton/_C\",\n         \"triton/common\",\n         \"triton/compiler\",\n-        \"triton/debugger\",\n+        \"triton/interpreter\",\n         \"triton/language\",\n         \"triton/language/extra\",\n         \"triton/ops\",\n         \"triton/ops/blocksparse\",\n         \"triton/runtime\",\n         \"triton/runtime/backends\",\n-        \"triton/third_party/cuda/bin\",\n-        \"triton/third_party/cuda/include\",\n-        \"triton/third_party/cuda/lib\",\n+        \"triton/third_party\",\n         \"triton/tools\",\n     ],\n     install_requires=["}, {"filename": "python/test/backend/extension_backend.c", "status": "added", "additions": 42, "deletions": 0, "changes": 42, "file_content_changes": "@@ -0,0 +1,42 @@\n+#include <Python.h>\n+#include <stdio.h>\n+#include <stdlib.h>\n+\n+static PyObject *getDeviceProperties(PyObject *self, PyObject *args) {\n+  // create a struct to hold device properties\n+  return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\", 1024,\n+                       \"multiprocessor_count\", 16, \"sm_clock_rate\", 2100,\n+                       \"mem_clock_rate\", 2300, \"mem_bus_width\", 2400);\n+}\n+\n+static PyObject *loadBinary(PyObject *self, PyObject *args) {\n+  // get allocated registers and spilled registers from the function\n+  int n_regs = 0;\n+  int n_spills = 0;\n+  int mod = 0;\n+  int fun = 0;\n+  return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs,\n+                       n_spills);\n+}\n+\n+static PyMethodDef ModuleMethods[] = {\n+    {\"load_binary\", loadBinary, METH_VARARGS,\n+     \"Load dummy binary for the extension device\"},\n+    {\"get_device_properties\", getDeviceProperties, METH_VARARGS,\n+     \"Get the properties for the extension device\"},\n+    {NULL, NULL, 0, NULL} // sentinel\n+};\n+\n+static struct PyModuleDef ModuleDef = {PyModuleDef_HEAD_INIT, \"ext_utils\",\n+                                       NULL, // documentation\n+                                       -1,   // size\n+                                       ModuleMethods};\n+\n+PyMODINIT_FUNC PyInit_ext_utils(void) {\n+  PyObject *m = PyModule_Create(&ModuleDef);\n+  if (m == NULL) {\n+    return NULL;\n+  }\n+  PyModule_AddFunctions(m, ModuleMethods);\n+  return m;\n+}"}, {"filename": "python/test/backend/test_device_backend.py", "status": "added", "additions": 262, "deletions": 0, "changes": 262, "file_content_changes": "@@ -0,0 +1,262 @@\n+import functools\n+import hashlib\n+import importlib\n+import os\n+import shutil\n+import subprocess\n+import sysconfig\n+import tempfile\n+from pathlib import Path\n+\n+import setuptools\n+import torch\n+\n+import triton\n+import triton.language as tl\n+from triton.common.backend import BaseBackend, register_backend\n+from triton.common.build import quiet\n+from triton.compiler.make_launcher import make_so_cache_key\n+from triton.runtime.cache import get_cache_manager\n+from triton.runtime.driver import DriverBase\n+from triton.runtime.jit import version_key\n+\n+\n+def build_for_backend(name, src, srcdir):\n+    suffix = sysconfig.get_config_var('EXT_SUFFIX')\n+    so = os.path.join(srcdir, '{name}{suffix}'.format(name=name, suffix=suffix))\n+    # try to avoid setuptools if possible\n+    cc = os.environ.get(\"CC\")\n+    if cc is None:\n+        # TODO: support more things here.\n+        clang = shutil.which(\"clang\")\n+        gcc = shutil.which(\"gcc\")\n+        cc = gcc if gcc is not None else clang\n+        if cc is None:\n+            raise RuntimeError(\"Failed to find C compiler. Please specify via CC environment variable.\")\n+    # This function was renamed and made public in Python 3.10\n+    if hasattr(sysconfig, 'get_default_scheme'):\n+        scheme = sysconfig.get_default_scheme()\n+    else:\n+        scheme = sysconfig._get_default_scheme()\n+    # 'posix_local' is a custom scheme on Debian. However, starting Python 3.10, the default install\n+    # path changes to include 'local'. This change is required to use triton with system-wide python.\n+    if scheme == 'posix_local':\n+        scheme = 'posix_prefix'\n+    py_include_dir = sysconfig.get_paths(scheme=scheme)[\"include\"]\n+\n+    ret = subprocess.check_call([cc, src, f\"-I{py_include_dir}\", f\"-I{srcdir}\", \"-shared\", \"-fPIC\", \"-o\", so])\n+    if ret == 0:\n+        return so\n+    # fallback on setuptools\n+    extra_compile_args = []\n+    library_dirs = []\n+    include_dirs = [srcdir]\n+    libraries = []\n+    # extra arguments\n+    extra_link_args = []\n+    # create extension module\n+    ext = setuptools.Extension(\n+        name=name,\n+        language='c',\n+        sources=[src],\n+        include_dirs=include_dirs,\n+        extra_compile_args=extra_compile_args + ['-O3'],\n+        extra_link_args=extra_link_args,\n+        library_dirs=library_dirs,\n+        libraries=libraries,\n+    )\n+    # build extension module\n+    args = ['build_ext']\n+    args.append('--build-temp=' + srcdir)\n+    args.append('--build-lib=' + srcdir)\n+    args.append('-q')\n+    args = dict(\n+        name=name,\n+        ext_modules=[ext],\n+        script_args=args,\n+    )\n+    with quiet():\n+        setuptools.setup(**args)\n+    return so\n+\n+\n+class ExtensionUtils:\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(ExtensionUtils, cls).__new__(cls)\n+        return cls.instance\n+\n+    def __init__(self):\n+        dirname = os.path.dirname(os.path.realpath(__file__))\n+        src = Path(os.path.join(dirname, \"extension_backend.c\")).read_text()\n+        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n+        cache = get_cache_manager(key)\n+        fname = \"ext_utils.so\"\n+        cache_path = cache.get_file(fname)\n+        if cache_path is None:\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                src_path = os.path.join(tmpdir, \"main.c\")\n+                with open(src_path, \"w\") as f:\n+                    f.write(src)\n+                so = build_for_backend(\"ext_utils\", src_path, tmpdir)\n+                with open(so, \"rb\") as f:\n+                    cache_path = cache.put(f.read(), fname, binary=True)\n+        import importlib.util\n+        spec = importlib.util.spec_from_file_location(\"ext_utils\", cache_path)\n+        mod = importlib.util.module_from_spec(spec)\n+        spec.loader.exec_module(mod)\n+        self.load_binary = mod.load_binary\n+        self.get_device_properties = mod.get_device_properties\n+\n+\n+class ExtensionDriver(DriverBase):\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(ExtensionDriver, cls).__new__(cls)\n+        return cls.instance\n+\n+    def __init__(self):\n+        self.utils = ExtensionUtils()\n+\n+\n+class ExtensionBackend(BaseBackend):\n+    stub_so_path = \"\"\n+\n+    def __init__(self, device_type: str) -> None:\n+        super(ExtensionBackend, self).__init__(device_type)\n+        self.driver = ExtensionDriver()\n+\n+    def add_stages(self, arch, extern_libs, stages):\n+        filter_in_stages = [\"ast\", \"ttir\", \"ttgir\"]\n+        filter_out_stages = []\n+        for key, _ in stages.items():\n+            if key not in filter_in_stages:\n+                filter_out_stages.append(key)\n+        for filter_out_key in filter_out_stages:\n+            stages.pop(filter_out_key)\n+\n+    def add_meta_info(self, ir, cur_module, next_module, metadata, asm):\n+        metadata[\"name\"] = \"extension_backend_name\"\n+\n+    def get_driver(self):\n+        return self.driver\n+\n+    def get_stream(self):\n+        return \"\"\n+\n+    @functools.lru_cache(None)\n+    def get_device_properties(self, device):\n+        return self.driver.utils.get_device_properties()\n+\n+    def get_current_device(self):\n+        return torch.device(\"cpu\")\n+\n+    def set_current_device(self, device):\n+        pass\n+\n+    def get_load_binary_fn(self):\n+        return self.driver.utils.load_binary\n+\n+    def get_kernel_bin(self):\n+        return \"ttgir\"\n+\n+    def get_architecture_descriptor(self, **kwargs):\n+        return \"\"\n+\n+    def make_launcher_stub(self, name, signature, constants):\n+        # name of files that are cached\n+        so_cache_key = make_so_cache_key(version_key(), signature, constants)\n+        so_cache_manager = get_cache_manager(so_cache_key)\n+        so_name = f\"{name}.so\"\n+        # retrieve stub from cache if it exists\n+        cache_path = so_cache_manager.get_file(so_name)\n+        if cache_path is None:\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                src = self._generate_launcher(constants, signature)\n+                src_path = os.path.join(tmpdir, \"main.c\")\n+                with open(src_path, \"w\") as f:\n+                    f.write(src)\n+                so = build_for_backend(name, src_path, tmpdir)\n+                with open(so, \"rb\") as f:\n+                    so_path = so_cache_manager.put(f.read(), so_name, binary=True)\n+                    type(self).stub_so_path = so_path\n+                    return so_path\n+        else:\n+            type(self).stub_so_path = cache_path\n+            return cache_path\n+\n+    def _generate_launcher(self, constants, signature):\n+        # generate glue code\n+        src = \"\"\"\n+        #define __EXTENSION_BACKEND__\n+        #include <Python.h>\n+        #include <stdio.h>\n+\n+        static PyObject* launch_counter(PyObject* self, PyObject* args) {\n+        static int64_t launch_counter = 0;\n+        launch_counter += 1;\n+        return PyLong_FromLong(launch_counter);\n+        }\n+\n+        static PyObject* launch(PyObject* self, PyObject* args) {\n+        if (PyErr_Occurred()) {\n+            return NULL;\n+        }\n+        launch_counter(self, args);\n+        // return None\n+        Py_INCREF(Py_None);\n+        return Py_None;\n+        }\n+\n+        static PyMethodDef ModuleMethods[] = {\n+        {\"launch\", launch, METH_VARARGS, \"Entry point for all kernels with this signature\"},\n+        {\"launch_counter\", launch_counter, METH_VARARGS, \"Entry point to get launch counter\"},\n+        {NULL, NULL, 0, NULL} // sentinel\n+        };\n+\n+        static struct PyModuleDef ModuleDef = {\n+        PyModuleDef_HEAD_INIT,\n+        \\\"__triton_launcher\\\",\n+        NULL, //documentation\n+        -1, //size\n+        ModuleMethods\n+        };\n+\n+        PyMODINIT_FUNC PyInit___triton_launcher(void) {\n+        PyObject *m = PyModule_Create(&ModuleDef);\n+        if(m == NULL) {\n+            return NULL;\n+        }\n+        PyModule_AddFunctions(m, ModuleMethods);\n+        return m;\n+        }\n+        \"\"\"\n+\n+        return src\n+\n+\n+def test_dummy_backend():\n+    register_backend(\"cpu\", ExtensionBackend)\n+\n+    @triton.jit\n+    def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n+        xnumel = 10\n+        xoffset = tl.program_id(0) * XBLOCK\n+        xindex = xoffset + tl.arange(0, XBLOCK)[:]\n+        xmask = xindex < xnumel\n+        x0 = xindex\n+        tmp0 = tl.load(in_ptr0 + (x0), xmask)\n+        tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp0, xmask)\n+\n+    inp = torch.randn(10)\n+    out = torch.randn(10)\n+    kernel[(10,)](inp, out, 10, XBLOCK=16)\n+    spec = importlib.util.spec_from_file_location(\"__triton_launcher\", ExtensionBackend.stub_so_path)\n+    mod = importlib.util.module_from_spec(spec)\n+    spec.loader.exec_module(mod)\n+    launch_counter = getattr(mod, \"launch_counter\")\n+\n+    for _ in range(100):\n+        kernel[(10,)](inp, out, 10, XBLOCK=16)\n+\n+    assert launch_counter() > 0"}, {"filename": "python/test/backend/third_party_backends/conftest.py", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+# content of conftest.py\n+\n+import pytest\n+\n+\n+def pytest_addoption(parser):\n+    parser.addoption(\n+        \"--backend\", action=\"store\", default=\"\", help=\"Codegen backend\"\n+    )\n+\n+\n+@pytest.fixture\n+def cmdopt(request):\n+    return request.config.getoption(\"--backend\")"}, {"filename": "python/test/backend/third_party_backends/test_xpu_backend.py", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "@@ -0,0 +1,33 @@\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+def test_xpu_backend(cmdopt):\n+    if cmdopt == \"xpu\":\n+        has_ipex = False\n+        try:\n+            # Import IPEX to provide Intel GPU runtime\n+            import intel_extension_for_pytorch  # type: ignore # noqa: F401\n+            has_ipex = True if hasattr(torch, \"xpu\") else False\n+        except Exception:\n+            has_ipex = False\n+\n+        @triton.jit()\n+        def kernel(x_ptr, y_ptr, out_ptr):\n+            pid = tl.program_id(axis=0)\n+            x = tl.load(x_ptr + pid)\n+            y = tl.load(y_ptr + pid)\n+            out = x + y\n+            tl.store(out_ptr + pid, out)\n+\n+        if has_ipex:\n+            for _ in range(1000):\n+                x = torch.randn((65536,), device=\"xpu\", dtype=torch.float32)\n+                y = torch.randn((65536,), device=\"xpu\", dtype=torch.float32)\n+                z = torch.zeros((65536,), device=\"xpu\", dtype=torch.float32)\n+                kernel[(65536,)](x, y, z, num_warps=32)\n+                assert torch.all(x + y == z)\n+    else:\n+        return"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -69,7 +69,7 @@ def nvsmi(attrs):\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n         (16, 4096, 4096): {'float16': 0.044, 'float32': 0.0457, 'int8': 0.0259},\n         (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n-        (64, 1024, 1024): {'float16': 0.030, 'float32': 0.0509, 'int8': 0.0169},\n+        (64, 1024, 1024): {'float16': 0.028, 'float32': 0.0509, 'int8': 0.0169},\n         (64, 4096, 4096): {'float16': 0.163, 'float32': 0.162, 'int8': 0.097},\n         (64, 8192, 8192): {'float16': 0.285, 'float32': 0.257, 'int8': 0.174},\n         (1024, 64, 1024): {'float16': 0.033, 'float32': 0.0458, 'int8': 0.017},"}, {"filename": "python/test/unit/interpreter/test_interpreter.py", "status": "renamed", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -4,7 +4,7 @@\n \n import triton\n import triton.language as tl\n-from triton.debugger.debugger import program_ids_from_grid\n+from triton.interpreter.interpreter import program_ids_from_grid\n \n \n def test_addition():"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 83, "deletions": 49, "changes": 132, "file_content_changes": "@@ -4,76 +4,104 @@\n import torch\n \n import triton\n+import triton.language as tl\n import triton.ops\n \n \n+def f8_to_f16(x):\n+\n+    @triton.jit\n+    def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n+        pid = tl.program_id(0)\n+        offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offs < N\n+        x = tl.load(X + offs, mask=mask)\n+        y = x.to(tl.float8e5)\n+        tl.store(Y + offs, y, mask=mask)\n+\n+    ret = torch.empty(x.shape, dtype=torch.float16, device=x.device)\n+    grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n+    kernel[grid](ret, triton.reinterpret(x, tl.float8e5), ret.numel(), BLOCK_SIZE=1024)\n+    return ret\n+\n+\n @pytest.mark.parametrize(\n-    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE\",\n+    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE\",\n     itertools.chain(\n         *[\n             [\n                 # 1 warp\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n-                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE),\n+                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # 2 warp\n-                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE),\n+                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # 4 warp\n-                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # 8 warp\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n-                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n-                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE),\n+                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # split-k\n-                (64, 64, 16, 2, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE),\n-                (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE),\n+                (64, 64, 16, 2, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # variable input\n-                (128, 128, 32, 1, 4, 2, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE),\n-                (128, 256, 64, 1, 8, 3, 1024, 1024, 1024, AT, BT, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n+                (128, 256, 64, 1, 8, 3, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n         *[\n             [\n-                (16, 16, 16, 1, 1, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (64, 32, 64, 1, 2, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (128, 64, 16, 1, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (256, 128, 32, 1, 8, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (128, 128, 32, 1, 4, STAGES, 384, 128, 640, AT, BT, DTYPE),\n+                (16, 16, 16, 1, 1, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (64, 32, 64, 1, 2, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (128, 64, 16, 1, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (256, 128, 32, 1, 8, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, STAGES, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n                 # split-k\n-                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n-                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 32, AT, BT, DTYPE),\n+                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 32, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [2, 3, 4]\n+        ],\n+        # mixed-precision\n+        *[\n+            [\n+                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (32, 64, 16, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n+                (128, 128, 32, 8, 4, 2, 1024, 1024, 1024, AT, BT, ADTYPE, BDTYPE),\n+            ] for ADTYPE, BDTYPE in [(\"float8\", \"float16\")] for AT in [False, True] for BT in [False, True]\n         ]\n     ),\n )\n-def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE):\n+def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n-    if capability[0] < 8 and DTYPE == \"bfloat16\":\n+    if capability[0] < 8 and (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\"):\n         pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n-    if DTYPE == \"bfloat16\" and SPLIT_K != 1:\n+    if (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\") and SPLIT_K != 1:\n         pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n     torch.manual_seed(0)\n     # nuke kernel decorators -- will set meta-parameters manually\n@@ -88,18 +116,24 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n+\n+    def get_input(n, m, t, dtype):\n+        if t:\n+            return get_input(m, n, False, dtype).t()\n+        if dtype == \"float8\":\n+            x = torch.randint(10, 50, (n, m), device=\"cuda\", dtype=torch.int8)\n+            return f8_to_f16(x)\n+        dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n+        return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n     # allocate/transpose inputs\n-    DTYPE = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[DTYPE]\n-    a = .1 * torch.randn((K, M) if AT else (M, K), device=\"cuda\", dtype=DTYPE)\n-    b = .1 * torch.randn((N, K) if BT else (K, N), device=\"cuda\", dtype=DTYPE)\n-    a = a.t() if AT else a\n-    b = b.t() if BT else b\n+    a = get_input(M, K, AT, ADTYPE)\n+    b = get_input(K, N, BT, BDTYPE)\n     # run test\n     th_c = torch.matmul(a, b)\n     try:\n         tt_c = triton.ops.matmul(a, b)\n         atol, rtol = 1e-2, 0\n-        if DTYPE == torch.bfloat16:\n+        if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:\n             atol, rtol = 3.5e-2, 0\n         torch.testing.assert_allclose(th_c, tt_c, atol=atol, rtol=rtol)\n     except triton.OutOfResources as e:"}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -18,7 +18,6 @@\n )\n from .runtime.jit import jit\n from .compiler import compile, CompilationError\n-from .debugger.debugger import program_ids_from_grid\n \n from . import language\n from . import testing\n@@ -43,7 +42,6 @@\n     \"runtime\",\n     \"TensorWrapper\",\n     \"testing\",\n-    \"program_ids_from_grid\",\n ]\n \n "}, {"filename": "python/triton/common/backend.py", "status": "added", "additions": 96, "deletions": 0, "changes": 96, "file_content_changes": "@@ -0,0 +1,96 @@\n+\n+import importlib\n+import importlib.util\n+from typing import Dict\n+\n+from ..runtime.driver import DriverBase\n+\n+\n+class BaseBackend:\n+    def __init__(self, device_type: str) -> None:\n+        self.device_type = device_type\n+\n+    def add_stages(self, arch, extern_libs, stages):\n+        \"\"\"\n+        Custom the arch, extern_libs and stages per backend specific requirement\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def add_meta_info(self, ir, cur_module, next_module, metadata, asm):\n+        \"\"\"\n+        Custom the ir, module, metadata and asm per backend specific requirement\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_load_binary_fn(self):\n+        \"\"\"\n+        Return a callable to load binary\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_driver(self) -> DriverBase:\n+        \"\"\"\n+        Get the backend driver. Please refer to \"DriverBase\" for more details\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_stream(self):\n+        \"\"\"\n+        Get stream for current device\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_device_properties(self, device):\n+        raise NotImplementedError\n+\n+    def get_current_device(self):\n+        \"\"\"\n+        Get current device\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def set_current_device(self, device):\n+        \"\"\"\n+        Set current device as the given device\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_kernel_bin(self):\n+        raise NotImplementedError\n+\n+    def make_launcher_stub(self, name, signature, constants):\n+        \"\"\"\n+        Generate the launcher stub to launch the kernel\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_architecture_descriptor(self, **kwargs):\n+        \"\"\"\n+        Get the architecture descriptor the backend\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    @classmethod\n+    def create_backend(cls, device_type: str):\n+        return cls(device_type)\n+\n+\n+_backends: Dict[str, BaseBackend] = {}\n+\n+\n+def register_backend(device_type: str, backend_cls: type):\n+    if device_type not in _backends:\n+        _backends[device_type] = backend_cls.create_backend(device_type)\n+\n+\n+def get_backend(device_type: str):\n+    if device_type not in _backends:\n+        device_backend_package_name = f\"triton.third_party.{device_type}\"\n+        if importlib.util.find_spec(device_backend_package_name):\n+            try:\n+                importlib.import_module(device_backend_package_name)\n+            except Exception:\n+                return None\n+        else:\n+            return None\n+    return _backends[device_type] if device_type in _backends else None"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -6,12 +6,12 @@\n from typing import Any, Callable, Dict, Optional, Tuple, Type, Union\n \n from .. import language\n+from .._C.libtriton.triton import ir\n from ..language import constexpr, tensor\n # ideally we wouldn't need any runtime component\n from ..runtime import JITFunction\n from .errors import (CompilationError, CompileTimeAssertionFailure,\n                      UnsupportedLanguageConstruct)\n-from triton._C.libtriton.triton import ir\n \n \n def mangle_ty(ty):"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 102, "deletions": 55, "changes": 157, "file_content_changes": "@@ -11,19 +11,26 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-import triton\n-import triton._C.libtriton.triton as _triton\n-from ..runtime import driver\n+# import triton\n+from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n+                                   get_shared_memory_size, ir,\n+                                   translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n+                                   translate_triton_gpu_to_llvmir)\n+from ..common.backend import get_backend\n+# from ..runtime import driver, jit, JITFunction\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n from ..runtime.cache import get_cache_manager\n+from ..runtime.driver import driver\n+from ..runtime.jit import (JITFunction, get_cuda_stream, get_current_device,\n+                           get_device_capability, version_key)\n from ..tools.disasm import extract\n from .code_generator import ast_to_ttir\n from .make_launcher import make_stub\n \n \n def inline_triton_ir(mod):\n-    pm = _triton.ir.pass_manager(mod.context)\n+    pm = ir.pass_manager(mod.context)\n     pm.enable_debug()\n     pm.add_inliner_pass()\n     pm.run(mod)\n@@ -33,7 +40,7 @@ def inline_triton_ir(mod):\n def ttir_compute_capability_rewrite(mod, arch):\n     # For hardware without support, we must rewrite all load/store\n     # with block (tensor) pointers into tensors of pointers\n-    pm = _triton.ir.pass_manager(mod.context)\n+    pm = ir.pass_manager(mod.context)\n     pm.enable_debug()\n     if _is_cuda(arch):\n         pm.add_rewrite_tensor_pointer_pass(arch)\n@@ -44,7 +51,7 @@ def ttir_compute_capability_rewrite(mod, arch):\n def optimize_ttir(mod, arch):\n     mod = inline_triton_ir(mod)\n     mod = ttir_compute_capability_rewrite(mod, arch)\n-    pm = _triton.ir.pass_manager(mod.context)\n+    pm = ir.pass_manager(mod.context)\n     pm.enable_debug()\n     pm.add_inliner_pass()\n     pm.add_triton_combine_pass()\n@@ -57,14 +64,14 @@ def optimize_ttir(mod, arch):\n \n \n def ttir_to_ttgir(mod, num_warps):\n-    pm = _triton.ir.pass_manager(mod.context)\n+    pm = ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n     pm.run(mod)\n     return mod\n \n \n def optimize_ttgir(mod, num_stages, arch):\n-    pm = _triton.ir.pass_manager(mod.context)\n+    pm = ir.pass_manager(mod.context)\n     pm.enable_debug()\n     pm.add_tritongpu_coalesce_pass()\n     pm.add_tritongpu_remove_layout_conversions_pass()\n@@ -88,17 +95,17 @@ def _add_external_libs(mod, libs):\n     for name, path in libs.items():\n         if len(name) == 0 or len(path) == 0:\n             return\n-    _triton.add_external_libs(mod, list(libs.keys()), list(libs.values()))\n+    add_external_libs(mod, list(libs.keys()), list(libs.values()))\n \n \n def ttgir_to_llir(mod, extern_libs, arch):\n     if extern_libs:\n         _add_external_libs(mod, extern_libs)\n     # TODO: separate tritongpu_to_llvmir for different backends\n     if _is_cuda(arch):\n-        return _triton.translate_triton_gpu_to_llvmir(mod, arch, False)\n+        return translate_triton_gpu_to_llvmir(mod, arch, False)\n     else:\n-        return _triton.translate_triton_gpu_to_llvmir(mod, 0, True)\n+        return translate_triton_gpu_to_llvmir(mod, 0, True)\n \n \n # PTX translation\n@@ -128,8 +135,9 @@ def path_to_ptxas():\n     ]\n \n     for ptxas in paths:\n-        if os.path.exists(ptxas) and os.path.isfile(ptxas):\n-            result = subprocess.check_output([ptxas, \"--version\"], stderr=subprocess.STDOUT)\n+        ptxas_bin = ptxas.split(\" \")[0]\n+        if os.path.exists(ptxas_bin) and os.path.isfile(ptxas_bin):\n+            result = subprocess.check_output([ptxas_bin, \"--version\"], stderr=subprocess.STDOUT)\n             if result is not None:\n                 version = re.search(r\".*release (\\d+\\.\\d+).*\", result.decode(\"utf-8\"), flags=re.MULTILINE)\n                 if version is not None:\n@@ -146,7 +154,7 @@ def llir_to_ptx(mod: Any, arch: int, ptx_version: int = None) -> str:\n     if ptx_version is None:\n         _, cuda_version = path_to_ptxas()\n         ptx_version = ptx_get_version(cuda_version)\n-    return _triton.translate_llvmir_to_ptx(mod, arch, ptx_version)\n+    return translate_llvmir_to_ptx(mod, arch, ptx_version)\n \n \n def ptx_to_cubin(ptx: str, arch: int):\n@@ -157,7 +165,7 @@ def ptx_to_cubin(ptx: str, arch: int):\n     :return: str\n     '''\n     ptxas, _ = path_to_ptxas()\n-    return _triton.compile_ptx_to_cubin(ptx, ptxas, arch)\n+    return compile_ptx_to_cubin(ptx, ptxas, arch)\n \n \n # AMDGCN translation\n@@ -223,7 +231,7 @@ def llir_to_amdgcn_and_hsaco(mod: Any, gfx_arch: str, gfx_triple: str, gfx_featu\n         - AMDGCN code\n         - Path to HSACO object\n     '''\n-    return _triton.translate_llvmir_to_hsaco(mod, gfx_arch, gfx_triple, gfx_features)\n+    return translate_llvmir_to_hsaco(mod, gfx_arch, gfx_triple, gfx_features)\n \n \n # ------------------------------------------------------------------------------\n@@ -250,7 +258,7 @@ def convert_type_repr(x):\n \n \n def make_hash(fn, arch, **kwargs):\n-    if isinstance(fn, triton.runtime.JITFunction):\n+    if isinstance(fn, JITFunction):\n         configs = kwargs[\"configs\"]\n         signature = kwargs[\"signature\"]\n         constants = kwargs.get(\"constants\", dict())\n@@ -263,7 +271,7 @@ def make_hash(fn, arch, **kwargs):\n         key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}-{debug}-{arch}\"\n         return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n     assert isinstance(fn, str)\n-    return hashlib.md5((Path(fn).read_text() + triton.runtime.jit.version_key()).encode(\"utf-8\")).hexdigest()\n+    return hashlib.md5((Path(fn).read_text() + version_key()).encode(\"utf-8\")).hexdigest()\n \n \n # - ^\\s*tt\\.func\\s+ : match the start of the string, any leading whitespace, the keyword func,\n@@ -307,7 +315,7 @@ def _is_jsonable(x):\n \n \n def parse_mlir_module(path, context):\n-    module = _triton.ir.parse_mlir_module(path, context)\n+    module = ir.parse_mlir_module(path, context)\n     # module takes ownership of the context\n     module.context = context\n     return module\n@@ -328,8 +336,8 @@ def get_architecture_descriptor(capability):\n         raise ImportError(\"Triton requires PyTorch to be installed\")\n     if capability is None:\n         if torch.version.hip is None:\n-            device = triton.runtime.jit.get_current_device()\n-            capability = triton.runtime.jit.get_device_capability(device)\n+            device = get_current_device()\n+            capability = get_device_capability(device)\n             capability = capability[0] * 10 + capability[1]\n         else:\n             capability = get_amdgpu_arch_fulldetails()\n@@ -362,9 +370,20 @@ def add_cuda_stages(arch, extern_libs, stages):\n \n \n def compile(fn, **kwargs):\n-    arch = get_architecture_descriptor(kwargs.get(\"cc\", None))\n-    is_cuda = _is_cuda(arch)\n-    context = _triton.ir.context()\n+    # Get device type to decide which backend should be used\n+    device_type = kwargs.get(\"device_type\", \"cuda\")\n+    _device_backend = get_backend(device_type)\n+\n+    if device_type in [\"cuda\", \"hip\"]:\n+        arch = get_architecture_descriptor(kwargs.get(\"cc\", None))\n+    else:\n+        _device_backend = get_backend(device_type)\n+        assert _device_backend\n+        arch = _device_backend.get_architecture_descriptor(**kwargs)\n+\n+    is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n+    is_hip = device_type in [\"cuda\", \"hip\"] and not is_cuda\n+    context = ir.context()\n     asm = dict()\n     constants = kwargs.get(\"constants\", dict())\n     num_warps = kwargs.get(\"num_warps\", 4)\n@@ -373,6 +392,7 @@ def compile(fn, **kwargs):\n     if extern_libs is None:\n         extern_libs = dict()\n     debug = kwargs.get(\"debug\", False)\n+\n     # build compilation stages\n     stages = dict()\n     stages[\"ast\"] = (lambda path: fn, None)\n@@ -384,11 +404,13 @@ def compile(fn, **kwargs):\n                       lambda src: ttgir_to_llir(src, extern_libs, arch))\n     if is_cuda:\n         add_cuda_stages(arch, extern_libs, stages)\n-    else:\n+    elif is_hip:\n         add_rocm_stages(arch, extern_libs, stages)\n+    else:\n+        _device_backend.add_stages(arch, extern_libs, stages)\n \n     # find out the signature of the function\n-    if isinstance(fn, triton.runtime.JITFunction):\n+    if isinstance(fn, JITFunction):\n         configs = kwargs.get(\"configs\", None)\n         signature = kwargs[\"signature\"]\n         if configs is None:\n@@ -402,27 +424,31 @@ def compile(fn, **kwargs):\n         kwargs[\"signature\"] = signature\n     else:\n         assert isinstance(fn, str)\n-        _, ir = os.path.basename(fn).split(\".\")\n+        _, ir_name = os.path.basename(fn).split(\".\")\n         src = Path(fn).read_text()\n         import re\n-        match = re.search(prototype_pattern[ir], src, re.MULTILINE)\n+        match = re.search(prototype_pattern[ir_name], src, re.MULTILINE)\n         name, signature = match.group(1), match.group(2)\n-        types = re.findall(arg_type_pattern[ir], signature)\n-        if ir == 'ttgir':\n+        types = re.findall(arg_type_pattern[ir_name], signature)\n+        if ir_name == 'ttgir':\n             num_warps_matches = re.findall(ttgir_num_warps_pattern, src)\n             assert len(num_warps_matches) == 1, \"Expected exactly one match for num_warps\"\n             assert \"num_warps\" not in kwargs or int(num_warps_matches[0]) == num_warps, \"num_warps in ttgir does not match num_warps in compile\"\n             num_warps = int(num_warps_matches[0])\n         param_tys = [convert_type_repr(ty) for ty in types]\n         signature = {k: v for k, v in enumerate(param_tys)}\n-        first_stage = list(stages.keys()).index(ir)\n+        first_stage = list(stages.keys()).index(ir_name)\n \n     # cache manager\n-    so_path = make_stub(name, signature, constants)\n+    if is_cuda or is_hip:\n+        so_path = make_stub(name, signature, constants)\n+    else:\n+        so_path = _device_backend.make_launcher_stub(name, signature, constants)\n+\n     # create cache manager\n     fn_cache_manager = get_cache_manager(make_hash(fn, arch, **kwargs))\n     # determine name and extension type of provided function\n-    if isinstance(fn, triton.runtime.JITFunction):\n+    if isinstance(fn, JITFunction):\n         name, ext = fn.__name__, \"ast\"\n     else:\n         name, ext = os.path.basename(fn).split(\".\")\n@@ -451,14 +477,17 @@ def compile(fn, **kwargs):\n             assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n             metadata[\"shared\"] = kwargs[\"shared\"]\n \n+    # Add device type to meta information\n+    metadata[\"device_type\"] = device_type\n+\n     first_stage = list(stages.keys()).index(ext)\n     asm = dict()\n     module = fn\n     # run compilation pipeline  and populate metadata\n-    for ir, (parse, compile_kernel) in list(stages.items())[first_stage:]:\n-        ir_filename = f\"{name}.{ir}\"\n+    for ir_name, (parse, compile_kernel) in list(stages.items())[first_stage:]:\n+        ir_filename = f\"{name}.{ir_name}\"\n \n-        if ir == ext:\n+        if ir_name == ext:\n             next_module = parse(fn)\n         else:\n             path = metadata_group.get(ir_filename)\n@@ -472,27 +501,29 @@ def compile(fn, **kwargs):\n                     metadata_group[ir_filename] = fn_cache_manager.put(next_module, ir_filename)\n                     fn_cache_manager.put(next_module, ir_filename)\n             else:\n-                if ir == \"amdgcn\":\n+                if ir_name == \"amdgcn\":\n                     extra_file_name = f\"{name}.hsaco_path\"\n                     hasco_path = metadata_group.get(extra_file_name)\n                     assert hasco_path is not None, \"Expected to have hsaco_path in metadata when we have the amdgcn\"\n                     next_module = (parse(path), parse(hasco_path))\n                 else:\n                     next_module = parse(path)\n \n-        if ir == \"cubin\":\n-            asm[ir] = next_module\n-        elif ir == \"amdgcn\":\n-            asm[ir] = str(next_module[0])\n+        if ir_name == \"cubin\":\n+            asm[ir_name] = next_module\n+        elif ir_name == \"amdgcn\":\n+            asm[ir_name] = str(next_module[0])\n         else:\n-            asm[ir] = str(next_module)\n-        if ir == \"llir\" and \"shared\" not in metadata:\n-            metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n-        if ir == \"ptx\":\n+            asm[ir_name] = str(next_module)\n+        if ir_name == \"llir\" and \"shared\" not in metadata:\n+            metadata[\"shared\"] = get_shared_memory_size(module)\n+        if ir_name == \"ptx\":\n             metadata[\"name\"] = get_kernel_name(next_module, pattern='// .globl')\n-        if ir == \"amdgcn\":\n+        if ir_name == \"amdgcn\":\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n+        if not is_cuda and not is_hip:\n+            _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n     # write-back metadata, if it didn't come from the cache\n     if metadata_path is None:\n@@ -518,10 +549,12 @@ def __init__(self, fn, so_path, metadata, asm):\n         spec.loader.exec_module(mod)\n         self.c_wrapper = getattr(mod, \"launch\")\n         # initialize metadata\n-        self.shared = metadata[\"shared\"]\n+        self.shared = metadata[\"shared\"] if \"shared\" in metadata else 0\n         self.num_warps = metadata[\"num_warps\"]\n         self.num_stages = metadata[\"num_stages\"]\n         self.constants = metadata[\"constants\"]\n+        self.device_type = metadata[\"device_type\"]\n+        self.device_backend = get_backend(self.device_type) if self.device_type not in [\"cuda\", \"hip\"] else None\n         # initialize asm dict\n         self.asm = asm\n         # binaries are lazily initialized\n@@ -534,15 +567,26 @@ def __init__(self, fn, so_path, metadata, asm):\n     def _init_handles(self):\n         if self.cu_module is not None:\n             return\n-        device = triton.runtime.jit.get_current_device()\n-        bin_path = {\n-            driver.HIP: \"hsaco_path\",\n-            driver.CUDA: \"cubin\"\n-        }[driver.backend]\n-        max_shared = driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n+\n+        if self.device_type in [\"cuda\", \"hip\"]:\n+            device = get_current_device()\n+            bin_path = {\n+                driver.HIP: \"hsaco_path\",\n+                driver.CUDA: \"cubin\"\n+            }[driver.backend]\n+            max_shared = driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n+            fn_load_binary = driver.utils.load_binary\n+        else:\n+            assert self.device_backend\n+            device = self.device_backend.get_current_device()\n+            bin_path = self.device_backend.get_kernel_bin()\n+            max_shared = self.device_backend.get_device_properties(device)[\"max_shared_mem\"]\n+            fn_load_binary = self.device_backend.get_load_binary_fn()\n+\n         if self.shared > max_shared:\n             raise OutOfResources(self.shared, max_shared, \"shared memory\")\n-        mod, func, n_regs, n_spills = driver.utils.load_binary(self.metadata[\"name\"], self.asm[bin_path], self.shared, device)\n+\n+        mod, func, n_regs, n_spills = fn_load_binary(self.metadata[\"name\"], self.asm[bin_path], self.shared, device)\n \n         self.n_spills = n_spills\n         self.n_regs = n_regs\n@@ -559,7 +603,10 @@ def __getitem__(self, grid):\n \n         def runner(*args, stream=None):\n             if stream is None:\n-                stream = triton.runtime.jit.get_cuda_stream()\n+                if self.device_type in [\"cuda\", \"rocm\"]:\n+                    stream = get_cuda_stream()\n+                else:\n+                    stream = get_backend(self.device_type).get_stream(None)\n             self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function,\n                            CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, self, *args)\n         return runner"}, {"filename": "python/triton/interpreter/__init__.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/debugger/__init__.py"}, {"filename": "python/triton/interpreter/core.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/debugger/core.py"}, {"filename": "python/triton/interpreter/interpreter.py", "status": "renamed", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -2,13 +2,14 @@\n import random\n from typing import Tuple\n \n-import triton\n-import triton.language as tl\n+from .. import language as tl\n+# import .language.core as lcore\n+from ..language import core as lcore\n+from . import torch_wrapper\n from .core import ExecutionContext\n from .memory_map import MemoryMap\n from .tl_lang import (TritonLangProxy, WrappedTensor, _primitive_to_tensor,\n                       debugger_constexpr)\n-from triton.debugger import torch_wrapper\n \n torch = torch_wrapper.torch\n tl_method_backup = {}\n@@ -59,12 +60,12 @@ def __init__(self, func, grid=(1,)):\n         self.grid = grid\n \n     def _is_constexpr(self, name):\n-        return name in self.func.__annotations__ and self.func.__annotations__[name] is triton.language.core.constexpr\n+        return name in self.func.__annotations__ and self.func.__annotations__[name] is lcore.constexpr\n \n     def _get_constexpr(self):\n         result = []\n         for name, annotation in self.func.__annotations__.items():\n-            if annotation is triton.language.core.constexpr:\n+            if annotation is lcore.constexpr:\n                 result.append(name)\n         return result\n "}, {"filename": "python/triton/interpreter/memory_map.py", "status": "renamed", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2,7 +2,7 @@\n \n import dataclasses\n \n-from triton.debugger import torch_wrapper\n+from triton.interpreter import torch_wrapper\n \n torch = torch_wrapper.torch\n "}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "renamed", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1,9 +1,10 @@\n from __future__ import annotations\n \n-import triton\n+# import triton\n+from ..language import core as lcore\n+from . import torch_wrapper\n from .core import ExecutionContext\n from .memory_map import MemoryMap\n-from triton.debugger import torch_wrapper\n \n torch = torch_wrapper.torch\n \n@@ -389,7 +390,7 @@ def zeros(self, shape, dtype):\n             if not isinstance(d.value, int):\n                 raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n         shape = [x.value for x in shape]\n-        if isinstance(dtype, triton.language.core.dtype):\n+        if isinstance(dtype, lcore.dtype):\n             if dtype.is_fp32():\n                 dtype = torch.float32\n             elif dtype.is_fp16():"}, {"filename": "python/triton/interpreter/torch_wrapper.py", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/debugger/torch_wrapper.py"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -77,7 +77,7 @@\n     static_range,\n     tensor,\n     trans,\n-    triton,\n+    # triton,\n     uint16,\n     uint32,\n     uint64,"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 16, "deletions": 15, "changes": 31, "file_content_changes": "@@ -5,9 +5,10 @@\n from functools import wraps\n from typing import Callable, List, Sequence, TypeVar\n \n-import triton\n+from .._C.libtriton.triton import ir\n+# import triton\n+from ..runtime.jit import jit\n from . import semantic\n-from triton._C.libtriton.triton import ir\n \n T = TypeVar('T')\n \n@@ -1344,7 +1345,7 @@ def _reduce_with_indices(input, axis, combine_fn, _builder=None, _generator=None\n     return rvalue, rindices\n \n \n-@triton.jit\n+@jit\n def minimum(x, y):\n     \"\"\"\n     Computes the element-wise minimum of :code:`x` and :code:`y`.\n@@ -1357,7 +1358,7 @@ def minimum(x, y):\n     return where(x < y, x, y)\n \n \n-@triton.jit\n+@jit\n def maximum(x, y):\n     \"\"\"\n     Computes the element-wise maximum of :code:`x` and :code:`y`.\n@@ -1372,20 +1373,20 @@ def maximum(x, y):\n # max and argmax\n \n \n-@triton.jit\n+@jit\n def _max_combine(a, b):\n     return maximum(a, b)\n \n \n-@triton.jit\n+@jit\n def _argmax_combine(value1, index1, value2, index2):\n     gt = value1 > value2\n     value_ret = where(gt, value1, value2)\n     index_ret = where(gt, index1, index2)\n     return value_ret, index_ret\n \n \n-@triton.jit\n+@jit\n @_add_reduction_docstr(\"maximum\")\n def max(input, axis=None, return_indices=False):\n     input = _promote_reduction_input(input)\n@@ -1395,7 +1396,7 @@ def max(input, axis=None, return_indices=False):\n         return reduce(input, axis, _max_combine)\n \n \n-@triton.jit\n+@jit\n @_add_reduction_docstr(\"maximum index\")\n def argmax(input, axis):\n     (_, ret) = max(input, axis, return_indices=True)\n@@ -1404,21 +1405,21 @@ def argmax(input, axis):\n # min and argmin\n \n \n-@triton.jit\n+@jit\n def _min_combine(a, b):\n     # TODO: minimum/maximum doesn't get lowered to fmin/fmax...\n     return minimum(a, b)\n \n \n-@triton.jit\n+@jit\n def _argmin_combine(value1, index1, value2, index2):\n     lt = value1 < value2\n     value_ret = where(lt, value1, value2)\n     index_ret = where(lt, index1, index2)\n     return value_ret, index_ret\n \n \n-@triton.jit\n+@jit\n @_add_reduction_docstr(\"minimum\")\n def min(input, axis=None, return_indices=False):\n     input = _promote_reduction_input(input)\n@@ -1428,28 +1429,28 @@ def min(input, axis=None, return_indices=False):\n         return reduce(input, axis, _min_combine)\n \n \n-@triton.jit\n+@jit\n @_add_reduction_docstr(\"minimum index\")\n def argmin(input, axis):\n     _, ret = min(input, axis, return_indices=True)\n     return ret\n \n \n-@triton.jit\n+@jit\n def _sum_combine(a, b):\n     return a + b\n \n # sum\n \n \n-@triton.jit\n+@jit\n @_add_reduction_docstr(\"sum\")\n def sum(input, axis=None):\n     input = _promote_reduction_input(input)\n     return reduce(input, axis, _sum_combine)\n \n \n-@triton.jit\n+@jit\n def _xor_combine(a, b):\n     return a ^ b\n "}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "file_content_changes": "@@ -1,4 +1,4 @@\n-import triton\n+from ..runtime.jit import jit\n from . import core as tl\n \n PHILOX_KEY_A: tl.constexpr = 0x9E3779B9\n@@ -12,7 +12,7 @@\n # -------------------\n \n \n-@triton.jit\n+@jit\n def philox_impl(c0, c1, c2, c3, k0, k1, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Run `n_rounds` rounds of Philox for state (c0, c1, c2, c3) and key (k0, k1).\n@@ -33,7 +33,7 @@ def philox_impl(c0, c1, c2, c3, k0, k1, n_rounds: tl.constexpr = N_ROUNDS_DEFAUL\n     return c0, c1, c2, c3\n \n \n-@triton.jit\n+@jit\n def philox(seed, c0, c1, c2, c3, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     seed = seed.to(tl.uint64)\n     seed_hi = ((seed >> 32) & 0xffffffff).to(tl.uint32)\n@@ -45,7 +45,7 @@ def philox(seed, c0, c1, c2, c3, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     return philox_impl(c0, c1, c2, c3, seed_lo, seed_hi, n_rounds)\n \n \n-@triton.jit\n+@jit\n def randint(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block, returns a single\n@@ -61,7 +61,7 @@ def randint(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     return ret\n \n \n-@triton.jit\n+@jit\n def randint4x(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block, returns four\n@@ -82,15 +82,15 @@ def randint4x(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n # rand\n # -------------------\n \n-# @triton.jit\n+# @jit\n # def uint32_to_uniform_float(x):\n #     \"\"\"\n #     Numerically stable function to convert a random uint32 into a random float uniformly sampled in [0, 1).\n #     \"\"\"\n #     two_to_the_minus_32: tl.constexpr = 2.328306e-10\n #     return x * two_to_the_minus_32\n \n-@triton.jit\n+@jit\n def uint32_to_uniform_float(x):\n     \"\"\"\n     Numerically stable function to convert a random uint32 into a random float uniformly sampled in [0, 1).\n@@ -102,7 +102,7 @@ def uint32_to_uniform_float(x):\n     return x * scale\n \n \n-@triton.jit\n+@jit\n def rand(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,\n@@ -116,7 +116,7 @@ def rand(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     return uint32_to_uniform_float(source)\n \n \n-@triton.jit\n+@jit\n def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offsets` block,\n@@ -138,7 +138,7 @@ def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n # -------------------\n \n \n-@triton.jit\n+@jit\n def pair_uniform_to_normal(u1, u2):\n     \"\"\"Box-Muller transform\"\"\"\n     u1 = tl.maximum(1.0e-7, u1)\n@@ -147,7 +147,7 @@ def pair_uniform_to_normal(u1, u2):\n     return r * tl.cos(th), r * tl.sin(th)\n \n \n-@triton.jit\n+@jit\n def randn(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,\n@@ -163,7 +163,7 @@ def randn(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     return n1\n \n \n-@triton.jit\n+@jit\n def randn4x(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -1218,12 +1218,13 @@ def dot(lhs: tl.tensor,\n         out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n-    assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n-    assert len(lhs.shape) == 2 and len(rhs.shape) == 2\n-    assert lhs.shape[1].value == rhs.shape[0].value\n+    assert lhs.dtype == rhs.dtype, f\"First input ({lhs.dtype}) and second input ({rhs.dtype}) must have the same dtype!\"\n+    assert len(lhs.shape) == 2, f\"First input shape ({lhs.shape}) is not two dimensional!\"\n+    assert len(rhs.shape) == 2, f\"Second input shape ({rhs.shape}) is not two dimensional!\"\n+    assert lhs.shape[1].value == rhs.shape[0].value, f\"First input shape ({lhs.shape}) and second input shape {rhs.shape} are not compatible for matmul (second index of first shape ({lhs.shape[1].value}) must be equal to first index of second shape ({rhs.shape[0].value})\"\n     assert lhs.shape[0].value >= 16 and lhs.shape[1].value >= 16 \\\n         and rhs.shape[1].value >= 16,\\\n-        \"small blocks not supported!\"\n+        f\"All values in both first input shape ({lhs.shape}) and second input shape ({rhs.shape}) must be >= 16!\"\n     if lhs.type.scalar.is_int():\n         assert lhs.type.scalar == tl.int8, \"only int8 supported!\"\n         # TODO: This is CUDA specific, check if ROCm has the same limitation"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "file_content_changes": "@@ -1,7 +1,10 @@\n import torch\n \n-import triton\n-import triton.language as tl\n+from ... import cdiv, heuristics, jit\n+from ... import language as tl\n+\n+# import triton\n+# import language as tl\n \n # ********************************************************\n # --------------------------------------------------------\n@@ -13,10 +16,10 @@\n # ********************************************************\n \n \n-@triton.heuristics({\n+@heuristics({\n     'EVEN_K': lambda nargs: nargs['K'] % nargs['TILE_K'] == 0,\n })\n-@triton.jit\n+@jit\n def _sdd_kernel(\n     A, B, C,\n     stride_za, stride_ha, stride_ma, stride_ak,\n@@ -127,7 +130,7 @@ def sdd_lut(layout, block, device):\n # -----------------------------\n \n \n-@triton.jit\n+@jit\n def _dsd_kernel(\n     A, B, C,\n     stride_az, stride_ha, stride_am, stride_ak,\n@@ -227,7 +230,7 @@ def dsd_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, lut, width, out=N\n     # meta-parameter heuristics\n     TILE_N = 128\n     # compute output\n-    grid = lambda meta: [triton.cdiv(BS3, meta['TILE_N']), width, BS0]\n+    grid = lambda meta: [cdiv(BS3, meta['TILE_N']), width, BS0]\n     _dsd_kernel[grid](\n         a, b, c,\n         a.stride(0), a.stride(1), a.stride(3 if trans_a else 2), a.stride(2 if trans_a else 3),"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "file_content_changes": "@@ -1,7 +1,10 @@\n import torch\n \n-import triton\n-import triton.language as tl\n+# import triton\n+# import language as tl\n+from ... import jit\n+from ... import language as tl\n+from ... import next_power_of_2\n \n \n def num_warps(n):\n@@ -16,7 +19,7 @@ def num_warps(n):\n     return 16\n \n \n-@triton.jit\n+@jit\n def _blocksparse_softmax_fwd(\n     Out, A, stride_xz, LUT,\n     R, extent, stride_zr, stride_hr,  # relative attention\n@@ -71,7 +74,7 @@ def _blocksparse_softmax_fwd(\n     tl.store(Out + off_a + lane_n, out, mask=mask)\n \n \n-@triton.jit\n+@jit\n def _blocksparse_softmax_bwd(\n     DA, stride_zdx,\n     DOut, stride_zdout,\n@@ -169,7 +172,7 @@ def forward(\n             scale,\n             is_causal,\n             BLOCK_SIZE=block,\n-            ROW_SIZE=triton.next_power_of_2(maxlut),\n+            ROW_SIZE=next_power_of_2(maxlut),\n             IS_DENSE=is_dense,\n             num_warps=num_warps(maxlut)\n         )\n@@ -208,7 +211,7 @@ def backward(ctx, dout):\n             dr, ctx.rel_shape[-1], ctx.rel_strides[0], ctx.rel_strides[1], ctx.rel_strides[2],\n             ctx.is_causal,\n             BLOCK_SIZE=ctx.block,\n-            ROW_SIZE=triton.next_power_of_2(ctx.maxlut),\n+            ROW_SIZE=next_power_of_2(ctx.maxlut),\n             IS_DENSE=ctx.is_dense,\n             num_warps=num_warps(ctx.maxlut)\n         )"}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 11, "deletions": 8, "changes": 19, "file_content_changes": "@@ -1,7 +1,10 @@\n import torch\n \n-import triton\n-import triton.language as tl\n+# import triton\n+# import language as tl\n+from .. import heuristics, jit\n+from .. import language as tl\n+from .. import next_power_of_2\n \n \n def num_warps(N):\n@@ -12,9 +15,9 @@ def num_warps(N):\n     return 16\n \n \n-@triton.heuristics({'num_warps': lambda nargs: num_warps(nargs['N'])})\n-@triton.heuristics({'BLOCK': lambda nargs: triton.next_power_of_2(nargs['N'])})\n-@triton.jit\n+@heuristics({'num_warps': lambda nargs: num_warps(nargs['N'])})\n+@heuristics({'BLOCK': lambda nargs: next_power_of_2(nargs['N'])})\n+@jit\n def _forward(LOGITS, PROBS, IDX, LOSS, N, BLOCK: tl.constexpr):\n     row = tl.program_id(0)\n     cols = tl.arange(0, BLOCK)\n@@ -37,9 +40,9 @@ def _forward(LOGITS, PROBS, IDX, LOSS, N, BLOCK: tl.constexpr):\n     tl.store(LOSS + row, probs)\n \n \n-@triton.heuristics({'num_warps': lambda nargs: num_warps(nargs['N'])})\n-@triton.heuristics({'BLOCK': lambda nargs: triton.next_power_of_2(nargs['N'])})\n-@triton.jit\n+@heuristics({'num_warps': lambda nargs: num_warps(nargs['N'])})\n+@heuristics({'BLOCK': lambda nargs: next_power_of_2(nargs['N'])})\n+@jit\n def _backward(PROBS, IDX, DPROBS, N, BLOCK: tl.constexpr):\n     row = tl.program_id(0)\n     cols = tl.arange(0, BLOCK)"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "file_content_changes": "@@ -7,11 +7,14 @@\n \n import torch\n \n-import triton\n-import triton.language as tl\n+from .. import cdiv, jit\n+from .. import language as tl\n \n+# import triton\n+# import language as tl\n \n-@triton.jit\n+\n+@jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,\n     L, M,\n@@ -87,7 +90,7 @@ def _fwd_kernel(\n     tl.store(out_ptrs, acc)\n \n \n-@triton.jit\n+@jit\n def _bwd_preprocess(\n     Out, DO, L,\n     NewDO, Delta,\n@@ -107,7 +110,7 @@ def _bwd_preprocess(\n     tl.store(Delta + off_m, delta)\n \n \n-@triton.jit\n+@jit\n def _bwd_kernel(\n     Q, K, V, sm_scale, Out, DO,\n     DQ, DK, DV,\n@@ -205,7 +208,7 @@ def forward(ctx, q, k, v, sm_scale):\n         assert Lq == Lk and Lk == Lv\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n-        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+        grid = (cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         num_warps = 4 if Lk <= 64 else 8"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 31, "deletions": 28, "changes": 59, "file_content_changes": "@@ -1,9 +1,12 @@\n import torch\n \n-import triton\n-import triton.language as tl\n+from .. import Config, autotune, cdiv, heuristics, jit\n+from .. import language as tl\n from .matmul_perf_model import early_config_prune, estimate_matmul_time\n \n+# import triton\n+# import language as tl\n+\n \n def init_to_zero(name):\n     return lambda nargs: nargs[name].zero_()\n@@ -17,37 +20,37 @@ def get_configs_io_bound():\n                 for block_n in [32, 64, 128, 256]:\n                     num_warps = 2 if block_n <= 64 else 4\n                     configs.append(\n-                        triton.Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': 1},\n-                                      num_stages=num_stages, num_warps=num_warps))\n+                        Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': 1},\n+                               num_stages=num_stages, num_warps=num_warps))\n                     # split_k\n                     for split_k in [2, 4, 8, 16]:\n-                        configs.append(triton.Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': split_k},\n-                                                     num_stages=num_stages, num_warps=num_warps, pre_hook=init_to_zero('C')))\n+                        configs.append(Config({'BLOCK_M': block_m, 'BLOCK_N': block_n, 'BLOCK_K': block_k, 'SPLIT_K': split_k},\n+                                              num_stages=num_stages, num_warps=num_warps, pre_hook=init_to_zero('C')))\n     return configs\n \n \n-@triton.autotune(\n+@autotune(\n     configs=[\n         # basic configs for compute-bound matmuls\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2),\n         # good for int8\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n-        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n-        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=5, num_warps=2),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8),\n+        Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4),\n+        Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=5, num_warps=2),\n     ] + get_configs_io_bound(),\n     key=['M', 'N', 'K'],\n     prune_configs_by={\n@@ -56,10 +59,10 @@ def get_configs_io_bound():\n         'top_k': 10\n     },\n )\n-@triton.heuristics({\n+@heuristics({\n     'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0,\n })\n-@triton.jit\n+@jit\n def _kernel(A, B, C, M, N, K,\n             stride_am, stride_ak,\n             stride_bk, stride_bn,\n@@ -146,7 +149,7 @@ def _call(a, b, dot_out_dtype):\n             else:\n                 dot_out_dtype = tl.int32\n         # launch kernel\n-        grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n+        grid = lambda META: (cdiv(M, META['BLOCK_M']) * cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n         _kernel[grid](a, b, c, M, N, K,\n                       a.stride(0), a.stride(1),\n                       b.stride(0), b.stride(1),"}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "file_content_changes": "@@ -2,10 +2,11 @@\n \n import torch\n \n-import triton\n-import triton._C.libtriton.triton as _triton\n-from triton.runtime import driver\n-from triton.testing import get_dram_gbps, get_max_simd_tflops, get_max_tensorcore_tflops\n+# import triton\n+from .. import cdiv\n+from .._C.libtriton.triton import runtime\n+from ..runtime import driver\n+from ..testing import get_dram_gbps, get_max_simd_tflops, get_max_tensorcore_tflops\n \n \n def get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype):\n@@ -41,13 +42,13 @@ def estimate_matmul_time(\n ):\n     ''' return estimated running time in ms\n           = max(compute, loading) + store '''\n-    backend = _triton.runtime.backend.CUDA\n+    backend = runtime.backend.CUDA\n     device = torch.cuda.current_device()\n     dtype = A.dtype\n     dtsize = A.element_size()\n \n-    num_cta_m = triton.cdiv(M, BLOCK_M)\n-    num_cta_n = triton.cdiv(N, BLOCK_N)\n+    num_cta_m = cdiv(M, BLOCK_M)\n+    num_cta_n = cdiv(N, BLOCK_N)\n     num_cta_k = SPLIT_K\n     num_ctas = num_cta_m * num_cta_n * num_cta_k\n "}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 85, "deletions": 18, "changes": 103, "file_content_changes": "@@ -8,9 +8,17 @@\n import subprocess\n import textwrap\n from collections import defaultdict, namedtuple\n-from typing import Callable, Generic, Iterable, Optional, TypeVar, Union, cast, overload\n+from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n+                    overload)\n \n-import triton\n+import torch\n+\n+# import triton\n+# from .. import compile, CompiledKernel\n+from ..common.backend import get_backend\n+\n+TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+TRITON_VERSION = \"2.1.0\"\n \n \n def get_cuda_stream(idx=None):\n@@ -65,7 +73,7 @@ def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n         while isinstance(lhs, ast.Attribute):\n             lhs = self.visit(lhs.value)\n-        if lhs is None or lhs is triton:\n+        if lhs is None or lhs.__name__ == \"triton\":\n             return None\n         return getattr(lhs, node.attr)\n \n@@ -100,15 +108,15 @@ def version_key():\n     with open(__file__, \"rb\") as f:\n         contents += [hashlib.md5(f.read()).hexdigest()]\n     # compiler\n-    compiler_path = os.path.join(*triton.__path__, 'compiler')\n+    compiler_path = os.path.join(TRITON_PATH, 'compiler')\n     for lib in pkgutil.iter_modules([compiler_path]):\n         with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n             contents += [hashlib.md5(f.read()).hexdigest()]\n     # backend\n-    with open(triton._C.libtriton.__file__, \"rb\") as f:\n+    with open(os.path.join(TRITON_PATH, \"_C/libtriton.so\"), \"rb\") as f:\n         contents += [hashlib.md5(f.read()).hexdigest()]\n     # language\n-    language_path = os.path.join(*triton.__path__, 'language')\n+    language_path = os.path.join(TRITON_PATH, 'language')\n     for lib in pkgutil.iter_modules([language_path]):\n         with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n             contents += [hashlib.md5(f.read()).hexdigest()]\n@@ -117,7 +125,7 @@ def version_key():\n         ptxas_version = hashlib.md5(subprocess.check_output([\"ptxas\", \"--version\"])).hexdigest()\n     except Exception:\n         ptxas_version = ''\n-    return '-'.join(triton.__version__) + '-' + ptxas_version + '-' + '-'.join(contents)\n+    return '-'.join(TRITON_VERSION) + '-' + ptxas_version + '-' + '-'.join(contents)\n \n \n class KernelInterface(Generic[T]):\n@@ -158,6 +166,22 @@ def _key_of(arg):\n         else:\n             raise TypeError(f'Unsupported type {type(arg)} for {arg}')\n \n+    @staticmethod\n+    def _device_of(arg):\n+        if hasattr(arg, \"device\"):\n+            if hasattr(arg.device, 'type'):\n+                return arg.device.type\n+\n+        return ''\n+\n+    @staticmethod\n+    def _pinned_memory_of(arg):\n+        if hasattr(arg, \"is_pinned\"):\n+            if isinstance(arg.is_pinned, Callable):\n+                return arg.is_pinned()\n+\n+        return False\n+\n     @staticmethod\n     def _spec_of(arg):\n         if hasattr(arg, \"data_ptr\"):\n@@ -261,12 +285,28 @@ def _get_arg_sig_key(self, arg) -> str:\n         else:\n             return f'_key_of({arg})'\n \n+    def _conclude_device_type(self, device_types: List[str], pinned_memory_flags: List[bool]) -> str:\n+        device_types = [device_type for device_type in device_types if device_type != '']\n+        # Return cuda if one of the input tensors is cuda\n+        if 'cuda' in device_types:\n+            return 'hip' if torch.version.hip else 'cuda'\n+\n+        is_cpu = all(device_type == 'cpu' for device_type in device_types)\n+        is_pinned_memory = any(pinned_memory_flag for pinned_memory_flag in pinned_memory_flags)\n+        # Return cuda if all the input tensors are cpu while the memory is pinned\n+        if is_cpu and is_pinned_memory:\n+            return 'cuda'\n+\n+        return device_types[0] if len(device_types) > 0 else 'cuda'\n+\n     def _make_launcher(self):\n         regular_args = [f'{arg}' for i, arg in enumerate(self.arg_names) if i not in self.constexprs]\n         constexpr_args = [f'{arg}' for i, arg in enumerate(self.arg_names) if i in self.constexprs]\n         args = ', '.join(regular_args)\n         # cache key for regular argument type\n         sig_keys = ', '.join([self._get_arg_sig_key(arg) for arg in regular_args])\n+        device_types = '[' + ', '.join([f'_device_of({arg})' for arg in regular_args]) + ']'\n+        pinned_memory_flags = '[' + ', '.join([f'_pinned_memory_of({arg})' for arg in regular_args]) + ']'\n         # cache key for constexpr argument values\n         constexpr_keys = ', '.join(constexpr_args)\n         # cache key for argument specialization\n@@ -280,7 +320,8 @@ def _make_launcher(self):\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n \n         src = f\"\"\"\n-def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None):\n+def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n+    from ..compiler import compile, CompiledKernel\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n@@ -294,15 +335,34 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     grid_0 = grid[0]\n     grid_1 = grid[1] if grid_size > 1 else 1\n     grid_2 = grid[2] if grid_size > 2 else 1\n+\n+    if device_type is None:\n+        device_types = [_device_type for _device_type in {device_types} if _device_type != '']\n+        device_type = self._conclude_device_type(device_types, {pinned_memory_flags})\n+\n+    device_backend = None\n+    if device_type not in ['cuda', 'hip']:\n+        device_backend = get_backend(device_type)\n+        if device_backend is None:\n+            raise ValueError('Cannot find backend for ' + device_type)\n+\n     if device is None:\n-        device = get_current_device()\n-        set_current_device(device)\n+        if device_type in ['cuda', 'hip']:\n+            device = get_current_device()\n+            set_current_device(device)\n+        else:\n+            device = device_backend.get_current_device()\n+            device_backend.set_current_device(device)\n     if stream is None and not warmup:\n-      stream = get_cuda_stream(device)\n+        if device_type in ['cuda', 'hip']:\n+            stream = get_cuda_stream(device)\n+        else:\n+            stream = device_backend.get_stream()\n+\n     bin = cache[device].get(key, None)\n     if bin is not None:\n       if not warmup:\n-          bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, {args})\n+          bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, bin, {args})\n       return bin\n     # kernel not cached -- compile\n     else:\n@@ -320,16 +380,23 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n         if callable(arg):\n           raise TypeError(f\"Callable constexpr at index {{i}} is not supported\")\n       if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n-        bin = triton.compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs, configs=configs, debug=self.debug)\n+        bin = compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs, configs=configs, debug=self.debug, device_type=device_type)\n         if not warmup:\n-            bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, *args)\n+            bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, bin, *args)\n         self.cache[device][key] = bin\n         return bin\n       return None\n \"\"\"\n-        scope = {\"version_key\": version_key(), \"get_cuda_stream\": get_cuda_stream,\n-                 \"self\": self, \"_spec_of\": self._spec_of, \"_key_of\": self._key_of,\n-                 \"cache\": self.cache, \"triton\": triton,\n+        scope = {\"version_key\": version_key(),\n+                 \"get_cuda_stream\": get_cuda_stream,\n+                 \"self\": self,\n+                 \"_spec_of\": self._spec_of,\n+                 \"_key_of\": self._key_of,\n+                 \"_device_of\": self._device_of,\n+                 \"_pinned_memory_of\": self._pinned_memory_of,\n+                 \"cache\": self.cache,\n+                 \"__spec__\": __spec__,\n+                 \"get_backend\": get_backend,\n                  \"get_current_device\": get_current_device,\n                  \"set_current_device\": set_current_device}\n         exec(src, scope)\n@@ -462,7 +529,7 @@ def jit(\n     def decorator(fn: T) -> JITFunction[T]:\n         assert callable(fn)\n         if interpret:\n-            from ..debugger.debugger import GridSelector\n+            from ..interpreter.interpreter import GridSelector\n             return GridSelector(fn)\n         else:\n             return JITFunction("}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -4,7 +4,7 @@\n import sys\n from contextlib import contextmanager\n \n-import triton._C.libtriton.triton as _triton\n+from ._C.libtriton.triton import runtime\n \n \n def nvsmi(attrs):\n@@ -281,7 +281,7 @@ def get_dram_gbps(backend=None, device=None):\n \n     from .runtime import driver\n     if not backend:\n-        backend = _triton.runtime.backend.CUDA\n+        backend = runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n     mem_clock_khz = driver.utils.get_device_properties(device)[\"mem_clock_rate\"]  # in kHz\n@@ -295,7 +295,7 @@ def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None)\n \n     from .runtime import driver\n     if not backend:\n-        backend = _triton.runtime.backend.CUDA\n+        backend = runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n \n@@ -398,7 +398,7 @@ def get_max_simd_tflops(dtype, backend=None, device=None):\n \n     from .runtime import driver\n     if not backend:\n-        backend = _triton.runtime.backend.CUDA\n+        backend = runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n "}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 18, "deletions": 15, "changes": 33, "file_content_changes": "@@ -1,8 +1,11 @@\n import argparse\n import sys\n \n-import triton._C.libtriton.triton as libtriton\n-import triton.compiler.compiler as tc\n+from .._C.libtriton.triton import ir\n+# import triton.compiler.compiler as tc\n+from ..compiler.compiler import (get_amdgpu_arch_fulldetails, llir_to_amdgcn_and_hsaco,\n+                                 llir_to_ptx, optimize_ttgir, optimize_ttir,\n+                                 ttgir_to_llir, ttir_to_ttgir)\n \n if __name__ == '__main__':\n \n@@ -32,12 +35,12 @@\n         sys.exit(0)\n \n     # parse source file to MLIR module\n-    context = libtriton.ir.context()\n-    module = libtriton.ir.parse_mlir_module(args.src, context)\n+    context = ir.context()\n+    module = ir.parse_mlir_module(args.src, context)\n     module.context = context\n \n     # optimizer triton-ir\n-    module = tc.optimize_ttir(module, arch=args.sm)\n+    module = optimize_ttir(module, arch=args.sm)\n     if args.target == 'triton-ir':\n         print(module.str())\n         sys.exit(0)\n@@ -49,7 +52,7 @@\n     if args.target == 'amdgcn':\n         # auto detect available architecture and features\n         # if nothing detected, set with default values\n-        arch_details = tc.get_amdgpu_arch_fulldetails()\n+        arch_details = get_amdgpu_arch_fulldetails()\n         if not arch_details:\n             arch_name = \"\"\n             arch_triple = \"amdgcn-amd-amdhsa\"\n@@ -71,13 +74,13 @@\n \n         # triton-ir -> triton-gpu-ir\n         # use compute_capability == 80\n-        module = tc.ttir_to_ttgir(module, num_warps=args.num_warps)  # num_stages=3, compute_capability=80)\n-        module = tc.optimize_ttgir(module, num_stages=3, arch=80)\n+        module = ttir_to_ttgir(module, num_warps=args.num_warps)  # num_stages=3, compute_capability=80)\n+        module = optimize_ttgir(module, num_stages=3, arch=80)\n         # triton-gpu-ir -> llvm-ir\n         # use compute_capability == 80\n-        module = tc.ttgir_to_llir(module, extern_libs=None, arch=80)\n+        module = ttgir_to_llir(module, extern_libs=None, arch=80)\n         # llvm-ir -> amdgcn asm, hsaco binary\n-        module, hsaco_path = tc.llir_to_amdgcn_and_hsaco(module, arch_name, arch_triple, arch_features)\n+        module, hsaco_path = llir_to_amdgcn_and_hsaco(module, arch_name, arch_triple, arch_features)\n \n         print(hsaco_path)\n         print(module)\n@@ -87,14 +90,14 @@\n         raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n \n     # triton-ir -> triton-gpu-ir\n-    module = tc.ttir_to_ttgir(module, num_warps=args.num_warps)\n-    module = tc.optimize_ttgir(module, num_stages=3, arch=args.sm)\n+    module = ttir_to_ttgir(module, num_warps=args.num_warps)\n+    module = optimize_ttgir(module, num_stages=3, arch=args.sm)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n         sys.exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n-    module = tc.ttgir_to_llir(module, extern_libs=None, arch=args.sm)\n+    module = ttgir_to_llir(module, extern_libs=None, arch=args.sm)\n     if args.target == 'llvm-ir':\n         print(module)\n         sys.exit(0)\n@@ -103,12 +106,12 @@\n     if args.target == 'ptx':\n         if not args.ptx_version:\n             raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")\n-        module = tc.llir_to_ptx(module, arch=args.sm, ptx_version=args.ptx_version)\n+        module = llir_to_ptx(module, arch=args.sm, ptx_version=args.ptx_version)\n \n     # llvm-ir -> amdgcn\n     if args.target == 'amdgcn':\n         if not args.gfx:\n             raise argparse.ArgumentError(None, \"Must specify --gfx for AMDGCN compilation\")\n-        module, hsaco_path = tc.llir_to_amdgcn_and_hsaco(module, args.gfx)\n+        module, hsaco_path = llir_to_amdgcn_and_hsaco(module, args.gfx)\n \n     print(module)"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 24, "deletions": 5, "changes": 29, "file_content_changes": "@@ -9,6 +9,8 @@\n // CHECK: [[$col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[$col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK-LABEL: cst\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n tt.func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -67,8 +69,6 @@ tt.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   tt.return\n }\n \n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<16x!tt.ptr<i32>, #layout1>\n   %1 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #layout1>\n@@ -80,7 +80,6 @@ tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   tt.store %5, %4 : tensor<16xi32, #layout0>\n   tt.return\n }\n-}\n \n // CHECK-LABEL: if\n tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n@@ -164,6 +163,8 @@ tt.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility =\n   tt.return\n }\n \n+}\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n #slice1dim1 = #triton_gpu.slice<{dim = 1, parent = #blocked1}>\n@@ -173,6 +174,7 @@ tt.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility =\n #blocked4 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n // CHECK-LABEL: transpose\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[$row_layout]]>\n@@ -212,8 +214,10 @@ tt.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32\n   tt.store %24, %25, %26 : tensor<64x64xf32, #blocked4>\n   tt.return\n }\n+}\n \n // CHECK-LABEL: loop\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>)\n@@ -266,8 +270,10 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n   tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n   tt.return\n }\n+}\n \n // CHECK-LABEL: loop_if\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n   %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n   %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n@@ -318,8 +324,10 @@ tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i3\n   tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n   tt.return\n }\n+}\n \n // CHECK-LABEL: vecadd\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c256_i32 = arith.constant 256 : i32\n@@ -349,9 +357,11 @@ tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   tt.return\n }\n+}\n \n // Select has args with different element types\n // CHECK-LABEL: select\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %cst = arith.constant dense<30000> : tensor<1x1xi32, #blocked2>\n@@ -400,9 +410,11 @@ tt.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr\n   }\n   tt.return\n }\n+}\n \n // Make sure the following IR doesn't hang the compiler.\n // CHECK-LABEL: long_func\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg12: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg14: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg15: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) {\n   %cst = arith.constant dense<1.000000e+00> : tensor<1024xf32, #blocked0>\n   %cst_0 = arith.constant dense<5.000000e-04> : tensor<1024xf32, #blocked0>\n@@ -796,10 +808,12 @@ tt.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg\n   tt.store %365, %366 : tensor<1024xf64, #blocked0>\n   tt.return\n }\n+}\n \n // A mnist model from torch inductor.\n // Check if topological sort is working correct and there's no unnecessary convert\n // CHECK-LABEL: mnist\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %cst = arith.constant dense<10> : tensor<16x1xi32, #blocked2>\n@@ -884,17 +898,19 @@ tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !\n   tt.store %61, %62, %63 : tensor<16x16xf32, #blocked4>\n   tt.return\n }\n+}\n \n // -----\n \n+// cmpf and cmpi have different operands and result types\n+// CHECK-LABEL: cmp\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [8, 1], order = [0, 1]}>\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 4], order = [0, 1]}>\n #blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8], order = [0, 1]}>\n #blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>\n #blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [8, 1], order = [1, 0]}>\n-// cmpf and cmpi have different operands and result types\n-// CHECK-LABEL: cmp\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) {\n   %c64 = arith.constant 64 : index\n   %c2048 = arith.constant 2048 : index\n@@ -1034,11 +1050,13 @@ tt.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt\n   }\n   tt.return\n }\n+}\n \n // -----\n \n // Just make sure it doesn't crash on non-tensor types.\n // CHECK-LABEL: if_no_tensor\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c-1_i64 = arith.constant -1 : i64\n@@ -1062,6 +1080,7 @@ tt.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %\n   tt.store %9, %8 {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.return\n }\n+}\n \n // -----\n "}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -11,6 +11,8 @@\n #BLR = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BLC = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n // CHECK: tt.func @push_elementwise1\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n // CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n@@ -122,3 +124,5 @@ tt.func @push_elementwise5(\n   %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av1> * tensor<16x16xf16, #Bv1> -> tensor<16x16xf32, #Cv1>\n   tt.return %newc : tensor<16x16xf32, #Cv1>\n }\n+\n+}"}, {"filename": "test/lib/Analysis/TestAlias.cpp", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -13,6 +13,13 @@ struct TestAliasPass\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAliasPass);\n \n+  static std::string getValueOperandName(Value value, AsmState &state) {\n+    std::string opName;\n+    llvm::raw_string_ostream ss(opName);\n+    value.printAsOperand(ss, state);\n+    return opName;\n+  }\n+\n   static void print(StringRef name, SmallVector<std::string, 4> &vals,\n                     raw_ostream &os) {\n     if (vals.empty())"}, {"filename": "third_party/intel_xpu_backend", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+Subproject commit 0bcc485f82b34d49494bd0264bacc24a20aafb7a"}]