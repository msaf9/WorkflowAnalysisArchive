[{"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -261,7 +261,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module) {\n   }\n \n   auto optPipeline = mlir::makeOptimizingTransformer(\n-      /*optLevel=*/0, /*sizeLevel=*/0,\n+      /*optLevel=*/3, /*sizeLevel=*/0,\n       /*targetMachine=*/nullptr);\n \n   if (auto err = optPipeline(llvmModule.get())) {"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 18, "deletions": 1, "changes": 19, "file_content_changes": "@@ -1215,7 +1215,24 @@ def max_contiguous(input, values, _builder=None):\n \n @triton.jit\n def abs(x):\n-    return where(x >= 0, x, -x)\n+    if x.dtype.is_fp8():\n+        y = x.to(triton.language.int8, bitcast=True) & 0xef\n+        z = y.to(triton.language.fp8, bitcast=True)\n+    elif x.dtype.is_fp16():\n+        y = x.to(triton.language.int16, bitcast=True) & 0xefff\n+        z = y.to(triton.language.fp16, bitcast=True)\n+    elif x.dtype.is_bf16():\n+        y = x.to(triton.language.int16, bitcast=True) & 0xefff\n+        z = y.to(triton.language.bf16, bitcast=True)\n+    elif x.dtype.is_fp32():\n+        y = x.to(triton.language.int32, bitcast=True) & 0xefffffff\n+        z = y.to(triton.language.fp32, bitcast=True)\n+    elif x.dtype.is_fp64():\n+        y = x.to(triton.language.int64, bitcast=True) & 0xefffffffffffffff\n+        z = y.to(triton.language.fp64, bitcast=True)\n+    else:\n+        z = where(x >= 0, x, -x)\n+    return z\n \n \n @triton.jit"}]