[{"filename": "README.md", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -18,7 +18,7 @@ Tentative Agenda for the conference (subject to change):\n \n |Time    |Title  |Speaker\n |--------|-------|-------|\n-|10:00 AM|Welcome|Microsoft|\n+|10:00 AM|Welcome|Kevin Scott (Microsoft)|\n |10:20 AM|The Triton Compiler: Past, Present and Future|Phil Tillet (OpenAI)|\n |11:00 AM|**Break**||\n |11:20 AM|Hopper support in Triton|Gustav Zhu (Nvidia)|\n@@ -35,6 +35,7 @@ Tentative Agenda for the conference (subject to change):\n |3:40 PM|Writing Grouped GEMMs in Triton|Vinod Grover (Nvidia)|\n |4:00 PM|**Reception**||\n \n+\n # Triton\n \n This is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs."}, {"filename": "docs/conf.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -105,7 +105,8 @@ def documenter(app, obj, parent):\n     'within_subsection_order': FileNameSortKey,\n     'reference_url': {\n         'sphinx_gallery': None,\n-    }\n+    },\n+    'abort_on_example_error': True,\n }\n \n # Add any paths that contain templates here, relative to this directory.\n@@ -144,7 +145,7 @@ def documenter(app, obj, parent):\n #\n # This is also used if you do content translation via gettext catalogs.\n # Usually you set \"language\" from the command line for these cases.\n-language = None\n+language = 'en'\n \n # List of patterns, relative to source directory, that match files and\n # directories to ignore when looking for source files."}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -128,7 +128,8 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n   for (int i = 0; i < aNumPtr; ++i) {\n     aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n   }\n-  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n+  auto elemTy = typeConverter->convertType(\n+      A.getType().cast<RankedTensorType>().getElementType());\n \n   Type ptrTy = ptr_ty(elemTy, 3);\n   SmallVector<Value> aPtrs(aNumPtr);\n@@ -192,7 +193,8 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n   for (int i = 0; i < bNumPtr; ++i) {\n     bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n   }\n-  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n+  auto elemTy = typeConverter->convertType(\n+      B.getType().cast<RankedTensorType>().getElementType());\n \n   Type ptrTy = ptr_ty(elemTy, 3);\n   SmallVector<Value> bPtrs(bNumPtr);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 34, "deletions": 0, "changes": 34, "file_content_changes": "@@ -102,6 +102,8 @@ warpsPerTileV3(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps,\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n   mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n+  mutable llvm::SmallVector<llvm::SetVector<Operation *>> dotOpSetVector;\n+  mutable llvm::SmallVector<unsigned> mmaV3InstrNs;\n \n   static bool bwdFilter(Operation *op) {\n     return op->getNumOperands() == 1 &&\n@@ -144,6 +146,36 @@ class BlockedToMMA : public mlir::RewritePattern {\n     }\n   }\n \n+  unsigned getMmaV3InstrN(tt::DotOp dotOp, unsigned currN) const {\n+    auto type = dotOp.getResult().getType().cast<RankedTensorType>();\n+    if (type.getEncoding().isa<MmaEncodingAttr>())\n+      return currN;\n+    for (size_t i = 0; i < dotOpSetVector.size(); ++i) {\n+      if (dotOpSetVector[i].count(dotOp.getOperation()) > 0)\n+        return mmaV3InstrNs[i];\n+    }\n+\n+    SetVector<Operation *> slices;\n+    mlir::getForwardSlice(dotOp.getResult(), &slices);\n+    mlir::getBackwardSlice(dotOp.getOperation(), &slices);\n+    unsigned N = currN;\n+    llvm::SetVector<Operation *> dotOpSet;\n+    for (Operation *iter : slices) {\n+      if (auto nextDotOp = dyn_cast<tt::DotOp>(iter)) {\n+        auto type = nextDotOp.getResult().getType().cast<RankedTensorType>();\n+        auto AType = nextDotOp.getOperand(0).getType().cast<RankedTensorType>();\n+        auto shapePerCTA = ttg::getShapePerCTA(type);\n+        auto instrShape = mmaVersionToInstrShape(3, shapePerCTA, AType);\n+        dotOpSet.insert(iter);\n+        if (instrShape[1] < N)\n+          N = instrShape[1];\n+      }\n+    }\n+    mmaV3InstrNs.push_back(N);\n+    dotOpSetVector.push_back(dotOpSet);\n+    return N;\n+  }\n+\n   static Value getMMAv3Operand(Value v, mlir::PatternRewriter &rewriter,\n                                int opIdx) {\n     auto cvtOp = dyn_cast_or_null<ttg::ConvertLayoutOp>(v.getDefiningOp());\n@@ -201,6 +233,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n     auto instrShape =\n         mmaVersionToInstrShape(versionMajor, retShapePerCTA, AType);\n+    if (versionMajor == 3)\n+      instrShape[1] = getMmaV3InstrN(dotOp, instrShape[1]);\n \n     // operands\n     Value a = dotOp.getA();"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 12, "deletions": 11, "changes": 23, "file_content_changes": "@@ -202,19 +202,20 @@ def format_of(ty):\n         return NULL;\n       }}\n \n-      if (launch_enter_hook != Py_None) {{\n-        PyObject_CallObject(launch_enter_hook, args);\n+      if (launch_enter_hook != Py_None && !PyObject_CallObject(launch_enter_hook, args)) {{\n+        return NULL;\n       }}\n \n       // raise exception asap\n       {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n       _launch(gridX, gridY, gridZ, num_warps, shared_memory, (hipStream_t)_stream, (hipFunction_t)_function{', ' + ', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\" for i, ty in signature.items()) if len(signature) > 0 else ''});\n-      if (launch_exit_hook != Py_None) {{\n-        PyObject_CallObject(launch_exit_hook, args);\n-      }}\n+\n       if (PyErr_Occurred()) {{\n         return NULL;\n       }}\n+      if (launch_exit_hook != Py_None && !PyObject_CallObject(launch_exit_hook, args)) {{\n+        return NULL;\n+      }}\n \n       // return None\n       Py_INCREF(Py_None);\n@@ -386,22 +387,22 @@ def format_of(ty):\n     return NULL;\n   }}\n \n-  if (launch_enter_hook != Py_None) {{\n-    PyObject_CallObject(launch_enter_hook, args);\n+  if (launch_enter_hook != Py_None && !PyObject_CallObject(launch_enter_hook, args)) {{\n+    return NULL;\n   }}\n \n \n   // raise exception asap\n   {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n   _launch(gridX, gridY, gridZ, num_warps, num_ctas, clusterDimX, clusterDimY, clusterDimZ, shared_memory, (CUstream)_stream, (CUfunction)_function{', ' + ', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items()) if len(signature) > 0 else ''});\n \n-  if (launch_exit_hook != Py_None) {{\n-    PyObject_CallObject(launch_exit_hook, args);\n+ if (PyErr_Occurred()) {{\n+    return NULL;\n   }}\n-\n-  if(PyErr_Occurred()) {{\n+  if (launch_exit_hook != Py_None && !PyObject_CallObject(launch_exit_hook, args)) {{\n     return NULL;\n   }}\n+\n   // return None\n   Py_INCREF(Py_None);\n   return Py_None;"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 12, "deletions": 3, "changes": 15, "file_content_changes": "@@ -17,9 +17,7 @@ static inline void gpuAssert(CUresult code, const char *file, int line) {\n \n #define CUDA_CHECK(ans)                                                        \\\n   {                                                                            \\\n-    gpuAssert((ans), __FILE__, __LINE__);                                      \\\n-    if (PyErr_Occurred())                                                      \\\n-      return NULL;                                                             \\\n+    { gpuAssert((ans), __FILE__, __LINE__); }                                  \\\n   }\n \n #define ADD_ENUM_ITEM(value)                                                   \\\n@@ -234,6 +232,8 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n   int32_t n_spills = 0;\n   // create driver handles\n   CUcontext pctx = 0;\n+\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuCtxGetCurrent(&pctx));\n   if (!pctx) {\n     CUDA_CHECK(cuDevicePrimaryCtxRetain(&pctx, device));\n@@ -264,6 +264,7 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n         cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n                            shared_optin - shared_static));\n   }\n+  Py_END_ALLOW_THREADS;\n \n   if (PyErr_Occurred()) {\n     return NULL;\n@@ -281,7 +282,9 @@ static PyObject *memAlloc(PyObject *self, PyObject *args) {\n     return NULL; // Error parsing arguments\n   }\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemAlloc(&dptr, bytesize));\n+  Py_END_ALLOW_THREADS;\n \n   return PyLong_FromUnsignedLongLong((unsigned long long)dptr);\n }\n@@ -300,7 +303,9 @@ static PyObject *memcpyHtoD(PyObject *self, PyObject *args) {\n   dstDevice = (CUdeviceptr)dstDevicePtr;\n   srcHost = (const void *)srcHostPtr;\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemcpyHtoD(dstDevice, srcHost, byteCount));\n+  Py_END_ALLOW_THREADS;\n \n   Py_RETURN_NONE;\n }\n@@ -312,7 +317,9 @@ static PyObject *memFree(PyObject *self, PyObject *args) {\n     return NULL; // Error parsing arguments\n   }\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemFree(dptr));\n+  Py_END_ALLOW_THREADS;\n \n   Py_RETURN_NONE;\n }\n@@ -400,10 +407,12 @@ static PyObject *tensorMapEncodeTiled(PyObject *self, PyObject *args) {\n     cuTensorMapEncodeTiledHandle = getCuTensorMapEncodeTiledHandle();\n   }\n   // Call the function\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuTensorMapEncodeTiledHandle(\n       tensorMap, tensorDataType, tensorRank, globalAddress, globalDim,\n       globalStrides, boxDim, elementStrides, interleave, swizzle, l2Promotion,\n       oobFill));\n+  Py_END_ALLOW_THREADS;\n \n   // Clean up\n   free(globalDim);"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 40, "deletions": 32, "changes": 72, "file_content_changes": "@@ -3,6 +3,7 @@\n import subprocess\n import sys\n from contextlib import contextmanager\n+from typing import Any, Dict, List\n \n from . import language as tl\n from ._C.libtriton.triton import runtime\n@@ -201,37 +202,41 @@ class Benchmark:\n \n     def __init__(\n         self,\n-        x_names,\n-        x_vals,\n-        line_arg,\n-        line_vals,\n-        line_names,\n-        plot_name,\n-        args,\n-        xlabel='',\n-        ylabel='',\n-        x_log=False,\n-        y_log=False,\n+        x_names: List[str],\n+        x_vals: List[Any],\n+        line_arg: str,\n+        line_vals: List[Any],\n+        line_names: List[str],\n+        plot_name: str,\n+        args: Dict[str, Any],\n+        xlabel: str = '',\n+        ylabel: str = '',\n+        x_log: bool = False,\n+        y_log: bool = False,\n         color=None,\n         styles=None,\n     ):\n         \"\"\"\n-        Constructor\n+        Constructor.\n+        x_vals can be a list of scalars or a list of tuples/lists. If x_vals is a list\n+        of scalars and there are multiple x_names, all arguments will have the same value.\n+        If x_vals is a list of tuples/lists, each element should have the same length as\n+        x_names.\n \n-        :param x_names: Name of the arguments that should appear on the x axis of the plot. If the list contains more than one element, all the arguments are assumed to have the same value.\n+        :param x_names: Name of the arguments that should appear on the x axis of the plot.\n         :type x_names: List[str]\n         :param x_vals: List of values to use for the arguments in :code:`x_names`.\n         :type x_vals: List[Any]\n         :param line_arg: Argument name for which different values correspond to different lines in the plot.\n         :type line_arg: str\n         :param line_vals: List of values to use for the arguments in :code:`line_arg`.\n-        :type line_vals: List[str]\n+        :type line_vals: List[Any]\n         :param line_names: Label names for the different lines.\n         :type line_names: List[str]\n         :param plot_name: Name of the plot.\n         :type plot_name: str\n-        :param args: List of arguments to remain fixed throughout the benchmark.\n-        :type args: List[str]\n+        :param args: Dictionary of keyword arguments to remain fixed throughout the benchmark.\n+        :type args: Dict[str, Any]\n         :param xlabel: Label for the x axis of the plot.\n         :type xlabel: str, optional\n         :param ylabel: Label for the y axis of the plot.\n@@ -261,23 +266,25 @@ def __init__(self, fn, benchmarks):\n         self.fn = fn\n         self.benchmarks = benchmarks\n \n-    def _run(self, bench, save_path, show_plots, print_data):\n+    def _run(self, bench: Benchmark, save_path: str, show_plots: bool, print_data: bool):\n         import os\n \n         import matplotlib.pyplot as plt\n         import pandas as pd\n         y_mean = bench.line_names\n         y_min = [f'{x}-min' for x in bench.line_names]\n         y_max = [f'{x}-max' for x in bench.line_names]\n-        x_names_str = str(bench.x_names)\n-        df = pd.DataFrame(columns=[x_names_str] + y_mean + y_min + y_max)\n+        x_names = list(bench.x_names)\n+        df = pd.DataFrame(columns=x_names + y_mean + y_min + y_max)\n         for x in bench.x_vals:\n-            if not isinstance(x, list):\n-                x = [x]\n-            if len(x) == 1:\n-                x = x * len(bench.x_names)\n-            x_str = str(x)\n-            x_args = {x_name: x_in for x_name, x_in in zip(bench.x_names, x)}\n+            # x can be a single value or a sequence of values.\n+            if not isinstance(x, (list, tuple)):\n+                x = [x for _ in x_names]\n+\n+            if len(x) != len(x_names):\n+                raise ValueError(f\"Expected {len(x_names)} values, got {x}\")\n+            x_args = dict(zip(x_names, x))\n+\n             row_mean, row_min, row_max = [], [], []\n             for y in bench.line_vals:\n                 ret = self.fn(**x_args, **{bench.line_arg: y}, **bench.args)\n@@ -288,23 +295,24 @@ def _run(self, bench, save_path, show_plots, print_data):\n                 row_mean += [y_mean]\n                 row_min += [y_min]\n                 row_max += [y_max]\n-            df.loc[len(df)] = [x_str] + row_mean + row_min + row_max\n+            df.loc[len(df)] = list(x) + row_mean + row_min + row_max\n+\n         if bench.plot_name:\n             plt.figure()\n             ax = plt.subplot()\n-            x = x_names_str\n+            # Plot first x value on x axis if there are multiple.\n+            first_x = x_names[0]\n             for i, y in enumerate(bench.line_names):\n                 y_min, y_max = df[y + '-min'], df[y + '-max']\n                 col = bench.styles[i][0] if bench.styles else None\n                 sty = bench.styles[i][1] if bench.styles else None\n-                ax.plot(df[x], df[y], label=y, color=col, ls=sty)\n+                ax.plot(df[first_x], df[y], label=y, color=col, ls=sty)\n                 if not y_min.isnull().all() and not y_max.isnull().all():\n                     y_min = y_min.astype(float)\n                     y_max = y_max.astype(float)\n-                    ax.fill_between(df[x], y_min, y_max, alpha=0.15, color=col)\n+                    ax.fill_between(df[first_x], y_min, y_max, alpha=0.15, color=col)\n             ax.legend()\n-            xlabel = bench.xlabel if bench.xlabel else \" = \".join(bench.x_names)\n-            ax.set_xlabel(xlabel)\n+            ax.set_xlabel(bench.xlabel or first_x)\n             ax.set_ylabel(bench.ylabel)\n             # ax.set_title(bench.plot_name)\n             ax.set_xscale(\"log\" if bench.x_log else \"linear\")\n@@ -313,7 +321,7 @@ def _run(self, bench, save_path, show_plots, print_data):\n                 plt.show()\n             if save_path:\n                 plt.savefig(os.path.join(save_path, f\"{bench.plot_name}.png\"))\n-        df = df[[x_names_str] + bench.line_names]\n+        df = df[x_names + bench.line_names]\n         if print_data:\n             print(bench.plot_name + ':')\n             print(df)"}, {"filename": "python/tutorials/09-experimental-tma-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -32,6 +32,11 @@\n import triton\n import triton.language as tl\n \n+if torch.cuda.get_device_capability()[0] < 9:\n+    import sys\n+    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n+    sys.exit(0)\n+\n \n @triton.autotune(\n     configs=[\n@@ -191,10 +196,5 @@ def perf(ms):\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n \n-if torch.cuda.get_device_capability()[0] < 9:\n-    import sys\n-    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n-    sys.exit(0)\n-\n test_matmul()\n benchmark.run(show_plots=False, print_data=True)"}, {"filename": "python/tutorials/10-experimental-tma-store-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -32,6 +32,11 @@\n import triton\n import triton.language as tl\n \n+if torch.cuda.get_device_capability()[0] < 9:\n+    import sys\n+    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n+    sys.exit(0)\n+\n \n @triton.autotune(\n     configs=[\n@@ -171,9 +176,4 @@ def perf(ms):\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n \n-if torch.cuda.get_device_capability()[0] < 9:\n-    import sys\n-    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n-    sys.exit(0)\n-\n benchmark.run(show_plots=False, print_data=True)"}]