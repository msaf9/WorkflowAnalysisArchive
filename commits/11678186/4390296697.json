[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 28, "deletions": 9, "changes": 37, "file_content_changes": "@@ -558,16 +558,35 @@ struct ConvertLayoutOpConversion\n           this->getTypeConverter()->convertType(srcTy.getElementType());\n       // for the destination type, we need to pack values together\n       // so they can be consumed by tensor core operations\n-      unsigned vecSize =\n-          std::max<unsigned>(32 / elemTy.getIntOrFloatBitWidth(), 1);\n-      Type vecTy = vec_ty(elemTy, vecSize);\n-      SmallVector<Type> types(elems / vecSize, vecTy);\n       SmallVector<Value> vecVals;\n-      for (unsigned i = 0; i < elems; i += vecSize) {\n-        Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-        for (unsigned j = 0; j < vecSize; j++)\n-          packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));\n-        vecVals.push_back(packed);\n+      SmallVector<Type> types;\n+      // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n+      // instructions to pack & unpack sub-word integers. A workaround is to\n+      // store the results of ldmatrix in i32\n+      auto elemSize = elemTy.getIntOrFloatBitWidth();\n+      if (auto intTy = elemTy.dyn_cast<IntegerType>() && elemSize <= 16) {\n+        auto fold = 32 / elemSize;\n+        for (unsigned i = 0; i < elems; i += fold) {\n+          Value val = i32_val(0);\n+          for (unsigned j = 0; j < fold; j++) {\n+            auto ext =\n+                shl(i32_ty, zext(i32_ty, vals[i + j]), i32_val(elemSize * j));\n+            val = or_(i32_ty, val, ext);\n+          }\n+          vecVals.push_back(val);\n+        }\n+        elems = elems / (32 / elemSize);\n+        types = SmallVector<Type>(elems, i32_ty);\n+      } else {\n+        unsigned vecSize = std::max<unsigned>(32 / elemSize, 1);\n+        Type vecTy = vec_ty(elemTy, vecSize);\n+        types = SmallVector<Type>(elems / vecSize, vecTy);\n+        for (unsigned i = 0; i < elems; i += vecSize) {\n+          Value packed = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n+          for (unsigned j = 0; j < vecSize; j++)\n+            packed = insert_element(vecTy, packed, vals[i + j], i32_val(j));\n+          vecVals.push_back(packed);\n+        }\n       }\n \n       // This needs to be ordered the same way that"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -259,7 +259,7 @@ struct FpToFpOpConversion\n                                  ConversionPatternRewriter &rewriter,\n                                  const Value &v) {\n     PTXBuilder builder;\n-    auto &cvt = *builder.create(\"cvt.rn.f32.bf16\");\n+    auto &cvt = *builder.create(\"cvt.f32.bf16\");\n     auto res = builder.newOperand(\"=r\");\n     auto operand = builder.newOperand(v, \"h\");\n     cvt(res, operand);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -27,6 +27,7 @@\n #define smin(...) rewriter.create<LLVM::SMinOp>(loc, __VA_ARGS__)\n #define umin(...) rewriter.create<LLVM::UMinOp>(loc, __VA_ARGS__)\n #define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n+#define shl(...) rewriter.create<LLVM::ShlOp>(loc, __VA_ARGS__)\n #define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n #define or_(...) rewriter.create<LLVM::OrOp>(loc, __VA_ARGS__)"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "file_content_changes": "@@ -93,26 +93,26 @@ bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   // same\n   if (isSingleValue(op->getOperand(0)))\n     return false;\n-  auto ptr = op->getOperand(0);\n-  // Case 2: We assume that `evict_last` loads/stores have high hit rate\n-  if (auto load = dyn_cast<triton::LoadOp>(op))\n-    if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-      return false;\n-  if (auto store = dyn_cast<triton::StoreOp>(op))\n-    if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-      return false;\n-  if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n-    auto encoding = tensorTy.getEncoding();\n-    // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n-    if (encoding.getTypeID() != targetEncoding.getTypeID())\n-      return true;\n-    auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n-    auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n-    auto order = triton::gpu::getOrder(encoding);\n-    auto targetOrder = triton::gpu::getOrder(targetEncoding);\n-    // Case 4: The targeEncoding may expose more vectorization opportunities\n-    return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n-  }\n+  // auto ptr = op->getOperand(0);\n+  //// Case 2: We assume that `evict_last` loads/stores have high hit rate\n+  // if (auto load = dyn_cast<triton::LoadOp>(op))\n+  //   if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+  //     return false;\n+  // if (auto store = dyn_cast<triton::StoreOp>(op))\n+  //   if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n+  //     return false;\n+  // if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n+  //   auto encoding = tensorTy.getEncoding();\n+  //   // Case 3: Different type conversion is expensive (e.g., mma <-> block)\n+  //   if (encoding.getTypeID() != targetEncoding.getTypeID())\n+  //     return true;\n+  //   auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n+  //   auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n+  //   auto order = triton::gpu::getOrder(encoding);\n+  //   auto targetOrder = triton::gpu::getOrder(targetEncoding);\n+  //   // Case 4: The targeEncoding may expose more vectorization opportunities\n+  //   return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n+  // }\n   return true;\n }\n "}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -5,7 +5,8 @@\n \n \n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n-def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+@pytest.mark.parametrize('dtype', [torch.float16, torch.bfloat16])\n+def test_op(Z, H, N_CTX, D_HEAD, dtype):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         pytest.skip(\"Flash attention only supported for compute capability < 80\")\n@@ -21,7 +22,7 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     for z in range(Z):\n         for h in range(H):\n             p[:, :, M == 0] = float(\"-inf\")\n-    p = torch.softmax(p.float(), dim=-1).half()\n+    p = torch.softmax(p.float(), dim=-1).to(dtype)\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n     ref_out.backward(dout)\n@@ -38,6 +39,7 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     tri_dq, q.grad = q.grad.clone(), None\n     # compare\n     triton.testing.assert_almost_equal(ref_out, tri_out)\n-    triton.testing.assert_almost_equal(ref_dv, tri_dv)\n+    decimal = 1 if dtype == torch.bfloat16 else 2\n+    triton.testing.assert_almost_equal(ref_dv, tri_dv, decimal=decimal)\n     triton.testing.assert_almost_equal(ref_dk, tri_dk)\n     triton.testing.assert_almost_equal(ref_dq, tri_dq)"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -63,7 +63,7 @@ def _fwd_kernel(\n         p *= l_rcp\n         acc *= (l_prev * l_rcp)[:, None]\n         # update acc\n-        p = p.to(tl.float16)\n+        p = p.to(Q.dtype.element_ty)\n         v = tl.load(v_ptrs)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n@@ -167,18 +167,18 @@ def _bwd_kernel(\n             p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n-            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n+            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n             # compute dp = dot(v, do)\n             Di = tl.load(D_ptrs + offs_m_curr)\n             dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n             dp += tl.dot(do, tl.trans(v))\n             # compute ds = p * (dp - delta[:, None])\n             ds = p * dp * sm_scale\n             # compute dk = dot(ds.T, q)\n-            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n             # compute dq\n             dq = tl.load(dq_ptrs)\n-            dq += tl.dot(ds.to(tl.float16), k)\n+            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n             tl.store(dq_ptrs, dq)\n             # increment pointers\n             dq_ptrs += BLOCK_M * stride_qm\n@@ -198,7 +198,7 @@ def forward(ctx, q, k, v, sm_scale):\n         # only support for Ampere now\n         capability = torch.cuda.get_device_capability()\n         if capability[0] < 8:\n-            raise RuntimeError(\"Flash attention currently only supported for compute capability < 80\")\n+            raise RuntimeError(\"Flash attention currently only supported for compute capability >= 80\")\n         BLOCK = 128\n         # shape constraints\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -15,7 +15,7 @@ def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by:\n         :param prune_configs_by: a dict of functions that are used to prune configs, fields:\n             'perf_model': performance model used to predicate running time with different configs, returns running time\n             'top_k': number of configs to bench\n-            'prune_num_stages_by'(optional): a function used to prune num_stages. It take configs:List[Config] as its input, and returns pruned configs.\n+            'prune_num_stages_by'(optional): a function used to prune num_stages. It takes configs:List[Config] as its input, and returns pruned configs.\n         '''\n         if not configs:\n             self.configs = [Config({}, num_warps=4, num_stages=2)]\n@@ -168,15 +168,15 @@ def kernel(x_ptr, x_size, **META):\n     :note: When all the configurations are evaluated, the kernel will run multiple time.\n            This means that whatever value the kernel updates will be updated multiple times.\n            To avoid this undesired behavior, you can use the `reset_to_zero` argument, which\n-           reset the value of the provided tensor to `zero` before running any configuration.\n+           resets the value of the provided tensor to `zero` before running any configuration.\n     :param configs: a list of :code:`triton.Config` objects\n     :type configs: list[triton.Config]\n     :param key: a list of argument names whose change in value will trigger the evaluation of all provided configs.\n     :type key: list[str]\n     :param prune_configs_by: a dict of functions that are used to prune configs, fields:\n         'perf_model': performance model used to predicate running time with different configs, returns running time\n         'top_k': number of configs to bench\n-        'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It take configs:List[Config] as its input, and returns pruned configs.\n+        'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It takes configs:List[Config] as its input, and returns pruned configs.\n     :param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs.\n     :type reset_to_zero: list[str]\n     \"\"\""}]