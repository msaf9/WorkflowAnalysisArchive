[{"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -1218,12 +1218,13 @@ def dot(lhs: tl.tensor,\n         out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n-    assert lhs.dtype == rhs.dtype, f\"lhs ({lhs.dtype}) and rhs ({rhs.dtype}) must have the same dtype!\"\n-    assert len(lhs.shape) == 2 and len(rhs.shape) == 2\n-    assert lhs.shape[1].value == rhs.shape[0].value\n+    assert lhs.dtype == rhs.dtype, f\"First input ({lhs.dtype}) and second input ({rhs.dtype}) must have the same dtype!\"\n+    assert len(lhs.shape) == 2, f\"First input shape ({lhs.shape}) is not two dimensional!\"\n+    assert len(rhs.shape) == 2, f\"Second input shape ({rhs.shape}) is not two dimensional!\"\n+    assert lhs.shape[1].value == rhs.shape[0].value, f\"First input shape ({lhs.shape}) and second input shape {rhs.shape} are not compatible for matmul (second index of first shape ({lhs.shape[1].value}) must be equal to first index of second shape ({rhs.shape[0].value})\"\n     assert lhs.shape[0].value >= 16 and lhs.shape[1].value >= 16 \\\n         and rhs.shape[1].value >= 16,\\\n-        \"small blocks not supported!\"\n+        f\"All values in both first input shape ({lhs.shape}) and second input shape ({rhs.shape}) must be >= 16!\"\n     if lhs.type.scalar.is_int():\n         assert lhs.type.scalar == tl.int8, \"only int8 supported!\"\n         # TODO: This is CUDA specific, check if ROCm has the same limitation"}]