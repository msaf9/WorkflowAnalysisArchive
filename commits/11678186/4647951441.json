[{"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 21, "deletions": 106, "changes": 127, "file_content_changes": "@@ -230,124 +230,39 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.insert_slice_async\n // CHECK: triton_gpu.async_commit_group\n-// CHECK: %[[LUT_PTR:.*]] = tt.addptr\n-// CHECK: %arg27 = %[[LUT_PTR]]\n-// CHECK: %[[LUT_BUFFER_0:.*]] = tt.load %arg27, {{.*}}\n+// CHECK: %[[LUT_BUFFER_0:.*]] = tt.load %arg15, {{.*}}\n // CHECK: %[[LUT_BUFFER_1:.*]] = arith.muli {{.*}}, %[[LUT_BUFFER_0]]\n // CHECK: %[[LUT_BUFFER_2:.*]] = tt.splat %[[LUT_BUFFER_1]]\n // CHECK: %[[NEXT_BUFFER_0:.*]] = tt.addptr {{.*}}, %[[LUT_BUFFER_2]]\n-// CHECK: %[[NEXT_BUFFER_1:.*]] = tt.addptr %arg26, {{.*}}\n+// CHECK: %[[NEXT_BUFFER_1:.*]] = tt.addptr %arg14, {{.*}}\n // CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_1]]\n // CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_0]]\n-#blocked = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#blocked1 = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n-#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n-func.func @lut_bmm(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}, %arg17: i32 {tt.divisibility = 16 : i32}) {\n-  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma>\n+// CHECK: triton_gpu.async_wait {num = 2 : i32}\n+func.func @lut_bmm(%77: i64 {tt.divisibility=16: i32},\n+                   %76: index,\n+                   %49: tensor<16x16x!tt.ptr<f16>, #AL> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %75: !tt.ptr<i64>,\n+                   %78: tensor<16x16xi32, #AL> {tt.constancy=16: i32, tt.divisibility=16: i32},\n+                   %60: tensor<16x16x!tt.ptr<f16>, #BL> {tt.divisibility=16: i32, tt.contiguity=16 : i32}) -> tensor<16x16xf32, #C>{\n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #C>\n   %c4_i32 = arith.constant 4 : i32\n   %c1 = arith.constant 1 : index\n   %c0 = arith.constant 0 : index\n   %c0_i64 = arith.constant 0 : i64\n   %c1_i32 = arith.constant 1 : i32\n-  %0 = tt.get_program_id {axis = 2 : i32} : i32\n-  %1 = tt.get_program_id {axis = 0 : i32} : i32\n-  %2 = tt.get_program_id {axis = 1 : i32} : i32\n-  %3 = tt.get_num_programs {axis = 0 : i32} : i32\n-  %4 = tt.get_num_programs {axis = 1 : i32} : i32\n-  %5 = arith.muli %1, %4 : i32\n-  %6 = arith.addi %5, %2 : i32\n-  %7 = arith.muli %4, %c4_i32 : i32\n-  %8 = arith.divsi %6, %7 : i32\n-  %9 = arith.muli %8, %c4_i32 : i32\n-  %10 = arith.subi %3, %9 : i32\n-  %11 = arith.cmpi slt, %10, %c4_i32 : i32\n-  %12 = arith.select %11, %10, %c4_i32 : i32\n-  %13 = arith.remsi %6, %12 : i32\n-  %14 = arith.addi %9, %13 : i32\n-  %15 = arith.remsi %6, %7 : i32\n-  %16 = arith.divsi %15, %12 : i32\n-  %17 = arith.muli %arg5, %0 : i32\n-  %18 = tt.addptr %arg4, %17 : !tt.ptr<i64>, i32\n-  %19 = tt.addptr %18, %14 : !tt.ptr<i64>, i32\n-  %20 = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n-  %21 = tt.addptr %19, %c1_i32 : !tt.ptr<i64>, i32\n-  %22 = tt.load %21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n-  %23 = arith.subi %22, %20 : i64\n-  %24 = arith.cmpi eq, %23, %c0_i64 : i64\n-  cf.cond_br %24, ^bb1, ^bb2\n-^bb1:  // pred: ^bb0\n-  return\n-^bb2:  // pred: ^bb0\n-  %25 = arith.muli %arg1, %0 : i32\n-  %26 = tt.addptr %arg0, %25 : !tt.ptr<f16>, i32\n-  %27 = arith.extsi %arg2 : i32 to i64\n-  %28 = arith.muli %27, %20 : i64\n-  %29 = tt.addptr %26, %28 : !tt.ptr<f16>, i64\n-  %30 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n-  %31 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n-  %32 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n-  %33 = tt.expand_dims %30 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n-  %34 = tt.expand_dims %31 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xi32, #blocked1>\n-  %35 = tt.expand_dims %32 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n-  %36 = tt.splat %arg3 : (i32) -> tensor<16x1xi32, #blocked>\n-  %37 = arith.muli %36, %33 : tensor<16x1xi32, #blocked>\n-  %38 = tt.splat %29 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n-  %39 = tt.addptr %38, %37 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n-  %40 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n-  %41 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n-  %42 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n-  %43 = tt.expand_dims %40 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n-  %44 = tt.expand_dims %41 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x16xi32, #blocked1>\n-  %45 = tt.expand_dims %42 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n-  %46 = tt.broadcast %39 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n-  %47 = tt.broadcast %43 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n-  %48 = tt.broadcast %45 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n-  %49 = tt.addptr %46, %47 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n-  %50 = arith.muli %arg9, %0 : i32\n-  %51 = tt.addptr %arg8, %50 : !tt.ptr<f16>, i32\n-  %52 = arith.muli %arg11, %16 : i32\n-  %53 = tt.addptr %51, %52 : !tt.ptr<f16>, i32\n-  %54 = tt.splat %53 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked1>\n-  %55 = tt.addptr %54, %34 : tensor<16x1x!tt.ptr<f16>, #blocked1>, tensor<16x1xi32, #blocked1>\n-  %56 = tt.splat %arg12 : (i32) -> tensor<1x16xi32, #blocked1>\n-  %57 = arith.muli %56, %44 : tensor<1x16xi32, #blocked1>\n-  %58 = tt.broadcast %55 : (tensor<16x1x!tt.ptr<f16>, #blocked1>) -> tensor<16x16x!tt.ptr<f16>, #blocked1>\n-  %59 = tt.broadcast %57 : (tensor<1x16xi32, #blocked1>) -> tensor<16x16xi32, #blocked1>\n-  %60 = tt.addptr %58, %59 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi32, #blocked1>\n-  %61 = arith.muli %arg14, %0 : i32\n-  %62 = tt.addptr %arg13, %61 : !tt.ptr<f16>, i32\n-  %63 = arith.muli %arg15, %14 : i32\n-  %64 = tt.addptr %62, %63 : !tt.ptr<f16>, i32\n-  %65 = arith.muli %arg16, %16 : i32\n-  %66 = tt.addptr %64, %65 : !tt.ptr<f16>, i32\n-  %67 = tt.splat %arg17 : (i32) -> tensor<16x1xi32, #blocked>\n-  %68 = arith.muli %67, %35 : tensor<16x1xi32, #blocked>\n-  %69 = tt.splat %66 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n-  %70 = tt.addptr %69, %68 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n-  %71 = tt.broadcast %70 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n-  %72 = tt.addptr %71, %48 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n-  %73 = arith.muli %arg7, %0 : i32\n-  %74 = tt.addptr %arg6, %73 : !tt.ptr<i64>, i32\n-  %75 = tt.addptr %74, %20 : !tt.ptr<i64>, i64\n-  %76 = arith.index_cast %23 : i64 to index\n-  %77 = arith.extsi %arg10 : i32 to i64\n-  %78 = tt.splat %arg2 : (i32) -> tensor<16x16xi32, #blocked>\n-  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>) {\n-    %82 = tt.load %arg20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked>\n+  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, !tt.ptr<i64>) {\n+    %82 = tt.load %arg20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n     %83 = tt.load %arg21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n     %84 = arith.muli %77, %83 : i64\n-    %85 = tt.splat %84 : (i64) -> tensor<16x16xi64, #blocked1>\n-    %86 = tt.addptr %60, %85 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi64, #blocked1>\n-    %87 = tt.load %86 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked1>\n-    %88 = triton_gpu.convert_layout %82 : (tensor<16x16xf16, #blocked>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>>\n-    %89 = triton_gpu.convert_layout %87 : (tensor<16x16xf16, #blocked1>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>>\n-    %90 = tt.dot %88, %89, %arg19 {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>> -> tensor<16x16xf32, #mma>\n-    %91 = tt.addptr %arg20, %78 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+    %85 = tt.splat %84 : (i64) -> tensor<16x16xi64, #BL>\n+    %86 = tt.addptr %60, %85 : tensor<16x16x!tt.ptr<f16>, #BL>, tensor<16x16xi64, #BL>\n+    %87 = tt.load %86 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BL>\n+    %88 = triton_gpu.convert_layout %82 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A>>\n+    %89 = triton_gpu.convert_layout %87 : (tensor<16x16xf16, #BL>) -> tensor<16x16xf16, #B>>\n+    %90 = tt.dot %88, %89, %arg19 {allowTF32 = true} : tensor<16x16xf16, #A>> * tensor<16x16xf16, #B>> -> tensor<16x16xf32, #C>\n+    %91 = tt.addptr %arg20, %78 : tensor<16x16x!tt.ptr<f16>, #AL>, tensor<16x16xi32, #AL>\n     %92 = tt.addptr %arg21, %c1_i32 : !tt.ptr<i64>, i32\n-    scf.yield %90, %91, %92 : tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>\n+    scf.yield %90, %91, %92 : tensor<16x16xf32, #C>, tensor<16x16x!tt.ptr<f16>, #AL>, !tt.ptr<i64>\n   }\n-  %80 = arith.truncf %79#0 : tensor<16x16xf32, #mma> to tensor<16x16xf16, #mma>\n-  %81 = triton_gpu.convert_layout %80 : (tensor<16x16xf16, #mma>) -> tensor<16x16xf16, #blocked>\n-  tt.store %72, %81 {cache = 1 : i32, evict = 1 : i32} : tensor<16x16xf16, #blocked>\n-  return\n+  return %79#0 : tensor<16x16xf32, #C>\n }"}]