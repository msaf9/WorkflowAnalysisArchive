[{"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "file_content_changes": "@@ -3119,25 +3119,21 @@ void generator::visit_layout_mma(analysis::mma_layout* layout) {\n }\n \n void generator::visit_layout_scanline(analysis::scanline_layout* layout) {\n-  Value *warp_size = i32(32);\n-  Value* u_thread_id_0 = tgt_->get_local_id(mod_, *builder_, 0);\n-  Value *u_thread_id = urem(u_thread_id_0, warp_size);\n-  Value *u_warp_id = udiv(u_thread_id_0, warp_size);\n-\n+  Value* u_thread_id = tgt_->get_local_id(mod_, *builder_, 0);\n   auto order = layout->get_order();\n   const auto& shape = layout->get_shape();\n-  Value* full_thread_id = add(mul(u_warp_id, i32(32)), u_thread_id);\n   // Delinearize\n   size_t dim = shape.size();\n   std::vector<Value*> thread_id(dim);\n   for(unsigned k = 0; k < dim - 1; k++){\n     Constant *dim_k = i32(layout->mts(order[k]));\n-    Value *rem = urem(full_thread_id, dim_k);\n-    full_thread_id = udiv(full_thread_id, dim_k);\n+    Value *rem = urem(u_thread_id, dim_k);\n+    u_thread_id = udiv(u_thread_id, dim_k);\n     thread_id[order[k]] = rem;\n   }\n   Constant *dim_k = i32(layout->mts(order[dim - 1]));\n-  thread_id[order[dim - 1]] = urem(full_thread_id, dim_k);\n+  thread_id[order[dim - 1]] = urem(u_thread_id, dim_k);\n+\n   // Create axes\n   for(unsigned k = 0; k < dim; k++) {\n     int nts = layout->nts(k);"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -557,6 +557,7 @@ def serialized_add(data, Lock):\n     ('float32', 'bfloat16', False),\n     ('bfloat16', 'float32', False),\n     ('float32', 'int32', True),\n+    ('float32', 'int1', False),\n ] + [\n     (f'uint{x}', f'int{x}', True) for x in [8, 16, 32, 64]\n ] + [\n@@ -565,6 +566,8 @@ def serialized_add(data, Lock):\n def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     x0 = 43 if dtype_x in int_dtypes else 43.5\n+    if dtype_x in float_dtypes and dtype_z == 'int1':\n+        x0 = 0.5\n     if dtype_x.startswith('bfloat'):\n         x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n     else:\n@@ -578,11 +581,12 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n         z = x.to(Z.dtype.element_ty, bitcast=BITCAST)\n         tl.store(Z, z)\n \n+    dtype_z_np = dtype_z if dtype_z != 'int1' else 'bool_'\n     # triton result\n     if dtype_z.startswith('bfloat'):\n         z_tri = torch.empty((1,), dtype=getattr(torch, dtype_z), device=device)\n     else:\n-        z_tri = to_triton(np.empty((1, ), dtype=getattr(np, dtype_z)), device=device)\n+        z_tri = to_triton(np.empty((1, ), dtype=getattr(np, dtype_z_np)), device=device)\n     kernel[(1, )](x_tri, z_tri, BITCAST=bitcast)\n     # torch result\n     if dtype_z.startswith('bfloat') or dtype_x.startswith('bfloat'):\n@@ -591,9 +595,9 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n         assert z_tri == z_ref\n     else:\n         if bitcast:\n-            z_ref = x.view(getattr(np, dtype_z))\n+            z_ref = x.view(getattr(np, dtype_z_np))\n         else:\n-            z_ref = x.astype(getattr(np, dtype_z))\n+            z_ref = x.astype(getattr(np, dtype_z_np))\n         assert to_numpy(z_tri) == z_ref\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "file_content_changes": "@@ -565,7 +565,6 @@ def cast(input: tl.tensor,\n         return input\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n-\n     # bf16 <=> (not fp32)\n     if (src_sca_ty.is_bf16() and not dst_sca_ty.is_fp32()) or \\\n        (dst_sca_ty.is_bf16() and not src_sca_ty.is_fp32()):\n@@ -601,9 +600,7 @@ def cast(input: tl.tensor,\n     if src_sca_ty.is_floating() and dst_sca_ty.is_int():\n         # TODO: is this correct?\n         if dst_sca_ty.is_bool():\n-            return tl.tensor(builder.create_fp_to_ui(input.handle,\n-                                                     dst_ty.to_ir(builder)),\n-                             dst_ty)\n+            return not_equal(input, tl._to_tensor(0, builder), builder)\n         else:\n             return tl.tensor(builder.create_fp_to_si(input.handle,\n                                                      dst_ty.to_ir(builder)),\n@@ -735,8 +732,8 @@ def store(ptr: tl.tensor,\n     elt_ty = ptr_ty.element_ty\n     # treat bool* as tl.int8*\n     if elt_ty == tl.int1:\n-        elt_ty = tl.int8\n-        ptr_ty = tl.pointer_type(elt_ty, ptr_ty.address_space)\n+        elt_ty_ptr = tl.int8\n+        ptr_ty = tl.pointer_type(elt_ty_ptr, ptr_ty.address_space)\n         ptr = cast(ptr, ptr_ty, builder)\n \n     # cast to target data-type"}]