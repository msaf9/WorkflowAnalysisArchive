[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 16, "deletions": 2, "changes": 18, "file_content_changes": "@@ -31,17 +31,31 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n SmallVector<unsigned> getSizePerThread(Attribute layout);\n \n+// Returns the number of contiguous elements that each thread\n+// has access to, on each dimension of the tensor. E.g.\n+// for a blocked layout with sizePerThread = [1, 4], returns [1, 4],\n+// regardless of the shape of the tensor.\n SmallVector<unsigned> getContigPerThread(Attribute layout);\n \n+// Returns the number of unique (distinct) contiguous elements that each thread\n+// has access to, on each dimension of the tensor. E.g. for a blocked layout\n+// with sizePerThread = [1, 4] and tensor shape = [128, 1], returns [1, 1].\n+SmallVector<unsigned> getUniqueContigPerThread(Type type);\n+\n+// Returns the number of threads per warp that have access to unique (distinct)\n+// elements of the tensor. E.g. for a blocked layout with threadsPerWarp = [2,\n+// 16] and tensor shape = [2, 2], returns [2, 1].\n SmallVector<unsigned>\n getThreadsPerWarpWithUniqueData(Attribute layout,\n                                 ArrayRef<int64_t> tensorShape);\n \n+// Returns the number of warps per CTA that have access to unique (distinct)\n+// elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,\n+// 1], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4] and tensor shape = [2, 2],\n+// returns [1, 1].\n SmallVector<unsigned>\n getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape);\n \n-SmallVector<unsigned> getUniqueContigPerThread(Type type);\n-\n SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n \n SmallVector<unsigned>"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -253,7 +253,7 @@ SmallVector<unsigned> getThreadsPerCTA(Attribute layout) {\n       threads.push_back(blockedLayout.getThreadsPerWarp()[d] *\n                         blockedLayout.getWarpsPerCTA()[d]);\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    if (mmaLayout.getVersionMajor() == 2) {\n+    if (mmaLayout.isAmpere()) {\n       threads = {8 * mmaLayout.getWarpsPerCTA()[0],\n                  4 * mmaLayout.getWarpsPerCTA()[1]};\n     } else"}]