[{"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -171,8 +171,10 @@ LogicalResult LoopPipeliner::initialize() {\n \n   // can we use forOp.walk(...) here?\n   SmallVector<triton::LoadOp, 2> allLoads;\n+  SmallVector<triton::LoadOp, 2> validLoads;\n   for (Operation &op : *loop)\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n+      allLoads.push_back(loadOp);\n       auto ptr = loadOp.getPtr();\n       unsigned vec = axisInfoAnalysis->getPtrContiguity(ptr);\n       if (auto mask = loadOp.getMask())\n@@ -184,17 +186,18 @@ LogicalResult LoopPipeliner::initialize() {\n                     .cast<triton::PointerType>()\n                     .getPointeeType();\n       unsigned width = vec * ty.getIntOrFloatBitWidth();\n+      // cp.async's cp-size can only be 4, 8 and 16.\n       if (width >= 32)\n-        allLoads.push_back(loadOp);\n+        validLoads.push_back(loadOp);\n     }\n \n   // Early stop: no need to continue if there is no load in the loop.\n-  if (allLoads.empty())\n+  if (validLoads.empty())\n     return failure();\n \n   // load => values that it depends on\n   DenseMap<Value, SetVector<Value>> loadDeps;\n-  for (triton::LoadOp loadOp : allLoads) {\n+  for (triton::LoadOp loadOp : validLoads) {\n     SetVector<Value> deps;\n     for (Value op : loadOp->getOperands())\n       collectDeps(op, numStages - 1, deps);\n@@ -205,7 +208,7 @@ LogicalResult LoopPipeliner::initialize() {\n   // (Because if a load depends on another load, this load needs to wait on the\n   //  other load in the prologue, which is against the point of the pipeline\n   //  pass)\n-  for (triton::LoadOp loadOp : allLoads) {\n+  for (triton::LoadOp loadOp : validLoads) {\n     bool isCandidate = true;\n     for (triton::LoadOp other : allLoads) {\n       if (loadDeps[loadOp].contains(other)) {"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 47, "deletions": 1, "changes": 48, "file_content_changes": "@@ -222,4 +222,50 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return %loop#1 : tensor<128x128xf32, #C>\n-}\n\\ No newline at end of file\n+}\n+\n+// If we have a load that depends on a previous load, we cannot pipeline.\n+// CHECK: func.func @matmul_loop_no_pipeline\n+// CHECK-NOT: triton_gpu.alloc_tensor\n+// CHECK-NOT: triton_gpu.insert_slice_async\n+func.func @matmul_loop_no_pipeline(%lb : index, %ub : index, %step : index,\n+                                   %A : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n+                                   %B : !tt.ptr<f16> {tt.divisibility = 16 : i32},\n+                                   %I : !tt.ptr<i32> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C> {\n+  // A ptrs\n+  %a_ptr_splat = tt.splat %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %a_tmp0 = tt.make_range {end = 32: i32, start = 0: i32} : tensor<32xi32, #ALs0>\n+  %a_tmp1 = tt.expand_dims %a_tmp0 {axis = 0 : i32} : (tensor<32xi32, #ALs0>) -> tensor<1x32xi32, #AL>\n+  %a_offs = tt.broadcast %a_tmp1 : (tensor<1x32xi32, #AL>) -> tensor<128x32xi32, #AL>\n+  %a_ptr_init = tt.addptr %a_ptr_splat, %a_offs : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+  // B ptrs\n+  %b_ptr_splat = tt.splat %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+  %b_tmp0 = tt.make_range {end = 128: i32, start = 0: i32} : tensor<128xi32, #BLs0>\n+  %b_tmp1 = tt.expand_dims %b_tmp0 {axis = 0 : i32} : (tensor<128xi32, #BLs0>) -> tensor<1x128xi32, #BL>\n+  %b_offs = tt.broadcast %b_tmp1 : (tensor<1x128xi32, #BL>) -> tensor<32x128xi32, #BL>\n+  %b_ptr_init = tt.addptr %b_ptr_splat, %b_offs : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n+\n+  %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n+  %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>\n+\n+  %a_ = tt.load %a_ptr_init, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+  %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+\n+  %b_mask = arith.constant dense<true> : tensor<32x128xi1, #BL>\n+  %b_other = arith.constant dense<0.00e+00> : tensor<32x128xf16, #BL>\n+  %c_init = arith.constant dense<0.00e+00> : tensor<128x128xf32, #C>\n+\n+  %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n+\n+  %loop:2 = scf.for %iv = %lb to %ub step %step iter_args(%b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n+    %i = tt.load %I {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i32\n+    %i_splat = tt.splat %i : (i32) -> tensor<32x128xi32, #BL>\n+    %idx = tt.addptr %b_ptr, %i_splat : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n+    %b_ = tt.load %idx, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n+    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n+    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n+    scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n+  }\n+  return %loop#1 : tensor<128x128xf32, #C>\n+}"}]