[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 6, "deletions": 22, "changes": 28, "file_content_changes": "@@ -36,9 +36,7 @@ def numpy_random(shape, dtype_str, rs: Optional[RandomState] = None, low=None, h\n         shape = (shape, )\n     if rs is None:\n         rs = RandomState(seed=17)\n-    if 'float8' in dtype_str or \\\n-            dtype_str in int_dtypes + uint_dtypes:\n-        dtype_str = 'int8' if 'float8' in dtype_str else dtype_str\n+    if dtype_str in int_dtypes + uint_dtypes:\n         iinfo = np.iinfo(getattr(np, dtype_str))\n         low = iinfo.min if low is None else max(low, iinfo.min)\n         high = iinfo.max if high is None else min(high, iinfo.max)\n@@ -1945,23 +1943,6 @@ def kernel(X, stride_xm, stride_xn,\n # ---------------\n \n \n-def fp8e5_to_fp16(x):\n-\n-    @triton.jit\n-    def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n-        pid = tl.program_id(0)\n-        offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-        mask = offs < N\n-        x = tl.load(X + offs, mask=mask)\n-        y = x.to(tl.float8e5)\n-        tl.store(Y + offs, y, mask=mask)\n-\n-    ret = torch.empty(x.shape, dtype=torch.float16, device=x.device)\n-    grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n-    kernel[grid](ret, triton.reinterpret(x, tl.float8e5), ret.numel(), BLOCK_SIZE=1024)\n-    return ret\n-\n-\n @pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype\",\n                          [(*shape, 4, False, False, epilogue, allow_tf32, in_dtype, out_dtype)\n                           for shape in [(64, 64, 64), (16, 16, 16)]\n@@ -2062,7 +2043,7 @@ def kernel(X, stride_xm, stride_xk,\n     else:\n         y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)\n     w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)\n-    if 'int' not in str(x.dtype):\n+    if 'int' not in in_dtype:\n         x *= .1\n         y *= .1\n     if in_dtype == 'float32' and allow_tf32:\n@@ -2084,7 +2065,10 @@ def kernel(X, stride_xm, stride_xk,\n \n     if out_dtype == 'int8':\n         out_dtype = tl.int8\n-    elif out_dtype == 'float16':\n+    elif out_dtype == 'float16' and epilogue != 'softmax':\n+        # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will\n+        # fail with the following error: 'llvm.fmul' op requires the same type\n+        # for all operands and results\n         out_dtype = tl.float16\n     else:\n         out_dtype = tl.float32"}]