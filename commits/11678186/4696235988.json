[{"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -5,9 +5,13 @@\n In this tutorial, you will write a simple vector addition using Triton.\n \n In doing so, you will learn about:\n-- The basic programming model of Triton.\n-- The `triton.jit` decorator, which is used to define Triton kernels.\n-- The best practices for validating and benchmarking your custom ops against native reference implementations.\n+\n+* The basic programming model of Triton.\n+\n+* The `triton.jit` decorator, which is used to define Triton kernels.\n+\n+* The best practices for validating and benchmarking your custom ops against native reference implementations.\n+\n \"\"\"\n \n # %%"}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -7,8 +7,11 @@\n the GPU's SRAM.\n \n In doing so, you will learn about:\n-- The benefits of kernel fusion for bandwidth-bound operations.\n-- Reduction operators in Triton.\n+\n+* The benefits of kernel fusion for bandwidth-bound operations.\n+\n+* Reduction operators in Triton.\n+\n \"\"\"\n \n # %%"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -5,10 +5,15 @@\n performance on parallel with cuBLAS.\n \n You will specifically learn about:\n-- Block-level matrix multiplications\n-- Multi-dimensional pointer arithmetic\n-- Program re-ordering for improved L2 cache hit rate\n-- Automatic performance tuning\n+\n+* Block-level matrix multiplications.\n+\n+* Multi-dimensional pointer arithmetics.\n+\n+* Program re-ordering for improved L2 cache hit rate.\n+\n+* Automatic performance tuning.\n+\n \"\"\"\n \n # %%"}, {"filename": "python/tutorials/04-low-memory-dropout.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -7,8 +7,11 @@\n whose state is generally composed of a bit mask tensor of the same shape as the input.\n \n In doing so, you will learn about:\n-- The limitations of naive implementations of Dropout with PyTorch\n-- Parallel pseudo-random number generation in Triton\n+\n+* The limitations of naive implementations of Dropout with PyTorch.\n+\n+* Parallel pseudo-random number generation in Triton.\n+\n \"\"\"\n \n # %%"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -5,8 +5,11 @@\n kernel that runs faster than the PyTorch implementation.\n \n In doing so, you will learn about:\n-- Implementing backward pass in Triton\n-- Implementing parallel reduction in Triton\n+\n+* Implementing backward pass in Triton.\n+\n+* Implementing parallel reduction in Triton.\n+\n \"\"\"\n \n # %%"}]