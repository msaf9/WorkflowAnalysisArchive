[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -31,7 +31,7 @@ SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout);\n \n SmallVector<unsigned> getSizePerThread(const Attribute &layout);\n \n-SmallVector<unsigned> getContigPerThread(Attribute layout);\n+SmallVector<unsigned> getContigPerThread(const Attribute &layout);\n \n SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n "}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -748,9 +748,19 @@ unsigned AxisInfoAnalysis::getPtrVectorSize(Value ptr) {\n   auto order = triton::gpu::getOrder(layout);\n   unsigned align = getPtrAlignment(ptr);\n \n+  unsigned numElemBits = 0;\n+  if (auto ptrTy = tensorTy.getElementType().cast<triton::PointerType>()) {\n+    auto pointeeType = ptrTy.getPointeeType();\n+    numElemBits = pointeeType.isa<triton::Float8Type>()\n+                      ? 8\n+                      : pointeeType.getIntOrFloatBitWidth();\n+  } else {\n+    numElemBits = tensorTy.getElementType().getIntOrFloatBitWidth();\n+  }\n   unsigned contigPerThread = triton::gpu::getSizePerThread(layout)[order[0]];\n   unsigned vec = std::min(align, contigPerThread);\n   vec = std::min<unsigned>(shape[order[0]], vec);\n+  vec = std::min<unsigned>(128 / numElemBits, vec);\n \n   return vec;\n }"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -143,7 +143,7 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n   }\n }\n \n-SmallVector<unsigned> getContigPerThread(Attribute layout) {\n+SmallVector<unsigned> getContigPerThread(const Attribute &layout) {\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.isVolta() || mmaLayout.isAmpere());\n     return {1, 2};"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 24, "deletions": 5, "changes": 29, "file_content_changes": "@@ -295,11 +295,30 @@ LogicalResult getForwardEncoding(Attribute sourceEncoding, Operation *op,\n   return failure();\n }\n \n-inline bool expensiveToRemat(Operation *op) {\n+inline bool expensiveLoadOrStore(Operation *op,\n+                                 const Attribute &targetEncoding) {\n+  // Case 1: A size 1 tensor is not expensive since all threads will load the\n+  // same\n+  if (isSingleValue(op->getOperand(0)))\n+    return false;\n+  // Case 2: the targeEncoding may expose more vectorization opportunities\n+  auto ptr = op->getOperand(0);\n+  if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n+    auto encoding = tensorTy.getEncoding();\n+    auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n+    auto targetSizePerThread = triton::gpu::getSizePerThread(targetEncoding);\n+    auto order = triton::gpu::getOrder(encoding);\n+    auto targetOrder = triton::gpu::getOrder(targetEncoding);\n+    return sizePerThread[order[0]] >= targetSizePerThread[targetOrder[0]];\n+  }\n+  return false;\n+}\n+\n+inline bool expensiveToRemat(Operation *op, const Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n-    return !isSingleValue(op->getOperand(0));\n+    return expensiveLoadOrStore(op, targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n@@ -326,7 +345,7 @@ LogicalResult simulateBackwardRematerialization(\n     queue.pop_back();\n     // If the current operation is expensive to rematerialize,\n     // we stop everything\n-    if (expensiveToRemat(currOp))\n+    if (expensiveToRemat(currOp, currLayout))\n       return mlir::failure();\n     // we would propagate the conversion here\n     numCvts -= 1;\n@@ -496,7 +515,7 @@ class FoldConvertAndReduce : public mlir::RewritePattern {\n     llvm::MapVector<Value, Attribute> toConvert;\n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensiveToRemat(op))\n+      if (expensiveToRemat(op, srcEncoding))\n         return failure();\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::Elementwise>())\n@@ -592,7 +611,7 @@ class RematerializeBackward : public mlir::RewritePattern {\n       queue.pop_back();\n       // If the current operation is expensive to rematerialize,\n       // we stop everything\n-      if (expensiveToRemat(currOp))\n+      if (expensiveToRemat(currOp, currLayout))\n         break;\n       // a conversion will be removed here (i.e. transferred to operands)\n       numCvts -= 1;"}]