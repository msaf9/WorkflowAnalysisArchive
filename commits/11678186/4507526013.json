[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1543,15 +1543,15 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n         in2_offsets = K_offsets[:, None] * in2_stride + N_offsets[None, :]\n \n         # Load inputs.\n-        x = tl.load(in1_ptr + in_offsets, mask=in_offsets < M * K)\n-        w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < K * N)\n+        x = tl.load(in1_ptr + in_offsets, mask=in_offsets < M*K)\n+        w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < K*N)\n \n         # Without a dot product the memory doesn't get promoted to shared.\n         o = tl.dot(x, w, out_dtype=tl.float32)\n \n         # Store output\n         output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]\n-        tl.store(output_ptr + output_offsets, o, mask=output_offsets < M * N)\n+        tl.store(output_ptr + output_offsets, o, mask=output_offsets < M*N)\n \n     pgm = _kernel[(1,)](in1, in2, out,\n                         in1.stride()[0],\n@@ -1563,7 +1563,7 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n                         M=M, N=N, K=K)\n \n     reference_out = torch.matmul(in1, in2)\n-    torch.testing.assert_allclose(out, reference_out)\n+    torch.testing.assert_allclose(out, reference_out, atol=1e-2, rtol=0)\n \n \n @pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])"}]