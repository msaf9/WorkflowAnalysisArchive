[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -94,7 +94,7 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n+    ms = triton.testing.do_bench(fn, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n@@ -150,7 +150,7 @@ def test_elementwise(N):\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n+    ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     torch.testing.assert_allclose(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n@@ -189,7 +189,7 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n         o = fn()\n         do = torch.randn_like(o)\n         fn = lambda: o.backward(do, retain_graph=True)\n-    ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n+    ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n     total_flops = 2 * flops_per_matmul"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -18,7 +18,7 @@ def nvsmi(attrs):\n \n \n def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n-             percentiles=(0.5, 0.2, 0.8),\n+             quantiles=(0.5, 0.2, 0.8),\n              fast_flush=False,\n              return_mode=\"min\"):\n     assert return_mode in [\"min\", \"max\", \"mean\", \"median\"]\n@@ -35,8 +35,8 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n     :type rep: int\n     :param grad_to_none: Reset the gradient of the provided tensor to None\n     :type grad_to_none: torch.tensor, optional\n-    :param percentiles: Performance percentile to return in addition to the median.\n-    :type percentiles: list[float]\n+    :param quantiles: Performance percentile to return in addition to the median.\n+    :type quantiles: list[float]\n     :param fast_flush: Use faster kernel to flush L2 between measurements\n     :type fast_flush: bool\n     \"\"\"\n@@ -84,9 +84,8 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n     # Record clocks\n     torch.cuda.synchronize()\n     times = torch.tensor([s.elapsed_time(e) for s, e in zip(start_event, end_event)])\n-    if percentiles is not None:\n-        percentiles = torch.quantile(times, torch.tensor(percentiles)).tolist()\n-        return tuple(percentiles)\n+    if quantiles is not None:\n+        return torch.quantile(times, torch.tensor(quantiles)).tolist()\n     return getattr(torch, return_mode)(times).item()\n \n "}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -120,10 +120,11 @@ def add(x: torch.Tensor, y: torch.Tensor):\n def benchmark(size, provider):\n     x = torch.rand(size, device='cuda', dtype=torch.float32)\n     y = torch.rand(size, device='cuda', dtype=torch.float32)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'torch':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\n     gbps = lambda ms: 12 * size / ms * 1e-6\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n "}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -177,12 +177,13 @@ def softmax(x):\n )\n def benchmark(M, N, provider):\n     x = torch.randn(M, N, device='cuda', dtype=torch.float32)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'torch-native':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1), quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: softmax(x), quantiles=quantiles)\n     if provider == 'torch-jit':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x))\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_softmax(x), quantiles=quantiles)\n     gbps = lambda ms: 2 * x.nelement() * x.element_size() * 1e-9 / (ms * 1e-3)\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n "}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -329,10 +329,11 @@ def matmul(a, b, activation=None):\n def benchmark(M, N, K, provider):\n     a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n+    quantiles = [0.5, 0.2, 0.8]\n     if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=100)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=100)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)\n "}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -339,6 +339,7 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n     x = -2.3 + 0.5 * torch.randn(x_shape, dtype=dtype, device='cuda')\n     dy = .1 * torch.randn_like(x)\n     x.requires_grad_(True)\n+    quantiles = [0.5, 0.2, 0.8]\n     # utility functions\n     if provider == 'triton':\n         y_fwd = lambda: layer_norm(x, w_shape, weight, bias, eps)\n@@ -350,13 +351,13 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n     # forward pass\n     if mode == 'forward':\n         gbps = lambda ms: 2 * x.numel() * x.element_size() / ms * 1e-6\n-        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, rep=500)\n+        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles, rep=500)\n     # backward pass\n     if mode == 'backward':\n         gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\n         y = y_fwd()\n         ms, min_ms, max_ms = triton.testing.do_bench(lambda: y.backward(dy, retain_graph=True),\n-                                                     grad_to_none=[x], rep=500)\n+                                                     quantiles=quantiles, grad_to_none=[x], rep=500)\n     return gbps(ms), gbps(max_ms), gbps(min_ms)\n \n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -341,7 +341,7 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n-        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n         return ms\n     if provider == \"flash\":\n         lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n@@ -353,7 +353,7 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n-        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n         return ms\n \n "}]