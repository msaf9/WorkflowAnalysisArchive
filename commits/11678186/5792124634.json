[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 19, "deletions": 3, "changes": 22, "file_content_changes": "@@ -27,7 +27,7 @@ jobs:\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n             echo '::set-output name=matrix-required::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"H100\"]]'\n-            echo '::set-output name=matrix-optional::[[\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n+            echo '::set-output name=matrix-optional::[]'\n           else\n             echo '::set-output name=matrix-required::[\"ubuntu-latest\"]'\n             echo '::set-output name=matrix-optional::[\"ubuntu-latest\"]'\n@@ -50,6 +50,8 @@ jobs:\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n         run: |\n           echo \"BACKEND=CUDA\" >> \"${GITHUB_ENV}\"\n+          echo \"ENABLE_TMA=0\" >> \"${GITHUB_ENV}\"\n+          echo \"ENABLE_MMA_V3=0\" >> \"${GITHUB_ENV}\"\n \n       - name: Clear cache\n         run: |\n@@ -79,8 +81,22 @@ jobs:\n           fi\n           lit -v \"${LIT_TEST_DIR}\"\n \n-      - name: Run python tests on CUDA\n-        if: ${{ env.BACKEND == 'CUDA'}}\n+      - name: Enable MMAV3 and TMA\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'H100')}}\n+        run: |\n+          echo \"ENABLE_TMA=1\" >> \"${GITHUB_ENV}\"\n+          echo \"ENABLE_MMA_V3=1\" >> \"${GITHUB_ENV}\"\n+\n+      - name: Run python tests on CUDA with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n+        if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '1' && env.ENABLE_MMA_V3 == '1'}}\n+        run: |\n+          cd python/test/unit\n+          python3 -m pytest -n 8 --ignore=runtime\n+          # run runtime tests serially to avoid race condition with cache handling.\n+          python3 -m pytest runtime/\n+\n+      - name: Run python tests on CUDA with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n+        if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n         run: |\n           cd python/test/unit\n           python3 -m pytest -n 8 --ignore=runtime"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -209,6 +209,7 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     TritonAnalysis\n     TritonTransforms\n     TritonGPUTransforms\n+    TritonNvidiaGPUTransforms\n     TritonLLVMIR\n     TritonPTX\n     TritonHSACO"}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -9,6 +9,7 @@ target_link_libraries(triton-opt PRIVATE\n   TritonAnalysis\n   TritonTransforms\n   TritonGPUTransforms\n+  TritonNvidiaGPUTransforms\n   ${dialect_libs}\n   ${conversion_libs}\n   # tests\n@@ -29,6 +30,7 @@ target_link_libraries(triton-reduce PRIVATE\n   TritonAnalysis\n   TritonTransforms\n   TritonGPUTransforms\n+  TritonNvidiaGPUTransforms\n   ${dialect_libs}\n   ${conversion_libs}\n   # tests\n@@ -48,6 +50,7 @@ llvm_update_compile_flags(triton-translate)\n          TritonAnalysis\n          TritonTransforms\n          TritonGPUTransforms\n+         TritonNvidiaGPUTransforms\n          TritonLLVMIR\n          TritonPTX\n          TritonHSACO"}, {"filename": "bin/RegisterTritonDialects.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1,9 +1,11 @@\n #pragma once\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n \n #include \"triton/Dialect/Triton/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n \n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n #include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n@@ -23,6 +25,7 @@ inline void registerTritonDialects(mlir::DialectRegistry &registry) {\n   mlir::registerAllPasses();\n   mlir::registerTritonPasses();\n   mlir::registerTritonGPUPasses();\n+  mlir::registerTritonNvidiaGPUPasses();\n   mlir::test::registerTestAliasPass();\n   mlir::test::registerTestAlignmentPass();\n   mlir::test::registerTestAllocationPass();\n@@ -32,6 +35,7 @@ inline void registerTritonDialects(mlir::DialectRegistry &registry) {\n \n   // TODO: register Triton & TritonGPU passes\n   registry.insert<mlir::triton::TritonDialect, mlir::cf::ControlFlowDialect,\n+                  mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect,\n                   mlir::triton::gpu::TritonGPUDialect, mlir::math::MathDialect,\n                   mlir::arith::ArithDialect, mlir::scf::SCFDialect,\n                   mlir::gpu::GPUDialect>();"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -14,6 +14,7 @@\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Target/HSACO/HSACOTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n@@ -38,6 +39,7 @@ OwningOpRef<ModuleOp> loadMLIRModule(llvm::StringRef inputFilename,\n   mlir::DialectRegistry registry;\n   registry\n       .insert<TritonDialect, triton::gpu::TritonGPUDialect,\n+              triton::nvidia_gpu::TritonNvidiaGPUDialect,\n               mlir::math::MathDialect, arith::ArithDialect, scf::SCFDialect>();\n \n   context.appendDialectRegistry(registry);\n@@ -121,8 +123,10 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   }\n \n   llvm::LLVMContext llvmContext;\n-  auto llvmir = translateTritonGPUToLLVMIR(&llvmContext, *module,\n-                                           SMArch.getValue(), false /*isRocm*/);\n+  mlir::triton::gpu::TMAMetadataTy tmaInfos;\n+  auto llvmir = translateTritonGPUToLLVMIR(\n+      &llvmContext, *module, SMArch.getValue(), tmaInfos, false /*isRocm*/);\n+\n   if (!llvmir) {\n     llvm::errs() << \"Translate to LLVM IR failed\";\n   }"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -192,3 +192,4 @@ Iterators\n     :nosignatures:\n \n     static_range\n+    multiple_of"}, {"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "file_content_changes": "@@ -9,6 +9,7 @@\n \n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include <atomic>\n #include <limits>\n \n@@ -147,17 +148,17 @@ class Allocation {\n     BufferKind kind;\n     BufferId id;\n     size_t size;\n+    size_t alignment;\n     size_t offset;\n \n     bool operator==(const BufferT &other) const { return id == other.id; }\n     bool operator<(const BufferT &other) const { return id < other.id; }\n \n-    BufferT() : BufferT(BufferKind::Explicit) {}\n-    BufferT(BufferKind kind)\n-        : kind(kind), id(InvalidBufferId), size(0), offset(0) {}\n-    BufferT(BufferKind kind, size_t size) : BufferT(kind, size, 0) {}\n-    BufferT(BufferKind kind, size_t size, size_t offset)\n-        : kind(kind), id(nextId++), size(size), offset(offset) {}\n+    BufferT() : BufferT(BufferKind::Explicit, 0) {}\n+    BufferT(BufferKind kind, size_t size, size_t alignment = 4,\n+            size_t offset = 0)\n+        : kind(kind), id(nextId++), size(size), alignment(alignment),\n+          offset(offset) {}\n   };\n \n   /// Op -> Scratch Buffer"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -21,7 +21,7 @@ namespace mlir {\n /// This lattice value represents known information on the axes of a lattice.\n class AxisInfo {\n public:\n-  typedef SmallVector<int64_t, 4> DimVectorT;\n+  typedef SmallVector<int64_t> DimVectorT;\n \n public:\n   /// Default constructor"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 10, "deletions": 1, "changes": 11, "file_content_changes": "@@ -3,6 +3,7 @@\n \n #include \"mlir/Analysis/DataFlowFramework.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include <algorithm>\n #include <numeric>\n@@ -121,7 +122,11 @@ bool isSingleValue(Value value);\n \n bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n \n-Type getElementType(Value value);\n+bool isMmaToMmaShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n+\n+// TODO: Move utility functions that belong to ConvertLayoutOp to class\n+// ConvertLayoutOpHelper in the future\n+bool shouldUseDistSmem(Attribute srcLayout, Attribute dstLayout);\n \n template <typename T_OUT, typename T_IN>\n inline SmallVector<T_OUT> convertType(ArrayRef<T_IN> in) {\n@@ -324,6 +329,10 @@ template <typename T> class CallGraph {\n   FuncDataMapT funcMap;\n   SmallVector<FunctionOpInterface> roots;\n };\n+// Create a basic DataFlowSolver with constant and dead code analysis included.\n+std::unique_ptr<DataFlowSolver> createDataFlowSolver();\n+\n+triton::MakeTensorPtrOp getMakeTensorPtrOp(Value v);\n \n } // namespace mlir\n "}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.td", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -19,13 +19,17 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n                              \"mlir::tensor::TensorDialect\",\n                              \"mlir::triton::TritonDialect\",\n                              \"mlir::triton::gpu::TritonGPUDialect\",\n+                             \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\",\n                              \"mlir::ROCDL::ROCDLDialect\",\n                              \"mlir::NVVM::NVVMDialect\"];\n \n     let options = [\n         Option<\"computeCapability\", \"compute-capability\",\n                \"int32_t\", /*default*/\"80\",\n                \"device compute capability\">,\n+        Option<\"TmaMetadata\", \"tma-metadata\",\n+               \"mlir::triton::gpu::TMAMetadataTy*\", /*default*/\"nullptr\",\n+               \"tma metadata to the runtime\">,\n         Option<\"isROCM\", \"is-rocm\",\n                \"bool\", /*default*/\"false\",\n                \"compile for ROCM-compatible LLVM\">,"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -3,6 +3,8 @@\n \n #include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n+#include \"triton/Target/PTX/TmaMetadata.h\"\n+\n #include <memory>\n \n namespace mlir {\n@@ -12,9 +14,10 @@ template <typename T> class OperationPass;\n \n namespace triton {\n \n-std::unique_ptr<OperationPass<ModuleOp>>\n-createConvertTritonGPUToLLVMPass(int computeCapability = 80,\n-                                 bool isROCM = false);\n+std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonGPUToLLVMPass(\n+    int computeCapability = 80,\n+    mlir::triton::gpu::TMAMetadataTy *tmaMetadata = nullptr,\n+    bool isROCM = false);\n \n } // namespace triton\n "}, {"filename": "include/triton/Conversion/TritonToTritonGPU/Passes.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -2,6 +2,7 @@\n #define TRITON_CONVERSION_PASSES_H\n \n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n+#include \"triton/Target/PTX/TmaMetadata.h\"\n \n namespace mlir {\n namespace triton {"}, {"filename": "include/triton/Conversion/TritonToTritonGPU/Passes.td", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -25,6 +25,12 @@ def ConvertTritonToTritonGPU: Pass<\"convert-triton-to-tritongpu\", \"mlir::ModuleO\n        Option<\"threadsPerWarp\", \"threads-per-warp\",\n               \"int32_t\", /*default*/\"32\",\n               \"number of threads per warp\">,\n+        Option<\"numCTAs\", \"num-ctas\",\n+              \"int32_t\", /*default*/\"1\",\n+              \"number of ctas in a cga\">,\n+        Option<\"computeCapability\", \"compute-capability\",\n+              \"int32_t\", /*default*/\"80\",\n+              \"compute capability\">\n    ];\n }\n "}, {"filename": "include/triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -11,6 +11,9 @@ template <typename T> class OperationPass;\n namespace triton {\n \n constexpr static char AttrNumWarpsName[] = \"triton_gpu.num-warps\";\n+constexpr static char AttrNumCTAsName[] = \"triton_gpu.num-ctas\";\n+constexpr static char AttrComputeCapabilityName[] =\n+    \"triton_gpu.compute-capability\";\n \n constexpr static char AttrNumThreadsPerWarp[] = \"triton_gpu.threads-per-warp\";\n \n@@ -19,7 +22,8 @@ std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonToTritonGPUPass();\n \n // Create the pass with numWarps set explicitly.\n std::unique_ptr<OperationPass<ModuleOp>>\n-createConvertTritonToTritonGPUPass(int numWarps, int threadsPerWarp = 32);\n+createConvertTritonToTritonGPUPass(int numWarps, int threadsPerWarp = 32,\n+                                   int numCTAs = 1, int computeCapability = 80);\n \n } // namespace triton\n } // namespace mlir"}, {"filename": "include/triton/Dialect/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,2 +1,4 @@\n add_subdirectory(Triton)\n add_subdirectory(TritonGPU)\n+add_subdirectory(TritonNvidiaGPU)\n+add_subdirectory(NVGPU)"}, {"filename": "include/triton/Dialect/NVGPU/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+add_subdirectory(IR)\n+#add_subdirectory(Transforms)"}, {"filename": "include/triton/Dialect/NVGPU/IR/CMakeLists.txt", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+set(LLVM_TARGET_DEFINITIONS NVGPUOps.td)\n+mlir_tablegen(Dialect.h.inc -gen-dialect-decls -dialect=nvgpu)\n+mlir_tablegen(Dialect.cpp.inc -gen-dialect-defs -dialect=nvgpu)\n+mlir_tablegen(OpsConversions.inc -gen-llvmir-conversions)\n+mlir_tablegen(Ops.h.inc -gen-op-decls)\n+mlir_tablegen(Ops.cpp.inc -gen-op-defs)\n+mlir_tablegen(OpsEnums.h.inc -gen-enum-decls)\n+mlir_tablegen(OpsEnums.cpp.inc -gen-enum-defs)\n+add_public_tablegen_target(NVGPUTableGen)\n+\n+set(LLVM_TARGET_DEFINITIONS NVGPUAttrDefs.td)\n+mlir_tablegen(NVGPUAttrDefs.h.inc -gen-attrdef-decls)\n+mlir_tablegen(NVGPUAttrDefs.cpp.inc -gen-attrdef-defs)\n+add_public_tablegen_target(NVGPUAttrDefsIncGen)"}, {"filename": "include/triton/Dialect/NVGPU/IR/Dialect.h", "status": "added", "additions": 47, "deletions": 0, "changes": 47, "file_content_changes": "@@ -0,0 +1,47 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITON_DIALECT_NVGPU_IR_DIALECT_H_\n+#define TRITON_DIALECT_NVGPU_IR_DIALECT_H_\n+\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/Dialect.h\"\n+#include \"triton/Dialect/NVGPU/IR/Dialect.h.inc\"\n+#include \"triton/Dialect/NVGPU/IR/OpsEnums.h.inc\"\n+\n+#define GET_ATTRDEF_CLASSES\n+#include \"triton/Dialect/NVGPU/IR/NVGPUAttrDefs.h.inc\"\n+\n+#define GET_OP_CLASSES\n+#include \"triton/Dialect/NVGPU/IR/Ops.h.inc\"\n+\n+namespace mlir {\n+namespace triton {\n+namespace nvgpu {} // namespace nvgpu\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/NVGPU/IR/NVGPUAttrDefs.td", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "@@ -0,0 +1,33 @@\n+// Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining\n+// a copy of this software and associated documentation files\n+// (the \"Software\"), to deal in the Software without restriction,\n+// including without limitation the rights to use, copy, modify, merge,\n+// publish, distribute, sublicense, and/or sell copies of the Software,\n+// and to permit persons to whom the Software is furnished to do so,\n+// subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be\n+// included in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+// EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+// IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+// CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+// TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+// SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+#ifndef NVGPU_ATTRDEFS\n+#define NVGPU_ATTRDEFS\n+\n+include \"triton/Dialect/NVGPU/IR/NVGPUDialect.td\"\n+include \"mlir/IR/AttrTypeBase.td\"\n+\n+class NVGPU_Attr<string name, list<Trait> traits = [],\n+                     string baseCppClass = \"::mlir::Attribute\">\n+  : AttrDef<NVGPU_Dialect, name, traits, baseCppClass> {\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/NVGPU/IR/NVGPUDialect.td", "status": "added", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -0,0 +1,40 @@\n+// Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining\n+// a copy of this software and associated documentation files\n+// (the \"Software\"), to deal in the Software without restriction,\n+// including without limitation the rights to use, copy, modify, merge,\n+// publish, distribute, sublicense, and/or sell copies of the Software,\n+// and to permit persons to whom the Software is furnished to do so,\n+// subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be\n+// included in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+// EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+// IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+// CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+// TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+// SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+#ifndef NVGPU_DIALECT\n+#define NVGPU_DIALECT\n+\n+include \"mlir/IR/OpBase.td\"\n+\n+def NVGPU_Dialect : Dialect {\n+  let name = \"nvgpu\";\n+  let cppNamespace = \"::mlir::triton::nvgpu\";\n+\n+  let description = [{\n+    NVGPU Dialect.\n+  }];\n+\n+  let dependentDialects = [\n+    \"mlir::LLVM::LLVMDialect\"\n+  ];\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/NVGPU/IR/NVGPUOps.td", "status": "added", "additions": 371, "deletions": 0, "changes": 371, "file_content_changes": "@@ -0,0 +1,371 @@\n+// Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining\n+// a copy of this software and associated documentation files\n+// (the \"Software\"), to deal in the Software without restriction,\n+// including without limitation the rights to use, copy, modify, merge,\n+// publish, distribute, sublicense, and/or sell copies of the Software,\n+// and to permit persons to whom the Software is furnished to do so,\n+// subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be\n+// included in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+// EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+// IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+// CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+// TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+// SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+#ifndef NVGPU_OPS\n+#define NVGPU_OPS\n+\n+include \"triton/Dialect/NVGPU/IR/NVGPUDialect.td\"\n+include \"triton/Dialect/NVGPU/IR/NVGPUAttrDefs.td\"\n+include \"mlir/IR/OpBase.td\"\n+include \"mlir/IR/EnumAttr.td\"\n+include \"mlir/Dialect/LLVMIR/LLVMOpBase.td\"\n+\n+def I8Ptr_global : LLVM_IntPtrBase<8, 1>;\n+def I8Ptr_shared : LLVM_IntPtrBase<8, 3>;\n+def I64Ptr_shared : LLVM_IntPtrBase<64, 3>;\n+\n+class NVGPU_Op<string mnemonic, list<Trait> traits = []> :\n+    LLVM_OpBase<NVGPU_Dialect, mnemonic, traits>;\n+\n+def NVGPU_WGMMAFenceOp : NVGPU_Op<\"wgmma_fence\", []> {\n+  string llvmBuilder = [{\n+      createWGMMAFence(builder);\n+  }];\n+  let assemblyFormat = \"attr-dict\";\n+}\n+\n+\n+def NVGPU_WGMMACommitGroupOp : NVGPU_Op<\"wgmma_commit_group\", []> {\n+  let assemblyFormat = \"attr-dict\";\n+  string llvmBuilder = [{\n+      createWGMMACommitGroup(builder);\n+  }];\n+}\n+\n+def NVGPU_WGMMAWaitOp : NVGPU_Op<\"wgmma_wait_group\", []> {\n+  let arguments = (ins I32Attr:$pendings);\n+  let assemblyFormat = \"attr-dict\";\n+  string llvmBuilder = [{\n+      createWGMMAWaitGroup(builder, $pendings);\n+  }];\n+}\n+\n+def NVGPU_MBarrierInitOp : NVGPU_Op<\"mbarrier_init\", [MemoryEffects<[MemWrite]>]> {\n+  let arguments = (ins I64Ptr_shared:$mbarrier, I1:$pred, I32Attr:$count);\n+  let assemblyFormat = \"$mbarrier `,` $pred attr-dict `:` type($mbarrier)\";\n+  string llvmBuilder = [{\n+      auto *i32Ty = builder.getInt32Ty();\n+      auto *arriveCnt = builder.getInt32($count);\n+        createExternalCall(builder, \"__nv_mbarrier_init\", {builder.CreatePtrToInt($mbarrier, i32Ty),\n+        arriveCnt,\n+        builder.CreateIntCast($pred, i32Ty, false)});\n+  }];\n+}\n+\n+def MBarrier_ArriveTypeAttr : I32EnumAttr<\"MBarriveType\",\n+    \"mbarrier arrive type, either 'normal', 'expect_tx', 'cp_async'\",\n+    [\n+      I32EnumAttrCase<\"normal\", 0>,\n+      I32EnumAttrCase<\"cp_async\", 1>,\n+      I32EnumAttrCase<\"expect_tx\", 2>,\n+      I32EnumAttrCase<\"remote\", 3>,\n+    ]>{\n+  let cppNamespace = \"::mlir::triton::nvgpu\";\n+}\n+\n+def NVGPU_MBarrierArriveOp : NVGPU_Op<\"mbarrier_arrive\", []> {\n+  let arguments = (ins I64Ptr_shared:$mbarrier, I1:$pred, Optional<I32>:$ctaId, MBarrier_ArriveTypeAttr:$arriveType, DefaultValuedAttr<I32Attr, \"0\">:$txCount);\n+  let assemblyFormat = \"$mbarrier `,` $pred (`,` $ctaId^)? attr-dict `:` type($mbarrier)\";\n+  string llvmBuilder = [{\n+      auto *i32Ty = builder.getInt32Ty();\n+      createMBarrierArrive(builder, $arriveType, builder.CreatePtrToInt($mbarrier, i32Ty),\n+        builder.CreateIntCast($pred, i32Ty, false), $ctaId,\n+        $txCount);\n+  }];\n+}\n+\n+def NVGPU_MBarrierWaitOp : NVGPU_Op<\"mbarrier_wait\", []> {\n+  let arguments = (ins I64Ptr_shared:$mbarrier, I1:$phase);\n+  let assemblyFormat = \"$mbarrier `,` $phase attr-dict `:` type(operands)\";\n+\n+  string llvmBuilder = [{\n+      auto *i32Ty = builder.getInt32Ty();\n+      createExternalCall(builder, \"__nv_mbarrier_wait\", {builder.CreatePtrToInt($mbarrier, i32Ty),\n+        builder.CreateIntCast($phase, i32Ty, false)});\n+  }];\n+}\n+\n+def NVGPU_NamedBarrierArriveOp : NVGPU_Op<\"bar_arrive\", []> {\n+  let arguments = (ins I32:$bar, I32:$numThreads);\n+  let assemblyFormat = \"$bar `,` $numThreads attr-dict `:` type(operands)\";\n+  string llvmBuilder = [{\n+      createExternalCall(builder, \"__nv_bar_arrive\", {$bar, $numThreads});\n+  }];\n+}\n+\n+def NVGPU_NamedBarrierWaitOp : NVGPU_Op<\"bar_wait\", []> {\n+  let arguments = (ins I32:$bar, I32:$numThreads);\n+  let assemblyFormat = \"$bar `,` $numThreads attr-dict `:` type(operands)\";\n+  string llvmBuilder = [{\n+      createExternalCall(builder, \"__nv_bar_wait\", {$bar, $numThreads});\n+  }];\n+}\n+\n+def WGMMADesc_ModeAttr : I32EnumAttr<\"WGMMADescMode\",\n+    \"wgmma desc mode, either 'none', 'swizzle128', 'swizzle64', or 'swizzle32'\",\n+    [\n+      I32EnumAttrCase<\"none\", 0>,\n+      I32EnumAttrCase<\"swizzle128\", 1>,\n+      I32EnumAttrCase<\"swizzle64\", 2>,\n+      I32EnumAttrCase<\"swizzle32\", 3>\n+    ]>{\n+  let cppNamespace = \"::mlir::triton::nvgpu\";\n+}\n+\n+def NVGPU_WGMMADescCreateOp : NVGPU_Op<\"wgmma_desc_create\", []> {\n+  let arguments = (ins LLVM_AnyPointer:$buffer, I32:$height, WGMMADesc_ModeAttr:$mode);\n+  let results = (outs I64:$res);\n+  let assemblyFormat = \"$buffer `,` $height attr-dict `:` functional-type(operands, results)\";\n+  string llvmBuilder = [{\n+    $res = createWGMMADesc(builder, builder.CreatePtrToInt($buffer, builder.getInt32Ty()), $mode, $height);\n+  }];\n+}\n+\n+def NVGPU_TMALoadTiledOp : NVGPU_Op<\"tma_load_tiled\", [AttrSizedOperandSegments]> {\n+  let arguments = (ins I8Ptr_shared:$dst, I64Ptr_shared:$mbarrier, I8Ptr_global:$tmaDesc, I64:$l2Desc,\n+                       I1:$pred, Variadic<I32>:$coords, Optional<I16>:$mcastMask);\n+  let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n+  string llvmBuilder = [{\n+    auto *i32Ty = builder.getInt32Ty();\n+    auto *i64Ty = builder.getInt64Ty();\n+    createTMALoadTiled(builder,\n+      builder.CreatePtrToInt($dst, i32Ty),\n+      builder.CreatePtrToInt($mbarrier, i32Ty),\n+      builder.CreatePtrToInt($tmaDesc, i64Ty),\n+      $l2Desc, $mcastMask, builder.CreateIntCast($pred, builder.getInt32Ty(), false), $coords);\n+  }];\n+}\n+\n+def NVGPU_TMALoadIm2colOp : NVGPU_Op<\"tma_load_im2col\", []> {\n+  let arguments = (ins I8Ptr_shared:$dst, I64Ptr_shared:$mbarrier, I8Ptr_global:$tmaDesc, I64:$l2Desc, LLVM_AnyStruct:$im2colOffsets, I1:$pred, Variadic<I32>:$coords, I16Attr:$mcastMask);\n+  let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n+  string llvmBuilder = [{\n+    auto *i32Ty = builder.getInt32Ty();\n+    auto *i64Ty = builder.getInt64Ty();\n+    createTMALoadIm2col(builder,\n+    builder.CreatePtrToInt($dst, i32Ty),\n+    builder.CreatePtrToInt($mbarrier, i32Ty),\n+    builder.CreatePtrToInt($tmaDesc, i64Ty),\n+    $l2Desc, $mcastMask, $im2colOffsets, builder.CreateIntCast($pred, builder.getInt32Ty(), false), $coords);\n+  }];\n+}\n+\n+def WGMMA_LayoutAttr : I32EnumAttr<\"WGMMALayout\",\n+    \"wgmma layout, either 'row' or 'col'\",\n+    [\n+      I32EnumAttrCase<\"row\", 0>,\n+      I32EnumAttrCase<\"col\", 1>\n+    ]>{\n+  let cppNamespace = \"::mlir::triton::nvgpu\";\n+}\n+\n+def WGMMA_EltTypeAttr : I32EnumAttr<\"WGMMAEltType\",\n+    \"wgmma operand type, either 's8', 's32', 'e4m3', 'e5m2', 'f16', 'bf16', 'tf32', or 'f32'\",\n+    [\n+      I32EnumAttrCase<\"s8\", 0>,\n+      I32EnumAttrCase<\"s32\", 1>,\n+      I32EnumAttrCase<\"e4m3\", 2>,\n+      I32EnumAttrCase<\"e5m2\", 3>,\n+      I32EnumAttrCase<\"f16\", 4>,\n+      I32EnumAttrCase<\"bf16\", 5>,\n+      I32EnumAttrCase<\"tf32\", 6>,\n+      I32EnumAttrCase<\"f32\", 7>\n+    ]>{\n+  let cppNamespace = \"::mlir::triton::nvgpu\";\n+}\n+\n+def WGMMA_OperandType : AnyTypeOf<[LLVM_AnyStruct, I64], \"wgmma operand A/B type\">;\n+\n+def NVGPU_WGMMAOp : NVGPU_Op<\"wgmma\", []> {\n+  let arguments = (ins WGMMA_OperandType:$opA, WGMMA_OperandType:$opB, LLVM_AnyStruct:$opC,\n+                   I32Attr:$m, I32Attr:$n, I32Attr:$k,\n+                   WGMMA_EltTypeAttr:$eltTypeC, WGMMA_EltTypeAttr:$eltTypeA, WGMMA_EltTypeAttr:$eltTypeB,\n+                   WGMMA_LayoutAttr:$layoutA, WGMMA_LayoutAttr:$layoutB);\n+  let results = (outs LLVM_AnyStruct:$res);\n+  let assemblyFormat = \"$opA `,` $opB `,` $opC attr-dict `:` functional-type(operands, $res)\";\n+  string llvmBuilder = [{\n+    $res = createWGMMA(builder, $m, $n, $k, $eltTypeC, $eltTypeA, $eltTypeB, $layoutA, $layoutB, $opA, $opB, $opC);\n+  }];\n+}\n+\n+def NVGPU_CGABarrierSyncOp : NVGPU_Op<\"cga_barrier_sync\", []> {\n+  let assemblyFormat = \"attr-dict\";\n+  string llvmBuilder = [{\n+      createExternalCall(builder, \"__nv_cga_barrier_sync\");\n+  }];\n+}\n+\n+def NVGPU_CGABarrierArriveOp : NVGPU_Op<\"cga_barrier_arrive\", []> {\n+  let assemblyFormat = \"attr-dict\";\n+  string llvmBuilder = [{\n+      createExternalCall(builder, \"__nv_cga_barrier_arrive\");\n+  }];\n+}\n+\n+def NVGPU_CGABarrierWaitOp : NVGPU_Op<\"cga_barrier_wait\", []> {\n+  let assemblyFormat = \"attr-dict\";\n+  string llvmBuilder = [{\n+      createExternalCall(builder, \"__nv_cga_barrier_wait\");\n+  }];\n+}\n+\n+def NVGPU_LoadDSmemOp : NVGPU_Op<\"load_dsmem\", [MemoryEffects<[MemRead]>]> {\n+  let arguments = (ins LLVM_AnyPointer:$addr, I32:$ctaId, I32Attr:$bitwidth, I32Attr:$vec);\n+  let builders = [\n+      OpBuilder<(ins \"Type\":$resultTy, \"Value\":$addr, \"Value\":$ctaId)>,\n+      OpBuilder<(ins \"Value\":$addr, \"Value\":$ctaId, \"unsigned\":$bitwidth, \"unsigned\":$vec)>,\n+      OpBuilder<(ins \"Value\":$addr, \"Value\":$ctaId, \"unsigned\":$bitwidth)>\n+  ];\n+  let results = (outs LLVM_LoadableType:$result);\n+  let assemblyFormat = \"operands attr-dict `:` functional-type(operands, results)\";\n+  string llvmBuilder = [{\n+      $result = createLoadSharedCluster(builder, $addr, $ctaId, $bitwidth, $vec);\n+  }];\n+}\n+\n+def NVGPU_StoreDSmemOp : NVGPU_Op<\"store_dsmem\", [MemoryEffects<[MemWrite]>]> {\n+  let arguments = (ins LLVM_AnyPointer:$addr, I32:$ctaId,\n+                       Variadic<LLVM_LoadableType>:$values, I1:$pred);\n+  let builders = [\n+      OpBuilder<(ins \"Value\":$addr, \"Value\":$ctaId, \"Value\":$value, \"Value\":$pred)>,\n+  ];\n+  let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n+  string llvmBuilder = [{\n+      createStoreSharedCluster(builder, $addr, $ctaId, $values, $pred, op.getBitwidth(), op.getVec());\n+  }];\n+  let extraClassDeclaration = [{\n+      unsigned getBitwidth();\n+      unsigned getVec();\n+  }];\n+}\n+\n+def NVGPU_FenceAsyncSharedOp : NVGPU_Op<\"fence_async_shared\", []> {\n+  let arguments = (ins BoolAttr:$bCluster);\n+  string llvmBuilder = [{\n+    if ($bCluster)\n+      createExternalCall(builder, \"__nv_fence_async_shared_cluster\", {});\n+    else\n+      createExternalCall(builder, \"__nv_fence_async_shared_cta\", {});\n+  }];\n+  let assemblyFormat = \"attr-dict\";\n+}\n+\n+def NVGPU_FenceMBarrierInitOp : NVGPU_Op<\"fence_mbarrier_init\", []> {\n+  string llvmBuilder = [{\n+      createExternalCall(builder, \"__nv_fence_mbarrier_init\", {});\n+  }];\n+  let assemblyFormat = \"attr-dict\";\n+}\n+\n+def NVGPU_ClusterArriveOp : NVGPU_Op<\"cluster_arrive\", []> {\n+  let arguments = (ins I1Attr:$relaxed);\n+\n+  string llvmBuilder = [{\n+    if ($relaxed)\n+      createExternalCall(builder, \"__nv_cluster_arrive_relaxed\", {});\n+    else\n+      createExternalCall(builder, \"__nv_cluster_arrive\", {});\n+  }];\n+  let assemblyFormat = \"attr-dict\";\n+}\n+\n+def NVGPU_ClusterWaitOp : NVGPU_Op<\"cluster_wait\", []> {\n+  string llvmBuilder = [{\n+      createExternalCall(builder, \"__nv_cluster_wait\", {});\n+  }];\n+  let assemblyFormat = \"attr-dict\";\n+}\n+\n+def NVGPU_TMAStoreTiledOp : NVGPU_Op<\"tma_store_tiled\", [MemoryEffects<[MemWrite]>]> {\n+  let arguments = (ins I8Ptr_global:$tmaDesc, I8Ptr_shared:$src, I1:$pred, Variadic<I32>:$coords);\n+  let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n+  string llvmBuilder = [{\n+    auto *i32Ty = builder.getInt32Ty();\n+    auto *i64Ty = builder.getInt64Ty();\n+    createTMAStoreTiled(builder,\n+      builder.CreatePtrToInt($tmaDesc, i64Ty),\n+      builder.CreatePtrToInt($src, i32Ty),\n+      builder.CreateIntCast($pred, i32Ty, false), $coords);\n+  }];\n+}\n+\n+def NVGPU_StoreMatrixOp : NVGPU_Op<\"stmatrix\", [MemoryEffects<[MemWrite]>]> {\n+  let arguments = (ins I8Ptr_shared:$addr, Variadic<I32>:$datas);\n+  let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n+  string llvmBuilder = [{\n+    auto *i32Ty = builder.getInt32Ty();\n+    createStoreMatrix(builder,\n+      builder.CreatePtrToInt($addr, i32Ty),\n+      $datas);\n+  }];\n+}\n+\n+def NVGPU_OffsetOfStmatrixV4Op : NVGPU_Op<\"offset_of_stmatrix_v4\", []> {\n+  let arguments = (ins I32:$threadId, I32:$rowOfWarp, I32:$elemIdx, I32Attr:$leadingDimOffset, I32Attr:$rowStride, I1Attr:$swizzleEnabled);\n+  let results = (outs I32:$offset);\n+  let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($offset)\";\n+  string llvmBuilder = [{\n+    $offset = createOffsetOfStmatrixV4(builder, $threadId, $rowOfWarp, $elemIdx, $leadingDimOffset, $rowStride, $swizzleEnabled);\n+  }];\n+}\n+\n+def NVGPU_OffsetOfSts64Op : NVGPU_Op<\"offset_of_sts64\", []> {\n+  let arguments = (ins I32:$threadId, I32:$rowOfWarp, I32:$elemIdx, I32Attr:$leadingDimOffset, I32Attr:$rowStride, I1Attr:$swizzleEnabled);\n+  let results = (outs I32:$offset);\n+  let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($offset)\";\n+  string llvmBuilder = [{\n+    $offset = createOffsetOfSts64(builder, $threadId, $rowOfWarp, $elemIdx, $leadingDimOffset, $rowStride, $swizzleEnabled);\n+  }];\n+}\n+\n+def NVGPU_Sts64Op : NVGPU_Op<\"sts64\", [MemoryEffects<[MemWrite]>]> {\n+  let arguments = (ins I32:$offset, AnyTypeOf<[F32, I32]>:$d0, AnyTypeOf<[F32, I32]>:$d1);\n+  let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n+  string llvmBuilder = [{\n+    createSts64(builder, $offset, $d0, $d1);\n+  }];\n+}\n+\n+def NVGPU_ClusterCTAIdOp : NVGPU_Op<\"cluster_id\", [Pure]> {\n+  let results = (outs I32:$result);\n+  let assemblyFormat = \"attr-dict\";\n+  string llvmBuilder = [{\n+      $result = createClusterId(builder);\n+      }];\n+}\n+\n+def NVGPU_RegAllocOp : NVGPU_Op<\"reg_alloc\", []> {\n+  let arguments = (ins I32Attr: $regCount);\n+  let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n+  string llvmBuilder = [{\n+      createRegAlloc(builder, $regCount);\n+  }];\n+}\n+\n+def NVGPU_RegDeallocOp : NVGPU_Op<\"reg_dealloc\", []> {\n+  let arguments = (ins I32Attr: $regCount);\n+  let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n+  string llvmBuilder = [{\n+      createRegDealloc(builder, $regCount);\n+  }];\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.h", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITON_DIALECT_NVGPU_NVGPUTOLLVMIRTRANSLATION_H\n+#define TRITON_DIALECT_NVGPU_NVGPUTOLLVMIRTRANSLATION_H\n+\n+namespace mlir {\n+\n+class DialectRegistry;\n+class MLIRContext;\n+\n+/// Register the nvgpu dialect and the translation from it to the LLVM IR in the\n+/// given registry;\n+void registerNVGPUDialectTranslation(DialectRegistry &registry);\n+\n+/// Register the nvgpu dialect and the translation from it in the registry\n+/// associated with the given context.\n+void registerNVGPUDialectTranslation(MLIRContext &context);\n+} // namespace mlir\n+\n+#endif // TRITON_DIALECT_NVGPU_NVGPUTOLLVMIRTRANSLATION_H"}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -9,6 +9,8 @@\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Dialect.h\"\n+#include \"mlir/IR/FunctionInterfaces.h\"\n+#include \"mlir/Interfaces/CallInterfaces.h\"\n #include \"mlir/Interfaces/ControlFlowInterfaces.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h.inc\"\n #include \"triton/Dialect/Triton/IR/OpsEnums.h.inc\""}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -9,6 +9,8 @@ include \"mlir/IR/OpBase.td\"\n include \"mlir/IR/FunctionInterfaces.td\" // FunctionOpInterface\n include \"mlir/IR/SymbolInterfaces.td\" // SymbolUserOpInterface\n include \"mlir/IR/OpAsmInterface.td\" // OpAsmOpInterface\n+include \"mlir/Interfaces/CallInterfaces.td\" // CallOpInterface\n+include \"mlir/Interfaces/CastInterfaces.td\" // CastOpInterface\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/ControlFlowInterfaces.td\" // BranchOpInterface\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n@@ -563,6 +565,7 @@ def TT_MakeTensorPtrOp : TT_Op<\"make_tensor_ptr\",\n \n   let results = (outs TT_TensorPtr:$result);\n \n+  // TODO(Keren): define a custom assembly format for this op because the result type cannot be printed correctly\n   // Add additional `[]` to increase readability and split variadic lists\n   let assemblyFormat = \"$base `,` `[` $shape `]` `,` `[` $strides `]` `,` `[` $offsets `]` attr-dict `:` type($result)\";\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -74,7 +74,7 @@ def TT_PtrType : TritonTypeDef<\"Pointer\", \"ptr\"> {\n // Scalar Pointer Type: `ptr<>`\n def TT_Ptr : TT_PtrOf<[AnyType]>;\n \n-// Tensor of Pointer Type\n+// Tensor of Pointer Type: `tensor<ptr<>>`\n def TT_PtrTensor : TensorOf<[TT_Ptr]>;\n \n // Tensor of Pointer Type or Pointer type: `tensor<ptr<>>` or `ptr<>`"}, {"filename": "include/triton/Dialect/Triton/IR/Types.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -14,6 +14,8 @@ namespace triton {\n \n bool isTensorPointerType(Type type);\n \n+bool isTensorOrTensorPointerType(Type type);\n+\n unsigned getPointeeBitWidth(Type type);\n \n Type getPointeeType(Type type);"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -9,7 +9,6 @@ namespace triton {\n std::unique_ptr<Pass> createCombineOpsPass();\n \n std::unique_ptr<Pass> createReorderBroadcastPass();\n-\n std::unique_ptr<Pass>\n createRewriteTensorPointerPass(int computeCapability = 80);\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -3,9 +3,13 @@ mlir_tablegen(Dialect.h.inc -gen-dialect-decls -dialect=triton_gpu)\n mlir_tablegen(Dialect.cpp.inc -gen-dialect-defs -dialect=triton_gpu)\n mlir_tablegen(Ops.h.inc -gen-op-decls)\n mlir_tablegen(Ops.cpp.inc -gen-op-defs)\n+mlir_tablegen(Types.h.inc -gen-typedef-decls -typedefs-dialect=triton_gpu)\n+mlir_tablegen(Types.cpp.inc -gen-typedef-defs -typedefs-dialect=triton_gpu)\n add_public_tablegen_target(TritonGPUTableGen)\n \n set(LLVM_TARGET_DEFINITIONS TritonGPUAttrDefs.td)\n mlir_tablegen(TritonGPUAttrDefs.h.inc -gen-attrdef-decls)\n mlir_tablegen(TritonGPUAttrDefs.cpp.inc -gen-attrdef-defs)\n+mlir_tablegen(OpsEnums.h.inc -gen-enum-decls)\n+mlir_tablegen(OpsEnums.cpp.inc -gen-enum-defs)\n add_public_tablegen_target(TritonGPUAttrDefsIncGen)"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 29, "deletions": 4, "changes": 33, "file_content_changes": "@@ -7,6 +7,7 @@\n #include \"mlir/IR/Dialect.h\"\n \n // TritonGPU depends on Triton\n+#include \"triton/Dialect/NVGPU/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h.inc\"\n@@ -71,17 +72,41 @@ getWarpsPerCTAWithUniqueData(Attribute layout, ArrayRef<int64_t> tensorShape);\n \n SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n \n+SmallVector<unsigned> getOrder(Attribute layout);\n+\n+CTALayoutAttr getCTALayout(Attribute layout);\n+\n+SmallVector<unsigned> getCTAsPerCGA(Attribute layout);\n+\n+SmallVector<unsigned> getCTASplitNum(Attribute layout);\n+\n+SmallVector<unsigned> getCTAOrder(Attribute layout);\n+\n+/* The difference between ShapePerCTATile and ShapePerCTA:\n+ * (1) ShapePerCTATile is defined by SizePerThread * ThreadsPerWarp *\n+ *     WarpsPerCTA in each dimension and is independent from the tensor shape.\n+ * (2) ShapePerCTA is defined by shape / CTASplitNum in each dimension.\n+ * (3) In the implementation of emitIndices, ShapePerCTATile will\n+ *     be replicated or wraped to fit ShapePerCTA.\n+ */\n SmallVector<unsigned>\n-getShapePerCTA(Attribute layout,\n-               ArrayRef<int64_t> tensorShape = ArrayRef<int64_t>());\n+getShapePerCTATile(Attribute layout,\n+                   ArrayRef<int64_t> tensorShape = ArrayRef<int64_t>());\n \n-SmallVector<unsigned> getOrder(Attribute layout);\n+SmallVector<int64_t> getShapePerCTA(ArrayRef<unsigned> CTASplitNum,\n+                                    ArrayRef<int64_t> shape);\n+SmallVector<int64_t> getShapePerCTA(Attribute layout, ArrayRef<int64_t> shape);\n+SmallVector<int64_t> getShapePerCTA(Type type);\n+\n+unsigned getNumWarpsPerCTA(Attribute layout);\n+\n+unsigned getNumCTAs(Attribute layout);\n \n bool isaDistributedLayout(Attribute layout);\n \n bool isSharedEncoding(Value value);\n \n-bool isExpensiveCat(CatOp cat, Attribute &targetEncoding);\n+bool isExpensiveCat(CatOp cat, Attribute targetEncoding);\n \n } // namespace gpu\n } // namespace triton"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 211, "deletions": 70, "changes": 281, "file_content_changes": "@@ -41,6 +41,19 @@ Right now, Triton implements two classes of layouts: shared, and distributed.\n   }];\n }\n \n+//===----------------------------------------------------------------------===//\n+// CTA Layout\n+//===----------------------------------------------------------------------===//\n+\n+def CTALayoutAttr : TritonGPU_Attr<\"CTALayout\"> {\n+  let parameters = (\n+    ins\n+    ArrayRefParameter<\"unsigned\">:$CTAsPerCGA,\n+    ArrayRefParameter<\"unsigned\">:$CTASplitNum,\n+    ArrayRefParameter<\"unsigned\">:$CTAOrder\n+  );\n+}\n+\n //===----------------------------------------------------------------------===//\n // Shared Layout Encoding\n //===----------------------------------------------------------------------===//\n@@ -64,26 +77,49 @@ are stored contiguously\n _ _ _ _ /\\_ _ _ _\n A_{2, 2}  A_{2, 3}  A_{2, 0}  A_{2, 1} ...   [phase 1] \\ per phase = 2\n A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n+\n+For MMAv3 eg Hopper GMMA, hasLeadingOffset should be true. In this case,\n+when the matrix is stored in shared memory, there will be an offset not\n+only in the stride dimension, but also in the leading dimension. For example,\n+a matrix of size 16x128 and data type I8 is stored in the shared memory with\n+64B-swizzle mode. The offset of the element with index (0, 64) will be 16*64,\n+compared to 1*64 when the hasLeadingOffset is false.\n   }];\n \n+  // swizzle info: vec, perPhase, maxPhase\n+  // order: the fastest-changing axis first\n   let parameters = (\n     ins\n-    // swizzle info\n-    \"unsigned\":$vec, \"unsigned\":$perPhase, \"unsigned\":$maxPhase,\n-    ArrayRefParameter<\"unsigned\", \"order of axes by the rate of changing\">:$order\n+    \"unsigned\":$vec,\n+    \"unsigned\":$perPhase,\n+    \"unsigned\":$maxPhase,\n+    ArrayRefParameter<\"unsigned\">:$order,\n+    \"CTALayoutAttr\":$CTALayout,\n+    \"bool\":$hasLeadingOffset\n   );\n \n   let builders = [\n+    AttrBuilder<(ins \"unsigned\":$vec,\n+                     \"unsigned\":$perPhase,\n+                     \"unsigned\":$maxPhase,\n+                     \"ArrayRef<unsigned>\":$order,\n+                     \"CTALayoutAttr\":$CTALayout), [{\n+        bool hasLeadingOffset = false; // default value\n+        return $_get(context, vec, perPhase, maxPhase, order, CTALayout, hasLeadingOffset);\n+    }]>,\n+\n     AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n                      \"ArrayRef<int64_t>\":$shape,\n                      \"ArrayRef<unsigned>\":$order,\n+                     \"CTALayoutAttr\":$CTALayout,\n                      \"unsigned\":$typeWidthInBit), [{\n         auto mmaEnc = dotOpEnc.getParent().dyn_cast<MmaEncodingAttr>();\n \n         if(!mmaEnc)\n-          return $_get(context, 1, 1, 1, order);\n+          return get(context, 1, 1, 1, order, CTALayout);\n \n         int opIdx = dotOpEnc.getOpIdx();\n+        auto shapePerCTA = getShapePerCTA(CTALayout.getCTASplitNum(), shape);\n \n         // number of rows per phase\n \n@@ -92,57 +128,93 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n \n         // ---- begin Volta ----\n         if (mmaEnc.isVolta()) {\n-          int perPhase = 128 / (shape[order[0]] * (typeWidthInBit / 8));\n+          int perPhase = 128 / (shapePerCTA[order[0]] * (typeWidthInBit / 8));\n           perPhase = std::max<int>(perPhase, 1);\n           bool is_row = order[0] != 0;\n-          bool is_vec4 = opIdx == 0 ? !is_row && (shape[order[0]] <= 16) :\n-              is_row && (shape[order[0]] <= 16);\n+          bool is_vec4 = opIdx == 0 ? !is_row && (shapePerCTA[order[0]] <= 16) :\n+              is_row && (shapePerCTA[order[0]] <= 16);\n           int pack_size = opIdx == 0 ? ((is_row || is_vec4) ? 1 : 2) :\n                                        ((is_row && !is_vec4) ? 2 : 1);\n           int rep = 2 * pack_size;\n           int maxPhase = (order[inner] == 1 ? 8 : 4) / perPhase;\n           int vec = 2 * rep;\n-          return $_get(context, vec, perPhase, maxPhase, order);\n+          return get(context, vec, perPhase, maxPhase, order, CTALayout);\n         }\n \n         // ---- begin Ampere ----\n         if (mmaEnc.isAmpere()) {\n-          int perPhase = 128 / (shape[order[0]] * 4 / dotOpEnc.getMMAv2kWidth());\n+          int perPhase = 128 / (shapePerCTA[order[0]] * 4 / dotOpEnc.getMMAv2kWidth());\n           perPhase = std::max<int>(perPhase, 1);\n           std::vector<size_t> matShape = {8, 8, 4 * dotOpEnc.getMMAv2kWidth()};\n           // for now, disable swizzle when using transposed int8 tensor cores\n           if ((32 / typeWidthInBit != dotOpEnc.getMMAv2kWidth()) && order[0] == inner)\n-            return $_get(context, 1, 1, 1, order);\n+            return get(context, 1, 1, 1, order, CTALayout);\n \n           // --- handle A operand ---\n           if (opIdx == 0) { // compute swizzling for A operand\n               int vec = (order[0] == 1) ? matShape[2] : matShape[0]; // k : m\n               int mmaStride = (order[0] == 1) ? matShape[0] : matShape[2];\n               int maxPhase = mmaStride / perPhase;\n-              return $_get(context, vec, perPhase, maxPhase, order);\n+              return get(context, vec, perPhase, maxPhase, order, CTALayout);\n           }\n \n           // --- handle B operand ---\n           if (opIdx == 1) {\n               int vec = (order[0] == 1) ? matShape[1] : matShape[2]; // n : k\n               int mmaStride = (order[0] == 1) ? matShape[2] : matShape[1];\n               int maxPhase = mmaStride / perPhase;\n-              return $_get(context, vec, perPhase, maxPhase, order);\n+              return get(context, vec, perPhase, maxPhase, order, CTALayout);\n           }\n \n           llvm_unreachable(\"invalid operand index\");\n         }\n \n+        // ---- begin version 3 ----\n+        if (mmaEnc.isHopper()) {\n+          llvm_unreachable(\"SharedEncodingAttr builder when the MMAEncodingAttr\"\n+                           \" is Hopper has not been implemented yet\");\n+          return $_get(context, 1, 1, 1, order, CTALayout, true);\n+        }\n+\n         // ---- not implemented ----\n         llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n     }]>,\n \n     AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n                      \"ArrayRef<int64_t>\":$shape,\n                      \"ArrayRef<unsigned>\":$order,\n+                     \"CTALayoutAttr\":$CTALayout,\n                      \"Type\":$eltTy), [{\n       unsigned bitwidth = eltTy.getIntOrFloatBitWidth();\n-      return get(context, dotOpEnc, shape, order, bitwidth);\n+      return get(context, dotOpEnc, shape, order, CTALayout, bitwidth);\n+    }]>,\n+\n+    AttrBuilder<(ins \"ArrayRef<int64_t>\":$shape,\n+                     \"ArrayRef<unsigned>\":$order,\n+                     \"CTALayoutAttr\":$CTALayout,\n+                     \"Type\":$eltTy), [{\n+        auto shapePerCTA = getShapePerCTA(CTALayout.getCTASplitNum(), shape);\n+\n+        int32_t eleBitWidth = eltTy.getIntOrFloatBitWidth();\n+        int32_t vec = 128 / eleBitWidth, perPhase = 1, maxPhase = 1;\n+\n+        // get proper shared memory swizzling mode from the contiguous dimension\n+        // size of the origin blocked layout.\n+        auto contigDimSizeInByte = shapePerCTA[order[0]] * eleBitWidth / 8;\n+        if (contigDimSizeInByte >= 128 && contigDimSizeInByte % 128 == 0) {\n+          perPhase = 1;\n+          maxPhase = 8;\n+        } else if (contigDimSizeInByte >= 64 && contigDimSizeInByte % 64 == 0) {\n+          perPhase = 2;\n+          maxPhase = 4;\n+        } else if (contigDimSizeInByte >= 32 && contigDimSizeInByte % 32 == 0) {\n+          perPhase = 4;\n+          maxPhase = 2;\n+        } else {\n+          llvm_unreachable(\"unsupported shared memory layout for MMAv3\");\n+        }\n+\n+        return $_get(context, vec, perPhase, maxPhase, order, CTALayout, true);\n     }]>\n   ];\n \n@@ -194,7 +266,7 @@ used to promote memory coalescing in LoadInst and StoreInst.\n It is characterized by three tuples -- thread tile size, warp tile size, and block tile size -- which\n specify the amount of elements owned by each CUDA thread, warp and CTA respectively.\n \n-For example, a row-major coalesced layout may partition a 16x16 tensor over 2 warps (i.e. 64 threads) as follows.\n+Example 1, a row-major coalesced layout may partition a 16x16 tensor over 2 warps (i.e. 64 threads) as follows:\n \n [ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n [ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n@@ -210,82 +282,143 @@ for\n   sizePerThread = {2, 2}\n   threadsPerWarp = {8, 4}\n   warpsPerCTA = {1, 2}\n+  CTAsPerCGA = {1, 1}\n+}>\n+\n+Example 2, a row-major coalesced layout may partition a 32x32 tensor over 2 warps (i.e. 64 threads) as follows:\n+\n+[ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35  0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n+[ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35  0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n+[ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39  4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]\n+[ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39  4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]\n+...                                                 ...\n+[ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63  28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]\n+[ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63  28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]\n+[ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35  0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n+[ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35  0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n+[ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39  4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]\n+[ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39  4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]\n+...                                                 ...\n+[ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63  28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]\n+[ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63  28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]\n+for\n+\n+#triton_gpu.blocked_layout<{\n+  sizePerThread = {2, 2}\n+  threadsPerWarp = {8, 4}\n+  warpsPerCTA = {1, 2}\n+  CTAsPerCGA = {1, 1}\n+}>\n+\n+Example 3, A row-major coalesced layout may partition a 32x32 tensor over 2 warps (i.e. 64 threads) and\n+4 CTAs (taking 2x2 for example) as follows:\n+\n+CTA [0,0]                                              CTA [0,1]\n+[ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]  [ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n+[ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]  [ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n+[ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]  [ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]\n+[ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]  [ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]\n+...                                                    ...\n+[ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]  [ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]\n+[ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]  [ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]\n+\n+CTA [1,0]                                              CTA [1,1]\n+[ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]  [ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n+[ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]  [ 0  0  1  1  2  2  3  3  ; 32 32 33 33 34 34 35 35 ]\n+[ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]  [ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]\n+[ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]  [ 4  4  5  5  6  6  7  7  ; 36 36 37 37 38 38 39 39 ]\n+...                                                    ...\n+[ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]  [ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]\n+[ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]  [ 28 28 29 29 30 30 31 31 ; 60 60 61 61 62 62 63 63 ]\n+for\n+\n+#triton_gpu.blocked_layout<{\n+  sizePerThread = {2, 2}\n+  threadsPerWarp = {8, 4}\n+  warpsPerCTA = {1, 2}\n+  CTAsPerCGA = {2, 2}\n }>\n }];\n \n+  let parameters = (\n+    ins\n+    ArrayRefParameter<\"unsigned\">:$sizePerThread,\n+    ArrayRefParameter<\"unsigned\">:$threadsPerWarp,\n+    ArrayRefParameter<\"unsigned\">:$warpsPerCTA,\n+    ArrayRefParameter<\"unsigned\">:$order, // the fastest-changing axis first\n+    \"CTALayoutAttr\":$CTALayout\n+  );\n \n   let builders = [\n-    // Custom builder initializes sizePerWarp and sizePerCTA automatically\n-    // TODO: compiles on MacOS but not linux?\n-    // AttrBuilder<(ins \"ArrayRef<unsigned>\":$sizePerThread,\n-    //                  \"ArrayRef<unsigned>\":$threadsPerWarp,\n-    //                  \"ArrayRef<unsigned>\":$warpsPerCTA,\n-    //                  \"ArrayRef<unsigned>\":$order), [{\n-    //   int rank = threadsPerWarp.size();\n-    //   SmallVector<unsigned, 4> sizePerWarp(rank);\n-    //   SmallVector<unsigned, 4> sizePerCTA(rank);\n-    //   for (unsigned i = 0; i < rank; i++) {\n-    //     sizePerWarp.push_back(sizePerThread[i] * threadsPerWarp[i]);\n-    //     sizePerCTA.push_back(sizePerWarp[i] * warpsPerCTA[i]);\n-    //   }\n-    //   return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order, sizePerWarp, sizePerCTA);\n-    // }]>,\n-    // Custom builder initializes sizePerWarp and sizePerCTA automatically\n-    // Default builder takes sizePerThread, order and numWarps, and tries to\n-    // pack numWarps*32 threads in the provided order for use in a type\n-    // of the given shape.\n     AttrBuilder<(ins \"ArrayRef<int64_t>\":$shape,\n                      \"ArrayRef<unsigned>\":$sizePerThread,\n                      \"ArrayRef<unsigned>\":$order,\n                      \"unsigned\":$numWarps,\n-                     \"unsigned\":$threadsPerWarp), [{\n-      int rank = sizePerThread.size();\n-      unsigned remainingLanes = threadsPerWarp;\n-      unsigned remainingThreads = numWarps*threadsPerWarp;\n+                     \"unsigned\":$numThreadsPerWarp,\n+                     \"CTALayoutAttr\":$CTALayout), [{\n+      unsigned rank = sizePerThread.size();\n+      SmallVector<unsigned, 4> threadsPerWarp(rank);\n+      SmallVector<unsigned, 4> warpsPerCTA(rank);\n+      SmallVector<int64_t> shapePerCTA = getShapePerCTA(CTALayout.getCTASplitNum(), shape);\n+\n+      unsigned remainingLanes = numThreadsPerWarp;\n+      unsigned remainingThreads = numWarps * numThreadsPerWarp;\n       unsigned remainingWarps = numWarps;\n       unsigned prevLanes = 1;\n       unsigned prevWarps = 1;\n-      SmallVector<unsigned, 4> rankedThreadsPerWarp(rank);\n-      SmallVector<unsigned, 4> warpsPerCTA(rank);\n-      for (int _dim = 0; _dim < rank - 1; ++_dim) {\n-        int i = order[_dim];\n-        unsigned threadsPerCTA = std::clamp<unsigned>(remainingThreads, 1, shape[i] / sizePerThread[i]);\n-        rankedThreadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n-        warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / rankedThreadsPerWarp[i], 1, remainingWarps);\n+\n+      // starting from the contiguous dimension\n+      for (unsigned d = 0; d < rank - 1; ++d) {\n+        unsigned i = order[d];\n+        unsigned threadsPerCTA = std::clamp<unsigned>(remainingThreads, 1, shapePerCTA[i] / sizePerThread[i]);\n+        threadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n+        warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / threadsPerWarp[i], 1, remainingWarps);\n         remainingWarps /= warpsPerCTA[i];\n-        remainingLanes /= rankedThreadsPerWarp[i];\n+        remainingLanes /= threadsPerWarp[i];\n         remainingThreads /= threadsPerCTA;\n-        prevLanes *= rankedThreadsPerWarp[i];\n+        prevLanes *= threadsPerWarp[i];\n         prevWarps *= warpsPerCTA[i];\n       }\n+\n       // Expand the last dimension to fill the remaining lanes and warps\n-      rankedThreadsPerWarp[order[rank-1]] = threadsPerWarp / prevLanes;\n-      warpsPerCTA[order[rank-1]] = numWarps / prevWarps;\n+      threadsPerWarp[order[rank - 1]] = numThreadsPerWarp / prevLanes;\n+      warpsPerCTA[order[rank - 1]] = numWarps / prevWarps;\n \n-      return $_get(context, sizePerThread, rankedThreadsPerWarp, warpsPerCTA, order);\n+      return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order, CTALayout);\n+    }]>,\n \n+    AttrBuilder<(ins \"ArrayRef<int64_t>\":$shape,\n+                     \"ArrayRef<unsigned>\":$sizePerThread,\n+                     \"ArrayRef<unsigned>\":$order,\n+                     \"unsigned\":$numWarps,\n+                     \"unsigned\":$numThreadsPerWarp,\n+                     \"unsigned\":$numCTAs), [{\n+      unsigned rank = sizePerThread.size();\n+      SmallVector<unsigned, 4> CTAsPerCGA(rank);\n+      SmallVector<unsigned, 4> CTASplitNum(rank);\n+      ArrayRef<unsigned> CTAOrder = order;\n+\n+      unsigned remainingCTAs = numCTAs;\n+\n+      // starting from the most strided dimension\n+      for (int d = rank - 1; d >= 0; --d) {\n+        unsigned i = order[d];\n+        CTAsPerCGA[i] = std::clamp<unsigned>(remainingCTAs, 1, shape[i] / sizePerThread[i]);\n+        CTASplitNum[i] = CTAsPerCGA[i];\n+        remainingCTAs /= CTAsPerCGA[i];\n+      }\n+\n+      CTAsPerCGA[rank - 1] *= remainingCTAs; // wrap at CTA level\n+\n+      CTALayoutAttr CTALayout = CTALayoutAttr::get(context, CTAsPerCGA, CTASplitNum, CTAOrder);\n+      return get(context, shape, sizePerThread, order, numWarps, numThreadsPerWarp, CTALayout);\n     }]>\n   ];\n \n   let extraClassDeclaration = extraBaseClassDeclaration # [{\n     SliceEncodingAttr squeeze(int axis);\n   }];\n \n-  let parameters = (\n-    ins\n-    ArrayRefParameter<\"unsigned\">:$sizePerThread,\n-    ArrayRefParameter<\"unsigned\">:$threadsPerWarp,\n-    ArrayRefParameter<\"unsigned\">:$warpsPerCTA,\n-    // fastest-changing axis first\n-    ArrayRefParameter<\n-      \"unsigned\",\n-      \"order of axes by the rate of changing\"\n-    >:$order\n-    // These attributes can be inferred from the rest\n-    // ArrayRefParameter<\"unsigned\">:$sizePerWarp,\n-    // ArrayRefParameter<\"unsigned\">:$sizePerCTA\n-  );\n-\n   let hasCustomAssemblyFormat = 1;\n }\n \n@@ -381,13 +514,17 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     ins\n     \"unsigned\":$versionMajor,\n     \"unsigned\":$versionMinor,\n-    ArrayRefParameter<\"unsigned\">:$warpsPerCTA\n+    ArrayRefParameter<\"unsigned\">:$warpsPerCTA,\n+    \"CTALayoutAttr\":$CTALayout,\n+    ArrayRefParameter<\"unsigned\">:$instrShape\n   );\n \n   let builders = [\n     // Specially for MMAV1(Volta)\n     AttrBuilder<(ins \"int\":$versionMajor,\n                      \"int\":$numWarps,\n+                     \"CTALayoutAttr\":$CTALayout,\n+                     \"ArrayRef<unsigned>\":$instrShape,\n                      \"ArrayRef<int64_t>\":$shapeC,\n                      \"bool\":$isARow,\n                      \"bool\":$isBRow,\n@@ -401,7 +538,6 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n                          (isAVec4 * (1<<2)) |\\\n                          (isBVec4 * (1<<3));\n \n-\n       // TODO: Share code with\n       // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n       // rep,spw and fpw.\n@@ -426,12 +562,14 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n           wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shapeC[1] / spw[1]);\n       } while (wpt_nm1 != wpt);\n \n-      return $_get(context, versionMajor, versionMinor, wpt);\n+      return $_get(context, versionMajor, versionMinor, wpt, CTALayout, instrShape);\n     }]>,\n \n \n     AttrBuilder<(ins \"int\":$versionMajor,\n                      \"int\":$numWarps,\n+                     \"CTALayoutAttr\":$CTALayout,\n+                     \"ArrayRef<unsigned>\":$instrShape,\n                      \"ArrayRef<int64_t>\":$shapeA,\n                      \"ArrayRef<int64_t>\":$shapeB,\n                      \"ArrayRef<int64_t>\":$shapeC,\n@@ -441,15 +579,20 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n       assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n       bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n       bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n-      return get(context, versionMajor, numWarps, shapeC, isARow, isBRow, isAVec4, isBVec4, id);\n+      return get(context, versionMajor, numWarps, CTALayout, instrShape, shapeC, isARow, isBRow, isAVec4, isBVec4, id);\n     }]>\n   ];\n \n   let extraClassDeclaration = extraBaseClassDeclaration # [{\n     bool isVolta() const;\n     bool isAmpere() const;\n+    bool isHopper() const;\n+\n+    unsigned getElemsPerThreadOfOperand(int opIdx, ArrayRef<int64_t> shape) const;\n+\n     // Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n     std::tuple<bool, bool, bool, bool, int> decodeVoltaLayoutStates() const;\n+\n     // Number of bits in versionMinor to hold the ID of the MMA encoding instance.\n     // Here 5 bits can hold 32 IDs in a single module.\n     static constexpr int numBitsToHoldMmaV1ID{5};\n@@ -544,6 +687,4 @@ section 9.7.13.4.1 for more details.\n   }];\n }\n \n-\n-\n #endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUDialect.td", "status": "modified", "additions": 17, "deletions": 4, "changes": 21, "file_content_changes": "@@ -16,29 +16,42 @@ def TritonGPU_Dialect : Dialect {\n \n   let dependentDialects = [\n     \"triton::TritonDialect\",\n+    \"mlir::triton::nvgpu::NVGPUDialect\",\n     \"mlir::gpu::GPUDialect\",\n     \"tensor::TensorDialect\",\n   ];\n \n   let extraClassDeclaration = [{\n     static std::string getNumWarpsAttrName() { return \"triton_gpu.num-warps\"; }\n     static int getNumWarps(ModuleOp mod) {\n-      Attribute numWarps = mod->getDiscardableAttr(\"triton_gpu.num-warps\");\n-      if(!numWarps)\n+      if(!mod->hasAttr(\"triton_gpu.num-warps\"))\n         llvm::report_fatal_error(\n             \"TritonGPU module should contain a triton_gpu.num-warps attribute\");\n-      return numWarps.cast<IntegerAttr>().getInt();\n+      return mod->getAttr(\"triton_gpu.num-warps\").cast<IntegerAttr>().getInt();\n     }\n+    static int getNumCTAs(ModuleOp mod) {\n+      if(!mod->hasAttr(\"triton_gpu.num-ctas\"))\n+        llvm::report_fatal_error(\n+            \"TritonGPU module should contain a triton_gpu.num-ctas attribute\");\n+      return mod->getAttr(\"triton_gpu.num-ctas\").cast<IntegerAttr>().getInt();\n+    }\n+    static int getComputeCapability(ModuleOp mod) {\n+      if(!mod->hasAttr(\"triton_gpu.compute-capability\"))\n+        llvm::report_fatal_error(\n+            \"TritonGPU module should contain a triton_gpu.compute-capability attribute\");\n+      return mod->getAttrOfType<IntegerAttr>(\"triton_gpu.compute-capability\").getInt();\n+    }\n+    void registerTypes();\n \n     static std::string getThreadsPerWarpAttrName() { return \"triton_gpu.threads-per-warp\"; }\n+\n     static int getThreadsPerWarp(ModuleOp mod) {\n       Attribute threadsPerWarp = mod->getDiscardableAttr(\"triton_gpu.threads-per-warp\");\n       if(!threadsPerWarp) {\n         return 32;\n       }\n       return threadsPerWarp.cast<IntegerAttr>().getInt();\n     }\n-\n   }];\n \n   let useDefaultAttributePrinterParser = 1;"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 122, "deletions": 2, "changes": 124, "file_content_changes": "@@ -2,6 +2,7 @@\n #define TRITONGPU_OPS\n \n include \"triton/Dialect/TritonGPU/IR/TritonGPUDialect.td\"\n+include \"triton/Dialect/TritonGPU/IR/TritonGPUTypes.td\"\n include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td\"\n include \"mlir/Dialect/Arith/IR/ArithBase.td\"\n include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n@@ -46,6 +47,20 @@ def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n   }];\n }\n \n+def TTG_AsyncBulkWaitOp : TTG_Op<\"async_bulk_wait\"> {\n+  let summary = \"async bulk wait\";\n+\n+  let arguments = (ins I32Attr:$num);\n+\n+  let assemblyFormat = \"attr-dict\";\n+\n+  let extraClassDeclaration = [{\n+    static bool isSupported(int computeCapability) {\n+      return computeCapability >= 90;\n+    }\n+  }];\n+}\n+\n def TTG_AsyncCommitGroupOp : TTG_Op<\"async_commit_group\"> {\n   let summary = \"async commit group\";\n \n@@ -58,6 +73,18 @@ def TTG_AsyncCommitGroupOp : TTG_Op<\"async_commit_group\"> {\n   }];\n }\n \n+def TTG_AsyncBulkCommitGroupOp : TTG_Op<\"async_bulk_commit_group\"> {\n+  let summary = \"async bulk commit group\";\n+\n+  let assemblyFormat = \"attr-dict\";\n+\n+  let extraClassDeclaration = [{\n+    static bool isSupported(int computeCapability) {\n+      return computeCapability >= 90;\n+    }\n+  }];\n+}\n+\n \n // Port Arith_CmpIOp & Arith_CmpFOp & Std_SelectOp to TritonGPU.\n // This is needed because these ops don't\n@@ -106,6 +133,98 @@ def TTG_SelectOp : TTG_Op<\"select\", [Pure, Elementwise,\n   let results = (outs TT_Type:$result);\n }\n \n+// TODO[goostavz]: extract a base class for InsertSlice & InsertSliceAsync once the op definition is verified\n+def TTG_InsertSliceOp : TTG_Op<\"insert_slice\",\n+                               [AttrSizedOperandSegments,\n+                                ResultsAreSharedEncoding,\n+                                MemoryEffects<[MemRead, MemWrite]>,\n+                                TypesMatchWith<\"infer mask type from src type\",\n+                                               \"src\", \"mask\", \"getI1SameShape($_self)\",\n+                                               \"($_op.getOperands().size() <= 3) || std::equal_to<>()\">,\n+                                TypesMatchWith<\"infer other type from src type\",\n+                                               \"src\", \"other\", \"getPointeeType($_self)\",\n+                                               \"($_op.getOperands().size() <= 4) || std::equal_to<>()\">]> {\n+  let summary = \"insert slice\";\n+\n+  let description = [{\n+      This operation inserts a tensor `$src` into another tensor `$dst` as specified by the operation\u2019s\n+      `$index` argument and `$axis` attribute.\n+\n+      It returns a copy of `$dst` with the proper slice updated with the value of `$src`.\n+\n+      When converting from `tt.load` to `triton_gpu.insert_slice`, the `$evict`, `$cache`, and `$isVolatile` fields\n+      might be ignored on certain hardware. For example, on NVIDIA GPUs, the cache policy is determined by the backend,\n+      and `$evict` and `$isVolatile` are ignored because they apply to L1 cache only.\n+\n+      The insert_slice operation supports the following arguments:\n+\n+      * src: the tensor that is inserted.\n+      * dst: the tensor into which the `$src` tensor is inserted.\n+      * index: the index of the `$src` tensor at the given `$axis` from which the `$dst` tensor is inserted into\n+      * mask: optional tensor-rank number of boolean masks which specify which\n+              elements of the `$src` tensor are inserted into the `$dst` tensor.\n+      * other: optional tensor-rank number of other tensors which specify what\n+              values are inserted into the `$dst` tensor if the corresponding\n+              element of the `$mask` tensor is false.\n+\n+      ttgpu.load_tile_async depracate\n+      triton_gpu.insert_slice might be further lowered into triton_gpu_async for different hardware implementations\n+\n+      like tt.load, ttgpu.insert_slice/insert_slice_async has two modes up to the type of src\n+      mode 1: ptr/src is a tensor of pointers\n+      mode 2: ptr/src is a tensor pointer\n+\n+      Some typical lowering paths are:\n+      in case the load is pipelined by the pipeline pass( load is inside kBlock loop, which means \"pipeline pass):\n+        Load from global + store to shared : tt.load(mode 1) -(tt->ttgpu+Coalesce)-> tt.load(mode 1) -(Pipeline)-> ttgpu.insert_slice(mode 1)\n+        Non-bulk cp.async                  : tt.load(mode 1) -(tt->ttgpu+Coalesce)-> tt.load(mode 1) -(Pipeline)-> ttgpu.insert_slice(mode 1) -(MaterializeLoad)> ttgpu.insert_slice_async(mode 1) + ttgpu.await-> llvm\n+        TMA load : tt.load(mode 2) -(tt->ttgpu+Coalesce)-> tt.load(mode 2) -(Pipeline)-> ttgpu.insert_slice(mode 2) -(MaterializeLoad)> ttgpu.insert_slice_async_v2(mode 2) + ttgpu.await-> llvm\n+\n+      otherwise:\n+        Load from global + store to shared : tt.load(mode 1) -(tt->ttgpu+Coalesce)-> tt.load(mode 1)\n+        Non-bulk cp.async                  : tt.load(mode 1) -(tt->ttgpu+Coalesce)-> tt.load(mode 1) -> ... -(MaterializeLoad)-> ttgpu.insert_slice_async(mode 1) + ttgpu.await -> llvm\n+        TMA load                           : tt.load(mode 2) -(tt->ttgpu+Coalesce)-> tt.load(mode 2) -> ... -(MaterializeLoad)-> ttgpu.insert_slice_async(mode 2) + ttgpu.await -> llvm\n+\n+      Example:\n+\n+      ```\n+      %1 = triton_gpu.alloc_tensor : tensor<2x32xf32>\n+      %2 = triton_gpu.insert_slice %0, %1, %index { axis = 0 } : tensor<32x!tt.ptr<f32>, #AL> -> tensor<2x32xf32, #A>\n+      ```\n+  }];\n+\n+  let arguments = (ins TT_PtrLike:$src, TT_Tensor:$dst, I32:$index,\n+                       Optional<I1Tensor>:$mask, Optional<TT_Type>:$other,\n+                       TT_CacheModifierAttr:$cache, TT_EvictionPolicyAttr:$evict,\n+                       BoolAttr:$isVolatile, I32Attr:$axis);\n+\n+  let builders = [\n+      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$index,\n+                     \"triton::CacheModifier\":$cache,\n+                     \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile, \"int\":$axis)>,\n+      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$index, \"Value\":$mask,\n+                     \"triton::CacheModifier\":$cache,\n+                     \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile, \"int\":$axis)>,\n+      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$index,\n+                     \"Value\":$mask, \"Value\":$other,\n+                     \"triton::CacheModifier\":$cache,\n+                     \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile, \"int\":$axis)>,\n+  ];\n+\n+  let results = (outs TT_Tensor:$result);\n+\n+  let extraClassDeclaration = [{\n+    static DenseSet<unsigned> getEligibleLoadByteWidth(int computeCapability) {\n+      DenseSet<unsigned> validLoadBytes;\n+      if (computeCapability >= 80) {\n+        validLoadBytes = {4, 8, 16};\n+      }\n+      return validLoadBytes;\n+    }\n+  }];\n+\n+  let hasCustomAssemblyFormat = 1;\n+}\n \n \n def TTG_ExtractSliceOp : TTG_Op<\"extract_slice\",\n@@ -173,7 +292,8 @@ def TTG_ExtractSliceOp : TTG_Op<\"extract_slice\",\n def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n                                     [AttrSizedOperandSegments,\n                                      ResultsAreSharedEncoding,\n-                                     MemoryEffects<[MemRead]>,\n+                                     // TODO: Check if MemWrite will degrade performance of non-warp-specialized kernel\n+                                     MemoryEffects<[MemRead, MemWrite]>,\n                                      TypesMatchWith<\"infer mask type from src type\",\n                                                     \"src\", \"mask\", \"getI1SameShape($_self)\",\n                                                     \"($_op.getOperands().size() <= 3) || std::equal_to<>()\">,\n@@ -219,7 +339,7 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n       ```\n   }];\n \n-  let arguments = (ins TT_PtrTensor:$src, TT_Tensor:$dst, I32:$index,\n+  let arguments = (ins TT_PtrLike:$src, TT_Tensor:$dst, I32:$index,\n                        Optional<I1Tensor>:$mask, Optional<TT_Type>:$other,\n                        TT_CacheModifierAttr:$cache, TT_EvictionPolicyAttr:$evict,\n                        BoolAttr:$isVolatile, I32Attr:$axis);"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUTypes.td", "status": "added", "additions": 26, "deletions": 0, "changes": 26, "file_content_changes": "@@ -0,0 +1,26 @@\n+#ifndef TRITONGPU_TYPES\n+#define TRITONGPU_TYPES\n+\n+include \"triton/Dialect/TritonGPU/IR/TritonGPUDialect.td\"\n+include \"mlir/IR/AttrTypeBase.td\"\n+\n+class TTG_TypeDef<string name, string _mnemonic, list<Trait> traits = []>\n+    : TypeDef<TritonGPU_Dialect, name, traits> {\n+    let mnemonic = _mnemonic;\n+}\n+\n+def TTG_TokenType : TTG_TypeDef<\"Token\", \"token\"> {\n+  let parameters = (ins \"int32_t\":$type);\n+\n+  let builders = [\n+    TypeBuilder<(ins \"unsigned\":$type), [{\n+      return $_get($_ctxt, type);\n+    }]>\n+  ];\n+\n+  let hasCustomAssemblyFormat = 1;\n+\n+  let skipDefaultBuilders = 1;\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Types.h", "status": "added", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -0,0 +1,10 @@\n+#ifndef TRITONGPU_IR_TYPES_H_\n+#define TRITONGPU_IR_TYPES_H_\n+\n+#include \"mlir/IR/TypeSupport.h\"\n+#include \"mlir/IR/Types.h\"\n+\n+#define GET_TYPEDEF_CLASSES\n+#include \"triton/Dialect/TritonGPU/IR/Types.h.inc\"\n+\n+#endif // TRITON_IR_TYPES_H_"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -2,9 +2,14 @@\n #define TRITON_DIALECT_TRITONGPU_TRANSFORMS_PASSES_H_\n \n #include \"mlir/Pass/Pass.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n \n namespace mlir {\n-std::unique_ptr<Pass> createTritonGPUPipelinePass(int numStages = 2);\n+\n+std::unique_ptr<Pass> createTritonGPUPipelinePass(int numStages = 3,\n+                                                  int numWarps = 4,\n+                                                  int numCTAs = 1,\n+                                                  int computeCapability = 80);\n \n std::unique_ptr<Pass>\n createTritonGPUAccelerateMatmulPass(int computeCapability = 80);\n@@ -25,6 +30,8 @@ std::unique_ptr<Pass> createTritonGPUVerifier();\n \n std::unique_ptr<Pass> createTritonGPUOptimizeDotOperandsPass();\n \n+std::unique_ptr<Pass> createTritonGPUOptimizeEpiloguePass();\n+\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\""}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 28, "deletions": 2, "changes": 30, "file_content_changes": "@@ -14,13 +14,23 @@ def TritonGPUPipeline : Pass<\"tritongpu-pipeline\", \"mlir::ModuleOp\"> {\n   let constructor = \"mlir::createTritonGPUPipelinePass()\";\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\",\n                            \"mlir::scf::SCFDialect\",\n                            \"mlir::arith::ArithDialect\"];\n \n   let options = [\n     Option<\"numStages\", \"num-stages\",\n-           \"int32_t\", /*default*/\"2\",\n-           \"number of pipeline stages\">\n+           \"int32_t\", /*default*/\"3\",\n+           \"number of pipeline stages\">,\n+    Option<\"numWarps\", \"num-warps\",\n+           \"int32_t\", /*default*/\"4\",\n+           \"number of warps per block\">,\n+    Option<\"numCTAs\", \"num-ctas\",\n+           \"int32_t\", /*default*/\"1\",\n+           \"number of CTAs per CGA\">,\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n   ];\n }\n \n@@ -50,6 +60,7 @@ def TritonGPUAccelerateMatmul : Pass<\"tritongpu-accelerate-matmul\", \"mlir::Modul\n   let constructor = \"mlir::createTritonGPUAccelerateMatmulPass()\";\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\",\n                            \"mlir::triton::TritonDialect\"];\n \n   let options = [\n@@ -70,6 +81,7 @@ def TritonGPUOptimizeDotOperands : Pass<\"tritongpu-optimize-dot-operands\", \"mlir\n   let constructor = \"mlir::createTritonGPUOptimizeDotOperandsPass()\";\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\",\n                            \"mlir::triton::TritonDialect\"];\n }\n \n@@ -96,6 +108,20 @@ def TritonGPURemoveLayoutConversions : Pass<\"tritongpu-remove-layout-conversions\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::triton::TritonDialect\"];\n+\n+}\n+\n+def TritonGPUOptimizeEpilogue : Pass<\"tritongpu-optimize-epilogue\", \"mlir::ModuleOp\"> {\n+  let summary = \"Optimize epilogue: (1) Store accumulators directly without going thorough SMEM in epilogue.\";\n+\n+  let description = [{\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUOptimizeEpiloguePass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+\n }\n \n def TritonGPUReorderInstructions: Pass<\"tritongpu-reorder-instructions\", \"mlir::ModuleOp\"> {"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -13,15 +13,17 @@ namespace mlir {\n \n class TritonGPUTypeConverter : public TypeConverter {\n public:\n-  TritonGPUTypeConverter(MLIRContext *context, int numWarps,\n-                         int threadsPerWarp);\n+  TritonGPUTypeConverter(MLIRContext *context, int numWarps, int threadsPerWarp,\n+                         int numCTAs);\n   int getNumWarps() const { return numWarps; }\n   int getThreadsPerWarp() const { return threadsPerWarp; }\n+  int getNumCTAs() const { return numCTAs; }\n \n private:\n   MLIRContext *context;\n   int numWarps;\n   int threadsPerWarp;\n+  int numCTAs;\n };\n \n class TritonGPUConversionTarget : public ConversionTarget {"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 108, "deletions": 0, "changes": 108, "file_content_changes": "@@ -10,8 +10,99 @@\n \n namespace mlir {\n \n+namespace triton {\n+class LoadOp;\n+class StoreOp;\n+class FuncOp;\n+namespace gpu {\n+class SharedEncodingAttr;\n+}\n+} // namespace triton\n+\n LogicalResult fixupLoops(ModuleOp mod);\n \n+SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n+                                                const ArrayRef<int64_t> &shape,\n+                                                RankedTensorType type);\n+\n+/// Returns true if the Load is for TMA\n+bool isLoadFromTensorPtr(triton::LoadOp op);\n+\n+/// Returns true if the store is for TMA\n+bool isStoreToTensorPtr(triton::StoreOp op);\n+\n+/// Return the first consumer of v\n+Operation *getFirstUser(Value v);\n+\n+/// Return the proper SharedEncodingAttr according to shape/order\n+triton::gpu::SharedEncodingAttr getSharedEncoding(RankedTensorType tensorTy);\n+\n+/* Dump Triton IR in graphviz dot format.\n+ *\n+ * You can override `onValue` and `onOperation` in a subclass to mark\n+ * specific Values and Operations. The below subclass\n+ * GraphLayoutMarker is an example.\n+ *\n+ * Default NodeInfo for Value nodes:\n+ *   {{\"shape\": \"box\"},\n+ *    {\"style\", \"filled\"},\n+ *    {\"fillcolor\", \"white\"},\n+ *    {\"label\", shapeStr}}\n+ *\n+ * Default NodeInfo for Operation nodes:\n+ *   {{\"shape\": \"ellipse\"},\n+ *    {\"style\", \"filled\"},\n+ *    {\"fillcolor\", \"white\"},\n+ *    {\"label\", operationName}}\n+ *\n+ * If the key \"label\" is not set by `onValue` or `onOperation`, default labels\n+ * will be generated. For Value node, the default label is the shape string and\n+ * for Operation node, it is the operation name.\n+ *\n+ * Reference:\n+ *   https://graphviz.org/doc/info/shapes.html\n+ *   https://graphviz.org/doc/info/colors.html\n+ *\n+ * Usage:\n+ *   C++:   GraphDumper().dumpToFile(func, \"func.dot\");\n+ *   Shell: dot -Tjpg func.dot -o func.jpg\n+ */\n+class GraphDumper {\n+public:\n+  using NodeInfo = std::map<std::string, std::string>;\n+\n+  // Override this function to mark specific Values\n+  virtual NodeInfo onValue(Value value) const;\n+  // Override this function to mark specific Operations\n+  virtual NodeInfo onOperation(Operation *op) const;\n+\n+  std::string dump(triton::FuncOp func) const;\n+  void dumpToFile(triton::FuncOp func, const std::string &filename) const;\n+\n+protected:\n+  std::string getShapeStr(const Type &type) const;\n+\n+  std::string getUniqueId(Value value) const;\n+  std::string getUniqueId(Operation *op) const;\n+\n+  std::string emitNode(const std::string &id, const NodeInfo style) const;\n+  std::string emitEdge(const std::string &srcId,\n+                       const std::string &destId) const;\n+\n+  std::string emitValueNode(Value value) const;\n+  std::string emitOperationNode(Operation *op) const;\n+};\n+\n+/* A subclass of GraphDumper that marks different layout kinds in different\n+ * colors.*/\n+class GraphLayoutMarker : public GraphDumper {\n+public:\n+  NodeInfo onValue(Value value) const override;\n+\n+protected:\n+  std::string getColor(const Type &type) const;\n+};\n+\n // TODO: Interface\n LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n                              Attribute &ret);\n@@ -38,6 +129,23 @@ void rematerializeConversionChain(\n LogicalResult canMoveOutOfLoop(BlockArgument arg,\n                                SmallVector<Operation *> &cvts);\n \n+// Convert an \\param index to a multi-dim coordinate given \\param shape and\n+// \\param order.\n+SmallVector<Value> delinearize(OpBuilder &b, Location loc, Value linear,\n+                               ArrayRef<unsigned> shape,\n+                               ArrayRef<unsigned> order);\n+\n+SmallVector<Value> delinearize(OpBuilder &b, Location loc, unsigned linear,\n+                               ArrayRef<unsigned> shape);\n+\n+SmallVector<Value> delinearize(OpBuilder &b, Location loc, Value linear,\n+                               ArrayRef<unsigned> shape);\n+Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n+                ArrayRef<unsigned> shape, ArrayRef<unsigned> order);\n+\n+Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n+                ArrayRef<unsigned> shape);\n+\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+add_subdirectory(IR)\n+add_subdirectory(Transforms)"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/CMakeLists.txt", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+set(LLVM_TARGET_DEFINITIONS TritonNvidiaGPUOps.td)\n+mlir_tablegen(Dialect.h.inc -gen-dialect-decls -dialect=triton_nvidia_gpu)\n+mlir_tablegen(Dialect.cpp.inc -gen-dialect-defs -dialect=triton_nvidia_gpu)\n+mlir_tablegen(Ops.h.inc -gen-op-decls)\n+mlir_tablegen(Ops.cpp.inc -gen-op-defs)\n+mlir_tablegen(Types.h.inc -gen-typedef-decls -typedefs-dialect=triton_nvidia_gpu)\n+mlir_tablegen(Types.cpp.inc -gen-typedef-defs -typedefs-dialect=triton_nvidia_gpu)\n+add_public_tablegen_target(TritonNvidiaGPUTableGen)\n+\n+set(LLVM_TARGET_DEFINITIONS TritonNvidiaGPUAttrDefs.td)\n+mlir_tablegen(TritonNvidiaGPUAttrDefs.h.inc -gen-attrdef-decls)\n+mlir_tablegen(TritonNvidiaGPUAttrDefs.cpp.inc -gen-attrdef-defs)\n+mlir_tablegen(OpsEnums.h.inc -gen-enum-decls)\n+mlir_tablegen(OpsEnums.cpp.inc -gen-enum-defs)\n+add_public_tablegen_target(TritonNvidiaGPUAttrDefsIncGen)"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/Dialect.h", "status": "added", "additions": 46, "deletions": 0, "changes": 46, "file_content_changes": "@@ -0,0 +1,46 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITON_DIALECT_TRITONNVIDIAGPU_IR_DIALECT_H_\n+#define TRITON_DIALECT_TRITONNVIDIAGPU_IR_DIALECT_H_\n+\n+#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"mlir/IR/BuiltinOps.h\"\n+#include \"mlir/IR/Dialect.h\"\n+\n+// TritonNvidiaGPU depends on Triton\n+#include \"triton/Dialect/NVGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h.inc\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Traits.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Types.h\"\n+\n+#define GET_ATTRDEF_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUAttrDefs.h.inc\"\n+\n+#define GET_OP_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Ops.h.inc\"\n+\n+#endif // TRITON_DIALECT_TRITONNVIDIAGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/Traits.h", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITON_NVIDIA_GPU_IR_TRAITS_H_\n+#define TRITON_NVIDIA_GPU_IR_TRAITS_H_\n+\n+#include \"mlir/IR/OpDefinition.h\"\n+\n+#include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+\n+namespace mlir {\n+namespace OpTrait {\n+\n+// These functions are out-of-line implementations of the methods in the\n+// corresponding trait classes.  This avoids them being template\n+// instantiated/duplicated.\n+namespace impl {\n+LogicalResult verifySource1IsSharedEncoding(Operation *op);\n+} // namespace impl\n+\n+template <typename ConcreteType>\n+class Source1IsSharedEncoding\n+    : public TraitBase<ConcreteType, Source1IsSharedEncoding> {\n+public:\n+  static LogicalResult verifyTrait(Operation *op) {\n+    return impl::verifySource1IsSharedEncoding(op);\n+  }\n+};\n+} // namespace OpTrait\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUAttrDefs.td", "status": "added", "additions": 29, "deletions": 0, "changes": 29, "file_content_changes": "@@ -0,0 +1,29 @@\n+// Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining\n+// a copy of this software and associated documentation files\n+// (the \"Software\"), to deal in the Software without restriction,\n+// including without limitation the rights to use, copy, modify, merge,\n+// publish, distribute, sublicense, and/or sell copies of the Software,\n+// and to permit persons to whom the Software is furnished to do so,\n+// subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be\n+// included in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+// EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+// IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+// CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+// TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+// SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+#ifndef TRITONNVIDIAGPU_ATTRDEFS\n+#define TRITONNVIDIAGPU_ATTRDEFS\n+\n+include \"mlir/IR/AttrTypeBase.td\"\n+include \"triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUDialect.td\"\n+include \"triton/Dialect/Triton/IR/TritonInterfaces.td\"\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUDialect.td", "status": "added", "additions": 82, "deletions": 0, "changes": 82, "file_content_changes": "@@ -0,0 +1,82 @@\n+// Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining\n+// a copy of this software and associated documentation files\n+// (the \"Software\"), to deal in the Software without restriction,\n+// including without limitation the rights to use, copy, modify, merge,\n+// publish, distribute, sublicense, and/or sell copies of the Software,\n+// and to permit persons to whom the Software is furnished to do so,\n+// subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be\n+// included in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+// EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+// IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+// CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+// TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+// SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+#ifndef TRITONNVIDIAGPU_DIALECT\n+#define TRITONNVIDIAGPU_DIALECT\n+\n+include \"mlir/IR/OpBase.td\"\n+\n+def TritonNvidiaGPU_Dialect : Dialect {\n+  let name = \"triton_nvidia_gpu\";\n+\n+  let cppNamespace = \"::mlir::triton::nvidia_gpu\";\n+\n+  let hasOperationAttrVerify = 1;\n+\n+  let description = [{\n+    Triton Nvidia GPU Dialect.\n+  }];\n+\n+  let dependentDialects = [\n+    \"triton::TritonDialect\",\n+    \"triton::gpu::TritonGPUDialect\",\n+    \"mlir::triton::nvgpu::NVGPUDialect\",\n+    \"mlir::gpu::GPUDialect\",\n+    \"tensor::TensorDialect\",\n+  ];\n+\n+  let extraClassDeclaration = [{\n+    static std::string getNumWarpsAttrName() { return \"triton_gpu.num-warps\"; }\n+    static int getNumWarps(ModuleOp mod) {\n+      if(!mod->hasAttr(\"triton_gpu.num-warps\"))\n+        llvm::report_fatal_error(\n+            \"TritonGPU module should contain a triton_gpu.num-warps attribute\");\n+      return mod->getAttr(\"triton_gpu.num-warps\").cast<IntegerAttr>().getInt();\n+    }\n+    static int getNumCTAs(ModuleOp mod) {\n+      if(!mod->hasAttr(\"triton_gpu.num-ctas\"))\n+        llvm::report_fatal_error(\n+            \"TritonGPU module should contain a triton_gpu.num-ctas attribute\");\n+      return mod->getAttr(\"triton_gpu.num-ctas\").cast<IntegerAttr>().getInt();\n+    }\n+    static int getComputeCapability(ModuleOp mod) {\n+      if(!mod->hasAttr(\"triton_gpu.compute-capability\"))\n+        llvm::report_fatal_error(\n+            \"TritonGPU module should contain a triton_gpu.compute-capability attribute\");\n+      return mod->getAttrOfType<IntegerAttr>(\"triton_gpu.compute-capability\").getInt();\n+    }\n+    void registerTypes();\n+\n+    // Warp specialization related:\n+    static std::string getWSSupportedAttrName() { return \"triton_gpu.enable-warp-specialization\"; }\n+    static int getWSSupportedAttr(ModuleOp mod) {\n+      auto name = getWSSupportedAttrName();\n+      if (!mod->hasAttr(name)) return 0;\n+      return mod->getAttrOfType<IntegerAttr>(name).getInt();\n+    }\n+  }];\n+\n+  let useDefaultTypePrinterParser = 1;\n+}\n+\n+include \"triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUTypes.td\"\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUOps.td", "status": "added", "additions": 386, "deletions": 0, "changes": 386, "file_content_changes": "@@ -0,0 +1,386 @@\n+// Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining\n+// a copy of this software and associated documentation files\n+// (the \"Software\"), to deal in the Software without restriction,\n+// including without limitation the rights to use, copy, modify, merge,\n+// publish, distribute, sublicense, and/or sell copies of the Software,\n+// and to permit persons to whom the Software is furnished to do so,\n+// subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be\n+// included in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+// EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+// IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+// CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+// TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+// SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+#ifndef TRITONNVIDIAGPU_OPS\n+#define TRITONNVIDIAGPU_OPS\n+\n+include \"triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUDialect.td\"\n+include \"triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUTypes.td\"\n+include \"triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUAttrDefs.td\"\n+include \"mlir/Dialect/Arith/IR/ArithBase.td\"\n+include \"triton/Dialect/Triton/IR/TritonTypes.td\"\n+include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n+include \"mlir/IR/OpBase.td\"\n+include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n+include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n+include \"mlir/Interfaces/DestinationStyleOpInterface.td\"\n+include \"mlir/Interfaces/ViewLikeInterface.td\"\n+\n+def Source1IsSharedEncoding: NativeOpTrait<\"Source1IsSharedEncoding\">;\n+\n+def ResultsAreSharedEncoding: NativeOpTrait<\"ResultsAreSharedEncoding\">;\n+\n+class TTNG_Op<string mnemonic, list<Trait> traits = []> :\n+    Op<TritonNvidiaGPU_Dialect, mnemonic, traits>;\n+\n+// --------------------------------------------------------------------------------------------------\n+// MBarrier related Ops:\n+// 1, These mbarrier commands are currently not needed, and not taken into consideration:\n+//    (1), mbarrier.expect_tx\n+//    (2), mbarrier.arrive_drop\n+//    (3), mbarrier.complete_tx\n+//    (4), mbarrier.inval\n+//\n+// 2, The mbarriers is supported to be created in vector, and accessed in seperate via tensor.extract.\n+//    The mbarriers created in vector will have counters initialized in the same configuration. A\n+//    typical example to demonstrate this:\n+//\n+//    %1 = triton_nvidia_gpu.alloc_mbarrier { count = 1 } : tensor<4x!tt.ptr<i64>>\n+//    scf.for %iv = %lb to %ub step %step iter_args() -> () {\n+//      %buffer_id = arith.remi %iv, %c4 : i32\n+//      %2 = triton_nvidia_gpu.extract_mbarrier %1[%buffer_id] : tensor<4xi64>, i32 -> !tt.ptr<i64>\n+//      triton_nvidia_gpu.mbarrier_arrive %2 {expectTx = 2048} : !tt.ptr<i64> -> ()\n+//    }\n+//    ...\n+//    scf.for %iv = %lb to %ub step %step iter_args() -> () {\n+//      %buffer_id = arith.remi %iv, %c4 : i32\n+//      %2 = triton_nvidia_gpu.extract_mbarrier %1[%buffer_id] : tensor<4xi64>, i32 -> !tt.ptr<i64>\n+//      triton_nvidia_gpu.mbarrier_wait %2, %c0 : !tt.ptr<i64>, i1 -> ()\n+//    }\n+\n+def TTNG_AllocMBarrierOp : TTNG_Op<\"alloc_mbarrier\", [MemoryEffects<[MemAlloc]>]> {\n+  let summary = \"allocate a vector of mbarriers\";\n+\n+  let description = [{\n+    Allocate and initialize a vector of mbarriers. The size of the vector is implied in the returned type.\n+    Each mbarrier is initialized as:\n+    1, the current phase initialized to 0.\n+    2, the expected arrival count initialized to 'count'.\n+    3, the pending arrival count initialized to 'count'.\n+    4, the tx-count initialized to 0.\n+\n+    Example:\n+\n+    case a. when created in vector:\n+    %1 = triton_nvidia_gpu.alloc_mbarrier { count = 1 } : tensor<4xi64>\n+\n+    case b. when created in scalar:\n+    %1 = triton_nvidia_gpu.alloc_mbarrier { count = 1 } : !tt.ptr<i64>\n+\n+  }];\n+\n+  let assemblyFormat = [{attr-dict `:` type($result)}];\n+\n+  let arguments = (ins I32Attr:$count);\n+\n+  let results = (outs AnyTypeOf<[TT_Ptr, I64Tensor]>:$result);\n+}\n+\n+def TTNG_ExtractMBarrierOp : TTNG_Op<\"extract_mbarrier\", [Pure]> {\n+  let summary = \"extract a mbarrier from a vector of mbarriers\";\n+\n+  let description = [{\n+    Extract a mbarrier from a vector of mbarriers\n+\n+    Example:\n+\n+    %1 = triton_nvidia_gpu.extract_mbarrier %mbarriers[%idx] : tensor<4xi64>, index -> !tt.ptr<i64>\n+\n+  }];\n+\n+  let assemblyFormat = \"$tensor `[` $index `]` attr-dict `:` type($tensor) `,` type($index) `->` type($result)\";\n+\n+  let arguments = (ins I64Tensor:$tensor, I32:$index);\n+\n+  let results = (outs TT_Ptr:$result);\n+}\n+\n+def TTNG_MBarrierWaitOp : TTNG_Op<\"mbarrier_wait\", [MemoryEffects<[MemRead, MemWrite]>]> {\n+  let summary = \"mbarrier wait\";\n+\n+  let description = [{\n+    This operation defining the waiting action for a mbarrier.\n+    The subsequent operations should not execute until this operation completes waiting.\n+\n+    Example:\n+\n+    triton_nvidia_gpu.mbarrier_wait %0, %1 : !tt.ptr<i64>\n+\n+  }];\n+\n+  let arguments = (ins TT_Ptr:$mbarrier, I1: $phase);\n+\n+  let assemblyFormat = \"$mbarrier `,` $phase attr-dict `:` type($mbarrier)\";\n+}\n+\n+def TTNG_MBarrierArriveOp : TTNG_Op<\"mbarrier_arrive\", [AttrSizedOperandSegments,\n+                                                      MemoryEffects<[MemWrite]>]> {\n+  let summary = \"mbarrier arrive\";\n+\n+  let description = [{\n+    This operation defining the arriving action for a mbarrier.\n+    txCount:\n+        An optional attribute that set tx-count. This Op will be lowered into\n+        mbarrier.arrive.expect_tx if the optional attribute exist.\n+    trackAsyncOp:\n+        If true, this op will be lowered into cp.async.mbarrier.arrive.noinc.\n+    pred:\n+        Only perform arrive action when pred is true.\n+    remoteCtaId:\n+        if set, perform an remote arrive action.\n+\n+    Example:\n+\n+    triton_nvidia_gpu.mbarrier_arrive %0 {trackAsyncOp = false} : !tt.ptr<i64>\n+\n+  }];\n+\n+  let arguments = (ins TT_Ptr:$mbarrier,\n+                       Optional<I1>:$pred,\n+                       Optional<I32>:$remoteCtaId,\n+                       I1Attr: $trackAsyncOp,\n+                       DefaultValuedAttr<I32Attr, \"0\">: $txCount\n+                  );\n+\n+  let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n+}\n+\n+def TTNG_FenceAsyncSharedOp : TTNG_Op<\"fence_async_shared\"> {\n+  let arguments = (ins BoolAttr:$bCluster);\n+\n+  let summary = \"fence proxy async\";\n+\n+  let assemblyFormat = \"attr-dict\";\n+\n+  let extraClassDeclaration = [{\n+    static bool isSupported(int computeCapability) {\n+      return computeCapability >= 90;\n+    }\n+  }];\n+}\n+\n+// TODO[goostavz]: ThreadId & ClusterCTAId should not be exposed to\n+//                 ttgpu level. Remove them when async dialect is ready.\n+def TTNG_GetThreadIdOp : TTNG_Op<\"get_thread_id\", [Pure]> {\n+  let description = [{\n+    Returns the one dimensional threadId.\n+  }];\n+\n+  let results = (outs I32:$result);\n+  let assemblyFormat = \"attr-dict `:` type($result)\";\n+}\n+\n+def TTNG_GetClusterCTAIdOp : TTNG_Op<\"get_cluster_cta_id\", [Pure]> {\n+  let description = [{\n+    Returns the one dimensional cluster_cta_id.\n+  }];\n+\n+  let results = (outs I32:$result);\n+  let assemblyFormat = \"attr-dict `:` type($result)\";\n+}\n+\n+def TTNG_NamedBarrierArriveOp : TTNG_Op<\"bar_arrive\", []> {\n+  let summary = \"named barrier arrive\";\n+\n+  let arguments = (ins I32:$bar, I32: $numThreads);\n+\n+  let assemblyFormat = \"$bar `,` $numThreads attr-dict `:` type(operands)\";\n+}\n+\n+def TTNG_NamedBarrierWaitOp : TTNG_Op<\"bar_wait\", []> {\n+  let summary = \"named barrier wait\";\n+\n+  let arguments = (ins I32:$bar, I32: $numThreads);\n+\n+  let assemblyFormat = \"$bar `,` $numThreads attr-dict `:` type(operands)\";\n+}\n+\n+def TTNG_InsertSliceAsyncV2Op : TTNG_Op<\"insert_slice_async_v2\",\n+                                      [AttrSizedOperandSegments,\n+                                       ResultsAreSharedEncoding,\n+                                       // TODO: Check if MemWrite will degrade performance of non-warp-specialized kernel\n+                                       MemoryEffects<[MemRead, MemWrite]>]> {\n+\n+  let arguments = (ins AnyTypeOf<[TT_Ptr, TT_PtrTensor]>:$src, TT_Tensor:$dst,\n+                       I32:$index, TT_Ptr:$mbar,\n+                       Optional<AnyTypeOf<[I1Tensor, I1]>>:$mask, Optional<TT_Type>:$other,\n+                       TT_CacheModifierAttr:$cache, TT_EvictionPolicyAttr:$evict,\n+                       BoolAttr:$isVolatile, I32Attr:$axis);\n+\n+  let results = (outs TT_Tensor:$result);\n+\n+  let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($result)\";\n+}\n+\n+// TODO: the abstraction of barriers in ttgpu level is pending, will revisit later\n+// def TTNG_AwaitOp : TTNG_Op<\"await\", []> {\n+//   let arguments = (ins TTNG_TokenType:$token);\n+//   let assemblyFormat = \"$token attr-dict `:` type($token)\";\n+// }\n+\n+def TTNG_ClusterArriveOp : TTNG_Op<\"cluster_arrive\", []> {\n+  let arguments = (ins I1Attr:$relaxed);\n+  let assemblyFormat = \"attr-dict\";\n+}\n+\n+def TTNG_ClusterWaitOp : TTNG_Op<\"cluster_wait\", []> {\n+  let assemblyFormat = \"attr-dict\";\n+}\n+\n+//\n+// DotAsync Op\n+//\n+def TTNG_DotAsyncOp : TTNG_Op<\"dot_async\", [Pure,\n+                             DeclareOpInterfaceMethods<InferTypeOpInterface>,\n+                             TypesMatchWith<\"result's type matches accumulator's type\",\n+                                            \"d\", \"c\", \"$_self\">]> {\n+    let summary = \"dot async\";\n+\n+    let description = [{\n+        $d = matrix_multiply($a, $b) + $c\n+    }];\n+\n+    let arguments = (ins TT_FpIntTensor:$a, TT_FpIntTensor:$b, TT_FpIntTensor:$c, BoolAttr:$allowTF32);\n+\n+    let results = (outs TT_FpIntTensor:$d);\n+\n+    let assemblyFormat = \"$a`,` $b`,` $c attr-dict `:` type($a) `*` type($b) `->` type($d)\";\n+}\n+\n+def TTNG_DotWaitOp : TTNG_Op<\"dot_wait\", []> {\n+  let summary = \"dot wait\";\n+\n+  let description = [{\n+    This operation defining the waiting action for a async dot, MMAv3 .e.g.\n+    The subsequent operations should not execute until this operation completes waiting.\n+  }];\n+\n+  let arguments = (ins I32Attr:$pendings);\n+\n+  let assemblyFormat = \"attr-dict\";\n+}\n+\n+def TTNG_StoreAsyncOp : TTNG_Op<\"store_async\",\n+                              [Source1IsSharedEncoding,\n+                               MemoryEffects<[MemWrite]>]> {\n+  let summary = \"store asynchronous by a tensor pointer\";\n+  let arguments = (ins TT_TensorPtr:$dst, TT_Tensor:$src,\n+                       DefaultValuedAttr<TT_CacheModifierAttr, \"triton::CacheModifier::NONE\">:$cache);\n+  let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n+}\n+\n+def TTNG_GetAgentIdOp : TTNG_Op<\"get_agent_id\", [Pure]> {\n+  let results = (outs I32:$result);\n+\n+  let builders = [OpBuilder<(ins)>];\n+\n+  let assemblyFormat = \"attr-dict `:` type($result)\";\n+}\n+\n+//\n+// Token\n+//\n+\n+def TTNG_CreateTokenOp : TTNG_Op<\"create_token\"> {\n+  let results = (outs TensorOf<[TTNG_TokenType]>:$result);\n+\n+  let arguments = (ins I32Attr:$num);\n+\n+  let builders = [OpBuilder<(ins \"uint32_t\":$num)>];\n+\n+  let assemblyFormat = \"attr-dict `:` type($result)\";\n+}\n+\n+def TTNG_ProducerAcquireOp : TTNG_Op<\"producer_acquire\"> {\n+  let arguments = (ins TensorOf<[TTNG_TokenType]>:$token, I32:$idx);\n+\n+  let assemblyFormat = \"$token `,` $idx attr-dict `:` type(operands)\";\n+}\n+\n+def TTNG_ProducerCommitOp : TTNG_Op<\"producer_commit\"> {\n+  let arguments = (ins TensorOf<[TTNG_TokenType]>:$token, I32:$idx);\n+\n+  let assemblyFormat = \"$token `,` $idx attr-dict `:` type(operands)\";\n+}\n+\n+def TTNG_ConsumerWaitOp : TTNG_Op<\"consumer_wait\"> {\n+  let arguments = (ins TensorOf<[TTNG_TokenType]>:$token, I32:$idx);\n+\n+  let assemblyFormat = \"$token `,` $idx attr-dict `:` type(operands)\";\n+}\n+\n+def TTNG_ConsumerReleaseOp : TTNG_Op<\"consumer_release\"> {\n+  let arguments = (ins TensorOf<[TTNG_TokenType]>:$token, I32:$idx);\n+\n+  let assemblyFormat = \"$token `,` $idx attr-dict `:` type(operands)\";\n+}\n+\n+//\n+// Mutex\n+//\n+\n+def TTNG_GetMutexRoleIdOp : TTNG_Op<\"get_mutex_role_id\"> {\n+  let results = (outs I32:$result);\n+\n+  let arguments = (ins I32Attr:$num);\n+\n+  let builders = [OpBuilder<(ins \"uint32_t\":$num)>];\n+\n+  let assemblyFormat = \"attr-dict `:` type($result)\";\n+}\n+\n+def TTNG_CreateMutexOp : TTNG_Op<\"create_mutex\"> {\n+  let results = (outs TTNG_MutexType:$result);\n+\n+  let builders = [OpBuilder<(ins)>];\n+\n+  let assemblyFormat = \"attr-dict `:` type($result)\";\n+}\n+\n+def TTNG_LockOp : TTNG_Op<\"lock\"> {\n+  let arguments = (ins TTNG_MutexType:$mutex);\n+\n+  let assemblyFormat = \"$mutex attr-dict `:` type(operands)\";\n+}\n+\n+def TTNG_UnlockOp : TTNG_Op<\"unlock\"> {\n+  let arguments = (ins TTNG_MutexType:$mutex);\n+\n+  let assemblyFormat = \"$mutex attr-dict `:` type(operands)\";\n+}\n+\n+def TTNG_RegAllocOp : TTNG_Op<\"reg_alloc\", []> {\n+  let summary = \"register allocation\";\n+\n+  let arguments = (ins I32Attr: $regCount);\n+\n+  let assemblyFormat = \"$regCount attr-dict `:` type(operands)\";\n+}\n+\n+def TTNG_RegDeallocOp : TTNG_Op<\"reg_dealloc\", []> {\n+  let summary = \"register deallocation\";\n+\n+  let arguments = (ins I32Attr: $regCount);\n+\n+  let assemblyFormat = \"$regCount attr-dict `:` type(operands)\";\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUTypes.td", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -0,0 +1,37 @@\n+// Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining\n+// a copy of this software and associated documentation files\n+// (the \"Software\"), to deal in the Software without restriction,\n+// including without limitation the rights to use, copy, modify, merge,\n+// publish, distribute, sublicense, and/or sell copies of the Software,\n+// and to permit persons to whom the Software is furnished to do so,\n+// subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be\n+// included in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+// EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+// IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+// CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+// TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+// SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+#ifndef TRITONNVIDIAGPU_TYPES\n+#define TRITONNVIDIAGPU_TYPES\n+\n+include \"triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUDialect.td\"\n+include \"mlir/IR/AttrTypeBase.td\"\n+\n+class TTNG_TypeDef<string name, string _mnemonic>\n+    : TypeDef<TritonNvidiaGPU_Dialect, name> {\n+  let mnemonic = _mnemonic;\n+}\n+\n+def TTNG_TokenType : TTNG_TypeDef<\"Token\", \"token\">;\n+\n+def TTNG_MutexType : TTNG_TypeDef<\"Mutex\", \"mutex\">;\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/Types.h", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "@@ -0,0 +1,33 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITONNVIDIAGPU_IR_TYPES_H_\n+#define TRITONNVIDIAGPU_IR_TYPES_H_\n+\n+#include \"mlir/IR/TypeSupport.h\"\n+#include \"mlir/IR/Types.h\"\n+\n+#define GET_TYPEDEF_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Types.h.inc\"\n+\n+#endif // TRITON_IR_TYPES_H_"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/Transforms/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls -name TritonNvidiaGPU)\n+add_public_tablegen_target(TritonNvidiaGPUTransformsIncGen)"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h", "status": "added", "additions": 81, "deletions": 0, "changes": 81, "file_content_changes": "@@ -0,0 +1,81 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITON_DIALECT_TRITONNVIDIAGPU_TRANSFORMS_PASSES_H_\n+#define TRITON_DIALECT_TRITONNVIDIAGPU_TRANSFORMS_PASSES_H_\n+\n+#include \"mlir/Pass/Pass.h\"\n+\n+namespace mlir {\n+namespace triton {\n+namespace nvidia_gpu {\n+\n+// Used by Triton runtime\n+struct ClusterInfo {\n+  ClusterInfo() : clusterDimX(1), clusterDimY(1), clusterDimZ(1) {}\n+  int clusterDimX;\n+  int clusterDimY;\n+  int clusterDimZ;\n+};\n+\n+} // namespace nvidia_gpu\n+} // namespace triton\n+} // namespace mlir\n+\n+namespace mlir {\n+\n+std::unique_ptr<Pass>\n+createTritonNvidiaGPUMaterializeLoadStorePass(int numWarps = 4,\n+                                              int computeCapability = 80);\n+\n+std::unique_ptr<Pass> createTritonNvidiaGPUPlanCTAPass(\n+    mlir::triton::nvidia_gpu::ClusterInfo *clusterInfo = nullptr);\n+\n+std::unique_ptr<Pass>\n+createTritonNvidiaGPUWSFeasibilityCheckingPass(int computeCapability = 90);\n+\n+std::unique_ptr<Pass>\n+createTritonNvidiaGPUWSDecomposingPass(int computeCapability = 90);\n+\n+std::unique_ptr<Pass>\n+createTritonNvidiaGPUWSPipelinePass(int numStages = 3, int numWarps = 4,\n+                                    int computeCapability = 90);\n+\n+std::unique_ptr<Pass>\n+createTritonNvidiaGPUWSMutexPass(int computeCapability = 90);\n+\n+std::unique_ptr<Pass>\n+createTritonNvidiaGPUWSMaterializationPass(int computeCapability = 90);\n+\n+std::unique_ptr<Pass>\n+createTritonNvidiaGPUFenceInsertionPass(int computeCapability = 90);\n+\n+std::unique_ptr<Pass>\n+createTritonGPURewriteTensorPointerPass(int computeCapability = 80);\n+\n+/// Generate the code for registering passes.\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+} // namespace mlir\n+#endif // TRITON_DIALECT_TRITONNVIDIAGPU_TRANSFORMS_PASSES_H_"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/Transforms/Passes.td", "status": "added", "additions": 228, "deletions": 0, "changes": 228, "file_content_changes": "@@ -0,0 +1,228 @@\n+// Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+//\n+// Permission is hereby granted, free of charge, to any person obtaining\n+// a copy of this software and associated documentation files\n+// (the \"Software\"), to deal in the Software without restriction,\n+// including without limitation the rights to use, copy, modify, merge,\n+// publish, distribute, sublicense, and/or sell copies of the Software,\n+// and to permit persons to whom the Software is furnished to do so,\n+// subject to the following conditions:\n+//\n+// The above copyright notice and this permission notice shall be\n+// included in all copies or substantial portions of the Software.\n+//\n+// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+// EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+// MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+// IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+// CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+// TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+// SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+\n+#ifndef TRITONNVIDIAGPU_PASSES\n+#define TRITONNVIDIAGPU_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+def MaterializeLoadStore : Pass<\"triton-nvidia-gpu-materialize-load-store\", \"mlir::ModuleOp\"> {\n+  let summary = \"materialize load & store\";\n+\n+  let description = [{\n+    This pass works after pipeline pass, converting the remaining tt.LoadOp taking\n+    ptr<tensor> as input into ttg.InsertSliceAsyncOp and emit proper barriers\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUMaterializeLoadStorePass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\",\n+                           \"mlir::scf::SCFDialect\",\n+                           \"mlir::arith::ArithDialect\"];\n+\n+  let options = [\n+    Option<\"numWarps\", \"num-warps\",\n+           \"int32_t\", /*default*/\"4\",\n+           \"number of warps per block\">,\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n+def TritonGPUPlanCTAPass : Pass<\"triton-nvidia-gpu-plan-cta\", \"mlir::ModuleOp\"> {\n+  let summary = \"plan CTA\";\n+\n+  let description = [{\n+    Plan CTAs in CGA\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUPlanCTAPass()\";\n+\n+  let dependentDialects = [\n+    \"mlir::triton::gpu::TritonGPUDialect\",\n+    \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\"\n+  ];\n+}\n+\n+def TritonGPUWSFeasibilityChecking : Pass<\"triton-nvidia-gpu-ws-feasibility-checking\", \"mlir::ModuleOp\"> {\n+  let summary = \"Attach attr named TritonNvidiaGPUDialect::getWSSupportedAttrName() if auto WS supported\";\n+\n+  let description = [{\n+    Since not every legal triton kernels can be auto WS, this pass does some (conservative) check\n+    and attaches an attribute named TritonNvidiaGPUDialect::getWSSupportedAttrName() on\n+    the input module op if the kernel is supported.\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUWSFeasibilityCheckingPass()\";\n+\n+  let dependentDialects = [\n+    \"mlir::triton::gpu::TritonGPUDialect\",\n+    \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\"\n+  ];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"90\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n+def TritonGPUWSDecomposing : Pass<\"triton-nvidia-gpu-ws-decomposing\", \"mlir::ModuleOp\"> {\n+  let summary = \"Clustering on the ops according to their performance hotspots\";\n+\n+  let description = [{\n+    Based on compute capability and heuristics,\n+    this pass will identify some operations to be executed in different agents,\n+    by marking them with async 'label'. E.g.,\n+    input:\n+      %1 = tt,load %0 ...\n+      %4 = tt.dot %1, %2, %3 ...\n+    output:\n+      %1 = tt,load %0 {async_agent = 0} ...\n+      %4 = tt.dot %1, %2, %3 {async_agent = 1} : ...\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUWSDecomposingPass()\";\n+\n+  let dependentDialects = [\n+    \"mlir::triton::gpu::TritonGPUDialect\",\n+    \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\"\n+  ];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n+def TritonGPUWSPipeline : Pass<\"triton-nvidia-gpu-ws-pipeline\", \"mlir::ModuleOp\"> {\n+  let summary = \"Warp specialization pipeline\";\n+\n+  let description = [{\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUWSPipelinePass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\",\n+                           \"mlir::scf::SCFDialect\",\n+                           \"mlir::arith::ArithDialect\"];\n+\n+  let options = [\n+    Option<\"numStages\", \"num-stages\",\n+           \"int32_t\", /*default*/\"3\",\n+           \"number of pipeline stages\">,\n+    Option<\"numWarps\", \"num-warps\",\n+           \"int32_t\", /*default*/\"12\",\n+           \"number of warps per block\">,\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"90\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n+def TritonGPUWSMutex : Pass<\"triton-nvidia-gpu-ws-mutex\", \"mlir::ModuleOp\"> {\n+  let summary = \"Warp specialization mutex syncronization\";\n+\n+  let description = [{\n+    create mutex syncronization for persistent kernel. (as \"2 Math WG\" persistent kernel in cutlass)\n+    For example, the agent containing dot and store will be divided into two sub-agent,\n+    which execute dot and store alternately. i.e.:\n+      sub-agent-0: dot | store |  dot  | ... | store\n+      sub-agent-1:     |  dot  | store | ... |  dot | store\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUWSMutexPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\",\n+                           \"mlir::scf::SCFDialect\",\n+                           \"mlir::arith::ArithDialect\"];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n+def TritonGPUWSMaterialization : Pass<\"triton-nvidia-gpu-ws-materialization\", \"mlir::ModuleOp\"> {\n+  let summary = \"Warp specialization materialization\";\n+\n+  let description = [{\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUWSMaterializationPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\"];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"90\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n+def TritonGPUFenceInsertion : Pass<\"triton-nvidia-gpu-fence-insertion\", \"mlir::ModuleOp\"> {\n+  let summary = \"Insert fences across generic and async proxy\";\n+\n+  let description = [{\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUFenceInsertionPass()\";\n+\n+  let dependentDialects = [\n+    \"mlir::triton::gpu::TritonGPUDialect\",\n+    \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\"\n+  ];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"90\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n+def TritonGPURewriteTensorPointer : Pass</*cli-arg*/\"tritongpu-rewrite-tensor-pointer\", /*Op*/\"mlir::ModuleOp\"> {\n+  let summary = \"Rewrite load/stores with tensor pointers into legacy load/stores\";\n+  let description = [{\n+    This pass rewrites all load/store semantics initiated by a `tt.make_tensor_ptr` and `tt.advance` into legacy\n+    semantics. After this pass, `tt.make_tensor_ptr` and `tt.advance` will disappear, and it generates logics to compute\n+    the pointer/mask/other for each load/store.\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPURewriteTensorPointerPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::TritonDialect\"];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n+}\n+\n+#endif"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h", "status": "added", "additions": 95, "deletions": 0, "changes": 95, "file_content_changes": "@@ -0,0 +1,95 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITON_DIALECT_TRITONNVIDIAGPU_TRANSFORMS_UTILITY_H_\n+#define TRITON_DIALECT_TRITONNVIDIAGPU_TRANSFORMS_UTILITY_H_\n+\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"llvm/ADT/MapVector.h\"\n+\n+namespace mlir {\n+\n+// 0 is reserved for default sync.\n+// TODO: comprehensive mechanism to globally manage namedbarrier.\n+static int const nameBarrierIdBegin = 1;\n+static int nameBarrierIdEnd = 16;\n+\n+/// Helper functions for async agent\n+typedef int AgentId;\n+SmallVector<AgentId> getAgentIds(Operation *op);\n+bool hasAgentId(Operation *op, AgentId agentId);\n+void setAgentIds(Operation *op, ArrayRef<AgentId> agentIds);\n+SmallVector<AgentId> collectAgentIds(Operation *op);\n+void addAgentIds(Operation *op, ArrayRef<int> agents);\n+SmallVector<int> getMutexBarIds(Operation *op);\n+SmallVector<int> getMutexNumThreads(Operation *op);\n+\n+class OpBuilderWithAgentIds : public OpBuilder {\n+public:\n+  OpBuilderWithAgentIds(MLIRContext *context) : OpBuilder(context) {}\n+\n+  void setAgentIdsFromArray(ArrayRef<AgentId> newAgentIds) {\n+    agentIds = SmallVector<AgentId>(newAgentIds.begin(), newAgentIds.end());\n+  }\n+\n+  void setAgentIdsFromOp(Operation *op) {\n+    setAgentIdsFromArray(getAgentIds(op));\n+  }\n+\n+  void setAgentIdsFromValueUsers(Value value) {\n+    SetVector<AgentId> agentIdSet;\n+    for (Operation *user : value.getUsers())\n+      for (AgentId agentId : getAgentIds(user))\n+        agentIdSet.insert(agentId);\n+    setAgentIdsFromArray(agentIdSet.getArrayRef());\n+  }\n+\n+  template <typename OpTy, typename... Args>\n+  OpTy createWithAgentIds(Args &&...args) {\n+    OpTy op = create<OpTy>(std::forward<Args>(args)...);\n+    if (!agentIds.empty())\n+      setAgentIds(op, agentIds);\n+    return op;\n+  }\n+\n+private:\n+  SmallVector<AgentId> agentIds;\n+};\n+\n+/// Constant agent ids\n+constexpr AgentId kLoadAgentId = 0;\n+constexpr AgentId kDotAgentId = 1;\n+\n+bool isWSCandidateLoad(Operation *op);\n+bool isWSSupported(ModuleOp m, int computeCapability);\n+\n+LogicalResult getDependentValues(Value val, DenseSet<Value> &depSet,\n+                                 const DenseSet<Value> &stopSet = {});\n+LogicalResult getDependentValues(Operation *op, DenseSet<Value> &depSet,\n+                                 const DenseSet<Value> &stopSet = {});\n+DenseSet<Operation *> getDependentOps(DenseSet<Value> &depSet);\n+\n+} // namespace mlir\n+\n+#endif // TRITON_DIALECT_TRITONNVIDIAGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "include/triton/Target/AMDGCN/AMDGCNTranslation.h", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -0,0 +1,19 @@\n+#ifndef TRITON_TARGET_AMDGCNTRANSLATION_H\n+#define TRITON_TARGET_AMDGCNTRANSLATION_H\n+\n+#include <string>\n+#include <tuple>\n+\n+namespace llvm {\n+class Module;\n+} // namespace llvm\n+\n+namespace triton {\n+\n+// Translate LLVM IR to AMDGCN code.\n+std::tuple<std::string, std::string>\n+translateLLVMIRToAMDGCN(llvm::Module &module, std::string cc);\n+\n+} // namespace triton\n+\n+#endif"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -1,5 +1,6 @@\n #ifndef TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n #define TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n+#include \"triton/Target/PTX/TmaMetadata.h\"\n #include \"llvm/ADT/StringRef.h\"\n #include <memory>\n #include <string>\n@@ -26,13 +27,17 @@ void addExternalLibs(mlir::ModuleOp &module,\n std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module, int computeCapability,\n+                           mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n                            bool isROCM);\n \n // Translate mlir LLVM dialect to LLVMIR, return null if failed.\n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n                       bool isROCM);\n \n+bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n+                   llvm::StringRef path, bool isROCM);\n+\n } // namespace triton\n } // namespace mlir\n "}, {"filename": "include/triton/Target/PTX/TmaMetadata.h", "status": "added", "additions": 107, "deletions": 0, "changes": 107, "file_content_changes": "@@ -0,0 +1,107 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITON_TARGET_PTX_TMAMETADATA_H\n+#define TRITON_TARGET_PTX_TMAMETADATA_H\n+\n+#include \"python/triton/third_party/cuda/include/cuda.h\"\n+#include \"llvm/ADT/StringRef.h\"\n+#include \"llvm/Support/Debug.h\"\n+#include \"llvm/Support/Format.h\"\n+#include \"llvm/Support/FormatVariadic.h\"\n+#include <map>\n+#include <utility>\n+#include <vector>\n+\n+namespace mlir {\n+namespace triton {\n+namespace gpu {\n+\n+struct TMAInfo {\n+  // --------------------------------------------\n+  // informations to be filled into CUtensorMaps\n+  int tensorDataType;\n+\n+  uint32_t tensorRank;\n+\n+  // the argument indices for the runtime to get globalAddresses\n+  size_t globalAddressArgIdx;\n+\n+  // the argument indices for the runtime to get globalDims, -1 stands for this\n+  // dim is padded\n+  std::vector<int32_t> globalDimsArgIdx;\n+\n+  // the argument indices for the runtime to get globalStrides, -1 stands for\n+  // this dim is padded the runtime need to map the value to internal format\n+  std::vector<int32_t> globalStridesArgIdx;\n+\n+  std::vector<uint32_t> boxDims;\n+\n+  std::vector<uint32_t> elementStrides;\n+\n+  int interleave;\n+\n+  int swizzle;\n+\n+  int l2Promotion;\n+\n+  int oobFill;\n+\n+  // --------------------------------------------\n+  // the argument indices for the runtime to send the address of tma_desc to the\n+  // binary\n+  int TMADescArgIdx;\n+\n+  template <typename T>\n+  void dump_vec(const std::vector<T> &vec, llvm::StringRef info) const {\n+    llvm::errs() << info << \": \";\n+    for (const T &e : vec)\n+      llvm::errs() << e << \",\";\n+    llvm::errs() << \"\\n\";\n+  }\n+\n+  void dump() const {\n+    llvm::errs() << \"TMA Info: ----------\"\n+                 << \"\\n\";\n+    llvm::errs() << \"-- tensorDataType: \" << tensorDataType\n+                 << \", tensorRank: \" << tensorRank << \"\\n\";\n+    llvm::errs() << \"-- globalAddressArgIdx: \" << globalAddressArgIdx << \"\\n\";\n+    llvm::errs() << \"-- TMADescArgIdx: \" << TMADescArgIdx << \"\\n\";\n+    dump_vec<int32_t>(globalDimsArgIdx, \"-- globalDimsArgIdx\");\n+    dump_vec<int32_t>(globalStridesArgIdx, \"-- globalStridesArgIdx\");\n+    dump_vec<uint32_t>(boxDims, \"-- boxDims\");\n+    dump_vec<uint32_t>(elementStrides, \"-- elementStrides\");\n+    llvm::errs() << \"-- interleave: \" << interleave << \"\\n\";\n+    llvm::errs() << \"-- swizzle: \" << swizzle << \"\\n\";\n+    llvm::errs() << \"-- l2Promotion: \" << l2Promotion << \"\\n\";\n+    llvm::errs() << \"-- oobFill: \" << oobFill << \"\\n\";\n+  };\n+};\n+\n+using TMAMetadataTy = std::vector<TMAInfo>;\n+\n+} // namespace gpu\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif // TRITON_TARGET_PTX_TMAMETADATA_H"}, {"filename": "include/triton/Tools/Sys/GetEnv.hpp", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -25,9 +25,13 @@\n #include <algorithm>\n #include <cstdlib>\n #include <string>\n-\n namespace triton {\n \n+const std::set<std::string> ENV_VARS = {\n+    \"ENABLE_MMA_V3\",     \"TRITON_DISABLE_LINE_INFO\", \"DISABLE_FAST_REDUCTION\",\n+    \"ENABLE_TMA\",        \"MLIR_ENABLE_DUMP\",         \"LLVM_IR_ENABLE_DUMP\",\n+    \"AMDGCN_ENABLE_DUMP\"};\n+\n namespace tools {\n \n inline std::string getenv(const char *name) {\n@@ -39,6 +43,9 @@ inline std::string getenv(const char *name) {\n }\n \n inline bool getBoolEnv(const std::string &env) {\n+  std::string msg = \"Environment variable \" + env + \" is not recognized\";\n+  assert(triton::ENV_VARS.find(env.c_str()) != triton::ENV_VARS.end() &&\n+         msg.c_str());\n   const char *s = std::getenv(env.c_str());\n   std::string str(s ? s : \"\");\n   std::transform(str.begin(), str.end(), str.begin(),"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -2,6 +2,7 @@\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n \n namespace mlir {\n \n@@ -27,17 +28,21 @@ void SharedMemoryAliasAnalysis::visitOperation(\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n     // XXX(Keren): the following ops are always aliasing for now\n-    if (isa<triton::gpu::ExtractSliceOp, triton::TransOp>(op)) {\n+    if (isa<triton::gpu::ExtractSliceOp, triton::TransOp,\n+            triton::nvidia_gpu::ExtractMBarrierOp>(op)) {\n       // extract_slice %src\n       // trans %src\n       aliasInfo = AliasInfo(operands[0]->getValue());\n       pessimistic = false;\n-    } else if (isa<tensor::InsertSliceOp, triton::gpu::InsertSliceAsyncOp>(\n-                   op)) {\n+    } else if (isa<tensor::InsertSliceOp, triton::gpu::InsertSliceAsyncOp,\n+                   triton::nvidia_gpu::InsertSliceAsyncV2Op>(op)) {\n       // insert_slice_async %src, %dst, %index\n       // insert_slice %src into %dst[%offsets]\n       aliasInfo = AliasInfo(operands[1]->getValue());\n       pessimistic = false;\n+    } else if (isa<triton::nvidia_gpu::StoreAsyncOp>(op)) {\n+      aliasInfo = AliasInfo(operands[0]->getValue());\n+      pessimistic = false;\n     } else if (triton::gpu::isSharedEncoding(result)) {\n       aliasInfo.insert(result);\n       pessimistic = false;"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 82, "deletions": 26, "changes": 108, "file_content_changes": "@@ -16,6 +16,7 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getShapePerCTATile;\n using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n@@ -57,11 +58,23 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   Attribute srcLayout = srcTy.getEncoding();\n   Attribute dstLayout = dstTy.getEncoding();\n \n-  // MmaToDotShortcut doesn't use shared mem\n-  if (srcLayout.isa<MmaEncodingAttr>() &&\n-      dstLayout.isa<DotOperandEncodingAttr>())\n-    if (isMmaToDotShortcut(srcTy, dstTy))\n-      return {};\n+  if (shouldUseDistSmem(srcLayout, dstLayout)) {\n+    // TODO: padding to avoid bank conflicts\n+    return convertType<unsigned, int64_t>(getShapePerCTA(srcTy));\n+  }\n+\n+  // MmaToDotShortcut and MmaToMmaShortcut doesn't use shared mem\n+  if (auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>()) {\n+    if (dstLayout.isa<DotOperandEncodingAttr>()) {\n+      if (isMmaToDotShortcut(srcTy, dstTy)) {\n+        return {};\n+      }\n+    } else if (auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>()) {\n+      if (isMmaToMmaShortcut(srcTy, dstTy)) {\n+        return {};\n+      }\n+    }\n+  }\n \n   assert(srcLayout && dstLayout &&\n          \"Unexpected layout in getScratchConfigForCvtLayout()\");\n@@ -73,18 +86,18 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;\n   outVec = outOrd[0] == 0 ? 1 : dstContigPerThread;\n \n-  auto srcShape = srcTy.getShape();\n-  auto dstShape = dstTy.getShape();\n-  auto srcShapePerCTA = getShapePerCTA(srcLayout, srcShape);\n-  auto dstShapePerCTA = getShapePerCTA(dstLayout, dstShape);\n+  auto srcShapePerCTA = getShapePerCTA(srcTy);\n+  auto dstShapePerCTA = getShapePerCTA(dstTy);\n+  auto srcShapePerCTATile = getShapePerCTATile(srcLayout, srcTy.getShape());\n+  auto dstShapePerCTATile = getShapePerCTATile(dstLayout, dstTy.getShape());\n \n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> paddedRepShape(rank);\n   unsigned pad = std::max(inVec, outVec);\n   for (unsigned d = 0; d < rank; ++d) {\n     paddedRepShape[d] =\n-        std::max(std::min<unsigned>(srcTy.getShape()[d], srcShapePerCTA[d]),\n-                 std::min<unsigned>(dstTy.getShape()[d], dstShapePerCTA[d]));\n+        std::max(std::min<unsigned>(srcShapePerCTA[d], srcShapePerCTATile[d]),\n+                 std::min<unsigned>(dstShapePerCTA[d], dstShapePerCTATile[d]));\n   }\n   if (rank == 1)\n     return paddedRepShape;\n@@ -146,20 +159,45 @@ class AllocationAnalysis {\n     // For example: %a = scf.if -> yield\n     // %a must be allocated elsewhere by other operations.\n     // FIXME(Keren): extract and insert are always alias for now\n-    if (!maybeSharedAllocationOp(op) || maybeAliasOp(op)) {\n+    if (!maybeSharedAllocationOp(op) || maybeAliasOp(op))\n       return;\n-    }\n \n+    // XXX(Keren): Why this hard-coded alignment?\n+    size_t kAlignment = 8;\n     for (Value result : op->getResults()) {\n       if (triton::gpu::isSharedEncoding(result)) {\n         // Bytes could be a different value once we support padding or other\n         // allocation policies.\n         auto tensorType = result.getType().dyn_cast<RankedTensorType>();\n-        auto bytes = tensorType.getNumElements() *\n+        auto shapePerCTA = triton::gpu::getShapePerCTA(tensorType);\n+        auto bytes = product<int64_t>(shapePerCTA) *\n                      tensorType.getElementTypeBitWidth() / 8;\n-        allocation->addBuffer<BufferT::BufferKind::Explicit>(result, bytes);\n+\n+        // XXX(Keren): magic numbers 256 and 1024\n+        // benzh@maybe alignment should be passed in.\n+        // Software swizzling calculates phase based on offset, while hardware\n+        // swizzling do that based on physical address. Thus only by setting the\n+        // alignment to 1024 can ensure the correctness.\u00a0\n+        if (bytes > 256)\n+          kAlignment = 1024;\n+        allocation->addBuffer<BufferT::BufferKind::Explicit>(result, bytes,\n+                                                             kAlignment);\n       }\n     }\n+    if (isa<triton::nvidia_gpu::AllocMBarrierOp>(op)) {\n+      Value result = op->getResult(0);\n+      if (!result.getType().isa<RankedTensorType>())\n+        // In case AllocMBarrierOp is allocating scalar mbarriers\n+        allocation->addBuffer<BufferT::BufferKind::Explicit>(result, 8,\n+                                                             kAlignment);\n+    }\n+  }\n+\n+  template <BufferT::BufferKind T>\n+  void maybeAddScratchBuffer(Operation *op, unsigned bytes,\n+                             unsigned alignment) {\n+    if (bytes > 0)\n+      allocation->addBuffer<T>(op, bytes, alignment);\n   }\n \n   template <BufferT::BufferKind T>\n@@ -170,14 +208,17 @@ class AllocationAnalysis {\n \n   /// Initializes temporary shared memory for a given operation.\n   void getScratchValueSize(Operation *op) {\n+    const size_t scratchAlignment = 128;\n     if (auto reduceOp = dyn_cast<triton::ReduceOp>(op)) {\n       ReduceOpHelper helper(reduceOp);\n       unsigned bytes = helper.getScratchSizeInBytes();\n-      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes,\n+                                                          scratchAlignment);\n     } else if (auto scanOp = dyn_cast<triton::ScanOp>(op)) {\n       ScanLoweringHelper helper(scanOp);\n       unsigned bytes = helper.getScratchSizeInBytes();\n-      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes,\n+                                                          scratchAlignment);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n       auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();\n@@ -201,7 +242,8 @@ class AllocationAnalysis {\n           srcTy.getElementType().isa<triton::PointerType>()\n               ? elems * kPtrBitWidth / 8\n               : elems * std::max<int>(8, srcTy.getElementTypeBitWidth()) / 8;\n-      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes,\n+                                                          scratchAlignment);\n     } else if (auto atomicRMWOp = dyn_cast<triton::AtomicRMWOp>(op)) {\n       auto value = op->getOperand(0);\n       // only scalar requires scratch memory\n@@ -218,7 +260,8 @@ class AllocationAnalysis {\n             elemTy.isa<triton::PointerType>()\n                 ? elems * kPtrBitWidth / 8\n                 : elems * std::max<int>(8, elemTy.getIntOrFloatBitWidth()) / 8;\n-        maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+        maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes,\n+                                                            scratchAlignment);\n       }\n     } else if (auto atomicCASOp = dyn_cast<triton::AtomicCASOp>(op)) {\n       auto value = op->getOperand(0);\n@@ -230,13 +273,15 @@ class AllocationAnalysis {\n       auto bytes = elemTy.isa<triton::PointerType>()\n                        ? elems * kPtrBitWidth / 8\n                        : elems * elemTy.getIntOrFloatBitWidth() / 8;\n-      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes,\n+                                                          scratchAlignment);\n     } else if (auto callOp = dyn_cast<CallOpInterface>(op)) {\n       auto callable = callOp.resolveCallable();\n       auto funcOp = dyn_cast<FunctionOpInterface>(callable);\n       auto *funcAlloc = &(*funcAllocMap)[funcOp];\n       auto bytes = funcAlloc->getSharedMemorySize();\n-      maybeAddScratchBuffer<BufferT::BufferKind::Virtual>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Virtual>(op, bytes,\n+                                                          scratchAlignment);\n     }\n   }\n \n@@ -356,6 +401,12 @@ class AllocationAnalysis {\n     // Analyze liveness of explicit buffers\n     Liveness liveness(operation);\n     auto getValueLivenessRange = [&](Value value) {\n+      // Shared memory allocated by mbarrier cannot be reused\n+      if (value.getDefiningOp() &&\n+          isa<triton::nvidia_gpu::AllocMBarrierOp>(value.getDefiningOp()))\n+        return Interval(std::numeric_limits<size_t>::min(),\n+                        std::numeric_limits<size_t>::max());\n+\n       auto liveOperations = liveness.resolveLiveness(value);\n       auto minId = std::numeric_limits<size_t>::max();\n       auto maxId = std::numeric_limits<size_t>::min();\n@@ -437,17 +488,22 @@ class AllocationAnalysis {\n             auto xRange = bufferRange[buffer];\n             bool res = xRange.intersects(range);\n             for (auto val : tripleMap)\n-              res = res && !val.second.intersects(xRange);\n+              res = res &&\n+                    !val.second.intersects(xRange); // only one buffer intersect\n             return res;\n           });\n       if (bufferIt != xBuffers.end()) {\n         auto buffer = *bufferIt;\n         auto xSize = buffer->size;\n         auto xRange = bufferRange.lookup(buffer);\n-        bufferStart[buffer] = size;\n-        tripleMap.insert(\n-            {size + xSize, Interval{std::max(range.start(), xRange.start()),\n-                                    std::min(range.end(), xRange.end())}});\n+        // TODO(Keren): A buffer's size shouldn't be determined here, have to\n+        // clean it up\n+        size_t alignment = buffer->alignment;\n+        size_t alignSize = ((size + alignment - 1) / alignment) * alignment;\n+        bufferStart[buffer] = alignSize;\n+        tripleMap.insert({alignSize + xSize,\n+                          Interval{std::max(range.start(), xRange.start()),\n+                                   std::min(range.end(), xRange.end())}});\n         // We could either insert (range.start, xRange.start) or (range.start,\n         // xRange.end), both are correct and determine the potential buffer\n         // offset, and the graph coloring algorithm will solve the interference,"}, {"filename": "lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -14,4 +14,5 @@ add_mlir_library(TritonAnalysis\n   MLIRLLVMDialect\n   TritonIR\n   TritonGPUIR\n+  TritonNvidiaGPUIR\n )"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 23, "deletions": 4, "changes": 27, "file_content_changes": "@@ -2,7 +2,11 @@\n #include \"triton/Analysis/Alias.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n+#include \"../lib/Conversion/TritonGPUToLLVM/Utility.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n+#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n+\n #include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include <deque>\n@@ -103,7 +107,11 @@ void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n     return;\n   }\n \n-  if (isa<gpu::BarrierOp>(op)) {\n+  // TODO(Keren): Don't expose LLVM Dialect ops here\n+  if (isa<gpu::BarrierOp>(op) ||\n+      (isa<LLVM::InlineAsmOp>(op) &&\n+       (dyn_cast<LLVM::InlineAsmOp>(op).getAsmString().find(\"bar.sync\") !=\n+        std::string::npos))) {\n     // If the current op is a barrier, we sync previous reads and writes\n     blockInfo->sync();\n     return;\n@@ -169,12 +177,23 @@ void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n   if (blockInfo->isIntersected(curBlockInfo)) {\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPoint(op);\n-    builder->create<gpu::BarrierOp>(op->getLoc());\n-    blockInfo->sync();\n+    // TODO(Keren): Don't expose LLVM Dialect ops here\n+    // TODO[shuhaoj]: Change hard code style of numThreads. Hide async_agent\n+    // attr. Better way to determine barId (number of agents are limited).\n+    if (op->hasAttr(\"async_agent\")) {\n+      int agentId = getAgentIds(op).front(), roleId = 0;\n+      if (op->hasAttr(\"agent.mutex_role\"))\n+        roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+      int barId = agentId + roleId + nameBarrierIdBegin;\n+      assert(barId < nameBarrierIdEnd);\n+      barSync(*builder, op, barId, 128);\n+    } else {\n+      builder->create<gpu::BarrierOp>(op->getLoc());\n+      blockInfo->sync();\n+    }\n   }\n   // Update the region info, even if barrier is inserted, we have to maintain\n   // the current op's read/write buffers.\n   blockInfo->join(curBlockInfo);\n }\n-\n } // namespace mlir"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 164, "deletions": 11, "changes": 175, "file_content_changes": "@@ -1,10 +1,14 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"mlir/Analysis/DataFlow/ConstantPropagationAnalysis.h\"\n #include \"mlir/Analysis/DataFlow/DeadCodeAnalysis.h\"\n+#include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/IR/Dialect.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include <deque>\n \n@@ -37,6 +41,51 @@ bool ReduceOpHelper::isFastReduction() {\n          getParentOrder(getSrcLayout())[0];\n }\n \n+// Cases where distributed shared memory is not required in ConvertLayout:\n+// (1) numCTAs == 1\n+// (2) numCTAs > 1 but srcCTALayout == dstCTALayout\n+// TODO: Case with SliceLayout as srcLayout and numCTAs > 1 is to be implemented\n+// in the future\n+bool shouldUseDistSmem(Attribute srcLayout, Attribute dstLayout) {\n+  unsigned numCTAs = triton::gpu::getNumCTAs(srcLayout);\n+  assert(numCTAs == triton::gpu::getNumCTAs(dstLayout) &&\n+         \"Invalid layout conversion: the numbers of CTAs of src and dst \"\n+         \"layouts are different\");\n+\n+  // Case (1): Never use dsmem when numCTAs == 1\n+  if (numCTAs == 1)\n+    return false;\n+\n+  // Case where CTAsPerCGA of srcLayout in the sliced dim is not 1 is not\n+  // implemented yet\n+  if (auto sliceLayout = srcLayout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    auto dim = sliceLayout.getDim();\n+    auto CTAsPerCGA = triton::gpu::getCTAsPerCGA(sliceLayout.getParent());\n+    if (CTAsPerCGA[dim] != 1)\n+      assert(0 && \"Layout conversion to be implemented\");\n+  }\n+\n+  // Case where CTAsPerCGA of dstLayout in the sliced dim is not 1 is supported\n+  if (auto sliceLayout = dstLayout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    auto dim = sliceLayout.getDim();\n+    auto CTAsPerCGA = triton::gpu::getCTAsPerCGA(sliceLayout.getParent());\n+    if (CTAsPerCGA[dim] != 1)\n+      return true;\n+  }\n+\n+  // The above two branches make sure that it is legal to call getCTALayout of\n+  // srcLayout and dstLayout\n+\n+  // Case (2): Do not use dsmem when srcCTALayout == dstCTALayout\n+  auto srcCTALayout = triton::gpu::getCTALayout(srcLayout);\n+  auto dstCTALayout = triton::gpu::getCTALayout(dstLayout);\n+  if (srcCTALayout == dstCTALayout)\n+    return false;\n+\n+  // Dsmem access is required when srcCTALayout != dstCTALayout\n+  return true;\n+}\n+\n unsigned ReduceOpHelper::getInterWarpSize() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   unsigned sizeIntraWarps = getIntraWarpSize();\n@@ -136,7 +185,7 @@ bool ReduceOpHelper::isSupportedLayout() {\n     return true;\n   }\n   if (auto mmaLayout = srcLayout.dyn_cast<triton::gpu::MmaEncodingAttr>()) {\n-    if (mmaLayout.isAmpere()) {\n+    if (mmaLayout.isAmpere() || mmaLayout.isHopper()) {\n       return true;\n     }\n   }\n@@ -282,6 +331,8 @@ bool maybeSharedAllocationOp(Operation *op) {\n   return dialect &&\n          (dialect->getTypeID() ==\n               mlir::TypeID::get<triton::gpu::TritonGPUDialect>() ||\n+          dialect->getTypeID() ==\n+              mlir::TypeID::get<triton::nvidia_gpu::TritonNvidiaGPUDialect>() ||\n           dialect->getTypeID() == mlir::TypeID::get<triton::TritonDialect>() ||\n           dialect->getTypeID() == mlir::TypeID::get<arith::ArithDialect>() ||\n           dialect->getTypeID() == mlir::TypeID::get<tensor::TensorDialect>());\n@@ -290,6 +341,8 @@ bool maybeSharedAllocationOp(Operation *op) {\n bool maybeAliasOp(Operation *op) {\n   return isa<triton::gpu::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n          isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n+         isa<triton::nvidia_gpu::InsertSliceAsyncV2Op>(op) ||\n+         isa<triton::nvidia_gpu::StoreAsyncOp>(op) ||\n          isa<tensor::InsertSliceOp>(op);\n }\n \n@@ -299,31 +352,43 @@ bool supportMMA(triton::DotOp op, int version) {\n   // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-fragment-mma-884-f16\n   auto aElemTy = op.getA().getType().cast<RankedTensorType>().getElementType();\n   auto bElemTy = op.getB().getType().cast<RankedTensorType>().getElementType();\n+  if (version == 3) {\n+    if (!::triton::tools::getBoolEnv(\"ENABLE_MMA_V3\"))\n+      return false;\n+    auto retType = op.getResult().getType().cast<RankedTensorType>();\n+    auto retShapePerCTA = triton::gpu::getShapePerCTA(retType);\n+    auto mod = op->getParentOfType<mlir::ModuleOp>();\n+    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+    if (!(numWarps % 4 == 0 && retShapePerCTA[0] % 64 == 0 &&\n+          retShapePerCTA[1] % 8 == 0 &&\n+          (aElemTy.isFloat8E5M2() || aElemTy.isFloat8E4M3FN() ||\n+           aElemTy.isInteger(8) || aElemTy.isF16() || aElemTy.isBF16() ||\n+           aElemTy.isF32()))) {\n+      return false;\n+    }\n+  }\n   if (aElemTy.isF32() && bElemTy.isF32()) {\n     return op.getAllowTF32() && version >= 2;\n   }\n   return supportMMA(op.getA(), version) && supportMMA(op.getB(), version);\n }\n \n bool supportMMA(Value value, int version) {\n-  // Tell whether a DotOp support HMMA by the operand type(either $a or $b).\n+  // Tell whether a DotOp support MMA by the operand type(either $a or $b).\n   // We cannot get both the operand types(in TypeConverter), here we assume the\n   // types of both the operands are identical here.\n-  assert((version == 1 || version == 2) &&\n+  assert((version == 1 || version == 2 || version == 3) &&\n          \"Unexpected MMA layout version found\");\n   auto elemTy = value.getType().cast<RankedTensorType>().getElementType();\n-  return elemTy.isF16() || elemTy.isBF16() ||\n+  // FP8 is not natively supported on all mma versions but it can always be\n+  // promoted to fp16 therefore we can always support it.\n+  bool isFP8 = elemTy.isFloat8E5M2() || elemTy.isFloat8E4M3FN() ||\n+               elemTy.isFloat8E5M2FNUZ() || elemTy.isFloat8E4M3FNUZ();\n+  return isFP8 || elemTy.isF16() || elemTy.isBF16() ||\n          (elemTy.isF32() && version >= 2) ||\n          (elemTy.isInteger(8) && version >= 2);\n }\n \n-Type getElementType(Value value) {\n-  auto type = value.getType();\n-  if (auto tensorType = type.dyn_cast<RankedTensorType>())\n-    return tensorType.getElementType();\n-  return type;\n-}\n-\n bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy) {\n   // dot_op<opIdx=0, parent=#mma> = #mma\n   // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n@@ -338,6 +403,17 @@ bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy) {\n          !srcTy.getElementType().isF32();\n }\n \n+bool isMmaToMmaShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy) {\n+  auto src = srcTy.getEncoding().cast<triton::gpu::MmaEncodingAttr>();\n+  auto dst = dstTy.getEncoding().cast<triton::gpu::MmaEncodingAttr>();\n+  auto srcElemsPerThread = triton::gpu::getTotalElemsPerThread(srcTy);\n+  auto dstElemsPerThread = triton::gpu::getTotalElemsPerThread(dstTy);\n+  // when #mma = MmaEncoding<version=3, warpsPerCTA=[..., 1]>\n+  return src.getVersionMajor() == 3 && src.getWarpsPerCTA()[1] == 1 &&\n+         dst.getVersionMajor() == 3 && dst.getWarpsPerCTA()[1] == 1 &&\n+         srcElemsPerThread == dstElemsPerThread;\n+}\n+\n bool isSingleValue(Value value) {\n   // Don't consider load as expensive if it is loading a scalar.\n   if (auto tensorTy = value.getType().dyn_cast<RankedTensorType>())\n@@ -557,4 +633,81 @@ std::unique_ptr<DataFlowSolver> createDataFlowSolver() {\n   return solver;\n }\n \n+static triton::MakeTensorPtrOp getMakeTensorPtrOpImpl(Operation *op, Value v) {\n+\n+  if (auto makeTensorPtrOp = dyn_cast<triton::MakeTensorPtrOp>(op)) {\n+    return makeTensorPtrOp;\n+  }\n+\n+  if (auto advanceOp = dyn_cast<triton::AdvanceOp>(op)) {\n+    return getMakeTensorPtrOp(advanceOp.getPtr());\n+  }\n+\n+  if (auto branch = dyn_cast<RegionBranchOpInterface>(op)) {\n+    auto idx = v.cast<OpResult>().getResultNumber();\n+    llvm::SmallVector<scf::YieldOp> yieldOps;\n+    op->walk([&](Operation *op) {\n+      if (auto yieldOp = dyn_cast<scf::YieldOp>(op))\n+        yieldOps.push_back(yieldOp);\n+    });\n+\n+    // benzh@ if multi yields, all yields operand should come from same arg.\n+    Value newValue = yieldOps[0].getOperands()[idx];\n+    return getMakeTensorPtrOp(newValue);\n+  }\n+\n+  llvm_unreachable(\"Unable to getMakeTensorPtr()\");\n+}\n+\n+triton::MakeTensorPtrOp getMakeTensorPtrOp(Value v) {\n+  using BranchOps = llvm::SetVector<std::pair<Operation *, int>>;\n+  llvm::DenseMap<Block *, BranchOps> blockToCFOps;\n+  auto moduleOp =\n+      v.getParentBlock()->getParentOp()->getParentOfType<ModuleOp>();\n+\n+  moduleOp.walk([&](Operation *op) {\n+    if (auto br = dyn_cast<cf::BranchOp>(op)) {\n+      Block *block = br.getDest();\n+      blockToCFOps[block].insert({op, -1});\n+    }\n+    if (auto condBr = dyn_cast<cf::CondBranchOp>(op)) {\n+      Block *blockT = condBr.getTrueDest();\n+      Block *blockF = condBr.getFalseDest();\n+      blockToCFOps[blockT].insert({condBr, 1});\n+      blockToCFOps[blockF].insert({condBr, 0});\n+    }\n+  });\n+\n+  if (Operation *definingOp = v.getDefiningOp()) {\n+    return getMakeTensorPtrOpImpl(definingOp, v);\n+  } else if (BlockArgument arg = v.cast<BlockArgument>()) {\n+    unsigned argNum = arg.getArgNumber();\n+    Operation *argOwner = arg.getOwner()->getParentOp();\n+\n+    if (auto forOp = dyn_cast<scf::ForOp>(argOwner)) {\n+      return getMakeTensorPtrOp(\n+          forOp.getOperand(argNum + forOp.getNumControlOperands() - 1));\n+    } else if (auto funcOp = dyn_cast<mlir::triton::FuncOp>(argOwner)) {\n+      Block *block = arg.getOwner();\n+      Operation *op;\n+      int tOrF;\n+      std::tie(op, tOrF) = blockToCFOps[block][0];\n+      if (auto br = dyn_cast<cf::BranchOp>(op)) {\n+        return getMakeTensorPtrOp(br.getDestOperands()[argNum]);\n+      }\n+      if (auto condBr = dyn_cast<cf::CondBranchOp>(op)) {\n+        if (tOrF) {\n+          return getMakeTensorPtrOp(condBr.getTrueDestOperands()[argNum]);\n+        } else {\n+          return getMakeTensorPtrOp(condBr.getFalseDestOperands()[argNum]);\n+        }\n+      }\n+    } else {\n+      return getMakeTensorPtrOp(argOwner->getOperand(argNum));\n+    }\n+  }\n+\n+  llvm_unreachable(\"Unable to getMakeTensorPtr()\");\n+}\n+\n } // namespace mlir"}, {"filename": "lib/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -3,3 +3,4 @@ add_subdirectory(Analysis)\n add_subdirectory(Conversion)\n add_subdirectory(Dialect)\n add_subdirectory(Target)\n+add_subdirectory(Hopper)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/BarrierOpToLLVM.cpp", "status": "added", "additions": 217, "deletions": 0, "changes": 217, "file_content_changes": "@@ -0,0 +1,217 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"BarrierOpToLLVM.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+// --------------------------------------------------------------------------\n+// -- MBarrier related Ops lowering, to be moved to a seperate file ---------\n+// --------------------------------------------------------------------------\n+struct AllocMBarrierOpConversion : public ConvertTritonGPUOpToLLVMPattern<\n+                                       triton::nvidia_gpu::AllocMBarrierOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::AllocMBarrierOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::AllocMBarrierOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getResult());\n+    auto resultTy = op.getType();\n+    auto resultTensorTy = resultTy.dyn_cast<RankedTensorType>();\n+    Type elemPtrTy;\n+    if (resultTensorTy) {\n+      auto llvmElemTy =\n+          getTypeConverter()->convertType(resultTensorTy.getElementType());\n+      elemPtrTy = ptr_ty(llvmElemTy, 3);\n+    } else {\n+      elemPtrTy = getTypeConverter()->convertType(resultTy);\n+    }\n+    smemBase = bitcast(smemBase, elemPtrTy);\n+    auto threadId = getThreadId(rewriter, loc);\n+    auto pred = icmp_eq(threadId, i32_val(0));\n+    int numMBarriers = 1;\n+    if (resultTensorTy) {\n+      assert(resultTensorTy.getRank() == 1 &&\n+             \"unexpected rank for AllocMBarrierOp\");\n+      numMBarriers = resultTensorTy.getShape()[0];\n+    }\n+    for (int i = 0; i < numMBarriers; ++i) {\n+      Value smem = smemBase;\n+      if (i > 0) {\n+        smem = gep(elemPtrTy, smem, i32_val(i));\n+      }\n+      rewriter.create<triton::nvgpu::MBarrierInitOp>(loc, smem, pred,\n+                                                     op.getCount());\n+    }\n+    if (resultTensorTy) {\n+      auto smemObj = SharedMemoryObject(smemBase, resultTensorTy.getShape(),\n+                                        {0}, loc, rewriter);\n+      auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n+      rewriter.replaceOp(op, retVal);\n+    } else {\n+      rewriter.replaceOp(op, smemBase);\n+    }\n+    return success();\n+  }\n+};\n+\n+struct MBarrierArriveOpConversion : public ConvertTritonGPUOpToLLVMPattern<\n+                                        triton::nvidia_gpu::MBarrierArriveOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::MBarrierArriveOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::MBarrierArriveOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    auto mbarrier = adaptor.getMbarrier();\n+    bool trackAsyncOp = op.getTrackAsyncOp();\n+    triton::nvgpu::MBarriveType type = triton::nvgpu::MBarriveType::normal;\n+    uint32_t txCount = op.getTxCount();\n+    auto remoteCtaId = adaptor.getRemoteCtaId();\n+    if (trackAsyncOp) {\n+      type = triton::nvgpu::MBarriveType::cp_async;\n+    } else if (remoteCtaId) {\n+      assert(txCount == 0 &&\n+             \"remote arrive of transaction mbarrier is not implemented yet\");\n+      type = triton::nvgpu::MBarriveType::remote;\n+    } else if (txCount > 0) {\n+      type = triton::nvgpu::MBarriveType::expect_tx;\n+    }\n+    Value pred = adaptor.getPred();\n+    if (pred == nullptr) {\n+      pred = int_val(/*width*/ 1, 1);\n+    }\n+    rewriter.replaceOpWithNewOp<triton::nvgpu::MBarrierArriveOp>(\n+        op, mbarrier, pred, remoteCtaId, type, txCount);\n+    return success();\n+  }\n+};\n+\n+struct MBarrierWaitOpConversion : public ConvertTritonGPUOpToLLVMPattern<\n+                                      triton::nvidia_gpu::MBarrierWaitOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::MBarrierWaitOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::MBarrierWaitOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    rewriter.replaceOpWithNewOp<triton::nvgpu::MBarrierWaitOp>(\n+        op, adaptor.getMbarrier(), adaptor.getPhase());\n+    return success();\n+  }\n+};\n+\n+struct ExtractMBarrierOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<\n+          triton::nvidia_gpu::ExtractMBarrierOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::ExtractMBarrierOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::ExtractMBarrierOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    auto elemTy =\n+        op.getTensor().getType().cast<RankedTensorType>().getElementType();\n+    auto tensorStruct = adaptor.getTensor();\n+    auto index = adaptor.getIndex();\n+    auto ptrTy =\n+        LLVM::LLVMPointerType::get(getTypeConverter()->convertType(elemTy), 3);\n+    auto basePtr =\n+        extract_val(ptrTy, tensorStruct, rewriter.getDenseI64ArrayAttr(0));\n+    Value result = gep(ptrTy, basePtr, index);\n+    rewriter.replaceOp(op, result);\n+    return success();\n+  }\n+};\n+\n+struct NamedBarrierArriveOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<\n+          triton::nvidia_gpu::NamedBarrierArriveOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::NamedBarrierArriveOp>::\n+      ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::NamedBarrierArriveOp op,\n+                  OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    rewriter.replaceOpWithNewOp<triton::nvgpu::NamedBarrierArriveOp>(\n+        op, adaptor.getBar(), adaptor.getNumThreads());\n+    return success();\n+  }\n+};\n+\n+struct NamedBarrierWaitOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<\n+          triton::nvidia_gpu::NamedBarrierWaitOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::NamedBarrierWaitOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::NamedBarrierWaitOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    rewriter.replaceOpWithNewOp<triton::nvgpu::NamedBarrierWaitOp>(\n+        op, adaptor.getBar(), adaptor.getNumThreads());\n+    return success();\n+  }\n+};\n+\n+struct FenceAsyncSharedOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<\n+          triton::nvidia_gpu::FenceAsyncSharedOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::FenceAsyncSharedOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::FenceAsyncSharedOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    rewriter.replaceOpWithNewOp<triton::nvgpu::FenceAsyncSharedOp>(\n+        op, adaptor.getBCluster());\n+    return success();\n+  }\n+};\n+\n+void populateBarrierOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    ModuleAllocation &allocation, PatternBenefit benefit) {\n+  patterns.add<AllocMBarrierOpConversion>(typeConverter, allocation, benefit);\n+  patterns.add<MBarrierArriveOpConversion>(typeConverter, allocation, benefit);\n+  patterns.add<MBarrierWaitOpConversion>(typeConverter, allocation, benefit);\n+  patterns.add<ExtractMBarrierOpConversion>(typeConverter, allocation, benefit);\n+  patterns.add<NamedBarrierArriveOpConversion>(typeConverter, allocation,\n+                                               benefit);\n+  patterns.add<NamedBarrierWaitOpConversion>(typeConverter, allocation,\n+                                             benefit);\n+  patterns.add<FenceAsyncSharedOpConversion>(typeConverter, allocation,\n+                                             benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/BarrierOpToLLVM.h", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -0,0 +1,37 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_BARRIER_OP_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_BARRIER_OP_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateBarrierOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    ModuleAllocation &allocation, PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "file_content_changes": "@@ -1,4 +1,16 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n+    ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp\n+    ConvertLayoutOpToLLVM.cpp\n+    DotOpToLLVM/FMA.cpp\n+    DotOpToLLVM/MMAv1.cpp\n+    DotOpToLLVM/MMAv2.cpp\n+    DotOpToLLVM/WGMMA.cpp\n+    DotOpToLLVM.cpp\n+    ElementwiseOpToLLVM.cpp\n+    LoadStoreOpToLLVM.cpp\n+    BarrierOpToLLVM.cpp\n     TritonGPUToLLVM.cpp\n     GCNAsmFormat.cpp\n     PTXAsmFormat.cpp\n@@ -15,12 +27,16 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     LoadStoreOpToLLVM.cpp\n     TritonGPUToLLVM.cpp\n     TritonGPUToLLVMPass.cpp\n+    GCNAsmFormat.cpp\n     PTXAsmFormat.cpp\n     ReduceOpToLLVM.cpp\n     ScanOpToLLVM.cpp\n-    Utility.cpp\n     TypeConverter.cpp\n+    Utility.cpp\n     ViewOpToLLVM.cpp\n+    TensorPtrOpsToLLVM.cpp\n+    ClusterOpsToLLVM.cpp\n+    RegReallocOpToLLVM.cpp\n \n     ADDITIONAL_HEADER_DIRS\n     ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonGPUToLLVM\n@@ -43,4 +59,6 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     TritonIR\n     TritonGPUIR\n     TritonGPUTransforms\n+    TritonNvidiaGPUTransforms\n+    NVGPUIR\n )"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ClusterOpsToLLVM.cpp", "status": "added", "additions": 62, "deletions": 0, "changes": 62, "file_content_changes": "@@ -0,0 +1,62 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"ClusterOpsToLLVM.h\"\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+struct ClusterArriveOpConversion : public ConvertTritonGPUOpToLLVMPattern<\n+                                       triton::nvidia_gpu::ClusterArriveOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::ClusterArriveOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::ClusterArriveOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<triton::nvgpu::ClusterArriveOp>(\n+        op, op.getRelaxed());\n+    return success();\n+  }\n+};\n+\n+struct ClusterWaitOpConversion : public ConvertTritonGPUOpToLLVMPattern<\n+                                     triton::nvidia_gpu::ClusterWaitOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::ClusterWaitOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::ClusterWaitOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<triton::nvgpu::ClusterWaitOp>(op);\n+    return success();\n+  }\n+};\n+\n+void populateClusterOpsToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    ModuleAllocation &allocation, PatternBenefit benefit) {\n+  patterns.add<ClusterArriveOpConversion>(typeConverter, benefit);\n+  patterns.add<ClusterWaitOpConversion>(typeConverter, benefit);\n+  return;\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ClusterOpsToLLVM.h", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -0,0 +1,37 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_CLUSTER_OPS_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_CLUSTER_OPS_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateClusterOpsToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    ModuleAllocation &allocation, PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 307, "deletions": 56, "changes": 363, "file_content_changes": "@@ -1,14 +1,18 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"Utility.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n-using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::LLVM::linearize;\n+\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getShapePerCTATile;\n using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n@@ -72,6 +76,13 @@ struct ConvertLayoutOpConversion\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       return lowerSharedToDotOperand(op, adaptor, rewriter);\n     }\n+    // forwarding on mma->mma shortcut, lower distributed->distributed otherwise\n+    if (srcLayout.isa<MmaEncodingAttr>() && dstLayout.isa<MmaEncodingAttr>()) {\n+      if (isMmaToMmaShortcut(srcTy, dstTy)) {\n+        rewriter.replaceOp(op, op.getSrc());\n+        return success();\n+      }\n+    }\n     if (isaDistributedLayout(srcLayout) && isaDistributedLayout(dstLayout)) {\n       return lowerDistributedToDistributed(op, adaptor, rewriter);\n     }\n@@ -89,23 +100,25 @@ struct ConvertLayoutOpConversion\n   }\n \n private:\n-  SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,\n-                                       ConversionPatternRewriter &rewriter,\n-                                       unsigned elemId, RankedTensorType type,\n-                                       ArrayRef<unsigned> multiDimCTAInRepId,\n-                                       ArrayRef<unsigned> shapePerCTA) const {\n+  SmallVector<Value>\n+  getMultiDimOffset(Attribute layout, Location loc,\n+                    ConversionPatternRewriter &rewriter, unsigned elemId,\n+                    RankedTensorType type,\n+                    ArrayRef<unsigned> multiDimCTAInRepId,\n+                    ArrayRef<unsigned> shapePerCTATile) const {\n     auto shape = type.getShape();\n     unsigned rank = shape.size();\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n       auto multiDimOffsetFirstElem =\n-          emitBaseIndexForLayout(loc, rewriter, blockedLayout, type);\n+          emitBaseIndexForLayout(loc, rewriter, blockedLayout, type, false);\n       SmallVector<Value> multiDimOffset(rank);\n       SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n           elemId, getSizePerThread(layout), getOrder(layout));\n       for (unsigned d = 0; d < rank; ++d) {\n-        multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n-                                i32_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n-                                        multiDimElemId[d]));\n+        multiDimOffset[d] =\n+            add(multiDimOffsetFirstElem[d],\n+                i32_val(multiDimCTAInRepId[d] * shapePerCTATile[d] +\n+                        multiDimElemId[d]));\n       }\n       return multiDimOffset;\n     }\n@@ -127,7 +140,7 @@ struct ConvertLayoutOpConversion\n       auto multiDimOffsetParent = getMultiDimOffset(\n           parentEncoding, loc, rewriter, idxs[elemId], parentTy,\n           sliceLayout.paddedShape(multiDimCTAInRepId),\n-          sliceLayout.paddedShape(shapePerCTA));\n+          sliceLayout.paddedShape(shapePerCTATile));\n       SmallVector<Value> multiDimOffset(rank);\n       for (unsigned d = 0; d < rank + 1; ++d) {\n         if (d == dim)\n@@ -138,34 +151,44 @@ struct ConvertLayoutOpConversion\n       return multiDimOffset;\n     }\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n+      auto instrShape = mmaLayout.getInstrShape();\n       SmallVector<Value> mmaColIdx(4);\n       SmallVector<Value> mmaRowIdx(2);\n       Value threadId = getThreadId(rewriter, loc);\n       Value warpSize = i32_val(32);\n       Value laneId = urem(threadId, warpSize);\n       Value warpId = udiv(threadId, warpSize);\n       // TODO: fix the bug in MMAEncodingAttr document\n+      SmallVector<Value> multiDimWarpId(2);\n       auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n-      auto order = triton::gpu::getOrder(mmaLayout);\n-      SmallVector<Value> multiDimWarpId =\n-          delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+      if (mmaLayout.isHopper()) {\n+        multiDimWarpId[0] = urem(warpId, i32_val(warpsPerCTA[0]));\n+        multiDimWarpId[1] = udiv(warpId, i32_val(warpsPerCTA[0]));\n+      } else {\n+        auto order = triton::gpu::getOrder(mmaLayout);\n+        multiDimWarpId = delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+      }\n       Value _1 = i32_val(1);\n       Value _2 = i32_val(2);\n       Value _4 = i32_val(4);\n       Value _8 = i32_val(8);\n       Value _16 = i32_val(16);\n-      if (mmaLayout.isAmpere()) {\n-        multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n-        multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n+      if (mmaLayout.isAmpere() || mmaLayout.isHopper()) {\n+        multiDimWarpId[0] =\n+            urem(multiDimWarpId[0], i32_val(shapePerCTA[0] / instrShape[0]));\n+        multiDimWarpId[1] =\n+            urem(multiDimWarpId[1], i32_val(shapePerCTA[1] / instrShape[1]));\n+\n         Value mmaGrpId = udiv(laneId, _4);\n         Value mmaGrpIdP8 = add(mmaGrpId, _8);\n         Value mmaThreadIdInGrp = urem(laneId, _4);\n         Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, _2);\n         Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, _1);\n-        Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n+        Value rowWarpOffset = mul(multiDimWarpId[0], i32_val(instrShape[0]));\n         mmaRowIdx[0] = add(mmaGrpId, rowWarpOffset);\n         mmaRowIdx[1] = add(mmaGrpIdP8, rowWarpOffset);\n-        Value colWarpOffset = mul(multiDimWarpId[1], _8);\n+        Value colWarpOffset = mul(multiDimWarpId[1], i32_val(instrShape[1]));\n         mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n         mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n       } else if (mmaLayout.isVolta()) {\n@@ -176,13 +199,27 @@ struct ConvertLayoutOpConversion\n \n       assert(rank == 2);\n       SmallVector<Value> multiDimOffset(rank);\n-      if (mmaLayout.isAmpere()) {\n+      if (mmaLayout.isHopper()) {\n+        unsigned elemIdRem4 = elemId % 4;\n+        unsigned nGrpId = elemId / 4;\n+        multiDimOffset[0] = elemIdRem4 < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n+        multiDimOffset[1] = elemIdRem4 % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n+        multiDimOffset[1] = add(multiDimOffset[1], i32_val(8 * nGrpId));\n+        multiDimOffset[0] =\n+            add(multiDimOffset[0],\n+                i32_val(multiDimCTAInRepId[0] * shapePerCTATile[0]));\n+        multiDimOffset[1] =\n+            add(multiDimOffset[1],\n+                i32_val(multiDimCTAInRepId[1] * shapePerCTATile[1]));\n+      } else if (mmaLayout.isAmpere()) {\n         multiDimOffset[0] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n         multiDimOffset[1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n-        multiDimOffset[0] = add(\n-            multiDimOffset[0], i32_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n-        multiDimOffset[1] = add(\n-            multiDimOffset[1], i32_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+        multiDimOffset[0] =\n+            add(multiDimOffset[0],\n+                i32_val(multiDimCTAInRepId[0] * shapePerCTATile[0]));\n+        multiDimOffset[1] =\n+            add(multiDimOffset[1],\n+                i32_val(multiDimCTAInRepId[1] * shapePerCTATile[1]));\n       } else if (mmaLayout.isVolta()) {\n         auto [isARow, isBRow, isAVec4, isBVec4, _] =\n             mmaLayout.decodeVoltaLayoutStates();\n@@ -211,11 +248,12 @@ struct ConvertLayoutOpConversion\n     auto rank = type.getRank();\n     auto sizePerThread = getSizePerThread(layout);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n-    SmallVector<unsigned> numCTAs(rank);\n+    SmallVector<unsigned> numCTATiles(rank);\n+    auto shapePerCTATile = getShapePerCTATile(layout);\n     auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n     auto order = getOrder(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n-      numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n+      numCTATiles[d] = ceil<unsigned>(shapePerCTA[d], shapePerCTATile[d]);\n     }\n     auto elemTy = type.getElementType();\n     bool isInt1 = elemTy.isInteger(1);\n@@ -238,17 +276,16 @@ struct ConvertLayoutOpConversion\n       }\n \n       auto linearCTAId =\n-          getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n+          getLinearIndex<unsigned>(multiDimCTAId, numCTATiles, order);\n       // TODO: This is actually redundant index calculation, we should\n       //       consider of caching the index calculation result in case\n       //       of performance issue observed.\n       for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n         SmallVector<Value> multiDimOffset =\n             getMultiDimOffset(layout, loc, rewriter, elemId, type,\n-                              multiDimCTAInRepId, shapePerCTA);\n+                              multiDimCTAInRepId, shapePerCTATile);\n         Value offset =\n             linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n-\n         auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n         Value ptr = gep(elemPtrTy, smemBase, offset);\n         auto vecTy = vec_ty(llvmElemTy, vec);\n@@ -305,7 +342,8 @@ struct ConvertLayoutOpConversion\n \n     SmallVector<unsigned> numCTAs(rank, 1);\n     SmallVector<unsigned> numCTAsEachRep(rank, 1);\n-    SmallVector<unsigned> shapePerCTA = getShapePerCTA(layout, shape);\n+    SmallVector<unsigned> shapePerCTATile = getShapePerCTATile(layout, shape);\n+    SmallVector<int64_t> shapePerCTA = getShapePerCTA(layout, shape);\n     auto elemTy = type.getElementType();\n \n     int ctaId = 0;\n@@ -335,15 +373,15 @@ struct ConvertLayoutOpConversion\n         // duplicate in Volta.\n         SmallVector<Value> multiDimOffset =\n             getMultiDimOffset(layout, loc, rewriter, elemId, type,\n-                              multiDimCTAInRepId, shapePerCTA);\n+                              multiDimCTAInRepId, shapePerCTATile);\n         coord2val[elemId] = std::make_pair(multiDimOffset, vals[elemId]);\n       }\n \n       if (needTrans) {\n         // do transpose\n         auto aEncoding =\n             DotOperandEncodingAttr::get(mma.getContext(), 0, mma, 0);\n-        int numM = aEncoding.getMMAv1NumOuter(shape);\n+        int numM = aEncoding.getMMAv1NumOuter(shapePerCTA);\n         int numN = accumSizePerThread / numM;\n \n         for (int r = 0; r < numM; r++) {\n@@ -382,6 +420,91 @@ struct ConvertLayoutOpConversion\n     }\n   }\n \n+  LogicalResult\n+  lowerDistToDistWithDistSmem(triton::gpu::ConvertLayoutOp op,\n+                              OpAdaptor adaptor,\n+                              ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+\n+    Value src = op.getSrc();\n+    Value dst = op.getResult();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto srcLayout = srcTy.getEncoding();\n+    auto dstLayout = dstTy.getEncoding();\n+    auto srcShapePerCTA = getShapePerCTA(srcTy);\n+    auto srcCTAsPerCGA = triton::gpu::getCTAsPerCGA(srcLayout);\n+    auto srcCTAOrder = triton::gpu::getCTAOrder(srcLayout);\n+    unsigned rank = srcShapePerCTA.size();\n+\n+    auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+    auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+    smemBase = bitcast(smemBase, elemPtrTy);\n+    auto smemShape = convertType<unsigned, int64_t>(srcShapePerCTA);\n+\n+    // Store to local shared memory\n+    {\n+      auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n+                                                         rewriter, srcTy);\n+      auto inIndices =\n+          emitIndices(loc, rewriter, srcLayout, srcTy, /*withCTAOffset*/ false);\n+\n+      assert(inIndices.size() == inVals.size() &&\n+             \"Unexpected number of indices emitted\");\n+\n+      for (unsigned i = 0; i < inIndices.size(); ++i) {\n+        Value offset = linearize(rewriter, loc, inIndices[i], smemShape);\n+        Value ptr = gep(elemPtrTy, smemBase, offset);\n+        store(inVals[i], ptr);\n+      }\n+    }\n+\n+    // Cluster barrier\n+    rewriter.create<triton::nvidia_gpu::ClusterArriveOp>(loc, false);\n+    rewriter.create<triton::nvidia_gpu::ClusterWaitOp>(loc);\n+\n+    // Load from remote shared memory\n+    {\n+      SmallVector<Value> srcShapePerCTACache;\n+      for (unsigned i = 0; i < rank; ++i)\n+        srcShapePerCTACache.push_back(i32_val(srcShapePerCTA[i]));\n+\n+      SmallVector<Value> outVals;\n+      auto outIndices =\n+          emitIndices(loc, rewriter, dstLayout, dstTy, /*withCTAOffset*/ true);\n+\n+      for (unsigned i = 0; i < outIndices.size(); ++i) {\n+        auto coord = outIndices[i];\n+        assert(coord.size() == rank && \"Unexpected rank of index emitted\");\n+\n+        SmallVector<Value> multiDimCTAId, localCoord;\n+        for (unsigned d = 0; d < rank; ++d) {\n+          multiDimCTAId.push_back(udiv(coord[d], srcShapePerCTACache[d]));\n+          localCoord.push_back(urem(coord[d], srcShapePerCTACache[d]));\n+        }\n+\n+        Value remoteCTAId =\n+            linearize(rewriter, loc, multiDimCTAId, srcCTAsPerCGA, srcCTAOrder);\n+        Value localOffset = linearize(rewriter, loc, localCoord, smemShape);\n+\n+        Value ptr = gep(elemPtrTy, smemBase, localOffset);\n+        outVals.push_back(load_dsmem(ptr, remoteCTAId));\n+      }\n+\n+      Value result =\n+          getTypeConverter()->packLLElements(loc, outVals, rewriter, dstTy);\n+      rewriter.replaceOp(op, result);\n+    }\n+\n+    // Cluster barrier\n+    rewriter.create<triton::nvidia_gpu::ClusterArriveOp>(loc, false);\n+    rewriter.create<triton::nvidia_gpu::ClusterWaitOp>(loc);\n+\n+    return success();\n+  }\n+\n   // blocked/mma -> blocked/mma.\n   // Data padding in shared memory to avoid bank conflict.\n   LogicalResult\n@@ -395,6 +518,10 @@ struct ConvertLayoutOpConversion\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n+\n+    if (shouldUseDistSmem(srcLayout, dstLayout))\n+      return lowerDistToDistWithDistSmem(op, adaptor, rewriter);\n+\n     auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n     Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n@@ -406,8 +533,9 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> outNumCTAsEachRep(rank);\n     SmallVector<unsigned> inNumCTAs(rank);\n     SmallVector<unsigned> outNumCTAs(rank);\n-    auto srcShapePerCTA = getShapePerCTA(srcLayout, srcTy.getShape());\n-    auto dstShapePerCTA = getShapePerCTA(dstLayout, shape);\n+    auto srcShapePerCTATile = getShapePerCTATile(srcLayout, srcTy.getShape());\n+    auto dstShapePerCTATile = getShapePerCTATile(dstLayout, shape);\n+    auto shapePerCTA = getShapePerCTA(srcLayout, shape);\n \n     // For Volta, all the coords for a CTA are calculated.\n     bool isSrcMmaV1{}, isDstMmaV1{};\n@@ -427,15 +555,17 @@ struct ConvertLayoutOpConversion\n     }\n \n     for (unsigned d = 0; d < rank; ++d) {\n-      unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n-      unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n+      unsigned inPerCTA =\n+          std::min<unsigned>(shapePerCTA[d], srcShapePerCTATile[d]);\n+      unsigned outPerCTA =\n+          std::min<unsigned>(shapePerCTA[d], dstShapePerCTATile[d]);\n       unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n-      numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n+      numReplicates[d] = ceil<unsigned>(shapePerCTA[d], maxPerCTA);\n       inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n       outNumCTAsEachRep[d] = maxPerCTA / outPerCTA;\n       assert(maxPerCTA % inPerCTA == 0 && maxPerCTA % outPerCTA == 0);\n-      inNumCTAs[d] = ceil<unsigned>(shape[d], inPerCTA);\n-      outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n+      inNumCTAs[d] = ceil<unsigned>(shapePerCTA[d], inPerCTA);\n+      outNumCTAs[d] = ceil<unsigned>(shapePerCTA[d], outPerCTA);\n     }\n     // Potentially we need to store for multiple CTAs in this replication\n     auto accumNumReplicates = product<unsigned>(numReplicates);\n@@ -456,8 +586,26 @@ struct ConvertLayoutOpConversion\n     for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n       auto multiDimRepId =\n           getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);\n-      if (repId != 0)\n-        barrier();\n+      if (repId != 0) {\n+        // TODO[shuhaoj]: change hard code style of numThreads. Hide async\n+        // attr.  Better way to determine barId (number of agents are limited).\n+        if (op->hasAttr(\"async_agent\")) {\n+          int agentId = getAgentIds(op).front(), roleId = 0;\n+          if (op->hasAttr(\"agent.mutex_role\"))\n+            roleId =\n+                op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+          int barId = agentId + roleId + nameBarrierIdBegin;\n+          assert(barId < nameBarrierIdEnd);\n+          auto bar = rewriter.create<LLVM::ConstantOp>(\n+              loc, i32_ty, rewriter.getI32IntegerAttr(barId));\n+          auto kNumThreads = rewriter.create<LLVM::ConstantOp>(\n+              loc, i32_ty, rewriter.getI32IntegerAttr(128));\n+          rewriter.create<triton::nvgpu::NamedBarrierWaitOp>(loc, bar,\n+                                                             kNumThreads);\n+        } else {\n+          barrier();\n+        }\n+      }\n       if (srcLayout.isa<BlockedEncodingAttr>() ||\n           srcLayout.isa<SliceEncodingAttr>() ||\n           srcLayout.isa<MmaEncodingAttr>()) {\n@@ -474,7 +622,23 @@ struct ConvertLayoutOpConversion\n         return failure();\n       }\n \n-      barrier();\n+      // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n+      // attr.  Better way to determine barId (number of agents are limited).\n+      if (op->hasAttr(\"async_agent\")) {\n+        int agentId = getAgentIds(op).front(), roleId = 0;\n+        if (op->hasAttr(\"agent.mutex_role\"))\n+          roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+        int barId = agentId + roleId + nameBarrierIdBegin;\n+        assert(barId < nameBarrierIdEnd);\n+        auto bar = rewriter.create<LLVM::ConstantOp>(\n+            loc, i32_ty, rewriter.getI32IntegerAttr(barId));\n+        auto kNumThreads = rewriter.create<LLVM::ConstantOp>(\n+            loc, i32_ty, rewriter.getI32IntegerAttr(128));\n+        rewriter.create<triton::nvgpu::NamedBarrierWaitOp>(loc, bar,\n+                                                           kNumThreads);\n+      } else {\n+        barrier();\n+      }\n       if (dstLayout.isa<BlockedEncodingAttr>() ||\n           dstLayout.isa<SliceEncodingAttr>() ||\n           dstLayout.isa<MmaEncodingAttr>()) {\n@@ -533,6 +697,15 @@ struct ConvertLayoutOpConversion\n     return success();\n   }\n \n+  // Pack two 16-bit values into a 32-bit register.\n+  static Value pack16bitsTo32(ConversionPatternRewriter &rewriter, Location loc,\n+                              Value hb, Value lb) {\n+    hb = zext(i32_ty, bitcast(hb, i16_ty));\n+    lb = zext(i32_ty, bitcast(lb, i16_ty));\n+    Value pack = or_(lb, shl(hb, i32_val(16)));\n+    return pack;\n+  }\n+\n   // blocked -> shared.\n   // Swizzling in shared memory to avoid bank conflict. Normally used for\n   // A/B operands of dots.\n@@ -545,7 +718,7 @@ struct ConvertLayoutOpConversion\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto srcShape = srcTy.getShape();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n-    auto dstShape = dstTy.getShape();\n+    auto dstShapePerCTA = triton::gpu::getShapePerCTA(dstTy);\n     assert(srcShape.size() == 2 &&\n            \"Unexpected rank of ConvertLayout(blocked->shared)\");\n     auto srcLayout = srcTy.getEncoding();\n@@ -557,13 +730,93 @@ struct ConvertLayoutOpConversion\n     auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n     smemBase = bitcast(smemBase, elemPtrTy);\n \n-    auto dstStrides =\n-        getStridesFromShapeAndOrder(dstShape, outOrd, loc, rewriter);\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n-    storeDistributedToShared(src, adaptor.getSrc(), dstStrides, srcIndices, dst,\n-                             smemBase, elemTy, loc, rewriter);\n+    int32_t elemSize = elemTy.getIntOrFloatBitWidth();\n+    auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n+    unsigned numElems = triton::gpu::getTotalElemsPerThread(srcTy);\n+    if (mmaLayout && mmaLayout.isHopper() && elemSize == 16 &&\n+        inOrd == outOrd && numElems >= 16) {\n+      auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n+                                                         rewriter, srcTy);\n+\n+      auto srcShapePerCTA = getShapePerCTA(mmaLayout, srcShape);\n+      auto instrShape = mmaLayout.getInstrShape();\n+      auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+      uint32_t repM =\n+          ceil<unsigned>(srcShapePerCTA[0], instrShape[0] * warpsPerCTA[0]);\n+      uint32_t numElemsPerRep = numElems / repM;\n+      // rowStride in bytes\n+      uint32_t rowStrideInBytes = dstShapePerCTA[outOrd[0]] * 2;\n+      uint32_t swizzlingByteWidth = rowStrideInBytes;\n+      if (swizzlingByteWidth > 128)\n+        swizzlingByteWidth = 128;\n+\n+      unsigned numElemsPerSwizzlingRow = swizzlingByteWidth * 8 / elemSize;\n+      unsigned leadingDimOffset =\n+          numElemsPerSwizzlingRow * srcShapePerCTA[outOrd[1]];\n+\n+      auto ptrI8SharedTy = LLVM::LLVMPointerType::get(\n+          typeConverter->convertType(rewriter.getI8Type()), 3);\n+\n+      uint32_t rowsPerRep = getShapePerCTATile(mmaLayout)[0];\n+\n+      Value threadId = getThreadId(rewriter, loc);\n+      Value warpId = udiv(threadId, i32_val(32));\n+      Value warpId0 = urem(urem(warpId, i32_val(warpsPerCTA[0])),\n+                           i32_val(srcShape[0] / instrShape[0]));\n+\n+      for (int rep = 0; rep < repM; ++rep) {\n+        Value rowOfWarp = add(mul(warpId0, i32_val(instrShape[0])),\n+                              i32_val(rep * rowsPerRep));\n+        uint32_t elemIdxOffset = rep * numElemsPerRep;\n+\n+        for (unsigned idx = 0; idx < numElemsPerRep; idx += 8) {\n+          uint32_t elemIdx = elemIdxOffset + idx;\n+\n+          Value offset = rewriter.create<triton::nvgpu::OffsetOfStmatrixV4Op>(\n+              loc, i32_ty, threadId, rowOfWarp, i32_val(idx), leadingDimOffset,\n+              numElemsPerSwizzlingRow, true);\n+\n+          Value addr = gep(elemPtrTy, smemBase, offset);\n+          Value data0 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 1],\n+                                       inVals[elemIdx + 0]);\n+          Value data1 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 3],\n+                                       inVals[elemIdx + 2]);\n+          Value data2 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 5],\n+                                       inVals[elemIdx + 4]);\n+          Value data3 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 7],\n+                                       inVals[elemIdx + 6]);\n+\n+          rewriter.create<triton::nvgpu::StoreMatrixOp>(\n+              loc, bitcast(addr, ptrI8SharedTy),\n+              ValueRange{data0, data1, data2, data3});\n+        }\n+      }\n+      // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n+      // attr.  Better way to determine barId (number of agents are limited).\n+      if (op->hasAttr(\"async_agent\")) {\n+        int agentId = getAgentIds(op).front(), roleId = 0;\n+        if (op->hasAttr(\"agent.mutex_role\"))\n+          roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+        int barId = agentId + roleId + nameBarrierIdBegin;\n+        assert(barId < nameBarrierIdEnd);\n+        auto bar = rewriter.create<LLVM::ConstantOp>(\n+            loc, i32_ty, rewriter.getI32IntegerAttr(barId));\n+        auto kNumThreads = rewriter.create<LLVM::ConstantOp>(\n+            loc, i32_ty, rewriter.getI32IntegerAttr(128));\n+        rewriter.create<triton::nvgpu::NamedBarrierWaitOp>(loc, bar,\n+                                                           kNumThreads);\n+      } else {\n+        barrier();\n+      }\n+    } else {\n+      auto dstStrides =\n+          getStridesFromShapeAndOrder(dstShapePerCTA, outOrd, loc, rewriter);\n+      auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy, false);\n+      storeDistributedToShared(src, adaptor.getSrc(), dstStrides, srcIndices,\n+                               dst, smemBase, elemTy, loc, rewriter);\n+    }\n     auto smemObj =\n-        SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n+        SharedMemoryObject(smemBase, dstShapePerCTA, outOrd, loc, rewriter);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n     rewriter.replaceOp(op, retVal);\n     return success();\n@@ -688,19 +941,16 @@ struct ConvertLayoutOpConversion\n     auto loc = op.getLoc();\n     Value src = op.getSrc();\n     Value dst = op.getResult();\n+    bool isMMA = supportMMA(dst, mmaLayout.getVersionMajor());\n \n     auto smemObj =\n         getSharedMemoryObjectFromStruct(loc, adaptor.getSrc(), rewriter);\n     Value res;\n-\n     if (!isOuter && mmaLayout.isAmpere()) { // tensor core v2\n-\n       res = SharedToDotOperandMMAv2::convertLayout(\n           dotOperandLayout.getOpIdx(), rewriter, loc, src, dotOperandLayout,\n-          smemObj, getTypeConverter(), tid_val());\n-\n-    } else if (!isOuter && mmaLayout.isVolta() &&\n-               supportMMA(dst, mmaLayout.getVersionMajor())) { // tensor core v1\n+          smemObj, getTypeConverter(), getThreadId(rewriter, loc));\n+    } else if (!isOuter && mmaLayout.isVolta() && isMMA) { // tensor core v1\n       bool isMMAv1Row = dotOperandLayout.getMMAv1IsRow();\n       auto srcSharedLayout = src.getType()\n                                  .cast<RankedTensorType>()\n@@ -722,10 +972,11 @@ struct ConvertLayoutOpConversion\n     }\n     return res;\n   }\n-}; // namespace triton::gpu::ConvertLayoutOp\n+}; // namespace triton::gpu::ConvertLayoutOp>\n \n void populateConvertLayoutOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n     ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -10,6 +10,7 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n \n void populateConvertLayoutOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n     ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 29, "deletions": 26, "changes": 55, "file_content_changes": "@@ -2,8 +2,10 @@\n #include \"../Utility.h\"\n \n using ValueTable = std::map<std::pair<int, int>, Value>;\n+using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::LLVM::linearize;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n@@ -14,31 +16,32 @@ using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n SmallVector<Value>\n-getThreadIds(Value threadId, ArrayRef<unsigned int> shapePerCTA,\n+getThreadIds(Value threadId, ArrayRef<unsigned int> shapePerCTATile,\n              ArrayRef<unsigned int> sizePerThread, ArrayRef<unsigned int> order,\n              ConversionPatternRewriter &rewriter, Location loc) {\n   int dim = order.size();\n   SmallVector<Value> threadIds(dim);\n   for (unsigned k = 0; k < dim - 1; k++) {\n-    Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n+    Value dimK = i32_val(shapePerCTATile[order[k]] / sizePerThread[order[k]]);\n     Value rem = urem(threadId, dimK);\n     threadId = udiv(threadId, dimK);\n     threadIds[order[k]] = rem;\n   }\n-  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+  Value dimK = i32_val(shapePerCTATile[order[dim - 1]]);\n   threadIds[order[dim - 1]] = urem(threadId, dimK);\n   return threadIds;\n }\n \n-int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n+// Get shapePerCTATile for M or N axis.\n+int getShapePerCTATileForMN(BlockedEncodingAttr layout, bool isM) {\n   auto order = layout.getOrder();\n-  auto shapePerCTA = getShapePerCTA(layout);\n+  auto shapePerCTATile = getShapePerCTATile(layout);\n \n-  int mShapePerCTA =\n-      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-  int nShapePerCTA =\n-      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-  return isM ? mShapePerCTA : nShapePerCTA;\n+  int mShapePerCTATile =\n+      order[0] == 1 ? shapePerCTATile[order[1]] : shapePerCTATile[order[0]];\n+  int nShapePerCTATile =\n+      order[0] == 0 ? shapePerCTATile[order[1]] : shapePerCTATile[order[0]];\n+  return isM ? mShapePerCTATile : nShapePerCTATile;\n }\n \n // Get sizePerThread for M or N axis.\n@@ -91,7 +94,7 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n                ConversionPatternRewriter &rewriter) {\n   auto aTensorTy = A.getType().cast<RankedTensorType>();\n   auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto aShape = aTensorTy.getShape();\n+  auto aShapePerCTA = getShapePerCTA(aTensorTy);\n \n   auto aOrder = aLayout.getOrder();\n   auto order = dLayout.getOrder();\n@@ -104,19 +107,19 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n   Value strideA0 = isARow ? strideAK : strideAM;\n   Value strideA1 = isARow ? strideAM : strideAK;\n   int aNumPtr = 8;\n-  int K = aShape[1];\n-  int M = aShape[0];\n+  int K = aShapePerCTA[1];\n+  int M = aShapePerCTA[0];\n \n-  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto shapePerCTATile = getShapePerCTATile(dLayout);\n   auto sizePerThread = getSizePerThread(dLayout);\n \n   Value _0 = i32_val(0);\n \n   Value mContig = i32_val(sizePerThread[order[1]]);\n \n   // threadId in blocked layout\n-  auto threadIds =\n-      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  auto threadIds = getThreadIds(thread, shapePerCTATile, sizePerThread, order,\n+                                rewriter, loc);\n   Value threadIdM = threadIds[0];\n \n   Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n@@ -134,11 +137,11 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n \n   SmallVector<Value> vas;\n \n-  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mShapePerCTATile = getShapePerCTATileForMN(dLayout, true /*isM*/);\n   int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n \n   for (unsigned k = 0; k < K; ++k)\n-    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+    for (unsigned m = 0; m < M; m += mShapePerCTATile)\n       for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n         Value offset =\n             add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n@@ -155,7 +158,7 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n                ConversionPatternRewriter &rewriter) {\n   auto bTensorTy = B.getType().cast<RankedTensorType>();\n   auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto bShape = bTensorTy.getShape();\n+  auto bShapePerCTA = getShapePerCTA(bTensorTy);\n \n   auto bOrder = bLayout.getOrder();\n   auto order = dLayout.getOrder();\n@@ -168,19 +171,19 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n   Value strideB0 = isBRow ? strideBN : strideBK;\n   Value strideB1 = isBRow ? strideBK : strideBN;\n   int bNumPtr = 8;\n-  int K = bShape[0];\n-  int N = bShape[1];\n+  int K = bShapePerCTA[0];\n+  int N = bShapePerCTA[1];\n \n-  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto shapePerCTATile = getShapePerCTATile(dLayout);\n   auto sizePerThread = getSizePerThread(dLayout);\n \n   Value _0 = i32_val(0);\n \n   Value nContig = i32_val(sizePerThread[order[0]]);\n \n   // threadId in blocked layout\n-  auto threadIds =\n-      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  auto threadIds = getThreadIds(thread, shapePerCTATile, sizePerThread, order,\n+                                rewriter, loc);\n   Value threadIdN = threadIds[1];\n \n   Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n@@ -198,11 +201,11 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n \n   SmallVector<Value> vbs;\n \n-  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nShapePerCTATile = getShapePerCTATileForMN(dLayout, false /*isM*/);\n   int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n \n   for (unsigned k = 0; k < K; ++k)\n-    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+    for (unsigned n = 0; n < N; n += nShapePerCTATile)\n       for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n         Value offset =\n             add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -504,7 +504,7 @@ std::function<void(int, int)> getLoadMatrixFn(\n           .cast<RankedTensorType>()\n           .getElementType()\n           .isa<mlir::Float8E4M3B11FNUZType>()) {\n-    bool noTrans = (isA ^ order[0] == 0);\n+    bool noTrans = (isA ^ (order[0] == 0));\n     assert(noTrans && \"float8e4b15 must have row-col layout\");\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 78, "deletions": 3, "changes": 81, "file_content_changes": "@@ -4,7 +4,9 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n \n LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n@@ -19,21 +21,32 @@ LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n                               TritonGPUToLLVMTypeConverter *typeConverter,\n                               ConversionPatternRewriter &rewriter);\n \n+LogicalResult convertWGMMA(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                           TritonGPUToLLVMTypeConverter *typeConverter,\n+                           ConversionPatternRewriter &rewriter, Value thread);\n+\n+LogicalResult convertAsyncWGMMA(triton::nvidia_gpu::DotAsyncOp op,\n+                                triton::nvidia_gpu::DotAsyncOp::Adaptor adaptor,\n+                                TritonGPUToLLVMTypeConverter *typeConverter,\n+                                ConversionPatternRewriter &rewriter,\n+                                Value thread);\n+\n struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::DotOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n   matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n     // D = A * B + C\n     Value A = op.getA();\n     Value D = op.getResult();\n \n     // Here we assume the DotOp's operands always comes from shared memory.\n-    auto AShape = A.getType().cast<RankedTensorType>().getShape();\n+    auto AShapePerCTA = getShapePerCTA(A.getType());\n     size_t reduceAxis = 1;\n-    unsigned K = AShape[reduceAxis];\n+    unsigned K = AShapePerCTA[reduceAxis];\n     bool isOuter = K == 1;\n \n     MmaEncodingAttr mmaLayout = D.getType()\n@@ -45,6 +58,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n         return convertMMA884(op, adaptor, getTypeConverter(), rewriter);\n       if (mmaLayout.isAmpere())\n         return convertMMA16816(op, adaptor, getTypeConverter(), rewriter);\n+      if (mmaLayout.isHopper())\n+        return convertWGMMA(op, adaptor, getTypeConverter(), rewriter,\n+                            getThreadId(rewriter, loc));\n \n       llvm::report_fatal_error(\n           \"Unsupported MMA kind found when converting DotOp to LLVM.\");\n@@ -61,9 +77,68 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   }\n };\n \n+struct DotAsyncOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::nvidia_gpu::DotAsyncOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::DotAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::DotAsyncOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    // D = A * B + C\n+    Value A = op.getA();\n+    Value D = op.getResult();\n+\n+    // Here we assume the DotOp's operands always comes from shared memory.\n+    auto AShapePerCTA = getShapePerCTA(A.getType());\n+    size_t reduceAxis = 1;\n+    unsigned K = AShapePerCTA[reduceAxis];\n+    bool isOuter = K == 1;\n+\n+    MmaEncodingAttr mmaLayout = D.getType()\n+                                    .cast<RankedTensorType>()\n+                                    .getEncoding()\n+                                    .dyn_cast<MmaEncodingAttr>();\n+    if (!isOuter && mmaLayout &&\n+        supportMMA(op.getOperand(0), mmaLayout.getVersionMajor())) {\n+      if (mmaLayout.isHopper()) {\n+        return convertAsyncWGMMA(op, adaptor, getTypeConverter(), rewriter,\n+                                 getThreadId(rewriter, loc));\n+      }\n+\n+      llvm::report_fatal_error(\n+          \"Unsupported MMA kind found when converting DotAsyncOp to LLVM.\");\n+    }\n+\n+    llvm::report_fatal_error(\n+        \"Unsupported DotAsyncOp found when converting TritonGPU to LLVM.\");\n+  }\n+};\n+\n+struct DotWaitOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::nvidia_gpu::DotWaitOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::DotWaitOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::DotWaitOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto pendings = op.getPendings();\n+    rewriter.create<triton::nvgpu::WGMMAWaitOp>(op.getLoc(), pendings);\n+\n+    // Safe to remove the op since it doesn't have any return value.\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n-                                 RewritePatternSet &patterns,\n+                                 RewritePatternSet &patterns, int numWarps,\n+                                 ModuleAxisInfoAnalysis &axisInfoAnalysis,\n                                  ModuleAllocation &allocation,\n                                  PatternBenefit benefit) {\n   patterns.add<DotOpConversion>(typeConverter, allocation, benefit);\n+  patterns.add<DotAsyncOpConversion>(typeConverter, allocation, benefit);\n+  patterns.add<DotWaitOpConversion>(typeConverter, allocation, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -7,7 +7,8 @@ using namespace mlir;\n using namespace mlir::triton;\n \n void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n-                                 RewritePatternSet &patterns,\n+                                 RewritePatternSet &patterns, int numWarps,\n+                                 ModuleAxisInfoAnalysis &axisInfoAnalysis,\n                                  ModuleAllocation &allocation,\n                                  PatternBenefit benefit);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/FMA.cpp", "status": "modified", "additions": 22, "deletions": 20, "changes": 42, "file_content_changes": "@@ -5,19 +5,20 @@ using namespace mlir;\n using namespace mlir::triton;\n \n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n \n using ValueTableFMA = std::map<std::pair<int, int>, Value>;\n \n static ValueTableFMA getValueTableFromStructFMA(\n-    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+    Value val, int K, int n0, int shapePerCTATile, int sizePerThread,\n     ConversionPatternRewriter &rewriter, Location loc,\n     TritonGPUToLLVMTypeConverter *typeConverter, Type type) {\n   ValueTableFMA res;\n   auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n   int index = 0;\n   for (unsigned k = 0; k < K; ++k) {\n-    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+    for (unsigned m = 0; m < n0; m += shapePerCTATile)\n       for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n         res[{m + mm, k}] = elems[index++];\n       }\n@@ -40,8 +41,8 @@ LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n   auto bTensorTy = B.getType().cast<RankedTensorType>();\n   auto dTensorTy = D.getType().cast<RankedTensorType>();\n \n-  auto aShape = aTensorTy.getShape();\n-  auto bShape = bTensorTy.getShape();\n+  auto aShapePerCTA = getShapePerCTA(aTensorTy);\n+  auto bShapePerCTA = getShapePerCTA(bTensorTy);\n \n   BlockedEncodingAttr dLayout =\n       dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n@@ -53,41 +54,42 @@ LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n   Value llB = adaptor.getB();\n \n   auto sizePerThread = getSizePerThread(dLayout);\n-  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto shapePerCTATile = getShapePerCTATile(dLayout);\n \n-  int K = aShape[1];\n-  int M = aShape[0];\n-  int N = bShape[1];\n+  int K = aShapePerCTA[1];\n+  int M = aShapePerCTA[0];\n+  int N = bShapePerCTA[1];\n \n-  int mShapePerCTA =\n-      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int mShapePerCTATile =\n+      order[0] == 1 ? shapePerCTATile[order[1]] : shapePerCTATile[order[0]];\n   int mSizePerThread =\n       order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-  int nShapePerCTA =\n-      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int nShapePerCTATile =\n+      order[0] == 0 ? shapePerCTATile[order[1]] : shapePerCTATile[order[0]];\n   int nSizePerThread =\n       order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n \n   auto has =\n-      getValueTableFromStructFMA(llA, K, M, mShapePerCTA, mSizePerThread,\n+      getValueTableFromStructFMA(llA, K, M, mShapePerCTATile, mSizePerThread,\n                                  rewriter, loc, typeConverter, aTensorTy);\n   auto hbs =\n-      getValueTableFromStructFMA(llB, K, N, nShapePerCTA, nSizePerThread,\n+      getValueTableFromStructFMA(llB, K, N, nShapePerCTATile, nSizePerThread,\n                                  rewriter, loc, typeConverter, bTensorTy);\n \n   SmallVector<Value> ret = cc;\n   bool isCRow = order[0] == 1;\n \n   for (unsigned k = 0; k < K; k++) {\n-    for (unsigned m = 0; m < M; m += mShapePerCTA)\n-      for (unsigned n = 0; n < N; n += nShapePerCTA)\n+    for (unsigned m = 0; m < M; m += mShapePerCTATile)\n+      for (unsigned n = 0; n < N; n += nShapePerCTATile)\n         for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n           for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n-            int mIdx = m / mShapePerCTA * mSizePerThread + mm;\n-            int nIdx = n / nShapePerCTA * nSizePerThread + nn;\n+            int mIdx = m / mShapePerCTATile * mSizePerThread + mm;\n+            int nIdx = n / nShapePerCTATile * nSizePerThread + nn;\n \n-            int z = isCRow ? mIdx * N / nShapePerCTA * mSizePerThread + nIdx\n-                           : nIdx * M / mShapePerCTA * nSizePerThread + mIdx;\n+            int z = isCRow\n+                        ? mIdx * N / nShapePerCTATile * mSizePerThread + nIdx\n+                        : nIdx * M / mShapePerCTATile * nSizePerThread + mIdx;\n             ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n                                                       hbs[{n + nn, k}], ret[z]);\n           }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -170,16 +170,17 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto bTensorTy = b.getType().cast<RankedTensorType>();\n   auto dTensorTy = d.getType().cast<RankedTensorType>();\n \n-  SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n-                              aTensorTy.getShape().end());\n-  auto dShape = dTensorTy.getShape();\n+  auto aShapePerCTA = triton::gpu::getShapePerCTA(aTensorTy);\n+  auto bShapePerCTA = triton::gpu::getShapePerCTA(bTensorTy);\n+  auto dShapePerCTA = triton::gpu::getShapePerCTA(dTensorTy);\n+\n   int bitwidth = aTensorTy.getElementType().getIntOrFloatBitWidth();\n   auto repA =\n       aTensorTy.getEncoding().cast<DotOperandEncodingAttr>().getMMAv2Rep(\n-          aTensorTy.getShape(), bitwidth);\n+          aShapePerCTA, bitwidth);\n   auto repB =\n       bTensorTy.getEncoding().cast<DotOperandEncodingAttr>().getMMAv2Rep(\n-          bTensorTy.getShape(), bitwidth);\n+          bShapePerCTA, bitwidth);\n \n   assert(repA[1] == repB[0]);\n   int repM = repA[0], repN = repB[1], repK = repA[1];"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "added", "additions": 431, "deletions": 0, "changes": 431, "file_content_changes": "@@ -0,0 +1,431 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"DotOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getShapePerCTATile;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+triton::nvgpu::WGMMAEltType getMmaRetType(Value d) {\n+  auto dTy = d.getType().cast<RankedTensorType>().getElementType();\n+  if (dTy.isF32()) {\n+    return triton::nvgpu::WGMMAEltType::f32;\n+  } else if (dTy.isF16()) {\n+    return triton::nvgpu::WGMMAEltType::f16;\n+  } else if (dTy.isInteger(32)) {\n+    return triton::nvgpu::WGMMAEltType::s32;\n+  } else {\n+    llvm::report_fatal_error(\"Unsupported mma result type found\");\n+  }\n+}\n+\n+triton::nvgpu::WGMMAEltType getMmaOperandType(Value a, bool allowTF32) {\n+  auto aTy = a.getType().cast<RankedTensorType>().getElementType();\n+  if (aTy.isF16()) {\n+    return triton::nvgpu::WGMMAEltType::f16;\n+  } else if (aTy.isBF16()) {\n+    return triton::nvgpu::WGMMAEltType::bf16;\n+  } else if (aTy.isF32() && allowTF32) {\n+    return triton::nvgpu::WGMMAEltType::tf32;\n+  } else if (aTy.isInteger(8)) {\n+    return triton::nvgpu::WGMMAEltType::s8;\n+  } else if (aTy.isFloat8E5M2()) {\n+    return triton::nvgpu::WGMMAEltType::e5m2;\n+  } else if (aTy.isFloat8E4M3FN()) {\n+    return triton::nvgpu::WGMMAEltType::e4m3;\n+  } else {\n+    llvm::report_fatal_error(\"Unsupported mma operand type found\");\n+  }\n+}\n+\n+mlir::triton::nvgpu::WGMMADescMode\n+getModeFromLayout(const SharedEncodingAttr &layout, uint32_t widthInByte) {\n+  int perPhase = layout.getPerPhase();\n+  int maxPhase = layout.getMaxPhase();\n+  uint32_t swizzlingByteWidth = 0;\n+\n+  mlir::triton::nvgpu::WGMMADescMode mode;\n+  if (perPhase == 4 && maxPhase == 2) {\n+    mode = mlir::triton::nvgpu::WGMMADescMode::swizzle32;\n+    swizzlingByteWidth = 32;\n+  } else if (perPhase == 2 && maxPhase == 4) {\n+    mode = mlir::triton::nvgpu::WGMMADescMode::swizzle64;\n+    swizzlingByteWidth = 64;\n+  } else if (perPhase == 1 && maxPhase == 8) {\n+    mode = mlir::triton::nvgpu::WGMMADescMode::swizzle128;\n+    swizzlingByteWidth = 128;\n+  } else {\n+    llvm::report_fatal_error(\"Unsupported shared layout.\");\n+  }\n+\n+  // TODO[biaow]: remove it once we support swizzling size larger than matrix\n+  // width, which requires padding the matrix width to the swizzling size when\n+  // allocating shared memory.\n+  assert(swizzlingByteWidth <= widthInByte &&\n+         \"swizzling size larger than matrix width is not supported.\");\n+  return mode;\n+}\n+\n+class DotOpMmaV3SmemLoader {\n+public:\n+  DotOpMmaV3SmemLoader(Value tensor, const SharedMemoryObject &smemObj,\n+                       SmallVector<int64_t> shape, Value warpId,\n+                       unsigned int dimWpt, bool trans,\n+                       SmallVector<unsigned int> instrShape,\n+                       ConversionPatternRewriter &rewriter, Location loc)\n+      : base(smemObj.base), shape(shape), warpId(warpId), dimWpt(dimWpt),\n+        trans(trans), instrShape(instrShape), rewriter(rewriter), loc(loc) {\n+    auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+    auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+    ord = sharedLayout.getOrder();\n+    const int perPhase = sharedLayout.getPerPhase();\n+    const int maxPhase = sharedLayout.getMaxPhase();\n+    elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n+    elemsPerSwizzlingRow = 128 / perPhase / elemBytes;\n+    elemsPerSwizzlingRowVal = i32_val(elemsPerSwizzlingRow);\n+\n+    uint32_t widthInByte = shape[ord[0]] * elemBytes;\n+    mode = getModeFromLayout(sharedLayout, widthInByte);\n+\n+    baseDesc = rewriter.create<triton::nvgpu::WGMMADescCreateOp>(\n+        loc, i64_ty, base, i32_val(shape[ord[1]]), mode);\n+  }\n+\n+  Value smemLoad(int a, int b) {\n+    Value k = i32_val(b * instrShape[1]);\n+    Value m = add(i32_val(a * dimWpt * instrShape[0]),\n+                  mul(warpId, i32_val(instrShape[0])));\n+    if (trans) {\n+      std::swap(k, m);\n+    }\n+    Value leading_offset = mul(udiv(k, elemsPerSwizzlingRowVal),\n+                               i32_val(shape[ord[1]] * elemsPerSwizzlingRow));\n+    Value stride_offset = mul(m, elemsPerSwizzlingRowVal);\n+    Value offset = add(add(leading_offset, stride_offset),\n+                       urem(k, elemsPerSwizzlingRowVal));\n+    Value off1 = mul(i32_val(elemBytes), offset);\n+    Value off_ = zext(i64_ty, udiv(off1, i32_val(16)));\n+\n+    return add(baseDesc, off_);\n+  }\n+\n+private:\n+  Value base;\n+  SmallVector<int64_t> shape;\n+  Value warpId;\n+  int dimWpt;\n+  bool trans;\n+  Value elemsPerSwizzlingRowVal;\n+  mlir::triton::nvgpu::WGMMADescMode mode;\n+  SmallVector<unsigned int> instrShape;\n+  ArrayRef<unsigned> ord;\n+  ConversionPatternRewriter &rewriter;\n+  Location loc;\n+  int elemsPerSwizzlingRow;\n+  int elemBytes;\n+  Value baseDesc;\n+};\n+\n+DotOpMmaV3SmemLoader loadA(TritonGPUToLLVMTypeConverter *typeConverter,\n+                           ConversionPatternRewriter &rewriter, Location loc,\n+                           const MmaEncodingAttr &mmaEncoding, Value tensor,\n+                           const SharedMemoryObject &smemObj, Value thread) {\n+  auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto aSharedLayout = aTensorTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n+  assert(aSharedLayout && \"only support load dot operand from shared.\");\n+  auto instrShape = mmaEncoding.getInstrShape();\n+  auto wpt = mmaEncoding.getWarpsPerCTA();\n+  auto aOrd = aSharedLayout.getOrder();\n+  bool transA = aOrd[0] == 0;\n+  auto shapePerCTA = getShapePerCTA(aTensorTy);\n+\n+  int numRepM = ceil<unsigned>(shapePerCTA[0], instrShape[0] * wpt[0]);\n+  int numRepK = ceil<unsigned>(shapePerCTA[1], instrShape[2]);\n+\n+  Value warp = udiv(thread, i32_val(32));\n+  Value warpM = urem(warp, i32_val(wpt[0]));\n+  Value warpId = urem(warpM, i32_val(shapePerCTA[0] / instrShape[0]));\n+\n+  return {tensor,\n+          smemObj,\n+          shapePerCTA,\n+          warpId,\n+          wpt[0],\n+          transA,\n+          {instrShape[0], instrShape[2]},\n+          rewriter,\n+          loc};\n+}\n+\n+DotOpMmaV3SmemLoader loadB(TritonGPUToLLVMTypeConverter *typeConverter,\n+                           ConversionPatternRewriter &rewriter, Location loc,\n+                           MmaEncodingAttr &mmaEncoding, Value tensor,\n+                           const SharedMemoryObject &smemObj, Value thread) {\n+  auto bTensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto bSharedLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  assert(bSharedLayout && \"only support load B from shared.\");\n+  auto instrShape = mmaEncoding.getInstrShape();\n+  auto wpt = mmaEncoding.getWarpsPerCTA();\n+  auto bOrd = bSharedLayout.getOrder();\n+  bool transB = bOrd[0] == 1;\n+  auto shapePerCTA = triton::gpu::getShapePerCTA(bTensorTy);\n+\n+  int numRepK = ceil<unsigned>(shapePerCTA[0], instrShape[2]);\n+  int numRepN = ceil<unsigned>(shapePerCTA[1], instrShape[1] * wpt[1]);\n+\n+  Value warp = udiv(thread, i32_val(32));\n+  Value warpMN = udiv(warp, i32_val(wpt[0]));\n+  Value warpN = urem(warpMN, i32_val(wpt[1]));\n+  Value warpId = urem(warpN, i32_val(shapePerCTA[1] / instrShape[1]));\n+\n+  return {tensor,\n+          smemObj,\n+          shapePerCTA,\n+          warpId,\n+          wpt[1],\n+          transB,\n+          {instrShape[1], instrShape[2]},\n+          rewriter,\n+          loc};\n+}\n+\n+// Return a vector of Value of the accumulator start at startIndex and pack the\n+// values into 32bits in case the accumulator is fp16.\n+llvm::SmallVector<Value> loadC(ConversionPatternRewriter &rewriter,\n+                               Location loc, const SmallVector<Value> &elements,\n+                               int startIndex, int numElements) {\n+  if (!elements[0].getType().isF16()) {\n+    llvm::SmallVector<Value> mmaOut(numElements);\n+    for (int i = 0; i < numElements; ++i)\n+      mmaOut[i] = elements[startIndex + i];\n+    return mmaOut;\n+  }\n+  // For FP16 we need to pack accumulator into 32-bit integers.\n+  llvm::SmallVector<Value> mmaOut(numElements / 2);\n+  for (int i = 0; i < numElements / 2; ++i) {\n+    Value a0 = elements[startIndex + 2 * i];\n+    Value a1 = elements[startIndex + 2 * i + 1];\n+    Type cPackTy = vec_ty(rewriter.getF16Type(), 2);\n+    Value pack = rewriter.create<LLVM::UndefOp>(loc, cPackTy);\n+    pack = insert_element(cPackTy, pack, a0, i32_val(0));\n+    pack = insert_element(cPackTy, pack, a1, i32_val(1));\n+    pack = bitcast(pack, rewriter.getIntegerType(32));\n+    mmaOut[i] = pack;\n+  }\n+  return mmaOut;\n+}\n+\n+// If the accumulator is fp16 unpack it from 32-bit integers.\n+SmallVector<Value> unpackAccumulator(ConversionPatternRewriter &rewriter,\n+                                     Location loc,\n+                                     const SmallVector<Value> &packed,\n+                                     RankedTensorType tensorTy) {\n+  if (!tensorTy.getElementType().isF16())\n+    return packed;\n+  // For fp16 the accumualtor is pack into 32-bit integers so we need to unpack\n+  // it.\n+  SmallVector<Value> results;\n+  for (Value elem : packed) {\n+    elem = bitcast(elem, vec_ty(rewriter.getF16Type(), 2));\n+    results.push_back(extract_element(rewriter.getF16Type(), elem, i32_val(0)));\n+    results.push_back(extract_element(rewriter.getF16Type(), elem, i32_val(1)));\n+  }\n+  return results;\n+}\n+\n+LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n+                         ConversionPatternRewriter &rewriter, Location loc,\n+                         Operation *op, Value a, Value b, Value c, Value d,\n+                         Value loadedA, Value loadedB, Value loadedC,\n+                         bool allowTF32, const SharedMemoryObject &smemObjA,\n+                         const SharedMemoryObject &smemObjB, bool sync,\n+                         Value thread) {\n+  auto aTensorTy = a.getType().cast<RankedTensorType>();\n+  auto bTensorTy = b.getType().cast<RankedTensorType>();\n+  auto dTensorTy = d.getType().cast<RankedTensorType>();\n+  auto aSharedLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bSharedLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto mmaEncoding = dTensorTy.getEncoding().cast<MmaEncodingAttr>();\n+  auto aOrd = aSharedLayout.getOrder();\n+  auto bOrd = bSharedLayout.getOrder();\n+  bool transA = aOrd[0] == 0;\n+  bool transB = bOrd[0] == 1;\n+  auto dShapePerCTA = getShapePerCTA(dTensorTy);\n+  auto instrShape = mmaEncoding.getInstrShape();\n+  auto accSize = 2 * (instrShape[1] / 4);\n+  int M = 4 * instrShape[0];\n+  int N = instrShape[1];\n+  int K = instrShape[2];\n+\n+  auto shapePerCTATile = getShapePerCTATile(mmaEncoding);\n+  int numRepM = ceil<unsigned>(dShapePerCTA[0], shapePerCTATile[0]);\n+  int numRepN = ceil<unsigned>(dShapePerCTA[1], shapePerCTATile[1]);\n+  int numRepK = ceil<unsigned>(aTensorTy.getShape()[1], instrShape[2]);\n+\n+  DotOpMmaV3SmemLoader aLoader =\n+      loadA(typeConverter, rewriter, loc, mmaEncoding, a, smemObjA, thread);\n+  DotOpMmaV3SmemLoader bLoader =\n+      loadB(typeConverter, rewriter, loc, mmaEncoding, b, smemObjB, thread);\n+\n+  auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n+\n+  triton::nvgpu::WGMMAEltType eltTypeC = getMmaRetType(d);\n+  triton::nvgpu::WGMMAEltType eltTypeA = getMmaOperandType(a, allowTF32);\n+  triton::nvgpu::WGMMAEltType eltTypeB = eltTypeA;\n+\n+  triton::nvgpu::WGMMALayout layoutA = transA ? triton::nvgpu::WGMMALayout::col\n+                                              : triton::nvgpu::WGMMALayout::row;\n+  triton::nvgpu::WGMMALayout layoutB = transB ? triton::nvgpu::WGMMALayout::row\n+                                              : triton::nvgpu::WGMMALayout::col;\n+\n+  auto func = op->getParentOfType<LLVM::LLVMFuncOp>();\n+  int numTMADescs =\n+      func->getAttr(kAttrNumTMALoadDescsName).cast<IntegerAttr>().getInt();\n+  if (numTMADescs == 0)\n+    rewriter.create<triton::nvgpu::FenceAsyncSharedOp>(loc, 0);\n+  rewriter.create<triton::nvgpu::WGMMAFenceOp>(loc);\n+\n+  SmallVector<Value> mmaResults;\n+  for (int m = 0; m < numRepM; ++m) {\n+    for (int n = 0; n < numRepN; ++n) {\n+      llvm::SmallVector<Value> mmaOut =\n+          loadC(rewriter, loc, fc, (m * numRepN + n) * accSize, accSize);\n+      llvm::SmallVector<Type> elemTypes;\n+      for (Value accEl : mmaOut)\n+        elemTypes.push_back(accEl.getType());\n+      auto accTy =\n+          LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n+      Value d = typeConverter->packLLElements(loc, mmaOut, rewriter, accTy);\n+      for (int k = 0; k < numRepK; ++k) {\n+        auto a = aLoader.smemLoad(m, k);\n+        auto b = bLoader.smemLoad(n, k);\n+        ValueRange operands{a, b, d};\n+        d = rewriter.create<triton::nvgpu::WGMMAOp>(loc, accTy, a, b, d, M, N,\n+                                                    K, eltTypeC, eltTypeA,\n+                                                    eltTypeB, layoutA, layoutB);\n+      }\n+      auto acc = typeConverter->unpackLLElements(loc, d, rewriter, accTy);\n+      for (int i = 0; i < acc.size(); ++i) {\n+        mmaResults.push_back(acc[i]);\n+      }\n+    }\n+  }\n+  rewriter.create<triton::nvgpu::WGMMACommitGroupOp>(loc);\n+\n+  if (sync)\n+    rewriter.create<triton::nvgpu::WGMMAWaitOp>(loc, 0);\n+\n+  SmallVector<Value> results =\n+      unpackAccumulator(rewriter, loc, mmaResults, dTensorTy);\n+\n+  // replace with new packed result\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      mmaEncoding.getContext(),\n+      SmallVector<Type>(results.size(), dTensorTy.getElementType()));\n+  auto res = typeConverter->packLLElements(loc, results, rewriter, structTy);\n+  rewriter.replaceOp(op, res);\n+  return success();\n+}\n+\n+// Loading $c to registers, returns a Value.\n+Value loadC(Value tensor, Value llTensor) {\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto mmaEncoding = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n+  assert(mmaEncoding && \"Currently, we only support $c with a mma layout.\");\n+  auto instrShape = mmaEncoding.getInstrShape();\n+  auto wpt = mmaEncoding.getWarpsPerCTA();\n+  auto shapePerCTA = getShapePerCTA(tensorTy);\n+  auto shapePerCTATile = getShapePerCTATile(mmaEncoding);\n+\n+  int numRepM = ceil<unsigned>(shapePerCTA[0], shapePerCTATile[0]);\n+  int numRepN = ceil<unsigned>(shapePerCTA[1], shapePerCTATile[1]);\n+\n+  size_t fcSize = 2 * (instrShape[1] / 4) * numRepM * numRepN;\n+\n+  auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n+  assert(structTy.getBody().size() == fcSize &&\n+         \"DotOp's $c operand should pass the same number of values as $d in \"\n+         \"mma layout.\");\n+  return llTensor;\n+}\n+\n+LogicalResult convertWGMMA(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                           TritonGPUToLLVMTypeConverter *typeConverter,\n+                           ConversionPatternRewriter &rewriter, Value thread) {\n+  auto loc = op.getLoc();\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  Value C = op.getC();\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+\n+  assert(ATensorTy.getEncoding().isa<SharedEncodingAttr>() &&\n+         BTensorTy.getEncoding().isa<SharedEncodingAttr>() &&\n+         \"Both $a and %b should be Shared layout.\");\n+\n+  Value llA, llB, llC;\n+  llA = adaptor.getA();\n+  llB = adaptor.getB();\n+  llC = loadC(C, adaptor.getC());\n+\n+  auto smemObjA = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n+  auto smemObjB = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n+  return convertDot(typeConverter, rewriter, loc, op.getOperation(), A, B, C,\n+                    op.getD(), llA, llB, llC, op.getAllowTF32(), smemObjA,\n+                    smemObjB, true, thread);\n+}\n+\n+LogicalResult convertAsyncWGMMA(triton::nvidia_gpu::DotAsyncOp op,\n+                                triton::nvidia_gpu::DotAsyncOp::Adaptor adaptor,\n+                                TritonGPUToLLVMTypeConverter *typeConverter,\n+                                ConversionPatternRewriter &rewriter,\n+                                Value thread) {\n+  auto loc = op.getLoc();\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  Value C = op.getC();\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+\n+  assert(ATensorTy.getEncoding().isa<SharedEncodingAttr>() &&\n+         BTensorTy.getEncoding().isa<SharedEncodingAttr>() &&\n+         \"Both $a and %b should be Shared layout.\");\n+\n+  Value llA, llB, llC;\n+  llA = adaptor.getA();\n+  llB = adaptor.getB();\n+  llC = loadC(C, adaptor.getC());\n+\n+  auto smemObjA = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n+  auto smemObjB = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n+  return convertDot(typeConverter, rewriter, loc, op.getOperation(), A, B, C,\n+                    op.getD(), llA, llB, llC, op.getAllowTF32(), smemObjA,\n+                    smemObjB, false, thread);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 174, "deletions": 1, "changes": 175, "file_content_changes": "@@ -106,6 +106,13 @@ const std::string Fp8E4M3B15_to_Fp16 =\n const std::string Fp16_to_Fp8E4M3B15 =\n     \"{                                      \\n\"\n     \".reg .b32 a<2>, b<2>;                  \\n\"\n+    \".reg .b32 min_val, max_val;            \\n\"\n+    \"mov.b32 min_val, 0xBF80BF80;           \\n\"\n+    \"mov.b32 max_val, 0x3F803F80;           \\n\"\n+    \"max.f16x2 $1, $1, min_val;             \\n\"\n+    \"min.f16x2 $1, $1, max_val;             \\n\"\n+    \"max.f16x2 $2, $2, min_val;             \\n\"\n+    \"min.f16x2 $2, $2, max_val;             \\n\"\n     \"shl.b32 a0, $1, 1;                     \\n\"\n     \"shl.b32 a1, $2, 1;                     \\n\"\n     \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n@@ -346,6 +353,13 @@ static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n   llvm_unreachable(\"unimplemented code path\");\n }\n \n+inline Type getElementType(Value value) {\n+  auto type = value.getType();\n+  if (auto tensorType = type.dyn_cast<RankedTensorType>())\n+    return tensorType.getElementType();\n+  return type;\n+}\n+\n inline SmallVector<Value> unpackI32(const SmallVector<Value> &inValues,\n                                     Type srcTy,\n                                     ConversionPatternRewriter &rewriter,\n@@ -1142,7 +1156,8 @@ struct IndexCastOpLowering\n \n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    PatternBenefit benefit) {\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    ModuleAllocation &allocation, PatternBenefit benefit) {\n #define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp)\n@@ -1215,3 +1230,161 @@ void populateElementwiseOpToLLVMPatterns(\n   // __nv_expf for higher-precision calculation\n   patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n }\n+\n+struct FPExtOpConversion\n+    : ElementwiseOpConversionBase<LLVM::FPExtOp, FPExtOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::FPExtOp, FPExtOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::FPExtOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isF32() && srcTy.isF16()) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  SmallVector<Value> createDestOps(LLVM::FPExtOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    return {\n+        FpToFpOpConversion::convertFp16ToFp32(loc, rewriter, operands[0][0])};\n+  }\n+};\n+\n+struct FPTruncOpConversion\n+    : ElementwiseOpConversionBase<LLVM::FPTruncOp, FPTruncOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<LLVM::FPTruncOp, FPTruncOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::FPTruncOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isF16() && srcTy.isF32()) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  SmallVector<Value> createDestOps(LLVM::FPTruncOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    return {\n+        FpToFpOpConversion::convertFp32ToFp16(loc, rewriter, operands[0][0])};\n+  }\n+};\n+\n+struct TruncOpConversion\n+    : ElementwiseOpConversionBase<LLVM::TruncOp, TruncOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::TruncOp, TruncOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::TruncOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isInteger(16) && srcTy.isInteger(32)) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  SmallVector<Value> createDestOps(LLVM::TruncOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.u16.u32\");\n+    auto res = builder.newOperand(\"=h\");\n+    auto operand = builder.newOperand(operands[0][0], \"r\");\n+    cvt(res, operand);\n+    return {builder.launch(rewriter, loc, i16_ty, false)};\n+  }\n+};\n+\n+struct SExtOpConversion\n+    : ElementwiseOpConversionBase<LLVM::SExtOp, SExtOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::SExtOp, SExtOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::SExtOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isInteger(32) && srcTy.isInteger(16)) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  SmallVector<Value> createDestOps(LLVM::SExtOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.s32.s16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(operands[0][0], \"h\");\n+    cvt(res, operand);\n+    return {builder.launch(rewriter, loc, i32_ty, false)};\n+  }\n+};\n+\n+struct ZExtOpConversion\n+    : ElementwiseOpConversionBase<LLVM::ZExtOp, ZExtOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::ZExtOp, ZExtOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::ZExtOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isInteger(32) && srcTy.isInteger(16)) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  SmallVector<Value> createDestOps(LLVM::ZExtOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.u32.u16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(operands[0][0], \"h\");\n+    cvt(res, operand);\n+    return {builder.launch(rewriter, loc, i32_ty, false)};\n+  }\n+};\n+\n+bool isLegalElementwiseOp(Operation *op) {\n+  if (isa<LLVM::FPExtOp>(op)) {\n+    return FPExtOpConversion::isLegalOp(cast<LLVM::FPExtOp>(op));\n+  } else if (isa<LLVM::FPTruncOp>(op)) {\n+    return FPTruncOpConversion::isLegalOp(cast<LLVM::FPTruncOp>(op));\n+  } else if (isa<LLVM::TruncOp>(op)) {\n+    return TruncOpConversion::isLegalOp(cast<LLVM::TruncOp>(op));\n+  } else if (isa<LLVM::SExtOp>(op)) {\n+    return SExtOpConversion::isLegalOp(cast<LLVM::SExtOp>(op));\n+  } else if (isa<LLVM::ZExtOp>(op)) {\n+    return ZExtOpConversion::isLegalOp(cast<LLVM::ZExtOp>(op));\n+  }\n+  return true;\n+}\n+\n+void populateElementwiseOpToPTXPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    PatternBenefit benefit) {\n+  patterns.add<FPExtOpConversion>(typeConverter, benefit);\n+  patterns.add<FPTruncOpConversion>(typeConverter, benefit);\n+  patterns.add<TruncOpConversion>(typeConverter, benefit);\n+  patterns.add<SExtOpConversion>(typeConverter, benefit);\n+  patterns.add<ZExtOpConversion>(typeConverter, benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -8,8 +8,13 @@ using namespace mlir::triton;\n \n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    PatternBenefit benefit);\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    ModuleAllocation &allocation, PatternBenefit benefit);\n \n bool isLegalElementwiseOp(Operation *op);\n \n+void populateElementwiseOpToPTXPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    PatternBenefit benefit);\n+\n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 634, "deletions": 2, "changes": 636, "file_content_changes": "@@ -4,10 +4,15 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"LoadStoreOpToLLVM.h\"\n \n+#include <numeric>\n+\n using namespace mlir;\n using namespace mlir::triton;\n \n+using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::linearize;\n+using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n@@ -64,6 +69,9 @@ struct LoadOpConversion\n     Value other = op.getOther();\n \n     // adaptor values\n+    assert(!isTensorPointerType(ptr.getType()) &&\n+           \"Cannot convert load with a tensor pointer into LLVM; \"\n+           \"this case should be transformed to normal load before lowering\");\n     Value llPtr = adaptor.getPtr();\n     Value llMask = adaptor.getMask();\n     Value llOther = adaptor.getOther();\n@@ -378,6 +386,251 @@ struct StoreOpConversion\n     return success();\n   }\n };\n+// TODO: refactor to save common logic with insertsliceasyncv2\n+struct StoreAsyncOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::nvidia_gpu::StoreAsyncOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::StoreAsyncOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  StoreAsyncOpConversion(TritonGPUToLLVMTypeConverter &converter,\n+                         ModuleAllocation &allocation,\n+                         mlir::triton::gpu::TMAMetadataTy *tmaMetadata,\n+                         const TensorPtrMapT *tensorPtrMap,\n+                         PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::nvidia_gpu::StoreAsyncOp>(\n+            converter, allocation, tmaMetadata, benefit),\n+        tensorPtrMap(tensorPtrMap) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::StoreAsyncOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    auto dst = op.getDst();\n+    auto src = op.getSrc();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto elemTy = srcTy.getElementType();\n+\n+    auto rank = srcTy.getRank();\n+    assert(rank > 0 && rank <= 5);\n+\n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for StoreAsyncOp\");\n+\n+    auto llFuncOp = op->getParentOfType<LLVM::LLVMFuncOp>();\n+    assert(llFuncOp && \"LLVMFuncOp not found for StoreAsyncOp\");\n+\n+    int numTMADescs = getNumTMADescs(llFuncOp);\n+    assert(numTMADescs > 0);\n+\n+    auto sharedLayout = srcTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n+    assert(sharedLayout && \"expected shared encoding\");\n+\n+    mlir::triton::gpu::TMAInfo tmaInfo;\n+\n+    tmaInfo.tensorDataType = getCUtensorMapDataType(elemTy);\n+    tmaInfo.tensorRank = rank;\n+    assert(tmaMetadata);\n+\n+    auto inOrder = sharedLayout.getOrder();\n+    unsigned TMADescIdx = tmaMetadata->size();\n+    unsigned numFuncArgs = llFuncOp.getBody().front().getNumArguments();\n+    auto makeTensorPtr = tensorPtrMap->lookup(op.getOperation());\n+    auto dstOrder = makeTensorPtr.getOrder();\n+\n+    unsigned globalAddressArgIdx = getArgIdx(makeTensorPtr.getBase());\n+    tmaInfo.globalAddressArgIdx = globalAddressArgIdx;\n+    tmaInfo.TMADescArgIdx = numFuncArgs - numTMADescs + TMADescIdx;\n+\n+    auto getDimOfOrder = [](ArrayRef<int32_t> order, int32_t i) {\n+      auto it = std::find(order.begin(), order.end(), i);\n+      assert(it != order.end());\n+      return std::distance(order.begin(), it);\n+    };\n+\n+    std::vector<int32_t> globalDimsArgIdx;\n+    std::vector<int32_t> globalStridesArgIdx;\n+    // constant values are mapped to (-1 - value)\n+    for (int i = 0; i < rank; ++i) {\n+      int32_t argIdx = -1;\n+      auto dim = getDimOfOrder(dstOrder, i);\n+      argIdx = getArgIdx(makeTensorPtr.getShape()[dim]);\n+      globalDimsArgIdx.emplace_back(argIdx);\n+      // handle constant stride\n+      argIdx = getArgIdx(makeTensorPtr.getStrides()[dim]);\n+      globalStridesArgIdx.emplace_back(argIdx);\n+    }\n+\n+    tmaInfo.globalDimsArgIdx = globalDimsArgIdx;\n+    tmaInfo.globalStridesArgIdx = globalStridesArgIdx;\n+    std::vector<uint32_t> boxDims;\n+    auto CTAsPerCGA = sharedLayout.getCTALayout().getCTAsPerCGA();\n+    auto CTAOrder = sharedLayout.getCTALayout().getCTAOrder();\n+    auto CTASplitNum = sharedLayout.getCTALayout().getCTASplitNum();\n+    auto tensorShape = makeTensorPtr.getResult()\n+                           .getType()\n+                           .cast<triton::PointerType>()\n+                           .getPointeeType()\n+                           .cast<RankedTensorType>()\n+                           .getShape();\n+    auto shapePerCTA = getShapePerCTA(CTASplitNum, tensorShape);\n+    // magic 128 bytes\n+    uint32_t bytesPerElem = elemTy.getIntOrFloatBitWidth() / 8;\n+    uint32_t numBox{1};\n+    for (int i = 0; i < rank; ++i) {\n+      auto dim = getDimOfOrder(dstOrder, i);\n+      auto tNumElems = shapePerCTA[dim];\n+      if (i == 0 && tNumElems * bytesPerElem > 128) {\n+        tNumElems = 128 / bytesPerElem;\n+        numBox = (shapePerCTA[dim] + tNumElems - 1) / tNumElems;\n+      }\n+      boxDims.emplace_back(tNumElems);\n+    }\n+    std::vector<uint32_t> elementStrides(rank, 1);\n+    tmaInfo.boxDims = boxDims;\n+    tmaInfo.elementStrides = elementStrides;\n+\n+    CUtensorMapSwizzle swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE;\n+    assert(\n+        ((elemTy.getIntOrFloatBitWidth() == 16 && sharedLayout.getVec() == 8) or\n+         (elemTy.getIntOrFloatBitWidth() == 32 &&\n+          sharedLayout.getVec() == 4)) &&\n+        \"Unexpected shared layout for StoreAsyncOp\");\n+    if (sharedLayout.getPerPhase() == 4 && sharedLayout.getMaxPhase() == 2)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_32B;\n+    else if (sharedLayout.getPerPhase() == 2 && sharedLayout.getMaxPhase() == 4)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_64B;\n+    else if (sharedLayout.getPerPhase() == 1 && sharedLayout.getMaxPhase() == 8)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B;\n+    else\n+      llvm::report_fatal_error(\"Unsupported shared layout for StoreAsyncOp\");\n+    tmaInfo.swizzle = swizzle;\n+    tmaInfo.interleave = CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE;\n+    tmaInfo.l2Promotion =\n+        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_L2_128B;\n+    tmaInfo.oobFill =\n+        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE;\n+\n+    tmaMetadata->emplace_back(tmaInfo);\n+\n+    Value llDst = adaptor.getDst();\n+    Value llSrc = adaptor.getSrc();\n+    auto srcShape = srcTy.getShape();\n+    auto smemObj = getSharedMemoryObjectFromStruct(loc, llSrc, rewriter);\n+\n+    SmallVector<Value> offsetVals;\n+    for (auto i = 0; i < srcShape.size(); ++i) {\n+      offsetVals.emplace_back(i32_val(0));\n+    }\n+\n+    Value tmaDesc =\n+        llFuncOp.getBody().front().getArgument(tmaInfo.TMADescArgIdx);\n+    auto ptrI8SharedTy = LLVM::LLVMPointerType::get(\n+        typeConverter->convertType(rewriter.getI8Type()), 3);\n+\n+    auto threadId = getThreadId(rewriter, loc);\n+    Value pred = icmp_eq(threadId, i32_val(0));\n+\n+    auto llCoord = getTypeConverter()->unpackLLElements(loc, llDst, rewriter,\n+                                                        dst.getType());\n+    uint32_t boxStride = std::accumulate(boxDims.begin(), boxDims.end(), 1,\n+                                         std::multiplies<uint32_t>());\n+\n+    Value clusterCTAId = getClusterCTAId(rewriter, loc);\n+    SmallVector<Value> multiDimClusterCTAId =\n+        delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n+\n+    rewriter.create<triton::nvgpu::FenceAsyncSharedOp>(loc, 0);\n+\n+    for (uint32_t b = 0; b < numBox; ++b) {\n+      SmallVector<Value> coord;\n+      // raw coord\n+      for (int i = 0; i < rank; ++i) {\n+        auto dim = getDimOfOrder(dstOrder, i);\n+        coord.push_back(llCoord[dim]);\n+      }\n+      // coord with box and cta offset\n+      for (int i = 0; i < rank; ++i) {\n+        auto dim = getDimOfOrder(dstOrder, i);\n+        if (i == 0) {\n+          coord[i] = add(coord[i], i32_val(b * boxDims[i]));\n+          auto CTAOffset =\n+              mul(multiDimClusterCTAId[dim], i32_val(numBox * boxDims[i]));\n+          coord[i] = add(coord[i], CTAOffset);\n+        } else {\n+          coord[i] = add(coord[i],\n+                         mul(multiDimClusterCTAId[dim], i32_val(boxDims[i])));\n+        }\n+      }\n+      Value srcOffset = i32_val(b * boxStride);\n+      auto srcPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n+      Value srcPtrBase = gep(srcPtrTy, smemObj.base, srcOffset);\n+      auto addr = bitcast(srcPtrBase, ptrI8SharedTy);\n+      rewriter.create<triton::nvgpu::TMAStoreTiledOp>(loc, tmaDesc, addr, pred,\n+                                                      coord);\n+    }\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+\n+private:\n+  CUtensorMapDataType getCUtensorMapDataType(Type ty) const {\n+    if (ty.isF16()) {\n+      return CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_FLOAT16;\n+    } else if (ty.isF32()) {\n+      return CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_FLOAT32;\n+    } else {\n+      llvm::report_fatal_error(\"Unsupported elemTy for StoreAsyncOp\");\n+      return CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_FLOAT16;\n+    }\n+  }\n+\n+  unsigned getArgIdx(Value v) const {\n+    if (auto op = v.getDefiningOp<mlir::arith::ConstantOp>()) {\n+      return -1 -\n+             op.getValue().dyn_cast<IntegerAttr>().getValue().getZExtValue();\n+    }\n+    if (v.getDefiningOp() &&\n+        isa<mlir::UnrealizedConversionCastOp>(v.getDefiningOp())) {\n+      return getArgIdx(v.getDefiningOp()->getOperand(0));\n+    } else if (v.getParentBlock()->isEntryBlock() && v.isa<BlockArgument>()) {\n+      // in entryblock and is BlockArgument; Because argument of func are\n+      // arugments of entryblock bb0 in MLIR\n+      return v.cast<BlockArgument>().getArgNumber();\n+    } else if (v.getParentBlock()->isEntryBlock() &&\n+               (!v.isa<BlockArgument>())) {\n+      // in entryblock but not BlockArgument\n+      return getArgIdx(v.getDefiningOp()->getOperand(0));\n+    } else if (!v.getParentBlock()->isEntryBlock()) {\n+      // in non-entryblock\n+      return getArgIdx(v.getDefiningOp()->getOperand(0));\n+    } else {\n+      llvm::report_fatal_error(\n+          \"Operand of `MakeTensorPtrOp` is not the function's argument\");\n+      return 0;\n+    }\n+  }\n+\n+  int getNumTMADescs(LLVM::LLVMFuncOp func) const {\n+    if (!func->hasAttr(kAttrNumTMALoadDescsName)) {\n+      llvm::report_fatal_error(\"TritonGPU module should contain a \"\n+                               \"triton_gpu.num-tma-load attribute\");\n+      return -1;\n+    }\n+    if (!func->hasAttr(kAttrNumTMAStoreDescsName)) {\n+      llvm::report_fatal_error(\"TritonGPU module should contain a \"\n+                               \"triton_gpu.num-tma-store attribute\");\n+      return -1;\n+    }\n+    return func->getAttr(kAttrNumTMAStoreDescsName)\n+               .cast<IntegerAttr>()\n+               .getInt() +\n+           func->getAttr(kAttrNumTMALoadDescsName).cast<IntegerAttr>().getInt();\n+  }\n+\n+  const TensorPtrMapT *tensorPtrMap;\n+};\n \n struct AtomicCASOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>,\n@@ -854,11 +1107,386 @@ struct InsertSliceAsyncOpConversion\n   }\n };\n \n+struct InsertSliceAsyncV2OpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<\n+          triton::nvidia_gpu::InsertSliceAsyncV2Op> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::InsertSliceAsyncV2Op>::\n+      ConvertTritonGPUOpToLLVMPattern;\n+\n+  InsertSliceAsyncV2OpConversion(TritonGPUToLLVMTypeConverter &converter,\n+\n+                                 ModuleAllocation &allocation,\n+                                 mlir::triton::gpu::TMAMetadataTy *tmaMetadata,\n+                                 const TensorPtrMapT *tensorPtrMap,\n+                                 PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<\n+            triton::nvidia_gpu::InsertSliceAsyncV2Op>(converter, allocation,\n+                                                      tmaMetadata, benefit),\n+        tensorPtrMap(tensorPtrMap) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::InsertSliceAsyncV2Op op,\n+                  OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+\n+    Location loc = op->getLoc();\n+    auto resultTy = op.getResult().getType().cast<RankedTensorType>();\n+    auto elemTy = resultTy.getElementType();\n+    auto rank = resultTy.getRank() - 1;\n+\n+    // TODO: support any valid rank in (3, 4, 5)\n+    assert(rank > 0 && rank <= 5);\n+    SmallVector<unsigned> shape;\n+    auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for InsertSliceAsyncV2Op\");\n+    auto llFuncOp = op->getParentOfType<LLVM::LLVMFuncOp>();\n+    assert(llFuncOp && \"LLVMFuncOp not found for InsertSliceAsyncV2Op\");\n+    int numTMADescs = getNumTMADescs(llFuncOp);\n+    assert(numTMADescs > 0);\n+    auto sharedLayout = resultTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n+    assert(sharedLayout && \"unexpected layout of InsertSliceAsyncV2Op\");\n+    auto CTAsPerCGA = sharedLayout.getCTALayout().getCTAsPerCGA();\n+    auto CTAOrder = sharedLayout.getCTALayout().getCTAOrder();\n+    auto CTASplitNum = sharedLayout.getCTALayout().getCTASplitNum();\n+\n+    mlir::triton::gpu::TMAInfo tmaInfo;\n+\n+    tmaInfo.tensorDataType = getCUtensorMapDataType(elemTy);\n+    tmaInfo.tensorRank = rank;\n+\n+    assert(tmaMetadata);\n+    unsigned TMADescIdx = tmaMetadata->size();\n+    unsigned numFuncArgs = llFuncOp.getBody().front().getNumArguments();\n+    auto makeTensorPtr = tensorPtrMap->lookup(op.getOperation());\n+    auto inOrder = makeTensorPtr.getOrder();\n+    unsigned globalAddressArgIdx = getArgIdx(makeTensorPtr.getBase());\n+    tmaInfo.globalAddressArgIdx = globalAddressArgIdx;\n+    tmaInfo.TMADescArgIdx = numFuncArgs - numTMADescs + TMADescIdx;\n+\n+    auto getDimOfOrder = [](ArrayRef<int32_t> order, int32_t i) {\n+      auto it = std::find(order.begin(), order.end(), i);\n+      assert(it != order.end());\n+      return std::distance(order.begin(), it);\n+    };\n+\n+    std::vector<int32_t> globalDimsArgIdx;\n+    std::vector<int32_t> globalStridesArgIdx;\n+    // constant values are mapped to (-1 - value)\n+    for (int i = 0; i < rank; ++i) {\n+      int32_t argIdx = -1;\n+      auto dim = getDimOfOrder(inOrder, i);\n+      argIdx = getArgIdx(makeTensorPtr.getShape()[dim]);\n+      globalDimsArgIdx.emplace_back(argIdx);\n+      // handle constant stride\n+      argIdx = getArgIdx(makeTensorPtr.getStrides()[dim]);\n+      globalStridesArgIdx.emplace_back(argIdx);\n+    }\n+\n+    tmaInfo.globalDimsArgIdx = globalDimsArgIdx;\n+    tmaInfo.globalStridesArgIdx = globalStridesArgIdx;\n+\n+    std::vector<uint32_t> boxDims;\n+    auto tensorShape = makeTensorPtr.getResult()\n+                           .getType()\n+                           .cast<triton::PointerType>()\n+                           .getPointeeType()\n+                           .cast<RankedTensorType>()\n+                           .getShape();\n+\n+    SmallVector<unsigned> numMcast(rank);\n+    unsigned accNumMcast = 1;\n+    for (unsigned i = 0; i < rank; ++i) {\n+      numMcast[i] = CTAsPerCGA[i] / CTASplitNum[i];\n+      accNumMcast *= numMcast[i];\n+    }\n+    auto shapePerCTA = getShapePerCTA(CTASplitNum, tensorShape);\n+    for (size_t i = 0; i < rank; ++i) {\n+      auto dim = getDimOfOrder(inOrder, i);\n+      // in case of TMA multicast, we should always slice along higher order\n+      // dimensions\n+      if (i == rank - 1) {\n+        assert(shapePerCTA[dim] >= accNumMcast &&\n+               \"cases when the size of the highest order is smaller \"\n+               \"than numMcasts is not implemented\");\n+        boxDims.emplace_back(shapePerCTA[dim] / accNumMcast);\n+      } else {\n+        boxDims.emplace_back(shapePerCTA[dim]);\n+      }\n+    }\n+\n+    std::vector<uint32_t> elementStrides(rank, 1);\n+    tmaInfo.elementStrides = elementStrides;\n+\n+    CUtensorMapSwizzle swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE;\n+    if (sharedLayout.getPerPhase() == 4 && sharedLayout.getMaxPhase() == 2)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_32B;\n+    else if (sharedLayout.getPerPhase() == 2 && sharedLayout.getMaxPhase() == 4)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_64B;\n+    else if (sharedLayout.getPerPhase() == 1 && sharedLayout.getMaxPhase() == 8)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B;\n+    else\n+      llvm::report_fatal_error(\n+          \"Unsupported shared layout for InsertSliceAsyncV2Op\");\n+\n+    tmaInfo.swizzle = swizzle;\n+    tmaInfo.interleave = CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE;\n+    tmaInfo.l2Promotion =\n+        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_L2_128B;\n+    tmaInfo.oobFill =\n+        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE;\n+\n+    uint32_t numBoxes = 1;\n+    uint32_t elemSizeOfBytes = elemTy.getIntOrFloatBitWidth() / 8;\n+    if (swizzle == CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B) {\n+      while (elemSizeOfBytes * boxDims[0] > 128) {\n+        boxDims[0] = boxDims[0] / 2;\n+        numBoxes *= 2;\n+      }\n+    }\n+    tmaInfo.boxDims = boxDims;\n+    tmaMetadata->emplace_back(tmaInfo);\n+\n+    uint32_t elemsPerBox =\n+        std::accumulate(boxDims.begin(), boxDims.end(), 1, std::multiplies{});\n+\n+    Value clusterCTAId = getClusterCTAId(rewriter, loc);\n+    SmallVector<Value> multiDimClusterCTAId =\n+        delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n+\n+    Value llDst = adaptor.getDst();\n+    Value llIndex = adaptor.getIndex();\n+    Value src = op.getSrc();\n+    Value dst = op.getDst();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto dstShape = dstTy.getShape();\n+    auto smemObj = getSharedMemoryObjectFromStruct(loc, llDst, rewriter);\n+\n+    // the offset of coord considering multicast slicing\n+    SmallVector<Value> mcastOffsetVals;\n+    // The index of slice is this CTAId is responsible for\n+    SmallVector<Value> multiDimSliceIdx(rank);\n+    for (auto i = 0; i < rank; ++i)\n+      multiDimSliceIdx[i] =\n+          udiv(multiDimClusterCTAId[i], i32_val(CTASplitNum[i]));\n+    Value sliceIdx =\n+        linearize(rewriter, loc, multiDimSliceIdx, numMcast, CTAOrder);\n+\n+    Value sliceCoord;\n+    for (auto i = 0; i < rank; ++i) {\n+      if (inOrder[i] == rank - 1) {\n+        // TODO[goostavz]: Cases when the size of the highest order is smaller\n+        //                 than numMcasts is not implemented.\n+        sliceCoord = mul(sliceIdx, i32_val(shapePerCTA[i] / accNumMcast));\n+        mcastOffsetVals.emplace_back(\n+            mul(sliceIdx, i32_val(shapePerCTA[i] / accNumMcast)));\n+      } else {\n+        mcastOffsetVals.emplace_back(i32_val(0));\n+      }\n+    }\n+\n+    uint32_t elemsPerSlice = std::accumulate(\n+        shapePerCTA.begin(), shapePerCTA.end(), 1, std::multiplies{});\n+    Value dstOffsetCommon = mul(llIndex, i32_val(elemsPerSlice));\n+    // [benzh] sliceCoord should be higher dimension's multiplier accumulate.\n+    // currently only support rank == 2.\n+    dstOffsetCommon =\n+        add(dstOffsetCommon, mul(sliceCoord, i32_val(boxDims[0])));\n+    auto dstPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n+\n+    Value tmaDesc =\n+        llFuncOp.getBody().front().getArgument(tmaInfo.TMADescArgIdx);\n+    // TODO: sink this logic into Triton::NVGPU dialect and support more\n+    // cache-policy modes\n+    Value l2Desc = int_val(64, 0x1000000000000000ll);\n+\n+    auto ptrI8SharedTy = LLVM::LLVMPointerType::get(\n+        typeConverter->convertType(rewriter.getI8Type()), 3);\n+\n+    SmallVector<Value> coordCommon;\n+    auto llCoord = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getSrc(), rewriter, src.getType());\n+\n+    for (int i = 0; i < rank; ++i) {\n+      auto dim = getDimOfOrder(inOrder, i);\n+      Value coordDim = bitcast(llCoord[dim], i32_ty);\n+      if (CTASplitNum[dim] != 1) {\n+        // Add offset for each CTA\n+        //   boxDims[i] * (multiDimClusterCTAId[i] % CTASplitNum[i]);\n+        auto CTAOffset =\n+            mul(i32_val(shapePerCTA[dim]),\n+                urem(multiDimClusterCTAId[dim], i32_val(CTASplitNum[dim])));\n+        coordDim = add(coordDim, CTAOffset);\n+      }\n+\n+      if (i == rank - 1)\n+        // Add offset in case of multicast slicing\n+        coordCommon.push_back(add(coordDim, mcastOffsetVals[dim]));\n+      else\n+        coordCommon.push_back(coordDim);\n+    }\n+\n+    auto threadId = getThreadId(rewriter, loc);\n+    Value pred = icmp_eq(threadId, i32_val(0));\n+\n+    auto mask = adaptor.getMask();\n+    if (mask) {\n+      // TODO(thomas): What is the right implementation for this case?\n+      assert(mask.getType().isInteger(1) &&\n+             \"need to implement cases with tensor mask\");\n+      pred = rewriter.create<arith::AndIOp>(loc, pred, mask);\n+    }\n+\n+    Value mcastMask = getMCastMask(sharedLayout, rewriter, loc, clusterCTAId);\n+\n+    for (size_t i = 0; i < numBoxes; ++i) {\n+      Value dstOffset =\n+          add(dstOffsetCommon, i32_val(i * elemsPerBox * accNumMcast));\n+      Value dstPtrBase = gep(dstPtrTy, smemObj.base, dstOffset);\n+      SmallVector<Value> coord = coordCommon;\n+      coord[0] = add(coordCommon[0], i32_val(i * boxDims[0]));\n+      rewriter.create<triton::nvgpu::TMALoadTiledOp>(\n+          loc, bitcast(dstPtrBase, ptrI8SharedTy), adaptor.getMbar(), tmaDesc,\n+          l2Desc, pred, coord, mcastMask);\n+    }\n+\n+    rewriter.replaceOp(op, llDst);\n+    return success();\n+  }\n+\n+private:\n+  Value getMCastMask(const SharedEncodingAttr &sharedLayout,\n+                     ConversionPatternRewriter &rewriter, Location loc,\n+                     Value clusterCTAId) const {\n+    auto CTAsPerCGA = sharedLayout.getCTALayout().getCTAsPerCGA();\n+    auto CTAOrder = sharedLayout.getCTALayout().getCTAOrder();\n+    auto CTASplitNum = sharedLayout.getCTALayout().getCTASplitNum();\n+\n+    // Short path when no multicast is needed\n+    if (CTAsPerCGA == CTASplitNum)\n+      return nullptr;\n+\n+    // Short path when bcastMask is a constant\n+    bool isConstMcastMask = true;\n+    for (unsigned s : CTASplitNum) {\n+      if (s > 1) {\n+        isConstMcastMask = false;\n+        break;\n+      }\n+    }\n+    if (isConstMcastMask) {\n+      unsigned numCTAs = std::accumulate(CTAsPerCGA.begin(), CTAsPerCGA.end(),\n+                                         1, std::multiplies{});\n+      return int_val(/*width*/ 16, (1u << numCTAs) - 1);\n+    }\n+\n+    SmallVector<Value> multiDimCTAId =\n+        delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n+    auto rank = CTAOrder.size();\n+    SmallVector<SmallVector<Value>> multiDimMask(rank);\n+    unsigned accNumMcast = 1;\n+    SmallVector<unsigned> numMcast(rank);\n+    for (unsigned i = 0; i < rank; ++i) {\n+      // For the ith dimension, CTAsPerCGA[i]/CTASplitNum[i] vals is to be\n+      // broadcasted, which for this CTAId is:\n+      //     multiDimCTAId[i] % CTASplitNum[i] + (0 ..\n+      //     (CTAsPerCGA[i]/CTASplitNum[i] - 1)) * CTASplitNum[i]\n+      // TODO: will there be cases if CTAsPerCGA[i]/CTASplitNum[i] < 1?\n+      Value rem = urem(multiDimCTAId[i], i32_val(CTASplitNum[i]));\n+      numMcast[i] = CTAsPerCGA[i] / CTASplitNum[i];\n+      accNumMcast *= numMcast[i];\n+      for (unsigned j = 0; j < numMcast[i]; ++j) {\n+        if (j == 0) {\n+          multiDimMask[i].push_back(rem);\n+        } else {\n+          multiDimMask[i].push_back(add(rem, i32_val(j * CTASplitNum[i])));\n+        }\n+      }\n+    }\n+\n+    Value bcastMask = int_val(/*width*/ 16, 0);\n+    Value _1_i16 = int_val(/*width*/ 16, 1);\n+    for (unsigned i = 0; i < accNumMcast; ++i) {\n+      SmallVector<unsigned> multiDimIdx =\n+          getMultiDimIndex<unsigned>(i, numMcast, CTAOrder);\n+      SmallVector<Value> multiDimMaskedCTAId(rank);\n+      for (unsigned dim = 0; dim < rank; ++dim) {\n+        multiDimMaskedCTAId[dim] = multiDimMask[dim][multiDimIdx[dim]];\n+      }\n+      Value bcastCTAId =\n+          linearize(rewriter, loc, multiDimMaskedCTAId, CTAsPerCGA, CTAOrder);\n+      // bcastMask |= 1u << bcastCTAId;\n+      bcastMask = or_(bcastMask, shl(_1_i16, trunc(i16_ty, bcastCTAId)));\n+    }\n+\n+    return bcastMask;\n+  }\n+\n+  CUtensorMapDataType getCUtensorMapDataType(Type ty) const {\n+    if (ty.isF16()) {\n+      return CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_FLOAT16;\n+    } else if (ty.isF32()) {\n+      return CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_FLOAT32;\n+    } else {\n+      llvm::report_fatal_error(\"Unsupported elemTy for InsertSliceAsyncV2Op\");\n+      return CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_FLOAT16;\n+    }\n+  }\n+\n+  unsigned getArgIdx(Value v) const {\n+    if (auto op = v.getDefiningOp<mlir::arith::ConstantOp>()) {\n+      return -1 -\n+             op.getValue().dyn_cast<IntegerAttr>().getValue().getZExtValue();\n+    }\n+    if (v.getDefiningOp() &&\n+        isa<mlir::UnrealizedConversionCastOp>(v.getDefiningOp())) {\n+      return getArgIdx(v.getDefiningOp()->getOperand(0));\n+    } else if (v.getParentBlock()->isEntryBlock() && v.isa<BlockArgument>()) {\n+      // in entryblock and is BlockArgument; Because argument of func are\n+      // arugments of entryblock bb0 in MLIR\n+      return v.cast<BlockArgument>().getArgNumber();\n+    } else if (v.getParentBlock()->isEntryBlock() &&\n+               (!v.isa<BlockArgument>())) {\n+      // in entryblock but not BlockArgument\n+      return getArgIdx(v.getDefiningOp()->getOperand(0));\n+    } else if (!v.getParentBlock()->isEntryBlock()) {\n+      // in non-entryblock\n+      return getArgIdx(v.getDefiningOp()->getOperand(0));\n+    } else {\n+      llvm::report_fatal_error(\n+          \"Operand of `MakeTensorPtrOp` is not the function's argument\");\n+      return 0;\n+    }\n+  }\n+\n+  int getNumTMADescs(LLVM::LLVMFuncOp func) const {\n+    if (!func->hasAttr(kAttrNumTMALoadDescsName)) {\n+      llvm::report_fatal_error(\"TritonGPU module should contain a \"\n+                               \"triton_gpu.num-tma-load attribute\");\n+      return -1;\n+    }\n+    if (!func->hasAttr(kAttrNumTMAStoreDescsName)) {\n+      llvm::report_fatal_error(\"TritonGPU module should contain a \"\n+                               \"triton_gpu.num-tma-store attribute\");\n+      return -1;\n+    }\n+    return func->getAttr(kAttrNumTMAStoreDescsName)\n+               .cast<IntegerAttr>()\n+               .getInt() +\n+           func->getAttr(kAttrNumTMALoadDescsName).cast<IntegerAttr>().getInt();\n+  }\n+\n+  const TensorPtrMapT *tensorPtrMap;\n+};\n+\n void populateLoadStoreOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    ModuleAxisInfoAnalysis &axisInfoAnalysis, ModuleAllocation &allocation,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n-    PatternBenefit benefit) {\n+    mlir::triton::gpu::TMAMetadataTy *tmaMetadata,\n+    const TensorPtrMapT *tensorPtrMap, PatternBenefit benefit) {\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<AtomicCASOpConversion>(typeConverter, allocation,\n@@ -869,4 +1497,8 @@ void populateLoadStoreOpToLLVMPatterns(\n                                         indexCacheInfo, benefit);\n   patterns.add<InsertSliceAsyncOpConversion>(\n       typeConverter, allocation, indexCacheInfo, axisInfoAnalysis, benefit);\n+  patterns.add<InsertSliceAsyncV2OpConversion>(\n+      typeConverter, allocation, tmaMetadata, tensorPtrMap, benefit);\n+  patterns.add<StoreAsyncOpConversion>(typeConverter, allocation, tmaMetadata,\n+                                       tensorPtrMap, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.h", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -8,8 +8,10 @@ using namespace mlir::triton;\n \n void populateLoadStoreOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n-    ModuleAxisInfoAnalysis &axisInfoAnalysis, ModuleAllocation &allocation,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n-    PatternBenefit benefit);\n+    mlir::triton::gpu::TMAMetadataTy *tmaMetadata,\n+    const TensorPtrMapT *tensorPtrMap, PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 280, "deletions": 155, "changes": 435, "file_content_changes": "@@ -1,5 +1,7 @@\n #include \"ReduceOpToLLVM.h\"\n #include \"Utility.h\"\n+#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n@@ -14,8 +16,13 @@ using ::mlir::triton::gpu::getTotalElemsPerThread;\n struct ReduceOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::ReduceOp> {\n public:\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::ReduceOp>::ConvertTritonGPUOpToLLVMPattern;\n+  ReduceOpConversion(\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n+      ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+      int computeCapability, PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::ReduceOp>(\n+            typeConverter, allocation, indexCacheInfo, benefit),\n+        computeCapability(computeCapability) {}\n \n   LogicalResult\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n@@ -26,14 +33,12 @@ struct ReduceOpConversion\n   }\n \n private:\n+  int computeCapability;\n+\n   void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n-                  llvm::SmallVectorImpl<Value> &acc, ValueRange cur,\n-                  bool isFirst) const {\n+                  SmallVector<Value> &acc, ValueRange cur, bool isFirst) const {\n     if (isFirst) {\n-      acc.resize(cur.size());\n-      for (unsigned i = 0; i < cur.size(); ++i) {\n-        acc[i] = cur[i];\n-      }\n+      acc = SmallVector<Value>(cur.begin(), cur.end());\n       return;\n     }\n \n@@ -114,7 +119,7 @@ struct ReduceOpConversion\n       // writeIdx[originalAxis] = index[originalAxis] / axisSizePerThread\n       writeIdx[originalAxis] = udiv(index[originalAxis], axisSizePerThread);\n     } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-      if (!mmaLayout.isAmpere()) {\n+      if (!mmaLayout.isAmpere() && !mmaLayout.isHopper()) {\n         llvm::report_fatal_error(\"Unsupported layout\");\n       }\n       if (originalAxis == 0) {\n@@ -157,7 +162,6 @@ struct ReduceOpConversion\n       elemPtrTys[i] = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n     }\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n \n     auto smemShape = helper.getScratchConfigBasic();\n     unsigned elems = product<unsigned>(smemShape);\n@@ -171,33 +175,10 @@ struct ReduceOpConversion\n                   elemPtrTys[i]);\n     }\n \n-    unsigned srcElems = getTotalElemsPerThread(srcTys[0]);\n-    // Emits indices of the original tensor that each thread\n-    // would own\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTys[0]);\n     auto srcValues = unpackInputs(loc, op, adaptor, rewriter);\n-\n-    // Emits offsets (the offset from the base index)\n-    // of the original tensor that each thread would own\n-    // NOTE: Assumes offsets don't actually depend on type\n-    SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForLayout(srcLayout, srcTys[0]);\n-\n-    // Keep track of accumulations and their indices\n     std::map<SmallVector<unsigned>, SmallVector<Value>> accs;\n     std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n-\n-    Region *combineOp = &op.getCombineOp();\n-\n-    // reduce within threads\n-    for (unsigned i = 0; i < srcElems; ++i) {\n-      SmallVector<unsigned> key = offset[i];\n-      key[axis] = 0;\n-      bool isFirst = accs.find(key) == accs.end();\n-      accumulate(rewriter, *combineOp, accs[key], srcValues[i], isFirst);\n-      if (isFirst)\n-        indices[key] = srcIndices[i];\n-    }\n+    reduceWithinThreads(helper, srcValues, accs, indices, rewriter);\n \n     // cached int32 constants\n     std::map<int, Value> ints;\n@@ -249,23 +230,25 @@ struct ReduceOpConversion\n           readPtrs[i] = gep(elemPtrTys[i], writePtrs[i], readOffset);\n         }\n \n-        barrier();\n+        sync(rewriter, loc, op);\n+\n         // Combine accumulator value from another thread\n         SmallVector<Value> cur(op.getNumOperands());\n         for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n           cur[i] = load(readPtrs[i]);\n         }\n-        accumulate(rewriter, *combineOp, acc, cur, false);\n+        accumulate(rewriter, op.getCombineOp(), acc, cur, false);\n+\n+        sync(rewriter, loc, op);\n \n-        barrier();\n         // Publish our new accumulator value to shared memory\n         for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n           store(acc[i], writePtrs[i]);\n         }\n       }\n     }\n \n-    barrier();\n+    sync(rewriter, loc, op);\n \n     // set output values\n     SmallVector<Value> results(op.getNumOperands());\n@@ -302,78 +285,186 @@ struct ReduceOpConversion\n     return success();\n   }\n \n-  // Use warp shuffle for reduction within warps and shared memory for data\n-  // exchange across warps\n-  LogicalResult matchAndRewriteFast(triton::ReduceOp op, OpAdaptor adaptor,\n-                                    ConversionPatternRewriter &rewriter) const {\n-    ReduceOpHelper helper(op);\n-    Location loc = op->getLoc();\n-    unsigned axis = adaptor.getAxis();\n-\n-    auto srcTys = op.getInputTypes();\n-    auto srcLayout = helper.getSrcLayout();\n-    if (!helper.isSupportedLayout()) {\n-      assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n-    }\n-    auto srcOrd = triton::gpu::getOrder(srcLayout);\n-    auto srcShape = helper.getSrcShape();\n-\n-    SmallVector<Type> elemPtrTys(srcTys.size());\n-    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n-      auto ty = srcTys[i].getElementType();\n-      auto llvmElemTy = getTypeConverter()->convertType(ty);\n-      elemPtrTys[i] = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+  void sync(ConversionPatternRewriter &rewriter, Location loc,\n+            triton::ReduceOp op) const {\n+    // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n+    // attr.\n+    if (op->hasAttr(\"async_agent\")) {\n+      barSync(rewriter, op, getAgentIds(op).front(), 128);\n+    } else {\n+      barrier();\n     }\n-    auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n-\n-    auto smemShapes = helper.getScratchConfigsFast();\n-    unsigned elems = product<unsigned>(smemShapes[0]);\n-    unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));\n-\n-    unsigned sizeIntraWarps = helper.getIntraWarpSizeWithUniqueData();\n-    unsigned sizeInterWarps = helper.getInterWarpSizeWithUniqueData();\n+  }\n \n-    SmallVector<Value> smemBases(op.getNumOperands());\n-    bool isWarpSync = helper.isWarpSynchronous();\n-\n-    if (!isWarpSync) {\n-      smemBases[0] = bitcast(\n-          getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys[0]);\n-      for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n-        smemBases[i] =\n-            bitcast(gep(elemPtrTys[i - 1], smemBases[i - 1], i32_val(maxElems)),\n-                    elemPtrTys[i]);\n-      }\n+  // Check if the reduction can use a redux op and return the kind.\n+  std::optional<NVVM::ReduxKind> matchReduxKind(triton::ReduceOp op) const {\n+    if (computeCapability < 80)\n+      return std::nullopt;\n+    if (op.getNumOperands() != 1 || op.getNumResults() != 1)\n+      return std::nullopt;\n+    Block *block = &(*op.getCombineOp().begin());\n+    Operation *yield = block->getTerminator();\n+    Operation *reduceOp = yield->getOperand(0).getDefiningOp();\n+    if (!reduceOp || reduceOp->getNumOperands() != 2 ||\n+        reduceOp->getNumResults() != 1 ||\n+        !reduceOp->getResultTypes()[0].isInteger(32))\n+      return std::nullopt;\n+    if (reduceOp->getOperand(0) != block->getArgument(0) ||\n+        reduceOp->getOperand(1) != block->getArgument(1))\n+      return std::nullopt;\n+    if (isa<arith::AddIOp>(reduceOp))\n+      return NVVM::ReduxKind::ADD;\n+    if (isa<arith::AndIOp>(reduceOp))\n+      return NVVM::ReduxKind::AND;\n+    if (isa<arith::OrIOp>(reduceOp))\n+      return NVVM::ReduxKind::OR;\n+    if (isa<arith::XOrIOp>(reduceOp))\n+      return NVVM::ReduxKind::XOR;\n+    if (auto externalCall =\n+            dyn_cast<triton::PureExternElementwiseOp>(reduceOp)) {\n+      if (externalCall.getSymbol() == \"__nv_min\")\n+        return NVVM::ReduxKind::MIN;\n+      if (externalCall.getSymbol() == \"__nv_umin\")\n+        return NVVM::ReduxKind::UMIN;\n+      if (externalCall.getSymbol() == \"__nv_max\")\n+        return NVVM::ReduxKind::MAX;\n+      if (externalCall.getSymbol() == \"__nv_umax\")\n+        return NVVM::ReduxKind::UMAX;\n     }\n+    return std::nullopt;\n+  }\n \n-    unsigned srcElems = getTotalElemsPerThread(srcTys[0]);\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTys[0]);\n-    auto srcValues = unpackInputs(loc, op, adaptor, rewriter);\n-\n-    std::map<SmallVector<unsigned>, SmallVector<Value>> accs;\n-    std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n-\n+  // Reduce along op axis for elements that are in the same thread. The\n+  // accumulated value is stored in accs.\n+  void reduceWithinThreads(\n+      ReduceOpHelper &helper, SmallVector<SmallVector<Value>> &srcValues,\n+      std::map<SmallVector<unsigned>, SmallVector<Value>> &accs,\n+      std::map<SmallVector<unsigned>, SmallVector<Value>> &indices,\n+      ConversionPatternRewriter &rewriter) const {\n+    triton::ReduceOp op = helper.getOperation();\n+    RankedTensorType operandType = op.getInputTypes()[0];\n     // Assumes offsets don't actually depend on type\n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForLayout(srcLayout, srcTys[0]);\n-\n+        emitOffsetForLayout(helper.getSrcLayout(), operandType);\n+    unsigned srcElems = getTotalElemsPerThread(operandType);\n     auto *combineOp = &op.getCombineOp();\n-\n+    auto srcIndices =\n+        emitIndices(op.getLoc(), rewriter, helper.getSrcLayout(), operandType);\n     // reduce within threads\n     for (unsigned i = 0; i < srcElems; ++i) {\n       SmallVector<unsigned> key = offset[i];\n-      key[axis] = 0;\n+      key[op.getAxis()] = 0;\n       bool isFirst = accs.find(key) == accs.end();\n       accumulate(rewriter, *combineOp, accs[key], srcValues[i], isFirst);\n       if (isFirst)\n         indices[key] = srcIndices[i];\n     }\n+  }\n+\n+  // Apply warp reduction across the given number of contiguous lanes using op\n+  // region and the accumulator values as source.\n+  void warpReduce(ConversionPatternRewriter &rewriter, Location loc,\n+                  SmallVector<Value> &acc, triton::ReduceOp op,\n+                  unsigned numLaneToReduce) const {\n+    if (auto kind = matchReduxKind(op)) {\n+      // Based on benchmarking on A100 redux op gives a speed up only when doing\n+      // a single reduction (not partioned) and when the mask is static.\n+      // Therefore we currently only enable it to reduce across all the lanes.\n+      if (numLaneToReduce == 32) {\n+        assert(acc.size() == 1);\n+        Value mask = i32_val(0xFFFFFFFF);\n+        // Even though we currently don't use redux for partitioned reduction\n+        // the code below supports it in case we want to tweak the heuristic.\n+        if (numLaneToReduce < 32) {\n+          // For partitioned reduction we need to caluclate the mask so that\n+          // each group of numLaneToReduce threads has the correct mask.\n+          unsigned bitmask = (1 << numLaneToReduce) - 1;\n+          Value threadId = getThreadId(rewriter, loc);\n+          Value laneId = urem(threadId, i32_val(32));\n+          mask = shl(i32_val(bitmask),\n+                     and_(laneId, i32_val(~(numLaneToReduce - 1))));\n+        }\n+        acc[0] = rewriter.create<NVVM::ReduxOp>(loc, acc[0].getType(), acc[0],\n+                                                *kind, mask);\n+        return;\n+      }\n+    }\n+\n+    for (unsigned N = numLaneToReduce / 2; N > 0; N >>= 1) {\n+      SmallVector<Value> shfl(acc.size());\n+      for (unsigned i = 0; i < acc.size(); ++i) {\n+        shfl[i] = shflSync(loc, rewriter, acc[i], N);\n+      }\n+      accumulate(rewriter, op.getCombineOp(), acc, shfl, false);\n+    }\n+  }\n+\n+  // Reduce across threads within each warp.\n+  void\n+  reduceWithinWarps(ReduceOpHelper &helper,\n+                    std::map<SmallVector<unsigned>, SmallVector<Value>> &accs,\n+                    ConversionPatternRewriter &rewriter) const {\n+    triton::ReduceOp op = helper.getOperation();\n+    unsigned sizeIntraWarps = helper.getIntraWarpSizeWithUniqueData();\n+    for (auto it : accs) {\n+      const SmallVector<unsigned> &key = it.first;\n+      SmallVector<Value> &acc = accs[key];\n+      warpReduce(rewriter, op.getLoc(), acc, op, sizeIntraWarps);\n+    }\n+  }\n+\n+  // Pack the accumualtor values and replace the reduce op with the result.\n+  void packResults(ReduceOpHelper &helper,\n+                   std::map<SmallVector<unsigned>, SmallVector<Value>> &accs,\n+                   ConversionPatternRewriter &rewriter) const {\n+    triton::ReduceOp op = helper.getOperation();\n+    Location loc = op.getLoc();\n+    unsigned axis = op.getAxis();\n+    SmallVector<Value> results(op.getNumOperands());\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      if (auto resultTy =\n+              op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n+        auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n+        unsigned resultElems = getTotalElemsPerThread(resultTy);\n+        SmallVector<SmallVector<unsigned>> resultOffset =\n+            emitOffsetForLayout(resultLayout, resultTy);\n+        SmallVector<Value> resultVals;\n+        for (int j = 0; j < resultElems; j++) {\n+          auto key = resultOffset[j];\n+          key.insert(key.begin() + axis, 0);\n+          resultVals.push_back(accs[key][i]);\n+        }\n+        results[i] = getTypeConverter()->packLLElements(loc, resultVals,\n+                                                        rewriter, resultTy);\n+      } else\n+        results[i] = accs.begin()->second[i];\n+    }\n+    rewriter.replaceOp(op, results);\n+  }\n+\n+  // Return the type of the shared memory pointer for operand i.\n+  Type getElementPtrType(triton::ReduceOp op, int i) const {\n+    auto ty = op.getInputTypes()[i].getElementType();\n+    auto llvmElemTy = getTypeConverter()->convertType(ty);\n+    return LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+  }\n \n+  void storeWarpReduceToSharedMemory(\n+      ReduceOpHelper &helper,\n+      std::map<SmallVector<unsigned>, SmallVector<Value>> &accs,\n+      std::map<SmallVector<unsigned>, SmallVector<Value>> &indices,\n+      SmallVector<Value> &smemBases,\n+      ConversionPatternRewriter &rewriter) const {\n+    triton::ReduceOp op = helper.getOperation();\n+    Location loc = op.getLoc();\n     Value threadId = getThreadId(rewriter, loc);\n     Value warpSize = i32_val(32);\n     Value warpId = udiv(threadId, warpSize);\n     Value laneId = urem(threadId, warpSize);\n+    auto srcLayout = helper.getSrcLayout();\n+    auto srcShape = helper.getSrcShape();\n+    unsigned axis = op.getAxis();\n+    auto smemShapes = helper.getScratchConfigsFast();\n \n     auto threadsPerWarp =\n         triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout, srcShape);\n@@ -391,67 +482,38 @@ struct ReduceOpConversion\n     Value zero = i32_val(0);\n     Value laneZero = icmp_eq(laneIdAxis, zero);\n \n-    std::map<SmallVector<unsigned>, SmallVector<Value>> finalAccs;\n     for (auto it : accs) {\n       const SmallVector<unsigned> &key = it.first;\n-      SmallVector<Value> acc = it.second;\n-\n-      // Reduce within warps\n-      for (unsigned N = sizeIntraWarps / 2; N > 0; N >>= 1) {\n-        SmallVector<Value> shfl(op.getNumOperands());\n-        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n-          shfl[i] = shflSync(loc, rewriter, acc[i], N);\n-        }\n-        accumulate(rewriter, *combineOp, acc, shfl, false);\n-      }\n-\n-      if (isWarpSync) {\n-        finalAccs[key] = acc;\n-        continue;\n-      }\n+      SmallVector<Value> &acc = it.second;\n \n       SmallVector<Value> writeIdx = indices[key];\n       writeIdx[axis] = warpIdAxis;\n       Value writeOffset =\n           linearize(rewriter, loc, writeIdx, smemShapes[0], order);\n       for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n-        Value writePtr = gep(elemPtrTys[i], smemBases[i], writeOffset);\n+        auto elemPtrTy = getElementPtrType(op, i);\n+        Value writePtr = gep(elemPtrTy, smemBases[i], writeOffset);\n         storeShared(rewriter, loc, writePtr, acc[i], laneZero);\n       }\n     }\n+  }\n \n-    if (isWarpSync) {\n-      SmallVector<Value> results(op.getNumOperands());\n-      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n-        if (auto resultTy =\n-                op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n-          auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n-          unsigned resultElems = getTotalElemsPerThread(resultTy);\n-          SmallVector<SmallVector<unsigned>> resultOffset =\n-              emitOffsetForLayout(resultLayout, resultTy);\n-          SmallVector<Value> resultVals;\n-          for (int j = 0; j < resultElems; j++) {\n-            auto key = resultOffset[j];\n-            key.insert(key.begin() + axis, 0);\n-            resultVals.push_back(finalAccs[key][i]);\n-          }\n-          results[i] = getTypeConverter()->packLLElements(loc, resultVals,\n-                                                          rewriter, resultTy);\n-        } else\n-          results[i] = finalAccs.begin()->second[i];\n-      }\n-      rewriter.replaceOp(op, results);\n-      return success();\n-    }\n-\n-    barrier();\n+  // Load the reduction of each warp and accumulate them to a final value and\n+  // store back to shared memory.\n+  void accumulatePartialReductions(ReduceOpHelper &helper,\n+                                   SmallVector<Value> &smemBases,\n+                                   ConversionPatternRewriter &rewriter) const {\n+    triton::ReduceOp op = helper.getOperation();\n+    auto srcLayout = helper.getSrcLayout();\n+    auto smemShapes = helper.getScratchConfigsFast();\n+    unsigned elems = product<unsigned>(smemShapes[0]);\n+    unsigned sizeInterWarps = helper.getInterWarpSizeWithUniqueData();\n+    Location loc = op.getLoc();\n \n-    // The second round of shuffle reduction\n-    //   now the problem size: sizeInterWarps, s1, s2, .. , sn\n-    //   where sizeInterWarps is 2^m\n-    //\n-    // Each thread needs to process:\n-    //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n+    Value threadId = getThreadId(rewriter, loc);\n+    Value warpSize = i32_val(32);\n+    Value laneId = urem(threadId, warpSize);\n+    Value zero = i32_val(0);\n \n     auto mod = op.getOperation()->getParentOfType<ModuleOp>();\n     unsigned numThreads =\n@@ -464,23 +526,18 @@ struct ReduceOpConversion\n       // i32_val(sizeInerWarps))\n       SmallVector<Value> acc(op.getNumOperands());\n       for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n-        Value readPtr = gep(elemPtrTys[i], smemBases[i], readOffset);\n+        auto elemPtrTy = getElementPtrType(op, i);\n+        Value readPtr = gep(elemPtrTy, smemBases[i], readOffset);\n         acc[i] = load(readPtr);\n       }\n-\n-      for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n-        SmallVector<Value> shfl(op.getNumOperands());\n-        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n-          shfl[i] = shflSync(loc, rewriter, acc[i], N);\n-        }\n-        accumulate(rewriter, *combineOp, acc, shfl, false);\n-      }\n+      warpReduce(rewriter, loc, acc, op, sizeInterWarps);\n \n       // only the first thread in each sizeInterWarps is writing\n       Value writeOffset = readOffset;\n       SmallVector<Value> writePtrs(op.getNumOperands());\n       for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n-        writePtrs[i] = gep(elemPtrTys[i], smemBases[i], writeOffset);\n+        auto elemPtrTy = getElementPtrType(op, i);\n+        writePtrs[i] = gep(elemPtrTy, smemBases[i], writeOffset);\n       }\n       Value threadIsNeeded = icmp_slt(threadId, i32_val(elems));\n       Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));\n@@ -496,10 +553,17 @@ struct ReduceOpConversion\n         readOffset = add(readOffset, i32_val(numThreads));\n       }\n     }\n+  }\n \n-    barrier();\n-\n-    // set output values\n+  // Load the final reduction from shared memory and replace the reduce result\n+  // with it.\n+  void loadReductionAndPackResult(ReduceOpHelper &helper,\n+                                  SmallVector<Value> &smemBases,\n+                                  ConversionPatternRewriter &rewriter) const {\n+    triton::ReduceOp op = helper.getOperation();\n+    Location loc = op.getLoc();\n+    auto smemShapes = helper.getScratchConfigsFast();\n+    auto order = getOrder(helper.getSrcLayout());\n     SmallVector<Value> results(op.getNumOperands());\n     for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n       if (auto resultTy =\n@@ -513,10 +577,11 @@ struct ReduceOpConversion\n         SmallVector<Value> resultVals(resultElems);\n         for (size_t j = 0; j < resultElems; ++j) {\n           SmallVector<Value> readIdx = resultIndices[j];\n-          readIdx.insert(readIdx.begin() + axis, i32_val(0));\n+          readIdx.insert(readIdx.begin() + op.getAxis(), i32_val(0));\n           Value readOffset =\n               linearize(rewriter, loc, readIdx, smemShapes[0], order);\n-          Value readPtr = gep(elemPtrTys[i], smemBases[i], readOffset);\n+          Value readPtr =\n+              gep(getElementPtrType(op, i), smemBases[i], readOffset);\n           resultVals[j] = load(readPtr);\n         }\n \n@@ -528,16 +593,76 @@ struct ReduceOpConversion\n       }\n     }\n     rewriter.replaceOp(op, results);\n+  }\n+\n+  // Use warp shuffle for reduction within warps and shared memory for data\n+  // exchange across warps\n+  LogicalResult matchAndRewriteFast(triton::ReduceOp op, OpAdaptor adaptor,\n+                                    ConversionPatternRewriter &rewriter) const {\n+    ReduceOpHelper helper(op);\n+    assert(helper.isSupportedLayout() &&\n+           \"Unexpected srcLayout in ReduceOpConversion\");\n+    Location loc = op->getLoc();\n+\n+    auto srcValues = unpackInputs(loc, op, adaptor, rewriter);\n+    std::map<SmallVector<unsigned>, SmallVector<Value>> accs;\n+    std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n+    // First reduce all the values along axis within each thread.\n+    reduceWithinThreads(helper, srcValues, accs, indices, rewriter);\n+\n+    // Then reduce across threads within a warp.\n+    reduceWithinWarps(helper, accs, rewriter);\n+\n+    if (helper.isWarpSynchronous()) {\n+      // If all the values to be reduced are within the same warp there is\n+      // nothing left to do.\n+      packResults(helper, accs, rewriter);\n+      return success();\n+    }\n+\n+    // Compute a shared memory base per operand.\n+    auto smemShapes = helper.getScratchConfigsFast();\n+    unsigned elems = product<unsigned>(smemShapes[0]);\n+    unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));\n+    SmallVector<Value> smemBases(op.getNumOperands());\n+    smemBases[0] =\n+        bitcast(getSharedMemoryBase(loc, rewriter, op.getOperation()),\n+                getElementPtrType(op, 0));\n+    for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n+      smemBases[i] = bitcast(gep(getElementPtrType(op, i - 1), smemBases[i - 1],\n+                                 i32_val(maxElems)),\n+                             getElementPtrType(op, i));\n+    }\n+    storeWarpReduceToSharedMemory(helper, accs, indices, smemBases, rewriter);\n+\n+    sync(rewriter, loc, op);\n+\n+    // The second round of shuffle reduction\n+    //   now the problem size: sizeInterWarps, s1, s2, .. , sn\n+    //   where sizeInterWarps is 2^m\n+    //\n+    // Each thread needs to process:\n+    //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n+    accumulatePartialReductions(helper, smemBases, rewriter);\n+\n+    // We could avoid this barrier in some of the layouts, however this is not\n+    // the general case.\n+    // TODO: optimize the barrier in case the layouts are accepted.\n+    sync(rewriter, loc, op);\n+\n+    // set output values\n+    loadReductionAndPackResult(helper, smemBases, rewriter);\n \n     return success();\n   }\n };\n \n void populateReduceOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n     ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n-    PatternBenefit benefit) {\n+    int computeCapability, PatternBenefit benefit) {\n   patterns.add<ReduceOpConversion>(typeConverter, allocation, indexCacheInfo,\n-                                   benefit);\n+                                   computeCapability, benefit);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -8,8 +8,9 @@ using namespace mlir::triton;\n \n void populateReduceOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n     ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n-    PatternBenefit benefit);\n+    int computeCapability, PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/RegReallocOpToLLVM.cpp", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "file_content_changes": "@@ -0,0 +1,43 @@\n+#include \"RegReallocOpToLLVM.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+struct RegAllocOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::nvidia_gpu::RegAllocOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::RegAllocOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::RegAllocOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    rewriter.replaceOpWithNewOp<triton::nvgpu::RegAllocOp>(\n+        op, adaptor.getRegCount());\n+    return success();\n+  }\n+};\n+\n+struct RegDeallocOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::nvidia_gpu::RegDeallocOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::RegDeallocOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::RegDeallocOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    rewriter.replaceOpWithNewOp<triton::nvgpu::RegDeallocOp>(\n+        op, adaptor.getRegCount());\n+    return success();\n+  }\n+};\n+\n+void populateRegReallocOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    const ModuleAllocation &allocation, PatternBenefit benefit) {\n+  patterns.add<RegAllocOpConversion>(typeConverter, benefit);\n+  patterns.add<RegDeallocOpConversion>(typeConverter, benefit);\n+  return;\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/RegReallocOpToLLVM.h", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_REGREALLOC_OP_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_REGREALLOC_OP_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateRegReallocOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    const ModuleAllocation &allocation, PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -316,6 +316,7 @@ ScanOpConversion::emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,\n \n void populateScanOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n     ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -8,6 +8,7 @@ using namespace mlir::triton;\n \n void populateScanOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n     ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TensorPtrOpsToLLVM.cpp", "status": "added", "additions": 104, "deletions": 0, "changes": 104, "file_content_changes": "@@ -0,0 +1,104 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"TensorPtrOpsToLLVM.h\"\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+struct MakeTensorPtrOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::MakeTensorPtrOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::MakeTensorPtrOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::MakeTensorPtrOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+\n+    // struct { offset0, offset1, shape0, shape1, stride0,\n+    // stride1, base_ptr};\n+    auto offsets = adaptor.getOffsets();\n+    auto shapes = adaptor.getShape();\n+    auto strides = adaptor.getStrides();\n+    auto base = adaptor.getBase();\n+    auto result = op.getResult();\n+\n+    SmallVector<Value> elems;\n+    for (auto offset : offsets)\n+      elems.push_back(offset);\n+    for (auto shape : shapes)\n+      elems.push_back(shape);\n+    for (auto stride : strides)\n+      elems.push_back(stride);\n+\n+    elems.push_back(base);\n+\n+    auto newValue = getTypeConverter()->packLLElements(\n+        op.getLoc(), elems, rewriter, result.getType());\n+    rewriter.replaceOp(op, newValue);\n+    return success();\n+  }\n+};\n+\n+struct AdvanceOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AdvanceOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::AdvanceOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AdvanceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // struct { offset0, offset1, shape0, shape1, stride0,\n+    // stride1, base_ptr};\n+    auto loc = op.getLoc();\n+    auto ptrType = op.getPtr().getType();\n+    auto tensorPtr = adaptor.getPtr();\n+\n+    auto offsets = adaptor.getOffsets();\n+    auto elems =\n+        getTypeConverter()->unpackLLElements(loc, tensorPtr, rewriter, ptrType);\n+\n+    SmallVector<Value, 2> newOffsets;\n+\n+    for (auto [offset, oldOffset] : llvm::zip_first(offsets, elems)) {\n+      newOffsets.push_back((add(offset, oldOffset)));\n+    }\n+\n+    for (size_t i = 0; i < newOffsets.size(); ++i) {\n+      elems[i] = newOffsets[i];\n+    }\n+\n+    auto newValue = getTypeConverter()->packLLElements(op.getLoc(), elems,\n+                                                       rewriter, ptrType);\n+    rewriter.replaceOp(op, newValue);\n+    return success();\n+  }\n+};\n+\n+void populateTensorPtrOpsToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    ModuleAllocation &allocation, PatternBenefit benefit) {\n+  patterns.add<MakeTensorPtrOpConversion>(typeConverter, benefit);\n+  patterns.add<AdvanceOpConversion>(typeConverter, benefit);\n+  return;\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TensorPtrOpsToLLVM.h", "status": "added", "additions": 37, "deletions": 0, "changes": 37, "file_content_changes": "@@ -0,0 +1,37 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_TENSOR_PTR_OPS_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_TENSOR_PTR_OPS_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateTensorPtrOpsToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+    ModuleAllocation &allocation, PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 106, "deletions": 17, "changes": 123, "file_content_changes": "@@ -389,18 +389,23 @@ struct GetProgramIdOpConversion\n   LogicalResult\n   matchAndRewrite(triton::GetProgramIdOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    // It is not easy to get the compute capability here, so we use numCTAs to\n+    // decide the semantic of GetProgramIdOp. If numCTAs = 1, then\n+    // GetProgramIdOp is converted to \"%ctaid\", otherwise it is converted to\n+    // \"%clusterid\".\n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for GetProgramIdOp\");\n+    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(moduleOp);\n+\n     Location loc = op->getLoc();\n     assert(op.getAxisAsInt() < 3);\n+    std::string sreg = numCTAs == 1 ? \"%ctaid.\" : \"%clusterid.\";\n+    sreg.append(1, 'x' + op.getAxisAsInt()); // 0 -> 'x', 1 -> 'y', 2 -> 'z'\n \n-    Value blockId =\n-        rewriter.create<::mlir::gpu::BlockIdOp>(loc, dims[op.getAxisAsInt()]);\n-    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(op, i32_ty, blockId);\n+    Value programId = getSRegValue(rewriter, loc, sreg);\n+    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(op, i32_ty, programId);\n     return success();\n   }\n-\n-  static constexpr mlir::gpu::Dimension dims[] = {mlir::gpu::Dimension::x,\n-                                                  mlir::gpu::Dimension::y,\n-                                                  mlir::gpu::Dimension::z};\n };\n \n struct GetNumProgramsOpConversion\n@@ -411,19 +416,54 @@ struct GetNumProgramsOpConversion\n   LogicalResult\n   matchAndRewrite(triton::GetNumProgramsOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    // It is not easy to get the compute capability here, so we use numCTAs to\n+    // decide the semantic of GetNumProgramsOp. If numCTAs = 1, then\n+    // GetNumProgramsOp is converted to \"%nctaid\", otherwise it is converted to\n+    // \"%nclusterid\".\n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for GetProgramIdOp\");\n+    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(moduleOp);\n+\n     Location loc = op->getLoc();\n     assert(op.getAxis() < 3);\n+    std::string sreg = numCTAs == 1 ? \"%nctaid.\" : \"%nclusterid.\";\n+    sreg.append(1, 'x' + op.getAxis()); // 0 -> 'x', 1 -> 'y', 2 -> 'z'\n \n-    Value blockId =\n-        rewriter.create<::mlir::gpu::GridDimOp>(loc, dims[op.getAxis()]);\n-    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(op, i32_ty, blockId);\n+    Value numPrograms = getSRegValue(rewriter, loc, sreg);\n+    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(op, i32_ty, numPrograms);\n+    return success();\n+  }\n+};\n \n+// TODO[goostavz]: GetThreadIdOp/GetClusterCTAIdOp is a temporary solution\n+// before async dialect is done. These concepts should appear in ttgpu\n+// level, and they are planned to be deprecated along with ttgpu.mbarrier_xxx\n+// ops.\n+struct GetThreadIdOpConversion : public ConvertTritonGPUOpToLLVMPattern<\n+                                     triton::nvidia_gpu::GetThreadIdOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::GetThreadIdOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::GetThreadIdOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOp(op, getThreadId(rewriter, op->getLoc()));\n     return success();\n   }\n+};\n+\n+struct GetClusterCTAIdOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<\n+          triton::nvidia_gpu::GetClusterCTAIdOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::nvidia_gpu::GetClusterCTAIdOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  static constexpr mlir::gpu::Dimension dims[] = {mlir::gpu::Dimension::x,\n-                                                  mlir::gpu::Dimension::y,\n-                                                  mlir::gpu::Dimension::z};\n+  LogicalResult\n+  matchAndRewrite(triton::nvidia_gpu::GetClusterCTAIdOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOp(op, getClusterCTAId(rewriter, op->getLoc()));\n+    return success();\n+  }\n };\n \n struct AddPtrOpConversion\n@@ -479,7 +519,8 @@ struct AllocTensorOpConversion\n         getTypeConverter()->convertType(resultTy.getElementType());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n     smemBase = bitcast(smemBase, elemPtrTy);\n-    auto order = resultTy.getEncoding().cast<SharedEncodingAttr>().getOrder();\n+    auto sharedLayout = resultTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto order = sharedLayout.getOrder();\n     // Workaround for 3D tensors\n     // TODO: we need to modify the pipeline pass to give a proper shared\n     // encoding to 3D tensors\n@@ -489,8 +530,9 @@ struct AllocTensorOpConversion\n     else\n       newOrder = SmallVector<unsigned>(order.begin(), order.end());\n \n-    auto smemObj = SharedMemoryObject(smemBase, resultTy.getShape(), newOrder,\n-                                      loc, rewriter);\n+    auto shapePerCTA = getShapePerCTA(sharedLayout, resultTy.getShape());\n+    auto smemObj =\n+        SharedMemoryObject(smemBase, shapePerCTA, newOrder, loc, rewriter);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n     rewriter.replaceOp(op, retVal);\n     return success();\n@@ -593,6 +635,49 @@ struct AsyncCommitGroupOpConversion\n   }\n };\n \n+struct AsyncBulkWaitOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::AsyncBulkWaitOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::AsyncBulkWaitOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::AsyncBulkWaitOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    PTXBuilder ptxBuilder;\n+    auto &asyncBulkWaitOp = *ptxBuilder.create<>(\"cp.async.bulk.wait_group\");\n+    auto num = op->getAttrOfType<IntegerAttr>(\"num\").getInt();\n+    asyncBulkWaitOp(ptxBuilder.newConstantOperand(num));\n+\n+    auto ctx = op.getContext();\n+    auto loc = op.getLoc();\n+    auto voidTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, voidTy);\n+\n+    // Safe to remove the op since it doesn't have any return value.\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n+struct AsyncBulkCommitGroupOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<\n+          triton::gpu::AsyncBulkCommitGroupOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::AsyncBulkCommitGroupOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::AsyncBulkCommitGroupOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+\n+    PTXBuilder ptxBuilder;\n+    ptxBuilder.create<>(\"cp.async.bulk.commit_group\")->operator()();\n+    ptxBuilder.launch(rewriter, op.getLoc(), void_ty(op.getContext()));\n+    // Safe to remove the op since it doesn't have any return value.\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+};\n+\n namespace mlir {\n namespace LLVM {\n \n@@ -618,6 +703,7 @@ void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n \n void populateTritonGPUToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n     ModuleAllocation &moduleAllocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit) {\n@@ -626,12 +712,15 @@ void populateTritonGPUToLLVMPatterns(\n                                         benefit);\n   patterns.add<AsyncCommitGroupOpConversion>(typeConverter, benefit);\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n+  patterns.add<AsyncBulkCommitGroupOpConversion>(typeConverter, benefit);\n+  patterns.add<AsyncBulkWaitOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n-\n   patterns.add<ExtractSliceOpConversion>(typeConverter, moduleAllocation,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n   patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n+  patterns.add<GetThreadIdOpConversion>(typeConverter, benefit);\n+  patterns.add<GetClusterCTAIdOpConversion>(typeConverter, benefit);\n   patterns.add<MakeRangeOpConversion>(typeConverter, indexCacheInfo, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n   patterns.add<PrintOpConversion>(typeConverter, benefit);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -8,6 +8,7 @@ using namespace mlir::triton;\n \n void populateTritonGPUToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n     ModuleAllocation &allocation,\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n     PatternBenefit benefit);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 291, "deletions": 92, "changes": 383, "file_content_changes": "@@ -11,16 +11,28 @@\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Target/PTX/TmaMetadata.h\"\n #include <set>\n+\n+#define DEBUG_TYPE \"ttgpu_to_llvm\"\n+\n+constexpr ::llvm::StringLiteral kAttrNumTMALoadDescsName =\n+    \"triton_gpu.num-tma-load\";\n+constexpr ::llvm::StringLiteral kAttrNumTMAStoreDescsName =\n+    \"triton_gpu.num-tma-store\";\n using namespace mlir;\n using namespace mlir::triton;\n \n using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::SharedMemoryObject;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::CTALayoutAttr;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n+using ::mlir::triton::gpu::TMAMetadataTy;\n+\n+typedef DenseMap<Operation *, triton::MakeTensorPtrOp> TensorPtrMapT;\n \n namespace mlir {\n namespace LLVM {\n@@ -141,36 +153,39 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<triton::FuncOp> {\n   }\n };\n \n-using IndexCacheKeyT = std::pair<Attribute, RankedTensorType>;\n+struct IndexCacheKeyT {\n+  Attribute layout;\n+  RankedTensorType type;\n+  bool withCTAOffset;\n+};\n \n struct CacheKeyDenseMapInfo {\n   static IndexCacheKeyT getEmptyKey() {\n     auto *pointer = llvm::DenseMapInfo<void *>::getEmptyKey();\n-    return std::make_pair(\n-        mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n-        RankedTensorType{});\n+    return {mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n+            RankedTensorType{}, true};\n   }\n   static IndexCacheKeyT getTombstoneKey() {\n     auto *pointer = llvm::DenseMapInfo<void *>::getTombstoneKey();\n     auto tombstone = llvm::DenseMapInfo<RankedTensorType>::getTombstoneKey();\n-    return std::make_pair(\n-        mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n-        tombstone);\n+    return {mlir::Attribute(static_cast<mlir::Attribute::ImplType *>(pointer)),\n+            tombstone, true};\n   }\n   static unsigned getHashValue(IndexCacheKeyT key) {\n-    auto shape = key.second.getShape();\n-    return llvm::hash_combine(mlir::hash_value(key.first),\n-                              mlir::hash_value(key.second));\n+    return llvm::hash_combine(mlir::hash_value(key.layout),\n+                              mlir::hash_value(key.type),\n+                              llvm::hash_value(key.withCTAOffset));\n   }\n   static bool isEqual(IndexCacheKeyT LHS, IndexCacheKeyT RHS) {\n-    return LHS == RHS;\n+    return LHS.layout == RHS.layout && LHS.type == RHS.type &&\n+           LHS.withCTAOffset == RHS.withCTAOffset;\n   }\n };\n \n class ConvertTritonGPUOpToLLVMPatternBase {\n public:\n   // Two levels of value cache in emitting indices calculation:\n-  // Key: pair<layout, shape>\n+  // Key: {layout, shape, withCTAOffset}\n   struct IndexCacheInfo {\n     DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n         *baseIndexCache;\n@@ -198,6 +213,12 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       : converter(&typeConverter), allocation(&allocation),\n         indexCacheInfo(indexCacheInfo) {}\n \n+  explicit ConvertTritonGPUOpToLLVMPatternBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n+      TMAMetadataTy *tmaMetadata)\n+      : converter(&typeConverter), allocation(&allocation),\n+        tmaMetadata(tmaMetadata) {}\n+\n   TritonGPUToLLVMTypeConverter *getTypeConverter() const { return converter; }\n \n   static Value\n@@ -223,6 +244,26 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return rewriter.create<arith::IndexCastOp>(loc, i32_ty, tid);\n   }\n \n+  static Value getSRegValue(OpBuilder &b, Location loc,\n+                            const std::string &sRegStr) {\n+    PTXBuilder builder;\n+    auto &mov = builder.create(\"mov\")->o(\"u32\");\n+    auto *destOpr = builder.newOperand(\"=r\");\n+    auto *sRegOpr = builder.newConstantOperand(sRegStr);\n+    mov(destOpr, sRegOpr);\n+    Value val = builder.launch(b, loc, b.getIntegerType(32), false);\n+\n+    auto cast = b.create<UnrealizedConversionCastOp>(\n+        loc, TypeRange{b.getIntegerType(32)}, ValueRange{val});\n+    return cast.getResult(0);\n+  }\n+\n+  Value getClusterCTAId(ConversionPatternRewriter &rewriter,\n+                        Location loc) const {\n+    return rewriter.create<triton::nvgpu::ClusterCTAIdOp>(\n+        loc, rewriter.getI32Type());\n+  }\n+\n   // -----------------------------------------------------------------------\n   // Shared memory utilities\n   // -----------------------------------------------------------------------\n@@ -259,7 +300,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     // for all indices (row, col) of `srcEncoding` such that idx % inVec = 0,\n     // the pointer: ptr[(row, col)] = base + (rowOff * strides[ord[1]] +\n     // colOff) where :\n-    //   compute phase = (row // perPhase) % maxPhase\n+    //   phase = (row // perPhase) % maxPhase\n     //   rowOff = row\n     //   colOff = colOffSwizzled + colOffOrdered\n     //     colOffSwizzled = ((col // outVec) ^ phase) * outVec\n@@ -280,60 +321,89 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     // then (x + y) XOR z = 0byyyyxxxx XOR 0b00000zzzz = (x XOR z) + y\n     // This means that we can use some immediate offsets for shared memory\n     // operations.\n-    auto dstPtrTy = ptr_ty(resElemTy, 3);\n+    auto dstPtrTy = ptr_ty(getTypeConverter()->convertType(resElemTy), 3);\n     auto dstOffset = dot(rewriter, loc, offsetVals, smemObj.strides);\n     Value dstPtrBase = gep(dstPtrTy, smemObj.base, dstOffset);\n \n     auto srcEncoding = srcTy.getEncoding();\n     auto srcShape = srcTy.getShape();\n+    auto srcShapePerCTA = triton::gpu::getShapePerCTA(srcTy);\n     unsigned numElems = triton::gpu::getTotalElemsPerThread(srcTy);\n     // swizzling params as described in TritonGPUAttrDefs.td\n     unsigned outVec = resSharedLayout.getVec();\n     unsigned perPhase = resSharedLayout.getPerPhase();\n     unsigned maxPhase = resSharedLayout.getMaxPhase();\n-    // order\n+    // Order\n     auto inOrder = triton::gpu::getOrder(srcEncoding);\n     auto outOrder = triton::gpu::getOrder(resSharedLayout);\n-    // tensor indices held by the current thread, as LLVM values\n-    auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcTy);\n-    // return values\n+    // Tensor indices held by the current thread, as LLVM values\n+    auto srcIndices = emitIndices(loc, rewriter, srcEncoding, srcTy, false);\n+    // Swizzling with leading offsets (e.g. Hopper GMMA)\n+    unsigned swizzlingByteWidth = 0;\n+    if (resSharedLayout.getHasLeadingOffset()) {\n+      if (perPhase == 4 && maxPhase == 2)\n+        swizzlingByteWidth = 32;\n+      else if (perPhase == 2 && maxPhase == 4)\n+        swizzlingByteWidth = 64;\n+      else if (perPhase == 1 && maxPhase == 8)\n+        swizzlingByteWidth = 128;\n+      else\n+        llvm::report_fatal_error(\"Unsupported shared layout.\");\n+    }\n+    unsigned numElemsPerSwizzlingRow =\n+        swizzlingByteWidth * 8 / resElemTy.getIntOrFloatBitWidth();\n+    Value numElemsPerSwizzlingRowVal = i32_val(numElemsPerSwizzlingRow);\n+    unsigned leadingDimOffset =\n+        numElemsPerSwizzlingRow * srcShapePerCTA[outOrder[1]];\n+    Value leadingDimOffsetVal = i32_val(leadingDimOffset);\n+    // Return values\n     DenseMap<unsigned, Value> ret;\n     // cache for non-immediate offsets\n     DenseMap<unsigned, Value> cacheCol, cacheRow;\n     unsigned minVec = std::min(outVec, inVec);\n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n-      // extract multi dimensional index for current element\n+      Value offset = i32_val(0);\n+      // Extract multi dimensional index for current element\n       auto idx = srcIndices[elemIdx];\n       Value idxCol = idx[outOrder[0]]; // contiguous dimension\n       Value idxRow = idx[outOrder[1]]; // discontiguous dimension\n       Value strideCol = srcStrides[outOrder[0]];\n       Value strideRow = srcStrides[outOrder[1]];\n+      // compute phase = (row // perPhase) % maxPhase\n+      Value phase = urem(udiv(idxRow, i32_val(perPhase)), i32_val(maxPhase));\n       // extract dynamic/static offset for immediate offsetting\n       unsigned immedateOffCol = 0;\n-      if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxCol.getDefiningOp()))\n-        if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n-                add.getRhs().getDefiningOp())) {\n-          unsigned cst =\n-              _cst.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n-          unsigned key = cst % (outVec * maxPhase);\n-          cacheCol.insert({key, idxCol});\n-          idxCol = cacheCol[key];\n-          immedateOffCol = cst / (outVec * maxPhase) * (outVec * maxPhase);\n-        }\n-      // extract dynamic/static offset for immediate offsetting\n       unsigned immedateOffRow = 0;\n-      if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxRow.getDefiningOp()))\n-        if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n-                add.getRhs().getDefiningOp())) {\n-          unsigned cst =\n-              _cst.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n-          unsigned key = cst % (perPhase * maxPhase);\n-          cacheRow.insert({key, idxRow});\n-          idxRow = cacheRow[key];\n-          immedateOffRow = cst / (perPhase * maxPhase) * (perPhase * maxPhase);\n-        }\n-      // compute phase = (row // perPhase) % maxPhase\n-      Value phase = urem(udiv(idxRow, i32_val(perPhase)), i32_val(maxPhase));\n+      if (leadingDimOffset) {\n+        // hopper\n+        offset =\n+            mul(udiv(idxCol, numElemsPerSwizzlingRowVal), leadingDimOffsetVal);\n+        // Shrink by swizzling blocks\n+        idxCol = urem(idxCol, numElemsPerSwizzlingRowVal);\n+        strideRow = numElemsPerSwizzlingRowVal;\n+      } else {\n+        if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxCol.getDefiningOp()))\n+          if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n+                  add.getRhs().getDefiningOp())) {\n+            unsigned cst =\n+                _cst.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n+            unsigned key = cst % (outVec * maxPhase);\n+            cacheCol.insert({key, idxCol});\n+            idxCol = cacheCol[key];\n+            immedateOffCol = cst / (outVec * maxPhase) * (outVec * maxPhase);\n+          }\n+        if (auto add = dyn_cast_or_null<LLVM::AddOp>(idxRow.getDefiningOp()))\n+          if (auto _cst = dyn_cast_or_null<LLVM::ConstantOp>(\n+                  add.getRhs().getDefiningOp())) {\n+            unsigned cst =\n+                _cst.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n+            unsigned key = cst % (perPhase * maxPhase);\n+            cacheRow.insert({key, idxRow});\n+            idxRow = cacheRow[key];\n+            immedateOffRow =\n+                cst / (perPhase * maxPhase) * (perPhase * maxPhase);\n+          }\n+      }\n       // row offset is simply row index\n       Value rowOff = mul(idxRow, strideRow);\n       // because swizzling happens at a granularity of outVec, we need to\n@@ -347,7 +417,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       colOffOrdered = mul(colOffOrdered, i32_val(minVec));\n       Value colOff = add(colOffSwizzled, colOffOrdered);\n       // compute non-immediate offset\n-      Value offset = add(rowOff, mul(colOff, strideCol));\n+      offset = add(offset, add(rowOff, mul(colOff, strideCol)));\n       Value currPtr = gep(dstPtrTy, dstPtrBase, offset);\n       // compute immediate offset\n       Value immedateOff =\n@@ -477,7 +547,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       auto threadsPerWarp = triton::gpu::getThreadsPerWarp(layout);\n       auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n       auto order = triton::gpu::getOrder(layout);\n-      auto shapePerCTA = triton::gpu::getShapePerCTA(layout, shape);\n+      auto shapePerCTATile = triton::gpu::getShapePerCTATile(layout, shape);\n       Value warpSize = i32_val(32);\n       Value laneId = urem(tid, warpSize);\n       Value warpId = udiv(tid, warpSize);\n@@ -487,7 +557,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n           delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n       for (unsigned dim = 0; dim < rank; ++dim) {\n         // if there is no data replication across threads on this dimension\n-        if (shape[dim] >= shapePerCTA[dim])\n+        if (shape[dim] >= shapePerCTATile[dim])\n           continue;\n         // Otherwise, we need to mask threads that will replicate data on this\n         // dimension. Calculate the thread index on this dimension for the CTA\n@@ -535,13 +605,48 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // Get offsets / indices for any layout\n   // -----------------------------------------------------------------------\n \n+  SmallVector<Value> emitCTAOffsetForLayout(Location loc,\n+                                            ConversionPatternRewriter &rewriter,\n+                                            Attribute layout,\n+                                            ArrayRef<int64_t> shape) const {\n+    unsigned rank = shape.size();\n+    SmallVector<unsigned> CTAsPerCGA = triton::gpu::getCTAsPerCGA(layout);\n+    SmallVector<unsigned> CTASplitNum = triton::gpu::getCTASplitNum(layout);\n+    SmallVector<unsigned> CTAOrder = triton::gpu::getCTAOrder(layout);\n+    SmallVector<int64_t> shapePerCTA =\n+        triton::gpu::getShapePerCTA(CTASplitNum, shape);\n+\n+    // Delinearize clusterCTAId\n+    Value clusterCTAId = getClusterCTAId(rewriter, loc);\n+    SmallVector<Value> multiDimClusterCTAId =\n+        delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n+\n+    // CTA Wrapping\n+    for (unsigned i = 0; i < rank; ++i) {\n+      // This wrapping rule must be consistent with getShapePerCTA\n+      unsigned splitNum = std::min<unsigned>(shape[i], CTASplitNum[i]);\n+      multiDimClusterCTAId[i] =\n+          urem(multiDimClusterCTAId[i], i32_val(splitNum));\n+    }\n+\n+    SmallVector<Value> CTAOffset(rank);\n+    for (unsigned i = 0; i < rank; ++i)\n+      CTAOffset[i] = mul(multiDimClusterCTAId[i], i32_val(shapePerCTA[i]));\n+\n+    return CTAOffset;\n+  }\n+\n   SmallVector<Value> emitBaseIndexForLayout(Location loc,\n                                             ConversionPatternRewriter &rewriter,\n                                             Attribute layout,\n-                                            RankedTensorType type) const {\n-    IndexCacheKeyT key = std::make_pair(layout, type);\n+                                            RankedTensorType type,\n+                                            bool withCTAOffset) const {\n+    auto shape = type.getShape();\n+    IndexCacheKeyT key{layout, type, withCTAOffset};\n     auto cache = indexCacheInfo.baseIndexCache;\n     auto insertPt = indexCacheInfo.indexInsertPoint;\n+\n+    SmallVector<Value> baseIndex;\n     if (cache && cache->count(key) > 0) {\n       return cache->lookup(key);\n     } else {\n@@ -550,23 +655,34 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         restoreInsertionPointIfSet(insertPt, rewriter);\n       SmallVector<Value> result;\n       if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        result =\n-            emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, type);\n+        result = emitBaseIndexWithinCTAForBlockedLayout(loc, rewriter,\n+                                                        blockedLayout, type);\n       } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n         if (mmaLayout.isVolta())\n-          result = emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, type);\n-        if (mmaLayout.isAmpere())\n-          result = emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, type);\n+          result = emitBaseIndexWithinCTAForMmaLayoutV1(loc, rewriter,\n+                                                        mmaLayout, type);\n+        if (mmaLayout.isAmpere() || mmaLayout.isHopper())\n+          result = emitBaseIndexWithinCTAForMmaLayoutV2V3(loc, rewriter,\n+                                                          mmaLayout, type);\n       } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n         auto parentLayout = sliceLayout.getParent();\n         auto parentShape = sliceLayout.paddedShape(type.getShape());\n         RankedTensorType parentTy = RankedTensorType::get(\n             parentShape, type.getElementType(), parentLayout);\n-        result = emitBaseIndexForLayout(loc, rewriter, parentLayout, parentTy);\n+        result = emitBaseIndexForLayout(loc, rewriter, parentLayout, parentTy,\n+                                        withCTAOffset);\n         result.erase(result.begin() + sliceLayout.getDim());\n+        // CTAOffset has been added in emitBaseIndexForLayout of parentLayout\n+        return result;\n       } else {\n         llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n       }\n+      if (withCTAOffset) {\n+        auto CTAOffset = emitCTAOffsetForLayout(loc, rewriter, layout, shape);\n+        assert(CTAOffset.size() == result.size() && \"Rank mismatch\");\n+        for (unsigned k = 0; k < result.size(); ++k)\n+          result[k] = add(result[k], CTAOffset[k]);\n+      }\n       if (cache) {\n         cache->insert(std::make_pair(key, result));\n         *insertPt = rewriter.saveInsertionPoint();\n@@ -584,6 +700,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         return emitOffsetForMmaLayoutV1(mmaLayout, type);\n       if (mmaLayout.isAmpere())\n         return emitOffsetForMmaLayoutV2(mmaLayout, type);\n+      if (mmaLayout.isHopper())\n+        return emitOffsetForMmaLayoutV3(mmaLayout, type);\n     }\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>())\n       return emitOffsetForSliceLayout(sliceLayout, type);\n@@ -593,11 +711,10 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n   // Emit indices\n   // -----------------------------------------------------------------------\n-  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n-                                              ConversionPatternRewriter &b,\n-                                              Attribute layout,\n-                                              RankedTensorType type) const {\n-    IndexCacheKeyT key(layout, type);\n+  SmallVector<SmallVector<Value>>\n+  emitIndices(Location loc, ConversionPatternRewriter &b, Attribute layout,\n+              RankedTensorType type, bool withCTAOffset = true) const {\n+    IndexCacheKeyT key{layout, type, withCTAOffset};\n     auto cache = indexCacheInfo.indexCache;\n     auto insertPt = indexCacheInfo.indexInsertPoint;\n     if (cache && cache->count(key) > 0) {\n@@ -608,11 +725,14 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         restoreInsertionPointIfSet(insertPt, b);\n       SmallVector<SmallVector<Value>> result;\n       if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        result = emitIndicesForDistributedLayout(loc, b, blocked, type);\n+        result = emitIndicesForDistributedLayout(loc, b, blocked, type,\n+                                                 withCTAOffset);\n       } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n-        result = emitIndicesForDistributedLayout(loc, b, mma, type);\n+        result =\n+            emitIndicesForDistributedLayout(loc, b, mma, type, withCTAOffset);\n       } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n-        result = emitIndicesForDistributedLayout(loc, b, slice, type);\n+        result =\n+            emitIndicesForDistributedLayout(loc, b, slice, type, withCTAOffset);\n       } else {\n         llvm_unreachable(\n             \"emitIndices for layouts other than blocked & slice not \"\n@@ -642,19 +762,20 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // Blocked layout indices\n   // -----------------------------------------------------------------------\n \n-  // Get an index-base for each dimension for a \\param blocked_layout.\n-  SmallVector<Value> emitBaseIndexForBlockedLayout(\n+  // Get an index-base for each dimension for a \\param blockedLayout.\n+  SmallVector<Value> emitBaseIndexWithinCTAForBlockedLayout(\n       Location loc, ConversionPatternRewriter &rewriter,\n-      const BlockedEncodingAttr &blocked_layout, RankedTensorType type) const {\n+      const BlockedEncodingAttr &blockedLayout, RankedTensorType type) const {\n     auto shape = type.getShape();\n     Value threadId = getThreadId(rewriter, loc);\n     Value warpSize = i32_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n-    auto sizePerThread = blocked_layout.getSizePerThread();\n-    auto threadsPerWarp = blocked_layout.getThreadsPerWarp();\n-    auto warpsPerCTA = blocked_layout.getWarpsPerCTA();\n-    auto order = blocked_layout.getOrder();\n+    auto sizePerThread = blockedLayout.getSizePerThread();\n+    auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n+    auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+    auto order = blockedLayout.getOrder();\n+    auto shapePerCTA = triton::gpu::getShapePerCTA(blockedLayout, shape);\n     unsigned rank = shape.size();\n \n     // delinearize threadId to get the base index\n@@ -666,10 +787,10 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     SmallVector<Value> multiDimBase(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n       // Wrap around multiDimWarpId/multiDimThreadId in case\n-      // shape[k] > shapePerCTA[k]\n+      // shapePerCTATile[k] > shapePerCTA[k]\n       auto maxWarps =\n-          ceil<unsigned>(shape[k], sizePerThread[k] * threadsPerWarp[k]);\n-      auto maxThreads = ceil<unsigned>(shape[k], sizePerThread[k]);\n+          ceil<unsigned>(shapePerCTA[k], sizePerThread[k] * threadsPerWarp[k]);\n+      auto maxThreads = ceil<unsigned>(shapePerCTA[k], sizePerThread[k]);\n       multiDimWarpId[k] = urem(multiDimWarpId[k], i32_val(maxWarps));\n       multiDimThreadId[k] = urem(multiDimThreadId[k], i32_val(maxThreads));\n       // multiDimBase[k] = (multiDimThreadId[k] +\n@@ -692,16 +813,17 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n     auto order = blockedLayout.getOrder();\n+    auto shapePerCTATile = getShapePerCTATile(blockedLayout);\n+    auto shapePerCTA = triton::gpu::getShapePerCTA(blockedLayout, shape);\n \n     unsigned rank = shape.size();\n-    SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n     SmallVector<unsigned> tilesPerDim(rank);\n     for (unsigned k = 0; k < rank; ++k)\n-      tilesPerDim[k] = ceil<unsigned>(shape[k], shapePerCTA[k]);\n+      tilesPerDim[k] = ceil<unsigned>(shapePerCTA[k], shapePerCTATile[k]);\n \n     SmallVector<SmallVector<unsigned>> offset(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      // 1 block in minimum if shape[k] is less than shapePerCTA[k]\n+      // 1 CTA tile in minimum if shapePerCTA[k] is less than shapePerCTATile[k]\n       for (unsigned blockOffset = 0; blockOffset < tilesPerDim[k];\n            ++blockOffset)\n         for (unsigned warpOffset = 0; warpOffset < warpsPerCTA[k]; ++warpOffset)\n@@ -741,12 +863,10 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // Mma layout indices\n   // -----------------------------------------------------------------------\n \n-  SmallVector<Value>\n-  emitBaseIndexForMmaLayoutV1(Location loc, ConversionPatternRewriter &rewriter,\n-                              const MmaEncodingAttr &mmaLayout,\n-                              RankedTensorType type) const {\n+  SmallVector<Value> emitBaseIndexWithinCTAForMmaLayoutV1(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const MmaEncodingAttr &mmaLayout, RankedTensorType type) const {\n     auto shape = type.getShape();\n-\n     auto wpt = mmaLayout.getWarpsPerCTA();\n     static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n     auto [isARow, isBRow, isAVec4, isBVec4, _] =\n@@ -879,9 +999,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n     SmallVector<Value> multiDimWarpId =\n         delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n-    unsigned lastAxis = order[order.size() - 1];\n-    multiDimWarpId[lastAxis] =\n-        urem(multiDimWarpId[lastAxis], i32_val(warpsPerCTA[lastAxis]));\n     multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n     multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n     Value offWarp0 = mul(multiDimWarpId[0], i32_val(16));\n@@ -897,10 +1014,13 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   emitOffsetForMmaLayoutV2(const MmaEncodingAttr &mmaLayout,\n                            RankedTensorType type) const {\n     auto shape = type.getShape();\n+    auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n     SmallVector<SmallVector<unsigned>> ret;\n \n-    for (unsigned i = 0; i < shape[0]; i += getShapePerCTA(mmaLayout)[0]) {\n-      for (unsigned j = 0; j < shape[1]; j += getShapePerCTA(mmaLayout)[1]) {\n+    for (unsigned i = 0; i < shapePerCTA[0];\n+         i += getShapePerCTATile(mmaLayout)[0]) {\n+      for (unsigned j = 0; j < shapePerCTA[1];\n+           j += getShapePerCTATile(mmaLayout)[1]) {\n         ret.push_back({i, j});\n         ret.push_back({i, j + 1});\n         ret.push_back({i + 8, j});\n@@ -910,17 +1030,88 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return ret;\n   }\n \n+  SmallVector<Value> emitBaseIndexWithinCTAForMmaLayoutV2V3(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const MmaEncodingAttr &mmaLayout, RankedTensorType type) const {\n+    auto shape = type.getShape();\n+    auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+    assert(_warpsPerCTA.size() == 2);\n+    auto order = triton::gpu::getOrder(mmaLayout);\n+    ArrayRef<unsigned int> instrShape = mmaLayout.getInstrShape();\n+    SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n+                                      i32_val(_warpsPerCTA[1])};\n+    auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n+\n+    Value threadId = getThreadId(rewriter, loc);\n+    Value warpSize = i32_val(32);\n+    Value laneId = urem(threadId, warpSize);\n+    Value warpId = udiv(threadId, warpSize);\n+\n+    uint32_t repM = (_warpsPerCTA[0] * instrShape[0]) / shapePerCTA[0];\n+    uint32_t repN = (_warpsPerCTA[1] * instrShape[1]) / shapePerCTA[1];\n+\n+    uint32_t warpsM;\n+    if (repM > 1)\n+      warpsM = _warpsPerCTA[0] / repM;\n+    else\n+      warpsM = shape[0] / instrShape[0];\n+\n+    uint32_t warpsN;\n+    if (repN > 1)\n+      warpsN = _warpsPerCTA[1] / repN;\n+    else\n+      warpsN = shape[1] / instrShape[1];\n+\n+    SmallVector<Value> multiDimWarpId =\n+        delinearize(rewriter, loc, warpId, _warpsPerCTA, order);\n+    Value warpId0 = urem(multiDimWarpId[0], i32_val(warpsM));\n+    Value warpId1 = urem(multiDimWarpId[1], i32_val(warpsN));\n+\n+    Value offWarp0 = mul(warpId0, i32_val(instrShape[0]));\n+    Value offWarp1 = mul(warpId1, i32_val(instrShape[1]));\n+\n+    SmallVector<Value> multiDimBase(2);\n+    multiDimBase[0] = add(udiv(laneId, i32_val(4)), offWarp0);\n+    multiDimBase[1] = add(mul(i32_val(2), urem(laneId, i32_val(4))), offWarp1);\n+    return multiDimBase;\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForMmaLayoutV3(const MmaEncodingAttr &mmaLayout,\n+                           RankedTensorType type) const {\n+    auto shape = type.getShape();\n+    auto shapePerCTA = getShapePerCTA(mmaLayout, shape);\n+    SmallVector<SmallVector<unsigned>> ret;\n+    ArrayRef<unsigned int> instrShape = mmaLayout.getInstrShape();\n+\n+    for (unsigned i = 0; i < shapePerCTA[0];\n+         i += getShapePerCTATile(mmaLayout)[0]) {\n+      for (unsigned j = 0; j < shapePerCTA[1];\n+           j += getShapePerCTATile(mmaLayout)[1]) {\n+        for (unsigned k = 0; k < instrShape[1]; k += 8) {\n+          ret.push_back({i, j + k});\n+          ret.push_back({i, j + k + 1});\n+          ret.push_back({i + 8, j + k});\n+          ret.push_back({i + 8, j + k + 1});\n+        }\n+      }\n+    }\n+    return ret;\n+  }\n+\n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n   SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n       Location loc, ConversionPatternRewriter &rewriter, Attribute layout,\n-      RankedTensorType type) const {\n+      RankedTensorType type, bool withCTAOffset) const {\n     // step 1, delinearize threadId to get the base index\n-    auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, type);\n+    auto multiDimBase =\n+        emitBaseIndexForLayout(loc, rewriter, layout, type, withCTAOffset);\n     // step 2, get offset of each element\n     auto offset = emitOffsetForLayout(layout, type);\n-    // step 3, add offset to base, and reorder the sequence of indices to\n-    // guarantee that elems in the same sizePerThread are adjacent in order\n+    // step 3, add offset to base, and reorder the sequence\n+    // of indices to guarantee that elems in the same\n+    // sizePerThread are adjacent in order\n     auto shape = type.getShape();\n     unsigned rank = shape.size();\n     unsigned elemsPerThread = offset.size();\n@@ -961,6 +1152,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   TritonGPUToLLVMTypeConverter *converter;\n   ModuleAllocation *allocation;\n   IndexCacheInfo indexCacheInfo;\n+  mlir::triton::gpu::TMAMetadataTy *tmaMetadata;\n };\n \n template <typename SourceOp>\n@@ -975,6 +1167,12 @@ class ConvertTritonGPUOpToLLVMPattern\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n         ConvertTritonGPUOpToLLVMPatternBase(typeConverter) {}\n \n+  explicit ConvertTritonGPUOpToLLVMPattern(\n+      TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n+      PatternBenefit benefit = 1)\n+      : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation) {}\n+\n   explicit ConvertTritonGPUOpToLLVMPattern(\n       TritonGPUToLLVMTypeConverter &typeConverter,\n       IndexCacheInfo indexCacheInfo, PatternBenefit benefit = 1)\n@@ -983,16 +1181,17 @@ class ConvertTritonGPUOpToLLVMPattern\n \n   explicit ConvertTritonGPUOpToLLVMPattern(\n       TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n-      PatternBenefit benefit = 1)\n+      IndexCacheInfo indexCacheInfo, PatternBenefit benefit = 1)\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n-        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation) {}\n+        ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation,\n+                                            indexCacheInfo) {}\n \n   explicit ConvertTritonGPUOpToLLVMPattern(\n       TritonGPUToLLVMTypeConverter &typeConverter, ModuleAllocation &allocation,\n-      IndexCacheInfo indexCacheInfo, PatternBenefit benefit = 1)\n+      mlir::triton::gpu::TMAMetadataTy *tmaMetadata, PatternBenefit benefit = 1)\n       : ConvertOpToLLVMPattern<SourceOp>(typeConverter, benefit),\n         ConvertTritonGPUOpToLLVMPatternBase(typeConverter, allocation,\n-                                            indexCacheInfo) {}\n+                                            tmaMetadata) {}\n \n protected:\n   TritonGPUToLLVMTypeConverter *getTypeConverter() const {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 235, "deletions": 29, "changes": 264, "file_content_changes": "@@ -14,27 +14,35 @@\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n #include \"mlir/Dialect/LLVMIR/ROCDLDialect.h\"\n #include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n #include \"triton/Analysis/Membar.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Tools/Sys/GetPlatform.hpp\"\n \n+#include \"BarrierOpToLLVM.h\"\n+#include \"ClusterOpsToLLVM.h\"\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"DotOpToLLVM.h\"\n #include \"ElementwiseOpToLLVM.h\"\n #include \"LoadStoreOpToLLVM.h\"\n #include \"ReduceOpToLLVM.h\"\n+#include \"RegReallocOpToLLVM.h\"\n #include \"ScanOpToLLVM.h\"\n+#include \"TensorPtrOpsToLLVM.h\"\n #include \"TritonGPUToLLVM.h\"\n+#include \"TritonGPUToLLVMBase.h\"\n #include \"TypeConverter.h\"\n #include \"ViewOpToLLVM.h\"\n \n #include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n+namespace ttng = mlir::triton::nvidia_gpu;\n \n #define GEN_PASS_CLASSES\n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n@@ -56,6 +64,30 @@ class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n   }\n };\n \n+class FoldSplatMaskInInsertAsync : public mlir::RewritePattern {\n+\n+public:\n+  FoldSplatMaskInInsertAsync(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(\n+            triton::nvidia_gpu::InsertSliceAsyncV2Op::getOperationName(), 1,\n+            context) {}\n+\n+  LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto insertOp = cast<triton::nvidia_gpu::InsertSliceAsyncV2Op>(op);\n+    if (!insertOp.getMask())\n+      return failure();\n+    auto splatOp = insertOp.getMask().getDefiningOp<triton::SplatOp>();\n+    if (!splatOp)\n+      return failure();\n+    rewriter.updateRootInPlace(insertOp, [&]() {\n+      insertOp.getMaskMutable().assign(splatOp->getOperand(0));\n+    });\n+    return mlir::success();\n+  }\n+};\n+\n struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n   using ConvertOpToLLVMPattern<triton::ReturnOp>::ConvertOpToLLVMPattern;\n \n@@ -146,6 +178,18 @@ struct FuncOpConversion : public FuncOpConversionBase {\n     if (!allocation.isRoot(funcOp))\n       amendedFuncOp = amendFuncOp(funcOp, rewriter);\n \n+    // Collect TMA informations.\n+    unsigned numTMALoad = 0;\n+    funcOp.walk(\n+        [&numTMALoad](triton::nvidia_gpu::InsertSliceAsyncV2Op insertSliceOp) {\n+          numTMALoad++;\n+        });\n+    unsigned numTMAStore = 0;\n+    funcOp.walk([&numTMAStore](triton::nvidia_gpu::StoreAsyncOp storeAsyncOp) {\n+      numTMAStore++;\n+    });\n+    unsigned numTMA = numTMALoad + numTMAStore;\n+\n     auto newFuncOp = convertFuncOpToLLVMFuncOp(amendedFuncOp, rewriter);\n     if (!newFuncOp) {\n       return failure();\n@@ -171,6 +215,30 @@ struct FuncOpConversion : public FuncOpConversionBase {\n     // The call graph is updated by mapping the old function to the new one.\n     allocation.mapFuncOp(funcOp, newFuncOp);\n \n+    // Append arguments to receive TMADesc in global memory in the runtime\n+    auto i8PtrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getI8Type()), 1);\n+    auto numArgs = newFuncOp.getBody().front().getNumArguments();\n+    auto funcTy = newFuncOp.getFunctionType().cast<LLVM::LLVMFunctionType>();\n+    SmallVector<Type> newInputsTy(funcTy.getParams().begin(),\n+                                  funcTy.getParams().end());\n+    for (unsigned i = 0; i < numTMA; ++i) {\n+      newFuncOp.getBody().front().addArgument(i8PtrTy, funcOp.getLoc());\n+      newInputsTy.push_back(i8PtrTy);\n+    }\n+    newFuncOp.setType(\n+        LLVM::LLVMFunctionType::get(funcTy.getReturnType(), newInputsTy));\n+    // required by AxisInfoAnalysis\n+    for (unsigned i = 0; i < numTMA; ++i) {\n+      newFuncOp.setArgAttr(numArgs + i, \"tt.divisibility\",\n+                           rewriter.getIntegerAttr(i32_ty, 1));\n+    }\n+\n+    newFuncOp->setAttr(kAttrNumTMALoadDescsName,\n+                       rewriter.getIntegerAttr(i32_ty, numTMALoad));\n+    newFuncOp->setAttr(kAttrNumTMAStoreDescsName,\n+                       rewriter.getIntegerAttr(i32_ty, numTMAStore));\n+\n     rewriter.eraseOp(funcOp);\n     return success();\n   }\n@@ -247,7 +315,6 @@ struct CallOpConversion : public ConvertOpToLLVMPattern<triton::CallOp> {\n                 this->getTypeConverter()->packFunctionResults(resultTypes)))\n         return nullptr;\n     }\n-\n     auto newCallOp = rewriter.create<LLVM::CallOp>(\n         callOp.getLoc(), packedResult ? TypeRange(packedResult) : TypeRange(),\n         promotedOperands, callOp->getAttrs());\n@@ -288,8 +355,10 @@ class TritonLLVMConversionTarget : public ConversionTarget {\n     } else {\n       addLegalDialect<NVVM::NVVMDialect>();\n     }\n+    addLegalDialect<mlir::triton::nvgpu::NVGPUDialect>();\n     addIllegalDialect<triton::TritonDialect>();\n     addIllegalDialect<triton::gpu::TritonGPUDialect>();\n+    addIllegalDialect<triton::nvidia_gpu::TritonNvidiaGPUDialect>();\n     addIllegalDialect<mlir::gpu::GPUDialect>();\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n@@ -299,8 +368,11 @@ class ConvertTritonGPUToLLVM\n     : public ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {\n \n public:\n-  explicit ConvertTritonGPUToLLVM(int computeCapability, bool isROCM)\n-      : computeCapability(computeCapability), isROCM(isROCM) {}\n+  explicit ConvertTritonGPUToLLVM(int computeCapability,\n+                                  mlir::triton::gpu::TMAMetadataTy *tmaMetadata,\n+                                  bool isROCM)\n+      : computeCapability(computeCapability), tmaMetadata(tmaMetadata),\n+        isROCM(isROCM) {}\n \n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n@@ -310,19 +382,54 @@ class ConvertTritonGPUToLLVM\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n     TritonLLVMConversionTarget target(*context, isROCM);\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+    int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(mod);\n     int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n \n     // Preprocess\n     decomposeFp8e4b15Convert(mod);\n-    decomposeMmaToDotOperand(mod, numWarps, threadsPerWarp);\n+    decomposeMmaToDotOperand(mod, numWarps, threadsPerWarp, numCTAs);\n     decomposeBlockedToDotOperand(mod);\n     decomposeInsertSliceAsyncOp(mod);\n+    decomposeMixedModeDotOp(mod);\n \n     // Allocate shared memory and set barrier\n     ModuleAllocation allocation(mod);\n     ModuleMembarAnalysis membarPass(&allocation);\n     membarPass.run();\n \n+    /* Get tensorPtrMap before conversion */\n+    TensorPtrMapT tensorPtrMap;\n+    mod.walk([&tensorPtrMap](\n+                 mlir::triton::nvidia_gpu::InsertSliceAsyncV2Op insertOp) {\n+      auto src = insertOp.getSrc();\n+      auto ptrTy = src.getType().dyn_cast<triton::PointerType>();\n+      if (ptrTy && ptrTy.getPointeeType().isa<RankedTensorType>()) {\n+        auto makeTensorPtrOp = getMakeTensorPtrOp(insertOp.getSrc());\n+        tensorPtrMap[insertOp.getOperation()] = makeTensorPtrOp;\n+      }\n+    });\n+\n+    mod.walk([&tensorPtrMap](mlir::triton::nvidia_gpu::StoreAsyncOp storeOp) {\n+      auto dst = storeOp.getDst();\n+      auto ptrTy = dst.getType().dyn_cast<triton::PointerType>();\n+      if (ptrTy && ptrTy.getPointeeType().isa<RankedTensorType>()) {\n+        auto makeTensorPtrOp = getMakeTensorPtrOp(storeOp.getDst());\n+        tensorPtrMap[storeOp.getOperation()] = makeTensorPtrOp;\n+      }\n+    });\n+\n+    // Hack: cleanup\n+    {\n+      RewritePatternSet patterns(context);\n+      patterns.add<FoldSplatMaskInInsertAsync>(context);\n+      SmallVector<Operation *> insertSlices;\n+      mod.walk([&insertSlices](triton::nvidia_gpu::InsertSliceAsyncV2Op op) {\n+        insertSlices.push_back(op);\n+      });\n+      if (applyOpPatternsAndFold(insertSlices, std::move(patterns)).failed())\n+        signalPassFailure();\n+    }\n+\n     // Lower functions\n     {\n       mlir::LowerToLLVMOptions option(context);\n@@ -358,31 +465,71 @@ class ConvertTritonGPUToLLVM\n     }\n \n     ModuleAxisInfoAnalysis axisInfoAnalysis(mod);\n-    // Rewrite ops\n-    RewritePatternSet patterns(context);\n-    // TritonGPU lowering patterns\n+\n+    // Emit logics to get threadId/blockIds/linearized clusterCTAId etc. and\n+    // cache the values. The reason to do it here is that cluster_ctaid is\n+    // currently implemented via inline asm, and thus cannot be CSEed.\n+    // clusterCTAId will be emitted only when numCTAs is larger than 1, and\n+    // other values will be DCEed if not used hereafter.\n+    bool isWarpSpecialization =\n+        ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod);\n     OpBuilder::InsertPoint indexInsertPoint;\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo indexCacheInfo{\n         &baseIndexCache, &indexCache, &indexInsertPoint};\n     // TODO: enable index cache if there are multiple functions\n     if (axisInfoAnalysis.getNumFunctions() > 1) {\n       indexCacheInfo = {nullptr, nullptr, nullptr};\n     }\n-    populateTritonGPUToLLVMPatterns(typeConverter, patterns, allocation,\n-                                    indexCacheInfo, /*benefit=*/1);\n-    populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, allocation,\n-                                          indexCacheInfo, /*benefit=*/1);\n-    populateDotOpToLLVMPatterns(typeConverter, patterns, allocation,\n-                                /*benefit=*/1);\n-    populateElementwiseOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n-    populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, axisInfoAnalysis,\n-                                      allocation, indexCacheInfo,\n-                                      /*benefit=*/1);\n-    populateReduceOpToLLVMPatterns(typeConverter, patterns, allocation,\n-                                   indexCacheInfo, /*benefit=*/1);\n-    populateScanOpToLLVMPatterns(typeConverter, patterns, allocation,\n-                                 indexCacheInfo, /*benefit=*/1);\n-    populateViewOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n+\n+    // tmaMetadata is absent in a triton-opt unit test, in this case, create a\n+    // local one and dump it after this pass is done.\n+    mlir::triton::gpu::TMAMetadataTy tmaMetaDataDebug;\n+    if (tmaMetadata == nullptr)\n+      tmaMetadata = &tmaMetaDataDebug;\n+\n+    RewritePatternSet patterns(context);\n+\n+    auto populatePatterns1 = [&](auto populateFunc) {\n+      populateFunc(typeConverter, patterns, numWarps, axisInfoAnalysis,\n+                   allocation, indexCacheInfo,\n+                   /*benefit*/ 10);\n+    };\n+\n+    auto populatePatterns2 = [&](auto populateFunc) {\n+      populateFunc(typeConverter, patterns, numWarps, axisInfoAnalysis,\n+                   allocation, /*benefit*/ 10);\n+    };\n+\n+    auto populatePatterns3 = [&](auto populateFunc) {\n+      populateFunc(typeConverter, patterns, numWarps, axisInfoAnalysis,\n+                   allocation, indexCacheInfo, tmaMetadata, &tensorPtrMap,\n+                   /*benefit*/ 10);\n+    };\n+\n+    auto populatePatterns4 = [&](auto populateFunc) {\n+      populateFunc(typeConverter, patterns, numWarps, axisInfoAnalysis,\n+                   allocation, indexCacheInfo, computeCapability,\n+                   /*benefit*/ 10);\n+    };\n+\n+    populatePatterns1(populateTritonGPUToLLVMPatterns);\n+    populatePatterns1(populateConvertLayoutOpToLLVMPatterns);\n+    populatePatterns2(populateDotOpToLLVMPatterns);\n+    populatePatterns2(populateElementwiseOpToLLVMPatterns);\n+    populatePatterns3(populateLoadStoreOpToLLVMPatterns);\n+    populatePatterns4(populateReduceOpToLLVMPatterns);\n+    populatePatterns1(populateScanOpToLLVMPatterns);\n+    populatePatterns2(populateViewOpToLLVMPatterns);\n+    populatePatterns2(populateBarrierOpToLLVMPatterns);\n+    populatePatterns2(populateTensorPtrOpsToLLVMPatterns);\n+    populatePatterns2(populateClusterOpsToLLVMPatterns);\n+    populatePatterns2(populateRegReallocOpToLLVMPatterns);\n+\n+    // TODO(thomas): this should probably be done in a separate step to not\n+    // interfere with our own lowering of arith ops. Add arith/math's patterns\n+    // to help convert scalar expression to LLVM.\n+    mlir::arith::populateArithToLLVMConversionPatterns(typeConverter, patterns);\n+    mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n \n     // Native lowering patterns\n     if (isROCM) {\n@@ -396,10 +543,18 @@ class ConvertTritonGPUToLLVM\n                                                           patterns);\n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();\n+\n+    // Fold CTAId when there is only 1 CTA.\n+    if (numCTAs == 1) {\n+      mod.walk([](triton::nvgpu::ClusterCTAIdOp id) {\n+        OpBuilder b(id);\n+        Value zero = LLVM::createConstantI32(id->getLoc(), b, 0);\n+        id.replaceAllUsesWith(zero);\n+      });\n+    }\n   }\n \n private:\n-  using IndexCacheKeyT = std::pair<Attribute, RankedTensorType>;\n   DenseMap<IndexCacheKeyT, SmallVector<Value>, CacheKeyDenseMapInfo>\n       baseIndexCache;\n   DenseMap<IndexCacheKeyT, SmallVector<SmallVector<Value>>,\n@@ -408,6 +563,7 @@ class ConvertTritonGPUToLLVM\n \n   int computeCapability{};\n   bool isROCM{};\n+  mlir::triton::gpu::TMAMetadataTy *tmaMetadata;\n \n   void initSharedMemory(ModuleAllocation &allocation,\n                         TritonGPUToLLVMTypeConverter &typeConverter) {\n@@ -470,8 +626,8 @@ class ConvertTritonGPUToLLVM\n     });\n   }\n \n-  void decomposeMmaToDotOperand(ModuleOp mod, int numWarps,\n-                                int threadsPerWarp) const {\n+  void decomposeMmaToDotOperand(ModuleOp mod, int numWarps, int threadsPerWarp,\n+                                int numCTAs) const {\n     // Replace `mma -> dot_op` with `mma -> blocked -> dot_op`\n     // unless certain conditions are met\n     mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n@@ -487,7 +643,7 @@ class ConvertTritonGPUToLLVM\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::BlockedEncodingAttr::get(\n                 mod.getContext(), srcType.getShape(), getSizePerThread(srcMma),\n-                getOrder(srcMma), numWarps, threadsPerWarp));\n+                getOrder(srcMma), numWarps, threadsPerWarp, numCTAs));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n         auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n@@ -514,7 +670,8 @@ class ConvertTritonGPUToLLVM\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::SharedEncodingAttr::get(\n                 mod.getContext(), dstDotOp, srcType.getShape(),\n-                getOrder(srcBlocked), srcType.getElementType()));\n+                srcBlocked.getOrder(), srcBlocked.getCTALayout(),\n+                srcType.getElementType()));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n         auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n@@ -632,6 +789,52 @@ class ConvertTritonGPUToLLVM\n       }\n     });\n   }\n+\n+  static Value promoteOperand(OpBuilder &builder, Location loc, Value operand,\n+                              Type promotedType) {\n+    Type tensorPromotedType =\n+        operand.getType().cast<RankedTensorType>().cloneWith(std::nullopt,\n+                                                             promotedType);\n+    return builder.create<triton::FpToFpOp>(loc, tensorPromotedType, operand);\n+  }\n+\n+  // promote operands of dot op if the existing combination is not natively\n+  // supported.\n+  void decomposeMixedModeDotOp(ModuleOp mod) const {\n+    mod.walk([](triton::DotOp dotOp) -> void {\n+      Value D = dotOp.getResult();\n+      OpBuilder builder(dotOp);\n+      Type AElType =\n+          dotOp.getA().getType().cast<RankedTensorType>().getElementType();\n+      Type promoteType;\n+      MmaEncodingAttr mmaLayout = D.getType()\n+                                      .cast<RankedTensorType>()\n+                                      .getEncoding()\n+                                      .dyn_cast<MmaEncodingAttr>();\n+      if (mmaLayout) {\n+        bool isNativeHopperFP8 =\n+            AElType.isFloat8E5M2() || AElType.isFloat8E4M3FN();\n+        bool isFP8 = isNativeHopperFP8 || AElType.isFloat8E5M2FNUZ() ||\n+                     AElType.isFloat8E4M3FNUZ();\n+        if (!isFP8 || (isNativeHopperFP8 && mmaLayout.isHopper()))\n+          return;\n+        promoteType = builder.getF16Type();\n+      } else {\n+        // FMA case.\n+        Type AElType =\n+            dotOp.getA().getType().cast<RankedTensorType>().getElementType();\n+        Type DElType = D.getType().cast<RankedTensorType>().getElementType();\n+        if (AElType == DElType)\n+          return;\n+        promoteType = DElType;\n+      }\n+      Location loc = dotOp.getLoc();\n+      Value promotedA = promoteOperand(builder, loc, dotOp.getA(), promoteType);\n+      Value promotedB = promoteOperand(builder, loc, dotOp.getB(), promoteType);\n+      dotOp.setOperand(0, promotedA);\n+      dotOp.setOperand(1, promotedB);\n+    });\n+  }\n };\n \n } // anonymous namespace\n@@ -640,8 +843,11 @@ namespace mlir {\n namespace triton {\n \n std::unique_ptr<OperationPass<ModuleOp>>\n-createConvertTritonGPUToLLVMPass(int computeCapability, bool isROCM) {\n-  return std::make_unique<::ConvertTritonGPUToLLVM>(computeCapability, isROCM);\n+createConvertTritonGPUToLLVMPass(int computeCapability,\n+                                 mlir::triton::gpu::TMAMetadataTy *tmaMetadata,\n+                                 bool isROCM) {\n+  return std::make_unique<::ConvertTritonGPUToLLVM>(computeCapability,\n+                                                    tmaMetadata, isROCM);\n }\n \n } // namespace triton"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 21, "deletions": 1, "changes": 22, "file_content_changes": "@@ -41,7 +41,27 @@ TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n \n Type TritonGPUToLLVMTypeConverter::convertTritonPointerType(\n     triton::PointerType type) {\n-  // Recursively translate pointee type\n+  auto ctx = type.getContext();\n+  auto pointeeType = type.getPointeeType();\n+  if (pointeeType.isa<RankedTensorType>()) {\n+    auto rankedTensorType = pointeeType.cast<RankedTensorType>();\n+    // struct { offset0, offset1, shape0, shape1, stride0,\n+    // stride1, base_ptr};\n+    auto eleType = rankedTensorType.getElementType();\n+    auto shape = rankedTensorType.getShape();\n+    SmallVector<Type, 4> types;\n+    // offsets\n+    for (size_t i = 0; i < shape.size(); ++i)\n+      types.push_back(IntegerType::get(ctx, 32));\n+    // shapes, strides\n+    for (size_t i = 0; i < 2 * shape.size(); ++i)\n+      types.push_back(IntegerType::get(ctx, 64));\n+\n+    types.push_back(\n+        LLVM::LLVMPointerType::get(eleType, type.getAddressSpace()));\n+\n+    return LLVM::LLVMStructType::getLiteral(ctx, types);\n+  }\n   return LLVM::LLVMPointerType::get(convertType(type.getPointeeType()),\n                                     type.getAddressSpace());\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 93, "deletions": 3, "changes": 96, "file_content_changes": "@@ -6,19 +6,19 @@ namespace mlir {\n namespace LLVM {\n using namespace mlir::triton;\n \n-Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n+Value createConstantI32(Location loc, OpBuilder &rewriter, int32_t v) {\n   auto i32ty = rewriter.getIntegerType(32);\n   return rewriter.create<LLVM::ConstantOp>(loc, i32ty,\n                                            IntegerAttr::get(i32ty, v));\n }\n \n-Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n+Value createConstantF32(Location loc, OpBuilder &rewriter, float v) {\n   auto type = type::f32Ty(rewriter.getContext());\n   return rewriter.create<LLVM::ConstantOp>(loc, type,\n                                            rewriter.getF32FloatAttr(v));\n }\n \n-Value createConstantF64(Location loc, PatternRewriter &rewriter, float v) {\n+Value createConstantF64(Location loc, OpBuilder &rewriter, float v) {\n   auto type = type::f64Ty(rewriter.getContext());\n   return rewriter.create<LLVM::ConstantOp>(loc, type,\n                                            rewriter.getF64FloatAttr(v));\n@@ -40,6 +40,96 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n                                           builder.getIntegerAttr(ty, value));\n }\n \n+// A wrapper of LoadDSmemOp when vec = 1\n+// (1) Get bitwidth from elemTy\n+// (2) Create LoadDSmemOp\n+// (3) Bitcast result from dataTy (u16/u32/u64) back to elemTy\n+Value createLoadDSmem(Location loc, PatternRewriter &rewriter, Value addr,\n+                      Value ctaId) {\n+  assert(addr.getType().isa<LLVMPointerType>() &&\n+         \"addr must be a pointer type\");\n+  auto ptrTy = addr.getType().cast<LLVMPointerType>();\n+  assert(ptrTy.getAddressSpace() == 3 && \"Invalid addr space for load_dsmem\");\n+  auto elemTy = ptrTy.getElementType();\n+  unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n+  Value ret =\n+      rewriter.create<triton::nvgpu::LoadDSmemOp>(loc, addr, ctaId, bitwidth);\n+  return bitcast(ret, elemTy);\n+}\n+\n+// A wrapper of LoadDSmemOp when vec > 1\n+// (1) Get bitwidth from elemTy\n+// (2) Create LoadDSmemOp and extract results from retStruct\n+// (3) Bitcast results from dataTy (u16/u32/u64) back to elemTy\n+SmallVector<Value> createLoadDSmem(Location loc, PatternRewriter &rewriter,\n+                                   Value addr, Value ctaId, unsigned vec) {\n+  assert(addr.getType().isa<LLVMPointerType>() &&\n+         \"addr must be a pointer type\");\n+  auto ptrTy = addr.getType().cast<LLVMPointerType>();\n+  assert(ptrTy.getAddressSpace() == 3 && \"Invalid addr space for load_dsmem\");\n+  auto elemTy = ptrTy.getElementType();\n+  unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n+  Value retStruct = rewriter.create<triton::nvgpu::LoadDSmemOp>(\n+      loc, addr, ctaId, bitwidth, vec);\n+  SmallVector<Value> retVals;\n+  for (unsigned i = 0; i < vec; ++i) {\n+    auto dataTy = rewriter.getIntegerType(bitwidth);\n+    Value data = extract_val(dataTy, retStruct, i);\n+    retVals.push_back(bitcast(data, elemTy));\n+  }\n+  return retVals;\n+}\n+\n+// A wrapper of StoreDSmemOp when vec = 1\n+// (1) Get bitwidth from elemTy\n+// (2) Bitcast value from elemTy to dataTy (u16/u32/u64)\n+// (3) Create StoreDSmemOp\n+void createStoreDSmem(Location loc, PatternRewriter &rewriter, Value addr,\n+                      Value ctaId, Value value, Value pred) {\n+  assert(addr.getType().isa<LLVMPointerType>() &&\n+         \"addr must be a pointer type\");\n+  auto ptrTy = addr.getType().cast<LLVMPointerType>();\n+  assert(ptrTy.getAddressSpace() == 3 && \"Invalid addr space for load_dsmem\");\n+  auto elemTy = ptrTy.getElementType();\n+  unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n+  auto dataTy = rewriter.getIntegerType(bitwidth);\n+  Value data = bitcast(value, dataTy);\n+  rewriter.create<triton::nvgpu::StoreDSmemOp>(loc, addr, ctaId, data, pred);\n+}\n+\n+// A wrapper of StoreDSmemOp when vec = 1 and pred = 1\n+void createStoreDSmem(Location loc, PatternRewriter &rewriter, Value addr,\n+                      Value ctaId, Value value) {\n+  Value pred = int_val(/*width=*/1, 1);\n+  createStoreDSmem(loc, rewriter, addr, ctaId, value, pred);\n+}\n+\n+// A wrapper of StoreDSmemOp when vec > 1\n+// (1) Get bitwidth from elemTy\n+// (2) Bitcast values from elemTy to dataTy (u16/u32/u64)\n+// (3) Create StoreDSmemOp\n+void createStoreDSmem(Location loc, PatternRewriter &rewriter, Value addr,\n+                      Value ctaId, ArrayRef<Value> values, Value pred) {\n+  assert(addr.getType().isa<LLVMPointerType>() &&\n+         \"addr must be a pointer type\");\n+  auto ptrTy = addr.getType().cast<LLVMPointerType>();\n+  assert(ptrTy.getAddressSpace() == 3 && \"Invalid addr space for load_dsmem\");\n+  auto elemTy = ptrTy.getElementType();\n+  unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n+  auto dataTy = rewriter.getIntegerType(bitwidth);\n+  SmallVector<Value> data;\n+  for (unsigned i = 0; i < values.size(); ++i)\n+    data.push_back(bitcast(values[i], dataTy));\n+  rewriter.create<triton::nvgpu::StoreDSmemOp>(loc, addr, ctaId, data, pred);\n+}\n+\n+// A wrapper of StoreDSmemOp when vec > 1 and pred = 1\n+void createStoreDSmem(Location loc, PatternRewriter &rewriter, Value addr,\n+                      Value ctaId, ArrayRef<Value> values) {\n+  Value pred = int_val(/*width=*/1, 1);\n+  createStoreDSmem(loc, rewriter, addr, ctaId, values, pred);\n+}\n+\n SharedMemoryObject\n getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n                                 ConversionPatternRewriter &rewriter) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 39, "deletions": 3, "changes": 42, "file_content_changes": "@@ -14,6 +14,7 @@\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n #define sext(...) rewriter.create<LLVM::SExtOp>(loc, __VA_ARGS__)\n #define fpext(...) rewriter.create<LLVM::FPExtOp>(loc, __VA_ARGS__)\n+#define trunc(...) rewriter.create<LLVM::TruncOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n@@ -43,6 +44,8 @@\n   rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)\n #define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)\n #define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n+#define load_dsmem(...) LLVM::createLoadDSmem(loc, rewriter, __VA_ARGS__)\n+#define store_dsmem(...) LLVM::createStoreDSmem(loc, rewriter, __VA_ARGS__)\n #define fcmp_ogt(lhs, rhs)                                                     \\\n   rewriter.create<LLVM::FCmpOp>(loc, rewriter.getI1Type(),                     \\\n                                 LLVM::FCmpPredicate::ogt, lhs, rhs)\n@@ -75,6 +78,15 @@\n #define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n #define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n+#define barSync(rewriter, op, bar, numThreads)                                 \\\n+  do {                                                                         \\\n+    ::mlir::triton::PTXBuilder ptxBuilder;                                     \\\n+    auto &barSyncOp = *ptxBuilder.create<>(\"bar.sync\");                        \\\n+    barSyncOp(ptxBuilder.newConstantOperand(bar),                              \\\n+              ptxBuilder.newConstantOperand(numThreads));                      \\\n+    auto voidTy = void_ty(op->getContext());                                   \\\n+    ptxBuilder.launch(rewriter, op->getLoc(), voidTy);                         \\\n+  } while (0)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define null(...) rewriter.create<LLVM::NullOp>(loc, __VA_ARGS__)\n #define call(...) rewriter.create<LLVM::CallOp>(loc, __VA_ARGS__)\n@@ -84,6 +96,8 @@\n #define i64_ty rewriter.getIntegerType(64)\n #define i32_ty rewriter.getIntegerType(32)\n #define i16_ty rewriter.getIntegerType(16)\n+#define i32_ty rewriter.getIntegerType(32)\n+#define i64_ty rewriter.getIntegerType(64)\n #define ui32_ty rewriter.getIntegerType(32, false)\n #define f16_ty rewriter.getF16Type()\n #define bf16_ty rewriter.getBF16Type()\n@@ -174,13 +188,13 @@ T getLinearIndex(llvm::ArrayRef<T> multiDimIndex, llvm::ArrayRef<T> shape,\n namespace LLVM {\n using namespace mlir::triton;\n \n-Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v);\n+Value createConstantI32(Location loc, OpBuilder &rewriter, int32_t v);\n \n /// Create a 32-bit float constant.\n-Value createConstantF32(Location loc, PatternRewriter &rewriter, float v);\n+Value createConstantF32(Location loc, OpBuilder &rewriter, float v);\n \n /// Create a 64-bit float constant.\n-Value createConstantF64(Location loc, PatternRewriter &rewriter, float v);\n+Value createConstantF64(Location loc, OpBuilder &rewriter, float v);\n \n /// Create an index type constant.\n Value createIndexConstant(OpBuilder &builder, Location loc,\n@@ -190,6 +204,28 @@ Value createIndexConstant(OpBuilder &builder, Location loc,\n Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n                                 int64_t value);\n \n+/// Usage of macro load_dsmem\n+/// (1) load_dsmem(addr, ctaId)\n+/// (2) load_dsmem(addr, ctaId, vec)\n+Value createLoadDSmem(Location loc, PatternRewriter &rewriter, Value addr,\n+                      Value ctaId);\n+SmallVector<Value> createLoadDSmem(Location loc, PatternRewriter &rewriter,\n+                                   Value addr, Value ctaId, unsigned vec);\n+\n+/// Usage of macro store_dsmem\n+/// (1) store_dsmem(addr, ctaId, value, pred)\n+/// (2) store_dsmem(addr, ctaId, value)\n+/// (3) store_dsmem(addr, ctaId, values, pred)\n+/// (4) store_dsmem(addr, ctaId, values)\n+void createStoreDSmem(Location loc, PatternRewriter &rewriter, Value addr,\n+                      Value ctaId, Value value, Value pred);\n+void createStoreDSmem(Location loc, PatternRewriter &rewriter, Value addr,\n+                      Value ctaId, Value value);\n+void createStoreDSmem(Location loc, PatternRewriter &rewriter, Value addr,\n+                      Value ctaId, ArrayRef<Value> values, Value pred);\n+void createStoreDSmem(Location loc, PatternRewriter &rewriter, Value addr,\n+                      Value ctaId, ArrayRef<Value> values);\n+\n /// Helper function to get strides from a given shape and its order\n SmallVector<Value>\n getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -104,6 +104,7 @@ struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n   using OpAdaptor = typename CatOp::Adaptor;\n \n   explicit CatOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n+\n                            PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<CatOp>(typeConverter, benefit) {}\n \n@@ -138,6 +139,7 @@ struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n struct ViewOpConversion : public ConvertTritonGPUOpToLLVMPattern<ViewOp> {\n   using OpAdaptor = typename ViewOp::Adaptor;\n   explicit ViewOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n+\n                             PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<ViewOp>(typeConverter, benefit) {}\n \n@@ -159,6 +161,7 @@ struct ExpandDimsOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<ExpandDimsOp> {\n   using OpAdaptor = typename ExpandDimsOp::Adaptor;\n   explicit ExpandDimsOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n+\n                                   PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<ExpandDimsOp>(typeConverter, benefit) {}\n \n@@ -221,7 +224,9 @@ struct TransOpConversion\n };\n \n void populateViewOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n-                                  RewritePatternSet &patterns,\n+                                  RewritePatternSet &patterns, int numWarps,\n+                                  ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+                                  ModuleAllocation &allocation,\n                                   PatternBenefit benefit) {\n   patterns.add<ViewOpConversion>(typeConverter, benefit);\n   patterns.add<ExpandDimsOpConversion>(typeConverter, benefit);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.h", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -7,7 +7,9 @@ using namespace mlir;\n using namespace mlir::triton;\n \n void populateViewOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,\n-                                  RewritePatternSet &patterns,\n+                                  RewritePatternSet &patterns, int numWarps,\n+                                  ModuleAxisInfoAnalysis &axisInfoAnalysis,\n+                                  ModuleAllocation &allocation,\n                                   PatternBenefit benefit);\n \n #endif"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 64, "deletions": 13, "changes": 77, "file_content_changes": "@@ -10,6 +10,7 @@\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#include \"triton/Target/PTX/TmaMetadata.h\"\n #include \"llvm/ADT/APSInt.h\"\n #include <numeric>\n \n@@ -240,10 +241,19 @@ struct TritonExpandDimsPattern\n     retWarpsPerCTA.insert(retWarpsPerCTA.begin() + op.getAxis(), 1);\n     SmallVector<unsigned, 4> retOrder(retShape.size());\n     std::iota(retOrder.begin(), retOrder.end(), 0);\n+\n+    auto argCTALayout = argEncoding.getCTALayout();\n+    auto retCTAsPerCGA = insertOne(argCTALayout.getCTAsPerCGA(), op.getAxis());\n+    auto retCTASplitNum =\n+        insertOne(argCTALayout.getCTASplitNum(), op.getAxis());\n+    auto retCTAOrder = insertOrder(argCTALayout.getCTAOrder(), op.getAxis());\n+    auto retCTALayout = triton::gpu::CTALayoutAttr::get(\n+        getContext(), retCTAsPerCGA, retCTASplitNum, retCTAOrder);\n+\n     triton::gpu::BlockedEncodingAttr retEncoding =\n         triton::gpu::BlockedEncodingAttr::get(getContext(), retSizePerThread,\n                                               retThreadsPerWarp, retWarpsPerCTA,\n-                                              retOrder);\n+                                              retOrder, retCTALayout);\n     // convert operand to slice of return type\n     Attribute newArgEncoding = triton::gpu::SliceEncodingAttr::get(\n         getContext(), op.getAxis(), retEncoding);\n@@ -257,6 +267,26 @@ struct TritonExpandDimsPattern\n                   adaptor.getAttributes());\n     return success();\n   }\n+\n+private:\n+  template <typename T>\n+  SmallVector<T> insertOne(ArrayRef<T> vec, unsigned axis) const {\n+    SmallVector<T> res(vec.begin(), vec.end());\n+    res.insert(res.begin() + axis, 1);\n+    return res;\n+  }\n+\n+  // Example:    order = [   0, 2, 1, 3], dim = 2\n+  //          resOrder = [2, 0, 3, 1, 4]\n+  SmallVector<unsigned> insertOrder(ArrayRef<unsigned> order,\n+                                    unsigned axis) const {\n+    SmallVector<unsigned> resOrder(order.begin(), order.end());\n+    for (unsigned i = 0; i < resOrder.size(); ++i)\n+      if (resOrder[i] >= axis)\n+        ++resOrder[i];\n+    resOrder.insert(resOrder.begin(), axis);\n+    return resOrder;\n+  }\n };\n \n struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n@@ -270,6 +300,7 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     auto typeConverter = getTypeConverter<TritonGPUTypeConverter>();\n     int numWarps = typeConverter->getNumWarps();\n     int threadsPerWarp = typeConverter->getThreadsPerWarp();\n+    int numCTAs = typeConverter->getNumCTAs();\n \n     SmallVector<unsigned> retSizePerThread = {1, 1};\n     if (origShape[0] * origShape[1] / (numWarps * threadsPerWarp) >= 4)\n@@ -279,7 +310,7 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     SmallVector<unsigned> retOrder = {1, 0};\n     Attribute dEncoding = triton::gpu::BlockedEncodingAttr::get(\n         getContext(), origShape, retSizePerThread, retOrder, numWarps,\n-        threadsPerWarp);\n+        threadsPerWarp, numCTAs);\n     RankedTensorType retType =\n         RankedTensorType::get(origShape, origType.getElementType(), dEncoding);\n     // a & b must be of smem layout\n@@ -354,9 +385,9 @@ struct TritonCatPattern : public OpConversionPattern<triton::CatOp> {\n     newRetSizePerThread[retOrder[0]] *=\n         newRetTotalElemsPerThread / retTotalElemsPerThread;\n     triton::gpu::BlockedEncodingAttr newRetEncoding =\n-        triton::gpu::BlockedEncodingAttr::get(getContext(), newRetSizePerThread,\n-                                              retThreadsPerWarp, retWarpsPerCTA,\n-                                              retOrder);\n+        triton::gpu::BlockedEncodingAttr::get(\n+            getContext(), newRetSizePerThread, retThreadsPerWarp,\n+            retWarpsPerCTA, retOrder, retEncoding.getCTALayout());\n     auto newRetType = RankedTensorType::get(retShape, retType.getElementType(),\n                                             newRetEncoding);\n     addNamedAttrs(rewriter.replaceOpWithNewOp<triton::CatOp>(\n@@ -386,8 +417,12 @@ struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n       if (auto srcBlockedEncoding =\n               srcEncoding.dyn_cast<triton::gpu::BlockedEncodingAttr>())\n         llvm::copy(srcBlockedEncoding.getOrder(), order.begin());\n-      srcEncoding =\n-          triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, order);\n+      // TODO(Qingyi): need to check whether the CTALayout of srcEncoding should\n+      // be used here. For tests where numCTAs = 1, this is not a problem since\n+      // all CTALayouts are the same.\n+      auto CTALayout = triton::gpu::getCTALayout(srcEncoding);\n+      srcEncoding = triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1,\n+                                                         order, CTALayout);\n       srcType = RankedTensorType::get(srcType.getShape(),\n                                       srcType.getElementType(), srcEncoding);\n       src = rewriter.create<triton::gpu::ConvertLayoutOp>(src.getLoc(), srcType,\n@@ -658,10 +693,12 @@ class TritonReturnOpPattern : public OpConversionPattern<ReturnOp> {\n };\n \n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n-                            RewritePatternSet &patterns) {\n+                            RewritePatternSet &patterns, unsigned numCTAs) {\n   MLIRContext *context = patterns.getContext();\n   patterns\n       .insert< // TODO: view should have custom pattern that views the layout\n+          TritonGenericPattern<triton::AdvanceOp>,\n+          TritonGenericPattern<triton::MakeTensorPtrOp>,\n           TritonGenericPattern<triton::ViewOp>,\n           TritonGenericPattern<triton::BitcastOp>,\n           TritonGenericPattern<triton::FpToFpOp>,\n@@ -889,24 +926,28 @@ class ConvertTritonToTritonGPU\n public:\n   ConvertTritonToTritonGPU() = default;\n   // constructor with some parameters set explicitly.\n-  ConvertTritonToTritonGPU(int numWarps, int threadsPerWarp) {\n+  ConvertTritonToTritonGPU(int numWarps, int threadsPerWarp, int numCTAs,\n+                           int computeCapability) {\n     this->numWarps = numWarps;\n     this->threadsPerWarp = threadsPerWarp;\n+    this->numCTAs = numCTAs;\n+    this->computeCapability = computeCapability;\n   }\n \n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n     ModuleOp mod = getOperation();\n     // type converter\n-    TritonGPUTypeConverter typeConverter(context, numWarps, threadsPerWarp);\n+    TritonGPUTypeConverter typeConverter(context, numWarps, threadsPerWarp,\n+                                         numCTAs);\n     TritonGPUConversionTarget target(*context, typeConverter);\n     // rewrite patterns\n     RewritePatternSet patterns(context);\n     // add rules\n     populateStdPatternsAndLegality(typeConverter, patterns, target);\n     populateArithPatternsAndLegality(typeConverter, patterns, target);\n     populateMathPatternsAndLegality(typeConverter, patterns, target);\n-    populateTritonPatterns(typeConverter, patterns);\n+    populateTritonPatterns(typeConverter, patterns, numCTAs);\n     // TODO: can we use\n     //    mlir::scf::populateSCFStructurealTypeConversionsAndLegality(...) here?\n     populateSCFPatterns(typeConverter, patterns);\n@@ -925,6 +966,13 @@ class ConvertTritonToTritonGPU\n         AttrNumThreadsPerWarp,\n         IntegerAttr::get(i32_ty, llvm::APInt(32, threadsPerWarp.getValue())));\n \n+    mod->setAttr(AttrNumCTAsName,\n+                 IntegerAttr::get(i32_ty, llvm::APInt(32, numCTAs.getValue())));\n+\n+    mod->setAttr(AttrComputeCapabilityName,\n+                 IntegerAttr::get(\n+                     i32_ty, llvm::APInt(32, computeCapability.getValue())));\n+\n     // update layouts\n     //  broadcast src => multicast, dst => broadcasted\n     // if (failed(target.refineLayouts(mod, numWarps)))\n@@ -936,8 +984,11 @@ class ConvertTritonToTritonGPU\n \n std::unique_ptr<OperationPass<ModuleOp>>\n mlir::triton::createConvertTritonToTritonGPUPass(int numWarps,\n-                                                 int threadsPerWarp) {\n-  return std::make_unique<::ConvertTritonToTritonGPU>(numWarps, threadsPerWarp);\n+                                                 int threadsPerWarp,\n+                                                 int numCTAs,\n+                                                 int computeCapability) {\n+  return std::make_unique<::ConvertTritonToTritonGPU>(\n+      numWarps, threadsPerWarp, numCTAs, computeCapability);\n }\n \n std::unique_ptr<OperationPass<ModuleOp>>"}, {"filename": "lib/Dialect/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -1,2 +1,4 @@\n add_subdirectory(Triton)\n add_subdirectory(TritonGPU)\n+add_subdirectory(TritonNvidiaGPU)\n+add_subdirectory(NVGPU)"}, {"filename": "lib/Dialect/NVGPU/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+add_subdirectory(IR)\n+add_subdirectory(ToLLVMIR)"}, {"filename": "lib/Dialect/NVGPU/IR/CMakeLists.txt", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -0,0 +1,9 @@\n+add_mlir_dialect_library(NVGPUIR\n+  Dialect.cpp\n+\n+  DEPENDS\n+  NVGPUTableGen\n+  NVGPUAttrDefsIncGen\n+\n+  LINK_LIBS PUBLIC\n+)"}, {"filename": "lib/Dialect/NVGPU/IR/Dialect.cpp", "status": "added", "additions": 108, "deletions": 0, "changes": 108, "file_content_changes": "@@ -0,0 +1,108 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"mlir/IR/DialectImplementation.h\"\n+#include \"mlir/IR/OpImplementation.h\"\n+\n+// clang-format off\n+#include \"triton/Dialect/NVGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/NVGPU/IR/Dialect.cpp.inc\"\n+// clang-format on\n+\n+using namespace mlir;\n+using namespace mlir::triton::nvgpu;\n+\n+void LoadDSmemOp::build(OpBuilder &builder, OperationState &state,\n+                        Type resultTy, Value addr, Value ctaId) {\n+  unsigned vec, bitwidth;\n+  if (auto structTy = resultTy.dyn_cast<LLVM::LLVMStructType>()) {\n+    auto types = structTy.getBody();\n+    assert(types.size() > 0 && \"Invalid result type of LoadDSmemOp\");\n+    vec = types.size();\n+    for (unsigned i = 0; i < vec; ++i)\n+      assert(types[0] == types[i]);\n+    bitwidth = types[0].getIntOrFloatBitWidth();\n+  } else {\n+    vec = 1;\n+    bitwidth = resultTy.getIntOrFloatBitWidth();\n+  }\n+  build(builder, state, resultTy, addr, ctaId, bitwidth, vec);\n+}\n+\n+void LoadDSmemOp::build(OpBuilder &builder, OperationState &state, Value addr,\n+                        Value ctaId, unsigned bitwidth, unsigned vec) {\n+  Type resultTy = builder.getIntegerType(bitwidth);\n+  if (vec > 1) {\n+    SmallVector<Type> types(vec, resultTy);\n+    resultTy = LLVM::LLVMStructType::getLiteral(builder.getContext(), types);\n+  }\n+  build(builder, state, resultTy, addr, ctaId, bitwidth, vec);\n+}\n+\n+void LoadDSmemOp::build(OpBuilder &builder, OperationState &state, Value addr,\n+                        Value ctaId, unsigned bitwidth) {\n+  build(builder, state, addr, ctaId, bitwidth, /*vec*/ 1);\n+}\n+\n+void StoreDSmemOp::build(OpBuilder &builder, OperationState &state, Value addr,\n+                         Value ctaId, Value value, Value pred) {\n+  SmallVector<Value> values = {value};\n+  build(builder, state, addr, ctaId, values, pred);\n+}\n+\n+unsigned StoreDSmemOp::getBitwidth() {\n+  auto addrTy = getAddr().getType();\n+  assert(addrTy.isa<LLVM::LLVMPointerType>() && \"addr must be a pointer type\");\n+  auto elemTy = addrTy.cast<LLVM::LLVMPointerType>().getElementType();\n+  return elemTy.getIntOrFloatBitWidth();\n+}\n+\n+unsigned StoreDSmemOp::getVec() { return getValues().size(); }\n+\n+static LogicalResult verify(mlir::triton::nvgpu::TMALoadTiledOp op) {\n+  return success();\n+}\n+\n+static LogicalResult verify(mlir::triton::nvgpu::TMALoadIm2colOp op) {\n+  return success();\n+}\n+\n+static LogicalResult verify(mlir::triton::nvgpu::WGMMAOp op) {\n+  return success();\n+}\n+\n+void mlir::triton::nvgpu::NVGPUDialect::initialize() {\n+  addAttributes<\n+#define GET_ATTRDEF_LIST\n+#include \"triton/Dialect/NVGPU/IR/NVGPUAttrDefs.cpp.inc\"\n+      >();\n+\n+  addOperations<\n+#define GET_OP_LIST\n+#include \"triton/Dialect/NVGPU/IR/Ops.cpp.inc\"\n+      >();\n+}\n+\n+#define GET_OP_CLASSES\n+#include \"triton/Dialect/NVGPU/IR/Ops.cpp.inc\"\n+#include \"triton/Dialect/NVGPU/IR/OpsEnums.cpp.inc\""}, {"filename": "lib/Dialect/NVGPU/ToLLVMIR/CMakeLists.txt", "status": "added", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -0,0 +1,17 @@\n+add_mlir_translation_library(NVGPUToLLVMIR\n+  NVGPUToLLVMIR.cpp\n+\n+  DEPENDS\n+  NVGPUTableGen\n+\n+  LINK_COMPONENTS\n+  Core\n+\n+  LINK_LIBS PUBLIC\n+  MLIRIR\n+  MLIRLLVMDialect\n+  MLIRNVVMDialect\n+  MLIRSupport\n+  MLIRTargetLLVMIRExport\n+  NVGPUIR\n+  )"}, {"filename": "lib/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.cpp", "status": "added", "additions": 782, "deletions": 0, "changes": 782, "file_content_changes": "@@ -0,0 +1,782 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"triton/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.h\"\n+#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/IR/Operation.h\"\n+#include \"mlir/Target/LLVMIR/ModuleTranslation.h\"\n+#include \"triton/Dialect/NVGPU/IR/Dialect.h\"\n+\n+#include \"llvm/IR/IRBuilder.h\"\n+#include \"llvm/IR/InlineAsm.h\"\n+#include \"llvm/IR/IntrinsicsNVPTX.h\"\n+\n+using namespace mlir;\n+using namespace mlir::LLVM;\n+\n+namespace {\n+static llvm::FunctionCallee\n+getExternalFuncOP(llvm::Module *module, llvm::StringRef funcName,\n+                  llvm::Type *retTy, ArrayRef<llvm::Type *> argTys = {}) {\n+  return module->getOrInsertFunction(\n+      funcName, llvm::FunctionType::get(retTy, argTys, false),\n+      llvm::AttributeList{});\n+}\n+\n+llvm::Value *createExternalCall(llvm::IRBuilderBase &builder,\n+                                llvm::StringRef funcName,\n+                                ArrayRef<llvm::Value *> args = {},\n+                                ArrayRef<llvm::Type *> tys = {}) {\n+  auto *module = builder.GetInsertBlock()->getModule();\n+  auto *func = module->getFunction(funcName);\n+\n+  if (func == nullptr) {\n+    llvm::SmallVector<llvm::Type *> argTys;\n+    for (auto *arg : args) {\n+      argTys.push_back(arg->getType());\n+    }\n+\n+    llvm::Type *retTy;\n+    if (tys.empty())\n+      retTy = builder.getVoidTy();\n+    else\n+      retTy = tys[0];\n+\n+    func = dyn_cast<llvm::Function>(\n+        getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n+  }\n+\n+  return builder.CreateCall(func, args);\n+}\n+\n+void createMBarrierArrive(llvm::IRBuilderBase &builder,\n+                          mlir::triton::nvgpu::MBarriveType arriveType,\n+                          llvm::Value *barrier, llvm::Value *pred,\n+                          llvm::Value *ctaId, uint32_t txCount) {\n+  auto *module = builder.GetInsertBlock()->getModule();\n+\n+  llvm::SmallVector<llvm::Type *> argTys;\n+  argTys.push_back(barrier->getType());\n+  llvm::Type *retTy = builder.getVoidTy();\n+\n+  if (arriveType == mlir::triton::nvgpu::MBarriveType::normal) {\n+    argTys.push_back(pred->getType());\n+    auto *func = dyn_cast<llvm::Function>(\n+        getExternalFuncOP(module, \"__nv_mbarrier_arrive_normal\", retTy, argTys)\n+            .getCallee());\n+    builder.CreateCall(func, {barrier, pred});\n+  } else if (arriveType == mlir::triton::nvgpu::MBarriveType::cp_async) {\n+    argTys.push_back(pred->getType());\n+    auto *func = dyn_cast<llvm::Function>(\n+        getExternalFuncOP(module, \"__nv_mbarrier_arrive_cp_async\", retTy,\n+                          argTys)\n+            .getCallee());\n+    builder.CreateCall(func, {barrier, pred});\n+  } else if (arriveType == mlir::triton::nvgpu::MBarriveType::expect_tx) {\n+    assert(txCount > 0 && \"txCount should be valid\");\n+    argTys.push_back(builder.getInt32Ty());\n+    argTys.push_back(pred->getType());\n+\n+    auto *func = dyn_cast<llvm::Function>(\n+        getExternalFuncOP(module, \"__nv_mbarrier_arrive_expect_tx\", retTy,\n+                          argTys)\n+            .getCallee());\n+    builder.CreateCall(func, {barrier, builder.getInt32(txCount), pred});\n+  } else if (arriveType == mlir::triton::nvgpu::MBarriveType::remote) {\n+    assert(ctaId && \"ctaId should have a valid value\");\n+    argTys.push_back(ctaId->getType());\n+    argTys.push_back(pred->getType());\n+\n+    auto *func = dyn_cast<llvm::Function>(\n+        getExternalFuncOP(module, \"__nv_mbarrier_arrive_remote\", retTy, argTys)\n+            .getCallee());\n+    builder.CreateCall(func, {barrier, ctaId, pred});\n+  }\n+\n+  return;\n+}\n+\n+llvm::Value *createWGMMADesc(llvm::IRBuilderBase &builder, llvm::Value *buffer,\n+                             mlir::triton::nvgpu::WGMMADescMode mode,\n+                             llvm::Value *height) {\n+  llvm::SmallVector<llvm::Type *> argTys;\n+  argTys.push_back(buffer->getType());\n+  argTys.push_back(builder.getInt32Ty());\n+  argTys.push_back(height->getType());\n+  llvm::Type *retTy = builder.getInt64Ty();\n+\n+  llvm::Value *mode_ = builder.getInt32((uint32_t)mode);\n+  auto *module = builder.GetInsertBlock()->getModule();\n+  auto *func = dyn_cast<llvm::Function>(\n+      getExternalFuncOP(module, \"__nv_get_wgmma_desc\", retTy, argTys)\n+          .getCallee());\n+  return builder.CreateCall(func, {buffer, mode_, height});\n+}\n+\n+static std::string getTMALoadFuncName(bool tiled, bool mcast,\n+                                      uint32_t dimSize) {\n+  std::string funcName;\n+  llvm::raw_string_ostream os(funcName);\n+  os << \"__nv_tma_load\";\n+  if (tiled)\n+    os << \"_tiled\";\n+  else\n+    os << \"_im2col\";\n+\n+  if (mcast)\n+    os << \"_mcast\";\n+\n+  os << \"_\" << dimSize << \"d\";\n+\n+  return funcName;\n+}\n+\n+void createTMALoadTiled(llvm::IRBuilderBase &builder, llvm::Value *dst,\n+                        llvm::Value *mbarrier, llvm::Value *tmaDesc,\n+                        llvm::Value *l2Desc, llvm::Value *mcastMask,\n+                        llvm::Value *pred,\n+                        llvm::SmallVector<llvm::Value *> coords) {\n+  assert(coords.size() >= 2 && coords.size() <= 5 && \"invalid coords.size()\");\n+  auto funcName = getTMALoadFuncName(true, mcastMask != 0, coords.size());\n+  llvm::Type *retTy = builder.getVoidTy();\n+  llvm::SmallVector<llvm::Value *> args;\n+  llvm::SmallVector<llvm::Type *> argTys;\n+\n+  argTys.push_back(tmaDesc->getType());\n+  args.push_back(tmaDesc);\n+\n+  argTys.push_back(dst->getType());\n+  args.push_back(dst);\n+\n+  argTys.push_back(mbarrier->getType());\n+  args.push_back(mbarrier);\n+  for (auto *c : coords) {\n+    argTys.push_back(c->getType());\n+    args.push_back(c);\n+  }\n+  argTys.push_back(l2Desc->getType());\n+  args.push_back(l2Desc);\n+\n+  if (mcastMask != nullptr) {\n+    argTys.push_back(builder.getInt16Ty());\n+    args.push_back(mcastMask);\n+  }\n+\n+  argTys.push_back(pred->getType());\n+  args.push_back(pred);\n+\n+  auto *module = builder.GetInsertBlock()->getModule();\n+  auto *func = dyn_cast<llvm::Function>(\n+      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n+  builder.CreateCall(func, args);\n+\n+  return;\n+}\n+\n+void createTMALoadIm2col(llvm::IRBuilderBase &builder, llvm::Value *dst,\n+                         llvm::Value *mbarrier, llvm::Value *tmaDesc,\n+                         llvm::Value *l2Desc, uint16_t mcastMask,\n+                         llvm::Value *im2colOffsets, llvm::Value *pred,\n+                         llvm::SmallVector<llvm::Value *> coords) {\n+  assert(coords.size() >= 3 && coords.size() <= 5 &&\n+         \"invalid coords.size() for im2col\");\n+  auto funcName = getTMALoadFuncName(false, mcastMask != 0, coords.size());\n+  llvm::Type *retTy = builder.getVoidTy();\n+  llvm::SmallVector<llvm::Value *> args;\n+  llvm::SmallVector<llvm::Type *> argTys;\n+\n+  argTys.push_back(tmaDesc->getType());\n+  args.push_back(tmaDesc);\n+\n+  argTys.push_back(dst->getType());\n+  args.push_back(dst);\n+\n+  argTys.push_back(mbarrier->getType());\n+  args.push_back(mbarrier);\n+  for (auto *c : coords) {\n+    argTys.push_back(c->getType());\n+    args.push_back(c);\n+  }\n+\n+  {\n+    auto offsetsType = dyn_cast<llvm::StructType>(im2colOffsets->getType());\n+    auto subTypes = offsetsType->elements();\n+    assert((coords.size() - subTypes.size() == 2) && \"wrong imcolOffsets\");\n+    unsigned idx = 0;\n+    for (auto subType : subTypes) {\n+      argTys.push_back(subType);\n+      args.push_back(builder.CreateExtractValue(im2colOffsets, {idx}));\n+      idx++;\n+    }\n+  }\n+\n+  argTys.push_back(l2Desc->getType());\n+  args.push_back(l2Desc);\n+\n+  if (mcastMask != 0) {\n+    argTys.push_back(builder.getInt16Ty());\n+    llvm::Value *mcastMask_ = builder.getInt16(mcastMask);\n+    args.push_back(mcastMask_);\n+  }\n+\n+  argTys.push_back(pred->getType());\n+  args.push_back(pred);\n+\n+  auto *module = builder.GetInsertBlock()->getModule();\n+  auto *func = dyn_cast<llvm::Function>(\n+      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n+  builder.CreateCall(func, args);\n+\n+  return;\n+}\n+\n+llvm::Value *createWGMMA(llvm::IRBuilderBase &builder, uint32_t m, uint32_t n,\n+                         uint32_t k, mlir::triton::nvgpu::WGMMAEltType eltTypeC,\n+                         mlir::triton::nvgpu::WGMMAEltType eltTypeA,\n+                         mlir::triton::nvgpu::WGMMAEltType eltTypeB,\n+                         mlir::triton::nvgpu::WGMMALayout layoutA,\n+                         mlir::triton::nvgpu::WGMMALayout layoutB,\n+                         llvm::Value *opA, llvm::Value *opB, llvm::Value *opC) {\n+  // Simplify enum namespace\n+  using namespace mlir::triton::nvgpu;\n+\n+  // Register checks\n+  auto typeA = opA->getType();\n+  auto typeB = opB->getType();\n+  auto typeC = opC->getType();\n+  auto structTypeA = dyn_cast<llvm::StructType>(typeA);\n+  auto structTypeB = dyn_cast<llvm::StructType>(typeB);\n+  auto structTypeC = dyn_cast<llvm::StructType>(typeC);\n+  assert(!structTypeB && \"Operand B can not be registers\");\n+  assert(structTypeC && \"Operand C must be registers\");\n+\n+  // Element type, MNK shape and transposing support check\n+  // Reference:\n+  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-mma\n+  bool transA = layoutA == WGMMALayout::col;\n+  bool transB = layoutB == WGMMALayout::row;\n+  bool supported = false, needTransArgs = false, floatTypeWGMMA = false;\n+  assert(m % 8 == 0 && n % 8 == 0 && k % 8 == 0);\n+  // Below instructions do support transposing, must pass `trans` arguments\n+  supported |=\n+      (eltTypeA == WGMMAEltType::f16) && (eltTypeB == WGMMAEltType::f16) &&\n+      (eltTypeC == WGMMAEltType::f16 || eltTypeC == WGMMAEltType::f32) &&\n+      (m == 64 && 8 <= n && n <= 256 && k == 16);\n+  supported |= (eltTypeA == WGMMAEltType::bf16) &&\n+               (eltTypeB == WGMMAEltType::bf16) &&\n+               (eltTypeC == WGMMAEltType::f32) &&\n+               (m == 64 && 8 <= n && n <= 256 && k == 16);\n+  needTransArgs = supported;\n+  floatTypeWGMMA = supported;\n+  // Below instructions do not support transposing\n+  if (!supported && !transA && !transB) {\n+    supported |= (eltTypeA == WGMMAEltType::tf32) &&\n+                 (eltTypeB == WGMMAEltType::tf32) &&\n+                 (eltTypeC == WGMMAEltType::f32) &&\n+                 (m == 64 && 8 <= n && n <= 256 && k == 8);\n+    supported |=\n+        (eltTypeA == WGMMAEltType::e4m3 || eltTypeA == WGMMAEltType::e5m2) &&\n+        (eltTypeB == WGMMAEltType::e4m3 || eltTypeB == WGMMAEltType::e5m2) &&\n+        (eltTypeC == WGMMAEltType::f16 || eltTypeC == WGMMAEltType::f32) &&\n+        (m == 64 && 8 <= n && n <= 256 && k == 32);\n+    floatTypeWGMMA = supported;\n+    // Below instructions are integer-based\n+    supported |= (eltTypeA == WGMMAEltType::s8) &&\n+                 (eltTypeB == WGMMAEltType::s8) &&\n+                 (eltTypeC == WGMMAEltType::s32) &&\n+                 (m == 64 && 8 <= n && n <= 224 && k == 32);\n+  }\n+  assert(supported && \"WGMMA type or shape is not supported\");\n+\n+  // Build PTX asm\n+  std::string ptxAsm;\n+  std::string constraints;\n+  llvm::raw_string_ostream asmOs(ptxAsm);\n+  llvm::raw_string_ostream conOs(constraints);\n+  llvm::SmallVector<llvm::Type *> argTypes;\n+  llvm::SmallVector<llvm::Value *> args;\n+\n+  // MMA instruction\n+  asmOs << \"wgmma.mma_async.sync.aligned\"\n+        << \".m\" << m << \"n\" << n << \"k\" << k << \".\" << stringifyEnum(eltTypeC)\n+        << \".\" << stringifyEnum(eltTypeA) << \".\" << stringifyEnum(eltTypeB)\n+        << \" \";\n+\n+  // Operands\n+  uint32_t asmOpIdx = 0;\n+\n+  // Operand C\n+  uint32_t numCRegs = structTypeC->getStructNumElements();\n+  asmOs << \"{\";\n+  for (uint32_t i = 0; i < numCRegs; ++i) {\n+    argTypes.push_back(structTypeC->getElementType(i));\n+    args.push_back(builder.CreateExtractValue(opC, {i}));\n+    asmOs << \"$\" << asmOpIdx++ << (i == numCRegs - 1 ? \"\" : \",\");\n+    // LLVM does not support `+` semantic, we must repeat the arguments for both\n+    // input and outputs\n+    if (structTypeC->getElementType(i)->isFloatTy())\n+      conOs << \"=f,\";\n+    else\n+      conOs << \"=r,\";\n+  }\n+  asmOs << \"}, \";\n+  for (uint32_t i = asmOpIdx - numCRegs; i < asmOpIdx; ++i)\n+    conOs << i << \",\";\n+  // Note that LLVM will not skip the indexed repeating placeholders\n+  asmOpIdx += numCRegs;\n+\n+  // Operand A\n+  if (structTypeA) {\n+    uint32_t numARegs = m * k / 128;\n+    assert(numARegs == structTypeA->getNumElements());\n+    asmOs << \"{\";\n+    for (uint32_t i = 0; i < numARegs; ++i) {\n+      argTypes.push_back(structTypeA->getElementType(i));\n+      args.push_back(builder.CreateExtractValue(opA, {i}));\n+      asmOs << \"$\" << asmOpIdx++ << (i == numARegs - 1 ? \"\" : \",\");\n+      conOs << \"f,\";\n+    }\n+    asmOs << \"}, \";\n+  } else {\n+    argTypes.push_back(typeA);\n+    args.push_back(opA);\n+    asmOs << \"$\" << asmOpIdx++ << \", \";\n+    conOs << \"l,\";\n+  }\n+\n+  // Operand B (must be `desc`)\n+  argTypes.push_back(typeB);\n+  args.push_back(opB);\n+  asmOs << \"$\" << asmOpIdx++ << \", \";\n+  conOs << \"l\";\n+\n+  // `scale-d` is 1 by default\n+  asmOs << \"1\";\n+\n+  // `imm-scale-a`, and `imm-scale-b` are 1 by default only for float-based\n+  // WGMMA\n+  if (floatTypeWGMMA)\n+    asmOs << \", 1, 1\";\n+\n+  // Push `trans-a` and `trans-b` args if needed (determined as constant)\n+  if (needTransArgs)\n+    asmOs << \", \" << transA << \", \" << transB;\n+  asmOs << \";\";\n+\n+  // Finally build `llvm::InlineAsm` and call it\n+  auto inlineAsm = llvm::InlineAsm::get(\n+      llvm::FunctionType::get(structTypeC, argTypes, false), ptxAsm,\n+      constraints, true);\n+  return builder.CreateCall(inlineAsm, args);\n+}\n+\n+void createWGMMAFence(llvm::IRBuilderBase &builder) {\n+  std::string asmStr = \"wgmma.fence.sync.aligned;\";\n+  llvm::InlineAsm *iasm =\n+      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n+                           asmStr, \"\", /*hasSideEffect*/ true);\n+  builder.CreateCall(iasm, {});\n+}\n+\n+void createWGMMACommitGroup(llvm::IRBuilderBase &builder) {\n+  std::string asmStr = \"wgmma.commit_group.sync.aligned;\";\n+  llvm::InlineAsm *iasm =\n+      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n+                           asmStr, \"\", /*hasSideEffect*/ true);\n+  builder.CreateCall(iasm, {});\n+}\n+\n+void createWGMMAWaitGroup(llvm::IRBuilderBase &builder, uint32_t pendings) {\n+  std::string asmStr = (llvm::Twine(\"wgmma.wait_group.sync.aligned \") +\n+                        llvm::Twine(pendings) + \";\")\n+                           .str();\n+  llvm::InlineAsm *iasm =\n+      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n+                           asmStr, \"\", /*hasSideEffect*/ true);\n+  builder.CreateCall(iasm, {});\n+}\n+\n+llvm::Value *createLoadSharedCluster(llvm::IRBuilderBase &builder,\n+                                     llvm::Value *addr, llvm::Value *ctaId,\n+                                     unsigned bitwidth, unsigned vec) {\n+  assert(\n+      (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n+      \"invalid bitwidth\");\n+  assert((vec == 1 || vec == 2 || vec == 4) && \"invalid vec size\");\n+\n+  // PTX string\n+  std::string ptxStr;\n+  llvm::raw_string_ostream asmOs(ptxStr);\n+  unsigned addrArgId = vec, ctaIdArgId = vec + 1;\n+  asmOs << \"{\\n\\t\"\n+        << \".reg .u32 remoteAddr;\\n\\t\"\n+        << \"mapa.shared::cluster.u32 remoteAddr, $\" << addrArgId << \", $\"\n+        << ctaIdArgId << \";\\n\\t\";\n+  asmOs << \"ld.shared::cluster\";\n+  if (vec > 1)\n+    asmOs << \".v\" << vec;\n+  asmOs << \".u\" << bitwidth << \" \";\n+  if (vec == 1)\n+    asmOs << \"$0\";\n+  else if (vec == 2)\n+    asmOs << \"{$0, $1}\";\n+  else\n+    asmOs << \"{$0, $1, $2, $3}\";\n+  asmOs << \", [remoteAddr];\\n\\t\"\n+        << \"}\\n\";\n+\n+  // Constraints\n+  std::string constraints;\n+  llvm::raw_string_ostream conOs(constraints);\n+  std::string c = bitwidth == 16 ? \"h\" : (bitwidth == 32 ? \"r\" : \"l\");\n+  for (unsigned i = 0; i < vec; ++i)\n+    conOs << \"=\" << c << \",\";\n+  conOs << \"r,r\";\n+\n+  // Arguments\n+  llvm::SmallVector<llvm::Type *> argTypes;\n+  llvm::SmallVector<llvm::Value *> args;\n+  argTypes.push_back(addr->getType());\n+  args.push_back(addr);\n+  argTypes.push_back(ctaId->getType());\n+  args.push_back(ctaId);\n+\n+  // Return type\n+  llvm::Type *retTy = builder.getIntNTy(bitwidth);\n+  llvm::SmallVector<llvm::Type *> retTys(vec, retTy);\n+  if (vec > 1)\n+    retTy = llvm::StructType::get(builder.getContext(), retTys);\n+\n+  // Call InlineAsm\n+  llvm::InlineAsm *iasm =\n+      llvm::InlineAsm::get(llvm::FunctionType::get(retTy, argTypes, false),\n+                           ptxStr, constraints, /*hasSideEffect*/ false);\n+  return builder.CreateCall(iasm, args);\n+}\n+\n+void createStoreSharedCluster(llvm::IRBuilderBase &builder, llvm::Value *addr,\n+                              llvm::Value *ctaId,\n+                              llvm::SmallVector<llvm::Value *> values,\n+                              llvm::Value *pred, unsigned bitwidth,\n+                              unsigned vec) {\n+  assert(\n+      (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n+      \"invalid bitwidth\");\n+  assert((vec == 1 || vec == 2 || vec == 4) && vec == values.size() &&\n+         \"invalid vec size\");\n+\n+  // PTX string\n+  std::string ptxStr;\n+  llvm::raw_string_ostream asmOs(ptxStr);\n+  asmOs << \"{\\n\\t\"\n+        << \".reg .u32 remoteAddr;\\n\\t\"\n+        << \"mapa.shared::cluster.u32 remoteAddr, $0, $1;\\n\\t\"\n+        << \".reg .pred p;\\n\\t\"\n+        << \"mov.pred p, $2;\\n\\t\";\n+  asmOs << \"@p st.shared::cluster\";\n+  if (vec > 1)\n+    asmOs << \".v\" << vec;\n+  asmOs << \".u\" << bitwidth << \" [remoteAddr], \";\n+  if (vec == 1)\n+    asmOs << \"$3\";\n+  else if (vec == 2)\n+    asmOs << \"{$3, $4}\";\n+  else if (vec == 4)\n+    asmOs << \"{$3, $4, $5, $6}\";\n+  asmOs << \";\\n\\t\"\n+        << \"}\\n\";\n+\n+  // Constraints\n+  std::string constraints;\n+  llvm::raw_string_ostream conOs(constraints);\n+  std::string c = bitwidth == 16 ? \"h\" : (bitwidth == 32 ? \"r\" : \"l\");\n+  conOs << \"r,r,b\";\n+  for (unsigned i = 0; i < vec; ++i)\n+    conOs << \",\" << c;\n+\n+  // Arguments\n+  llvm::SmallVector<llvm::Type *> argTypes;\n+  llvm::SmallVector<llvm::Value *> args;\n+  argTypes.push_back(addr->getType());\n+  args.push_back(addr);\n+  argTypes.push_back(ctaId->getType());\n+  args.push_back(ctaId);\n+  argTypes.push_back(pred->getType());\n+  args.push_back(pred);\n+  for (llvm::Value *value : values) {\n+    argTypes.push_back(value->getType());\n+    args.push_back(value);\n+  }\n+\n+  // Call InlineAsm\n+  llvm::InlineAsm *iasm = llvm::InlineAsm::get(\n+      llvm::FunctionType::get(builder.getVoidTy(), argTypes, false), ptxStr,\n+      constraints, /*hasSideEffect*/ true);\n+  builder.CreateCall(iasm, args);\n+}\n+\n+static std::string getTMAStoreFuncName(bool tiled, uint32_t dimSize) {\n+  std::string funcName;\n+  llvm::raw_string_ostream os(funcName);\n+  os << \"__nv_tma_store\";\n+  if (tiled)\n+    os << \"_tiled\";\n+  else\n+    os << \"_im2col\";\n+\n+  os << \"_\" << dimSize << \"d\";\n+\n+  return funcName;\n+}\n+\n+void createTMAStoreTiled(llvm::IRBuilderBase &builder, llvm::Value *tmaDesc,\n+                         llvm::Value *src, llvm::Value *pred,\n+                         llvm::SmallVector<llvm::Value *> coords) {\n+  assert(coords.size() >= 2 && coords.size() <= 5 && \"invalid coords.size()\");\n+  auto funcName = getTMAStoreFuncName(true, coords.size());\n+  llvm::Type *retTy = builder.getVoidTy();\n+  llvm::SmallVector<llvm::Value *> args;\n+  llvm::SmallVector<llvm::Type *> argTys;\n+\n+  argTys.push_back(tmaDesc->getType());\n+  args.push_back(tmaDesc);\n+\n+  argTys.push_back(src->getType());\n+  args.push_back(src);\n+\n+  for (auto *c : coords) {\n+    argTys.push_back(c->getType());\n+    args.push_back(c);\n+  }\n+  argTys.push_back(pred->getType());\n+  args.push_back(pred);\n+\n+  auto *module = builder.GetInsertBlock()->getModule();\n+  auto *func = dyn_cast<llvm::Function>(\n+      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n+  builder.CreateCall(func, args);\n+\n+  return;\n+}\n+\n+void createStoreMatrix(llvm::IRBuilderBase &builder, llvm::Value *addr,\n+                       llvm::SmallVector<llvm::Value *> datas) {\n+  auto size = datas.size();\n+  assert((size == 1 || size == 2 || size == 4) &&\n+         \"not support size with stmatrix\");\n+\n+  std::string funcName;\n+  llvm::raw_string_ostream os(funcName);\n+  os << \"__nv_stmatrix_x\" << size;\n+\n+  llvm::Type *retTy = builder.getVoidTy();\n+  llvm::SmallVector<llvm::Value *> args;\n+  llvm::SmallVector<llvm::Type *> argTys;\n+\n+  argTys.push_back(addr->getType());\n+  args.push_back(addr);\n+\n+  for (size_t i = 0; i < datas.size(); ++i) {\n+    argTys.push_back(datas[i]->getType());\n+    args.push_back(datas[i]);\n+  }\n+\n+  auto *module = builder.GetInsertBlock()->getModule();\n+  auto *func = dyn_cast<llvm::Function>(\n+      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n+  builder.CreateCall(func, args);\n+}\n+\n+llvm::Value *createOffsetOfStmatrixV4(llvm::IRBuilderBase &builder,\n+                                      llvm::Value *threadId,\n+                                      llvm::Value *rowOfWarp,\n+                                      llvm::Value *elemIdx,\n+                                      uint32_t leadingDimOffset,\n+                                      uint32_t rowStride, bool swizzleEnabled) {\n+  if (swizzleEnabled) {\n+    assert((rowStride == 16 || rowStride == 32 || rowStride == 64) &&\n+           \"wrong rowString for swizzleEnabled\");\n+  }\n+  llvm::Type *retTy = builder.getInt32Ty();\n+  llvm::SmallVector<llvm::Value *> args;\n+  llvm::SmallVector<llvm::Type *> argTys;\n+\n+  argTys.push_back(threadId->getType());\n+  args.push_back(threadId);\n+\n+  argTys.push_back(rowOfWarp->getType());\n+  args.push_back(rowOfWarp);\n+\n+  argTys.push_back(elemIdx->getType());\n+  args.push_back(elemIdx);\n+\n+  argTys.push_back(builder.getInt32Ty());\n+  args.push_back(builder.getInt32(leadingDimOffset));\n+\n+  argTys.push_back(builder.getInt32Ty());\n+  args.push_back(builder.getInt32(rowStride));\n+\n+  std::string funcName(\"__nv_offset_of_stmatrix_v4\");\n+  if (!swizzleEnabled)\n+    funcName = \"__nv_offset_of_stmatrix_v4_no_swizzle\";\n+  auto *module = builder.GetInsertBlock()->getModule();\n+  auto *func = dyn_cast<llvm::Function>(\n+      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n+  return builder.CreateCall(func, args);\n+}\n+\n+llvm::Value *createOffsetOfSts64(llvm::IRBuilderBase &builder,\n+                                 llvm::Value *threadId, llvm::Value *rowOfWarp,\n+                                 llvm::Value *elemIdx,\n+                                 uint32_t leadingDimOffset, uint32_t rowStride,\n+                                 bool swizzleEnabled) {\n+  if (swizzleEnabled) {\n+    assert((rowStride == 32 || rowStride == 64 || rowStride == 128) &&\n+           \"wrong rowString for swizzleEnabled\");\n+  }\n+  llvm::Type *retTy = builder.getInt32Ty();\n+  llvm::SmallVector<llvm::Value *> args;\n+  llvm::SmallVector<llvm::Type *> argTys;\n+\n+  argTys.push_back(threadId->getType());\n+  args.push_back(threadId);\n+\n+  argTys.push_back(rowOfWarp->getType());\n+  args.push_back(rowOfWarp);\n+\n+  argTys.push_back(elemIdx->getType());\n+  args.push_back(elemIdx);\n+\n+  argTys.push_back(builder.getInt32Ty());\n+  args.push_back(builder.getInt32(leadingDimOffset));\n+\n+  argTys.push_back(builder.getInt32Ty());\n+  args.push_back(builder.getInt32(rowStride));\n+\n+  std::string funcName(\"__nv_offset_of_sts64\");\n+  auto *module = builder.GetInsertBlock()->getModule();\n+  auto *func = dyn_cast<llvm::Function>(\n+      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n+  return builder.CreateCall(func, args);\n+}\n+\n+void createSts64(llvm::IRBuilderBase &builder, llvm::Value *offset,\n+                 llvm::Value *d0, llvm::Value *d1) {\n+  std::string funcName(\"__nv_sts64\");\n+\n+  llvm::Type *retTy = builder.getVoidTy();\n+  llvm::SmallVector<llvm::Value *> args;\n+  llvm::SmallVector<llvm::Type *> argTys;\n+  auto i32Ty = builder.getInt32Ty();\n+  argTys.push_back(i32Ty);\n+  args.push_back(offset);\n+\n+  argTys.push_back(i32Ty);\n+  args.push_back(builder.CreateBitCast(d0, i32Ty));\n+\n+  argTys.push_back(i32Ty);\n+  args.push_back(builder.CreateBitCast(d1, i32Ty));\n+\n+  auto *module = builder.GetInsertBlock()->getModule();\n+  auto *func = dyn_cast<llvm::Function>(\n+      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n+  builder.CreateCall(func, args);\n+\n+  return;\n+}\n+\n+static llvm::Value *getSRegValue(llvm::IRBuilderBase &builder,\n+                                 llvm::StringRef name) {\n+  std::string ptxStr;\n+  llvm::raw_string_ostream asmOs(ptxStr);\n+  asmOs << \"mov.u32 $0, \" << name << \";\";\n+  std::string constraints = \"=r\";\n+  llvm::InlineAsm *inlineAsm =\n+      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getInt32Ty(), false),\n+                           ptxStr, constraints, /*hasSideEffect*/ false);\n+  return builder.CreateCall(inlineAsm);\n+}\n+\n+static llvm::Value *createClusterId(llvm::IRBuilderBase &builder) {\n+  llvm::Value *x = getSRegValue(builder, \"%cluster_ctaid.x\");\n+  llvm::Value *y = getSRegValue(builder, \"%cluster_ctaid.y\");\n+  llvm::Value *z = getSRegValue(builder, \"%cluster_ctaid.z\");\n+  llvm::Value *nx = getSRegValue(builder, \"%cluster_nctaid.x\");\n+  llvm::Value *ny = getSRegValue(builder, \"%cluster_nctaid.y\");\n+  llvm::Value *clusterCTAId = builder.CreateAdd(\n+      x, builder.CreateMul(builder.CreateAdd(y, builder.CreateMul(z, ny)), nx));\n+  return clusterCTAId;\n+}\n+void createRegAlloc(llvm::IRBuilderBase &builder, const uint32_t regCount) {\n+  std::string ptxStr;\n+  llvm::raw_string_ostream asmOs(ptxStr);\n+  asmOs << \"setmaxnreg.inc.sync.aligned.u32 \" << regCount << \";\\n\";\n+  // Call InlineAsm\n+  llvm::InlineAsm *iasm =\n+      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n+                           ptxStr, \"\", /*hasSideEffect*/ true);\n+  builder.CreateCall(iasm, {});\n+}\n+\n+void createRegDealloc(llvm::IRBuilderBase &builder, const uint32_t regCount) {\n+  std::string ptxStr;\n+  llvm::raw_string_ostream asmOs(ptxStr);\n+  asmOs << \"setmaxnreg.dec.sync.aligned.u32 \" << regCount << \";\\n\";\n+  // Call InlineAsm\n+  llvm::InlineAsm *iasm =\n+      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n+                           ptxStr, \"\", /*hasSideEffect*/ true);\n+  builder.CreateCall(iasm, {});\n+}\n+\n+class NVGPUDialectLLVMIRTranslationInterface\n+    : public LLVMTranslationDialectInterface {\n+public:\n+  using LLVMTranslationDialectInterface::LLVMTranslationDialectInterface;\n+\n+  /// Translates the given operation to LLVM IR using the provided IR builder\n+  /// and saving the state in `moduleTranslation`.\n+  LogicalResult\n+  convertOperation(Operation *op, llvm::IRBuilderBase &builder,\n+                   LLVM::ModuleTranslation &moduleTranslation) const final {\n+    Operation &opInst = *op;\n+#include \"triton/Dialect/NVGPU/IR/OpsConversions.inc\"\n+\n+    return failure();\n+  }\n+};\n+} // namespace\n+\n+void mlir::registerNVGPUDialectTranslation(DialectRegistry &registry) {\n+  registry.insert<mlir::triton::nvgpu::NVGPUDialect>();\n+  registry.addExtension(\n+      +[](MLIRContext *ctx, mlir::triton::nvgpu::NVGPUDialect *dialect) {\n+        dialect->addInterfaces<NVGPUDialectLLVMIRTranslationInterface>();\n+      });\n+}\n+\n+void mlir::registerNVGPUDialectTranslation(MLIRContext &context) {\n+  DialectRegistry registry;\n+  registerNVGPUDialectTranslation(registry);\n+  context.appendDialectRegistry(registry);\n+}"}, {"filename": "lib/Dialect/Triton/IR/Dialect.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -7,6 +7,7 @@\n #include \"llvm/ADT/TypeSwitch.h\"\n #include \"llvm/Support/raw_ostream.h\"\n \n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n #include \"mlir/IR/DialectImplementation.h\"\n \n #include \"mlir/Transforms/InliningUtils.h\""}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 18, "deletions": 7, "changes": 25, "file_content_changes": "@@ -7,15 +7,17 @@ using namespace mlir;\n \n static LogicalResult verifySameEncoding(Type typeA, Type typeB,\n                                         bool allowTensorPointerType) {\n+  // TODO(Keren): the allowTensorPointerType argument is a hack to allow.\n+  // The type checking code is kind of a mess with the current design.\n   auto getEncoding = [=](Type type) -> Attribute {\n-    auto rankedType = type.dyn_cast<RankedTensorType>();\n-    if (allowTensorPointerType) {\n-      if (auto ptrType = type.dyn_cast<triton::PointerType>())\n-        rankedType = ptrType.getPointeeType().dyn_cast<RankedTensorType>();\n-    } else {\n+    Attribute ret;\n+    if (auto tensorType = dyn_cast<RankedTensorType>(type)) {\n+      ret = tensorType.getEncoding();\n+    }\n+    if (!allowTensorPointerType) {\n       assert(!triton::isTensorPointerType(type));\n     }\n-    return rankedType ? rankedType.getEncoding() : Attribute();\n+    return ret;\n   };\n   auto encodingA = getEncoding(typeA);\n   auto encodingB = getEncoding(typeB);\n@@ -127,7 +129,16 @@ OpTrait::impl::verifySameLoadStoreOperandsAndResultShape(Operation *op) {\n bool OpTrait::impl::verifyLoadStorePointerAndValueType(Type valueType,\n                                                        Type ptrType) {\n   if (triton::isTensorPointerType(ptrType)) {\n-    return ptrType.cast<triton::PointerType>().getPointeeType() == valueType;\n+    // The encoding of tensor pointers is meaningless, we only check shapes and\n+    // the type of elements\n+    auto tensorAType = ptrType.cast<triton::PointerType>()\n+                           .getPointeeType()\n+                           .cast<RankedTensorType>();\n+    if (!isa<RankedTensorType>(valueType))\n+      return false;\n+    auto tensorBType = valueType.cast<RankedTensorType>();\n+    return tensorAType.getShape() == tensorBType.getShape() &&\n+           tensorAType.getElementType() == tensorBType.getElementType();\n   } else if (auto rankedType = ptrType.dyn_cast<RankedTensorType>()) {\n     if (auto elementPtrType =\n             dyn_cast<triton::PointerType>(rankedType.getElementType())) {"}, {"filename": "lib/Dialect/Triton/IR/Types.cpp", "status": "modified", "additions": 12, "deletions": 3, "changes": 15, "file_content_changes": "@@ -27,15 +27,20 @@ Type PointerType::parse(AsmParser &parser) {\n   if (parser.parseType(pointeeType))\n     return Type();\n \n+  int addressSpace = 1;\n+  if (succeeded(parser.parseOptionalComma())) {\n+    if (parser.parseInteger(addressSpace))\n+      return Type();\n+  }\n+\n   if (parser.parseGreater())\n     return Type();\n \n-  // TODO: also print address space?\n-  return PointerType::get(pointeeType, 1);\n+  return PointerType::get(pointeeType, addressSpace);\n }\n \n void PointerType::print(AsmPrinter &printer) const {\n-  printer << \"<\" << getPointeeType() << \">\";\n+  printer << \"<\" << getPointeeType() << \", \" << getAddressSpace() << \">\";\n }\n \n namespace mlir {\n@@ -99,6 +104,10 @@ bool isTensorPointerType(Type type) {\n   return false;\n }\n \n+bool isTensorOrTensorPointerType(Type type) {\n+  return type.isa<RankedTensorType>() || isTensorPointerType(type);\n+}\n+\n Type getElementTypeOfTensorPointerType(Type type) {\n   if (auto ptrType = type.dyn_cast<PointerType>())\n     if (auto tensorTy = ptrType.getPointeeType().dyn_cast<RankedTensorType>())"}, {"filename": "lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -11,6 +11,8 @@ using namespace mlir;\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/Triton/Transforms/Passes.h.inc\"\n \n+namespace {\n+\n /// An additional struct to record the meta information of operations\n /// with tensor pointers\n struct RewritedInfo {\n@@ -186,6 +188,8 @@ struct RewritedInfo {\n   }\n };\n \n+} // namespace\n+\n class RewriteTensorPointerPass\n     : public TritonRewriteTensorPointerBase<RewriteTensorPointerPass> {\n private:\n@@ -470,8 +474,8 @@ class RewriteTensorPointerPass\n \n   void runOnOperation() override {\n     // Only rewrite if the hardware does not support\n-    // if (computeCapability >= 90)\n-    //   return;\n+    if (computeCapability >= 90)\n+      return;\n \n     // NOTES(Chenggang): we don't use `ConversionPatternRewriter`, because\n     // MLIR does not support one-multiple value mapping. For example, if we use"}, {"filename": "lib/Dialect/TritonGPU/IR/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,6 +1,7 @@\n add_mlir_dialect_library(TritonGPUIR\n   Dialect.cpp\n   Traits.cpp\n+  Types.cpp\n \n   DEPENDS\n   TritonGPUTableGen"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 459, "deletions": 68, "changes": 527, "file_content_changes": "@@ -16,6 +16,21 @@ using namespace mlir::triton::gpu;\n namespace mlir {\n namespace triton {\n \n+static Type getI1SameShapeFromTensorOrTensorPtr(Type type) {\n+  auto i1Type = IntegerType::get(type.getContext(), 1);\n+  if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n+    return RankedTensorType::get(tensorType.getShape(), i1Type,\n+                                 tensorType.getEncoding());\n+  } else if (auto ptrType = type.dyn_cast<triton::PointerType>()) {\n+    Type pointeeType = ptrType.getPointeeType();\n+    if (auto tensorType = pointeeType.dyn_cast<RankedTensorType>()) {\n+      return RankedTensorType::get(tensorType.getShape(), i1Type,\n+                                   tensorType.getEncoding());\n+    }\n+  }\n+  return Type();\n+}\n+\n namespace gpu {\n \n // TODO: Inheritance of layout attributes\n@@ -80,6 +95,8 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n       return {4, 8};\n     if (mmaLayout.isAmpere())\n       return {8, 4};\n+    if (mmaLayout.isHopper())\n+      return {8, 4};\n   }\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parent = sliceLayout.getParent();\n@@ -179,6 +196,10 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n       return {2, 2};\n     } else if (mmaLayout.isVolta()) {\n       return {1, 2};\n+    } else if (mmaLayout.isHopper()) {\n+      auto instrShape = mmaLayout.getInstrShape();\n+      // TODO(thomas): what are those magic numbers?\n+      return SmallVector<unsigned>{instrShape[0] * 4 / 32, instrShape[1] / 4};\n     } else {\n       llvm_unreachable(\"Unexpected mma version\");\n     }\n@@ -188,7 +209,6 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n     if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n       assert(parentMmaLayout.isAmpere() &&\n              \"mmaLayout version = 1 is not implemented yet\");\n-      auto parentShapePerCTA = getShapePerCTA(parentLayout);\n       auto opIdx = dotLayout.getOpIdx();\n       if (opIdx == 0) {\n         return {2, 4};\n@@ -211,7 +231,7 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n \n SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    assert(mmaLayout.isVolta() || mmaLayout.isAmpere());\n+    assert(mmaLayout.isVolta() || mmaLayout.isAmpere() || mmaLayout.isHopper());\n     return {1, 2};\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n@@ -258,28 +278,23 @@ SmallVector<unsigned> getThreadsPerCTA(Attribute layout) {\n     } else\n       assert(0 && \"Unimplemented usage of MmaEncodingAttr\");\n   } else {\n-    assert(0 && \"Unimplemented usage of getShapePerCTA\");\n+    assert(0 && \"Unimplemented usage of getThreadsPerCTA\");\n   }\n \n   return threads;\n }\n \n-SmallVector<unsigned> getShapePerCTA(Attribute layout,\n-                                     ArrayRef<int64_t> tensorShape) {\n+SmallVector<unsigned> getShapePerCTATile(Attribute layout,\n+                                         ArrayRef<int64_t> tensorShape) {\n   SmallVector<unsigned> shape;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     for (unsigned d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n       shape.push_back(blockedLayout.getSizePerThread()[d] *\n                       blockedLayout.getThreadsPerWarp()[d] *\n                       blockedLayout.getWarpsPerCTA()[d]);\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-    unsigned dim = sliceLayout.getDim();\n-    auto parent = sliceLayout.getParent();\n-    for (unsigned d = 0, n = getOrder(parent).size(); d < n; ++d) {\n-      if (d == dim)\n-        continue;\n-      shape.push_back(getShapePerCTA(parent, tensorShape)[d]);\n-    }\n+    shape = getShapePerCTATile(sliceLayout.getParent(), tensorShape);\n+    shape.erase(shape.begin() + sliceLayout.getDim());\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.isAmpere())\n       return {16 * mmaLayout.getWarpsPerCTA()[0],\n@@ -292,19 +307,25 @@ SmallVector<unsigned> getShapePerCTA(Attribute layout,\n       return {static_cast<unsigned>(tensorShape[0]),\n               static_cast<unsigned>(tensorShape[1])};\n     }\n+    if (mmaLayout.isHopper()) {\n+      auto instrShape = mmaLayout.getInstrShape();\n+      return {16 * mmaLayout.getWarpsPerCTA()[0],\n+              instrShape[1] * mmaLayout.getWarpsPerCTA()[1]};\n+    }\n     assert(0 && \"Unexpected MMA layout version found\");\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n     if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n       assert(parentMmaLayout.isAmpere() &&\n              \"mmaLayout version = 1 is not implemented yet\");\n-      auto parentShapePerCTA = getShapePerCTA(parentLayout, tensorShape);\n+      auto parentShapePerCTATile =\n+          getShapePerCTATile(parentLayout, tensorShape);\n       auto opIdx = dotLayout.getOpIdx();\n       if (opIdx == 0) {\n-        return {parentShapePerCTA[0], 16};\n+        return {parentShapePerCTATile[0], 16};\n       } else if (opIdx == 1) {\n-        return {16, parentShapePerCTA[1]};\n+        return {16, parentShapePerCTATile[1]};\n       } else {\n         assert(0 && \"DotOperandEncodingAttr opIdx must be 0 or 1\");\n       }\n@@ -313,11 +334,32 @@ SmallVector<unsigned> getShapePerCTA(Attribute layout,\n                   \"supported yet\");\n     }\n   } else {\n-    assert(0 && \"Unimplemented usage of getShapePerCTA\");\n+    assert(0 && \"Unimplemented usage of getShapePerCTATile\");\n   }\n   return shape;\n }\n \n+namespace {\n+\n+/* Utility function used by getOrder and getCTAOrder of SliceEncodingAttr.\n+ * Erase dim and decrease all values larger than dim by 1.\n+ * Example:    order = [0, 2, 4, 3, 1], dim = 2\n+ *          resOrder = [0,    3, 2, 1]\n+ */\n+SmallVector<unsigned> eraseOrder(ArrayRef<unsigned> order, unsigned dim) {\n+  unsigned rank = order.size();\n+  assert(dim < rank && \"Invalid dim to erase\");\n+  SmallVector<unsigned> resOrder;\n+  for (unsigned i : order)\n+    if (i < dim)\n+      resOrder.push_back(i);\n+    else if (i > dim)\n+      resOrder.push_back(i - 1);\n+  return resOrder;\n+}\n+\n+} // namespace\n+\n SmallVector<unsigned> getOrder(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getOrder().begin(),\n@@ -344,8 +386,178 @@ SmallVector<unsigned> getOrder(Attribute layout) {\n                                  sharedLayout.getOrder().end());\n   } else {\n     assert(0 && \"Unimplemented usage of getOrder\");\n-    return {};\n   }\n+  return {};\n+};\n+\n+CTALayoutAttr getCTALayout(Attribute layout) {\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+    return blockedLayout.getCTALayout();\n+  else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>())\n+    return CTALayoutAttr::get(layout.getContext(), getCTAsPerCGA(sliceLayout),\n+                              getCTASplitNum(sliceLayout),\n+                              getCTAOrder(sliceLayout));\n+  else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>())\n+    return mmaLayout.getCTALayout();\n+  else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>())\n+    return CTALayoutAttr::get(layout.getContext(), getCTAsPerCGA(dotLayout),\n+                              getCTASplitNum(dotLayout),\n+                              getCTAOrder(dotLayout));\n+  else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>())\n+    return sharedLayout.getCTALayout();\n+  else\n+    assert(0 && \"Unimplemented usage of getCTALayout\");\n+  return {};\n+}\n+\n+SmallVector<unsigned> getCTAsPerCGA(Attribute layout) {\n+  ArrayRef<unsigned> ref;\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+    ref = blockedLayout.getCTALayout().getCTAsPerCGA();\n+  else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    auto parentCTAsPerCGA = getCTAsPerCGA(sliceLayout.getParent());\n+    if (parentCTAsPerCGA[sliceLayout.getDim()] == 1) {\n+      parentCTAsPerCGA.erase(parentCTAsPerCGA.begin() + sliceLayout.getDim());\n+      return parentCTAsPerCGA;\n+    }\n+    /* For getCTAsPerCGA of a slice layout, we have two choices:\n+     * (1) Return CTAsPerCGA of its parent. This is not a perfect solution\n+     * because the rank of the returned CTAsPerCGA does not match the rank of\n+     * tensorShape.\n+     * (2) Get CTAsPerCGA of its parent and erase the sliced dim. This is not a\n+     * perfect solution because the product of the returned CTAsPerCGA might not\n+     * match numCTAs.\n+     * To avoid introducing inconsistencies to the shape and\n+     * layout system, the usage of directly getting CTAsPerCGA of a slice layout\n+     * in which the sliced dim is not 1 is banned. You should always consider\n+     * slice layout as a special case and use getCTAsPerCGA(layout.getParent())\n+     * in the branch where layout is an instance of SliceEncodingAttr. This is\n+     * inconvenient but safe.\n+     */\n+    assert(0 && \"getCTAsPerCGA for SliceEncodingAttr is not well-defined\");\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>())\n+    ref = mmaLayout.getCTALayout().getCTAsPerCGA();\n+  else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>())\n+    return getCTAsPerCGA(dotLayout.getParent());\n+  else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>())\n+    ref = sharedLayout.getCTALayout().getCTAsPerCGA();\n+  else\n+    assert(0 && \"Unimplemented usage of getCTAsPerCGA\");\n+  return SmallVector<unsigned>(ref.begin(), ref.end());\n+}\n+\n+SmallVector<unsigned> getCTASplitNum(Attribute layout) {\n+  SmallVector<unsigned> res;\n+\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    res.assign(blockedLayout.getCTALayout().getCTASplitNum().begin(),\n+               blockedLayout.getCTALayout().getCTASplitNum().end());\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    res = getCTASplitNum(sliceLayout.getParent());\n+    res.erase(res.begin() + sliceLayout.getDim());\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    res.assign(mmaLayout.getCTALayout().getCTASplitNum().begin(),\n+               mmaLayout.getCTALayout().getCTASplitNum().end());\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    res = getCTASplitNum(dotLayout.getParent());\n+    assert(res.size() == 2 && \"Invalid dotLayout\");\n+\n+    // Do not split CTA in K dimension\n+    dotLayout.getOpIdx() == 0 ? res[1] = 1 : res[0] = 1;\n+  } else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n+    res.assign(sharedLayout.getCTALayout().getCTASplitNum().begin(),\n+               sharedLayout.getCTALayout().getCTASplitNum().end());\n+  } else {\n+    assert(false && \"Unimplemented usage of getCTASplitNum\");\n+  }\n+\n+  return res;\n+}\n+\n+SmallVector<unsigned> getCTAOrder(Attribute layout) {\n+  ArrayRef<unsigned> ref;\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    ref = blockedLayout.getCTALayout().getCTAOrder();\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    auto parentCTAOrder = getCTAOrder(sliceLayout.getParent());\n+    return eraseOrder(parentCTAOrder, sliceLayout.getDim());\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    ref = mmaLayout.getCTALayout().getCTAOrder();\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    return getCTAOrder(dotLayout.getParent());\n+  } else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n+    ref = sharedLayout.getCTALayout().getCTAOrder();\n+  } else {\n+    assert(0 && \"Unimplemented usage of getCTAOrder\");\n+  }\n+  return SmallVector<unsigned>(ref.begin(), ref.end());\n+}\n+\n+SmallVector<int64_t> getShapePerCTA(ArrayRef<unsigned> CTASplitNum,\n+                                    ArrayRef<int64_t> shape) {\n+  unsigned rank = shape.size();\n+  SmallVector<int64_t> shapePerCTA(rank);\n+  for (unsigned i = 0; i < rank; ++i) {\n+    // This wrapping rule must be consistent with emitCTAOffsetForLayout\n+    unsigned splitNum = std::min<unsigned>(shape[i], CTASplitNum[i]);\n+    shapePerCTA[i] = shape[i] / splitNum;\n+  }\n+  return shapePerCTA;\n+}\n+\n+SmallVector<int64_t> getShapePerCTA(Attribute layout, ArrayRef<int64_t> shape) {\n+  if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n+    // Special logic for pipeline pass, where shape is 3D and CTALayout is 2D.\n+    // The first dim of shape is numStages. This is a work around, otherwise too\n+    // many places would have to be modified in pipeline pass. Maybe we need to\n+    // refactor this logic in the future.\n+    auto CTASplitNum = sharedLayout.getCTALayout().getCTASplitNum();\n+    if (shape.size() == CTASplitNum.size() + 1) {\n+      auto res = getShapePerCTA(CTASplitNum, shape.drop_front());\n+      res.insert(res.begin(), shape.front());\n+      return res;\n+    }\n+  }\n+  return getShapePerCTA(getCTASplitNum(layout), shape);\n+}\n+\n+SmallVector<int64_t> getShapePerCTA(Type type) {\n+  auto tensorType = type.cast<RankedTensorType>();\n+  return getShapePerCTA(tensorType.getEncoding(), tensorType.getShape());\n+}\n+\n+unsigned getNumWarpsPerCTA(Attribute layout) {\n+  ArrayRef<unsigned> warpsPerCTA;\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+    warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+  else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>())\n+    return getNumWarpsPerCTA(sliceLayout.getParent());\n+  else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>())\n+    warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+  else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>())\n+    return getNumWarpsPerCTA(dotLayout.getParent());\n+  else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>())\n+    assert(0 && \"Cannot get numWarps from SharedEncodingAttr\");\n+  else\n+    assert(0 && \"Unimplemented usage of getNumWarpsPerCTA\");\n+  return product<unsigned>(warpsPerCTA);\n+}\n+\n+unsigned getNumCTAs(Attribute layout) {\n+  ArrayRef<unsigned> CTAsPerCGA;\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+    CTAsPerCGA = blockedLayout.getCTALayout().getCTAsPerCGA();\n+  else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>())\n+    return getNumCTAs(sliceLayout.getParent());\n+  else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>())\n+    CTAsPerCGA = mmaLayout.getCTALayout().getCTAsPerCGA();\n+  else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>())\n+    return getNumCTAs(dotLayout.getParent());\n+  else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>())\n+    CTAsPerCGA = sharedLayout.getCTALayout().getCTAsPerCGA();\n+  else\n+    assert(0 && \"Unimplemented usage of getNumCTAs\");\n+  return product<unsigned>(CTAsPerCGA);\n }\n \n bool isaDistributedLayout(Attribute layout) {\n@@ -362,7 +574,7 @@ bool isSharedEncoding(Value value) {\n   return false;\n }\n \n-bool isExpensiveCat(CatOp cat, Attribute &targetEncoding) {\n+bool isExpensiveCat(CatOp cat, Attribute targetEncoding) {\n   // If the new elements per thread is less than the old one, we will need to do\n   // convert encoding that goes through shared memory anyway. So we consider it\n   // as expensive.\n@@ -411,10 +623,21 @@ static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,\n   return success();\n }\n \n+static LogicalResult parseBoolAttrValue(AsmParser &parser, Attribute attr,\n+                                        bool &value, StringRef desc) {\n+  auto boolAttr = attr.dyn_cast<BoolAttr>();\n+  if (!boolAttr) {\n+    parser.emitError(parser.getNameLoc(), \"expected an bool type in \") << desc;\n+    return failure();\n+  }\n+  value = boolAttr.getValue();\n+  return success();\n+}\n+\n // parse an array of integers\n static LogicalResult parseIntArrayAttr(AsmParser &parser,\n                                        const NamedAttribute &attr,\n-                                       SmallVector<unsigned, 2> &res,\n+                                       SmallVector<unsigned> &res,\n                                        StringRef desc) {\n   auto arrayAttr = attr.getValue().dyn_cast<ArrayAttr>();\n   if (!arrayAttr) {\n@@ -435,6 +658,11 @@ static LogicalResult parseUInt(AsmParser &parser, const NamedAttribute &attr,\n   return parseIntAttrValue(parser, attr.getValue(), value, desc);\n };\n \n+static LogicalResult parseBool(AsmParser &parser, const NamedAttribute &attr,\n+                               bool &value, StringRef desc) {\n+  return parseBoolAttrValue(parser, attr.getValue(), value, desc);\n+};\n+\n //===----------------------------------------------------------------------===//\n // Attribute methods\n //===----------------------------------------------------------------------===//\n@@ -451,12 +679,13 @@ BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n   auto sizePerThread = getSizePerThread();\n   auto warpsPerCTA = getWarpsPerCTA();\n   auto threadsPerWarp = getThreadsPerWarp();\n+  auto shapePerCTA = getShapePerCTA(*this, shape);\n   assert(rank == sizePerThread.size() &&\n          \"unexpected rank in BlockedEncodingAttr::getElemsPerThread\");\n   SmallVector<unsigned> elemsPerThread(rank);\n   for (size_t i = 0; i < rank; ++i) {\n     unsigned t = sizePerThread[i] * threadsPerWarp[i] * warpsPerCTA[i];\n-    elemsPerThread[i] = ceil<unsigned>(shape[i], t) * sizePerThread[i];\n+    elemsPerThread[i] = ceil<unsigned>(shapePerCTA[i], t) * sizePerThread[i];\n   }\n   return elemsPerThread;\n }\n@@ -503,7 +732,10 @@ SmallVector<unsigned>\n MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const {\n   size_t rank = shape.size();\n   assert(rank == 2 && \"Unexpected rank of mma layout\");\n-  assert((isVolta() || isAmpere()) && \"Only version 1 and 2 is supported\");\n+  assert((isVolta() || isAmpere() || isHopper()) &&\n+         \"For MmaEncodingAttr only version 1~3 is supported\");\n+\n+  auto shapePerCTA = getShapePerCTA(getCTALayout().getCTASplitNum(), shape);\n \n   SmallVector<unsigned> elemsPerThread(rank);\n   if (isVolta()) {\n@@ -517,22 +749,62 @@ MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape, Type eltTy) const {\n     unsigned spwN = fpw[1] * 4 * repN;\n     unsigned wptM = getWarpsPerCTA()[0];\n     unsigned wptN = getWarpsPerCTA()[1];\n-    unsigned resM = repM * std::max<int>(1, shape[0] / (spwM * wptM));\n-    unsigned resN = 2 * repN * std::max<int>(1, shape[1] / (spwN * wptN));\n+    unsigned resM = repM * std::max<int>(1, shapePerCTA[0] / (spwM * wptM));\n+    unsigned resN = 2 * repN * std::max<int>(1, shapePerCTA[1] / (spwN * wptN));\n     elemsPerThread[0] = resM;\n     elemsPerThread[1] = resN;\n   } else if (isAmpere()) {\n-    unsigned elemsRow = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n-    unsigned elemsCol = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n+    unsigned elemsRow =\n+        ceil<unsigned>(shapePerCTA[0], 16 * getWarpsPerCTA()[0]) * 2;\n+    unsigned elemsCol =\n+        ceil<unsigned>(shapePerCTA[1], 8 * getWarpsPerCTA()[1]) * 2;\n     elemsPerThread[0] = elemsRow;\n     elemsPerThread[1] = elemsCol;\n+  } else if (isHopper()) {\n+    auto wpt = getWarpsPerCTA();\n+    auto instrMNK = getInstrShape();\n+    int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n+    int repN = ceil<unsigned>(shapePerCTA[1], instrMNK[1] * wpt[1]);\n+    elemsPerThread[0] = 2 * repM;\n+    elemsPerThread[1] = (instrMNK[1] / 4) * repN;\n   } else {\n     llvm_unreachable(\"Unexpected mma version\");\n   }\n \n   return elemsPerThread;\n }\n \n+unsigned\n+MmaEncodingAttr::getElemsPerThreadOfOperand(int opIdx,\n+                                            ArrayRef<int64_t> shape) const {\n+  size_t rank = shape.size();\n+  assert(rank == 2 && \"Unexpected rank of mma layout\");\n+  auto shapePerCTA = getShapePerCTA(*this, shape);\n+  int res = 0;\n+  if (isVolta()) {\n+    llvm_unreachable(\n+        \"getElemsPerThreadOfOperand() not supported for version 1\");\n+  } else if (isAmpere()) {\n+    llvm_unreachable(\n+        \"getElemsPerThreadOfOperand() not supported for version 2\");\n+  } else if (isHopper()) {\n+    auto wpt = getWarpsPerCTA();\n+    auto instrMNK = getInstrShape();\n+    if (opIdx == 0) {\n+      int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n+      int repK = ceil<unsigned>(shapePerCTA[1], instrMNK[2]);\n+      return 8 * repM * repK;\n+\n+    } else if (opIdx == 1) {\n+      int repK = ceil<unsigned>(shapePerCTA[0], instrMNK[2]);\n+      int repN = ceil<unsigned>(shapePerCTA[1], instrMNK[1] * wpt[1]);\n+      // benzh@ here need more check\n+      return 4 * std::max<int>(instrMNK[1] / 32, 1) * repK * repN;\n+    }\n+  }\n+  return res;\n+}\n+\n unsigned MmaEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n                                                  Type eltTy) const {\n   return product<unsigned>(getElemsPerThread(shape, eltTy));\n@@ -580,7 +852,6 @@ unsigned DotOperandEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n   if (auto mmaParent = getParent().dyn_cast<MmaEncodingAttr>()) {\n     int warpsPerCTAM = mmaParent.getWarpsPerCTA()[0];\n     int warpsPerCTAN = mmaParent.getWarpsPerCTA()[1];\n-    // A100\n     if (mmaParent.isAmpere()) {\n       auto rep = getMMAv2Rep(shape, eltTy.getIntOrFloatBitWidth());\n       if (getOpIdx() == 0)\n@@ -650,7 +921,7 @@ unsigned DotOperandEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n     }\n   }\n   if (auto blockedLayout = getParent().dyn_cast<BlockedEncodingAttr>()) {\n-    auto shapePerCTA = getShapePerCTA(blockedLayout);\n+    auto shapePerCTATile = getShapePerCTATile(blockedLayout);\n     auto order = blockedLayout.getOrder();\n     auto sizePerThread = getSizePerThread(blockedLayout);\n \n@@ -665,13 +936,13 @@ unsigned DotOperandEncodingAttr::getTotalElemsPerThread(ArrayRef<int64_t> shape,\n         order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n     int sizePerThreadMN = isM ? mSizePerThread : nSizePerThread;\n \n-    int mShapePerCTA =\n-        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int nShapePerCTA =\n-        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int shapePerCTAMN = isM ? mShapePerCTA : nShapePerCTA;\n+    int mShapePerCTATile =\n+        order[0] == 1 ? shapePerCTATile[order[1]] : shapePerCTATile[order[0]];\n+    int nShapePerCTATile =\n+        order[0] == 0 ? shapePerCTATile[order[1]] : shapePerCTATile[order[0]];\n+    int shapePerCTAMNTile = isM ? mShapePerCTATile : nShapePerCTATile;\n \n-    return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n+    return K * std::max<int>(otherDim / shapePerCTAMNTile, 1) * sizePerThreadMN;\n   }\n   llvm_unreachable(\"unknown dot operand parent layout\");\n   return 0;\n@@ -691,10 +962,13 @@ Attribute BlockedEncodingAttr::parse(AsmParser &parser, Type type) {\n   if (parser.parseGreater().failed())\n     return {};\n \n-  SmallVector<unsigned, 2> sizePerThread;\n-  SmallVector<unsigned, 2> threadsPerWarp;\n-  SmallVector<unsigned, 2> warpsPerCTA;\n-  SmallVector<unsigned, 2> order;\n+  SmallVector<unsigned> sizePerThread;\n+  SmallVector<unsigned> threadsPerWarp;\n+  SmallVector<unsigned> warpsPerCTA;\n+  SmallVector<unsigned> order;\n+  SmallVector<unsigned> CTAsPerCGA;\n+  SmallVector<unsigned> CTASplitNum;\n+  SmallVector<unsigned> CTAOrder;\n \n   for (const NamedAttribute &attr : dict) {\n     if (attr.getName() == \"sizePerThread\") {\n@@ -715,16 +989,28 @@ Attribute BlockedEncodingAttr::parse(AsmParser &parser, Type type) {\n     } else if (attr.getName() == \"order\") {\n       if (parseIntArrayAttr(parser, attr, order, \"order\").failed())\n         return {};\n+    } else if (attr.getName() == \"CTAsPerCGA\") {\n+      if (parseIntArrayAttr(parser, attr, CTAsPerCGA, \"CTAsPerCGA\").failed())\n+        return {};\n+    } else if (attr.getName() == \"CTASplitNum\") {\n+      if (parseIntArrayAttr(parser, attr, CTASplitNum, \"CTASplitNum\").failed())\n+        return {};\n+    } else if (attr.getName() == \"CTAOrder\") {\n+      if (parseIntArrayAttr(parser, attr, CTAOrder, \"CTAOrder\").failed())\n+        return {};\n     } else {\n       parser.emitError(parser.getNameLoc(), \"unexpected key: \")\n           << attr.getName().strref();\n       return {};\n     }\n   }\n \n-  auto ret = parser.getChecked<BlockedEncodingAttr>(\n-      parser.getContext(), sizePerThread, threadsPerWarp, warpsPerCTA, order);\n-  return ret;\n+  auto CTALayout = CTALayoutAttr::get(parser.getContext(), CTAsPerCGA,\n+                                      CTASplitNum, CTAOrder);\n+\n+  return parser.getChecked<BlockedEncodingAttr>(parser.getContext(),\n+                                                sizePerThread, threadsPerWarp,\n+                                                warpsPerCTA, order, CTALayout);\n }\n \n void BlockedEncodingAttr::print(mlir::AsmPrinter &printer) const {\n@@ -733,6 +1019,9 @@ void BlockedEncodingAttr::print(mlir::AsmPrinter &printer) const {\n           << \", threadsPerWarp = [\" << getThreadsPerWarp() << \"]\"\n           << \", warpsPerCTA = [\" << getWarpsPerCTA() << \"]\"\n           << \", order = [\" << getOrder() << \"]\"\n+          << \", CTAsPerCGA = [\" << getCTALayout().getCTAsPerCGA() << \"]\"\n+          << \", CTASplitNum = [\" << getCTALayout().getCTASplitNum() << \"]\"\n+          << \", CTAOrder = [\" << getCTALayout().getCTAOrder() << \"]\"\n           << \"}>\";\n }\n \n@@ -751,7 +1040,11 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n \n   unsigned versionMajor = 0;\n   unsigned versionMinor = 0;\n-  SmallVector<unsigned, 2> warpsPerCTA;\n+  SmallVector<unsigned> warpsPerCTA;\n+  SmallVector<unsigned> CTAsPerCGA;\n+  SmallVector<unsigned> CTASplitNum;\n+  SmallVector<unsigned> CTAOrder;\n+  SmallVector<unsigned> instrShape;\n \n   for (const NamedAttribute &attr : dict) {\n     if (attr.getName() == \"versionMajor\") {\n@@ -766,17 +1059,42 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n       if (parseIntArrayAttr(parser, attr, warpsPerCTA, \"warpsPerCTA\").failed())\n         return {};\n     }\n+    if (attr.getName() == \"CTAsPerCGA\") {\n+      if (parseIntArrayAttr(parser, attr, CTAsPerCGA, \"CTAsPerCGA\").failed())\n+        return {};\n+    }\n+    if (attr.getName() == \"CTASplitNum\") {\n+      if (parseIntArrayAttr(parser, attr, CTASplitNum, \"CTASplitNum\").failed())\n+        return {};\n+    }\n+    if (attr.getName() == \"CTAOrder\") {\n+      if (parseIntArrayAttr(parser, attr, CTAOrder, \"CTAOrder\").failed())\n+        return {};\n+    }\n+    if (attr.getName() == \"instrShape\") {\n+      if (parseIntArrayAttr(parser, attr, instrShape, \"instrShape\").failed()) {\n+        return {};\n+      }\n+    }\n   }\n \n+  auto CTALayout = CTALayoutAttr::get(parser.getContext(), CTAsPerCGA,\n+                                      CTASplitNum, CTAOrder);\n+\n   return parser.getChecked<MmaEncodingAttr>(parser.getContext(), versionMajor,\n-                                            versionMinor, warpsPerCTA);\n+                                            versionMinor, warpsPerCTA,\n+                                            CTALayout, instrShape);\n }\n \n void MmaEncodingAttr::print(AsmPrinter &printer) const {\n   printer << \"<{\"\n           << \"versionMajor = \" << getVersionMajor() << \", \"\n           << \"versionMinor = \" << getVersionMinor() << \", \"\n-          << \"warpsPerCTA = [\" << getWarpsPerCTA() << \"]\"\n+          << \"warpsPerCTA = [\" << getWarpsPerCTA() << \"], \"\n+          << \"CTAsPerCGA = [\" << getCTALayout().getCTAsPerCGA() << \"], \"\n+          << \"CTASplitNum = [\" << getCTALayout().getCTASplitNum() << \"], \"\n+          << \"CTAOrder = [\" << getCTALayout().getCTAOrder() << \"], \"\n+          << \"instrShape = [\" << getInstrShape() << \"]\"\n           << \"}>\";\n }\n \n@@ -820,7 +1138,11 @@ Attribute SharedEncodingAttr::parse(AsmParser &parser, Type type) {\n   unsigned vec = 0;\n   unsigned perPhase = 0;\n   unsigned maxPhase = 0;\n-  SmallVector<unsigned, 2> order;\n+  SmallVector<unsigned> order;\n+  SmallVector<unsigned> CTAsPerCGA;\n+  SmallVector<unsigned> CTASplitNum;\n+  SmallVector<unsigned> CTAOrder;\n+  bool hasLeadingOffset = false;\n \n   for (const NamedAttribute &attr : dict) {\n     if (attr.getName() == \"vec\") {\n@@ -835,23 +1157,44 @@ Attribute SharedEncodingAttr::parse(AsmParser &parser, Type type) {\n     } else if (attr.getName() == \"order\") {\n       if (parseIntArrayAttr(parser, attr, order, \"order\").failed())\n         return {};\n+    } else if (attr.getName() == \"CTAsPerCGA\") {\n+      if (parseIntArrayAttr(parser, attr, CTAsPerCGA, \"CTAsPerCGA\").failed())\n+        return {};\n+    } else if (attr.getName() == \"CTASplitNum\") {\n+      if (parseIntArrayAttr(parser, attr, CTASplitNum, \"CTASplitNum\").failed())\n+        return {};\n+    } else if (attr.getName() == \"CTAOrder\") {\n+      if (parseIntArrayAttr(parser, attr, CTAOrder, \"CTAOrder\").failed())\n+        return {};\n+    } else if (attr.getName() == \"hasLeadingOffset\") {\n+      if (parseBool(parser, attr, hasLeadingOffset, \"hasLeadingOffset\")\n+              .failed())\n+        return {};\n     } else {\n       parser.emitError(parser.getNameLoc(), \"unexpected key: \")\n           << attr.getName().strref();\n       return {};\n     }\n   }\n \n+  auto CTALayout = CTALayoutAttr::get(parser.getContext(), CTAsPerCGA,\n+                                      CTASplitNum, CTAOrder);\n+\n   return parser.getChecked<SharedEncodingAttr>(parser.getContext(), vec,\n-                                               perPhase, maxPhase, order);\n+                                               perPhase, maxPhase, order,\n+                                               CTALayout, hasLeadingOffset);\n }\n \n void SharedEncodingAttr::print(AsmPrinter &printer) const {\n   printer << \"<{\"\n-          << \"vec = \" << getVec() << \", perPhase = \" << getPerPhase()\n-          << \", maxPhase = \" << getMaxPhase() << \", order = [\" << getOrder()\n-          << \"]\"\n-          << \"}>\";\n+          << \"vec = \" << getVec() << \", \"\n+          << \"perPhase = \" << getPerPhase() << \", \"\n+          << \"maxPhase = \" << getMaxPhase() << \", \"\n+          << \"order = [\" << getOrder() << \"], \"\n+          << \"CTAsPerCGA = [\" << getCTALayout().getCTAsPerCGA() << \"], \"\n+          << \"CTASplitNum = [\" << getCTALayout().getCTASplitNum() << \"], \"\n+          << \"CTAOrder = [\" << getCTALayout().getCTAOrder() << \"], \"\n+          << \"hasLeadingOffset = \" << getHasLeadingOffset() << \"}>\";\n }\n \n //===----------------------------------------------------------------------===//\n@@ -862,6 +1205,8 @@ bool MmaEncodingAttr::isVolta() const { return getVersionMajor() == 1; }\n \n bool MmaEncodingAttr::isAmpere() const { return getVersionMajor() == 2; }\n \n+bool MmaEncodingAttr::isHopper() const { return getVersionMajor() == 3; }\n+\n // Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n std::tuple<bool, bool, bool, bool, int>\n MmaEncodingAttr::decodeVoltaLayoutStates() const {\n@@ -968,11 +1313,11 @@ int DotOperandEncodingAttr::getMMAv1NumOuter(ArrayRef<int64_t> shape) const {\n }\n \n //===----------------------------------------------------------------------===//\n-// InsertSliceAsyncOp\n+// InsertSliceOp / InsertSliceAsyncOp\n //===----------------------------------------------------------------------===//\n \n-ParseResult InsertSliceAsyncOp::parse(OpAsmParser &parser,\n-                                      OperationState &result) {\n+template <class OpT>\n+ParseResult parseInsertSliceOp(OpAsmParser &parser, OperationState &result) {\n   SmallVector<OpAsmParser::UnresolvedOperand, 8> allOperands;\n   Type srcType, dstType;\n   SMLoc allOperandLoc = parser.getCurrentLocation();\n@@ -991,7 +1336,8 @@ ParseResult InsertSliceAsyncOp::parse(OpAsmParser &parser,\n \n   int hasMask = 0, hasOther = 0;\n   if (allOperands.size() >= 4) {\n-    operandTypes.push_back(triton::getI1SameShape(srcType)); // mask\n+    operandTypes.push_back(\n+        triton::getI1SameShapeFromTensorOrTensorPtr(srcType)); // mask\n     hasMask = 1;\n   }\n   if (allOperands.size() >= 5) {\n@@ -1004,24 +1350,43 @@ ParseResult InsertSliceAsyncOp::parse(OpAsmParser &parser,\n     return failure();\n \n   // Deduce operand_segment_sizes from the number of the operands.\n-  auto operand_segment_sizesAttrName =\n-      InsertSliceAsyncOp::getOperandSegmentSizesAttrName(result.name);\n+  auto operandSegmentSizesAttrName =\n+      OpT::getOperandSegmentSizesAttrName(result.name);\n   result.addAttribute(\n-      operand_segment_sizesAttrName,\n+      operandSegmentSizesAttrName,\n       parser.getBuilder().getDenseI32ArrayAttr({1, 1, 1, hasMask, hasOther}));\n   return success();\n }\n \n-void InsertSliceAsyncOp::print(OpAsmPrinter &printer) {\n+template <class OpT>\n+void printInsertSliceOp(OpAsmPrinter &printer, OpT insertSliceOp) {\n   printer << \" \";\n-  printer << getOperation()->getOperands();\n+  printer << insertSliceOp.getOperation()->getOperands();\n   // \"operand_segment_sizes\" can be deduced, so we don't print it.\n-  printer.printOptionalAttrDict(getOperation()->getAttrs(),\n-                                {getOperandSegmentSizesAttrName()});\n+  printer.printOptionalAttrDict(\n+      insertSliceOp->getAttrs(),\n+      {insertSliceOp.getOperandSegmentSizesAttrName()});\n   printer << \" : \";\n-  printer.printStrippedAttrOrType(getSrc().getType());\n+  printer.printStrippedAttrOrType(insertSliceOp.getSrc().getType());\n   printer << \" -> \";\n-  printer.printStrippedAttrOrType(getResult().getType());\n+  printer.printStrippedAttrOrType(insertSliceOp.getDst().getType());\n+}\n+\n+ParseResult InsertSliceOp::parse(OpAsmParser &parser, OperationState &result) {\n+  return parseInsertSliceOp<InsertSliceOp>(parser, result);\n+}\n+\n+void InsertSliceOp::print(OpAsmPrinter &printer) {\n+  printInsertSliceOp<InsertSliceOp>(printer, *this);\n+}\n+\n+ParseResult InsertSliceAsyncOp::parse(OpAsmParser &parser,\n+                                      OperationState &result) {\n+  return parseInsertSliceOp<InsertSliceAsyncOp>(parser, result);\n+}\n+\n+void InsertSliceAsyncOp::print(OpAsmPrinter &printer) {\n+  printInsertSliceOp<InsertSliceAsyncOp>(printer, *this);\n }\n \n //===----------------------------------------------------------------------===//\n@@ -1071,9 +1436,12 @@ struct TritonGPUInferLayoutInterface\n     SmallVector<unsigned> retOrder(sharedEncoding.getOrder().begin(),\n                                    sharedEncoding.getOrder().end());\n     std::reverse(retOrder.begin(), retOrder.end());\n+    // TODO(Qingyi): Need to check whether CTAOrder should also be reversed.\n+    // This is not a problem for tests where numCTAs = 1.\n     resultEncoding = SharedEncodingAttr::get(\n         getDialect()->getContext(), sharedEncoding.getVec(),\n-        sharedEncoding.getPerPhase(), sharedEncoding.getMaxPhase(), retOrder);\n+        sharedEncoding.getPerPhase(), sharedEncoding.getMaxPhase(), retOrder,\n+        sharedEncoding.getCTALayout(), sharedEncoding.getHasLeadingOffset());\n     return mlir::success();\n   }\n \n@@ -1092,11 +1460,17 @@ struct TritonGPUInferLayoutInterface\n     return success();\n   }\n \n-  LogicalResult\n-  inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n-                     Attribute retEncoding,\n-                     std::optional<Location> location) const override {\n-    if (auto dotOpEnc = operandEncoding.dyn_cast<DotOperandEncodingAttr>()) {\n+  LogicalResult inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n+                                   Attribute retEncoding,\n+                                   Optional<Location> location) const override {\n+    auto mmaRetEncoding = retEncoding.dyn_cast<MmaEncodingAttr>();\n+    if (mmaRetEncoding && mmaRetEncoding.isHopper()) {\n+      // TODO: support gmma when A/B does not reside in shared memory\n+      if (!operandEncoding.isa<SharedEncodingAttr>())\n+        return emitOptionalError(\n+            location, \"unexpected operand layout for MmaEncodingAttr v3\");\n+    } else if (auto dotOpEnc =\n+                   operandEncoding.dyn_cast<DotOperandEncodingAttr>()) {\n       if (opIdx != dotOpEnc.getOpIdx())\n         return emitOptionalError(location, \"Wrong opIdx\");\n       if (retEncoding != dotOpEnc.getParent())\n@@ -1138,6 +1512,20 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>() &&\n       srcType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n     return mlir::failure();\n+  // for hopper MMAv3\n+  if (!op.use_empty()) {\n+    bool hasDotUser = false;\n+    for (Operation *dot : op.getResult().getUsers())\n+      if (isa<triton::DotOp>(dot))\n+        hasDotUser = true;\n+\n+    if (hasDotUser) {\n+      if (dstType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n+          srcType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n+        return mlir::failure();\n+    }\n+  }\n+\n   // convert to the same layout -- we can delete\n   if (op->getResultTypes() == op->getOperandTypes()) {\n     rewriter.replaceOp(op, op->getOperands());\n@@ -1295,13 +1683,16 @@ void ExtractSliceOp::build(OpBuilder &b, OperationState &result,\n //===----------------------------------------------------------------------===//\n \n void TritonGPUDialect::initialize() {\n+  registerTypes();\n+\n   addAttributes<\n #define GET_ATTRDEF_LIST\n #include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.cpp.inc\"\n       >();\n   addOperations<\n #define GET_OP_LIST\n #include \"triton/Dialect/TritonGPU/IR/Ops.cpp.inc\"\n+#include \"triton/Dialect/TritonGPU/IR/OpsEnums.cpp.inc\"\n       >();\n   addInterfaces<TritonGPUOpAsmInterface>();\n   addInterfaces<TritonGPUInferLayoutInterface>();"}, {"filename": "lib/Dialect/TritonGPU/IR/Types.cpp", "status": "added", "additions": 38, "deletions": 0, "changes": 38, "file_content_changes": "@@ -0,0 +1,38 @@\n+#include \"triton/Dialect/TritonGPU/IR/Types.h\"\n+#include \"mlir/IR/DialectImplementation.h\" // required by `Types.cpp.inc`\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"llvm/ADT/TypeSwitch.h\" // required by `Types.cpp.inc`\n+\n+using namespace mlir;\n+using namespace mlir::triton::gpu;\n+\n+#define GET_TYPEDEF_CLASSES\n+#include \"triton/Dialect/TritonGPU/IR/Types.cpp.inc\"\n+\n+Type TokenType::parse(AsmParser &parser) {\n+  if (parser.parseLess())\n+    return Type();\n+\n+  int type = 1;\n+  if (parser.parseInteger(type))\n+    return Type();\n+\n+  if (parser.parseGreater())\n+    return Type();\n+\n+  return TokenType::get(parser.getContext(), type);\n+}\n+\n+void TokenType::print(AsmPrinter &printer) const {\n+  printer << \"<\" << getType() << \">\";\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Triton Dialect\n+//===----------------------------------------------------------------------===//\n+void ::mlir::triton::gpu::TritonGPUDialect::registerTypes() {\n+  addTypes<\n+#define GET_TYPEDEF_LIST\n+#include \"triton/Dialect/TritonGPU/IR/Types.cpp.inc\"\n+      >();\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 163, "deletions": 68, "changes": 231, "file_content_changes": "@@ -4,31 +4,43 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n+#include \"llvm/Support/Debug.h\"\n #include <memory>\n \n using namespace mlir;\n+namespace tt = mlir::triton;\n+namespace ttg = mlir::triton::gpu;\n namespace {\n-using triton::DotOp;\n-using triton::gpu::BlockedEncodingAttr;\n-using triton::gpu::ConvertLayoutOp;\n-using triton::gpu::DotOperandEncodingAttr;\n-using triton::gpu::MmaEncodingAttr;\n-using triton::gpu::SliceEncodingAttr;\n-\n-int computeCapabilityToMMAVersion(int computeCapability) {\n-  if (computeCapability < 70) {\n-    return 0;\n-  } else if (computeCapability < 80) {\n-    return 1;\n+using tt::DotOp;\n+using ttg::BlockedEncodingAttr;\n+using ttg::ConvertLayoutOp;\n+using ttg::DotOperandEncodingAttr;\n+using ttg::MmaEncodingAttr;\n+using ttg::SliceEncodingAttr;\n+\n+// higher mma version is prefered, will fallback to lower version if not\n+// supported\n+static int getMMAVersionSafe(int computeCapability, tt::DotOp op) {\n+  int baseVersion = 0;\n+  if (computeCapability < 80) {\n+    baseVersion = 1;\n   } else if (computeCapability < 90) {\n-    return 2;\n+    baseVersion = 2;\n   } else if (computeCapability < 100) {\n-    // FIXME: temporarily add this to pass unis tests\n-    return 2;\n+    baseVersion = 3;\n   } else {\n-    assert(false && \"computeCapability > 100 not supported\");\n-    return 3;\n+    assert(false && \"computeCapability not supported\");\n   }\n+\n+  for (; baseVersion >= 1; baseVersion--) {\n+    if (supportMMA(op, baseVersion)) {\n+      return baseVersion;\n+    }\n+  }\n+\n+  return 0;\n }\n \n SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n@@ -42,20 +54,23 @@ SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n   }\n }\n \n-SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n-                                        const ArrayRef<int64_t> shape,\n-                                        int numWarps) {\n+SmallVector<unsigned, 2>\n+warpsPerTileV2(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps) {\n   auto filter = [&dotOp](Operation *op) {\n     return op->getParentRegion() == dotOp->getParentRegion();\n   };\n   auto slices = mlir::getSlice(dotOp, {filter});\n   for (Operation *op : slices)\n-    if (isa<triton::DotOp>(op) && (op != dotOp))\n+    if (isa<tt::DotOp>(op) && (op != dotOp))\n       return {(unsigned)numWarps, 1};\n \n   SmallVector<unsigned, 2> ret = {1, 1};\n   SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n   bool changed = false;\n+  // TODO (@daadaada): double-check.\n+  // original logic in\n+  // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n+  // seems buggy for shape = [32, 16] ?\n   do {\n     changed = false;\n     if (ret[0] * ret[1] >= numWarps)\n@@ -73,14 +88,37 @@ SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n   return ret;\n }\n \n+SmallVector<unsigned, 2>\n+warpsPerTileV3(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps,\n+               const SmallVector<unsigned, 3> &instrShape) {\n+  SetVector<Operation *> slices;\n+  mlir::getForwardSlice(dotOp.getResult(), &slices);\n+  if (llvm::find_if(slices, [](Operation *op) { return isa<tt::DotOp>(op); }) !=\n+      slices.end())\n+    return {(unsigned)numWarps, 1};\n+\n+  // For MMAv3, the smallest indivisible unit of warp shape is (4, 1).\n+  SmallVector<unsigned, 2> ret = {4, 1};\n+  SmallVector<int64_t, 2> shapePerWarp = {16, instrShape[1]};\n+  do {\n+    if (ret[0] * ret[1] >= numWarps)\n+      break;\n+    if (shape[0] > shapePerWarp[0] * ret[0]) {\n+      ret[0] *= 2;\n+    } else {\n+      ret[1] *= 2;\n+    }\n+  } while (true);\n+  return ret;\n+}\n+\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n   mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n \n   static bool bwdFilter(Operation *op) {\n     return op->getNumOperands() == 1 &&\n-           (isa<triton::FpToFpOp, triton::BitcastOp,\n-                triton::gpu::ConvertLayoutOp>(op) ||\n+           (isa<tt::FpToFpOp, tt::BitcastOp, ttg::ConvertLayoutOp>(op) ||\n             op->getDialect()->getTypeID() ==\n                 mlir::TypeID::get<arith::ArithDialect>());\n   }\n@@ -102,39 +140,88 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n public:\n   BlockedToMMA(mlir::MLIRContext *context, int computeCapability)\n-      : mlir::RewritePattern(triton::DotOp::getOperationName(), 2, context),\n+      : mlir::RewritePattern(tt::DotOp::getOperationName(), 2, context),\n         computeCapability(computeCapability) {}\n \n+  static SmallVector<unsigned, 3>\n+  getWarpsPerTile(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int version,\n+                  int numWarps, const SmallVector<unsigned, 3> &instrShape) {\n+    switch (version) {\n+    case 2:\n+      return warpsPerTileV2(dotOp, shape, numWarps);\n+    case 3:\n+      return warpsPerTileV3(dotOp, shape, numWarps, instrShape);\n+    default:\n+      assert(false && \"not supported version\");\n+      return {0, 0};\n+    }\n+  }\n+\n+  static Value getMMAv3Operand(Value v, mlir::PatternRewriter &rewriter,\n+                               int opIdx) {\n+    auto cvtOp = dyn_cast_or_null<ttg::ConvertLayoutOp>(v.getDefiningOp());\n+    auto arg = cvtOp.getSrc();\n+    auto argType = arg.getType().cast<RankedTensorType>();\n+    auto eltType = argType.getElementType();\n+    assert(argType.getEncoding() && \"unexpected tensor type\");\n+    auto newOrder = ttg::getOrder(argType.getEncoding());\n+\n+    // MMAv3 with transpose only supports f16 and bf16 data type\n+    // fallback to MMAv3 without transpose for other data types\n+    if (!eltType.isF16() && !eltType.isBF16()) {\n+      if (opIdx == 1) {\n+        newOrder = {0, 1};\n+      } else {\n+        newOrder = {1, 0};\n+      }\n+    }\n+\n+    auto CTALayout = ttg::getCTALayout(argType.getEncoding());\n+    auto newLayout = ttg::SharedEncodingAttr::get(\n+        argType.getContext(), argType.getShape(), newOrder, CTALayout,\n+        argType.getElementType());\n+    auto newType = RankedTensorType::get(argType.getShape(),\n+                                         argType.getElementType(), newLayout);\n+\n+    return rewriter.create<ttg::ConvertLayoutOp>(arg.getLoc(), newType, arg);\n+  }\n+\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n     if (computeCapability < 70)\n       return failure();\n-    auto dotOp = cast<triton::DotOp>(op);\n+    auto dotOp = cast<tt::DotOp>(op);\n     auto ctx = op->getContext();\n     // TODO: Check data-types and SM compatibility\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n     if (!oldRetType.getEncoding() ||\n-        oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n+        oldRetType.getEncoding().isa<ttg::MmaEncodingAttr>())\n       return failure();\n \n-    // for FMA, should retain the blocked layout.\n-    int versionMajor = computeCapabilityToMMAVersion(computeCapability);\n-    if (!supportMMA(dotOp, versionMajor))\n-      return failure();\n+    auto AType = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n+    auto BType = dotOp.getOperand(1).getType().cast<RankedTensorType>();\n \n     // get MMA encoding for the given number of warps\n-    auto retShape = oldRetType.getShape();\n+    auto retShapePerCTA = ttg::getShapePerCTA(oldRetType);\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n-    int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+    int numWarps = ttg::TritonGPUDialect::getNumWarps(mod);\n+    auto CTALayout = ttg::getCTALayout(oldRetType.getEncoding());\n+\n+    int versionMajor = getMMAVersionSafe(computeCapability, dotOp);\n+    if (!versionMajor)\n+      return failure();\n+\n+    auto instrShape =\n+        mmaVersionToInstrShape(versionMajor, retShapePerCTA, AType);\n \n     // operands\n     Value a = dotOp.getA();\n     Value b = dotOp.getB();\n     auto oldAType = a.getType().cast<RankedTensorType>();\n     auto oldBType = b.getType().cast<RankedTensorType>();\n \n-    triton::gpu::MmaEncodingAttr mmaEnc;\n+    ttg::MmaEncodingAttr mmaEnc;\n     if (versionMajor == 1) {\n       SetVector<Operation *> aBwdSlices, bBwdSlices;\n       auto isCvt = [](Operation *op) { return isa<ConvertLayoutOp>(op); };\n@@ -163,46 +250,54 @@ class BlockedToMMA : public mlir::RewritePattern {\n       if (bOp)\n         isBRow = getCvtArgOrder(bOp)[0] == 1;\n \n-      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-          oldRetType.getContext(), versionMajor, numWarps, oldAType.getShape(),\n-          oldBType.getShape(), retShape, isARow, isBRow, mmaV1Counter++);\n-    } else if (versionMajor == 2) {\n-      auto warpsPerTile = warpsPerTileV2(dotOp, retShape, numWarps);\n-      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-          oldRetType.getContext(), versionMajor, 0 /*versionMinor*/,\n-          warpsPerTile);\n-    } else {\n-      llvm_unreachable(\"Mma layout only supports versionMajor in {1, 2}\");\n+      mmaEnc = ttg::MmaEncodingAttr::get(\n+          oldRetType.getContext(), versionMajor, numWarps, CTALayout,\n+          instrShape, oldAType.getShape(), oldBType.getShape(), retShapePerCTA,\n+          isARow, isBRow, mmaV1Counter++);\n+    } else if (versionMajor == 2 || versionMajor == 3) {\n+      auto warpsPerTile = getWarpsPerTile(dotOp, retShapePerCTA, versionMajor,\n+                                          numWarps, instrShape);\n+      mmaEnc = ttg::MmaEncodingAttr::get(oldRetType.getContext(), versionMajor,\n+                                         0 /*versionMinor*/, warpsPerTile,\n+                                         CTALayout, instrShape);\n     }\n-    auto newRetType =\n-        RankedTensorType::get(retShape, oldRetType.getElementType(), mmaEnc);\n+    auto newRetType = RankedTensorType::get(\n+        oldRetType.getShape(), oldRetType.getElementType(), mmaEnc);\n     // convert accumulator\n     auto oldAcc = dotOp.getOperand(2);\n-    auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        oldAcc.getLoc(), newRetType, oldAcc);\n-    // convert operands\n-    int minBitwidth = std::min(computeOrigBitWidth(a), computeOrigBitWidth(b));\n-    Type minType = IntegerType::get(ctx, minBitwidth);\n-    // convert A operand\n-    auto newAEncoding = triton::gpu::DotOperandEncodingAttr::get(\n-        oldAType.getContext(), 0, newRetType.getEncoding(),\n-        minBitwidth > 0 ? minType : oldAType.getElementType());\n-    auto newAType = RankedTensorType::get(\n-        oldAType.getShape(), oldAType.getElementType(), newAEncoding);\n-    a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n-    // convert B operand\n-    auto newBEncoding = triton::gpu::DotOperandEncodingAttr::get(\n-        oldBType.getContext(), 1, newRetType.getEncoding(),\n-        minBitwidth > 0 ? minType : oldBType.getElementType());\n-    auto newBType = RankedTensorType::get(\n-        oldBType.getShape(), oldBType.getElementType(), newBEncoding);\n-    b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n+    auto newAcc = rewriter.create<ttg::ConvertLayoutOp>(oldAcc.getLoc(),\n+                                                        newRetType, oldAcc);\n+\n+    if (versionMajor == 3) {\n+      a = getMMAv3Operand(a, rewriter, 0);\n+      b = getMMAv3Operand(b, rewriter, 1);\n+    } else {\n+\n+      // convert operands\n+      int minBitwidth =\n+          std::min(computeOrigBitWidth(a), computeOrigBitWidth(b));\n+      Type minType = IntegerType::get(ctx, minBitwidth);\n+      // convert A operand\n+      auto newAEncoding = ttg::DotOperandEncodingAttr::get(\n+          oldAType.getContext(), 0, newRetType.getEncoding(),\n+          minBitwidth > 0 ? minType : oldAType.getElementType());\n+      auto newAType = RankedTensorType::get(\n+          oldAType.getShape(), oldAType.getElementType(), newAEncoding);\n+      a = rewriter.create<ttg::ConvertLayoutOp>(a.getLoc(), newAType, a);\n+      // convert B operand\n+      auto newBEncoding = ttg::DotOperandEncodingAttr::get(\n+          oldBType.getContext(), 1, newRetType.getEncoding(),\n+          minBitwidth > 0 ? minType : oldBType.getElementType());\n+      auto newBType = RankedTensorType::get(\n+          oldBType.getShape(), oldBType.getElementType(), newBEncoding);\n+      b = rewriter.create<ttg::ConvertLayoutOp>(b.getLoc(), newBType, b);\n+    }\n     // convert dot instruction\n-    auto newDot = rewriter.create<triton::DotOp>(\n-        dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.getAllowTF32());\n+    auto newDot = rewriter.create<tt::DotOp>(dotOp.getLoc(), newRetType, a, b,\n+                                             newAcc, dotOp.getAllowTF32());\n \n-    rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n-        op, oldRetType, newDot.getResult());\n+    rewriter.replaceOpWithNewOp<ttg::ConvertLayoutOp>(op, oldRetType,\n+                                                      newDot.getResult());\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -3,6 +3,7 @@ add_mlir_dialect_library(TritonGPUTransforms\n   Coalesce.cpp\n   DecomposeConversions.cpp\n   OptimizeDotOperands.cpp\n+  OptimizeEpilogue.cpp\n   Pipeline.cpp\n   Prefetch.cpp\n   RemoveLayoutConversions.cpp\n@@ -19,4 +20,6 @@ add_mlir_dialect_library(TritonGPUTransforms\n   TritonAnalysis\n   TritonIR\n   TritonGPUIR\n+  TritonNvidiaGPUIR\n+  MLIRTransformUtils\n )"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 127, "deletions": 39, "changes": 166, "file_content_changes": "@@ -3,6 +3,8 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"llvm/Support/Debug.h\"\n+#include <iterator>\n #include <numeric>\n \n using namespace mlir;\n@@ -24,94 +26,156 @@ typedef DenseMap<Value, std::function<Type(Type)>> LayoutMap;\n struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n   Attribute getCoalescedEncoding(ModuleAxisInfoAnalysis &axisInfoAnalysis,\n                                  Value ptr, int numWarps, int threadsPerWarp) {\n-    auto origType = ptr.getType().cast<RankedTensorType>();\n-    // Get the shape of the tensor.\n-    size_t rank = origType.getRank();\n+    auto refType = ptr.getType();\n+    if (refType.isa<PointerType>())\n+      refType = refType.cast<PointerType>().getPointeeType();\n+    auto refTensorType = refType.cast<RankedTensorType>();\n+\n+    // TODO(Keren): integrate it into AxisInfoAnalysis\n+    // Get axis info\n+    auto queryAxisInfo = [&](const Value &val) -> AxisInfo {\n+      auto valType = val.getType();\n+      // Tensor pointer\n+      // TODO(Chenggang): encoding for tensor pointers is meaningless, remove\n+      // these later while merging into the GitHub main\n+      if (auto ptrType = valType.dyn_cast<PointerType>()) {\n+        auto tensorTy = ptrType.getPointeeType().dyn_cast<RankedTensorType>();\n+        assert(tensorTy);\n+        auto makeTensorPtr = getMakeTensorPtrOp(val);\n+        auto order = makeTensorPtr.getOrder();\n+        auto tileShape = triton::gpu::getShapePerCTA(tensorTy);\n+        size_t rank = order.size();\n+        auto elemSizeInBytes =\n+            tensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+        SmallVector<int64_t> contiguity(rank, 1);\n+        SmallVector<int64_t> divisibility(rank, 1);\n+        SmallVector<int64_t> constancy(rank, 1);\n+        // The contiguity in `order[0]` is `tileShape[order[0]]`\n+        // The divisibility in `order[0]` is 16\n+        // TODO[goostavz]: confirm the legality of it\n+        contiguity[order[0]] = tileShape[order[0]];\n+        divisibility[order[0]] = 16 * 8 / elemSizeInBytes;\n+        return AxisInfo(contiguity, divisibility, constancy);\n+      }\n+      // Normal cases\n+      assert(valType.isa<RankedTensorType>());\n+      return *axisInfoAnalysis.getAxisInfo(val);\n+    };\n+\n     // Get the contiguity order of `ptr`\n-    auto order = argSort(axisInfoAnalysis.getAxisInfo(ptr)->getContiguity());\n+    SmallVector<unsigned> order;\n+    if (auto ptrType = ptr.getType().dyn_cast<PointerType>()) {\n+      // Tensor pointer\n+      auto makeTensorPtr = getMakeTensorPtrOp(ptr);\n+      std::copy(makeTensorPtr.getOrder().begin(),\n+                makeTensorPtr.getOrder().end(), std::back_inserter(order));\n+    } else {\n+      // Normal cases\n+      order = argSort(queryAxisInfo(ptr).getContiguity());\n+    }\n+\n     // The desired divisibility is the maximum divisibility\n     // among all dependent pointers who have the same order as\n-    // `ptr`\n+    // `ptr`.\n+    // We only do it for normal tensors of pointers, not tensor pointers.\n     SetVector<Value> withSameOrder;\n     withSameOrder.insert(ptr);\n-    if (ptr.getDefiningOp())\n+    if (refType.isa<RankedTensorType>() && ptr.getDefiningOp()) {\n       for (Operation *op : mlir::multiRootGetSlice(ptr.getDefiningOp())) {\n         for (Value val : op->getResults()) {\n-          if (val.getType() != origType)\n+          if (val.getType() != refTensorType)\n             continue;\n           auto currOrder =\n               argSort(axisInfoAnalysis.getAxisInfo(val)->getContiguity());\n           if (order == currOrder)\n             withSameOrder.insert(val);\n         }\n       }\n-    int numElems = product(origType.getShape());\n+    }\n+\n+    auto shapePerCTA = triton::gpu::getShapePerCTA(refTensorType);\n+    int numElems = product<int64_t>(shapePerCTA);\n     int numThreads = numWarps * threadsPerWarp;\n     int numElemsPerThread = std::max(numElems / numThreads, 1);\n+\n+    // For tensor of pointers, the element to access is the pointee type;\n+    // while for tensor pointer type (`refType` is directly the final shape),\n+    // the element to access is itself.\n+    auto typeForMem = refTensorType.getElementType().isa<PointerType>()\n+                          ? refTensorType.getElementType()\n+                                .cast<PointerType>()\n+                                .getPointeeType()\n+                          : refTensorType.getElementType();\n+\n     // Thread tile size depends on memory alignment\n-    SmallVector<unsigned, 4> sizePerThread(rank, 1);\n-    unsigned elemNumBits = triton::getPointeeBitWidth(origType);\n+    SmallVector<unsigned, 4> sizePerThread(refTensorType.getRank(), 1);\n+    unsigned elemNumBits = typeForMem.getIntOrFloatBitWidth();\n     unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n     unsigned perThread = 1;\n     for (Value val : withSameOrder) {\n-      unsigned maxMultipleBytes =\n-          axisInfoAnalysis.getAxisInfo(val)->getDivisibility(order[0]);\n+      auto valInfo = queryAxisInfo(val);\n+      unsigned maxMultipleBytes = valInfo.getDivisibility(order[0]);\n       unsigned maxMultiple = std::max(maxMultipleBytes / elemNumBytes, 1u);\n       unsigned maxContig =\n-          axisInfoAnalysis.getAxisInfo(val)->getContiguity(order[0]);\n+          std::min(valInfo.getContiguity(order[0]), shapePerCTA[order[0]]);\n       unsigned alignment = std::min(maxMultiple, maxContig);\n       unsigned currPerThread = std::min(alignment, 128 / elemNumBits);\n       perThread = std::max(perThread, currPerThread);\n     }\n     sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n-    SmallVector<unsigned> dims(rank);\n-    std::iota(dims.begin(), dims.end(), 0);\n-    // create encoding\n-    Attribute encoding = triton::gpu::BlockedEncodingAttr::get(\n-        &getContext(), origType.getShape(), sizePerThread, order, numWarps,\n-        threadsPerWarp);\n-    return encoding;\n+\n+    auto CTALayout = triton::gpu::getCTALayout(refTensorType.getEncoding());\n+    return triton::gpu::BlockedEncodingAttr::get(\n+        &getContext(), refTensorType.getShape(), sizePerThread, order, numWarps,\n+        threadsPerWarp, CTALayout);\n   }\n \n   std::function<Type(Type)>\n   getTypeConverter(ModuleAxisInfoAnalysis &axisInfoAnalysis, Value ptr,\n                    int numWarps, int threadsPerWarp) {\n     Attribute encoding =\n         getCoalescedEncoding(axisInfoAnalysis, ptr, numWarps, threadsPerWarp);\n-    return [encoding](Type _type) {\n-      RankedTensorType type = _type.cast<RankedTensorType>();\n-      return RankedTensorType::get(type.getShape(), type.getElementType(),\n-                                   encoding);\n+    return [encoding](Type type) {\n+      RankedTensorType tensorType = type.cast<RankedTensorType>();\n+      return RankedTensorType::get(tensorType.getShape(),\n+                                   tensorType.getElementType(), encoding);\n     };\n   }\n \n   template <class T>\n   void coalesceOp(LayoutMap &layoutMap, Operation *op, Value ptr,\n                   OpBuilder builder) {\n-    RankedTensorType ty = ptr.getType().template dyn_cast<RankedTensorType>();\n-    if (!ty)\n+    if (!layoutMap.count(ptr))\n       return;\n+\n+    // Convert operands\n+    // For load/store with tensor pointers, we don't have to change the\n+    // operands' type, we do this by changing the outputs' type of\n+    // `make_tensor_ptr`\n     auto convertType = layoutMap.lookup(ptr);\n-    // convert operands\n     SmallVector<Value, 4> newArgs;\n-    for (auto v : op->getOperands()) {\n-      auto vTy = v.getType().dyn_cast<RankedTensorType>();\n-      if (vTy && !vTy.getEncoding().isa<triton::gpu::SharedEncodingAttr>())\n+    for (auto operand : op->getOperands()) {\n+      auto tensorType = operand.getType().dyn_cast<RankedTensorType>();\n+      if (tensorType &&\n+          !tensorType.getEncoding().isa<triton::gpu::SharedEncodingAttr>())\n         newArgs.push_back(builder.create<triton::gpu::ConvertLayoutOp>(\n-            op->getLoc(), convertType(v.getType()), v));\n+            op->getLoc(), convertType(tensorType), operand));\n       else\n-        newArgs.push_back(v);\n+        newArgs.push_back(operand);\n     }\n-    // convert output types\n+\n+    // Convert output types\n     SmallVector<Type, 4> newTypes;\n     for (auto t : op->getResultTypes()) {\n-      bool is_async = std::is_same<T, triton::gpu::InsertSliceAsyncOp>::value;\n-      newTypes.push_back(is_async ? t : convertType(t));\n+      bool isAsync = std::is_same<T, triton::gpu::InsertSliceAsyncOp>::value;\n+      newTypes.push_back(isAsync ? t : convertType(t));\n     }\n-    // construct new op with the new encoding\n+\n+    // Construct new op with the new encoding\n     Operation *newOp =\n         builder.create<T>(op->getLoc(), newTypes, newArgs, op->getAttrs());\n-    // cast the results back to the original layout\n+\n+    // Cast the results back to the original layout\n     for (size_t i = 0; i < op->getNumResults(); i++) {\n       Value newResult = newOp->getResult(i);\n       if (newTypes[i] != op->getResultTypes()[i]) {\n@@ -123,6 +187,25 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     op->erase();\n   }\n \n+  void coalesceMakeTensorPtrOpResult(LayoutMap &layoutMap, Operation *op,\n+                                     Value ptr, OpBuilder builder) {\n+    if (!layoutMap.count(ptr))\n+      return;\n+\n+    // Convert result type\n+    auto convertType = layoutMap.lookup(ptr);\n+    auto ptrType = ptr.getType().cast<PointerType>();\n+    auto resultTensorType = convertType(ptrType.getPointeeType());\n+    auto newResultType =\n+        PointerType::get(resultTensorType, ptrType.getAddressSpace());\n+\n+    // Build new operation and replace\n+    Operation *newOp = builder.create<MakeTensorPtrOp>(\n+        op->getLoc(), newResultType, op->getOperands(), op->getAttrs());\n+    op->getResult(0).replaceAllUsesWith(newOp->getResult(0));\n+    op->erase();\n+  }\n+\n   void runOnOperation() override {\n     // Run axis info analysis\n     ModuleOp moduleOp = getOperation();\n@@ -145,8 +228,13 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n         ptr = op.getPtr();\n       if (!ptr)\n         return;\n-      RankedTensorType ty = ptr.getType().template dyn_cast<RankedTensorType>();\n-      if (!ty || !ty.getElementType().isa<PointerType>())\n+      // We only convert `tensor<tt.ptr<>>` or `tt.ptr<tensor<>>` load/store\n+      bool isPtrTensor = false, isTensorPointer = false;\n+      if (auto tensorType = ptr.getType().dyn_cast<RankedTensorType>())\n+        isPtrTensor = tensorType.getElementType().isa<PointerType>();\n+      if (auto ptrType = ptr.getType().dyn_cast<PointerType>())\n+        isTensorPointer = ptrType.getPointeeType().isa<RankedTensorType>();\n+      if (!isPtrTensor && !isTensorPointer)\n         return;\n       auto mod = curr->getParentOfType<ModuleOp>();\n       int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/DecomposeConversions.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -52,7 +52,9 @@ class TritonGPUDecomposeConversionsPass\n           dstType.getShape(), dstType.getElementType(),\n           triton::gpu::SharedEncodingAttr::get(\n               mod.getContext(), dstDotOp, srcType.getShape(),\n-              triton::gpu::getOrder(srcEncoding), srcType.getElementType()));\n+              triton::gpu::getOrder(srcEncoding),\n+              triton::gpu::getCTALayout(srcEncoding),\n+              srcType.getElementType()));\n       auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n           cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n       auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>("}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 82, "deletions": 1, "changes": 83, "file_content_changes": "@@ -56,10 +56,13 @@ class ConvertTransConvert : public mlir::RewritePattern {\n     if (!ZEncoding)\n       return mlir::failure();\n     // new X encoding\n+    // TODO(Qingyi): need to check whether the CTALayout of XEncoding should be\n+    // used here. For tests where numCTAs = 1, this is not a problem since all\n+    // CTALayouts are the same.\n     auto newXOrder = triton::gpu::getOrder(argEncoding);\n     auto newXEncoding = triton::gpu::SharedEncodingAttr::get(\n         getContext(), ZEncoding, XType.getShape(), newXOrder,\n-        XType.getElementType());\n+        XEncoding.getCTALayout(), XType.getElementType());\n     auto newXType = RankedTensorType::get(XType.getShape(),\n                                           XType.getElementType(), newXEncoding);\n     if (XEncoding == newXEncoding)\n@@ -143,6 +146,83 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n   }\n };\n \n+// convert(trans(convert(arg)))\n+// x = convert_layout arg: #distributed -> #shared_x\n+// y = trans x: #shared_x -> #shared_y\n+// z = convert_layout y: #shared_y -> #shared_z\n+class FuseTransHopper : public mlir::RewritePattern {\n+\n+public:\n+  FuseTransHopper(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             1, context) {}\n+\n+  LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    if (!op->hasOneUse())\n+      return mlir::failure();\n+    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto tmpOp =\n+        dyn_cast_or_null<triton::TransOp>(dstOp.getSrc().getDefiningOp());\n+    if (!tmpOp)\n+      return mlir::failure();\n+    auto srcOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n+        tmpOp.getSrc().getDefiningOp());\n+    if (!srcOp)\n+      return mlir::failure();\n+    auto arg = srcOp.getSrc();\n+    auto X = tmpOp.getSrc();\n+    // types\n+    auto argType = arg.getType().cast<RankedTensorType>();\n+    auto XType = X.getType().cast<RankedTensorType>();\n+    auto ZType = dstOp.getResult().getType().cast<RankedTensorType>();\n+    // encodings\n+    auto argEncoding = argType.getEncoding();\n+    auto XEncoding =\n+        XType.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n+    auto ZEncoding =\n+        ZType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n+    if (!ZEncoding)\n+      return mlir::failure();\n+    // new X encoding\n+    auto newXOrder = triton::gpu::getOrder(argEncoding);\n+\n+    auto dotOp = *op->getUsers().begin();\n+    if (isa<triton::DotOp, triton::nvidia_gpu::DotAsyncOp>(dotOp)) {\n+      auto dotTy = dotOp->getResult(0).getType().cast<RankedTensorType>();\n+      auto dotEncoding =\n+          dotTy.getEncoding().dyn_cast<triton::gpu::MmaEncodingAttr>();\n+      auto eltType = XType.getElementType();\n+      if (!dotEncoding || dotEncoding.getVersionMajor() != 3)\n+        return mlir::failure();\n+      // MMAv3 with transpose only supports f16 and bf16 data type\n+      // fallback to MMAv3 without transpose for other data types\n+      if (!eltType.isF16() && !eltType.isBF16()) {\n+        if (dstOp.getResult() == dotOp->getOperand(0)) {\n+          newXOrder = {0, 1};\n+        } else if (dstOp.getResult() == dotOp->getOperand(1)) {\n+          newXOrder = {1, 0};\n+        }\n+      }\n+    }\n+\n+    // TODO(Qingyi): need to check whether the CTALayout of XEncoding should be\n+    // used here. For tests where numCTAs = 1, this is not a problem since all\n+    // CTALayouts are the same.\n+    auto newXEncoding = triton::gpu::SharedEncodingAttr::get(\n+        getContext(), XType.getShape(), newXOrder, XEncoding.getCTALayout(),\n+        XType.getElementType());\n+    auto newXType = RankedTensorType::get(XType.getShape(),\n+                                          XType.getElementType(), newXEncoding);\n+\n+    auto newX = rewriter.create<triton::gpu::ConvertLayoutOp>(srcOp.getLoc(),\n+                                                              newXType, arg);\n+    rewriter.replaceOpWithNewOp<triton::TransOp>(dstOp, newX);\n+    return mlir::success();\n+  }\n+};\n+\n } // namespace\n \n #define GEN_PASS_CLASSES\n@@ -165,6 +245,7 @@ class TritonGPUOptimizeDotOperandsPass\n     mlir::RewritePatternSet patterns(context);\n     patterns.add<ConvertTransConvert>(context);\n     patterns.add<MoveOpAfterLayoutConversion>(context);\n+    patterns.add<FuseTransHopper>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();\n     if (fixupLoops(m).failed())"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeEpilogue.cpp", "status": "added", "additions": 138, "deletions": 0, "changes": 138, "file_content_changes": "@@ -0,0 +1,138 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Pass/PassManager.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"mlir/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+\n+using namespace mlir;\n+\n+namespace {\n+\n+// convert(val) : mma -> blocked\n+// tt.store(ptr, val, mask, ...) : blocked\n+// ==>\n+// convert(ptr) : blocked -> mma\n+// convert(mask) : blocked -> mma\n+// tt.store(ptr, val, mask, ...) : mma\n+//\n+// Store with mma layout directly\n+class BypassEpilogueSMEM : public mlir::RewritePattern {\n+\n+public:\n+  explicit BypassEpilogueSMEM(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::StoreOp::getOperationName(), 1, context) {}\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+\n+    auto stOp = dyn_cast<triton::StoreOp>(op);\n+    if (!stOp)\n+      return mlir::failure();\n+    Value ptr = stOp.getPtr();\n+    Value val = stOp.getValue();\n+    Value mask = stOp.getMask();\n+    auto ptrType = ptr.getType().dyn_cast<RankedTensorType>();\n+    auto valType = val.getType().dyn_cast<RankedTensorType>();\n+    if (!ptrType || !valType ||\n+        !ptrType.getEncoding().isa<triton::gpu::BlockedEncodingAttr>() ||\n+        !valType.getEncoding().isa<triton::gpu::BlockedEncodingAttr>())\n+      return mlir::failure();\n+\n+    auto cvtOp = dyn_cast<triton::gpu::ConvertLayoutOp>(val.getDefiningOp());\n+    if (!cvtOp)\n+      return mlir::failure();\n+\n+    if (!cvtOp.getSrc()\n+             .getType()\n+             .cast<RankedTensorType>()\n+             .getEncoding()\n+             .isa<triton::gpu::MmaEncodingAttr>())\n+      return mlir::failure();\n+\n+    if (!cvtOp.getResult().hasOneUse())\n+      return mlir::failure();\n+\n+    auto newEncoding =\n+        cvtOp.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+\n+    auto newVal = cvtOp.getOperand();\n+\n+    auto newPtrType = RankedTensorType::get(\n+        ptrType.getShape(), ptrType.getElementType(), newEncoding);\n+    Value newPtr = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        ptr.getLoc(), newPtrType, ptr);\n+\n+    Value newMask = mask;\n+    if (mask) {\n+      auto maskType = mask.getType().dyn_cast<RankedTensorType>();\n+      auto newMaskType = RankedTensorType::get(\n+          maskType.getShape(), maskType.getElementType(), newEncoding);\n+      newMask = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+          mask.getLoc(), newMaskType, mask);\n+    }\n+\n+    rewriter.replaceOpWithNewOp<triton::StoreOp>(\n+        stOp, newPtr, newVal, newMask, stOp.getCache(), stOp.getEvict());\n+    return mlir::success();\n+  }\n+};\n+\n+} // namespace\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+class TritonGPUOptimizeEpiloguePass\n+    : public TritonGPUOptimizeEpilogueBase<TritonGPUOptimizeEpiloguePass> {\n+\n+public:\n+  TritonGPUOptimizeEpiloguePass() = default;\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp m = getOperation();\n+\n+    mlir::RewritePatternSet patterns(context);\n+\n+    patterns.add<BypassEpilogueSMEM>(context);\n+\n+    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n+      signalPassFailure();\n+    }\n+    if (fixupLoops(m).failed()) {\n+      signalPassFailure();\n+    }\n+  }\n+};\n+\n+std::unique_ptr<Pass> mlir::createTritonGPUOptimizeEpiloguePass() {\n+  return std::make_unique<TritonGPUOptimizeEpiloguePass>();\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 919, "deletions": 140, "changes": 1059, "file_content_changes": "@@ -8,7 +8,10 @@\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"llvm/ADT/MapVector.h\"\n+#include \"llvm/Support/Debug.h\"\n \n //===----------------------------------------------------------------------===//\n // This file implements software pipelining for loops. The implementation here\n@@ -78,7 +81,10 @@\n \n using llvm::MapVector;\n using namespace mlir;\n-namespace ttg = triton::gpu;\n+namespace tt = mlir::triton;\n+namespace ttg = mlir::triton::gpu;\n+/// FIXME(Keren): The pipeline pass shouldn't be aware of nvidia_gpu dialect\n+namespace ttng = mlir::triton::nvidia_gpu;\n \n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n@@ -104,6 +110,19 @@ void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n   }\n }\n \n+struct ConsumerReleaseInfo {\n+  Value iterVar;\n+  Value stageVar;\n+  Value phaseVar;\n+  Value nextIVVar;\n+  Value stepVar;\n+  Value upperBoundVar;\n+  ttg::CTALayoutAttr CTALayout;\n+  DenseMap</*consumer=*/Operation *, /*stage=*/int> consumerStageMap;\n+};\n+typedef DenseMap</*mbarrierTensor=*/Value, ConsumerReleaseInfo>\n+    ConsumerReleaseMap;\n+\n class LoopPipeliner {\n   /// Cache of ForOp and YieldOp related to this pipeliner.\n   scf::ForOp forOp;\n@@ -122,10 +141,35 @@ class LoopPipeliner {\n   /// load => after extract\n   DenseMap<Value, Value> loadsExtract;\n \n+  /// XXX(Keren): The following are h100 only and disabled\n+  /// load => full barrier arrive\n+  DenseMap<Value, Operation *> loadsBarrierArvOp;\n+  /// load => mbarriers\n+  DenseMap<Value, Value> loadsFullBarriers;\n+  DenseMap<Value, Value> loadsEmptyBarriers;\n+  /// load => null value or previous load which can share barrier with\n+  DenseMap<Value, Value> loadsCanShareBarriers;\n+  /// Maintains the information to emit consumer_release mbarrier_arrive\n+  ConsumerReleaseMap &consumerReleaseMap;\n+  bool hasHopperDot = false;\n+  // XXX(Keren): why the variable name is hopper dot and why do we need this\n+  // check?\n+  void checkHopperDots(SetVector<Operation *> &ops);\n+  // XXX(Keren): it looks more like an optimization to be, not sure if it should\n+  // exist in the base pipeliner\n+  void checkOpShareBarriers(SetVector<Operation *> &ops);\n+  int numLoadsRequireAsyncWait = 0;\n+  int numLoadsRequireMBarrier = 0;\n+\n   /// Iterator values\n+  Value nextIV;\n   Value pipelineIterIdx;\n+  Value curWaitIdx;\n+\n+  // Only needed when numLoadsRequireMBarrier > 0\n   Value loopIterIdx;\n-  Value nextIV;\n+  Value curPhase;\n+  Value curEmptyPhase;\n \n   /// Yield values\n   SmallVector<Value> nextBuffers;\n@@ -138,9 +182,16 @@ class LoopPipeliner {\n   int numStages;\n \n   /// Arg indicies\n-  size_t bufferIdx, loadIdx, depArgsBeginIdx, ivIndex;\n+  size_t bufferIdx, loadIdx, depArgsBeginIdx, ivIdx;\n   DenseMap<BlockArgument, size_t> depArgsIdx;\n \n+  /// XXX(Keren): The mode parameter is hacky, should be refactored\n+  // false: legacy mode as a temporary solution for backward compatibility\n+  // true: new mode for hopper\n+  bool mode;\n+  int numWarps;\n+  int numCTAs;\n+\n   /// value (in loop) => value at stage N\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n   /// loop iter arg => value\n@@ -204,12 +255,11 @@ class LoopPipeliner {\n \n   /// Get the load mask for `loadOp`, given the mapped mask `mappedMask` (if\n   /// exists) and the current iteration's `loopCond`.\n-  Value getLoadMask(triton::LoadOp loadOp, Value mappedMask, Value loopCond,\n+  Value getLoadMask(tt::LoadOp loadOp, Value mappedMask, Value loopCond,\n                     OpBuilder &builder);\n \n   /// Return an empty buffer of size <numStages, ...>\n-  ttg::AllocTensorOp allocateEmptyBuffer(triton::LoadOp loadOp,\n-                                         OpBuilder &builder);\n+  ttg::AllocTensorOp allocateEmptyBuffer(tt::LoadOp loadOp, OpBuilder &builder);\n \n   /// Collect all args of the new loop\n   SmallVector<Value> collectNewLoopArgs();\n@@ -220,15 +270,25 @@ class LoopPipeliner {\n   /// Prefetch the next iteration for `newForOp`\n   void prefetchNextIteration(scf::ForOp newForOp, OpBuilder &builder);\n \n+  /// Check if curIdx is out of bound and wrap value around if necessary\n+  Value getBoundedIterationValue(OpBuilder &builder, Value curIdx,\n+                                 Value upperBoundIdx, Value curValue,\n+                                 Value initValue);\n+\n   /// Assemble `newForOp`'s yield op\n   void finalizeYield(scf::ForOp newForOp, OpBuilder &builder);\n \n public:\n-  LoopPipeliner(scf::ForOp forOp, int numStages)\n-      : forOp(forOp), numStages(numStages) {\n+  LoopPipeliner(scf::ForOp forOp, int numStages, int numWarps, int numCTAs,\n+                bool mode, ConsumerReleaseMap &consumerReleaseMap)\n+      : forOp(forOp), numStages(numStages), numWarps(numWarps),\n+        numCTAs(numCTAs), mode(mode), consumerReleaseMap(consumerReleaseMap) {\n+    // cache yieldOp\n     yieldOp = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n   }\n \n+  LoopPipeliner() = delete;\n+\n   /// Collect loads to pipeline. Return success if we can pipeline this loop\n   LogicalResult initialize();\n \n@@ -252,27 +312,30 @@ LogicalResult LoopPipeliner::collectOps(SetVector<Operation *> &ops) {\n   // We cannot use forOp.walk(...) here because we only want to visit the\n   // operations in the loop body block. Nested blocks are handled separately.\n   for (Operation &op : forOp)\n-    if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n-      auto ptr = loadOp.getPtr();\n-      unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n-\n-      if (auto mask = loadOp.getMask())\n-        vec = std::min<unsigned>(vec, axisInfoAnalysis.getMaskAlignment(mask));\n-\n-      auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n-      if (!tensorTy || tensorTy.getRank() < 2)\n-        continue;\n-      auto ty = tensorTy.getElementType()\n-                    .cast<triton::PointerType>()\n-                    .getPointeeType();\n-      unsigned width = vec * ty.getIntOrFloatBitWidth();\n-      // We do not pipeline all loads for the following reasons:\n-      // 1. On nvidia GPUs, cp.async's cp-size can only be 4, 8 and 16.\n-      // 2. It's likely that pipling small loads won't offer much performance\n-      //    improvement and may even hurt performance by increasing register\n-      //    pressure.\n-      if (width >= 32)\n+    if (auto loadOp = dyn_cast<tt::LoadOp>(&op)) {\n+      if (isLoadFromTensorPtr(loadOp)) {\n         ops.insert(loadOp);\n+      } else {\n+        auto ptr = loadOp.getPtr();\n+        unsigned vec = axisInfoAnalysis.getPtrContiguity(ptr);\n+        if (auto mask = loadOp.getMask())\n+          vec =\n+              std::min<unsigned>(vec, axisInfoAnalysis.getMaskAlignment(mask));\n+\n+        auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+        if (!tensorTy || tensorTy.getRank() < 2)\n+          continue;\n+        auto ty =\n+            tensorTy.getElementType().cast<tt::PointerType>().getPointeeType();\n+        unsigned width = vec * ty.getIntOrFloatBitWidth();\n+        // We do not pipeline all loads for the following reasons:\n+        // 1. On nvidia GPUs, cp.async's cp-size can only be 4, 8 and 16.\n+        // 2. It's likely that pipling small loads won't offer much performance\n+        //    improvement and may even hurt performance by increasing register\n+        //    pressure.\n+        if (width >= 32)\n+          ops.insert(loadOp);\n+      }\n     }\n \n   if (ops.empty())\n@@ -324,23 +387,25 @@ LogicalResult LoopPipeliner::checkOpUses(SetVector<Operation *> &ops) {\n   collectDeps(ops, opDeps);\n \n   for (Operation *op : ops) {\n-    if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+    if (auto loadOp = dyn_cast<tt::LoadOp>(op)) {\n       // Don't pipeline valid loads that depend on other valid loads\n       // (Because if a valid load depends on another valid load, this load needs\n       // to wait on the other load in the prologue, which is against the point\n       // of the pipeline pass)\n       bool isCandidate = true;\n       for (Operation *other : ops)\n-        if (isa<triton::LoadOp>(other))\n+        if (isa<tt::LoadOp>(other))\n           if (opDeps[op].contains(other->getResult(0))) {\n             isCandidate = false;\n             break;\n           }\n       // We only pipeline loads that have one covert_layout (to dot_op) use\n       // TODO: lift this constraint in the future\n-      if (isCandidate && loadOp.getResult().hasOneUse()) {\n+      if (isCandidate && loadOp.getResult().hasOneUse() &&\n+          !isLoadFromTensorPtr(loadOp)) {\n         isCandidate = false;\n         Operation *use = *loadOp.getResult().getUsers().begin();\n+        Operation *preUse = nullptr;\n \n         // Advance to the first conversion as long as the use resides in shared\n         // memory and it has a single use itself\n@@ -351,10 +416,11 @@ LogicalResult LoopPipeliner::checkOpUses(SetVector<Operation *> &ops) {\n               use->getResult(0).getType().dyn_cast<RankedTensorType>();\n           if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n             break;\n+          preUse = use;\n           use = *use->getResult(0).getUsers().begin();\n         }\n \n-        if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use))\n+        if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use)) {\n           if (auto tensorType = convertLayout.getResult()\n                                     .getType()\n                                     .dyn_cast<RankedTensorType>())\n@@ -363,13 +429,43 @@ LogicalResult LoopPipeliner::checkOpUses(SetVector<Operation *> &ops) {\n               isCandidate = true;\n               loadsMapping[loadOp] = convertLayout;\n             }\n+        } else if (preUse && isa<tt::DotOp>(use)) {\n+          isCandidate = false;\n+          // for MMAv3 whose dot take SharedEncoding as operands directly\n+          Operation *post = *loadOp.getResult().getUsers().begin();\n+          auto newOrder = post->getResult(0)\n+                              .getType()\n+                              .cast<RankedTensorType>()\n+                              .getEncoding()\n+                              .cast<ttg::SharedEncodingAttr>()\n+                              .getOrder();\n+          auto ty = loadOp.getType().cast<RankedTensorType>();\n+          auto oldOrder = ttg::getOrder(ty.getEncoding());\n+          // The operand of MMAv3 is in SharedEncoding and it's order should not\n+          // be changed after FuseTranspositions Pass. So we only pipeline the\n+          // load if the order of the loaded BlockedEncoding is the same as the\n+          // order of the SharedEncoding it is converted to.\n+          // TODO: remove this constraint once the LoadOp supports transpose\n+          // fusion\n+          if (newOrder[0] == oldOrder[0] || newOrder[1] == oldOrder[1]) {\n+            isCandidate = true;\n+            loadsMapping[loadOp] = preUse->getResult(0);\n+          }\n+        }\n+      } else if (isCandidate && mode && isLoadFromTensorPtr(loadOp)) {\n+        loadsMapping[loadOp] = loadOp.getResult();\n       } else\n         isCandidate = false;\n \n       if (!isCandidate)\n         invalidOps.insert(loadOp);\n-      else\n+      else {\n         validLoads.insert(loadOp);\n+        if (!isLoadFromTensorPtr(loadOp))\n+          numLoadsRequireAsyncWait++;\n+        else\n+          numLoadsRequireMBarrier++;\n+      }\n     }\n   }\n \n@@ -382,6 +478,67 @@ LogicalResult LoopPipeliner::checkOpUses(SetVector<Operation *> &ops) {\n     return success();\n }\n \n+void LoopPipeliner::checkHopperDots(SetVector<Operation *> &ops) {\n+  // dots to be pipelined\n+  SetVector<Value> dots;\n+  for (Operation &op : forOp) {\n+    if (auto dotOp = dyn_cast<tt::DotOp>(&op)) {\n+      auto resTy = dotOp.getResult().getType().dyn_cast<RankedTensorType>();\n+      if (auto resEnc = resTy.getEncoding().dyn_cast<ttg::MmaEncodingAttr>()) {\n+        if (resEnc && resEnc.isHopper()) {\n+          // Don't pipeline valid dots that depend on ops other than scf.yield\n+          // and scf.for\n+          auto dot = dotOp.getResult();\n+          bool valid = true;\n+\n+          // all users of dot should be scf.yield\n+          if (!dot.hasOneUse())\n+            valid = false;\n+          if (!isa<scf::YieldOp>(*dot.getUsers().begin()))\n+            valid = false;\n+\n+          // C should be a block argument\n+          auto CArg = dotOp.getOperand(2).dyn_cast<BlockArgument>();\n+          if (!CArg || !CArg.hasOneUse())\n+            valid = false;\n+\n+          if (valid)\n+            dots.insert(dotOp);\n+        }\n+      }\n+    }\n+  }\n+\n+  hasHopperDot = true;\n+}\n+\n+void LoopPipeliner::checkOpShareBarriers(SetVector<Operation *> &ops) {\n+  // Check if loads can share barriers\n+  auto canShare = [&](Value load0, Value load1) -> bool {\n+    if (!load0.hasOneUse() || !load1.hasOneUse())\n+      return false;\n+    auto use0 = *load0.getUsers().begin();\n+    auto use1 = *load1.getUsers().begin();\n+    if (!use0->hasOneUse() || !use1->hasOneUse())\n+      return false;\n+    if (*use0->getUsers().begin() != *use1->getUsers().begin())\n+      return false;\n+    return true;\n+  };\n+  // XXX(Keren): the logic here is pretty weird and might be incomplete\n+  for (Value loadOp : validLoads) {\n+    Value depLoad;\n+    for (auto oldPair : loadsCanShareBarriers) {\n+      Value oldLoad = oldPair.first;\n+      if (canShare(loadOp, oldLoad)) {\n+        depLoad = oldLoad;\n+        break;\n+      }\n+    }\n+    loadsCanShareBarriers[loadOp] = depLoad;\n+  }\n+}\n+\n void LoopPipeliner::checkOpDeps(SetVector<Operation *> &ops) {\n   SetVector<BlockArgument> nonImmediateDepArgs;\n   SetVector<Operation *> nonImmediateOps;\n@@ -413,9 +570,8 @@ void LoopPipeliner::checkOpDeps(SetVector<Operation *> &ops) {\n     }\n   }\n \n-  // XXX: We could remove the following constraints if we can rematerialize in\n-  // the loop.\n-  // Check if immediateDepArgs and nonImmediateDepArgs are disjoint.\n+  // We could remove the following constraints if we can rematerialize in the\n+  // loop. Check if immediateDepArgs and nonImmediateDepArgs are disjoint.\n   for (auto &[arg, stages] : immediateArgStages) {\n     assert(stages.size() == 1 &&\n            \"Triton doesn't support an argument provides values for \"\n@@ -485,18 +641,28 @@ void LoopPipeliner::createBufferTypes() {\n   for (auto loadCvt : loadsMapping) {\n     auto loadOp = loadCvt.first;\n     Value cvt = loadCvt.second;\n-    auto dotOpEnc = cvt.getType()\n-                        .cast<RankedTensorType>()\n-                        .getEncoding()\n-                        .cast<ttg::DotOperandEncodingAttr>();\n     auto ty = loadOp.getType().cast<RankedTensorType>();\n     SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n                                      ty.getShape().end());\n     bufferShape.insert(bufferShape.begin(), numStages);\n-    unsigned bitWidth = ty.getElementType().getIntOrFloatBitWidth();\n-    auto sharedEnc =\n-        ttg::SharedEncodingAttr::get(ty.getContext(), dotOpEnc, ty.getShape(),\n-                                     ttg::getOrder(ty.getEncoding()), bitWidth);\n+    auto CTALayout = ttg::getCTALayout(ty.getEncoding());\n+    Attribute sharedEnc;\n+    if (auto dotOpEnc = cvt.getType()\n+                            .cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n+      // MMAv1 and MMAv2\n+      unsigned bitWidth = ty.getElementType().getIntOrFloatBitWidth();\n+      sharedEnc = ttg::SharedEncodingAttr::get(\n+          ty.getContext(), dotOpEnc, ty.getShape(),\n+          ttg::getOrder(ty.getEncoding()), CTALayout, bitWidth);\n+    } else {\n+      // MMAv3\n+      sharedEnc = ttg::SharedEncodingAttr::get(ty.getContext(), ty.getShape(),\n+                                               ttg::getOrder(ty.getEncoding()),\n+                                               CTALayout, ty.getElementType());\n+    }\n+    // FIXME(Keren): block ptr not handled\n     loadsBufferType[loadOp] =\n         RankedTensorType::get(bufferShape, ty.getElementType(), sharedEnc);\n   }\n@@ -525,7 +691,7 @@ int LoopPipeliner::getValueDefStage(Value v, int stage) {\n     return stage;\n }\n \n-ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(triton::LoadOp loadOp,\n+ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(tt::LoadOp loadOp,\n                                                       OpBuilder &builder) {\n   // Allocate a buffer for each pipelined tensor\n   // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n@@ -546,6 +712,11 @@ LogicalResult LoopPipeliner::initialize() {\n   if (checkOpUses(ops).failed())\n     return failure();\n \n+  // XXX(Keren): hopper specific, should be cleaned up\n+  checkHopperDots(ops);\n+\n+  checkOpShareBarriers(ops);\n+\n   checkOpDeps(ops);\n \n   createBufferTypes();\n@@ -555,21 +726,21 @@ LogicalResult LoopPipeliner::initialize() {\n   return success();\n }\n \n-Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n+Value LoopPipeliner::getLoadMask(tt::LoadOp loadOp, Value mappedMask,\n                                  Value loopCond, OpBuilder &builder) {\n-  Type maskType = triton::getI1SameShape(loadOp.getType());\n+  Type maskType = tt::getI1SameShape(loadOp.getType());\n   Value mask = loadOp.getMask();\n   Value newMask;\n   if (mask) {\n     Value cond = loopCond;\n     if (isa<RankedTensorType>(maskType)) {\n-      cond = builder.create<triton::SplatOp>(mask.getLoc(), maskType, loopCond);\n+      cond = builder.create<tt::SplatOp>(mask.getLoc(), maskType, loopCond);\n     }\n     newMask = builder.create<arith::AndIOp>(mask.getLoc(), mappedMask, cond);\n   } else {\n     if (isa<RankedTensorType>(maskType)) {\n-      newMask = builder.create<triton::SplatOp>(loopCond.getLoc(), maskType,\n-                                                loopCond);\n+      newMask =\n+          builder.create<tt::SplatOp>(loopCond.getLoc(), maskType, loopCond);\n     } else {\n       newMask = loopCond;\n     }\n@@ -585,7 +756,53 @@ void LoopPipeliner::emitPrologue() {\n     setValueMapping(arg, operand.get(), 0);\n   }\n \n-  // Emit prologue from [0, numStage-1)\n+  // Alloc a vector of MBarriers in size numStages for each load to be pipelined\n+  bool isMcast = false;\n+  for (Value loadOp : validLoads) {\n+    auto load = cast<tt::LoadOp>(loadOp.getDefiningOp());\n+    if (isLoadFromTensorPtr(load)) {\n+      auto loadTy = loadOp.getType().cast<RankedTensorType>();\n+      auto CTALayout = ttg::CTALayoutAttr::get(\n+          load.getContext(),\n+          /*CTAsPerCGA*/ {static_cast<unsigned>(numCTAs)},\n+          /*CTASplitNum*/ {1},\n+          /*CTAOrder*/ {0});\n+      auto sharedEncoding = ttg::SharedEncodingAttr::get(\n+          load.getContext(), 1, 1, 1, {0}, CTALayout, false);\n+      auto mBarriersTy = RankedTensorType::get(\n+          {numStages}, builder.getIntegerType(64), sharedEncoding);\n+\n+      if (!loadsCanShareBarriers[loadOp]) {\n+        Value fullBarriers = builder.create<ttng::AllocMBarrierOp>(\n+            load.getLoc(), mBarriersTy, 1);\n+        loadsFullBarriers[loadOp] = fullBarriers;\n+      }\n+      auto layout = loadTy.getEncoding();\n+      auto CTASplitNum = ttg::getCTASplitNum(layout);\n+      auto CTAsPerCGA = ttg::getCTAsPerCGA(layout);\n+      if (CTASplitNum != CTAsPerCGA) {\n+        isMcast = true;\n+        // FIXME: numConsumerThreads could be 32 as well instead of 128\n+        // incase the consumer is not GMMA\n+        unsigned arriveCnt = ttg::getNumWarpsPerCTA(layout);\n+        if (hasHopperDot)\n+          arriveCnt /= 4;\n+        arriveCnt *=\n+            product<unsigned>(CTAsPerCGA) / product<unsigned>(CTASplitNum);\n+\n+        Value emptyBarriers = builder.create<ttng::AllocMBarrierOp>(\n+            load.getLoc(), mBarriersTy, arriveCnt);\n+        loadsEmptyBarriers[loadOp] = emptyBarriers;\n+      }\n+    }\n+  }\n+\n+  if (isMcast) {\n+    builder.create<ttng::ClusterArriveOp>(forOp.getLoc(), /*relaxed*/ 1);\n+    builder.create<ttng::ClusterWaitOp>(forOp.getLoc());\n+  }\n+\n+  // prologue from [0, numStage-1)\n   Value iv = forOp.getLowerBound();\n   pipelineIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (int stage = 0; stage < numStages - 1; ++stage) {\n@@ -600,33 +817,99 @@ void LoopPipeliner::emitPrologue() {\n     for (Operation *op : orderedDeps) {\n       Operation *newOp = nullptr;\n       if (validLoads.contains(op->getResult(0))) {\n-        auto load = cast<triton::LoadOp>(op);\n+        auto load = cast<tt::LoadOp>(op);\n         // Allocate empty buffer\n         if (stage == 0) {\n           loadsBuffer[load] = allocateEmptyBuffer(load, builder);\n           loadStageBuffer[load] = {loadsBuffer[load]};\n         }\n         // load => copy async\n-        if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n+        if (auto loadOp = llvm::dyn_cast<tt::LoadOp>(op)) {\n           Value newMask =\n               getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n                           loopCond, builder);\n-          newOp = builder.create<ttg::InsertSliceAsyncOp>(\n-              op->getLoc(), loadsBuffer[loadOp].getType(),\n-              lookupOrDefault(loadOp.getPtr(), stage),\n-              loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n-              lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n-              loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n-          builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n+\n+          if (mode && isLoadFromTensorPtr(loadOp)) {\n+            auto loc = op->getLoc();\n+            auto mBarTy = tt::PointerType::get(builder.getIntegerType(64), 3);\n+            Value stageVal =\n+                builder.create<arith::ConstantIntOp>(loc, stage, 32);\n+            // producer_acquire\n+            if (loadsEmptyBarriers.count(loadOp)) {\n+              Value emptyBarrier = builder.create<ttng::ExtractMBarrierOp>(\n+                  loc, mBarTy, loadsEmptyBarriers[loadOp], stageVal);\n+              auto trueVal =\n+                  builder.create<arith::ConstantIntOp>(loc, 1, /*bitWidth*/ 1);\n+              builder.create<ttng::MBarrierWaitOp>(loc, emptyBarrier, trueVal);\n+            }\n+\n+            // producer_commit\n+            Value fullBarrier;\n+            if (!loadsCanShareBarriers[loadOp]) {\n+              fullBarrier = builder.create<ttng::ExtractMBarrierOp>(\n+                  loc, mBarTy, loadsFullBarriers[loadOp], stageVal);\n+              loadsExtract[loadOp] = fullBarrier;\n+            } else {\n+              // Reuse the barrier from previouse load.\n+              fullBarrier = loadsExtract[loadsCanShareBarriers[loadOp]];\n+            }\n+\n+            auto loadTy = loadOp.getType().dyn_cast<RankedTensorType>();\n+            assert(loadTy);\n+            auto CTASplitNum = ttg::getCTASplitNum(loadTy.getEncoding());\n+            auto shapePerSlice =\n+                ttg::getShapePerCTA(CTASplitNum, loadTy.getShape());\n+            unsigned elems =\n+                std::accumulate(shapePerSlice.begin(), shapePerSlice.end(), 1,\n+                                std::multiplies{});\n+            elems *= (loadTy.getElementType().getIntOrFloatBitWidth() / 8);\n+\n+            if (!loadsCanShareBarriers[loadOp]) {\n+              Value _0 = builder.create<arith::ConstantIntOp>(loc, 0, 32);\n+              Value threadId = builder.create<ttng::GetThreadIdOp>(loc);\n+              Value pred = builder.create<arith::CmpIOp>(\n+                  loc, arith::CmpIPredicate::eq, threadId, _0);\n+              pred = builder.create<arith::AndIOp>(loc, pred, loopCond);\n+              Operation *barrierArvOp = builder.create<ttng::MBarrierArriveOp>(\n+                  loc, fullBarrier, pred,\n+                  /*remoteCtaId*/ nullptr, /*trackAsyncOp*/ false, elems);\n+              loadsBarrierArvOp[loadOp] = barrierArvOp;\n+            } else {\n+              // Increase the transcnt for barrier of previouse load by the\n+              // bytes of current load.\n+              Operation *barrierArvOp =\n+                  loadsBarrierArvOp[loadsCanShareBarriers[loadOp]];\n+              unsigned base_elems =\n+                  barrierArvOp->getAttr(\"txCount\").cast<IntegerAttr>().getInt();\n+              barrierArvOp->setAttr(\"txCount\",\n+                                    IntegerAttr::get(builder.getIntegerType(32),\n+                                                     base_elems + elems));\n+            }\n+            newOp = builder.create<ttng::InsertSliceAsyncV2Op>(\n+                loc, loadsBuffer[loadOp].getType(),\n+                lookupOrDefault(loadOp.getPtr(), stage),\n+                loadStageBuffer[loadOp][stage], pipelineIterIdx, fullBarrier,\n+                newMask, lookupOrDefault(loadOp.getOther(), stage),\n+                loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile(),\n+                /*axis*/ 0);\n+          } else {\n+            newOp = builder.create<ttg::InsertSliceAsyncOp>(\n+                op->getLoc(), loadsBuffer[loadOp].getType(),\n+                lookupOrDefault(loadOp.getPtr(), stage),\n+                loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n+                lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n+                loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n+            builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n+          }\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n       } else {\n-        if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+        if (auto loadOp = dyn_cast<tt::LoadOp>(op)) {\n           Value newMask =\n               getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n                           loopCond, builder);\n-          newOp = builder.create<triton::LoadOp>(\n+          newOp = builder.create<tt::LoadOp>(\n               loadOp.getLoc(), loadOp.getResult().getType(),\n               lookupOrDefault(loadOp.getPtr(), stage), newMask,\n               lookupOrDefault(loadOp.getOther(), stage),\n@@ -667,9 +950,9 @@ void LoopPipeliner::emitPrologue() {\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n   // async.wait & extract_slice\n-  builder.create<ttg::AsyncWaitOp>(validLoads.front().getLoc(),\n-                                   validLoads.size() * (numStages - 2));\n-  loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n+  if (numLoadsRequireAsyncWait > 0)\n+    builder.create<ttg::AsyncWaitOp>(validLoads.front().getLoc(),\n+                                     validLoads.size() * (numStages - 2));\n   for (Value loadOp : validLoads) {\n     auto bufferType = loadStageBuffer[loadOp][numStages - 1]\n                           .getType()\n@@ -688,19 +971,20 @@ void LoopPipeliner::emitPrologue() {\n         SmallVector<OpFoldResult>{int_attr(1), int_attr(1), int_attr(1)});\n     loadsExtract[loadOp] = extractSlice;\n   }\n-  // Bump up loopIterIdx, this is used for getting the correct slice for the\n-  // `next` iteration\n-  loopIterIdx = builder.create<arith::AddIOp>(\n-      loopIterIdx.getLoc(), loopIterIdx,\n-      builder.create<arith::ConstantIntOp>(loopIterIdx.getLoc(), 1, 32));\n+  curWaitIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n+  loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n+  curPhase = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 1);\n+  curEmptyPhase = builder.create<arith::ConstantIntOp>(iv.getLoc(), 1, 1);\n }\n \n void LoopPipeliner::emitEpilogue() {\n   // If there's any outstanding async copies, we need to wait for them.\n-  OpBuilder builder(forOp);\n-  OpBuilder::InsertionGuard g(builder);\n-  builder.setInsertionPointAfter(forOp);\n-  builder.create<ttg::AsyncWaitOp>(forOp.getLoc(), 0);\n+  if (numLoadsRequireAsyncWait > 0) {\n+    OpBuilder builder(forOp);\n+    OpBuilder::InsertionGuard g(builder);\n+    builder.setInsertionPointAfter(forOp);\n+    builder.create<ttg::AsyncWaitOp>(forOp.getLoc(), 0);\n+  }\n }\n \n SmallVector<Value> LoopPipeliner::collectNewLoopArgs() {\n@@ -714,6 +998,9 @@ SmallVector<Value> LoopPipeliner::collectNewLoopArgs() {\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n+  //   (wait index)\n+  //   (phase index)\n+  //   (empty phase index)\n \n   // We need this to update operands for yield\n   // original block arg => new arg's idx\n@@ -739,10 +1026,16 @@ SmallVector<Value> LoopPipeliner::collectNewLoopArgs() {\n       newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n   }\n \n-  ivIndex = newLoopArgs.size();\n+  ivIdx = newLoopArgs.size();\n   newLoopArgs.push_back(valueMapping[forOp.getInductionVar()][numStages - 2]);\n   newLoopArgs.push_back(pipelineIterIdx);\n-  newLoopArgs.push_back(loopIterIdx);\n+  newLoopArgs.push_back(curWaitIdx);\n+  if (numLoadsRequireMBarrier > 0) {\n+    newLoopArgs.push_back(loopIterIdx);\n+    newLoopArgs.push_back(curPhase);\n+    newLoopArgs.push_back(curEmptyPhase);\n+  }\n+\n   return newLoopArgs;\n }\n \n@@ -759,34 +1052,140 @@ scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n+  // Loop iteration args\n+  Value upperBound = newForOp.getUpperBound();\n+  Value step = newForOp.getStep();\n+  Value curIV = newForOp.getRegionIterArgs()[ivIdx];\n+  pipelineIterIdx = newForOp.getRegionIterArgs()[ivIdx + 1];\n+  curWaitIdx = newForOp.getRegionIterArgs()[ivIdx + 2];\n+  if (numLoadsRequireMBarrier > 0) {\n+    loopIterIdx = newForOp.getRegionIterArgs()[ivIdx + 3];\n+    curPhase = newForOp.getRegionIterArgs()[ivIdx + 4];\n+    curEmptyPhase = newForOp.getRegionIterArgs()[ivIdx + 5];\n+  }\n+\n   // Clone the loop body, replace original args with args of the new ForOp.\n-  // We want to find cvt ops that match the following pattern:\n-  // %0 = load %ptr\n-  // %1 (dotOperand) = cvt %0\n+  SmallVector<Value> loadsFromTensorPtr;\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n-    if (auto cvtOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+    if (auto cvtOp = dyn_cast<ttg::ConvertLayoutOp>(op)) {\n       auto result = op.getResult(0);\n       auto cvtDstTy = result.getType().cast<RankedTensorType>();\n-      if (cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n-        auto it =\n-            std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n-        if (it != validLoads.end()) {\n+      auto it =\n+          std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n+      if (it != validLoads.end()) {\n+        auto loadArgIdx = std::distance(validLoads.begin(), it);\n+        if (cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n+          // We want to find cvt ops that match the following pattern:\n+          // %0 = load %ptr\n+          // %1 (dotOperand) = cvt %0\n           // We replace the use new load use with a convert layout\n-          auto loadArgIdx = std::distance(validLoads.begin(), it);\n           auto cvt = builder.create<ttg::ConvertLayoutOp>(\n               result.getLoc(), cvtDstTy,\n               newForOp.getRegionIterArgs()[loadIdx + loadArgIdx]);\n           mapping.map(result, cvt.getResult());\n           continue;\n+        } else if (cvtDstTy.getEncoding().isa<ttg::SharedEncodingAttr>()) {\n+          // We want to find cvt ops that match the following pattern:\n+          // %0 = load %ptr\n+          // %1 (sharedEncoding) = cvt %0\n+          // We replace the use new load use with insert_slice_async's result\n+          mapping.map(result,\n+                      newForOp.getRegionIterArgs()[loadIdx + loadArgIdx]);\n+          continue;\n+        }\n+      }\n+    } else if (auto loadOp = dyn_cast<tt::LoadOp>(op)) {\n+      if (isLoadFromTensorPtr(loadOp)) {\n+        // XXX(Keren): The comparison operator using std::find on tensor ptr\n+        // doesn't work as expected\n+        auto operand = loadOp.getPtr();\n+        auto tensorTy =\n+            operand.getType().cast<tt::PointerType>().getPointeeType();\n+        auto loadArgIdx = 0;\n+        for (auto validLoad : validLoads) {\n+          auto defOp = cast<tt::LoadOp>(validLoad.getDefiningOp());\n+          if (isLoadFromTensorPtr(defOp)) {\n+            auto validOperand = defOp.getOperand(0);\n+            auto validTensorTy =\n+                validOperand.getType().cast<tt::PointerType>().getPointeeType();\n+            if (tensorTy == validTensorTy)\n+              break;\n+          }\n+          loadArgIdx++;\n         }\n+        // consumer_wait, emitted before the first consumer\n+        auto firstConsumer = getFirstUser(loadOp);\n+        mapping.map(loadOp, newForOp.getRegionIterArgs()[loadIdx + loadArgIdx]);\n+\n+        // If current load can reuse barriers shared by previous load, then we\n+        // do nothing.\n+        if (!loadsCanShareBarriers[loadOp]) {\n+          // emit mbarrier wait before the first consumer of the loaD\n+          OpBuilder mBarBuilder(firstConsumer);\n+          auto mBarTy = tt::PointerType::get(builder.getIntegerType(64), 3);\n+          Value fullBarrier = mBarBuilder.create<ttng::ExtractMBarrierOp>(\n+              loadOp.getLoc(), mBarTy, loadsFullBarriers[loadOp], curWaitIdx);\n+          mBarBuilder.create<ttng::MBarrierWaitOp>(loadOp.getLoc(), fullBarrier,\n+                                                   curPhase);\n+        }\n+\n+        loadsFromTensorPtr.push_back(loadOp);\n+        continue;\n       }\n     }\n     cloneWithInferType(builder, &op, mapping);\n   }\n \n+  for (Value load : loadsFromTensorPtr) {\n+    // consumer_relase, emitted after the last consumer\n+    // 'the last consumer' might be updated in the following Phase_1 since\n+    // some of the consumers might be pipelined. Thus we maintain this\n+    // information in 'consumerReleaseMap' and move the position of\n+    // consumer_release barrier in a seperate Phase_2 in case necessary.\n+    if (loadsEmptyBarriers.count(load)) {\n+      auto users = mapping.lookup(load).getUsers();\n+      DenseMap</*consumer=*/Operation *, /*stage=*/int> consumerStageMap;\n+      for (Operation *user : users) {\n+        // All the stage is initialized to zero before Phase_1,\n+        // since no consumers has been pipelined yet.\n+        consumerStageMap[user] = 0;\n+      }\n+      auto CTALayout = ttg::getCTALayout(\n+          load.getType().cast<RankedTensorType>().getEncoding());\n+      ConsumerReleaseInfo info{\n+          loopIterIdx, pipelineIterIdx, curEmptyPhase, curIV,\n+          step,        upperBound,      CTALayout,     consumerStageMap};\n+      consumerReleaseMap[loadsEmptyBarriers[load]] = info;\n+    }\n+  }\n+\n+  // Remove redundant conversions\n+  // e.g., %145 = triton_gpu.convert_layout %arg15 : (tensor<128x64xf16,\n+  // #shared1>) -> tensor<128x64xf16, #shared1>\n+  for (Operation &op : newForOp.getBody()->without_terminator()) {\n+    if (auto convert_layout = dyn_cast<ttg::ConvertLayoutOp>(op)) {\n+      auto result = op.getResult(0);\n+      auto cvtDstTy = result.getType();\n+      auto operand = convert_layout.getOperand();\n+      auto tensorTy = operand.getType();\n+      if (cvtDstTy == tensorTy)\n+        result.replaceAllUsesWith(operand);\n+    }\n+  }\n+\n   return newForOp;\n }\n \n+Value LoopPipeliner::getBoundedIterationValue(OpBuilder &builder, Value curIdx,\n+                                              Value upperBoundIdx,\n+                                              Value curValue, Value initValue) {\n+  Value cond = builder.create<arith::CmpIOp>(\n+      curIdx.getLoc(), arith::CmpIPredicate::uge, curIdx, upperBoundIdx);\n+  Value selectValue = builder.create<mlir::arith::SelectOp>(\n+      curIdx.getLoc(), cond, initValue, curValue);\n+  return selectValue;\n+}\n+\n void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n                                           OpBuilder &builder) {\n   // Map the dep args of the next iteration to the dep args of the current\n@@ -798,22 +1197,36 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n     ++argIdx;\n   }\n \n+  // Update loop iteration args\n+  Value curIV = newForOp.getRegionIterArgs()[ivIdx];\n+  pipelineIterIdx = newForOp.getRegionIterArgs()[ivIdx + 1];\n+  curWaitIdx = newForOp.getRegionIterArgs()[ivIdx + 2];\n+  if (numLoadsRequireMBarrier > 0) {\n+    loopIterIdx = newForOp.getRegionIterArgs()[ivIdx + 3];\n+    curPhase = newForOp.getRegionIterArgs()[ivIdx + 4];\n+    curEmptyPhase = newForOp.getRegionIterArgs()[ivIdx + 5];\n+  }\n+\n   // Special handling for iv & loop condition\n-  Value curIV = newForOp.getRegionIterArgs()[ivIndex];\n-  nextIV = builder.create<arith::AddIOp>(newForOp.getInductionVar().getLoc(),\n-                                         curIV, newForOp.getStep());\n-  Value nextLoopCond =\n-      builder.create<arith::CmpIOp>(nextIV.getLoc(), arith::CmpIPredicate::slt,\n-                                    nextIV, newForOp.getUpperBound());\n-\n-  pipelineIterIdx = newForOp.getRegionIterArgs()[ivIndex + 1];\n-  Value insertSliceIndex = builder.create<arith::RemSIOp>(\n-      nextIV.getLoc(), pipelineIterIdx,\n-      builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n-  loopIterIdx = newForOp.getRegionIterArgs()[ivIndex + 2];\n-  Value extractSliceIndex = builder.create<arith::RemSIOp>(\n-      nextIV.getLoc(), loopIterIdx,\n-      builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n+  auto idxLoc = curIV.getLoc();\n+  nextIV = builder.create<arith::AddIOp>(idxLoc, curIV, newForOp.getStep());\n+  Value nextLoopCond = builder.create<arith::CmpIOp>(\n+      idxLoc, arith::CmpIPredicate::slt, nextIV, newForOp.getUpperBound());\n+\n+  // Constants\n+  Value _0 = builder.create<arith::ConstantIntOp>(idxLoc, 0, 32);\n+  Value _1 = builder.create<arith::ConstantIntOp>(idxLoc, 1, 32);\n+  Value numStagesVal =\n+      builder.create<arith::ConstantIntOp>(idxLoc, numStages, 32);\n+\n+  // nextWaitIdx\n+  Value waitIdxPlusOne = builder.create<arith::AddIOp>(idxLoc, curWaitIdx, _1);\n+  Value nextWaitIdx = getBoundedIterationValue(\n+      builder, waitIdxPlusOne, numStagesVal, waitIdxPlusOne, _0);\n+\n+  // Indices of InsertSliceAsyncOp and ExtractSliceOp\n+  Value insertSliceIndex = pipelineIterIdx;\n+  Value extractSliceIndex = nextWaitIdx;\n \n   // Prefetch load deps\n   // If a load-dependent instruction that uses a block argument, we\n@@ -841,11 +1254,11 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n       else\n         curMapping.map(forOp.getInductionVar(), nextIV);\n       Operation *nextOp;\n-      if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+      if (auto loadOp = dyn_cast<tt::LoadOp>(op)) {\n         auto newMask =\n             getLoadMask(loadOp, curMapping.lookupOrDefault(loadOp.getMask()),\n                         nextLoopCond, builder);\n-        nextOp = builder.create<triton::LoadOp>(\n+        nextOp = builder.create<tt::LoadOp>(\n             loadOp.getLoc(), loadOp.getResult().getType(),\n             curMapping.lookupOrDefault(loadOp.getPtr()), newMask,\n             curMapping.lookupOrDefault(loadOp.getOther()),\n@@ -870,7 +1283,7 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n     Operation *nextOp = nullptr;\n     // Update loading mask\n     if (validLoads.contains(op->getResult(0))) {\n-      auto loadOp = llvm::cast<triton::LoadOp>(op);\n+      auto loadOp = llvm::cast<tt::LoadOp>(op);\n       auto mask = loadOp.getMask();\n       auto newMask =\n           getLoadMask(loadOp, nextMapping.lookupOrDefault(loadOp.getMask()),\n@@ -879,28 +1292,95 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n         // If mask is defined outside the loop, don't update the map more than\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n-          nextMapping.map(loadOp.getMask(), newMask);\n-        newMask = nextMapping.lookupOrDefault(mask);\n+          nextMapping.map(mask, newMask);\n+        newMask = nextMapping.lookupOrDefault(loadOp.getMask());\n+      }\n+      Value insertedVal;\n+      if (mode && isLoadFromTensorPtr(loadOp)) {\n+        auto loc = op->getLoc();\n+        auto mBarTy = tt::PointerType::get(builder.getIntegerType(64), 3);\n+\n+        // producer_acquire\n+        if (loadsEmptyBarriers.count(loadOp)) {\n+          auto ifOp = builder.create<scf::IfOp>(loc, ArrayRef<Type>{},\n+                                                nextLoopCond, false);\n+          builder.setInsertionPointToStart(ifOp.thenBlock());\n+          Value emptyBarrier = builder.create<ttng::ExtractMBarrierOp>(\n+              loc, mBarTy, loadsEmptyBarriers[loadOp], insertSliceIndex);\n+          builder.create<ttng::MBarrierWaitOp>(loc, emptyBarrier,\n+                                               curEmptyPhase);\n+          builder.setInsertionPointAfter(ifOp);\n+        }\n+\n+        // producer_commit\n+        Value fullBarrier;\n+        if (!loadsCanShareBarriers[loadOp]) {\n+          fullBarrier = builder.create<ttng::ExtractMBarrierOp>(\n+              loc, mBarTy, loadsFullBarriers[loadOp], insertSliceIndex);\n+          loadsExtract[loadOp] = fullBarrier;\n+        } else {\n+          // Reuse the barrier from previouse load.\n+          fullBarrier = loadsExtract[loadsCanShareBarriers[loadOp]];\n+        }\n+\n+        auto loadTy = loadOp.getType().dyn_cast<RankedTensorType>();\n+        assert(loadTy);\n+        auto CTASplitNum = ttg::getCTASplitNum(loadTy.getEncoding());\n+        auto shapePerSlice =\n+            ttg::getShapePerCTA(CTASplitNum, loadTy.getShape());\n+        unsigned elems = std::accumulate(\n+            shapePerSlice.begin(), shapePerSlice.end(), 1, std::multiplies{});\n+        elems *= (loadTy.getElementType().getIntOrFloatBitWidth() / 8);\n+        if (!loadsCanShareBarriers[loadOp]) {\n+          Value _0 = builder.create<arith::ConstantIntOp>(loc, 0, 32);\n+          Value threadId = builder.create<ttng::GetThreadIdOp>(loc);\n+          Value pred = builder.create<arith::CmpIOp>(\n+              loc, arith::CmpIPredicate::eq, threadId, _0);\n+          pred = builder.create<arith::AndIOp>(loc, pred, nextLoopCond);\n+          Operation *barrierArvOp = builder.create<ttng::MBarrierArriveOp>(\n+              loc, fullBarrier, pred,\n+              /*remoteCtaId*/ nullptr,\n+              /*trackAsyncOp*/ false, elems);\n+          loadsBarrierArvOp[loadOp] = barrierArvOp;\n+        } else {\n+          // Increase the transcnt for barrier of previouse load by the bytes of\n+          // current load.\n+          Operation *barrierArvOp =\n+              loadsBarrierArvOp[loadsCanShareBarriers[loadOp]];\n+          unsigned base_elems =\n+              barrierArvOp->getAttr(\"txCount\").cast<IntegerAttr>().getInt();\n+          barrierArvOp->setAttr(\n+              \"txCount\",\n+              IntegerAttr::get(builder.getIntegerType(32), base_elems + elems));\n+        }\n+        insertedVal = builder.create<tt::nvidia_gpu::InsertSliceAsyncV2Op>(\n+            loc, loadsBuffer[loadOp].getType(),\n+            nextMapping.lookupOrDefault(loadOp.getPtr()),\n+            newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n+            insertSliceIndex, fullBarrier, newMask,\n+            nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n+            loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n+      } else {\n+        insertedVal = builder.create<ttg::InsertSliceAsyncOp>(\n+            op->getLoc(), loadsBuffer[loadOp].getType(),\n+            nextMapping.lookupOrDefault(loadOp.getPtr()),\n+            newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n+            insertSliceIndex, newMask,\n+            nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n+            loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n+        builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n       }\n-      Value insertAsyncOp = builder.create<ttg::InsertSliceAsyncOp>(\n-          op->getLoc(), loadsBuffer[loadOp].getType(),\n-          nextMapping.lookupOrDefault(loadOp.getPtr()),\n-          newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n-          insertSliceIndex, newMask,\n-          nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n-          loadOp.getEvict(), loadOp.getIsVolatile(), /*axis*/ 0);\n-      builder.create<ttg::AsyncCommitGroupOp>(op->getLoc());\n-      nextBuffers.push_back(insertAsyncOp);\n+      nextBuffers.push_back(insertedVal);\n       // Extract slice\n-      auto bufferType = insertAsyncOp.getType().cast<RankedTensorType>();\n+      auto bufferType = insertedVal.getType().cast<RankedTensorType>();\n       auto bufferShape = bufferType.getShape();\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n       sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                         sliceType.getElementType(),\n                                         loadsBufferType[loadOp].getEncoding());\n \n       nextOp = builder.create<ttg::ExtractSliceOp>(\n-          op->getLoc(), sliceType, insertAsyncOp,\n+          op->getLoc(), sliceType, insertedVal,\n           SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),\n                                     int_attr(0)},\n           SmallVector<OpFoldResult>{int_attr(1),\n@@ -923,20 +1403,43 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n                          newForOp.getRegionIterArgs()[depArgsIdx[arg]]);\n \n   // async.wait & extract_slice\n-  Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n-      validLoads[0].getLoc(), validLoads.size() * (numStages - 2));\n-  for (auto it = extractSlices.rbegin(); it != extractSlices.rend(); ++it) {\n-    // move extract_slice after asyncWait\n-    it->getDefiningOp()->moveAfter(asyncWait);\n-  }\n-\n-  // Bump iteration count\n-  pipelineIterIdx = builder.create<arith::AddIOp>(\n-      nextIV.getLoc(), pipelineIterIdx,\n-      builder.create<arith::ConstantIntOp>(nextIV.getLoc(), 1, 32));\n-  loopIterIdx = builder.create<arith::AddIOp>(\n-      nextIV.getLoc(), loopIterIdx,\n-      builder.create<arith::ConstantIntOp>(nextIV.getLoc(), 1, 32));\n+  if (numLoadsRequireAsyncWait > 0) {\n+    Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n+        validLoads[0].getLoc(), validLoads.size() * (numStages - 2));\n+    for (auto it = extractSlices.rbegin(); it != extractSlices.rend(); ++it) {\n+      // move extract_slice after asyncWait\n+      it->getDefiningOp()->moveAfter(asyncWait);\n+    }\n+  }\n+\n+  // Bump pipelineIterIdx\n+  Value pipelineIterIdxPlusOne =\n+      builder.create<arith::AddIOp>(idxLoc, pipelineIterIdx, _1);\n+  pipelineIterIdx =\n+      getBoundedIterationValue(builder, pipelineIterIdxPlusOne, numStagesVal,\n+                               pipelineIterIdxPlusOne, _0);\n+\n+  // Bump curWaitIdx\n+  curWaitIdx = nextWaitIdx;\n+\n+  if (numLoadsRequireMBarrier > 0) {\n+    // Bump loopIterIdx\n+    loopIterIdx = builder.create<arith::AddIOp>(idxLoc, loopIterIdx, _1);\n+\n+    Value _1_1b = builder.create<arith::ConstantIntOp>(idxLoc, 1, 1);\n+\n+    // Flip curPhase\n+    Value nextPhase = builder.create<arith::XOrIOp>(idxLoc, curPhase, _1_1b);\n+    curPhase = getBoundedIterationValue(builder, waitIdxPlusOne, numStagesVal,\n+                                        curPhase, nextPhase);\n+\n+    // Flip curEmptyPhase\n+    Value nextEmptyPhase =\n+        builder.create<arith::XOrIOp>(idxLoc, curEmptyPhase, _1_1b);\n+    curEmptyPhase =\n+        getBoundedIterationValue(builder, pipelineIterIdxPlusOne, numStagesVal,\n+                                 curEmptyPhase, nextEmptyPhase);\n+  }\n }\n \n void LoopPipeliner::finalizeYield(scf::ForOp newForOp, OpBuilder &builder) {\n@@ -948,14 +1451,21 @@ void LoopPipeliner::finalizeYield(scf::ForOp newForOp, OpBuilder &builder) {\n   for (Value nextSlice : extractSlices)\n     yieldValues.push_back(nextSlice);\n \n-  for (size_t i = depArgsBeginIdx; i < ivIndex; ++i) {\n+  for (size_t i = depArgsBeginIdx; i < ivIdx; ++i) {\n     auto arg = newForOp.getRegionIterArgs()[i];\n     assert(depArgsMapping.count(arg) && \"Missing loop-carried value\");\n     yieldValues.push_back(depArgsMapping[arg]);\n   }\n+\n+  // Loop iteration args\n   yieldValues.push_back(nextIV);\n   yieldValues.push_back(pipelineIterIdx);\n-  yieldValues.push_back(loopIterIdx);\n+  yieldValues.push_back(curWaitIdx);\n+  if (numLoadsRequireMBarrier > 0) {\n+    yieldValues.push_back(loopIterIdx);\n+    yieldValues.push_back(curPhase);\n+    yieldValues.push_back(curEmptyPhase);\n+  }\n \n   builder.setInsertionPointToEnd(newForOp.getBody());\n   builder.create<scf::YieldOp>(yieldOp->getLoc(), yieldValues);\n@@ -973,14 +1483,26 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n // ref: mlir/lib/Dialect/SCF/Transforms/LoopPipelining.cpp\n struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n   PipelinePass() = default;\n-  PipelinePass(int numStages) { this->numStages = numStages; }\n+  PipelinePass(int numStages, int numWarps, int numCTAs,\n+               int computeCapability) {\n+    this->numStages = numStages;\n+    this->numWarps = numWarps;\n+    this->numCTAs = numCTAs;\n+    this->computeCapability = computeCapability;\n+  }\n \n   void runOnOperation() override {\n-    int numStages = this->numStages;\n-\n-    if (numStages <= 1)\n+    // TODO[goostavz]: mode = 0 is temporary for backward compatible, will be\n+    // deprecated after the refactor of pipeline fully gets done\n+    // TODO[goostavz]: When mode = 1, the mask of prefetch insert_slice in the\n+    // prologue is currently not properly provided. Need some second thought on\n+    // the mask definition of InsertSliceOp when the src is ptr<tensor>\n+    bool mode =\n+        computeCapability >= 90 && ::triton::tools::getBoolEnv(\"ENABLE_TMA\");\n+    if (this->numStages <= 1)\n       return;\n \n+    // phase 0: pipeline loads in loops\n     // Pre-processing\n     // we make sure element-wise ops are done *after* the conversion\n     // to dot operands\n@@ -991,26 +1513,283 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n     // auto didPreprocess =\n     //     applyPatternsAndFoldGreedily(getOperation(), std::move(patterns));\n \n+    llvm::SmallVector<scf::ForOp> newForOps;\n     // Do the pipelining\n     getOperation()->walk([&](scf::ForOp forOp) -> void {\n-      LoopPipeliner pipeliner(forOp, numStages);\n-\n+      LoopPipeliner pipeliner(forOp, this->numStages, this->numWarps,\n+                              this->numCTAs, mode, consumerReleaseMap);\n       if (pipeliner.initialize().failed())\n         return;\n \n       pipeliner.emitPrologue();\n       scf::ForOp newForOp = pipeliner.createNewForOp();\n       pipeliner.emitEpilogue();\n+      newForOps.push_back(newForOp);\n \n       // Replace the original loop\n       for (unsigned i = 0; i < forOp->getNumResults(); ++i)\n         forOp->getResult(i).replaceAllUsesWith(newForOp->getResult(i));\n       forOp->erase();\n     });\n+\n+    // phase 1: pipeline dots in loops\n+    // A tt.dot suitable for GMMA will be converted to ttg.dot_async. And a\n+    // ttg.DotWaitOp will synchronize it lagging just one iteration, which is\n+    // a hueristic rule.\n+    for (auto forOp : newForOps)\n+      asyncLaunchDots(forOp);\n+\n+    // phase 2: emit consumer_release (empty barrier arrive) logics in case of\n+    //          TMA multicast.\n+    // For each load ops, it is emitted after its last consumer, if the consumer\n+    // is another async op, find its associated sync op. Each async load will be\n+    // emitted with a consumer_release action. The merge of redundant mbarriers\n+    // will be processed in the consequent OptimizeBarriers pass.\n+    for (const auto &item : consumerReleaseMap)\n+      emitConsumerRelease(item.first, item.second, numStages);\n   }\n+\n+private:\n+  Value getRemoteCTAId(OpBuilder &b, Location loc, ttg::CTALayoutAttr CTALayout,\n+                       Value remoteCTAIdIdx) const;\n+  void updateConsumerReleaseInfo(Operation *oldOp, Operation *newOp, int stage);\n+  void asyncLaunchDots(scf::ForOp forOp);\n+  void emitConsumerRelease(Value mbarTensor, const ConsumerReleaseInfo &info,\n+                           int numStages);\n+\n+  ConsumerReleaseMap consumerReleaseMap;\n };\n+\n+void PipelinePass::updateConsumerReleaseInfo(Operation *oldOp, Operation *newOp,\n+                                             int stage) {\n+  for (auto &item : consumerReleaseMap) {\n+    auto &m = item.second.consumerStageMap;\n+    if (m.count(oldOp)) {\n+      m.erase(oldOp);\n+      m[newOp] = stage;\n+    }\n+\n+    for (Value operand : oldOp->getOperands()) {\n+      Operation *op = operand.getDefiningOp();\n+      if (op && isa<ttg::ConvertLayoutOp>(op)) {\n+        auto cvt = cast<ttg::ConvertLayoutOp>(op);\n+        auto src = cvt.getSrc();\n+        auto srcEncoding = src.getType().cast<RankedTensorType>().getEncoding();\n+        auto dstEncoding =\n+            cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n+        if (srcEncoding == dstEncoding && m.count(op)) {\n+          m.erase(op);\n+          m[newOp] = stage;\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void PipelinePass::asyncLaunchDots(scf::ForOp forOp) {\n+  Block *loop = forOp.getBody();\n+\n+  /// XXX(Keren): Clean up the following duplicate code with checkDotOp\n+  /// dots to be pipelined\n+  SetVector<Value> dots;\n+  for (Operation &op : *loop) {\n+    if (auto dotOp = dyn_cast<tt::DotOp>(&op)) {\n+      auto resTy = dotOp.getResult().getType().dyn_cast<RankedTensorType>();\n+      if (auto resEnc = resTy.getEncoding().dyn_cast<ttg::MmaEncodingAttr>()) {\n+        if (resEnc && resEnc.isHopper()) {\n+          // Don't pipeline valid dots that depend on ops other than scf.yield\n+          // and scf.for\n+          auto dot = dotOp.getResult();\n+          bool valid = true;\n+\n+          // all users of dot should be scf.yield\n+          if (!dot.hasOneUse())\n+            valid = false;\n+          if (!isa<scf::YieldOp>(*dot.getUsers().begin()))\n+            valid = false;\n+\n+          // C should be a block argument\n+          auto CArg = dotOp.getOperand(2).dyn_cast<BlockArgument>();\n+          if (!CArg || !CArg.hasOneUse())\n+            valid = false;\n+\n+          if (valid)\n+            dots.insert(dotOp);\n+        }\n+      }\n+    }\n+  }\n+\n+  // Early stop: no need to continue if there is no valid dot in the loop.\n+  if (dots.empty())\n+    return;\n+\n+  OpBuilder builder(forOp);\n+\n+  // 0. insert dot_wait after the last dot in the loop\n+  Value dot = dots.back();\n+  auto loc = dot.getLoc();\n+  builder.setInsertionPointAfter(dot.getDefiningOp());\n+  auto dotWait = builder.create<tt::nvidia_gpu::DotWaitOp>(loc, dots.size());\n+\n+  // 1. replace Dot with DotAsync\n+  for (size_t idx = 0; idx < dots.size(); ++idx) {\n+    Value dot = dots[idx];\n+    auto dotOp = cast<tt::DotOp>(dot.getDefiningOp());\n+    builder.setInsertionPoint(dot.getDefiningOp());\n+    auto dotAsync = builder.create<tt::nvidia_gpu::DotAsyncOp>(\n+        loc, dotOp.getA(), dotOp.getB(), dotOp.getC(), dotOp.getAllowTF32());\n+    dot.replaceAllUsesWith(dotAsync.getResult());\n+    updateConsumerReleaseInfo(dot.getDefiningOp(), dotWait, /*stage=*/1);\n+    dot.getDefiningOp()->erase();\n+  }\n+\n+  // 2. If there's any outstanding DotAsyncOps, we need to wait for them.\n+  builder.setInsertionPointAfter(forOp);\n+  Value loopNotEmpty = builder.create<arith::CmpIOp>(\n+      loc, arith::CmpIPredicate::slt, forOp.getLowerBound(),\n+      forOp.getUpperBound());\n+  // TODO[goostavz]: it's a workaround to put the DotWaitOp in an IfOp for\n+  // a bug in ptxas which mistakenly analysis the control flow and turn the GMMA\n+  // into synchronuous implementation for safety.\n+  // Remove this If once the bug is fixed.\n+  auto ifOp = builder.create<scf::IfOp>(loc, ArrayRef<Type>{}, loopNotEmpty,\n+                                        /*hasElse*/ false);\n+  builder.setInsertionPointToStart(ifOp.thenBlock());\n+  builder.create<tt::nvidia_gpu::DotWaitOp>(forOp.getLoc(), 0);\n+}\n+\n+Value PipelinePass::getRemoteCTAId(OpBuilder &b, Location loc,\n+                                   ttg::CTALayoutAttr CTALayout,\n+                                   Value remoteCTAIdIdx) const {\n+  auto CTAsPerCGA = CTALayout.getCTAsPerCGA();\n+  auto CTAOrder = CTALayout.getCTAOrder();\n+  auto CTASplitNum = CTALayout.getCTASplitNum();\n+\n+  // Short path when bcastMask is a constant\n+  bool isConstMcastMask = true;\n+  for (unsigned s : CTASplitNum) {\n+    if (s > 1) {\n+      isConstMcastMask = false;\n+      break;\n+    }\n+  }\n+  if (isConstMcastMask)\n+    return remoteCTAIdIdx;\n+\n+  Value linearCTAId = b.create<ttng::GetClusterCTAIdOp>(loc);\n+  SmallVector<Value> multiDimCTAId =\n+      delinearize(b, loc, linearCTAId, CTAsPerCGA, CTAOrder);\n+  auto rank = CTAOrder.size();\n+  int bcastDim = -1;\n+  for (size_t i = 0; i < rank; ++i) {\n+    if (CTAsPerCGA[i] != CTASplitNum[i]) {\n+      assert(bcastDim < 0 && \"bcast in multiple dims is not expected\");\n+      bcastDim = i;\n+    }\n+  }\n+  multiDimCTAId[bcastDim] = remoteCTAIdIdx;\n+  return linearize(b, loc, multiDimCTAId, CTAsPerCGA, CTAOrder);\n+}\n+\n+void PipelinePass::emitConsumerRelease(Value mbarTensor,\n+                                       const ConsumerReleaseInfo &info,\n+                                       int numStages) {\n+  Value iterVar = info.iterVar;\n+  Value stage = info.stageVar;\n+  Value phase = info.phaseVar;\n+  Value nextIV = info.nextIVVar;\n+  Value step = info.stepVar;\n+  Value upperBound = info.upperBoundVar;\n+\n+  const auto &consumerStageMap = info.consumerStageMap;\n+  // find the the last consumer among all the consumers with the largest stage.\n+  SmallVector<Operation *> consumersWithLargestStage;\n+  int maxStage = 0;\n+  for (const auto &it : consumerStageMap) {\n+    if (it.second > maxStage) {\n+      consumersWithLargestStage.clear();\n+      consumersWithLargestStage.push_back(it.first);\n+      maxStage = it.second;\n+    } else if (it.second == maxStage) {\n+      consumersWithLargestStage.push_back(it.first);\n+    }\n+  }\n+  assert(consumersWithLargestStage.size() > 0);\n+  DenseMap<Operation *, size_t> operationId;\n+  consumersWithLargestStage[0]->getBlock()->walk<WalkOrder::PostOrder>(\n+      [&](Operation *op) { operationId[op] = operationId.size(); });\n+  size_t maxId = 0;\n+  Operation *lastUserWithLargestStage;\n+  for (Operation *op : consumersWithLargestStage) {\n+    assert(operationId.find(op) != operationId.end());\n+    size_t userId = operationId[op];\n+    if (userId > maxId) {\n+      maxId = userId;\n+      lastUserWithLargestStage = op;\n+    }\n+  }\n+\n+  OpBuilder b(&getContext());\n+  b.setInsertionPointAfter(lastUserWithLargestStage);\n+  auto loc = lastUserWithLargestStage->getLoc();\n+  auto maxStageVal = b.create<arith::ConstantIntOp>(loc, maxStage, 32);\n+\n+  // pred = (iterVar >= maxStage) &&\n+  //        (threadId % (numConsumerThreads / numRemoteCTAs) == 0);\n+\n+  // [benzh] maybe we can simplify the logics here\n+  auto cmpOp = arith::CmpIPredicate::sge;\n+  if (maxStage == 0)\n+    cmpOp = arith::CmpIPredicate::sgt;\n+  Value pred = b.create<arith::CmpIOp>(loc, cmpOp, iterVar, maxStageVal);\n+\n+  Value threadId = b.create<ttng::GetThreadIdOp>(loc);\n+  auto CTAsPerCGA = info.CTALayout.getCTAsPerCGA();\n+  auto CTASplitNum = info.CTALayout.getCTASplitNum();\n+  auto numRemoteCTAs = std::accumulate(CTAsPerCGA.begin(), CTAsPerCGA.end(), 1,\n+                                       std::multiplies{}) /\n+                       std::accumulate(CTASplitNum.begin(), CTASplitNum.end(),\n+                                       1, std::multiplies{});\n+  auto numConsumerThreads =\n+      isa<ttng::DotWaitOp>(lastUserWithLargestStage) ? 128 : 32;\n+  Value _0 = b.create<arith::ConstantIntOp>(loc, 0, 32);\n+  Value numArrives = b.create<arith::ConstantIntOp>(\n+      loc, numConsumerThreads / numRemoteCTAs, 32);\n+  pred = b.create<arith::AndIOp>(\n+      loc, pred,\n+      b.create<arith::CmpIOp>(\n+          loc, arith::CmpIPredicate::eq,\n+          b.create<arith::RemUIOp>(loc, threadId, numArrives), _0));\n+  // remoteCtaIdIdx = (threadId % numConsumerThreads) / (numConsumerThreads /\n+  // numRemoteCTAs);\n+  Value remoteCTAIdIdx = b.create<arith::DivUIOp>(\n+      loc,\n+      b.create<arith::RemUIOp>(\n+          loc, threadId,\n+          b.create<arith::ConstantIntOp>(loc, numConsumerThreads, 32)),\n+      numArrives);\n+  Value remoteCTAId = getRemoteCTAId(b, loc, info.CTALayout, remoteCTAIdIdx);\n+  Value emptyBarrier = b.create<ttng::ExtractMBarrierOp>(\n+      loc, tt::PointerType::get(b.getIntegerType(64), 3), mbarTensor, stage);\n+\n+  Value newNextIV = b.create<arith::AddIOp>(loc, nextIV, step);\n+  Value nextLoopCond = b.create<arith::CmpIOp>(loc, arith::CmpIPredicate::slt,\n+                                               newNextIV, upperBound);\n+  auto ifOp = b.create<scf::IfOp>(loc, ArrayRef<Type>{}, nextLoopCond,\n+                                  /*hasElse*/ false);\n+  b.setInsertionPointToStart(ifOp.thenBlock());\n+\n+  b.create<ttng::MBarrierArriveOp>(loc, emptyBarrier, pred, remoteCTAId,\n+                                   /*trackAsyncOp*/ false);\n+}\n+\n } // anonymous namespace\n \n-std::unique_ptr<Pass> mlir::createTritonGPUPipelinePass(int numStages) {\n-  return std::make_unique<PipelinePass>(numStages);\n+std::unique_ptr<Pass> mlir::createTritonGPUPipelinePass(int numStages,\n+                                                        int numWarps,\n+                                                        int numCTAs,\n+                                                        int computeCapability) {\n+  return std::make_unique<PipelinePass>(numStages, numWarps, numCTAs,\n+                                        computeCapability);\n }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 57, "deletions": 6, "changes": 63, "file_content_changes": "@@ -126,6 +126,13 @@ class SimplifyReduceCvt : public mlir::RewritePattern {\n       return failure();\n     }\n \n+    // ReduceOp does not support SharedLayout as its src layout, therefore\n+    // ConvertLayoutOp and ReduceOp should not be swapped when the conversion is\n+    // from SharedLayout to DistributedLayout\n+    if (newEncoding.isa<triton::gpu::SharedEncodingAttr>()) {\n+      return failure();\n+    }\n+\n     for (unsigned i = 1; i < newOperands.size(); ++i) {\n       auto oldTy = newOperands[i].getType().cast<RankedTensorType>();\n       RankedTensorType newTy =\n@@ -195,7 +202,11 @@ void pushConversionForward(triton::gpu::ConvertLayoutOp cvt,\n     if (arg.getDefiningOp() == cvt)\n       mapping.map(arg, cvt.getOperand());\n     else {\n-      auto oldType = arg.getType().cast<RankedTensorType>();\n+      auto oldType = arg.getType().dyn_cast<RankedTensorType>();\n+      // TODO: we may be creating block pointer load/store with mismatching\n+      // pointer type.\n+      if (!oldType)\n+        continue;\n       auto newType = RankedTensorType::get(\n           oldType.getShape(), oldType.getElementType(), srcEncoding);\n       auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(arg.getLoc(),\n@@ -207,7 +218,7 @@ void pushConversionForward(triton::gpu::ConvertLayoutOp cvt,\n   }\n   rewriter.setInsertionPoint(op);\n   if (op->getNumResults() == 0) {\n-    Operation *newOp = rewriter.clone(*op, mapping);\n+    Operation *newOp = cloneWithInferType(rewriter, op, mapping);\n     rewriter.eraseOp(op);\n     return;\n   }\n@@ -299,7 +310,7 @@ class MoveConvertOutOfIf : public mlir::RewritePattern {\n             mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n           continue;\n         }\n-        rewriter.clone(op, mapping);\n+        cloneWithInferType(rewriter, &op, mapping);\n       }\n     };\n     rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n@@ -458,7 +469,7 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n                        size_t i, RankedTensorType newType,\n                        triton::gpu::ConvertLayoutOp origConversion) const {\n     // Rewrite init argument\n-    Type origType = forOp.getInitArgs()[i].getType();\n+    auto origType = forOp.getInitArgs()[i].getType().cast<RankedTensorType>();\n     SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n     newInitArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         newInitArgs[i].getLoc(), newType, newInitArgs[i]);\n@@ -475,13 +486,52 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n \n     mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n     for (Operation &op : forOp.getBody()->without_terminator()) {\n-      if (&op == (Operation *)(&origConversion))\n+      if (dyn_cast<triton::gpu::ConvertLayoutOp>(op) == origConversion)\n         continue;\n-      Operation *newOp = rewriter.clone(op, mapping);\n+\n+      bool convert = llvm::any_of(op.getOperands(), [&](auto operand) {\n+        return operand == origConversion.getOperand();\n+      });\n+      auto convertLayout = [&](Value operand, Value value, Attribute encoding) {\n+        auto tensorType = value.getType().cast<RankedTensorType>();\n+        auto cvtType = RankedTensorType::get(\n+            tensorType.getShape(), tensorType.getElementType(), encoding);\n+        auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+            op.getLoc(), cvtType, value);\n+        mapping.map(operand, cvt);\n+      };\n+      DenseMap<Value, Value> cvtValues;\n+      if (convert) {\n+        for (auto operand : op.getOperands()) {\n+          if (operand == origConversion.getOperand() ||\n+              !isa<RankedTensorType>(operand.getType()))\n+            continue;\n+          auto value = mapping.lookupOrDefault(operand);\n+          // Convert to the new type\n+          convertLayout(operand, value, newType.getEncoding());\n+          // Other ops don't use the converted value and we need to restore\n+          cvtValues[operand] = value;\n+        }\n+      }\n+      auto *newOp = cloneWithInferType(rewriter, &op, mapping);\n+      if (convert) {\n+        for (auto result : op.getResults()) {\n+          if (!isa<RankedTensorType>(result.getType()))\n+            continue;\n+          auto value = mapping.lookupOrDefault(result);\n+          auto tensorType = result.getType().cast<RankedTensorType>();\n+          // Convert to the original type\n+          convertLayout(result, value, tensorType.getEncoding());\n+        }\n+        // Restore original values\n+        for (auto [operand, value] : cvtValues)\n+          mapping.map(operand, value);\n+      }\n     }\n     // create yield, inserting conversions if necessary\n     auto yieldOp = forOp.getBody()->getTerminator();\n     SmallVector<Value, 4> newYieldArgs;\n+    // We use the new type for the result of the conversion\n     for (Value arg : yieldOp->getOperands())\n       newYieldArgs.push_back(mapping.lookup(arg));\n     if (newYieldArgs[i].getType() != newType)\n@@ -494,6 +544,7 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n     newResults[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         newForOp.getLoc(), origType, newForOp->getResult(i));\n     newResults[i].getDefiningOp()->moveAfter(newForOp);\n+\n     return newResults;\n   }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 21, "deletions": 3, "changes": 24, "file_content_changes": "@@ -12,9 +12,13 @@ using namespace mlir::triton::gpu;\n // TypeConverter\n //\n TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n-                                               int numWarps, int threadsPerWarp)\n-    : context(context), numWarps(numWarps), threadsPerWarp(threadsPerWarp) {\n+                                               int numWarps, int threadsPerWarp,\n+                                               int numCTAs)\n+    : context(context), numWarps(numWarps), threadsPerWarp(threadsPerWarp),\n+      numCTAs(numCTAs) {\n   addConversion([](Type type) { return type; });\n+\n+  // Add encoding for tensor\n   addConversion([this](RankedTensorType tensorType) -> RankedTensorType {\n     // types with encoding are already in the right format\n     // TODO: check for layout encodings more specifically\n@@ -30,10 +34,24 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n     llvm::SmallVector<unsigned> sizePerThread(rank, 1);\n     Attribute encoding = triton::gpu::BlockedEncodingAttr::get(\n         this->context, shape, sizePerThread, order, this->numWarps,\n-        this->threadsPerWarp);\n+        this->threadsPerWarp, this->numCTAs);\n     return RankedTensorType::get(shape, tensorType.getElementType(), encoding);\n   });\n \n+  // Add encoding for tensor pointer\n+  addConversion([this](triton::PointerType ptrType) -> triton::PointerType {\n+    // Check whether tensor pointer `tt.ptr<tensor<>>`\n+    auto pointeeTensorType =\n+        ptrType.getPointeeType().dyn_cast<RankedTensorType>();\n+    if (pointeeTensorType == nullptr)\n+      return ptrType;\n+\n+    // Add layout into the tensor\n+    auto convertedTensorType = convertType(pointeeTensorType);\n+    return triton::PointerType::get(convertedTensorType,\n+                                    ptrType.getAddressSpace());\n+  });\n+\n   //\n   // Materializations\n   //"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 402, "deletions": 19, "changes": 421, "file_content_changes": "@@ -5,6 +5,7 @@\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+#include <fstream>\n \n namespace mlir {\n \n@@ -63,6 +64,235 @@ LogicalResult fixupLoops(ModuleOp mod) {\n   return success();\n }\n \n+SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n+                                                const ArrayRef<int64_t> &shape,\n+                                                RankedTensorType type) {\n+  if (version == 1)\n+    return {16, 16};\n+  else if (version == 2)\n+    return {16, 8};\n+  else if (version == 3) {\n+    unsigned k = 256 / type.getElementTypeBitWidth();\n+    if (shape[0] % 64 != 0 || shape[1] % 8 != 0) {\n+      assert(false && \"type not supported\");\n+      return {0, 0, 0};\n+    }\n+    auto eltType = type.getElementType();\n+    SmallVector<unsigned> validN;\n+\n+    // MMAv3 with larger instruction shape is preferred.\n+    if (eltType.isFloat8E5M2() || eltType.isFloat8E4M3FN() || eltType.isF16() ||\n+        eltType.isBF16() || eltType.isF32()) {\n+      validN.assign({256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176,\n+                     168, 160, 152, 144, 136, 128, 120, 112, 104, 96,  88,\n+                     80,  72,  64,  56,  48,  40,  32,  24,  16,  8});\n+    }\n+\n+    if (eltType.isInteger(8)) {\n+      validN.assign({224, 208, 192, 176, 160, 144, 128, 112, 96, 80, 64, 48, 32,\n+                     24, 16, 8});\n+    }\n+\n+    for (auto n : validN) {\n+      if (shape[1] % n == 0) {\n+        return {16, n, k};\n+      }\n+    }\n+\n+    assert(false && \"type not supported\");\n+    return {0, 0, 0};\n+  } else {\n+    assert(false && \"version not supported\");\n+    return {0, 0};\n+  }\n+}\n+\n+bool isLoadFromTensorPtr(triton::LoadOp op) {\n+  return mlir::triton::isTensorPointerType(op.getPtr().getType());\n+}\n+\n+bool isStoreToTensorPtr(triton::StoreOp op) {\n+  return mlir::triton::isTensorPointerType(op.getPtr().getType());\n+}\n+\n+Operation *getFirstUser(Value v) {\n+  DenseMap<Operation *, size_t> operationId;\n+  v.getParentBlock()->walk<WalkOrder::PostOrder>(\n+      [&](Operation *op) { operationId[op] = operationId.size(); });\n+  size_t minId = std::numeric_limits<size_t>::max();\n+  Operation *firstUser = nullptr;\n+  for (Operation *user : v.getUsers()) {\n+    assert(operationId.find(user) != operationId.end());\n+    size_t userId = operationId[user];\n+    if (userId < minId) {\n+      minId = userId;\n+      firstUser = user;\n+    }\n+  }\n+  assert(firstUser);\n+  return firstUser;\n+}\n+\n+triton::gpu::SharedEncodingAttr getSharedEncoding(RankedTensorType tensorTy) {\n+  auto blockedLayout =\n+      tensorTy.getEncoding().cast<triton::gpu::BlockedEncodingAttr>();\n+  return triton::gpu::SharedEncodingAttr::get(\n+      tensorTy.getContext(), tensorTy.getShape(), blockedLayout.getOrder(),\n+      blockedLayout.getCTALayout(), tensorTy.getElementType());\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// GraphDumper\n+//===----------------------------------------------------------------------===//\n+\n+GraphDumper::NodeInfo GraphDumper::onValue(Value value) const {\n+  return {{\"shape\", \"box\"}, {\"style\", \"filled\"}, {\"fillcolor\", \"white\"}};\n+}\n+\n+GraphDumper::NodeInfo GraphDumper::onOperation(Operation *op) const {\n+  return {{\"shape\", \"ellipse\"}, {\"style\", \"filled\"}, {\"fillcolor\", \"white\"}};\n+}\n+\n+std::string GraphDumper::dump(triton::FuncOp func) const {\n+  llvm::SetVector<Value> values;\n+  llvm::SetVector<Operation *> operations;\n+\n+  func.walk([&](Operation *op) {\n+    operations.insert(op);\n+    for (Value operand : op->getOperands())\n+      values.insert(operand);\n+    for (Value result : op->getResults())\n+      values.insert(result);\n+  });\n+\n+  std::ostringstream oss;\n+  oss << \"// Generated by Triton GraphDumper\\n\"\n+      << \"\\n\"\n+      << \"digraph {\\n\";\n+\n+  oss << \"    // Value Nodes\\n\";\n+  for (Value value : values)\n+    oss << \"    \" << emitValueNode(value) << \"\\n\";\n+  oss << \"\\n\";\n+\n+  oss << \"    // Operation Nodes\\n\";\n+  for (Operation *op : operations)\n+    oss << \"    \" << emitOperationNode(op) << \"\\n\";\n+  oss << \"\\n\";\n+\n+  oss << \"    // Edges\\n\";\n+  for (Operation *op : operations) {\n+    for (Value operand : op->getOperands())\n+      oss << \"    \" << emitEdge(getUniqueId(operand), getUniqueId(op)) << \"\\n\";\n+    for (Value result : op->getResults())\n+      oss << \"    \" << emitEdge(getUniqueId(op), getUniqueId(result)) << \"\\n\";\n+  }\n+\n+  oss << \"}\\n\";\n+  return oss.str();\n+}\n+\n+void GraphDumper::dumpToFile(triton::FuncOp func,\n+                             const std::string &filename) const {\n+  std::ofstream ofs(filename);\n+  ofs << dump(func);\n+}\n+\n+std::string GraphDumper::getShapeStr(const Type &type) const {\n+  std::ostringstream oss;\n+  oss << \"[\";\n+  if (auto tensorTy = type.dyn_cast<RankedTensorType>()) {\n+    auto shape = tensorTy.getShape();\n+    for (unsigned i = 0; i < shape.size(); ++i) {\n+      if (i > 0)\n+        oss << \", \";\n+      oss << shape[i];\n+    }\n+  }\n+  oss << \"]\";\n+  return oss.str();\n+}\n+\n+std::string GraphDumper::getUniqueId(Value value) const {\n+  std::ostringstream oss;\n+  oss << value.getImpl();\n+  return oss.str();\n+}\n+\n+std::string GraphDumper::getUniqueId(Operation *op) const {\n+  std::ostringstream oss;\n+  oss << op;\n+  return oss.str();\n+}\n+\n+std::string GraphDumper::emitNode(const std::string &id,\n+                                  const GraphDumper::NodeInfo info) const {\n+  std::ostringstream oss;\n+  oss << \"\\\"\" << id << \"\\\" [\";\n+  for (auto it = info.begin(); it != info.end(); ++it) {\n+    if (it != info.begin())\n+      oss << \", \";\n+    oss << it->first << \" = \\\"\" << it->second << \"\\\"\";\n+  }\n+  oss << \"];\";\n+  return oss.str();\n+}\n+\n+std::string GraphDumper::emitEdge(const std::string &srcId,\n+                                  const std::string &destId) const {\n+  std::ostringstream oss;\n+  oss << \"\\\"\" << srcId << \"\\\" -> \\\"\" << destId << \"\\\";\";\n+  return oss.str();\n+}\n+\n+std::string GraphDumper::emitValueNode(Value value) const {\n+  NodeInfo info = onValue(value);\n+  if (info.find(\"label\") == info.end()) {\n+    std::string shapeStr = getShapeStr(value.getType());\n+    if (auto arg = value.dyn_cast<BlockArgument>())\n+      info[\"label\"] =\n+          \"BlockArg\" + std::to_string(arg.getArgNumber()) + \" \" + shapeStr;\n+    else\n+      info[\"label\"] = shapeStr;\n+  }\n+  return emitNode(getUniqueId(value), info);\n+}\n+\n+std::string GraphDumper::emitOperationNode(Operation *op) const {\n+  NodeInfo info = onOperation(op);\n+  if (info.find(\"label\") == info.end())\n+    info[\"label\"] = op->getName().getStringRef().str();\n+  return emitNode(getUniqueId(op), info);\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// GraphLayoutMarker\n+//===----------------------------------------------------------------------===//\n+\n+GraphDumper::NodeInfo GraphLayoutMarker::onValue(Value value) const {\n+  std::string color = getColor(value.getType());\n+  return {{\"shape\", \"box\"}, {\"style\", \"filled\"}, {\"fillcolor\", color}};\n+}\n+\n+std::string GraphLayoutMarker::getColor(const Type &type) const {\n+  if (auto tensorTy = type.dyn_cast<RankedTensorType>()) {\n+    auto layout = tensorTy.getEncoding();\n+    if (layout.isa<triton::gpu::BlockedEncodingAttr>())\n+      return \"green\";\n+    else if (layout.isa<triton::gpu::SliceEncodingAttr>())\n+      return \"yellow\";\n+    else if (layout.isa<triton::gpu::MmaEncodingAttr>())\n+      return \"lightslateblue\";\n+    else if (layout.isa<triton::gpu::DotOperandEncodingAttr>())\n+      return \"orange\";\n+    else if (layout.isa<triton::gpu::SharedEncodingAttr>())\n+      return \"orangered\";\n+    else\n+      assert(0 && \"Unrecognized layout\");\n+  } else {\n+    return \"white\";\n+  }\n+}\n // -------------------------------------------------------------------------- //\n \n // TODO: Interface\n@@ -89,11 +319,15 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n }\n \n bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n-  // Case 1: A size 1 tensor is not expensive since all threads will load the\n+  // Case 1: Pointer of tensor is always expensive\n+  auto operandType = op->getOperand(0).getType();\n+  if (triton::isTensorPointerType(operandType))\n+    return true;\n+  // Case 2a: A size 1 tensor is not expensive since all threads will load the\n   // same\n   if (isSingleValue(op->getOperand(0)))\n     return false;\n-  // Case 2: Tensor of pointers has more threads than elements\n+  // Case 2b: Tensor of pointers has more threads than elements\n   // we can presume a high hit-rate that makes it cheap to load\n   auto ptrType = op->getOperand(0).getType().cast<RankedTensorType>();\n   auto mod = op->getParentOfType<ModuleOp>();\n@@ -121,7 +355,7 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   return false;\n }\n \n-bool canFoldConversion(Operation *op, Attribute &targetEncoding) {\n+bool canFoldConversion(Operation *op, Attribute targetEncoding) {\n   if (isa<triton::CatOp>(op))\n     return !triton::gpu::isExpensiveCat(cast<triton::CatOp>(op),\n                                         targetEncoding);\n@@ -162,6 +396,12 @@ int simulateBackwardRematerialization(\n         return INT_MAX;\n       if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n         return INT_MAX;\n+      if (auto ptrTy = argI.getType().dyn_cast<triton::PointerType>()) {\n+        if (ptrTy.getPointeeType().isa<RankedTensorType>()) {\n+          return INT_MAX;\n+        }\n+      }\n+\n       Operation *opArgI = argI.getDefiningOp();\n       toConvert.insert({argI, newEncoding});\n       // 1. Only convert RankedTensorType\n@@ -224,6 +464,103 @@ Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n   return newOp;\n }\n \n+namespace {\n+\n+struct OpUseInfo {\n+  Value value;\n+  Operation *op;\n+  unsigned index;\n+};\n+\n+void getForwardSliceOpUseInfo(Operation *op,\n+                              SetVector<Operation *> *forwardSliceOps,\n+                              SmallVector<OpUseInfo> *forwardOpUseInfo) {\n+  if (!op)\n+    return;\n+\n+  for (Region &region : op->getRegions())\n+    for (Block &block : region)\n+      for (Operation &blockOp : block)\n+        if (forwardSliceOps->count(&blockOp) == 0)\n+          getForwardSliceOpUseInfo(&blockOp, forwardSliceOps, forwardOpUseInfo);\n+  for (Value result : op->getResults()) {\n+    for (OpOperand &operand : result.getUses()) {\n+      auto *blockOp = operand.getOwner();\n+      forwardOpUseInfo->push_back(\n+          {operand.get(), blockOp, operand.getOperandNumber()});\n+      if (forwardSliceOps->count(blockOp) == 0)\n+        getForwardSliceOpUseInfo(blockOp, forwardSliceOps, forwardOpUseInfo);\n+    }\n+  }\n+\n+  forwardSliceOps->insert(op);\n+}\n+} // namespace\n+\n+LogicalResult simulateForwardRematerializationInLoop(Operation *startOp,\n+                                                     BlockArgument arg,\n+                                                     Attribute targetEncoding) {\n+  // heuristics for flash attention\n+  if (targetEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+    return failure();\n+  SetVector<Operation *> cvtSliceOps;\n+  SmallVector<OpUseInfo> cvtSliceOpUseInfo;\n+  getForwardSliceOpUseInfo(startOp, &cvtSliceOps, &cvtSliceOpUseInfo);\n+\n+  // Check if any additional conversion is needed along the way\n+  for (Operation *op : cvtSliceOps) {\n+    if (isa<scf::YieldOp>(op))\n+      continue;\n+    // The first op doesn't push forward any conversion\n+    if (op != startOp) {\n+      if (isa<triton::ReduceOp>(op) &&\n+          !op->getResult(0).getType().isa<RankedTensorType>())\n+        return failure();\n+      // don't rematerialize anything expensive\n+      if (isExpensiveToRemat(op, targetEncoding))\n+        return failure();\n+      // don't rematerialize non-element-wise\n+      if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n+          !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n+          !isa<triton::StoreOp, triton::AssertOp, triton::PrintOp,\n+               triton::ReduceOp>(op))\n+        return failure();\n+    }\n+    // don't rematerialize if it adds an extra conversion that can't\n+    // be removed\n+    for (Value value : op->getOperands()) {\n+      Operation *argOp = arg.getDefiningOp();\n+      SetVector<Operation *> processed;\n+      SetVector<Attribute> layout;\n+      llvm::MapVector<Value, Attribute> toConvert;\n+      int numAddedConvs = simulateBackwardRematerialization(\n+          argOp, processed, layout, toConvert, targetEncoding);\n+      if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n+          cvtSliceOps.count(argOp) == 0 && numAddedConvs > 0)\n+        return failure();\n+    }\n+  }\n+\n+  // We apply conservative analysis. Only when the final operand's index\n+  // matches the argument's index or their encoding match, we can rematerialize.\n+  for (auto &opUseInfo : cvtSliceOpUseInfo) {\n+    Operation *op = opUseInfo.op;\n+    if (isa<scf::YieldOp>(op)) {\n+      auto yieldIdx = opUseInfo.index;\n+      // 0 is the induction variable\n+      auto argIdx = arg.getArgNumber() - 1;\n+      if (yieldIdx != argIdx) {\n+        auto argType = arg.getType().cast<RankedTensorType>();\n+        auto yieldType =\n+            op->getOperand(yieldIdx).getType().dyn_cast<RankedTensorType>();\n+        if (!yieldType || argType.getEncoding() != yieldType.getEncoding())\n+          return failure();\n+      }\n+    }\n+  }\n+  return success();\n+}\n+\n void rematerializeConversionChain(\n     const llvm::MapVector<Value, Attribute> &toConvert,\n     mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n@@ -311,9 +648,6 @@ LogicalResult canMoveOutOfLoop(BlockArgument arg,\n   if (cvts.empty())\n     return success();\n   if (cvtTypes.size() == 1) {\n-    // Second condition\n-    if (others.empty())\n-      return success();\n     // Third condition - part 1:\n     // If the other or the cvt is in the different block, we cannot push the\n     // conversion forward or backward\n@@ -329,23 +663,72 @@ LogicalResult canMoveOutOfLoop(BlockArgument arg,\n       if (other->getBlock() != forOp.getBody())\n         return failure();\n       // Third condition - part 3:\n-      // %0 (enc1) = cvt %arg (enc0)\n-      // other %0 (enc1), %1 (enc0) => other %0 (enc1), %1 (enc1)\n-      // Check if %2 (enc1) = cvt %1 (enc0) can be eliminated\n-      SetVector<Operation *> processed;\n-      SetVector<Attribute> layout;\n-      llvm::MapVector<Value, Attribute> toConvert;\n-      for (auto operand : other->getOperands()) {\n-        auto argOp = operand.getDefiningOp();\n-        if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n-            simulateBackwardRematerialization(argOp, processed, layout,\n-                                              toConvert, targetEncoding) > 0)\n-          return failure();\n-      }\n+      // Check if we can directly use arg without conversion\n+      if (simulateForwardRematerializationInLoop(other, arg, targetEncoding)\n+              .failed())\n+        return failure();\n     }\n     return success();\n   }\n   return failure();\n }\n \n+// TODO(thomas): this is duplicated with what is in GPUToLLVM\n+//  Convert an \\param index to a multi-dim coordinate given \\param shape and\n+//  \\param order.\n+SmallVector<Value> delinearize(OpBuilder &b, Location loc, Value linear,\n+                               ArrayRef<unsigned> shape,\n+                               ArrayRef<unsigned> order) {\n+  unsigned rank = shape.size();\n+  assert(rank == order.size());\n+  auto reordered = reorder(shape, order);\n+  auto reorderedMultiDim = delinearize(b, loc, linear, reordered);\n+  SmallVector<Value> multiDim(rank);\n+  for (unsigned i = 0; i < rank; ++i) {\n+    multiDim[order[i]] = reorderedMultiDim[i];\n+  }\n+  return multiDim;\n+}\n+\n+SmallVector<Value> delinearize(OpBuilder &b, Location loc, Value linear,\n+                               ArrayRef<unsigned> shape) {\n+  unsigned rank = shape.size();\n+  assert(rank > 0);\n+  SmallVector<Value> multiDim(rank);\n+  if (rank == 1) {\n+    multiDim[0] = linear;\n+  } else {\n+    Value remained = linear;\n+    for (auto &&en : llvm::enumerate(shape.drop_back())) {\n+      auto dimSize = b.create<arith::ConstantIntOp>(loc, en.value(), 32);\n+      multiDim[en.index()] = b.create<arith::RemSIOp>(loc, remained, dimSize);\n+      remained = b.create<arith::DivSIOp>(loc, remained, dimSize);\n+    }\n+    multiDim[rank - 1] = remained;\n+  }\n+  return multiDim;\n+}\n+\n+Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n+                ArrayRef<unsigned> shape, ArrayRef<unsigned> order) {\n+  return linearize(b, loc, reorder<Value>(multiDim, order),\n+                   reorder<unsigned>(shape, order));\n+}\n+\n+Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n+                ArrayRef<unsigned> shape) {\n+  auto rank = multiDim.size();\n+  Value linear = b.create<arith::ConstantIntOp>(loc, 0, 32);\n+  if (rank > 0) {\n+    linear = multiDim.back();\n+    for (auto [dim, dimShape] :\n+         llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n+      Value dimSize = b.create<arith::ConstantIntOp>(loc, dimShape, 32);\n+      linear = b.create<arith::AddIOp>(\n+          loc, b.create<arith::MulIOp>(loc, linear, dimSize), dim);\n+    }\n+  }\n+  return linear;\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/CMakeLists.txt", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -0,0 +1,2 @@\n+add_subdirectory(IR)\n+add_subdirectory(Transforms)"}, {"filename": "lib/Dialect/TritonNvidiaGPU/IR/CMakeLists.txt", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+add_mlir_dialect_library(TritonNvidiaGPUIR\n+  Dialect.cpp\n+  Ops.cpp\n+  Traits.cpp\n+  Types.cpp\n+\n+  DEPENDS\n+  TritonNvidiaGPUTableGen\n+  TritonNvidiaGPUAttrDefsIncGen\n+\n+  LINK_LIBS PUBLIC\n+  TritonIR\n+  TritonGPUIR\n+)"}, {"filename": "lib/Dialect/TritonNvidiaGPU/IR/Dialect.cpp", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -0,0 +1,69 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+\n+#include <numeric>\n+\n+#include \"mlir/IR/DialectImplementation.h\"\n+#include \"mlir/IR/OpImplementation.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"llvm/ADT/TypeSwitch.h\"\n+#include \"llvm/Support/Debug.h\"\n+\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.cpp.inc\"\n+\n+using namespace mlir;\n+using namespace mlir::triton::nvidia_gpu;\n+\n+//===----------------------------------------------------------------------===//\n+// Attribute methods\n+//===----------------------------------------------------------------------===//\n+#define GET_ATTRDEF_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUAttrDefs.cpp.inc\"\n+\n+//===----------------------------------------------------------------------===//\n+\n+void TritonNvidiaGPUDialect::initialize() {\n+  registerTypes();\n+\n+  addAttributes<\n+#define GET_ATTRDEF_LIST\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUAttrDefs.cpp.inc\"\n+      >();\n+  addOperations<\n+#define GET_OP_LIST\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Ops.cpp.inc\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/OpsEnums.cpp.inc\"\n+      >();\n+}\n+\n+// verify TritonNvidiaGPU ops\n+LogicalResult\n+TritonNvidiaGPUDialect::verifyOperationAttribute(Operation *op,\n+                                                 NamedAttribute attr) {\n+  // TODO: fill this.\n+  return success();\n+}"}, {"filename": "lib/Dialect/TritonNvidiaGPU/IR/Ops.cpp", "status": "added", "additions": 84, "deletions": 0, "changes": 84, "file_content_changes": "@@ -0,0 +1,84 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"mlir/IR/Builders.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+\n+#define GET_OP_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Ops.cpp.inc\"\n+\n+namespace mlir {\n+namespace triton {\n+namespace nvidia_gpu {\n+\n+///--- DotAsyncOp ---\n+mlir::LogicalResult DotAsyncOp::inferReturnTypes(\n+    MLIRContext *context, std::optional<Location> location, ValueRange operands,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n+    SmallVectorImpl<Type> &inferredReturnTypes) {\n+  // type is the same as the accumulator\n+  auto accTy = operands[2].getType().cast<RankedTensorType>();\n+  inferredReturnTypes.push_back(accTy);\n+\n+  // verify encodings\n+  auto aEnc = operands[0].getType().cast<RankedTensorType>().getEncoding();\n+  auto bEnc = operands[1].getType().cast<RankedTensorType>().getEncoding();\n+  auto retEnc = accTy.getEncoding();\n+  if (aEnc) {\n+    assert(bEnc);\n+    Dialect &dialect = aEnc.getDialect();\n+    auto interface = dyn_cast<DialectInferLayoutInterface>(&dialect);\n+    if (interface->inferDotOpEncoding(aEnc, 0, retEnc, location).failed())\n+      return mlir::failure();\n+    if (interface->inferDotOpEncoding(bEnc, 1, retEnc, location).failed())\n+      return mlir::failure();\n+  }\n+  return mlir::success();\n+}\n+\n+///--- Async related ops ---\n+void GetAgentIdOp::build(::mlir::OpBuilder &builder,\n+                         ::mlir::OperationState &state) {\n+  build(builder, state, builder.getI32Type());\n+}\n+\n+void CreateTokenOp::build(::mlir::OpBuilder &builder,\n+                          ::mlir::OperationState &state, uint32_t num) {\n+  auto tokenType = TokenType::get(builder.getContext());\n+  auto resultType = RankedTensorType::get({num}, tokenType);\n+  build(builder, state, resultType, num);\n+}\n+\n+void GetMutexRoleIdOp::build(::mlir::OpBuilder &builder,\n+                             ::mlir::OperationState &state, uint32_t num) {\n+  build(builder, state, builder.getI32Type(), num);\n+}\n+\n+void CreateMutexOp::build(::mlir::OpBuilder &builder,\n+                          ::mlir::OperationState &state) {\n+  build(builder, state, MutexType::get(builder.getContext()));\n+}\n+\n+} // namespace nvidia_gpu\n+} // namespace triton\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/IR/Traits.cpp", "status": "added", "additions": 36, "deletions": 0, "changes": 36, "file_content_changes": "@@ -0,0 +1,36 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Traits.h\"\n+#include \"triton/Analysis/Utility.h\"\n+\n+mlir::LogicalResult\n+mlir::OpTrait::impl::verifySource1IsSharedEncoding(Operation *op) {\n+  if (failed(verifyAtLeastNOperands(op, 2)))\n+    return failure();\n+\n+  if (!mlir::triton::gpu::isSharedEncoding(op->getOperand(1)))\n+    return op->emitOpError() << \"requires operand 1 to be shared encoding\";\n+\n+  return success();\n+};"}, {"filename": "lib/Dialect/TritonNvidiaGPU/IR/Types.cpp", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "file_content_changes": "@@ -0,0 +1,43 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Types.h\"\n+#include \"mlir/IR/DialectImplementation.h\" // required by `Types.cpp.inc`\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"llvm/ADT/TypeSwitch.h\" // required by `Types.cpp.inc`\n+\n+using namespace mlir;\n+using namespace mlir::triton::nvidia_gpu;\n+\n+#define GET_TYPEDEF_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Types.cpp.inc\"\n+\n+//===----------------------------------------------------------------------===//\n+// Triton Dialect\n+//===----------------------------------------------------------------------===//\n+void ::mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect::registerTypes() {\n+  addTypes<\n+#define GET_TYPEDEF_LIST\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Types.cpp.inc\"\n+      >();\n+}"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/CMakeLists.txt", "status": "added", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -0,0 +1,22 @@\n+add_mlir_dialect_library(TritonNvidiaGPUTransforms\n+  MaterializeLoadStore.cpp\n+  PlanCTA.cpp\n+  WSDecomposing.cpp\n+  WSFeasibilityChecking.cpp\n+  WSPipeline.cpp\n+  WSMutex.cpp\n+  WSMaterialization.cpp\n+  FenceInsertion.cpp\n+  RewriteTensorPointer.cpp\n+  Utility.cpp\n+\n+  DEPENDS\n+  TritonNvidiaGPUTransformsIncGen\n+\n+  LINK_LIBS PUBLIC\n+  TritonIR\n+  TritonGPUIR\n+  TritonGPUTransforms\n+  TritonNvidiaGPUIR\n+  MLIRTransformUtils\n+)"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "@@ -0,0 +1,73 @@\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include \"llvm/Support/Debug.h\"\n+\n+//===----------------------------------------------------------------------===//\n+//\n+// This pass works after all other passes, inserting fences to ensure that\n+// memory operations are properly ordered acorss genric and async proxy.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+using namespace mlir;\n+namespace tt = ::mlir::triton;\n+namespace ttg = ::mlir::triton::gpu;\n+namespace ttng = ::mlir::triton::nvidia_gpu;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+namespace {\n+\n+struct FenceInsertionPass\n+    : public TritonGPUFenceInsertionBase<FenceInsertionPass> {\n+\n+public:\n+  FenceInsertionPass() = default;\n+  FenceInsertionPass(int computeCapability) {\n+    this->computeCapability = computeCapability;\n+  }\n+  // TODO: support more patterns to insert fences\n+  // only support insertion between convert layout ops and dot ops to protect\n+  // flashattention\n+  void runOnOperation() override {\n+    // Only insert fences for compute capability 9.0\n+    if (computeCapability < 90)\n+      return;\n+    ModuleOp mod = getOperation();\n+    mod.walk([&](Operation *op) {\n+      if (isa<tt::DotOp>(op)) {\n+        auto a = op->getOperand(0);\n+        auto b = op->getOperand(1);\n+        auto mmaEncoding = op->getResult(0)\n+                               .getType()\n+                               .cast<RankedTensorType>()\n+                               .getEncoding()\n+                               .dyn_cast<ttg::MmaEncodingAttr>();\n+        auto isHopperEncoding = mmaEncoding && mmaEncoding.isHopper();\n+        if (isHopperEncoding && (isa<ttg::ConvertLayoutOp>(a.getDefiningOp()) &&\n+                                 ttg::isSharedEncoding(a)) ||\n+            (isa<ttg::ConvertLayoutOp>(b.getDefiningOp()) &&\n+             ttg::isSharedEncoding(b))) {\n+\n+          // TODO: check whether cluster fence is needed\n+          OpBuilder builder(op);\n+          builder.create<ttng::FenceAsyncSharedOp>(op->getLoc(),\n+                                                   false /*bCluster*/);\n+        }\n+      }\n+    });\n+  }\n+};\n+\n+} // namespace\n+\n+std::unique_ptr<Pass>\n+mlir::createTritonNvidiaGPUFenceInsertionPass(int computeCapability) {\n+  return std::make_unique<FenceInsertionPass>(computeCapability);\n+}"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/MaterializeLoadStore.cpp", "status": "added", "additions": 187, "deletions": 0, "changes": 187, "file_content_changes": "@@ -0,0 +1,187 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n+#include \"llvm/Support/Debug.h\"\n+#include <numeric>\n+\n+//===----------------------------------------------------------------------===//\n+//\n+// This pass works after pipeline pass, converts the remaining tt.LoadOp taking\n+// ptr<tensor> as input into ttg.InsertSliceAsyncOp and emit proper barriers\n+//\n+//===----------------------------------------------------------------------===//\n+\n+using namespace mlir;\n+namespace ttg = mlir::triton::gpu;\n+namespace ttng = mlir::triton::nvidia_gpu;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::getCTALayout;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+namespace {\n+\n+struct MaterializeLoadStorePass\n+    : public MaterializeLoadStoreBase<MaterializeLoadStorePass> {\n+\n+public:\n+  MaterializeLoadStorePass() = default;\n+  MaterializeLoadStorePass(int numWarps, int computeCapability) {\n+    this->numWarps = numWarps;\n+    this->computeCapability = computeCapability;\n+  }\n+\n+  void runOnOperation() override {\n+    SmallVector<mlir::triton::LoadOp> worklists;\n+    getOperation()->walk([&](mlir::triton::LoadOp load) -> void {\n+      if (isLoadFromTensorPtr(load)) {\n+        worklists.push_back(load);\n+      }\n+    });\n+    for (auto load : worklists) {\n+      materializeLoadTilePtr(load);\n+    }\n+\n+    SmallVector<mlir::triton::StoreOp> storeOpWorklists;\n+    getOperation()->walk([&](mlir::triton::StoreOp store) -> void {\n+      if (isStoreToTensorPtr(store)) {\n+        storeOpWorklists.push_back(store);\n+      }\n+    });\n+    for (auto store : storeOpWorklists) {\n+      materializeStoreTilePtr(store);\n+    }\n+  }\n+\n+private:\n+  void materializeLoadTilePtr(mlir::triton::LoadOp load);\n+  void materializeStoreTilePtr(mlir::triton::StoreOp store);\n+};\n+\n+void MaterializeLoadStorePass::materializeLoadTilePtr(\n+    mlir::triton::LoadOp load) {\n+  if (computeCapability < 90)\n+    return;\n+  if (!::triton::tools::getBoolEnv(\"ENABLE_TMA\"))\n+    return;\n+  auto loc = load.getLoc();\n+  OpBuilder b(load);\n+  auto loadTy = load.getType().dyn_cast<RankedTensorType>();\n+  auto loadShape = loadTy.getShape();\n+  auto CTASplitNum = ttg::getCTASplitNum(loadTy.getEncoding());\n+  auto shapePerSlice = ttg::getShapePerCTA(CTASplitNum, loadShape);\n+  auto elemTy = loadTy.getElementType();\n+  assert(loadTy);\n+  SmallVector<int64_t> bufferShape(loadShape.begin(), loadShape.end());\n+  bufferShape.insert(bufferShape.begin(), 1);\n+\n+  auto sharedEncoding = getSharedEncoding(loadTy);\n+  auto bufferTy = RankedTensorType::get(bufferShape, elemTy, sharedEncoding);\n+  Value buffer = b.create<ttg::AllocTensorOp>(loc, bufferTy);\n+  unsigned elems = std::accumulate(shapePerSlice.begin(), shapePerSlice.end(),\n+                                   1, std::multiplies{});\n+  elems *= (elemTy.getIntOrFloatBitWidth() / 8);\n+  auto mBarrierTy = mlir::triton::PointerType::get(b.getIntegerType(64), 3);\n+  Value mBarrier = b.create<ttng::AllocMBarrierOp>(loc, mBarrierTy, 1);\n+  Value _0 = b.create<arith::ConstantIntOp>(loc, 0, 32);\n+  Value threadId = b.create<ttng::GetThreadIdOp>(loc);\n+  Value pred =\n+      b.create<arith::CmpIOp>(loc, arith::CmpIPredicate::eq, threadId, _0);\n+  b.create<ttng::MBarrierArriveOp>(loc, mBarrier, pred, /*remoteCtaId*/ nullptr,\n+                                   /*trackAsyncOp*/ false, elems);\n+  Value inserted = b.create<ttng::InsertSliceAsyncV2Op>(\n+      loc, bufferTy, load.getPtr(), buffer,\n+      /*index*/ _0, mBarrier, load.getMask(), load.getOther(), load.getCache(),\n+      load.getEvict(), load.getIsVolatile(),\n+      /*axis*/ 0);\n+  auto extractedTy = RankedTensorType::get(loadShape, elemTy, sharedEncoding);\n+  Value extracted = b.create<mlir::triton::gpu::ExtractSliceOp>(\n+      loc, extractedTy, inserted,\n+      SmallVector<OpFoldResult>{b.getI64IntegerAttr(0), b.getI64IntegerAttr(0),\n+                                b.getI64IntegerAttr(0)},\n+      SmallVector<OpFoldResult>{b.getI64IntegerAttr(1),\n+                                b.getI64IntegerAttr(loadShape[0]),\n+                                b.getI64IntegerAttr(loadShape[1])},\n+      SmallVector<OpFoldResult>{b.getI64IntegerAttr(1), b.getI64IntegerAttr(1),\n+                                b.getI64IntegerAttr(1)});\n+\n+  Value phase = b.create<arith::ConstantIntOp>(loc, 0, 1);\n+  b.create<ttng::MBarrierWaitOp>(loc, mBarrier, phase);\n+  Value newValue =\n+      b.create<ttg::ConvertLayoutOp>(loc, load.getType(), extracted);\n+  load.getResult().replaceAllUsesWith(newValue);\n+  load->erase();\n+}\n+\n+void MaterializeLoadStorePass::materializeStoreTilePtr(\n+    mlir::triton::StoreOp store) {\n+  if (computeCapability < 90 || !::triton::tools::getBoolEnv(\"ENABLE_TMA\"))\n+    return;\n+  auto loc = store.getLoc();\n+  OpBuilder builder(store);\n+  auto *ctx = store.getContext();\n+  auto value = store.getValue();\n+  auto dst = store.getPtr();\n+  auto storeTy = value.getType().dyn_cast<RankedTensorType>();\n+  assert(storeTy);\n+  auto storeElemTy = storeTy.getElementType();\n+  auto ctaLayout = getCTALayout(storeTy.getEncoding());\n+  auto storeShape = storeTy.getShape();\n+  SmallVector<int64_t> bufferShape(storeShape.begin(), storeShape.end());\n+  auto rank = storeShape.size();\n+  // The order of smem should be consistent with gmem.\n+  auto makeTensorPtrOp = getMakeTensorPtrOp(dst);\n+  SmallVector<unsigned> sharedOrder;\n+  for (auto o : makeTensorPtrOp.getOrder()) {\n+    sharedOrder.emplace_back(o);\n+  }\n+  auto sharedEncoding = SharedEncodingAttr::get(ctx, storeShape, sharedOrder,\n+                                                ctaLayout, storeElemTy);\n+  auto bufferTy =\n+      RankedTensorType::get(bufferShape, storeElemTy, sharedEncoding);\n+  Value cvt = builder.create<ttg::ConvertLayoutOp>(loc, bufferTy, value);\n+  builder.create<ttng::StoreAsyncOp>(loc, dst, cvt);\n+  builder.create<mlir::triton::gpu::AsyncBulkCommitGroupOp>(loc);\n+  builder.create<mlir::triton::gpu::AsyncBulkWaitOp>(loc, 0);\n+  store->erase();\n+}\n+\n+} // anonymous namespace\n+\n+std::unique_ptr<Pass>\n+mlir::createTritonNvidiaGPUMaterializeLoadStorePass(int numWarps,\n+                                                    int computeCapability) {\n+  return std::make_unique<MaterializeLoadStorePass>(numWarps,\n+                                                    computeCapability);\n+}"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp", "status": "added", "additions": 1013, "deletions": 0, "changes": 1013, "file_content_changes": "@@ -0,0 +1,1013 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include <queue>\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+namespace {\n+\n+using namespace mlir;\n+namespace ttg = ::mlir::triton::gpu;\n+namespace ttng = ::mlir::triton::nvidia_gpu;\n+\n+// TODO: use ConvertLayoutOp\n+using CastOp = ::mlir::UnrealizedConversionCastOp;\n+\n+unsigned getNumUsers(Value value) {\n+  return std::distance(value.user_begin(), value.user_end());\n+}\n+\n+Type replaceLayout(const Type &type, const Attribute &newLayout) {\n+  Type curType = type;\n+  auto ptrTy = curType.dyn_cast<triton::PointerType>();\n+  if (ptrTy)\n+    curType = ptrTy.getPointeeType();\n+  if (auto tensorTy = curType.dyn_cast<RankedTensorType>())\n+    curType = RankedTensorType::get(tensorTy.getShape(),\n+                                    tensorTy.getElementType(), newLayout);\n+  if (ptrTy)\n+    curType = triton::PointerType::get(curType, ptrTy.getAddressSpace());\n+  return curType;\n+}\n+\n+Attribute replaceCTALayout(Attribute layout, llvm::ArrayRef<int64_t> shape,\n+                           const ttg::CTALayoutAttr &newCTALayout) {\n+  if (auto blockedLayout = layout.dyn_cast<ttg::BlockedEncodingAttr>()) {\n+    return ttg::BlockedEncodingAttr::get(\n+        layout.getContext(), shape, blockedLayout.getSizePerThread(),\n+        blockedLayout.getOrder(), ttg::getNumWarpsPerCTA(layout), 32,\n+        newCTALayout);\n+  } else if (auto sliceLayout = layout.dyn_cast<ttg::SliceEncodingAttr>()) {\n+    return ttg::SliceEncodingAttr::get(\n+        layout.getContext(), sliceLayout.getDim(),\n+        replaceCTALayout(sliceLayout.getParent(), shape, newCTALayout));\n+  } else {\n+    // Other layouts are generated by passes after PlanCTAPass\n+    assert(0 && \"replaceCTALayout not implemented\");\n+  }\n+}\n+\n+class CTAPlanner {\n+public:\n+  CTAPlanner(ttng::ClusterInfo *clusterInfo_);\n+  ~CTAPlanner();\n+\n+  void run(triton::FuncOp &funcOp);\n+\n+private:\n+  CastOp markBackward(CastOp cast) const;\n+  CastOp markForward(CastOp cast) const;\n+  bool isBackward(CastOp cast) const;\n+  bool isForward(CastOp cast) const;\n+\n+  void setTiling(llvm::ArrayRef<unsigned> CTAsPerCGA);\n+  bool processDot(triton::FuncOp &funcOp);\n+  bool processReduce(triton::FuncOp &funcOp);\n+  void processStoreLikeOps(triton::FuncOp &funcOp);\n+\n+  bool propagate(CastOp cast);\n+  bool propagateBackward(CastOp cast);\n+  bool propagateForward(CastOp cast);\n+\n+  void eraseCastOp(CastOp cast);\n+  void eraseCastOpFromQueue(CastOp cast);\n+  void eraseCastOpsFromQueue(llvm::ArrayRef<CastOp> casts);\n+\n+  void insertCasts(Operation *op, llvm::ArrayRef<Attribute> newOperandLayouts,\n+                   llvm::ArrayRef<Attribute> newResultLayouts);\n+  void eliminateAdjacentCasts(CastOp cast0, CastOp cast1);\n+\n+  bool isLoadStoreOp(Operation *op) const;\n+  bool processLoadStore(Operation *op, Attribute layout);\n+\n+  bool isElementwiseOp(Operation *op) const;\n+  bool processElementwise(Operation *op, Attribute layout);\n+\n+  bool processConstant(arith::ConstantOp constant, Attribute layout);\n+  bool processSplat(triton::SplatOp splat, Attribute layout);\n+  bool processMakeRange(triton::MakeRangeOp makeRange, Attribute layout);\n+  bool processMakeTensorPtr(triton::MakeTensorPtrOp makeTensorPtr,\n+                            Attribute layout);\n+\n+  bool processBroadcast(triton::BroadcastOp broadcast, Attribute layout);\n+  bool processExpandDimsBackward(triton::ExpandDimsOp expandDims,\n+                                 Attribute newResultLayout);\n+  bool processExpandDimsForward(triton::ExpandDimsOp expandDims,\n+                                Attribute newSrcLayout);\n+\n+  bool processConvertLayoutBackward(ttg::ConvertLayoutOp convertLayout,\n+                                    CastOp cast);\n+  bool processConvertLayoutForward(ttg::ConvertLayoutOp convertLayout,\n+                                   CastOp cast);\n+\n+  bool processIfOp(scf::IfOp ifOp, int index, const Type &newType);\n+  bool processForOp(scf::ForOp forOp, int index, const Type &newType);\n+\n+  bool processIfOpBackward(scf::IfOp ifOp, CastOp cast);\n+  bool processForOpBackward(scf::ForOp forOp, CastOp cast);\n+  bool processBlockArgBackward(BlockArgument arg, CastOp cast);\n+  bool processForOpForward(scf::ForOp forOp, CastOp cast);\n+  bool processYieldOpForward(scf::YieldOp yieldOp, CastOp cast);\n+\n+  bool processOpFallback(Operation *op);\n+\n+  bool processMultiUsersBackward(Value input, CastOp cast);\n+  bool processMultiUsersForward(Value output, CastOp cast);\n+\n+  // This flag indicates whether clusterInfo needs to be deleted in the\n+  // destructor of CTAPlanner. The flag `ownInfo` is set to false when a\n+  // non-null pointer to clusterInfo is passed to the constructor of CTAPlanner.\n+  // Otherwise, a self-managed ClusterInfo will be created and the ownInfo will\n+  // be set to true.\n+  bool ownInfo;\n+  ttng::ClusterInfo *clusterInfo;\n+  bool tiled;\n+  unsigned step;\n+  unsigned stepUnchanged;\n+  std::queue<CastOp> queue;\n+};\n+\n+CTAPlanner::CTAPlanner(ttng::ClusterInfo *clusterInfo_)\n+    : ownInfo(false), clusterInfo(clusterInfo_), tiled(false), step(0),\n+      stepUnchanged(0) {\n+  if (clusterInfo == nullptr) {\n+    clusterInfo = new ttng::ClusterInfo();\n+    ownInfo = true;\n+  }\n+}\n+\n+CTAPlanner::~CTAPlanner() {\n+  if (ownInfo) {\n+    delete clusterInfo;\n+    // Actually not necessary but safer\n+    ownInfo = false;\n+    clusterInfo = nullptr;\n+  }\n+}\n+\n+void CTAPlanner::run(triton::FuncOp &funcOp) {\n+  assert(!tiled && \"Please create a new CTAPlanner\");\n+  static const unsigned maxSteps = 10000;\n+\n+  auto nextStep = [&]() {\n+    ++step;\n+    assert(step < maxSteps && \"Maximum number of steps exceeded\");\n+  };\n+\n+  processDot(funcOp);\n+  nextStep();\n+\n+  processReduce(funcOp);\n+  nextStep();\n+\n+  if (!tiled) {\n+    processStoreLikeOps(funcOp);\n+    nextStep();\n+  }\n+\n+  while (!queue.empty()) {\n+    CastOp cast = queue.front();\n+    queue.pop();\n+    bool changed = propagate(cast);\n+    if (changed) {\n+      stepUnchanged = 0;\n+    } else {\n+      queue.push(cast);\n+      ++stepUnchanged;\n+    }\n+    nextStep();\n+  }\n+}\n+\n+CastOp CTAPlanner::markBackward(CastOp cast) const {\n+  cast->setAttr(\"direction\", StringAttr::get(cast.getContext(), \"backward\"));\n+  return cast;\n+}\n+\n+CastOp CTAPlanner::markForward(CastOp cast) const {\n+  cast->setAttr(\"direction\", StringAttr::get(cast.getContext(), \"forward\"));\n+  return cast;\n+}\n+\n+bool CTAPlanner::isBackward(CastOp cast) const {\n+  return cast->getAttrOfType<StringAttr>(\"direction\") == \"backward\";\n+}\n+\n+bool CTAPlanner::isForward(CastOp cast) const {\n+  return cast->getAttrOfType<StringAttr>(\"direction\") == \"forward\";\n+}\n+\n+void CTAPlanner::setTiling(llvm::ArrayRef<unsigned> CTAsPerCGA) {\n+  assert(!tiled && \"CTA tiling is already determinted\");\n+  assert(clusterInfo && \"ClusterInfo pointer is null\");\n+  assert(CTAsPerCGA.size() <= 3 && \"setTiling not implemented\");\n+  if (CTAsPerCGA.size() > 0)\n+    clusterInfo->clusterDimX = CTAsPerCGA[0];\n+  if (CTAsPerCGA.size() > 1)\n+    clusterInfo->clusterDimY = CTAsPerCGA[1];\n+  if (CTAsPerCGA.size() > 2)\n+    clusterInfo->clusterDimZ = CTAsPerCGA[2];\n+  tiled = true;\n+}\n+\n+bool CTAPlanner::processDot(triton::FuncOp &funcOp) {\n+  // TODO: This is a naive implementation and should be refactored\n+  auto getCTATiling = [](int64_t M, int64_t N, int64_t K,\n+                         unsigned numCTAs) -> std::pair<unsigned, unsigned> {\n+    unsigned splitM = std::clamp<unsigned>(M / 64, 1, numCTAs);\n+    unsigned splitN = numCTAs / splitM;\n+    return {splitM, splitN};\n+  };\n+\n+  funcOp.walk([&](triton::DotOp dot) {\n+    MLIRContext *ctx = dot.getContext();\n+\n+    auto aTy = dot.getA().getType().cast<RankedTensorType>();\n+    auto bTy = dot.getB().getType().cast<RankedTensorType>();\n+    auto dTy = dot.getD().getType().cast<RankedTensorType>();\n+\n+    assert(aTy.getEncoding().isa<ttg::DotOperandEncodingAttr>() &&\n+           bTy.getEncoding().isa<ttg::DotOperandEncodingAttr>() &&\n+           dTy.getEncoding().isa<ttg::BlockedEncodingAttr>() &&\n+           \"PlanCTAPass should follow immediately after CoalescePass\");\n+\n+    auto aLayout = aTy.getEncoding().cast<ttg::DotOperandEncodingAttr>();\n+    auto bLayout = bTy.getEncoding().cast<ttg::DotOperandEncodingAttr>();\n+    auto dLayout = dTy.getEncoding().cast<ttg::BlockedEncodingAttr>();\n+\n+    unsigned M = dTy.getShape()[0];\n+    unsigned N = dTy.getShape()[1];\n+    unsigned K = aTy.getShape()[1];\n+\n+    unsigned splitM, splitN;\n+    std::tie(splitM, splitN) = getCTATiling(M, N, K, ttg::getNumCTAs(dLayout));\n+    // FIXME: Should consider IR with more than one DotOps\n+    setTiling({splitM, splitN, 1});\n+\n+    auto newCTALayout = ttg::CTALayoutAttr::get(ctx, {splitM, splitN},\n+                                                {splitM, splitN}, {1, 0});\n+    auto newDLayout = ttg::BlockedEncodingAttr::get(\n+        ctx, dTy.getShape(), dLayout.getSizePerThread(), dLayout.getOrder(),\n+        ttg::getNumWarpsPerCTA(dLayout), 32, newCTALayout);\n+    auto newALayout = ttg::DotOperandEncodingAttr::get(ctx, aLayout.getOpIdx(),\n+                                                       newDLayout, 0);\n+    auto newBLayout = ttg::DotOperandEncodingAttr::get(ctx, bLayout.getOpIdx(),\n+                                                       newDLayout, 0);\n+\n+    insertCasts(dot.getOperation(), {newALayout, newBLayout, newDLayout},\n+                {newDLayout});\n+  });\n+\n+  return true;\n+}\n+\n+bool CTAPlanner::processReduce(triton::FuncOp &funcOp) {\n+  ModuleOp mod = funcOp->getParentOfType<ModuleOp>();\n+  unsigned numCTAs = ttg::TritonGPUDialect::getNumCTAs(mod);\n+\n+  funcOp.walk([&](triton::ReduceOp reduce) {\n+    MLIRContext *context = reduce.getContext();\n+    Value src = reduce.getOperands()[0];\n+    unsigned axis = reduce.getAxis();\n+\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    auto srcLayout = srcTy.getEncoding();\n+\n+    auto rank = srcShape.size();\n+    auto order = ttg::getOrder(srcLayout);\n+    auto sizePerThread = ttg::getSizePerThread(srcLayout);\n+    auto CTAOrder = ttg::getCTAOrder(srcLayout);\n+\n+    llvm::SmallVector<unsigned> CTAsPerCGA(rank, 0);\n+    unsigned remainingCTAs = numCTAs;\n+    for (int i = rank - 1; i >= 0; --i) {\n+      unsigned dim = order[i];\n+      if (dim == axis) {\n+        CTAsPerCGA[dim] = 1;\n+      } else {\n+        CTAsPerCGA[dim] = std::min<unsigned>(srcShape[dim] / sizePerThread[dim],\n+                                             remainingCTAs);\n+        remainingCTAs /= CTAsPerCGA[dim];\n+      }\n+    }\n+\n+    for (int i = rank - 1; i >= 0; --i) {\n+      unsigned dim = order[i];\n+      if (dim != axis) {\n+        CTAsPerCGA[dim] *= remainingCTAs;\n+        break;\n+      }\n+    }\n+\n+    auto CTALayout =\n+        ttg::CTALayoutAttr::get(context, CTAsPerCGA, CTAsPerCGA, CTAOrder);\n+    if (!tiled)\n+      setTiling(CTALayout.getCTAsPerCGA());\n+    auto newSrcLayout = replaceCTALayout(srcLayout, srcShape, CTALayout);\n+    auto newResultLayout =\n+        ttg::SliceEncodingAttr::get(context, axis, newSrcLayout);\n+    unsigned numOperands = reduce.getNumOperands();\n+    SmallVector<Attribute> newSrcLayoutVec(numOperands, newSrcLayout);\n+    SmallVector<Attribute> newResultLayoutVec(numOperands, newResultLayout);\n+\n+    insertCasts(reduce.getOperation(), newSrcLayoutVec, newResultLayoutVec);\n+  });\n+  return true;\n+}\n+\n+void CTAPlanner::processStoreLikeOps(triton::FuncOp &funcOp) {\n+  assert(!tiled && \"CTA tiling is already determinted\");\n+\n+  llvm::SmallVector<Operation *> stores;\n+  funcOp.walk([&](Operation *op) {\n+    if (llvm::isa<triton::StoreOp, triton::AtomicRMWOp, triton::AtomicCASOp>(\n+            op))\n+      stores.push_back(op);\n+  });\n+  assert(stores.size() > 0 && \"Cannot find store-like ops\");\n+\n+  ttg::CTALayoutAttr CTALayout;\n+  for (Operation *store : stores) {\n+    if (auto tensorTy =\n+            store->getOperand(0).getType().dyn_cast<RankedTensorType>()) {\n+      if (!tiled) {\n+        // Use CTA tiling of the first store-like op as global CTA tiling\n+        CTALayout = ttg::getCTALayout(tensorTy.getEncoding());\n+        setTiling(CTALayout.getCTAsPerCGA());\n+      }\n+      auto newLayout = replaceCTALayout(tensorTy.getEncoding(),\n+                                        tensorTy.getShape(), CTALayout);\n+      processElementwise(store, newLayout);\n+    }\n+  }\n+\n+  // If all store-like ops are processing scalar values and no ReduceOp is\n+  // found, we can conclude that this is an all-scalar computation, since\n+  // ReduceOp is the only op that converts tensor values to scalar values.\n+  if (!tiled)\n+    setTiling({1, 1, 1});\n+}\n+\n+bool CTAPlanner::propagate(CastOp cast) {\n+  return isBackward(cast) ? propagateBackward(cast) : propagateForward(cast);\n+}\n+\n+bool CTAPlanner::propagateBackward(CastOp cast) {\n+  Value input = cast.getOperand(0);\n+  Value output = cast.getResult(0);\n+  unsigned numUsers = getNumUsers(input);\n+  if (numUsers == 0) {\n+    assert(0 && \"Unreachable branch\");\n+  } else if (numUsers == 1) {\n+    Type outTy = output.getType();\n+    if (auto ptrTy = outTy.dyn_cast<triton::PointerType>())\n+      outTy = ptrTy.getPointeeType();\n+    Attribute layout = outTy.cast<RankedTensorType>().getEncoding();\n+    Operation *op = input.getDefiningOp();\n+    if (op == nullptr) {\n+      assert(input.isa<BlockArgument>() &&\n+             \"Unexpected Value without defining op\");\n+      processBlockArgBackward(input.cast<BlockArgument>(), cast);\n+    } else if (auto prevCast = llvm::dyn_cast<CastOp>(op)) {\n+      eliminateAdjacentCasts(prevCast, cast);\n+    } else if (isLoadStoreOp(op)) {\n+      processLoadStore(op, layout);\n+    } else if (isElementwiseOp(op)) {\n+      processElementwise(op, layout);\n+    } else if (auto constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n+      processConstant(constant, layout);\n+    } else if (auto splat = llvm::dyn_cast<triton::SplatOp>(op)) {\n+      processSplat(splat, layout);\n+    } else if (auto makeRange = llvm::dyn_cast<triton::MakeRangeOp>(op)) {\n+      processMakeRange(makeRange, layout);\n+    } else if (auto makeTensorPtr =\n+                   llvm::dyn_cast<triton::MakeTensorPtrOp>(op)) {\n+      processMakeTensorPtr(makeTensorPtr, layout);\n+    } else if (llvm::isa<triton::AdvanceOp>(op)) {\n+      // ptr operand and result have the same layout, while other operands are\n+      // scalar values\n+      processElementwise(op, layout);\n+    } else if (auto broadcast = llvm::dyn_cast<triton::BroadcastOp>(op)) {\n+      processBroadcast(broadcast, layout);\n+    } else if (auto expandDims = llvm::dyn_cast<triton::ExpandDimsOp>(op)) {\n+      processExpandDimsBackward(expandDims, layout);\n+    } else if (auto ifOp = llvm::dyn_cast<scf::IfOp>(op)) {\n+      processIfOpBackward(ifOp, cast);\n+    } else if (auto forOp = llvm::dyn_cast<scf::ForOp>(op)) {\n+      processForOpBackward(forOp, cast);\n+    } else if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(op)) {\n+      return processConvertLayoutBackward(convertLayout, cast);\n+    } else {\n+      // Keep original layouts. This may result in a loss of performance.\n+      return processOpFallback(op);\n+    }\n+    return true;\n+  } else {\n+    return processMultiUsersBackward(input, cast);\n+  }\n+}\n+\n+bool CTAPlanner::propagateForward(CastOp cast) {\n+  Value input = cast.getOperand(0);\n+  Value output = cast.getResult(0);\n+  unsigned numUsers = getNumUsers(output);\n+  if (numUsers == 0) {\n+    cast.erase();\n+  } else if (numUsers == 1) {\n+    Type inTy = input.getType();\n+    if (auto ptrTy = inTy.dyn_cast<triton::PointerType>())\n+      inTy = ptrTy.getPointeeType();\n+    Attribute layout = inTy.cast<RankedTensorType>().getEncoding();\n+    Operation *op = *output.user_begin();\n+    if (auto nextCast = llvm::dyn_cast<CastOp>(op)) {\n+      eliminateAdjacentCasts(cast, nextCast);\n+    } else if (isLoadStoreOp(op)) {\n+      processLoadStore(op, layout);\n+    } else if (isElementwiseOp(op)) {\n+      processElementwise(op, layout);\n+    } else if (llvm::isa<triton::AdvanceOp>(op)) {\n+      // ptr operand and result have the same layout, while other operands are\n+      // scalar values\n+      processElementwise(op, layout);\n+    } else if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(op)) {\n+      return processConvertLayoutForward(convertLayout, cast);\n+    } else if (auto forOp = llvm::dyn_cast<scf::ForOp>(op)) {\n+      processForOpForward(forOp, cast);\n+    } else if (auto yieldOp = llvm::dyn_cast<scf::YieldOp>(op)) {\n+      processYieldOpForward(yieldOp, cast);\n+    } else {\n+      // Keep original layouts. This may result in a loss of performance.\n+      return processOpFallback(op);\n+    }\n+  } else {\n+    processMultiUsersForward(output, cast);\n+  }\n+  return true;\n+}\n+\n+void CTAPlanner::eraseCastOp(CastOp cast) {\n+  Value output = cast.getResult(0);\n+  assert(getNumUsers(output) == 0 &&\n+         \"Cannot erase CastOp because it is still in use\");\n+  cast.erase();\n+}\n+\n+void CTAPlanner::eraseCastOpFromQueue(CastOp cast) {\n+  eraseCastOpsFromQueue({cast});\n+}\n+\n+void CTAPlanner::eraseCastOpsFromQueue(llvm::ArrayRef<CastOp> casts) {\n+  llvm::DenseSet<CastOp> erased;\n+  for (CastOp cast : casts) {\n+    eraseCastOp(cast);\n+    erased.insert(cast);\n+  }\n+\n+  decltype(queue) tempQueue;\n+  std::swap(queue, tempQueue);\n+\n+  // This is only a naive implementation. Should refactor with linked-list.\n+  while (!tempQueue.empty()) {\n+    auto cast = tempQueue.front();\n+    tempQueue.pop();\n+    if (!erased.contains(cast))\n+      queue.push(cast);\n+  }\n+}\n+\n+void CTAPlanner::insertCasts(Operation *op,\n+                             llvm::ArrayRef<Attribute> newOperandLayouts,\n+                             llvm::ArrayRef<Attribute> newResultLayouts) {\n+  assert(op->getNumOperands() == newOperandLayouts.size() &&\n+         \"NumOperands mismatched\");\n+  assert(op->getNumResults() == newResultLayouts.size() &&\n+         \"NumResults mismatched\");\n+\n+  Location loc = op->getLoc();\n+  OpBuilder builder(op->getContext());\n+\n+  builder.setInsertionPoint(op);\n+  for (unsigned i = 0; i < op->getNumOperands(); ++i) {\n+    Value operand = op->getOperand(i);\n+    auto operandTy = operand.getType();\n+    if (triton::isTensorOrTensorPointerType(operandTy)) {\n+      operandTy = replaceLayout(operandTy, newOperandLayouts[i]);\n+      auto cast = markBackward(builder.create<CastOp>(loc, operandTy, operand));\n+      op->setOperand(i, cast.getResult(0));\n+      queue.push(cast);\n+    }\n+  }\n+\n+  builder.setInsertionPointAfter(op);\n+  for (unsigned i = 0; i < op->getNumResults(); ++i) {\n+    Value result = op->getResult(i);\n+    auto resultTy = result.getType();\n+    if (triton::isTensorOrTensorPointerType(resultTy)) {\n+      resultTy = replaceLayout(resultTy, newResultLayouts[i]);\n+      auto cast =\n+          markForward(builder.create<CastOp>(loc, result.getType(), result));\n+      result.setType(resultTy);\n+      result.replaceAllUsesExcept(cast.getResult(0), cast.getOperation());\n+      queue.push(cast);\n+    }\n+  }\n+}\n+\n+void CTAPlanner::eliminateAdjacentCasts(CastOp cast0, CastOp cast1) {\n+  assert(cast0.getResult(0) == cast1.getOperand(0) &&\n+         \"The two casts are not adjacent\");\n+  assert(isForward(cast0) && isBackward(cast1) &&\n+         \"Expected pattern of adjacent casts: forward + backward\");\n+\n+  Value input = cast0.getOperand(0);\n+  Value output = cast1.getResult(0);\n+\n+  if (input.getType() == output.getType()) {\n+    output.replaceAllUsesWith(input);\n+    eraseCastOpsFromQueue({cast1, cast0});\n+  } else {\n+    OpBuilder builder(cast1.getOperation());\n+    auto cvt = builder.create<ttg::ConvertLayoutOp>(cast1.getLoc(),\n+                                                    output.getType(), input);\n+    output.replaceAllUsesWith(cvt.getResult());\n+    eraseCastOpsFromQueue({cast1, cast0});\n+  }\n+}\n+\n+bool CTAPlanner::isLoadStoreOp(Operation *op) const {\n+  return llvm::isa<triton::LoadOp, triton::StoreOp, triton::AtomicRMWOp,\n+                   triton::AtomicCASOp>(op);\n+}\n+\n+bool CTAPlanner::processLoadStore(Operation *op, Attribute layout) {\n+  // Special logic for:\n+  //     LoadOp -> SliceLayout\n+  // Transform to:\n+  //     LoadOp -> originalLayout -> ConvertLayout(DSmem) -> SliceLayout\n+  if (auto sliceLayout = layout.dyn_cast<ttg::SliceEncodingAttr>()) {\n+    auto dim = sliceLayout.getDim();\n+    auto CTAsPerCGA = ttg::getCTAsPerCGA(sliceLayout.getParent());\n+    if (CTAsPerCGA[dim] > 1) {\n+      // Find an input or output value of LoadOp or StoreOp to get its layout\n+      Value val =\n+          op->getNumResults() > 0 ? op->getResult(0) : op->getOperand(0);\n+      Attribute originalLayout =\n+          val.getType().cast<RankedTensorType>().getEncoding();\n+      // Insert casts using originalLayout. Adjacent casts will be eliminated\n+      // and generate a ConvertLayoutOp with DSmem access\n+      return processLoadStore(op, originalLayout);\n+    }\n+  }\n+\n+  auto CTALayout = ttg::getCTALayout(layout);\n+\n+  llvm::SmallVector<Attribute> newOperandLayouts;\n+  for (unsigned i = 0; i < op->getNumOperands(); ++i) {\n+    auto type = op->getOperand(i).getType();\n+    if (auto ptrTy = type.dyn_cast<triton::PointerType>())\n+      type = ptrTy.getPointeeType();\n+    auto tensorTy = type.cast<RankedTensorType>();\n+    auto newLayout = replaceCTALayout(tensorTy.getEncoding(),\n+                                      tensorTy.getShape(), CTALayout);\n+    newOperandLayouts.push_back(newLayout);\n+  }\n+\n+  llvm::SmallVector<Attribute> newResultLayouts;\n+  for (unsigned i = 0; i < op->getNumResults(); ++i) {\n+    auto type = op->getResult(i).getType();\n+    if (auto ptrTy = type.dyn_cast<triton::PointerType>())\n+      type = ptrTy.getPointeeType();\n+    auto tensorTy = type.cast<RankedTensorType>();\n+    auto newLayout = replaceCTALayout(tensorTy.getEncoding(),\n+                                      tensorTy.getShape(), CTALayout);\n+    newResultLayouts.push_back(newLayout);\n+  }\n+\n+  insertCasts(op, newOperandLayouts, newResultLayouts);\n+  return true;\n+}\n+\n+bool CTAPlanner::isElementwiseOp(Operation *op) const {\n+  if (llvm::isa<arith::AddFOp, arith::AddIOp, arith::AndIOp, arith::CeilDivSIOp,\n+                arith::CeilDivUIOp, arith::DivFOp, arith::DivSIOp,\n+                arith::DivUIOp, arith::ExtFOp, arith::ExtSIOp, arith::ExtUIOp,\n+                arith::FloorDivSIOp, arith::FPToSIOp, arith::FPToUIOp,\n+                arith::MaxFOp, arith::MaxSIOp, arith::MaxUIOp, arith::MinFOp,\n+                arith::MinSIOp, arith::MinUIOp, arith::MulFOp, arith::MulIOp,\n+                arith::NegFOp, arith::OrIOp, arith::RemFOp, arith::RemSIOp,\n+                arith::RemUIOp, arith::ShLIOp, arith::ShRSIOp, arith::ShRUIOp,\n+                arith::SIToFPOp, arith::SubFOp, arith::SubIOp, arith::TruncFOp,\n+                arith::TruncIOp, arith::UIToFPOp, arith::XOrIOp>(op))\n+    return true;\n+  if (llvm::isa<math::AbsFOp, math::AbsIOp, math::AtanOp, math::Atan2Op,\n+                math::CeilOp, math::CopySignOp, math::CosOp, math::SinOp,\n+                math::CountLeadingZerosOp, math::CountTrailingZerosOp,\n+                math::CtPopOp, math::ErfOp, math::ExpOp, math::Exp2Op,\n+                math::ExpM1Op, math::FloorOp, math::FmaOp, math::LogOp,\n+                math::Log10Op, math::Log1pOp, math::Log2Op, math::PowFOp,\n+                math::RsqrtOp, math::SqrtOp, math::TanhOp>(op))\n+    return true;\n+  if (llvm::isa<triton::IntToPtrOp, triton::PtrToIntOp, triton::BitcastOp,\n+                triton::FpToFpOp, triton::AddPtrOp,\n+                triton::PureExternElementwiseOp>(op))\n+    return true;\n+  if (llvm::isa<ttg::CmpIOp, ttg::CmpFOp, ttg::SelectOp>(op))\n+    return true;\n+  return false;\n+}\n+\n+bool CTAPlanner::processElementwise(Operation *op, Attribute layout) {\n+  llvm::SmallVector<Attribute> newOperandLayouts(op->getNumOperands(), layout);\n+  llvm::SmallVector<Attribute> newResultLayouts(op->getNumResults(), layout);\n+  insertCasts(op, newOperandLayouts, newResultLayouts);\n+  return true;\n+}\n+\n+bool CTAPlanner::processConstant(arith::ConstantOp constant, Attribute layout) {\n+  if (auto tensorTy =\n+          constant.getResult().getType().dyn_cast<RankedTensorType>()) {\n+    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n+\n+      auto newTensorTy = RankedTensorType::get(\n+          tensorTy.getShape(), tensorTy.getElementType(), layout);\n+      constant.setValueAttr(\n+          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>()));\n+    }\n+  }\n+  insertCasts(constant.getOperation(), {}, {layout});\n+  return true;\n+}\n+\n+bool CTAPlanner::processSplat(triton::SplatOp splat, Attribute layout) {\n+  insertCasts(splat.getOperation(), {{}}, {layout});\n+  return true;\n+}\n+\n+bool CTAPlanner::processMakeRange(triton::MakeRangeOp makeRange,\n+                                  Attribute layout) {\n+  insertCasts(makeRange.getOperation(), {}, {layout});\n+  return true;\n+}\n+\n+bool CTAPlanner::processMakeTensorPtr(triton::MakeTensorPtrOp makeTensorPtr,\n+                                      Attribute layout) {\n+  // All inputs of `makeTensorPtr` are scalar types\n+  llvm::SmallVector<Attribute> dummyInAttrs(makeTensorPtr.getNumOperands(), {});\n+  insertCasts(makeTensorPtr.getOperation(), dummyInAttrs, {layout});\n+  return true;\n+}\n+\n+bool CTAPlanner::processBroadcast(triton::BroadcastOp broadcast,\n+                                  Attribute layout) {\n+  insertCasts(broadcast.getOperation(), {layout}, {layout});\n+  return true;\n+}\n+\n+bool CTAPlanner::processExpandDimsBackward(triton::ExpandDimsOp expandDims,\n+                                           Attribute newResultLayout) {\n+  auto newSrcLayout = ttg::SliceEncodingAttr::get(\n+      newResultLayout.getContext(), expandDims.getAxis(), newResultLayout);\n+  insertCasts(expandDims.getOperation(), {newSrcLayout}, {newResultLayout});\n+  return true;\n+}\n+\n+bool CTAPlanner::processExpandDimsForward(triton::ExpandDimsOp expandDims,\n+                                          Attribute newSrcLayout) {\n+  assert(0 && \"processExpandDimsForward not implemented yet\");\n+  return true;\n+}\n+\n+bool CTAPlanner::processConvertLayoutBackward(\n+    ttg::ConvertLayoutOp convertLayout, CastOp cast) {\n+  Value src = convertLayout.getSrc();\n+  Value result = convertLayout.getResult();\n+  assert(getNumUsers(result) == 1 &&\n+         \"Expect to call processMultiUsersBackward first\");\n+  result.replaceAllUsesWith(src);\n+  convertLayout.erase();\n+  queue.push(cast);\n+  return true;\n+}\n+\n+bool CTAPlanner::processConvertLayoutForward(ttg::ConvertLayoutOp convertLayout,\n+                                             CastOp cast) {\n+  Value src = convertLayout.getSrc();\n+  Value result = convertLayout.getResult();\n+  assert(getNumUsers(src) == 1 &&\n+         \"Expect to call processMultiUsersForward first\");\n+  src.setType(result.getType());\n+  result.replaceAllUsesWith(src);\n+  convertLayout.erase();\n+  queue.push(cast);\n+  return true;\n+}\n+\n+bool CTAPlanner::processIfOp(scf::IfOp ifOp, int index, const Type &newType) {\n+  // Check index\n+  assert(index < ifOp.getNumResults() && \"Invalid result index of IfOp\");\n+  assert(index < ifOp.thenYield().getNumOperands() &&\n+         \"Invalid operand index of YieldOp\");\n+  assert(index < ifOp.elseYield().getNumOperands() &&\n+         \"Invalid operand index of YieldOp\");\n+\n+  Location loc = ifOp.getLoc();\n+  OpBuilder builder(ifOp.getContext());\n+\n+  // Insert forward cast after ifOp\n+  Value result = ifOp.getResult(index);\n+  builder.setInsertionPointAfter(ifOp.getOperation());\n+  auto newCast =\n+      markForward(builder.create<CastOp>(loc, result.getType(), result));\n+  result.setType(newType);\n+  result.replaceAllUsesExcept(newCast.getResult(0), newCast.getOperation());\n+  queue.push(newCast);\n+\n+  // Insert backward casts before yield\n+  for (scf::YieldOp yield : {ifOp.thenYield(), ifOp.elseYield()}) {\n+    Value yieldSrc = yield.getOperand(index);\n+    builder.setInsertionPoint(yield.getOperation());\n+    newCast = markBackward(builder.create<CastOp>(loc, newType, yieldSrc));\n+    yield->setOperand(index, newCast.getResult(0));\n+    queue.push(newCast);\n+  }\n+\n+  return true;\n+}\n+\n+bool CTAPlanner::processForOp(scf::ForOp forOp, int index,\n+                              const Type &newType) {\n+  Block *body = forOp.getBody();\n+  auto yield = llvm::cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n+\n+  // Check index\n+  assert(index + forOp.getNumControlOperands() < forOp.getNumOperands() &&\n+         \"Invalid operand index of ForOp\");\n+  assert(index + forOp.getNumInductionVars() < body->getNumArguments() &&\n+         \"Invalid block arg index of ForOp\");\n+  assert(index < yield.getNumOperands() && \"Invalid operand index of YieldOp\");\n+  assert(index < forOp.getNumResults() && \"Invalid result index of IfOp\");\n+\n+  Location loc = forOp.getLoc();\n+  OpBuilder builder(forOp.getContext());\n+\n+  // Insert backward cast before forOp\n+  OpOperand &operand =\n+      forOp->getOpOperand(index + forOp.getNumControlOperands());\n+  builder.setInsertionPoint(forOp.getOperation());\n+  auto newCast =\n+      markBackward(builder.create<CastOp>(loc, newType, operand.get()));\n+  operand.set(newCast.getResult(0));\n+  queue.push(newCast);\n+\n+  // Insert forward cast after block arg\n+  Value arg = body->getArgument(index + forOp.getNumInductionVars());\n+  builder.setInsertionPointToStart(body);\n+  newCast = markForward(builder.create<CastOp>(loc, arg.getType(), arg));\n+  arg.setType(newType);\n+  arg.replaceAllUsesExcept(newCast.getResult(0), newCast.getOperation());\n+  queue.push(newCast);\n+\n+  // Insert backward cast before yield\n+  Value yieldSrc = yield.getOperand(index);\n+  builder.setInsertionPoint(yield.getOperation());\n+  newCast = markBackward(builder.create<CastOp>(loc, newType, yieldSrc));\n+  yield->setOperand(index, newCast.getResult(0));\n+  queue.push(newCast);\n+\n+  // Insert forward cast after forOp\n+  Value result = forOp.getResult(index);\n+  builder.setInsertionPointAfter(forOp.getOperation());\n+  newCast = markForward(builder.create<CastOp>(loc, result.getType(), result));\n+  result.setType(newType);\n+  result.replaceAllUsesExcept(newCast.getResult(0), newCast.getOperation());\n+  queue.push(newCast);\n+\n+  return true;\n+}\n+\n+int findResultIndex(Operation *op, Value result) {\n+  for (int i = 0; i < op->getNumResults(); ++i)\n+    if (op->getResult(i) == result)\n+      return i;\n+  assert(0 && \"Invalid index of op result\");\n+  return -1;\n+}\n+\n+bool CTAPlanner::processIfOpBackward(scf::IfOp ifOp, CastOp cast) {\n+  int index = findResultIndex(ifOp.getOperation(), cast.getOperand(0));\n+  auto newType = cast.getResult(0).getType();\n+  return processIfOp(ifOp, index, newType);\n+}\n+\n+bool CTAPlanner::processForOpBackward(scf::ForOp forOp, CastOp cast) {\n+  int index = findResultIndex(forOp.getOperation(), cast.getOperand(0));\n+  auto newType = cast.getResult(0).getType();\n+  return processForOp(forOp, index, newType);\n+}\n+\n+bool CTAPlanner::processBlockArgBackward(BlockArgument arg, CastOp cast) {\n+  if (auto forOp = llvm::dyn_cast<scf::ForOp>(arg.getOwner()->getParentOp())) {\n+    int index = int(arg.getArgNumber()) - forOp.getNumInductionVars();\n+    auto newType = cast.getResult(0).getType();\n+    return processForOp(forOp, index, newType);\n+  } else {\n+    assert(0 && \"Unexpected parent op of block argument\");\n+    return true;\n+  }\n+}\n+\n+bool CTAPlanner::processForOpForward(scf::ForOp forOp, CastOp cast) {\n+  int index = cast.getResult(0).use_begin()->getOperandNumber() -\n+              forOp.getNumControlOperands();\n+  auto newType = cast.getOperand(0).getType();\n+  return processForOp(forOp, index, newType);\n+}\n+\n+bool CTAPlanner::processYieldOpForward(scf::YieldOp yieldOp, CastOp cast) {\n+  int index = cast.getResult(0).use_begin()->getOperandNumber();\n+  auto newType = cast.getOperand(0).getType();\n+  if (auto ifOp = llvm::dyn_cast<scf::IfOp>(yieldOp->getParentOp()))\n+    return processIfOp(ifOp, index, newType);\n+  else if (auto forOp = llvm::dyn_cast<scf::ForOp>(yieldOp->getParentOp()))\n+    return processForOp(forOp, index, newType);\n+  else\n+    assert(0 && \"Unexpected parent op of YieldOp\");\n+  return true;\n+}\n+\n+bool CTAPlanner::processOpFallback(Operation *op) {\n+  Location loc = op->getLoc();\n+  OpBuilder builder(op->getContext());\n+\n+  builder.setInsertionPoint(op);\n+  for (unsigned i = 0; i < op->getNumOperands(); ++i) {\n+    Value operand = op->getOperand(i);\n+    auto operandTy = operand.getType();\n+    if (triton::isTensorOrTensorPointerType(operandTy)) {\n+      auto cast = markBackward(builder.create<CastOp>(loc, operandTy, operand));\n+      op->setOperand(i, cast.getResult(0));\n+      queue.push(cast);\n+    }\n+  }\n+\n+  builder.setInsertionPointAfter(op);\n+  for (unsigned i = 0; i < op->getNumResults(); ++i) {\n+    Value result = op->getResult(i);\n+    auto resultTy = result.getType();\n+    if (triton::isTensorOrTensorPointerType(resultTy)) {\n+      auto cast = markForward(builder.create<CastOp>(loc, resultTy, result));\n+      result.replaceAllUsesExcept(cast.getResult(0), cast.getOperation());\n+      queue.push(cast);\n+    }\n+  }\n+\n+  return true;\n+}\n+\n+bool CTAPlanner::processMultiUsersBackward(Value input, CastOp cast) {\n+  Location loc = input.getLoc();\n+  OpBuilder builder(input.getContext());\n+\n+  llvm::DenseMap<Type, llvm::SmallVector<CastOp>> typeToIndices;\n+  for (OpOperand &operand : input.getUses()) {\n+    auto brotherCast = llvm::dyn_cast<CastOp>(operand.getOwner());\n+    if (!brotherCast) {\n+      if (stepUnchanged <= queue.size())\n+        return false;\n+      builder.setInsertionPoint(operand.getOwner());\n+      brotherCast = markBackward(\n+          builder.create<CastOp>(loc, cast.getResult(0).getType(), input));\n+      auto newCast = markForward(builder.create<CastOp>(\n+          loc, input.getType(), brotherCast.getResult(0)));\n+      operand.set(newCast.getResult(0));\n+      queue.push(brotherCast);\n+      queue.push(newCast);\n+    }\n+    auto type = brotherCast.getResult(0).getType();\n+    typeToIndices[type].push_back(brotherCast);\n+  }\n+\n+  bool first = true;\n+  for (auto it : typeToIndices) {\n+    Type &type = it.first;\n+    llvm::SmallVector<CastOp> &casts = it.second;\n+    Value newInput = input;\n+    if (!first) {\n+      if (Operation *defOp = input.getDefiningOp()) {\n+        builder.setInsertionPointAfter(defOp);\n+        Operation *clonedOp = builder.clone(*defOp);\n+        newInput = clonedOp->getResult(0);\n+      } else {\n+        assert(0 && \"Layout conflict for block arg\"); // TODO\n+      }\n+    }\n+    first = false;\n+    if (Operation *defOp = newInput.getDefiningOp()) {\n+      builder.setInsertionPointAfter(defOp);\n+    } else {\n+      assert(newInput.isa<BlockArgument>() &&\n+             \"Unexpected Value without defining op\");\n+      builder.setInsertionPointToStart(\n+          newInput.cast<BlockArgument>().getOwner());\n+    }\n+    auto newCast = markBackward(builder.create<CastOp>(loc, type, newInput));\n+    queue.push(newCast);\n+    auto newResult = newCast.getResult(0);\n+    for (CastOp &brotherCast : casts) {\n+      brotherCast.getResult(0).replaceAllUsesWith(newResult);\n+      eraseCastOpFromQueue(brotherCast);\n+    }\n+  }\n+  return true;\n+}\n+\n+bool CTAPlanner::processMultiUsersForward(Value castResult, CastOp cast) {\n+  Value castSrc = cast.getOperand(0);\n+\n+  Location loc = cast.getLoc();\n+  OpBuilder builder(cast.getContext());\n+  builder.setInsertionPointAfter(cast.getOperation());\n+\n+  while (!castResult.use_empty()) {\n+    auto newCast =\n+        markForward(builder.create<CastOp>(loc, castResult.getType(), castSrc));\n+    castResult.use_begin()->set(newCast.getResult(0));\n+    queue.push(newCast);\n+  }\n+\n+  eraseCastOp(cast);\n+  return true;\n+}\n+\n+struct PlanCTAPass : public TritonGPUPlanCTAPassBase<PlanCTAPass> {\n+  PlanCTAPass(ttng::ClusterInfo *clusterInfo_ = nullptr)\n+      : clusterInfo(clusterInfo_) {}\n+\n+  void runOnOperation() override {\n+    ModuleOp mod = getOperation();\n+\n+    // Skip PlanCTAPass when numCTAs == 1\n+    if (ttg::TritonGPUDialect::getNumCTAs(mod) == 1)\n+      return;\n+\n+    mod.walk([&](triton::FuncOp funcOp) {\n+      CTAPlanner planner(clusterInfo);\n+      planner.run(funcOp);\n+\n+      // FIXME: Clone funcOp so that the IR change can be identified after\n+      // PlanCTAPass. Without this, the change after PlanCTAPass will not be\n+      // displayed when MLIR_ENABLE_DUMP=1. This is not reasonable and should\n+      // be fixed later.\n+      OpBuilder builder(funcOp);\n+      builder.clone(*funcOp.getOperation());\n+      funcOp.erase();\n+    });\n+  }\n+\n+  ttng::ClusterInfo *clusterInfo;\n+};\n+\n+} // namespace\n+\n+std::unique_ptr<Pass>\n+mlir::createTritonNvidiaGPUPlanCTAPass(ttng::ClusterInfo *clusterInfo) {\n+  return std::make_unique<PlanCTAPass>(clusterInfo);\n+}\n+\n+/* TODO\n+ * - Use ConvertLayoutOp instead of UnrealizedConversionCastOp.\n+ * - Move PlanCTAPass to the front of CoalescePass.\n+ * - Design better tiling strategy for DotOp and ReduceOp.\n+ * - Consider cases where there are more than one DotOps.\n+ * - Use better data structure for erasing CastOps from queue (linked list?).\n+ * - Process eliminable CastOps in higher priority.\n+ * - Fix the clone func bug in PlanCTAPass::runOnOperation.\n+ * - Add some comments to introduce the overall idea of this pass.\n+ * - Add some lit tests for this pass.\n+ */"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/RewriteTensorPointer.cpp", "status": "added", "additions": 752, "deletions": 0, "changes": 752, "file_content_changes": "@@ -0,0 +1,752 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"mlir/Pass/Pass.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n+\n+#include <memory>\n+#include <stack>\n+\n+using namespace mlir;\n+namespace tt = mlir::triton;\n+namespace ttg = mlir::triton::gpu;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+namespace {\n+bool isDivisible(Value v, unsigned divisor) {\n+  if (auto op = v.getDefiningOp<mlir::arith::ConstantOp>()) {\n+    return op.getValue().dyn_cast<IntegerAttr>().getValue().getZExtValue() %\n+               divisor ==\n+           0;\n+  }\n+  if (v.getDefiningOp() &&\n+      isa<mlir::UnrealizedConversionCastOp>(v.getDefiningOp())) {\n+    return isDivisible(v.getDefiningOp()->getOperand(0), divisor);\n+  } else if (v.getParentBlock()->isEntryBlock() && v.isa<BlockArgument>()) {\n+    BlockArgument blockArg = v.cast<BlockArgument>();\n+    Operation *parentOp = blockArg.getOwner()->getParentOp();\n+    auto func = dyn_cast<tt::FuncOp>(parentOp);\n+    assert(func);\n+    if (auto attr = func.getArgAttrOfType<IntegerAttr>(blockArg.getArgNumber(),\n+                                                       \"tt.max_divisibility\"))\n+      return attr.getValue().getZExtValue() % divisor == 0;\n+    return false;\n+  } else if (v.getParentBlock()->isEntryBlock() && (!v.isa<BlockArgument>())) {\n+    // in entryblock but not BlockArgument\n+    return isDivisible(v.getDefiningOp()->getOperand(0), divisor);\n+  } else if (!v.getParentBlock()->isEntryBlock()) {\n+    // in non-entryblock\n+    return isDivisible(v.getDefiningOp()->getOperand(0), divisor);\n+  } else {\n+    llvm::report_fatal_error(\n+        \"Operand of `MakeTensorPtrOp` is not the function's argument\");\n+    return false;\n+  }\n+}\n+\n+bool shouldRemove(tt::MakeTensorPtrOp &op, int computeCapability) {\n+  if (computeCapability < 90 || !::triton::tools::getBoolEnv(\"ENABLE_TMA\"))\n+    return true;\n+  auto resType = op.getResult()\n+                     .getType()\n+                     .cast<tt::PointerType>()\n+                     .getPointeeType()\n+                     .cast<RankedTensorType>();\n+  auto elemType = resType.getElementType();\n+  auto ord = op.getOrder();\n+  auto stride = op.getStrides();\n+  auto shape = ttg::getShapePerCTA(resType);\n+  // TMA load/store requires the box dimension to be more than 32 bytes.\n+  // Because we only support 32B-swizzle, 64B-swizzle and 128B-swizzleon for\n+  // now. Remove this constraint when we support non-swizzle smem.\n+  bool boxDimSwizzle =\n+      shape[ord[0]] >= (256 / elemType.getIntOrFloatBitWidth());\n+  // we only support TMA load with 2D tensor for now.\n+  // TMA load/store requires the stride to be divisible by 16 bytes.\n+  bool strideDivisible = false;\n+  if (stride.size() == 2)\n+    strideDivisible =\n+        isDivisible(stride[ord[1]], 128 / elemType.getIntOrFloatBitWidth());\n+  bool enableTMA = ::triton::tools::getBoolEnv(\"ENABLE_TMA\");\n+  return !(boxDimSwizzle && strideDivisible && enableTMA);\n+}\n+\n+// TODO: When encoding exists use triton::gpu::CmpIOp as arith::CmpIOp doesn't\n+// play well with encoding attributes. Move back to arith::CmpIOp when this pass\n+// moves back to triton IR level.\n+Value createCmpOp(OpBuilder &builder, Location loc, RankedTensorType type,\n+                  arith::CmpIPredicate pred, Value lhs, Value rhs) {\n+  if (type.getEncoding())\n+    return builder.create<ttg::CmpIOp>(loc, type, pred, lhs, rhs);\n+  return builder.create<arith::CmpIOp>(loc, type, pred, lhs, rhs);\n+}\n+\n+/// An additional struct to record the meta information of operations\n+/// with tensor pointers\n+struct RewritedInfo {\n+private:\n+  Value base;\n+  SmallVector<Value> shape;\n+  SmallVector<Value> strides;\n+  SmallVector<Value> offsets;\n+  ArrayRef<int64_t> tensorShape;\n+  Attribute layout;\n+\n+  // A cache to avoid generating the same offset with range\n+  DenseMap<unsigned, Value> cachedOffsetWithRange;\n+\n+  template <typename T>\n+  SmallVector<T> insertOne(ArrayRef<T> vec, unsigned axis) const {\n+    SmallVector<T> res(vec.begin(), vec.end());\n+    res.insert(res.begin() + axis, 1);\n+    return res;\n+  }\n+\n+  // Example:    order = [   0, 2, 1, 3], dim = 2\n+  //          resOrder = [2, 0, 3, 1, 4]\n+  SmallVector<unsigned> insertOrder(ArrayRef<unsigned> order,\n+                                    unsigned axis) const {\n+    SmallVector<unsigned> resOrder(order.begin(), order.end());\n+    for (unsigned i = 0; i < resOrder.size(); ++i)\n+      if (resOrder[i] >= axis)\n+        ++resOrder[i];\n+    resOrder.insert(resOrder.begin(), axis);\n+    return resOrder;\n+  }\n+\n+public:\n+  RewritedInfo() = default;\n+\n+  RewritedInfo(const RewritedInfo &other) = default;\n+\n+  RewritedInfo(Value base, const SmallVector<Value> &shape,\n+               const SmallVector<Value> &strides,\n+               const SmallVector<Value> &offsets,\n+               const ArrayRef<int64_t> &tensorShape, Attribute layout)\n+      : base(base), shape(shape), strides(strides), offsets(offsets),\n+        tensorShape(tensorShape), layout(layout) {\n+    assert(shape.size() == strides.size() && shape.size() == offsets.size() &&\n+           shape.size() == tensorShape.size());\n+  }\n+\n+  unsigned int length() const { return shape.size(); }\n+\n+  Value getOffset(unsigned i) { return offsets[i]; }\n+\n+  SmallVector<Value> getOffsets() { return offsets; }\n+\n+  void setOffset(unsigned i, Value newOffset) {\n+    offsets[i] = newOffset;\n+    cachedOffsetWithRange.clear();\n+  }\n+\n+  void setOffsets(const SmallVector<Value> &newOffsets) {\n+    offsets = newOffsets;\n+    cachedOffsetWithRange.clear();\n+  }\n+\n+  void setEncoding(Attribute newLayout) { layout = newLayout; }\n+\n+  Value getExpandedOffsetWithRange(OpBuilder &builder, const Location &loc,\n+                                   unsigned i) {\n+    if (cachedOffsetWithRange.count(i))\n+      return cachedOffsetWithRange[i];\n+\n+    // Add range\n+    auto indexI32RowType =\n+        RankedTensorType::get({tensorShape[i]}, builder.getI32Type(), layout);\n+    auto indexRowType =\n+        RankedTensorType::get({tensorShape[i]}, builder.getI64Type(), layout);\n+    Value splatOffset =\n+        builder.create<tt::SplatOp>(loc, indexRowType, offsets[i]);\n+    Value range = builder.create<tt::MakeRangeOp>(loc, indexI32RowType, 0,\n+                                                  tensorShape[i]);\n+    Value i64Range = builder.create<arith::ExtSIOp>(loc, indexRowType, range);\n+\n+    // Expand dimensions\n+    Value expandedResult =\n+        builder.create<arith::AddIOp>(loc, splatOffset, i64Range);\n+    for (int axis = 0; axis < tensorShape.size(); ++axis) {\n+      if (axis == i)\n+        continue;\n+\n+      if (layout) {\n+        auto argEncoding = layout.cast<ttg::BlockedEncodingAttr>();\n+        auto retSizePerThread = insertOne(argEncoding.getSizePerThread(), axis);\n+        auto retThreadsPerWarp =\n+            insertOne(argEncoding.getThreadsPerWarp(), axis);\n+        auto retWarpsPerCTA = insertOne(argEncoding.getWarpsPerCTA(), axis);\n+        auto retOrder = insertOrder(argEncoding.getOrder(), axis);\n+\n+        auto argCTALayout = argEncoding.getCTALayout();\n+        auto retCTAsPerCGA = insertOne(argCTALayout.getCTAsPerCGA(), axis);\n+        auto retCTASplitNum = insertOne(argCTALayout.getCTASplitNum(), axis);\n+        auto retCTAOrder = insertOrder(argCTALayout.getCTAOrder(), axis);\n+\n+        auto retCTALayout = ttg::CTALayoutAttr::get(\n+            loc.getContext(), retCTAsPerCGA, retCTASplitNum, retCTAOrder);\n+\n+        auto retEncoding = ttg::BlockedEncodingAttr::get(\n+            loc.getContext(), retSizePerThread, retThreadsPerWarp,\n+            retWarpsPerCTA, retOrder, retCTALayout);\n+\n+        auto newArgEncoding =\n+            ttg::SliceEncodingAttr::get(loc.getContext(), axis, retEncoding);\n+        auto newArgType = RankedTensorType::get(indexRowType.getShape(),\n+                                                indexRowType.getElementType(),\n+                                                newArgEncoding);\n+        Value newArg = builder.create<ttg::ConvertLayoutOp>(loc, newArgType,\n+                                                            expandedResult);\n+        expandedResult = builder.create<tt::ExpandDimsOp>(loc, newArg, axis);\n+      } else\n+        expandedResult =\n+            builder.create<tt::ExpandDimsOp>(loc, expandedResult, axis);\n+    }\n+\n+    return cachedOffsetWithRange[i] = expandedResult;\n+  }\n+\n+  Value generatePtr(OpBuilder &builder, const Location &loc) {\n+    assert(tensorShape.size() == offsets.size() &&\n+           tensorShape.size() == strides.size());\n+    auto ptrType = base.getType().cast<tt::PointerType>();\n+    auto ptrTensorType = RankedTensorType::get(tensorShape, ptrType, layout);\n+\n+    // Generate offsets per dimension\n+    Value ptr = builder.create<tt::SplatOp>(loc, ptrTensorType, base);\n+    for (unsigned i = 0; i < tensorShape.size(); ++i) {\n+      auto offsetWithRange = getExpandedOffsetWithRange(builder, loc, i);\n+      // We must splat strides into the expanded shape not a row for retaining\n+      // the divisibility information given by strides\n+      Value splatStride = builder.create<tt::SplatOp>(\n+          loc, offsetWithRange.getType(), strides[i]);\n+      Value offsetWithStride =\n+          builder.create<arith::MulIOp>(loc, offsetWithRange, splatStride);\n+      auto offsetType = offsetWithRange.getType().cast<RankedTensorType>();\n+      auto indexTensorType = RankedTensorType::get(\n+          tensorShape, offsetType.getElementType(), offsetType.getEncoding());\n+      Value broadcasted = builder.create<tt::BroadcastOp>(loc, indexTensorType,\n+                                                          offsetWithStride);\n+      if (offsetType.getEncoding() != ptrTensorType.getEncoding()) {\n+        auto newArgType =\n+            RankedTensorType::get(tensorShape, offsetType.getElementType(),\n+                                  ptrTensorType.getEncoding());\n+        broadcasted =\n+            builder.create<ttg::ConvertLayoutOp>(loc, newArgType, broadcasted);\n+      }\n+      // Add to the pointer\n+      ptr = builder.create<tt::AddPtrOp>(loc, ptrTensorType, ptr, broadcasted);\n+    }\n+\n+    return ptr;\n+  }\n+\n+  Value generateMask(OpBuilder &builder, const Location &loc,\n+                     const std::optional<ArrayRef<int32_t>> &boundaryCheck) {\n+    if (!boundaryCheck.has_value() || boundaryCheck.value().empty())\n+      return {};\n+\n+    // Generate mask per dimension\n+    auto maskTensorType =\n+        RankedTensorType::get(tensorShape, builder.getI1Type(), layout);\n+    Value mask;\n+    for (auto i : boundaryCheck.value()) {\n+      auto offsetWithRange = getExpandedOffsetWithRange(builder, loc, i);\n+      auto offsetType = offsetWithRange.getType().cast<RankedTensorType>();\n+      RankedTensorType cmpTensorType = RankedTensorType::get(\n+          offsetType.getShape(), builder.getI1Type(), offsetType.getEncoding());\n+\n+      // Compare with lower bound\n+      Value lowerBound = builder.create<mlir::arith::ConstantIntOp>(\n+          loc, 0, offsetType.getElementType());\n+      Value splatLowerBound = builder.create<tt::SplatOp>(\n+          loc, offsetWithRange.getType(), lowerBound);\n+      Value cmpLower =\n+          createCmpOp(builder, loc, cmpTensorType, arith::CmpIPredicate::sge,\n+                      offsetWithRange, splatLowerBound);\n+\n+      // Compare with upper bound\n+      Value splatUpperBound =\n+          builder.create<tt::SplatOp>(loc, offsetWithRange.getType(), shape[i]);\n+      Value cmpUpper =\n+          createCmpOp(builder, loc, cmpTensorType, arith::CmpIPredicate::slt,\n+                      offsetWithRange, splatUpperBound);\n+\n+      // And and broadcast\n+      Value andResult = builder.create<arith::AndIOp>(loc, cmpLower, cmpUpper);\n+      if (offsetType.getEncoding() != maskTensorType.getEncoding()) {\n+        auto newArgType =\n+            RankedTensorType::get(offsetType.getShape(), builder.getI1Type(),\n+                                  maskTensorType.getEncoding());\n+        andResult =\n+            builder.create<ttg::ConvertLayoutOp>(loc, newArgType, andResult);\n+      }\n+\n+      Value broadcasted =\n+          builder.create<tt::BroadcastOp>(loc, maskTensorType, andResult);\n+\n+      // And up all results\n+      if (!mask) {\n+        mask = broadcasted;\n+      } else {\n+        mask = builder.create<arith::AndIOp>(loc, mask, broadcasted);\n+      }\n+    }\n+\n+    return mask;\n+  }\n+\n+  Value generateOther(OpBuilder &builder, const Location &loc,\n+                      const std::optional<tt::PaddingOption> &padding) {\n+    if (!padding.has_value())\n+      return Value();\n+\n+    // Create element attribute\n+    auto elementType = base.getType().cast<tt::PointerType>().getPointeeType();\n+    auto otherTensorType =\n+        RankedTensorType::get(tensorShape, elementType, layout);\n+\n+    // Set zero padding value\n+    TypedAttr attr =\n+        elementType.isIntOrIndex()\n+            ? builder.getIntegerAttr(elementType, 0).cast<TypedAttr>()\n+            : builder.getFloatAttr(elementType, 0).cast<TypedAttr>();\n+\n+    // Float NaN padding case\n+    if (padding.value() == tt::PaddingOption::PAD_NAN) {\n+      assert(!elementType.isIntOrIndex());\n+      auto apNaN = llvm::APFloat::getNaN(\n+          attr.cast<FloatAttr>().getValue().getSemantics());\n+      attr = builder.getFloatAttr(elementType, apNaN);\n+    }\n+\n+    // Create tensor\n+    Value constant = builder.create<arith::ConstantOp>(loc, attr);\n+    return builder.create<tt::SplatOp>(loc, otherTensorType, constant);\n+  }\n+};\n+} // namespace\n+\n+class TritonGPURewriteTensorPointerPass\n+    : public TritonGPURewriteTensorPointerBase<\n+          TritonGPURewriteTensorPointerPass> {\n+private:\n+  int computeCapability;\n+  DenseMap<Value, RewritedInfo> rewritedInfo;\n+\n+public:\n+  explicit TritonGPURewriteTensorPointerPass(int computeCapability)\n+      : computeCapability(computeCapability) {}\n+\n+  static bool needRewrite(Operation *op, const DenseSet<Value> &valueToRemove) {\n+    return std::any_of(op->getOperands().begin(), op->getOperands().end(),\n+                       [&valueToRemove](Value operand) {\n+                         return tt::isTensorPointerType(operand.getType()) &&\n+                                valueToRemove.count(operand);\n+                       });\n+  }\n+\n+  static SmallVector<Value>\n+  generateNewOperands(const SmallVector<Value> &oldOperands, unsigned index,\n+                      const SmallVector<Value> &newValues) {\n+    assert(index < oldOperands.size());\n+    SmallVector<Value> newOperands;\n+    for (int i = 0; i < index; ++i)\n+      newOperands.push_back(oldOperands[i]);\n+    for (auto value : newValues)\n+      newOperands.push_back(value);\n+    for (auto i = index + 1; i < oldOperands.size(); ++i)\n+      newOperands.push_back(oldOperands[i]);\n+    return newOperands;\n+  }\n+\n+  Operation *rewriteMakeTensorPtrOp(OpBuilder &builder, tt::MakeTensorPtrOp op,\n+                                    std::stack<Operation *> &eraser,\n+                                    const DenseSet<Value> &valueToRemove) {\n+    if (!valueToRemove.count(op.getResult()))\n+      return nullptr;\n+    // Save info for later use\n+    auto ptrType = op.getResult().getType().cast<tt::PointerType>();\n+    auto tensorType = ptrType.getPointeeType().cast<RankedTensorType>();\n+\n+    // Cast I32 offsets into I64\n+    SmallVector<Value> i64Offsets;\n+    for (auto offset : op.getOffsets()) {\n+      auto i64Offset = builder.create<arith::ExtSIOp>(\n+          op.getLoc(), builder.getI64Type(), offset);\n+      i64Offsets.push_back(i64Offset);\n+    }\n+\n+    // Save information\n+    rewritedInfo[op.getResult()] =\n+        RewritedInfo(op.getBase(), op.getShape(), op.getStrides(), i64Offsets,\n+                     tensorType.getShape(), tensorType.getEncoding());\n+\n+    // Erase the original operation\n+    eraser.push(op);\n+    return nullptr;\n+  }\n+\n+  Operation *rewriteAdvanceOp(OpBuilder &builder, tt::AdvanceOp op,\n+                              std::stack<Operation *> &eraser,\n+                              const DenseSet<Value> &valueToRemove) {\n+    if (!valueToRemove.count(op.getResult())) {\n+      return nullptr;\n+    }\n+    // Get info from previous results\n+    assert(rewritedInfo.count(op.getPtr()));\n+    auto info = rewritedInfo[op.getPtr()];\n+\n+    // Calculate new offsets\n+    assert(info.length() == op.getOffsets().size());\n+    SmallVector<Value> newOffsets;\n+    for (int i = 0; i < info.length(); ++i) {\n+      Value i64Offset = builder.create<arith::ExtSIOp>(\n+          op.getLoc(), builder.getI64Type(), op.getOffsets()[i]);\n+      Value newOffset = builder.create<arith::AddIOp>(\n+          op.getLoc(), info.getOffset(i), i64Offset);\n+      newOffsets.push_back(newOffset);\n+    }\n+\n+    // Save info for later use\n+    info.setOffsets(newOffsets);\n+    rewritedInfo[op.getResult()] = info;\n+\n+    // Erase the original operation\n+    eraser.push(op);\n+    return nullptr;\n+  }\n+\n+  Operation *rewriteLoadStoreOp(OpBuilder &builder, Operation *op,\n+                                std::stack<Operation *> &eraser,\n+                                const DenseSet<Value> &valueToRemove) {\n+    if (!valueToRemove.count(op->getOperand(0)))\n+      return nullptr;\n+\n+    // We only have to rewrite load/stores with tensor pointers\n+    auto ptr = op->getOperand(0);\n+    if (!tt::isTensorPointerType(ptr.getType()))\n+      return nullptr;\n+\n+    // Get info from previous results\n+    assert(rewritedInfo.count(ptr));\n+    auto info = rewritedInfo[ptr];\n+\n+    // Load/store with tensor pointers implicitly will check the bound while\n+    // accessing memory, so we should set `mask` and `other` (according to the\n+    // padding). Also note that load with tensor pointers do not have `mask` and\n+    // `other` while building IR from Python AST\n+    std::optional<ArrayRef<int>> boundaryCheck;\n+    if (auto loadOp = dyn_cast<tt::LoadOp>(op)) {\n+      assert(!loadOp.getMask() && !loadOp.getOther());\n+      boundaryCheck = loadOp.getBoundaryCheck();\n+      if (auto valueType =\n+              dyn_cast<RankedTensorType>(loadOp.getResult().getType()))\n+        info.setEncoding(valueType.getEncoding());\n+    } else if (auto storeOp = dyn_cast<tt::StoreOp>(op)) {\n+      assert(!storeOp.getMask());\n+      boundaryCheck = storeOp.getBoundaryCheck();\n+      if (auto valueType =\n+              dyn_cast<RankedTensorType>(storeOp.getValue().getType()))\n+        info.setEncoding(valueType.getEncoding());\n+    }\n+\n+    // Generate new `ptr`, `mask` and `other`\n+    auto newPtr = info.generatePtr(builder, op->getLoc());\n+    auto newMask = info.generateMask(builder, op->getLoc(), boundaryCheck);\n+    Value newOther;\n+    if (auto loadOp = dyn_cast<tt::LoadOp>(op))\n+      newOther = info.generateOther(builder, op->getLoc(), loadOp.getPadding());\n+\n+    // Create a new operation\n+    if (auto loadOp = dyn_cast<tt::LoadOp>(op)) {\n+      auto newResult = builder.create<tt::LoadOp>(\n+          loadOp.getLoc(), loadOp.getResult().getType(), newPtr, newMask,\n+          newOther, loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n+          loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n+      op->getResult(0).replaceAllUsesWith(newResult);\n+    } else if (auto storeOp = dyn_cast<tt::StoreOp>(op)) {\n+      builder.create<tt::StoreOp>(storeOp.getLoc(), newPtr, storeOp.getValue(),\n+                                  newMask, storeOp.getCache(),\n+                                  storeOp.getEvict());\n+    }\n+\n+    // Erase the original operation\n+    eraser.push(op);\n+    return nullptr;\n+  }\n+\n+  Operation *rewriteForOp(OpBuilder &builder, scf::ForOp op,\n+                          std::stack<Operation *> &eraser,\n+                          DenseSet<Value> &valueToRemove) {\n+    // Generate new iteration operands and set rewrited information\n+    SmallVector<Value> oldIterOperands = op.getIterOperands();\n+    SmallVector<Value> newIterOperands = op.getIterOperands();\n+    for (unsigned i = 0, oldI = 0, size = op.getNumIterOperands(); i < size;\n+         ++i, ++oldI) {\n+      if (!tt::isTensorPointerType(newIterOperands[i].getType()))\n+        continue;\n+      if (!valueToRemove.count(newIterOperands[i]))\n+        continue;\n+\n+      // Expand the tensor pointer into offsets\n+      assert(rewritedInfo.count(newIterOperands[i]));\n+      auto info = rewritedInfo[newIterOperands[i]];\n+      newIterOperands =\n+          generateNewOperands(newIterOperands, i, info.getOffsets());\n+      i += info.length() - 1;\n+      size += info.length() - 1;\n+    }\n+\n+    // Rebuild the loop type\n+    auto newForOp = builder.create<scf::ForOp>(op.getLoc(), op.getLowerBound(),\n+                                               op.getUpperBound(), op.getStep(),\n+                                               newIterOperands);\n+\n+    // Create value mapping. Note that for tensor pointers, we use identity\n+    // mapping. It may refer to a value in the old loop, but we will rewrite it\n+    // later\n+    IRMapping mapping;\n+    for (unsigned i = 0, oldI = 0; oldI < op.getNumIterOperands();\n+         ++i, ++oldI) {\n+      auto oldRegionIterArg = op.getRegionIterArg(oldI);\n+      if (tt::isTensorPointerType(oldRegionIterArg.getType()) &&\n+          valueToRemove.count(oldIterOperands[oldI])) {\n+        // Pass rewrited info inside\n+        assert(rewritedInfo.count(oldIterOperands[oldI]));\n+        auto info = rewritedInfo[oldIterOperands[oldI]];\n+        mapping.map(oldRegionIterArg, oldRegionIterArg);\n+        for (unsigned j = 0; j < info.length(); ++j)\n+          info.setOffset(j, newForOp.getRegionIterArg(i + j));\n+        rewritedInfo[oldRegionIterArg] = info;\n+        i += info.length() - 1;\n+      } else {\n+        mapping.map(oldRegionIterArg, newForOp.getRegionIterArg(i));\n+      }\n+    }\n+    mapping.map(op.getInductionVar(), newForOp.getInductionVar());\n+\n+    // Clone body\n+    builder.setInsertionPointToStart(newForOp.getBody());\n+    for (Operation &opInFor : *op.getBody()) {\n+      Operation *newOp = builder.clone(opInFor, mapping);\n+      for (unsigned i = 0; i < opInFor.getNumResults(); ++i) {\n+        if (valueToRemove.count(opInFor.getResult(i)))\n+          valueToRemove.insert(newOp->getResult(i));\n+        mapping.map(opInFor.getResult(i), newOp->getResult(i));\n+      }\n+    }\n+\n+    // supported nested scf.for ops\n+    for (auto &[k, v] : mapping.getValueMap())\n+      if (valueToRemove.find(k) != valueToRemove.end())\n+        valueToRemove.insert(v);\n+\n+    // Replace later usages\n+    assert(op.getNumResults() == op.getNumIterOperands());\n+    for (unsigned i = 0, oldI = 0; oldI < op.getNumResults(); ++i, ++oldI) {\n+      auto oldResult = op.getResult(oldI);\n+      if (tt::isTensorPointerType(oldResult.getType()) &&\n+          valueToRemove.count(oldIterOperands[oldI])) {\n+        // Pack new offsets into rewrited info\n+        assert(rewritedInfo.count(oldIterOperands[oldI]));\n+        auto info = rewritedInfo[oldIterOperands[oldI]];\n+        for (unsigned j = 0; j < info.length(); ++j)\n+          info.setOffset(j, newForOp.getResult(i + j));\n+        i += info.length() - 1;\n+        rewritedInfo[oldResult] = info;\n+      } else {\n+        oldResult.replaceAllUsesWith(newForOp.getResult(i));\n+      }\n+    }\n+\n+    // Erase later\n+    eraser.push(op);\n+    return newForOp;\n+  }\n+\n+  Operation *rewriteYieldOp(OpBuilder &builder, scf::YieldOp op,\n+                            std::stack<Operation *> &eraser,\n+                            const DenseSet<Value> &valueToRemove) {\n+    // Replace tensor pointers with offsets\n+    SmallVector<Value> newOperands = op->getOperands();\n+    for (unsigned i = 0, size = op.getNumOperands(); i < size; ++i) {\n+      if (!tt::isTensorPointerType(newOperands[i].getType()))\n+        continue;\n+      if (!valueToRemove.count(newOperands[i]))\n+        continue;\n+\n+      assert(rewritedInfo.count(newOperands[i]));\n+      auto info = rewritedInfo[newOperands[i]];\n+      newOperands = generateNewOperands(newOperands, i, info.getOffsets());\n+      i += info.length() - 1;\n+      size += info.length() - 1;\n+    }\n+    op->setOperands(newOperands);\n+\n+    // No need to erase\n+    return nullptr;\n+  }\n+\n+  Operation *rewriteOp(Operation *op, std::stack<Operation *> &eraser,\n+                       DenseSet<Value> &valueToRemove) {\n+    OpBuilder builder(op);\n+\n+    // Rewrite `make_tensor_ptr` and `advance` and make a tensor of pointers\n+    // Rewriting functions return the next operation to visit, if there is no\n+    // next one, simply return `nullptr`\n+    std::pair<Value, RewritedInfo> rewrited;\n+    if (auto makeTensorPtrOp = dyn_cast<tt::MakeTensorPtrOp>(op)) {\n+      return rewriteMakeTensorPtrOp(builder, makeTensorPtrOp, eraser,\n+                                    valueToRemove);\n+    } else if (auto advanceOp = dyn_cast<tt::AdvanceOp>(op)) {\n+      return rewriteAdvanceOp(builder, advanceOp, eraser, valueToRemove);\n+    } else if (isa<tt::LoadOp>(op) || isa<tt::StoreOp>(op)) {\n+      return rewriteLoadStoreOp(builder, op, eraser, valueToRemove);\n+    } else if (auto storeOp = dyn_cast<tt::StoreOp>(op)) {\n+      return rewriteLoadStoreOp(builder, op, eraser, valueToRemove);\n+    } else if (op->getDialect()->getNamespace() == \"scf\" ||\n+               op->getDialect()->getNamespace() == \"cf\") {\n+      if (!needRewrite(op, valueToRemove))\n+        return op;\n+\n+      if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+        return rewriteForOp(builder, forOp, eraser, valueToRemove);\n+      } else if (auto yieldOp = dyn_cast<scf::YieldOp>(op)) {\n+        return rewriteYieldOp(builder, yieldOp, eraser, valueToRemove);\n+      } else {\n+        llvm_unreachable(\"Currently we only support tensor pointer usages \"\n+                         \"inside a `scf::ForOp`, others such as `scf::IfOp`,\"\n+                         \"`scf::WhileOp`, `cf::BranchOp` or `cf::CondBranchOp` \"\n+                         \"are not supported yet\");\n+      }\n+    }\n+\n+    // Otherwise return the original one\n+    return op;\n+  }\n+\n+  void visitOperation(Operation *op, std::stack<Operation *> &eraser,\n+                      DenseSet<Value> &valueToRemove) {\n+    for (auto &region : op->getRegions()) {\n+      for (auto &block : region) {\n+        // We need an extra copy because erasing operations may break the\n+        // iterator behavior\n+        SmallVector<Operation *> blockCopy;\n+        for (auto &nestedOp : block)\n+          blockCopy.push_back(&nestedOp);\n+\n+        // Rewrite and recursively visit\n+        for (auto &nestedOp : blockCopy) {\n+          if (auto newOp = rewriteOp(nestedOp, eraser, valueToRemove))\n+            visitOperation(newOp, eraser, valueToRemove);\n+        }\n+      }\n+    }\n+  }\n+\n+  void runOnOperation() override {\n+    ModuleOp mod = getOperation();\n+\n+    DenseSet<Value> valueToRemove;\n+    mod.walk([&valueToRemove,\n+              computeCapability = this->computeCapability](Operation *op) {\n+      if (auto makeTensorPtrOp = dyn_cast<tt::MakeTensorPtrOp>(op)) {\n+        if (shouldRemove(makeTensorPtrOp, computeCapability))\n+          valueToRemove.insert(op->getResult(0));\n+      }\n+      if (llvm::isa<tt::AdvanceOp>(op)) {\n+        auto src = op->getOperand(0);\n+        if (tt::isTensorPointerType(src.getType())) {\n+          auto makeTensorPtrOp = getMakeTensorPtrOp(src);\n+          if (shouldRemove(makeTensorPtrOp, computeCapability)) {\n+            valueToRemove.insert(op->getResult(0));\n+          }\n+        }\n+      }\n+      if (llvm::isa<tt::LoadOp, tt::StoreOp>(op)) {\n+        auto src = op->getOperand(0);\n+        if (tt::isTensorPointerType(src.getType())) {\n+          auto makeTensorPtrOp = getMakeTensorPtrOp(src);\n+          if (shouldRemove(makeTensorPtrOp, computeCapability))\n+            valueToRemove.insert(src);\n+        }\n+      }\n+      if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+        SmallVector<Value> iterOperands = forOp.getIterOperands();\n+        for (unsigned i = 0, size = forOp.getNumIterOperands(); i < size; ++i) {\n+          if (tt::isTensorPointerType(iterOperands[i].getType())) {\n+            auto makeTensorPtrOp = getMakeTensorPtrOp(iterOperands[i]);\n+            if (shouldRemove(makeTensorPtrOp, computeCapability))\n+              valueToRemove.insert(iterOperands[i]);\n+          }\n+        }\n+      } else if (auto yieldOp = dyn_cast<scf::YieldOp>(op)) {\n+        SmallVector<Value> operands = yieldOp->getOperands();\n+        for (unsigned i = 0, size = yieldOp.getNumOperands(); i < size; ++i) {\n+          if (tt::isTensorPointerType(operands[i].getType())) {\n+            auto makeTensorPtrOp = getMakeTensorPtrOp(operands[i]);\n+            if (shouldRemove(makeTensorPtrOp, computeCapability))\n+              valueToRemove.insert(operands[i]);\n+          }\n+        }\n+      }\n+    });\n+\n+    // NOTES(Chenggang): we don't use `ConversionPatternRewriter`, because\n+    // MLIR does not support one-multiple value mapping. For example, if we use\n+    // `ConversionPatternRewriter`, we can not make a type converter, which\n+    // converts `ptr<tensor>` into multiple types `ptr<>, int64, int64, ...`\n+    // (containing the base/offsets/strides...). What we can do is to convert\n+    // `ptr<tensor>` into a single type `Tuple<ptr<>, int64, int64, ...>`. But\n+    // in this way, we also have to define `PackTuple` and `UnpackTuple`\n+    // operations and make a canonicalization pass to optimize, which is much\n+    // So here we recursively build the IR, to be specific, we have to rewrite\n+    // `tt.make_tensor_ptr`, `tt.advance`, `tt.load`, `tt.store`,\n+    // `scf.for` (tensor pointer usages may be in a loop fashion)\n+    std::stack<Operation *> eraser;\n+    visitOperation(getOperation(), eraser, valueToRemove);\n+\n+    // The operation could not be erased during visit, because they may have\n+    // later usages, so we erase after visit\n+    rewritedInfo.clear();\n+    valueToRemove.clear();\n+    while (!eraser.empty()) {\n+      auto op = eraser.top();\n+      eraser.pop();\n+      op->erase();\n+    }\n+  }\n+};\n+\n+std::unique_ptr<Pass>\n+mlir::createTritonGPURewriteTensorPointerPass(int computeCapability) {\n+  return std::make_unique<TritonGPURewriteTensorPointerPass>(computeCapability);\n+}"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/Utility.cpp", "status": "added", "additions": 542, "deletions": 0, "changes": 542, "file_content_changes": "@@ -0,0 +1,542 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n+\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"mlir/Dialect/SCF/IR/SCF.h\"\n+#include \"mlir/IR/IRMapping.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"llvm/Support/Debug.h\"\n+#include <fstream>\n+\n+namespace mlir {\n+\n+namespace ttg = triton::gpu;\n+\n+namespace {\n+\n+// Suppose the kernel has following structure:\n+// ```\n+// scf.for(...) {\n+//   compute_0(i)\n+//   barrier(...)\n+//   compute_1(i)\n+// }\n+// ```\n+// Due to the barrier between compute_0(i) and compute_1(i), we\n+// can not pre-compute compute_0(i+1) before compute_1(i) semantically.\n+// In some case, it may be still functionally correct to pre-compute\n+// compute_0(i+1) while it's very hard to prove it at compile time.\n+//\n+// Here we use a simple strategy: skip auto wrap specialize those kernels that\n+// use global barriers.\n+//\n+// Another remaining question is how to detect barrier in a triton program.\n+// There is not a barrier op in triton yet. It's usually implemented using\n+// atomic_* ops. Hence we simply detect if there are some atomc_* ops. It may\n+// miss some auto-WS opportunities and we leave it for the future to improve it.\n+bool hasUnsafeBarrier(triton::FuncOp funcOp) {\n+  return funcOp\n+      ->walk([](Operation *op) {\n+        if (isa<triton::AtomicRMWOp, triton::AtomicCASOp>(op))\n+          return WalkResult::interrupt();\n+        return WalkResult::advance();\n+      })\n+      .wasInterrupted();\n+}\n+\n+// Assigns `dependentSet` and returns ok if the analysis is successful.\n+// We do not support dependency analysis across load/store, thus a failure will\n+// be returned if encountering such cases.\n+LogicalResult getDependentPointers(Value ptr, DenseSet<Value> &dependentSet,\n+                                   DenseSet<Value> &processedSet) {\n+  // early return if processed\n+  if (!processedSet.insert(ptr).second)\n+    return success();\n+\n+  if (auto blockArg = ptr.dyn_cast<BlockArgument>()) {\n+    if (!blockArg.getOwner()->isEntryBlock())\n+      return failure();\n+    auto parentOp = blockArg.getOwner()->getParentOp();\n+    if (auto forOp = dyn_cast<scf::ForOp>(parentOp)) {\n+      if (blockArg.getArgNumber() >= forOp.getNumInductionVars()) {\n+        if (failed(getDependentPointers(\n+                forOp.getOpOperandForRegionIterArg(blockArg).get(),\n+                dependentSet, processedSet)))\n+          return failure();\n+\n+        unsigned operandIdx =\n+            blockArg.getArgNumber() - forOp.getNumInductionVars();\n+        return getDependentPointers(\n+            forOp.getBody()->getTerminator()->getOperand(operandIdx),\n+            dependentSet, processedSet);\n+      }\n+    } else if (auto funcOp = dyn_cast<triton::FuncOp>(parentOp)) {\n+      dependentSet.insert(ptr);\n+      return success();\n+    }\n+    // unknown ops, return failure for correctness.\n+    return failure();\n+  }\n+\n+  auto definingOp = ptr.getDefiningOp();\n+  assert(definingOp);\n+  if (auto makeTensorPtrOp = ptr.getDefiningOp<triton::MakeTensorPtrOp>()) {\n+    return getDependentPointers(makeTensorPtrOp.getBase(), dependentSet,\n+                                processedSet);\n+  } else if (auto advanceOp = ptr.getDefiningOp<triton::AdvanceOp>()) {\n+    return getDependentPointers(advanceOp.getPtr(), dependentSet, processedSet);\n+  } else if (auto addPtrOp = ptr.getDefiningOp<triton::AddPtrOp>()) {\n+    return getDependentPointers(addPtrOp.getPtr(), dependentSet, processedSet);\n+  } else if (auto loadOp = ptr.getDefiningOp<triton::AddPtrOp>()) {\n+    // not support load dependent ptr\n+    return failure();\n+  } else if (auto forOp = ptr.getDefiningOp<scf::ForOp>()) {\n+    unsigned idx = ptr.cast<OpResult>().getResultNumber();\n+    return getDependentPointers(\n+        forOp.getBody()->getTerminator()->getOperand(idx), dependentSet,\n+        processedSet);\n+  } else if (auto ifOp = ptr.getDefiningOp<scf::IfOp>()) {\n+    unsigned idx = ptr.cast<OpResult>().getResultNumber();\n+    if (ifOp.elseBlock() &&\n+        failed(getDependentPointers(ifOp.elseYield()->getOperand(idx),\n+                                    dependentSet, processedSet)))\n+      return failure();\n+    return getDependentPointers(ifOp.thenYield()->getOperand(idx), dependentSet,\n+                                processedSet);\n+  } else if (!definingOp->getNumRegions()) {\n+    for (Value operand : definingOp->getOperands())\n+      if (failed(getDependentPointers(operand, dependentSet, processedSet)))\n+        return failure();\n+    return success();\n+  }\n+  // unknown ops, return failure for correctness.\n+  return failure();\n+}\n+\n+// Suppose the kernel has following structure:\n+// ```\n+// scf.for(...) {\n+//   v(i) = load(ptr)\n+//   new_v(i) = some_compute(v(i), ...)\n+//   store(new_v(i), ptr)\n+// }\n+// ```\n+//\n+// There is an implicit dependency between load(i+1) and store(i), which means\n+// we can not pre-compute load(i+1) before store(i).\n+//\n+// To avoid such load after store conflict, we simply disallow mixed load and\n+// store for the same buffer. It's a conservative strategy and can be relaxed in\n+// case necessary.\n+bool hasUnsafeLoadAfterStore(triton::FuncOp funcOp) {\n+  // TODO: support CFG\n+  if (funcOp.getBody().getBlocks().size() > 1)\n+    return true;\n+\n+  DenseMap<Value, bool> ptrStoreMap;\n+  DenseMap<Value, bool> ptrLoadMap;\n+  if (funcOp\n+          ->walk([&](triton::LoadOp loadOp) {\n+            DenseSet<Value> dependentSet, processedSet;\n+            if (failed(getDependentPointers(loadOp.getPtr(), dependentSet,\n+                                            processedSet)))\n+              return WalkResult::interrupt();\n+            for (Value v : dependentSet)\n+              ptrLoadMap[v] = true;\n+            return WalkResult::advance();\n+          })\n+          .wasInterrupted())\n+    return false;\n+  auto result = funcOp->walk([&](Operation *op) {\n+    if (auto storeOp = dyn_cast<triton::StoreOp>(op)) {\n+      DenseSet<Value> dependentSet, processedSet;\n+      if (failed(getDependentPointers(storeOp.getPtr(), dependentSet,\n+                                      processedSet)))\n+        return WalkResult::interrupt();\n+\n+      for (Value v : dependentSet)\n+        ptrStoreMap[v] = true;\n+\n+      // TODO: relax the restriction in case necessary.\n+      // If a store is inside a region, e.g. scf.while/for/if, its\n+      // dependent ptrs are not allowed to be loaded.\n+      if (op->getParentOp() != funcOp) {\n+        for (Value v : dependentSet)\n+          if (ptrLoadMap.find(v) != ptrLoadMap.end())\n+            return WalkResult::interrupt();\n+      }\n+    } else if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+      DenseSet<Value> dependentSet, processedSet;\n+      if (failed(getDependentPointers(loadOp.getPtr(), dependentSet,\n+                                      processedSet)))\n+        return WalkResult::interrupt();\n+      for (Value v : dependentSet)\n+        if (ptrStoreMap.find(v) != ptrStoreMap.end())\n+          return WalkResult::interrupt();\n+    }\n+    return WalkResult::advance();\n+  });\n+\n+  return result.wasInterrupted();\n+}\n+\n+bool hasWSCandidateLoad(triton::FuncOp funcOp) {\n+  SmallVector<triton::LoadOp> loadOps;\n+  funcOp->walk([&](triton::LoadOp loadOp) {\n+    if (isWSCandidateLoad(loadOp))\n+      loadOps.push_back(loadOp);\n+  });\n+  if (loadOps.empty())\n+    return false;\n+\n+  // All the candidate ops should be in the same block and have compatible\n+  // types.\n+  Block *block = loadOps[0]->getBlock();\n+  auto refTy = loadOps[0].getPtr().getType().dyn_cast<triton::PointerType>();\n+  bool isPtrToTensor = refTy && refTy.getPointeeType().isa<RankedTensorType>();\n+  for (auto loadOp : loadOps) {\n+    if (loadOp->getBlock() != block)\n+      return false;\n+    // not support mixed ptr to tensor and tensor of ptr currently.\n+    auto ty = loadOp.getPtr().getType().dyn_cast<triton::PointerType>();\n+    if (isPtrToTensor != (ty && ty.getPointeeType().isa<RankedTensorType>()))\n+      return false;\n+  }\n+\n+  // S0 = dependent value set of all the candidate ops\n+  // S1 = dependent value set of all the store ops\n+  // S2 = S1 & S0\n+  // any value in S2 should not be the output of an op having regions.\n+  // TODO: lift the limitation of WSPipeline pass to remove this check.\n+  DenseSet<Value> loadDepSet;\n+  DenseSet<Value> loadSet;\n+  for (auto op : loadOps) {\n+    if (failed(getDependentValues(op.getOperation(), loadDepSet)))\n+      return false;\n+    loadSet.insert(op->getResult(0));\n+  }\n+\n+  DenseSet<Value> storeDepSet;\n+  if (funcOp\n+          ->walk([&](triton::StoreOp op) {\n+            if (failed(getDependentValues(op.getOperation(), storeDepSet,\n+                                          loadSet)))\n+              return WalkResult::interrupt();\n+            return WalkResult::advance();\n+          })\n+          .wasInterrupted())\n+    return false;\n+\n+  for (Value v : loadDepSet)\n+    if (storeDepSet.find(v) != storeDepSet.end()) {\n+      auto op = v.getDefiningOp();\n+      if (op && op->getNumRegions())\n+        return false;\n+    }\n+\n+  return true;\n+}\n+\n+} // namespace\n+\n+//===----------------------------------------------------------------------===//\n+// Helper functions for async agent\n+//===----------------------------------------------------------------------===//\n+\n+SmallVector<AgentId> getAgentIds(Operation *op) {\n+  SmallVector<AgentId> agentIds;\n+  if (auto attr = op->getAttrOfType<DenseIntElementsAttr>(\"async_agent\"))\n+    for (AgentId agentId : attr.getValues<AgentId>())\n+      agentIds.push_back(agentId);\n+  return agentIds;\n+}\n+\n+bool hasAgentId(Operation *op, AgentId agentId) {\n+  for (AgentId candidate : getAgentIds(op))\n+    if (candidate == agentId)\n+      return true;\n+  return false;\n+}\n+\n+void setAgentIds(Operation *op, ArrayRef<AgentId> agentIds) {\n+  SmallVector<AgentId> sortedAgentIds(agentIds.begin(), agentIds.end());\n+  sort(sortedAgentIds);\n+  auto i32Ty = IntegerType::get(op->getContext(), 32);\n+  auto size = static_cast<int64_t>(sortedAgentIds.size());\n+  auto vecTy = VectorType::get(size, i32Ty);\n+  op->setAttr(\"async_agent\", DenseIntElementsAttr::get(vecTy, sortedAgentIds));\n+}\n+\n+SmallVector<AgentId> collectAgentIds(Operation *op) {\n+  SetVector<AgentId> agentIds;\n+  op->walk([&](Operation *curOp) {\n+    for (AgentId agentId : getAgentIds(curOp))\n+      agentIds.insert(agentId);\n+  });\n+  SmallVector<AgentId> res(agentIds.begin(), agentIds.end());\n+  llvm::sort(res);\n+  return res;\n+}\n+\n+void addAgentIds(Operation *op, ArrayRef<int> agents) {\n+  auto agentsVec = getAgentIds(op);\n+  DenseSet<int> agentsSet(agentsVec.begin(), agentsVec.end());\n+  for (int a : agents) {\n+    if (!agentsSet.contains(a)) {\n+      agentsVec.push_back(a);\n+    }\n+  }\n+  if (agentsVec.size() > 0) {\n+    setAgentIds(op, agentsVec);\n+  }\n+}\n+\n+SmallVector<int> getMutexBarIds(Operation *op) {\n+  SmallVector<int> barIds;\n+  if (auto attr = op->getAttrOfType<DenseIntElementsAttr>(\"mutex.barId\"))\n+    for (int id : attr.getValues<int>())\n+      barIds.push_back(id);\n+  return barIds;\n+}\n+\n+SmallVector<int> getMutexNumThreads(Operation *op) {\n+  SmallVector<int> numThreads;\n+  if (auto attr = op->getAttrOfType<DenseIntElementsAttr>(\"mutex.numThreads\"))\n+    for (int n : attr.getValues<int>())\n+      numThreads.push_back(n);\n+  return numThreads;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Implementations for general auto WS\n+//===----------------------------------------------------------------------===//\n+\n+// Populates `depSet` with the values that `val` depends on and Returns success.\n+// Returns failure() if encountering any unsupported conditions.\n+LogicalResult getDependentValues(Value val, DenseSet<Value> &depSet,\n+                                 const DenseSet<Value> &stopSet) {\n+  auto tryInsertAndPropagate = [&](Value other) {\n+    if (stopSet.find(other) == stopSet.end() && depSet.insert(other).second)\n+      return getDependentValues(other, depSet, stopSet);\n+    return success();\n+  };\n+  auto addControlOperandsForForOp = [&](scf::ForOp forOp) {\n+    for (Value operand :\n+         forOp->getOperands().take_front(forOp.getNumControlOperands()))\n+      if (failed(tryInsertAndPropagate(operand)))\n+        return failure();\n+    return success();\n+  };\n+  auto addControlOperandsForIfOp = [&](scf::IfOp ifOp) {\n+    return tryInsertAndPropagate(ifOp.getCondition());\n+  };\n+  auto propagateParentOp = [&](Operation *op) {\n+    while (Operation *parentOp = op->getParentOp()) {\n+      if (auto forOp = dyn_cast<scf::ForOp>(parentOp))\n+        return addControlOperandsForForOp(forOp);\n+      else if (auto ifOp = dyn_cast<scf::IfOp>(parentOp))\n+        return addControlOperandsForIfOp(ifOp);\n+      else if (auto funcOp = dyn_cast<triton::FuncOp>(parentOp))\n+        return success();\n+      else\n+        break;\n+      op = parentOp;\n+    }\n+    // unknown ops, return failure for correctness.\n+    return failure();\n+  };\n+\n+  if (auto blockArg = val.dyn_cast<BlockArgument>()) {\n+    auto parentOp = blockArg.getOwner()->getParentOp();\n+    if (auto forOp = dyn_cast<scf::ForOp>(parentOp)) {\n+      // add control operands of forOp into dependent set\n+      if (failed(addControlOperandsForForOp(forOp)))\n+        return failure();\n+      if (blockArg.getArgNumber() >= forOp.getNumInductionVars()) {\n+        Value operand = forOp.getOpOperandForRegionIterArg(blockArg).get();\n+        if (failed(tryInsertAndPropagate(operand)))\n+          return failure();\n+\n+        unsigned operandIdx =\n+            blockArg.getArgNumber() - forOp.getNumInductionVars();\n+        return tryInsertAndPropagate(\n+            forOp.getBody()->getTerminator()->getOperand(operandIdx));\n+      }\n+      return propagateParentOp(parentOp);\n+    } else if (auto funcOp = dyn_cast<triton::FuncOp>(parentOp)) {\n+      if (stopSet.find(val) == stopSet.end())\n+        depSet.insert(val);\n+      return success();\n+    } else {\n+      // unknown ops, return failure for correctness.\n+      return failure();\n+    }\n+  }\n+\n+  auto definingOp = val.getDefiningOp();\n+  assert(definingOp);\n+  if (auto forOp = val.getDefiningOp<scf::ForOp>()) {\n+    if (failed(addControlOperandsForForOp(forOp)))\n+      return failure();\n+    unsigned idx = val.cast<OpResult>().getResultNumber();\n+    if (failed(tryInsertAndPropagate(\n+            forOp->getOperand(idx + forOp.getNumControlOperands()))))\n+      return failure();\n+    return tryInsertAndPropagate(\n+        forOp.getBody()->getTerminator()->getOperand(idx));\n+  } else if (auto ifOp = val.getDefiningOp<scf::IfOp>()) {\n+    if (failed(addControlOperandsForIfOp(ifOp)))\n+      return failure();\n+    unsigned idx = val.cast<OpResult>().getResultNumber();\n+    if (ifOp.elseBlock() &&\n+        failed(tryInsertAndPropagate(ifOp.elseYield()->getOperand(idx))))\n+      return failure();\n+    return tryInsertAndPropagate(ifOp.thenYield()->getOperand(idx));\n+  } else if (!definingOp->getNumRegions()) {\n+    for (Value operand : definingOp->getOperands())\n+      if (failed(tryInsertAndPropagate(operand)))\n+        return failure();\n+    return success();\n+  } else {\n+    // unknown ops, return failure for correctness.\n+    return failure();\n+  }\n+\n+  return propagateParentOp(definingOp);\n+}\n+\n+LogicalResult getDependentValues(Operation *op, DenseSet<Value> &depSet,\n+                                 const DenseSet<Value> &stopSet) {\n+  if (op->getNumResults() > 0) {\n+    for (Value result : op->getResults())\n+      if (failed(getDependentValues(result, depSet, stopSet)))\n+        return failure();\n+  } else {\n+    // Not support op with regions\n+    if (op->getNumRegions() != 0)\n+      return failure();\n+    for (Value operand : op->getOperands()) {\n+      if (stopSet.find(operand) != stopSet.end())\n+        continue;\n+      depSet.insert(operand);\n+      if (failed(getDependentValues(operand, depSet, stopSet)))\n+        return failure();\n+    }\n+  }\n+  return success();\n+}\n+\n+DenseSet<Operation *> getDependentOps(DenseSet<Value> &depSet) {\n+  DenseSet<Operation *> depOps;\n+  for (Value val : depSet) {\n+    Operation *op = val.getDefiningOp();\n+    if (auto blockArg = val.dyn_cast<BlockArgument>())\n+      op = blockArg.getOwner()->getParentOp();\n+\n+    while (op && !isa<triton::FuncOp>(op)) {\n+      depOps.insert(op);\n+      op = op->getParentOp();\n+    }\n+  }\n+  return depOps;\n+}\n+\n+bool isWSCandidateLoad(Operation *op) {\n+  auto loadOp = dyn_cast<triton::LoadOp>(op);\n+  if (!loadOp)\n+    return false;\n+\n+  Value result = loadOp->getResult(0);\n+  auto resultTy = result.getType().cast<RankedTensorType>();\n+  // Skip those tensors that are too small.\n+  if (resultTy.getNumElements() <= 64)\n+    return false;\n+  // TODO: remove this limit once we refator ws pipeline pass.\n+  if (resultTy.getNumElements() % 128 != 0)\n+    return false;\n+  // pattern match: load + convert_layout(blocked, shared)\n+  if (!result.hasOneUse())\n+    return false;\n+  auto cvtOp = dyn_cast<ttg::ConvertLayoutOp>(*result.getUsers().begin());\n+  if (!cvtOp)\n+    return false;\n+  auto encoding =\n+      cvtOp.getResult().getType().cast<RankedTensorType>().getEncoding();\n+  if (!encoding || !encoding.dyn_cast<ttg::SharedEncodingAttr>())\n+    return false;\n+  if (ttg::getNumCTAs(encoding) > 1)\n+    return false;\n+\n+  DenseSet<Value> depSet;\n+  if (failed(getDependentValues(op->getResult(0), depSet)))\n+    return false;\n+  auto depOps = getDependentOps(depSet);\n+  for (Operation *depOp : depOps) {\n+    if (isa<triton::DotOp, triton::LoadOp, triton::ReduceOp>(depOp))\n+      return false;\n+  }\n+  return op->getParentOfType<scf::ForOp>() ||\n+         op->getParentOfType<scf::WhileOp>();\n+}\n+\n+bool isWSSupported(ModuleOp mod, int computeCapability) {\n+  // Early return if the target device is not feasible.\n+  if (computeCapability / 10 < 9) {\n+    return false;\n+  }\n+\n+  // TODO: support function call.\n+  triton::FuncOp funcOp;\n+  if (mod->walk([&](triton::FuncOp op) {\n+           if (funcOp)\n+             return WalkResult::interrupt();\n+           funcOp = op;\n+           return WalkResult::advance();\n+         })\n+          .wasInterrupted() ||\n+      !funcOp)\n+    return false;\n+\n+  // Triton programs with global barrier are much harder to do auto warp\n+  // specialization. Here we do some conservative checks to skip the bad cases.\n+  if (hasUnsafeBarrier(funcOp))\n+    return false;\n+\n+  // load after store for the same buffer forces an implicit dependency, which\n+  // may break auto WS. Here we do some conservative checks to skip the bad\n+  // cases.\n+  if (hasUnsafeLoadAfterStore(funcOp))\n+    return false;\n+\n+  if (!hasWSCandidateLoad(funcOp))\n+    return false;\n+\n+  return true;\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSDecomposing.cpp", "status": "added", "additions": 260, "deletions": 0, "changes": 260, "file_content_changes": "@@ -0,0 +1,260 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+using namespace mlir;\n+namespace ttng = mlir::triton::nvidia_gpu;\n+\n+class Heuristics {\n+  //===--------------------------Label rules--------------------------===//\n+  //  - Op without agent attr: opeations shared by all agents but will NOT\n+  //    be copied into each agent region\n+  //  - Op with one agent: exclusive for one agent\n+  //  - Op with agents: shared by agents and will be copied into each agent\n+  //  region\n+  //===---------------------------------------------------------------===//\n+public:\n+  Heuristics(MLIRContext *context, ModuleOp mod, const int &computeCapability)\n+      : context(context), mod(mod), computeCapability(computeCapability),\n+        builder(OpBuilder(context)) {}\n+  virtual bool run() = 0;\n+\n+protected:\n+  // Set agentId when condition is satisfied\n+  virtual void\n+  setAgentId_if(int agentId,\n+                const std::function<bool(Operation *)> &condition) {\n+    mod.walk([&](Operation *op) -> void {\n+      if (condition(op)) {\n+        setAgentIds(op, {agentId});\n+      }\n+    });\n+  }\n+\n+  static bool isTritonLoadOp(Operation *op) { return isa<triton::LoadOp>(op); }\n+\n+  static bool isTritonDotOp(Operation *op) { return isa<triton::DotOp>(op); }\n+\n+  static bool isTritonStoreOp(Operation *op) {\n+    return isa<triton::StoreOp>(op);\n+  }\n+\n+  /// Becuase we set some special filter rules in populateAgentRegion,\n+  /// there may be unlabeled Ops, e.g. YieldOps, some definingOps of ForOps.\n+  /// or Ops without relations to agentOps\n+  virtual void populateUnlabledOpsAtLast(ArrayRef<int> allAgents) {\n+    // Label agents' parentOps\n+    for (int i : allAgents) {\n+      DenseSet<Operation *> agentParentOps;\n+      getAllParentOps(agentParentOps, i);\n+      for (auto op : agentParentOps) {\n+        addAgentIds(op, {i});\n+      }\n+    }\n+\n+    // Get unlabeled Ops\n+    DenseSet<Operation *> unlabeledOps;\n+    mod.walk([&](Operation *op) -> void {\n+      if (isa<ModuleOp>(op) || isa<triton::FuncOp>(op) ||\n+          isa<triton::ReturnOp>(op)) {\n+        return;\n+      }\n+      if (!op->hasAttr(\"async_agent\")) {\n+        unlabeledOps.insert(op);\n+      }\n+    });\n+\n+    // Label Ops using its parentOp\n+    for (auto op : unlabeledOps) {\n+      if (auto parent = op->getParentOp()) {\n+        if (!isa<triton::FuncOp>(parent)) {\n+          assert(parent->hasAttr(\"async_agent\"));\n+          auto agents = getAgentIds(parent);\n+          setAgentIds(op, agents);\n+          unlabeledOps.erase(op);\n+        }\n+      }\n+    }\n+\n+    // Label Ops using dependency\n+    for (auto op : unlabeledOps) {\n+      labelByUsers(op, allAgents);\n+      unlabeledOps.erase(op);\n+    }\n+    assert(unlabeledOps.size() == 0);\n+  }\n+\n+  // Return all Ops that are marked with target agent\n+  void getAgentOps(DenseSet<Operation *> &agentOps, int agentId) {\n+    SmallVector tmpArray{agentId};\n+    auto agentAttr = builder.getI32VectorAttr(ArrayRef<int>(tmpArray));\n+    mod.walk([&](Operation *op) -> void {\n+      if (op->hasAttr(\"async_agent\") &&\n+          op->getAttr(\"async_agent\") == agentAttr) {\n+        agentOps.insert(op);\n+      }\n+    });\n+  }\n+\n+  void getAllParentOps(DenseSet<Operation *> &parentOps, int agentId) {\n+    DenseSet<Operation *> targetOps;\n+    getAgentOps(targetOps, agentId);\n+    for (auto op : targetOps) {\n+      getAllParentOps(parentOps, op);\n+    }\n+  }\n+\n+  void getAllParentOps(DenseSet<Operation *> &parentOps, Operation *targetOp) {\n+    auto op = targetOp;\n+    while (auto parent = op->getParentOp()) {\n+      if (!isa<ModuleOp>(parent) && !isa<triton::FuncOp>(parent)) {\n+        parentOps.insert(parent);\n+        op = parent;\n+      } else {\n+        break;\n+      }\n+    }\n+  }\n+\n+  void labelByUsers(Operation *op, ArrayRef<int> allAgents) {\n+    for (Value result : op->getResults()) {\n+      for (Operation *userOp : result.getUsers()) {\n+        if (!userOp->hasAttr(\"async_agent\")) {\n+          labelByUsers(userOp, allAgents);\n+        }\n+        addAgentIds(op, getAgentIds(userOp));\n+      }\n+    }\n+    if (!op->hasAttr(\"async_agent\")) {\n+      addAgentIds(op, allAgents);\n+    }\n+  }\n+\n+protected:\n+  MLIRContext *context;\n+  ModuleOp mod;\n+  int computeCapability;\n+  OpBuilder builder;\n+};\n+\n+//===------------------------heuristics list------------------------===//\n+// List all heuristics here:\n+//  - Heuristic_Load_MathStore: assign load and math+store to two\n+//    different agents respectively.\n+//===---------------------------------------------------------------===//\n+\n+class Heuristic_Load_MathStore : public Heuristics {\n+public:\n+  Heuristic_Load_MathStore(MLIRContext *context, ModuleOp mod,\n+                           const int &computeCapability)\n+      : Heuristics(context, mod, computeCapability) {}\n+  bool run() override {\n+    constexpr int kLoadAgentId = 0;\n+    constexpr int kStoreAgentId = 1;\n+    constexpr int kNumAgents = 2;\n+\n+    //===--------------------1. label key operations--------------------===//\n+    setAgentId_if(kLoadAgentId, isWSCandidateLoad);\n+    setAgentId_if(kStoreAgentId, isTritonStoreOp);\n+\n+    //===--------------2. populate based on key operations--------------===//\n+    // find the roots (outputs) of LoadAgent\n+    DenseSet<Operation *> loadOps;\n+    getAgentOps(loadOps, kLoadAgentId);\n+    // find LoadAgent dependent ops\n+    DenseSet<Value> loadValues;\n+    DenseSet<Value> loadAgentDepValues;\n+    for (Operation *op : loadOps) {\n+      if (failed(getDependentValues(op, loadAgentDepValues)))\n+        return false;\n+      loadValues.insert(op->getResult(0));\n+    }\n+    for (Operation *op : getDependentOps(loadAgentDepValues))\n+      addAgentIds(op, kLoadAgentId);\n+\n+    // find the roots (outputs) of StoreAgent\n+    DenseSet<Operation *> storeOps;\n+    getAgentOps(storeOps, kStoreAgentId);\n+    // find StoreAgent dependent ops\n+    DenseSet<Value> storeAgentDepValues;\n+    for (Operation *op : storeOps)\n+      if (failed(getDependentValues(op, storeAgentDepValues, loadValues)))\n+        return false;\n+    for (Operation *op : getDependentOps(storeAgentDepValues))\n+      addAgentIds(op, kStoreAgentId);\n+\n+    //===---------------------3. label unlabeld Ops---------------------===//\n+    populateUnlabledOpsAtLast({kLoadAgentId, kDotAgentId});\n+\n+    // Erase labels of MakeTensorPtrOp and its definingOps,\n+    // because we don't want them to be copied in each agent\n+    SetVector<Operation *> backwardSlice;\n+    mod.walk([&](triton::MakeTensorPtrOp op) -> void {\n+      assert(isa<triton::FuncOp>(op->getParentOp()));\n+      getBackwardSlice(op.getOperation(), &backwardSlice);\n+      op->removeAttr(\"async_agent\");\n+    });\n+    for (auto op : backwardSlice) {\n+      op->removeAttr(\"async_agent\");\n+    }\n+    // Set num-agents for wsmaterialization pass\n+    mod->setAttr(\"async.num-agents\", builder.getI32IntegerAttr(kNumAgents));\n+    return true;\n+  }\n+};\n+\n+class TritonGPUWSDecomposingPass\n+    : public TritonGPUWSDecomposingBase<TritonGPUWSDecomposingPass> {\n+public:\n+  TritonGPUWSDecomposingPass() = default;\n+  TritonGPUWSDecomposingPass(int computeCapability) {\n+    this->computeCapability = computeCapability;\n+  }\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp mod = getOperation();\n+    if (!ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod))\n+      return signalPassFailure();\n+\n+    // Build Heuristics\n+    Heuristic_Load_MathStore hLoadMathBasic(context, mod, computeCapability);\n+    if (!(hLoadMathBasic.run())) {\n+      return signalPassFailure();\n+    }\n+  }\n+};\n+\n+std::unique_ptr<Pass>\n+mlir::createTritonNvidiaGPUWSDecomposingPass(int computeCapability) {\n+  return std::make_unique<TritonGPUWSDecomposingPass>(computeCapability);\n+}"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSFeasibilityChecking.cpp", "status": "added", "additions": 64, "deletions": 0, "changes": 64, "file_content_changes": "@@ -0,0 +1,64 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+namespace mlir {\n+\n+namespace ttng = triton::nvidia_gpu;\n+\n+namespace {\n+\n+class TritonGPUWSFeasibilityCheckingPass\n+    : public TritonGPUWSFeasibilityCheckingBase<\n+          TritonGPUWSFeasibilityCheckingPass> {\n+public:\n+  TritonGPUWSFeasibilityCheckingPass() = default;\n+  TritonGPUWSFeasibilityCheckingPass(int computeCapability) {\n+    this->computeCapability = computeCapability;\n+  }\n+\n+  void runOnOperation() override {\n+    ModuleOp mod = getOperation();\n+    int wsSupported = isWSSupported(mod, this->computeCapability);\n+    auto i32_ty = IntegerType::get(mod->getContext(), 32);\n+    mod->setAttr(ttng::TritonNvidiaGPUDialect::getWSSupportedAttrName(),\n+                 IntegerAttr::get(i32_ty, llvm::APInt(32, wsSupported)));\n+  }\n+};\n+\n+} // namespace\n+\n+std::unique_ptr<Pass>\n+createTritonNvidiaGPUWSFeasibilityCheckingPass(int computeCapability) {\n+  return std::make_unique<TritonGPUWSFeasibilityCheckingPass>(\n+      computeCapability);\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMaterialization.cpp", "status": "added", "additions": 764, "deletions": 0, "changes": 764, "file_content_changes": "@@ -0,0 +1,764 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+\n+#include \"mlir/IR/OperationSupport.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/Triton/IR/Types.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n+\n+using namespace mlir;\n+namespace ttg = triton::gpu;\n+namespace ttng = triton::nvidia_gpu;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+namespace {\n+\n+enum class LoadType {\n+  Uninitialized,\n+  InsertSliceAsyncOp,\n+  InsertSliceAsyncV2Op,\n+  MultiKinds,\n+};\n+\n+// This helper function returns the real threadId while ttng::GetThreadIdOp is\n+// actually threadId % 128 when warp specialization is enabled\n+Value getThreadId(OpBuilder &builder, Location loc) {\n+  Value threadId = builder.create<::mlir::gpu::ThreadIdOp>(\n+      loc, builder.getIndexType(), ::mlir::gpu::Dimension::x);\n+  auto cast = builder.create<UnrealizedConversionCastOp>(\n+      loc, TypeRange{builder.getIntegerType(32)}, ValueRange{threadId});\n+  return cast.getResult(0);\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Materialize GetAgentIdOp\n+//===----------------------------------------------------------------------===//\n+\n+void materializeGetAgentIdOp(Operation *parentOp) {\n+  parentOp->walk([](ttng::GetAgentIdOp op) {\n+    // In Hopper, each agent is a warpgroup consisting with 4 warps.\n+    auto loc = op.getLoc();\n+    OpBuilder builder(op);\n+\n+    Value _128 = builder.create<arith::ConstantIntOp>(loc, 128, 32);\n+    Value threadId = getThreadId(builder, loc);\n+    Value agentId = builder.create<arith::DivUIOp>(loc, threadId, _128);\n+    op.getResult().replaceAllUsesWith(agentId);\n+    op->erase();\n+\n+    // Update agent condition and insert \"agent.num-warps\"\n+    auto agentIdOp = agentId.getDefiningOp();\n+    builder.setInsertionPoint(agentIdOp);\n+    Value globalRoleId = builder.create<arith::ConstantIntOp>(loc, 0, 32);\n+    int globalNumWarps = 0;\n+    for (auto cmpOp : agentIdOp->getUsers()) {\n+      assert(isa<arith::CmpIOp>(cmpOp));\n+      for (auto u : cmpOp->getUsers()) {\n+        if (isa<scf::IfOp>(u) && isa<triton::FuncOp>(u->getParentOp()) &&\n+            u->hasAttr(\"async_agent\") && getAgentIds(u).size() == 1) {\n+          loc = u->getLoc();\n+          builder.setInsertionPoint(u);\n+          int numRoles = 1;\n+          if (u->hasAttr(\"agent.num-roles\")) {\n+            numRoles =\n+                u->getAttrOfType<IntegerAttr>(\"agent.num-roles\").getInt();\n+            // TODO: more flexible ways to get numWarps.\n+            auto numWarps = builder.getI32IntegerAttr(4 * numRoles);\n+            auto numWarpsBase = builder.getI32IntegerAttr(globalNumWarps);\n+            u->setAttr(\"agent.num-warps\", numWarps);\n+            u->walk([&](ttng::GetMutexRoleIdOp roleIdOp) {\n+              roleIdOp->setAttr(\"agent.num-warps\", numWarps);\n+              roleIdOp->setAttr(\"agent.num-warps-base\", numWarpsBase);\n+            });\n+          }\n+          globalNumWarps += numRoles * 4;\n+          Value offset =\n+              builder.create<arith::ConstantIntOp>(loc, numRoles, 32);\n+          Value lowerBound = builder.create<arith::CmpIOp>(\n+              loc, arith::CmpIPredicate::uge, agentId, globalRoleId);\n+          globalRoleId =\n+              builder.create<arith::AddIOp>(loc, globalRoleId, offset);\n+          Value upperBound = builder.create<arith::CmpIOp>(\n+              loc, arith::CmpIPredicate::ult, agentId, globalRoleId);\n+          Value cond =\n+              builder.create<arith::AndIOp>(loc, lowerBound, upperBound);\n+          cmpOp->getResult(0).replaceAllUsesWith(cond);\n+          cmpOp->erase();\n+          break;\n+        }\n+      }\n+    }\n+  });\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Materialize token operations\n+//===----------------------------------------------------------------------===//\n+\n+LoadType scanLoadTypes(ttng::CreateTokenOp createTokenOp) {\n+  // TODO: Attach information of binded tensors to CreateTokenOp\n+  std::set<LoadType> loadTypes;\n+  createTokenOp->getBlock()->walk([&](Operation *op) {\n+    if (auto insertOp = dyn_cast<ttg::InsertSliceOp>(op)) {\n+      if (triton::isTensorPointerType(insertOp.getSrc().getType()))\n+        loadTypes.insert(LoadType::InsertSliceAsyncV2Op);\n+      else\n+        loadTypes.insert(LoadType::InsertSliceAsyncOp);\n+    } else if (isa<ttg::InsertSliceAsyncOp>(op)) {\n+      loadTypes.insert(LoadType::InsertSliceAsyncOp);\n+    } else if (isa<ttng::InsertSliceAsyncV2Op>(op)) {\n+      loadTypes.insert(LoadType::InsertSliceAsyncV2Op);\n+    }\n+  });\n+  assert(loadTypes.size() > 0 && \"InsertSliceOp not found\");\n+  assert(loadTypes.size() == 1 &&\n+         \"Multiple kinds of load types are not supported\");\n+  return *loadTypes.begin();\n+}\n+\n+Value getMBarrierPhaseBit(OpBuilder &builder, Operation *op,\n+                          bool skipFirstWait) {\n+  // TODO: currently we only support one loop, no nested loop, while or\n+  // condition.\n+  auto loc = op->getLoc();\n+  auto forOp = op->getParentOfType<scf::ForOp>();\n+  if (!forOp) {\n+    return builder.create<arith::ConstantIntOp>(loc, skipFirstWait, 1);\n+  }\n+\n+  auto defOp = op->getOperand(0).getDefiningOp();\n+  assert(isa<ttng::CreateTokenOp>(defOp) &&\n+         \"mbarrier's definingOp is not createTokenOp\");\n+  ttng::CreateTokenOp createTokenOp = dyn_cast<ttng::CreateTokenOp>(defOp);\n+  Value numStage =\n+      builder.create<arith::ConstantIntOp>(loc, createTokenOp.getNum(), 32);\n+  Value curStep = forOp.getBody()->getArguments().back();\n+  if (curStep.getType() == builder.getIndexType()) {\n+    curStep =\n+        builder.create<arith::IndexCastOp>(loc, numStage.getType(), curStep);\n+  }\n+  Value curPhase = builder.create<arith::DivUIOp>(loc, curStep, numStage);\n+  if (skipFirstWait) {\n+    // If skipFirstWait, it waits for phaseBit 1\n+    Value _1 = builder.create<arith::ConstantIntOp>(loc, 1, 32);\n+    curPhase = builder.create<arith::AddIOp>(loc, curPhase, _1);\n+  }\n+  Value _2 = builder.create<arith::ConstantIntOp>(loc, 2, 32);\n+  // TODO: May use alternative methods of phaseBit calculation to avoid high\n+  // overhead of RemOp\n+  Value phaseBit = builder.create<arith::RemUIOp>(loc, curPhase, _2);\n+  Value _0 = builder.create<arith::ConstantIntOp>(loc, 0, 32);\n+  return builder.create<arith::CmpIOp>(loc, arith::CmpIPredicate::ne, phaseBit,\n+                                       _0);\n+}\n+\n+int getTxBytes(ttng::InsertSliceAsyncV2Op load) {\n+  // Both support ptr of tensor and tensor of ptr.\n+  RankedTensorType srcTensorType;\n+  if (auto srcType = dyn_cast<RankedTensorType>(load.getSrc().getType())) {\n+    srcTensorType = srcType;\n+  } else if (auto srcType =\n+                 dyn_cast<triton::PointerType>(load.getSrc().getType())) {\n+    srcTensorType = dyn_cast<RankedTensorType>(srcType.getPointeeType());\n+  } else {\n+    llvm_unreachable(\"Unexpected src type\");\n+  }\n+  auto shapePerCTA = ttg::getShapePerCTA(srcTensorType);\n+  auto elemTy =\n+      dyn_cast<RankedTensorType>(load.getDst().getType()).getElementType();\n+  int bytesPerElem = elemTy.getIntOrFloatBitWidth() / 8;\n+  return product<int64_t>(shapePerCTA) * bytesPerElem;\n+}\n+\n+int applyCommit(OpBuilder &builder, ttng::ProducerCommitOp &op,\n+                Value mbarrier) {\n+  // TODO: currently it only handles loads in ProducerCommitOp's nearest parent\n+  // block. Neither support multiple ProducerCommitOp, e.g. fused attention,\n+  // epilogue fusion.\n+  int txCnt = 0;\n+  SmallVector<Operation *> deprecatedOps;\n+  auto agentIds = getAgentIds(op);\n+  // Materialize InsertSliceOp\n+  for (auto &ItrOp : op->getBlock()->getOperations()) {\n+    // Check operations before ProducerCommitOp\n+    if (OperationEquivalence::isEquivalentTo(&ItrOp, op.getOperation(),\n+                                             OperationEquivalence::None)) {\n+      break;\n+    }\n+    if (auto insertOp = dyn_cast<ttg::InsertSliceOp>(ItrOp)) {\n+      deprecatedOps.push_back(&ItrOp);\n+      builder.setInsertionPoint(insertOp);\n+      if (!::mlir::triton::isTensorPointerType(insertOp.getSrc().getType())) {\n+        // Transform to InsertSliceAsyncOp\n+        auto newSliceOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n+            /*loc=*/insertOp.getLoc(), /*result=*/insertOp.getDst().getType(),\n+            /*src=*/insertOp.getSrc(), /*dst=*/insertOp.getDst(),\n+            /*index=*/insertOp.getIndex(),\n+            /*mask=*/insertOp.getMask(), /*other=*/insertOp.getOther(),\n+            /*cache=*/insertOp.getCache(), /*evict=*/insertOp.getEvict(),\n+            /*isVolatile=*/insertOp.getIsVolatile(),\n+            /*axis=*/insertOp.getAxis());\n+        insertOp.getResult().replaceAllUsesWith(newSliceOp.getResult());\n+        setAgentIds(newSliceOp, agentIds);\n+      } else {\n+        // Transform to InsertSliceAsyncV2Op\n+        auto extractBarrierOp = dyn_cast<ttng::ExtractMBarrierOp>(\n+            builder.clone(*(mbarrier.getDefiningOp())));\n+        auto newSliceOp = builder.create<ttng::InsertSliceAsyncV2Op>(\n+            /*loc=*/insertOp.getLoc(), /*result=*/insertOp.getDst().getType(),\n+            /*src=*/insertOp.getSrc(), /*dst=*/insertOp.getDst(),\n+            /*index=*/insertOp.getIndex(),\n+            /*mbar*/ extractBarrierOp.getResult(), /*mask=*/insertOp.getMask(),\n+            /*other=*/insertOp.getOther(),\n+            /*cache=*/insertOp.getCache(), /*evict=*/insertOp.getEvict(),\n+            /*isVolatile=*/insertOp.getIsVolatile(),\n+            /*axis=*/insertOp.getAxis());\n+        insertOp.getResult().replaceAllUsesWith(newSliceOp.getResult());\n+        setAgentIds(newSliceOp, agentIds);\n+        txCnt += getTxBytes(newSliceOp);\n+      }\n+    }\n+  }\n+  builder.setInsertionPoint(op);\n+  for (auto d : deprecatedOps) {\n+    d->erase();\n+  }\n+\n+  return txCnt;\n+}\n+\n+void processProducerAcquireOp(OpBuilder &builder, ttng::ProducerAcquireOp op,\n+                              Value bufferEmpty) {\n+  auto loc = op.getLoc();\n+  // The first producer_aquire should be met immediately, so initailly producer\n+  // skips the fisrt wait\n+  Value phase = getMBarrierPhaseBit(builder, op, 1);\n+  auto waitOp = builder.create<ttng::MBarrierWaitOp>(loc, bufferEmpty, phase);\n+  assert(op.getOperation()->hasAttr(\"async_agent\"));\n+  setAgentIds(waitOp, getAgentIds(op.getOperation()));\n+}\n+\n+void processProducerCommitOp(OpBuilder &builder, ttng::ProducerCommitOp op,\n+                             Value bufferFull, LoadType loadType) {\n+  auto loc = op.getLoc();\n+  int txCnt = applyCommit(builder, op, bufferFull);\n+  ttng::MBarrierArriveOp arriveOp;\n+\n+  if (loadType == LoadType::InsertSliceAsyncOp) {\n+    // Each thread arrives\n+    Value pred = builder.create<arith::ConstantIntOp>(loc, 1, 1);\n+    arriveOp = builder.create<ttng::MBarrierArriveOp>(\n+        loc, bufferFull, pred, /*remoteCTAId*/ nullptr, /*trackAsyncOp*/ true,\n+        txCnt);\n+  } else {\n+    // Only thread 0 arrives\n+    Value _0 = builder.create<arith::ConstantIntOp>(loc, 0, 32);\n+    Value threadId = getThreadId(builder, loc);\n+    Value pred = builder.create<arith::CmpIOp>(loc, arith::CmpIPredicate::eq,\n+                                               threadId, _0);\n+    arriveOp = builder.create<ttng::MBarrierArriveOp>(\n+        loc, bufferFull, pred, /*remoteCTAId*/ nullptr, /*trackAsyncOp*/ false,\n+        txCnt);\n+  }\n+\n+  assert(op.getOperation()->hasAttr(\"async_agent\"));\n+  setAgentIds(arriveOp, getAgentIds(op.getOperation()));\n+}\n+\n+void processConsumerWaitOp(OpBuilder &builder, ttng::ConsumerWaitOp op,\n+                           Value bufferFull) {\n+  auto loc = op.getLoc();\n+  Value phase = getMBarrierPhaseBit(builder, op, 0);\n+  auto waitOp = builder.create<ttng::MBarrierWaitOp>(loc, bufferFull, phase);\n+  assert(op.getOperation()->hasAttr(\"async_agent\"));\n+  setAgentIds(waitOp, getAgentIds(op.getOperation()));\n+}\n+\n+void processConsumerReleaseOp(OpBuilder &builder, ttng::ConsumerReleaseOp op,\n+                              Value bufferEmpty, int numCTAs) {\n+  auto loc = op.getLoc();\n+\n+  // Constants\n+  Value _0 = builder.create<arith::ConstantIntOp>(loc, 0, 32);\n+  Value _4 = builder.create<arith::ConstantIntOp>(loc, 4, 32);\n+  Value _8 = builder.create<arith::ConstantIntOp>(loc, 8, 32);\n+  Value _32 = builder.create<arith::ConstantIntOp>(loc, 32, 32);\n+  Value _128 = builder.create<arith::ConstantIntOp>(loc, 128, 32);\n+\n+  // threadId = threadId % 128\n+  Value threadId =\n+      builder.create<arith::RemUIOp>(loc, getThreadId(builder, loc), _128);\n+\n+  // k = threadId / 8\n+  Value k = builder.create<arith::DivUIOp>(loc, threadId, _8);\n+\n+  // row = k / 4\n+  Value row = builder.create<arith::DivUIOp>(loc, k, _4);\n+\n+  // col = k % 4\n+  Value col = builder.create<arith::RemUIOp>(loc, k, _4);\n+\n+  // remoteCTAId = (col ^ row) * 4 + col\n+  Value remoteCTAId = builder.create<arith::AddIOp>(\n+      loc,\n+      Value{builder.create<arith::MulIOp>(\n+          loc, Value{builder.create<arith::XOrIOp>(loc, col, row)}, _4)},\n+      col);\n+\n+  // pred0 = threadId % 8 == 0\n+  Value pred0 = builder.create<arith::CmpIOp>(\n+      loc, arith::CmpIPredicate::eq,\n+      builder.create<arith::RemUIOp>(loc, threadId, _8), _0);\n+\n+  // pred1 = remoteCTAId < numCTAs\n+  Value pred1 = builder.create<arith::CmpIOp>(\n+      loc, arith::CmpIPredicate::ult, remoteCTAId,\n+      builder.create<arith::ConstantIntOp>(loc, numCTAs, 32));\n+\n+  // pred = pred0 & pred1\n+  Value pred = builder.create<arith::AndIOp>(loc, pred0, pred1);\n+\n+  // bufferEmpty arrive\n+  auto arriveOp = builder.create<ttng::MBarrierArriveOp>(loc, bufferEmpty, pred,\n+                                                         remoteCTAId, false, 0);\n+\n+  assert(op.getOperation()->hasAttr(\"async_agent\"));\n+  setAgentIds(arriveOp, getAgentIds(op.getOperation()));\n+}\n+\n+void materializeTokenOperations(Operation *parentOp, int numCTAs) {\n+  SmallVector<Operation *> deprecatedOps;\n+  parentOp->walk([&](ttng::CreateTokenOp createTokenOp) {\n+    // Scan load type\n+    LoadType loadType = scanLoadTypes(createTokenOp);\n+\n+    // mBarrierTy\n+    MLIRContext *context = createTokenOp.getContext();\n+    auto i64Ty = IntegerType::get(context, 64);\n+    auto mBarrierTy = triton::PointerType::get(i64Ty, 3);\n+\n+    // mBarriersTy\n+    auto CTALayout = ttg::CTALayoutAttr::get(context, {1}, {1}, {0});\n+    auto sharedLayout =\n+        ttg::SharedEncodingAttr::get(context, 1, 1, 1, {0}, CTALayout, false);\n+    auto mBarriersTy =\n+        RankedTensorType::get({createTokenOp.getNum()}, i64Ty, sharedLayout);\n+\n+    // Process CreateTokenOp\n+    OpBuilder builder(createTokenOp);\n+    auto tokenLoc = createTokenOp.getLoc();\n+    unsigned bufferFullCount =\n+        loadType == LoadType::InsertSliceAsyncV2Op ? 1 : 128;\n+    Value bufferFullArray = builder.create<ttng::AllocMBarrierOp>(\n+        tokenLoc, mBarriersTy, bufferFullCount);\n+    Value bufferEmptyArray =\n+        builder.create<ttng::AllocMBarrierOp>(tokenLoc, mBarriersTy, numCTAs);\n+\n+    // Make sure that MBarriers are initialized in all CTAs\n+    if (numCTAs > 1) {\n+      builder.create<triton::nvidia_gpu::ClusterArriveOp>(tokenLoc, false);\n+      builder.create<triton::nvidia_gpu::ClusterWaitOp>(tokenLoc);\n+    }\n+\n+    // Helper function for extracting bufferFull\n+    auto extractBufferFull = [&](Location loc, Value idx) -> Value {\n+      return builder.create<ttng::ExtractMBarrierOp>(loc, mBarrierTy,\n+                                                     bufferFullArray, idx);\n+    };\n+\n+    // Helper function for extracting bufferEmpty\n+    auto extractBufferEmpty = [&](Location loc, Value idx) -> Value {\n+      return builder.create<ttng::ExtractMBarrierOp>(loc, mBarrierTy,\n+                                                     bufferEmptyArray, idx);\n+    };\n+\n+    // Process token users\n+    for (Operation *user : createTokenOp.getResult().getUsers()) {\n+      auto loc = user->getLoc();\n+      builder.setInsertionPoint(user);\n+      if (auto op = dyn_cast<ttng::ProducerAcquireOp>(user)) {\n+        Value bufferEmpty = extractBufferEmpty(loc, op.getIdx());\n+        assert(user->hasAttr(\"async_agent\"));\n+        setAgentIds(bufferEmpty.getDefiningOp(), getAgentIds(user));\n+        processProducerAcquireOp(builder, op, bufferEmpty);\n+      } else if (auto op = dyn_cast<ttng::ProducerCommitOp>(user)) {\n+        Value bufferFull = extractBufferFull(loc, op.getIdx());\n+        assert(user->hasAttr(\"async_agent\"));\n+        setAgentIds(bufferFull.getDefiningOp(), getAgentIds(user));\n+        processProducerCommitOp(builder, op, bufferFull, loadType);\n+      } else if (auto op = dyn_cast<ttng::ConsumerWaitOp>(user)) {\n+        Value bufferFull = extractBufferFull(loc, op.getIdx());\n+        assert(user->hasAttr(\"async_agent\"));\n+        setAgentIds(bufferFull.getDefiningOp(), getAgentIds(user));\n+        processConsumerWaitOp(builder, op, bufferFull);\n+      } else if (auto op = dyn_cast<ttng::ConsumerReleaseOp>(user)) {\n+        Value bufferEmpty = extractBufferEmpty(loc, op.getIdx());\n+        assert(user->hasAttr(\"async_agent\"));\n+        setAgentIds(bufferEmpty.getDefiningOp(), getAgentIds(user));\n+        processConsumerReleaseOp(builder, op, bufferEmpty, numCTAs);\n+      } else {\n+        llvm_unreachable(\"Unexpected user of token\");\n+      }\n+      deprecatedOps.push_back(user);\n+    }\n+\n+    deprecatedOps.push_back(createTokenOp);\n+  });\n+  for (auto op : deprecatedOps) {\n+    op->erase();\n+  }\n+\n+  // Insert a cluster barrier before the kernel exits. Without this barrier,\n+  // mbarrier_remote_arrive will fail if the remote CTA already exits.\n+  if (numCTAs > 1) {\n+    parentOp->walk([&](triton::FuncOp funcOp) {\n+      Block *block = &funcOp.getBody().front();\n+      auto returnOp = llvm::cast<triton::ReturnOp>(block->getTerminator());\n+      OpBuilder builder(returnOp);\n+      auto loc = returnOp.getLoc();\n+      builder.create<triton::nvidia_gpu::ClusterArriveOp>(loc, false);\n+      builder.create<triton::nvidia_gpu::ClusterWaitOp>(loc);\n+    });\n+  }\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// Materialize mutex operations\n+//===----------------------------------------------------------------------===//\n+\n+void mutexSyncPingPang(Operation *parentOp, int numAgents, int &nameBarrierId,\n+                       int &globalNumRoles) {\n+  // ping-pang mutex sync: using named barrier and only suitable for two roles.\n+  // Take mutex syncronization between dot and store as an example:\n+  // * For dot loop:\n+  //   * role 0 waits for named barrier 15 (loop enter), arrives named barrier\n+  //   14 (loop leave)\n+  //   * role 1 waits for named barrier 14 (loop enter), arrives named barrier\n+  //   15 (loop leave)\n+  // * For store:\n+  //   * role 0 waits for named barrier 13 (store enter), arrives named barrier\n+  //   12 (store leave)\n+  //   * role 1 waits for named barrier 12 (store enter), arrives named barrier\n+  //   13 (store leave)\n+  // As number of named barriers is limited (16), theoretically this mechanism\n+  // only support few roles and agents.\n+  int numRoles = 2, times = 0;\n+  globalNumRoles += numRoles;\n+  Value roleId;\n+  parentOp->walk([&](ttng::GetMutexRoleIdOp getMutexRoleIdOp) {\n+    // GetMutexRoleIdOp only occures once.\n+    assert(times == 0);\n+    OpBuilder builder(getMutexRoleIdOp);\n+    numRoles = getMutexRoleIdOp.getNum();\n+    auto loc = getMutexRoleIdOp->getLoc();\n+    Value threadId = getThreadId(builder, loc);\n+    assert(getMutexRoleIdOp->hasAttr(\"agent.num-warps\"));\n+    int numThreads =\n+        32 * getMutexRoleIdOp->getAttrOfType<IntegerAttr>(\"agent.num-warps\")\n+                 .getInt();\n+    int numThreadsBase =\n+        32 *\n+        getMutexRoleIdOp->getAttrOfType<IntegerAttr>(\"agent.num-warps-base\")\n+            .getInt();\n+    assert(numThreads % numRoles == 0);\n+    // TODO: more flexible ways to determine numWarps of each agent.\n+    Value numThreadsValue =\n+        builder.create<arith::ConstantIntOp>(loc, numThreads, 32);\n+    Value numRolesValue =\n+        builder.create<arith::ConstantIntOp>(loc, numRoles, 32);\n+    Value numThreadsBaseValue =\n+        builder.create<arith::ConstantIntOp>(loc, numThreadsBase, 32);\n+    Value numThreadsPerRole =\n+        builder.create<arith::DivUIOp>(loc, numThreadsValue, numRolesValue);\n+    Value numRemThreads =\n+        builder.create<arith::SubIOp>(loc, threadId, numThreadsBaseValue);\n+    roleId =\n+        builder.create<arith::DivUIOp>(loc, numRemThreads, numThreadsPerRole);\n+    getMutexRoleIdOp.getResult().replaceAllUsesWith(roleId);\n+    getMutexRoleIdOp->erase();\n+    times++;\n+  });\n+\n+  parentOp->walk<WalkOrder::PreOrder>([&](ttng::CreateMutexOp createMutexOp) {\n+    // Currently, inner-agent sync counts from barId 1 (see membar.cpp, bar 0\n+    // is used for whole block sync).\n+    // We need to guarantee mutex sync won't use bars of inner-agent sync.\n+    assert(nameBarrierId > globalNumRoles);\n+    // Process CreateMutexOp\n+    OpBuilder builder(createMutexOp);\n+    // TODO: change the hard code of numThreads\n+    auto loc = createMutexOp->getLoc();\n+    Value numThreads = builder.create<arith::ConstantIntOp>(loc, 256, 32);\n+    Value _0 = builder.create<arith::ConstantIntOp>(loc, 0, 32);\n+    Value isRole0 = builder.create<arith::CmpIOp>(loc, arith::CmpIPredicate::eq,\n+                                                  roleId, _0);\n+    assert(nameBarrierId < nameBarrierIdEnd &&\n+           nameBarrierId - 1 >= nameBarrierIdBegin);\n+    Value namedBarrierId0 =\n+        builder.create<arith::ConstantIntOp>(loc, nameBarrierId, 32);\n+    Value namedBarrierId1 =\n+        builder.create<arith::ConstantIntOp>(loc, nameBarrierId - 1, 32);\n+    // Process mutex users\n+    int numUsers = 0;\n+    for (Operation *user : createMutexOp.getResult().getUsers()) {\n+      numUsers++;\n+      assert(numUsers <= 2);\n+      auto loc = user->getLoc();\n+      builder.setInsertionPoint(user);\n+      if (auto op = dyn_cast<ttng::LockOp>(user)) {\n+        Value barEnter = builder.create<arith::SelectOp>(\n+            loc, isRole0, namedBarrierId0, namedBarrierId1);\n+        builder.create<ttng::NamedBarrierWaitOp>(loc, barEnter, numThreads);\n+      } else if (auto op = dyn_cast<ttng::UnlockOp>(user)) {\n+        Value barLeave = builder.create<arith::SelectOp>(\n+            loc, isRole0, namedBarrierId1, namedBarrierId0);\n+        builder.create<ttng::NamedBarrierArriveOp>(loc, barLeave, numThreads);\n+      } else\n+        llvm_unreachable(\"Unexpected user of mutex\");\n+      user->erase();\n+    }\n+    nameBarrierId -= 2;\n+    nameBarrierIdEnd -= 2;\n+    createMutexOp.erase();\n+  });\n+}\n+\n+void processLockOp(OpBuilder &builder, ttng::LockOp op) {\n+  auto loc = op.getLoc();\n+  assert(op->hasAttr(\"mutex.barId\") && op->hasAttr(\"mutex.numThreads\"));\n+  auto barIds = getMutexBarIds(op);\n+  auto threads = getMutexNumThreads(op);\n+  assert(barIds.size() > 0 && barIds.size() == threads.size());\n+  for (int i = 0; i < barIds.size(); ++i) {\n+    Value numThreads =\n+        builder.create<arith::ConstantIntOp>(loc, threads[i], 32);\n+    Value barrier = builder.create<arith::ConstantIntOp>(loc, barIds[i], 32);\n+    builder.create<ttng::NamedBarrierWaitOp>(loc, barrier, numThreads);\n+  }\n+}\n+\n+void processUnlockOp(OpBuilder &builder, ttng::UnlockOp op) {\n+  auto loc = op.getLoc();\n+  assert(op->hasAttr(\"mutex.barId\") && op->hasAttr(\"mutex.numThreads\"));\n+  auto barIds = getMutexBarIds(op);\n+  auto threads = getMutexNumThreads(op);\n+  assert(barIds.size() > 0 && barIds.size() == threads.size());\n+  for (int i = 0; i < barIds.size(); ++i) {\n+    Value numThreads =\n+        builder.create<arith::ConstantIntOp>(loc, threads[i], 32);\n+    Value barrier = builder.create<arith::ConstantIntOp>(loc, barIds[i], 32);\n+    builder.create<ttng::NamedBarrierArriveOp>(loc, barrier, numThreads);\n+  }\n+}\n+\n+void materializeMutexOperationsOthers(ModuleOp parentOp) {\n+  parentOp->walk([](ttng::CreateMutexOp createMutexOp) {\n+    // Process CreateMutexOp\n+    OpBuilder builder(createMutexOp);\n+\n+    // Process mutex users\n+    for (Operation *user : createMutexOp.getResult().getUsers()) {\n+      auto loc = user->getLoc();\n+      builder.setInsertionPoint(user);\n+      if (auto op = dyn_cast<ttng::LockOp>(user))\n+        processLockOp(builder, op);\n+      else if (auto op = dyn_cast<ttng::UnlockOp>(user))\n+        processUnlockOp(builder, op);\n+      else\n+        llvm_unreachable(\"Unexpected user of mutex\");\n+      user->erase();\n+    }\n+\n+    createMutexOp.erase();\n+  });\n+}\n+\n+void materializeMutexOperations(ModuleOp parentOp) {\n+  nameBarrierIdEnd = 16;\n+  int nameBarrierId = 15;\n+  int globalNumRoles = 0;\n+  // Materialize mutex operations from WSMutex, i.e. auto-mutex\n+  parentOp->walk([&](scf::IfOp IfOp) {\n+    int numRoles = 0;\n+    if (IfOp->hasAttr(\"agent.num-roles\")) {\n+      assert(parentOp->hasAttr(\"async.num-agents\"));\n+      int numAgents =\n+          parentOp->getAttrOfType<IntegerAttr>(\"async.num-agents\").getInt();\n+      numRoles = IfOp->getAttrOfType<IntegerAttr>(\"agent.num-roles\").getInt();\n+      // TODO: To support arbitrary number of roles, use mbarrier.\n+      assert(numRoles == 2);\n+      mutexSyncPingPang(IfOp, numAgents, nameBarrierId, globalNumRoles);\n+    }\n+  });\n+  // Materialize mutex operations for remaining cases.\n+  // User needs to guarantee correctness of synchronization.\n+  materializeMutexOperationsOthers(parentOp);\n+}\n+\n+// TODO: may also not support 8-warp kernel.\n+void tryRegisterRealloc(ModuleOp mod) {\n+  constexpr int LoadRegisterRequirement = 40;\n+  constexpr int MmaRegisterRequirement = 232;\n+  OpBuilderWithAgentIds builder(mod.getContext());\n+\n+  auto isLoadAgent = [](scf::IfOp ifOp) -> bool {\n+    return ifOp\n+        ->walk([](Operation *op) {\n+          if (isa<ttg::InsertSliceOp, ttg::InsertSliceAsyncOp,\n+                  ttng::InsertSliceAsyncV2Op>(op))\n+            return WalkResult::interrupt();\n+          return WalkResult::advance();\n+        })\n+        .wasInterrupted();\n+  };\n+\n+  auto isMmaAgent = [](scf::IfOp ifOp) -> bool {\n+    return ifOp\n+        ->walk([](Operation *op) {\n+          if (isa<triton::DotOp, ttng::DotAsyncOp>(op))\n+            return WalkResult::interrupt();\n+          return WalkResult::advance();\n+        })\n+        .wasInterrupted();\n+  };\n+\n+  // TODO: we need to make agent info more handy\n+  SmallVector<scf::IfOp> agentOps;\n+  mod->walk([&agentOps](triton::FuncOp funcOp) {\n+    Block *block = &funcOp.getBody().front();\n+    for (Operation &op : block->getOperations()) {\n+      if (auto ifOp = dyn_cast<scf::IfOp>(&op)) {\n+        if (getAgentIds(ifOp).size() == 1) {\n+          agentOps.push_back(ifOp);\n+        }\n+      }\n+    }\n+  });\n+  for (auto ifOp : agentOps) {\n+    builder.setInsertionPointToStart(&(ifOp.getThenRegion().front()));\n+    builder.setAgentIdsFromOp(ifOp);\n+    auto loc = ifOp.getLoc();\n+    Type i32_ty = builder.getIntegerType(32);\n+    // If an agent has both mma and load, do nothing.\n+    if (isMmaAgent(ifOp) && isLoadAgent(ifOp))\n+      continue;\n+    if (isMmaAgent(ifOp)) {\n+      builder.createWithAgentIds<ttng::RegAllocOp>(\n+          loc, builder.getIntegerAttr(i32_ty, MmaRegisterRequirement));\n+    } else if (isLoadAgent(ifOp)) {\n+      builder.createWithAgentIds<ttng::RegDeallocOp>(\n+          loc, builder.getIntegerAttr(i32_ty, LoadRegisterRequirement));\n+    }\n+  }\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// WSMaterializationPass\n+//===----------------------------------------------------------------------===//\n+\n+struct WSMaterializationPass\n+    : public TritonGPUWSMaterializationBase<WSMaterializationPass> {\n+  WSMaterializationPass() = default;\n+  WSMaterializationPass(int computeCapability) {\n+    this->computeCapability = computeCapability;\n+  }\n+\n+  void runOnOperation() override {\n+    ModuleOp mod = getOperation();\n+    if (!ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod))\n+      return signalPassFailure();\n+\n+    if (computeCapability / 10 < 9) {\n+      llvm_unreachable(\"WSMaterialization pass only supports sm_9x as of now.\");\n+      signalPassFailure();\n+    }\n+\n+    int numCTAs = ttg::TritonGPUDialect::getNumCTAs(mod);\n+\n+    materializeGetAgentIdOp(mod);\n+    materializeTokenOperations(mod, numCTAs);\n+    materializeMutexOperations(mod);\n+    tryRegisterRealloc(mod);\n+\n+    mod->walk([](Operation *op) {\n+      bool hasTensor = 0;\n+      auto results = op->getResults();\n+      auto operands = op->getOperands();\n+      for (auto i : results) {\n+        if (isa<RankedTensorType>(i.getType())) {\n+          hasTensor = 1;\n+          break;\n+        }\n+      }\n+      if (!hasTensor) {\n+        for (auto i : operands) {\n+          if (isa<RankedTensorType>(i.getType())) {\n+            hasTensor = 1;\n+            break;\n+          }\n+        }\n+      }\n+\n+      if (!hasTensor && !isa<ttng::MBarrierWaitOp>(op) &&\n+          !isa<ttng::ExtractMBarrierOp>(op) &&\n+          !isa<ttng::MBarrierArriveOp>(op)) {\n+        op->removeAttr(\"async_agent\");\n+      }\n+    });\n+\n+    // TODO: More flexible way to set num-warps\n+    // One dma, one math warp group, set num-warps = 8\n+    auto i32_ty = IntegerType::get(mod->getContext(), 32);\n+    mod->setAttr(\"triton_gpu.num-warps\",\n+                 IntegerAttr::get(i32_ty, llvm::APInt(32, 8)));\n+\n+    WalkResult result = mod->walk([&](scf::IfOp ifOp) {\n+      if (ifOp->hasAttr(\"agent.num-roles\")) {\n+        return WalkResult::interrupt();\n+      }\n+      return WalkResult::advance();\n+    });\n+    if (result.wasInterrupted()) {\n+      mod->setAttr(\"triton_gpu.num-warps\",\n+                   IntegerAttr::get(i32_ty, llvm::APInt(32, 12)));\n+    }\n+    mod->removeAttr(\"async.num-agents\");\n+  }\n+};\n+\n+} // namespace\n+\n+//===----------------------------------------------------------------------===//\n+// createTritonNvidiaGPUWSMaterializationPass\n+//===----------------------------------------------------------------------===//\n+\n+std::unique_ptr<Pass>\n+mlir::createTritonNvidiaGPUWSMaterializationPass(int computeCapability) {\n+  return std::make_unique<WSMaterializationPass>(computeCapability);\n+}"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMutex.cpp", "status": "added", "additions": 315, "deletions": 0, "changes": 315, "file_content_changes": "@@ -0,0 +1,315 @@\n+#include <algorithm>\n+\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n+\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+\n+using namespace mlir;\n+namespace ttg = triton::gpu;\n+namespace ttng = triton::nvidia_gpu;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+namespace {\n+\n+// Target operations: dot, load, store. Add more when necessary.\n+#define KEY_TYPES triton::DotOp, ttg::InsertSliceOp, triton::StoreOp\n+\n+template <typename Head, typename... Tails>\n+void getKeyTypeId(Operation *op, int &id, bool &found) {\n+  if (isa<Head>(op))\n+    found = true;\n+\n+  if (!found) {\n+    id++;\n+    if constexpr (sizeof...(Tails) > 0)\n+      getKeyTypeId<Tails...>(op, id, found);\n+  }\n+}\n+\n+template <typename... T> int getKeyTypeIdWrapper(Operation *op) {\n+  bool found = false;\n+  int id = 0;\n+  getKeyTypeId<T...>(op, id, found);\n+  return found ? id : -1;\n+}\n+\n+bool isEligible(Operation *agent,\n+                DenseMap<int, DenseSet<Operation *>> &keyTypeOpMap,\n+                scf::ForOp &persistentForOp) {\n+  // metrics:\n+  //   1. Have more than one key type of operation.\n+  //   2. persistent (all key operations are in one forOp)\n+  DenseSet<int> keyTypes;\n+  DenseSet<Operation *> keyOperations;\n+  agent->walk([&](Operation *op) {\n+    auto typeId = getKeyTypeIdWrapper<KEY_TYPES>(op);\n+    if (typeId >= 0 && op != agent) {\n+      keyTypes.insert(typeId);\n+      keyOperations.insert(op);\n+      keyTypeOpMap[typeId].insert(op);\n+    }\n+  });\n+\n+  if (keyTypes.size() <= 1) {\n+    return false;\n+  }\n+\n+  auto getPersistentFor = [&](DenseSet<Operation *> keyOps,\n+                              scf::ForOp &innerMostForOp) -> bool {\n+    DenseSet<scf::ForOp> commonForOps0, commonForOps1;\n+    DenseSet<scf::ForOp> *commonForOpsPre = &commonForOps0,\n+                         *commonForOpsPro = &commonForOps1;\n+    assert(keyOps.size() > 1);\n+    SmallVector<scf::ForOp> forOps;\n+    agent->walk<WalkOrder::PreOrder>(\n+        [&](scf::ForOp forOp) { forOps.push_back(forOp); });\n+\n+    bool hasCommon = false;\n+    for (auto &f : forOps) {\n+      bool isCommon = true;\n+      for (auto &k : keyOps) {\n+        if (!f->isAncestor(k)) {\n+          isCommon = false;\n+          break;\n+        }\n+      }\n+      if (isCommon) {\n+        innerMostForOp = f;\n+        hasCommon = true;\n+      }\n+    }\n+    return hasCommon;\n+  };\n+\n+  // Persistent agents with more than one key types are eligible.\n+  return getPersistentFor(keyOperations, persistentForOp);\n+}\n+\n+void mutexSync(ModuleOp &mod, scf::IfOp &ifOp, scf::ForOp &persistentForOp,\n+               DenseMap<int, DenseSet<Operation *>> &keyTypeOpMap) {\n+  // Modify keyTypeOpMap: DenseMap<int, DenseSet<Operation *>> --> DenseMap<int,\n+  // Operation *>. Conservetively, assign each key operation one mutex.\n+  // =======================detail description (TODO: to be\n+  // deleted)========================== because it's hard to check if two\n+  // operations with same typeid can share same mutex, we assign each key\n+  // operation one mutex. To illustrate the hardness of this analysis, say we\n+  // have two operations with same typeid: a and b, if there is another\n+  // operation (say c) of different typeid between a and b, and their locations\n+  // are a -- c -- b, then if the dependency is:\n+  //   * b depends on c, then a and b can NOT share the same mutex.\n+  //   * otherwise, a and b can share after move b before c.\n+  // It would be more complicated when there are more types and operations.\n+  DenseMap<int, Operation *> ProxyKeyTypeOpMap;\n+  for (auto &[id, ops] : keyTypeOpMap) {\n+    for (auto itr = ops.begin(); itr != ops.end(); ++itr) {\n+      auto op = *itr;\n+      ProxyKeyTypeOpMap[ProxyKeyTypeOpMap.size()] = op;\n+    }\n+  }\n+\n+  int numRoles = ProxyKeyTypeOpMap.size();\n+  auto loc = ifOp.getLoc();\n+  OpBuilderWithAgentIds builder(ifOp.getContext());\n+  // Set num-roles for wsmaterialization pass\n+  ifOp->setAttr(\"agent.num-roles\", builder.getI32IntegerAttr(numRoles));\n+  builder.setAgentIdsFromOp(ifOp);\n+  builder.setInsertionPointToStart(&(ifOp.getThenRegion().front()));\n+  Value _0 = builder.create<arith::ConstantIntOp>(loc, 0, 32);\n+  Value curRoleId =\n+      builder.createWithAgentIds<ttng::GetMutexRoleIdOp>(loc, numRoles);\n+  Value isNotRole0 = builder.create<arith::CmpIOp>(\n+      loc, arith::CmpIPredicate::ne, curRoleId, _0);\n+\n+  SmallVector<Value> mutexBarriers;\n+  for (int i = 0; i < numRoles; ++i) {\n+    auto v = builder.createWithAgentIds<ttng::CreateMutexOp>(loc);\n+    mutexBarriers.push_back(v);\n+  }\n+\n+  // Update lower bound, step and pipelineIdx of persistentForOp\n+  builder.setInsertionPoint(persistentForOp);\n+  Value start = builder.createWithAgentIds<arith::MulIOp>(\n+      loc, persistentForOp.getStep(), curRoleId);\n+  Value oldLB = persistentForOp.getLowerBound();\n+  Value pipelineIdx =\n+      persistentForOp->getOperand(persistentForOp->getNumOperands() - 1);\n+\n+  start = builder.createWithAgentIds<arith::AddIOp>(loc, oldLB, start);\n+  persistentForOp.setLowerBound(start);\n+\n+  Value numRolesValue =\n+      builder.createWithAgentIds<arith::ConstantIntOp>(loc, numRoles, 32);\n+  Value step = builder.createWithAgentIds<arith::MulIOp>(\n+      loc, persistentForOp.getStep(), numRolesValue);\n+  persistentForOp.setStep(step);\n+\n+  Value newIdx =\n+      builder.createWithAgentIds<arith::AddIOp>(loc, pipelineIdx, curRoleId);\n+  persistentForOp.setIterArg(persistentForOp.getNumIterOperands() - 1, newIdx);\n+  auto yield =\n+      llvm::cast<scf::YieldOp>(persistentForOp.getBody()->getTerminator());\n+  auto idxPlusOneOp =\n+      yield->getOperand(yield->getNumOperands() - 1).getDefiningOp();\n+  assert(isa<arith::AddIOp>(idxPlusOneOp));\n+  assert(idxPlusOneOp->getOperand(0) ==\n+         persistentForOp.getBody()->getArgument(\n+             persistentForOp.getBody()->getNumArguments() - 1));\n+  idxPlusOneOp->setOperand(1, numRolesValue);\n+\n+  // Add operations at the start of persistentForOp\n+  builder.setInsertionPointToStart(persistentForOp.getBody());\n+  // If( role != 0 || !is_first_tile )\n+  Value isNotTileId0 = builder.create<arith::CmpIOp>(\n+      loc, arith::CmpIPredicate::ne, persistentForOp.getBody()->getArgument(0),\n+      oldLB);\n+  Value cond = builder.create<arith::OrIOp>(loc, isNotTileId0, isNotRole0);\n+\n+  // Determine boundaries: get the largest exclusive op for each key op.\n+  DenseMap<int, Operation *> lockLocs, unlockLocs;\n+  DenseMap<int, SmallVector<Operation *>> parentOps;\n+  for (int i = 0; i < numRoles; ++i) {\n+    auto op = ProxyKeyTypeOpMap[i]->getParentOp();\n+    while (op != persistentForOp->getParentOp()) {\n+      parentOps[i].push_back(op);\n+      op = op->getParentOp();\n+    }\n+  }\n+\n+  std::map<std::pair<int, int>, std::pair<SmallVector<Operation *>::iterator,\n+                                          SmallVector<Operation *>::iterator>>\n+      rangeMap;\n+  for (auto &[i, opsI] : parentOps) {\n+    // Check exlusiveness\n+    auto op = ProxyKeyTypeOpMap[i];\n+    for (auto &[j, opsJ] : parentOps) {\n+      if (i == j)\n+        continue;\n+      auto pair = std::pair<int, int>(i, j);\n+      auto pairConj = std::pair<int, int>(j, i);\n+      auto end0 = rangeMap.count(pair) ? rangeMap[pair].first : opsI.end();\n+      auto end1 = rangeMap.count(pair) ? rangeMap[pair].second : opsJ.end();\n+      for (auto m = opsI.begin(); m != end0; ++m) {\n+        auto itr = std::find(opsJ.begin(), end1, *m);\n+        if (itr == end1) {\n+          op = *m;\n+          rangeMap[pair] = std::make_pair(m, itr);\n+          rangeMap[pairConj] = rangeMap[pair];\n+        } else\n+          goto exit;\n+      }\n+    }\n+  exit:;\n+    lockLocs[i] = op;\n+    unlockLocs[i] = op;\n+  }\n+\n+  // Only cases where all lock/unlock locations are in same level make sense.\n+  for (int i = 1; i < numRoles; ++i) {\n+    if (lockLocs[i]->getParentOp() != lockLocs[i - 1]->getParentOp() ||\n+        unlockLocs[i]->getParentOp() != unlockLocs[i - 1]->getParentOp()) {\n+      llvm_unreachable(\"Only cases where all locl/unlock locations are in same \"\n+                       \"level make sense\");\n+    }\n+  }\n+\n+  // Extend boundaries: wait and release as early as possible\n+  DenseMap<int, int> prevTypeIds;\n+  int prevId = -1;\n+  persistentForOp->walk<WalkOrder::PreOrder>([&](Operation *op) {\n+    for (int i = 0; i < numRoles; ++i) {\n+      if (lockLocs[i] == op) {\n+        prevTypeIds[i] = prevId;\n+        prevId = i;\n+        break;\n+      }\n+    }\n+  });\n+\n+  // Update lockLocs\n+  for (int i = 0; i < numRoles; ++i) {\n+    if (prevTypeIds[i] == -1)\n+      lockLocs[i] = cond.getDefiningOp();\n+    else\n+      lockLocs[i] = unlockLocs[prevTypeIds[i]];\n+  }\n+  // lock\n+  for (int i = 0; i < numRoles; ++i) {\n+    builder.setInsertionPointAfter(lockLocs[i]);\n+    auto waitIfOp = builder.create<scf::IfOp>(loc, cond);\n+    builder.setInsertionPointToStart(&(waitIfOp.getThenRegion().front()));\n+    builder.create<ttng::LockOp>(loc, mutexBarriers[i]);\n+  }\n+\n+  // unlock\n+  for (int i = 0; i < numRoles; ++i) {\n+    builder.setInsertionPointAfter(unlockLocs[i]);\n+    builder.create<ttng::UnlockOp>(loc, mutexBarriers[i]);\n+  }\n+\n+  // Add attr \"agent.mutex_role\" for barrier analysis\n+  int roleId = -1;\n+  for (Operation &bodyOp : lockLocs[0]->getBlock()->getOperations()) {\n+    Operation *op = &bodyOp;\n+    if (roleId != -1)\n+      op->walk([&](Operation *subOp) {\n+        if (!isa<scf::YieldOp>(op) && !isa<ttng::LockOp>(op) &&\n+            !isa<ttng::UnlockOp>(op))\n+          subOp->setAttr(\"agent.mutex_role\", builder.getI32IntegerAttr(roleId));\n+      });\n+    for (int i = 0; i < numRoles; ++i) {\n+      if (lockLocs[i] == op) {\n+        roleId = i;\n+        op->setAttr(\"agent.mutex_role\", builder.getI32IntegerAttr(i));\n+        break;\n+      }\n+    }\n+  }\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// WSMaterializationPass\n+//===----------------------------------------------------------------------===//\n+\n+struct WSMutexPass : public TritonGPUWSMutexBase<WSMutexPass> {\n+public:\n+  WSMutexPass() = default;\n+  WSMutexPass(int computeCapability) {\n+    this->computeCapability = computeCapability;\n+  }\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp mod = getOperation();\n+    mod.walk([&](triton::FuncOp funcOp) {\n+      for (Operation &bodyOp : funcOp.getBody().front().getOperations()) {\n+        Operation *op = &bodyOp;\n+        scf::ForOp persistentForOp;\n+        // premise: agent region is encapsulated with scf.if\n+        if (isa<scf::IfOp>(op) && getAgentIds(op).size() == 1) {\n+          DenseMap<int, DenseSet<Operation *>> keyTypeOpMap;\n+          if (isEligible(op, keyTypeOpMap, persistentForOp)) {\n+            auto ifOp = cast<scf::IfOp>(op);\n+            mutexSync(mod, ifOp, persistentForOp, keyTypeOpMap);\n+          }\n+        }\n+      }\n+    });\n+  }\n+};\n+\n+} // namespace\n+\n+//===----------------------------------------------------------------------===//\n+// createTritonNvidiaGPUWSMutexPass\n+//===----------------------------------------------------------------------===//\n+\n+std::unique_ptr<Pass>\n+mlir::createTritonNvidiaGPUWSMutexPass(int computeCapability) {\n+  return std::make_unique<WSMutexPass>(computeCapability);\n+}"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSPipeline.cpp", "status": "added", "additions": 953, "deletions": 0, "changes": 953, "file_content_changes": "@@ -0,0 +1,953 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n+\n+#include \"mlir/Analysis/SliceAnalysis.h\"\n+\n+#include <algorithm>\n+#include <unordered_set>\n+\n+using namespace mlir;\n+namespace ttg = triton::gpu;\n+namespace ttng = triton::nvidia_gpu;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+namespace {\n+struct Channel {\n+public:\n+  using Relation = std::pair<int, int>;\n+\n+  Channel(int producer, int consumer, Operation *src, Operation *dst)\n+      : relation(producer, consumer), srcOp(src), dstOp(dst) {}\n+\n+  bool operator==(const Channel &c) {\n+    return relation == c.relation && srcOp == c.srcOp && dstOp == c.dstOp;\n+  }\n+\n+  Relation relation;\n+  Operation *srcOp;\n+  Operation *dstOp;\n+};\n+\n+//===----------------------------------------------------------------------===//\n+// createToken\n+//===----------------------------------------------------------------------===//\n+\n+DenseMap<Channel *, Value>\n+createToken(const DenseMap<Operation *, SmallVector<Channel *>> &map,\n+            triton::FuncOp funcOp, int numStages) {\n+  DenseMap<Channel *, Value> ret;\n+  OpBuilder builder(funcOp);\n+  builder.setInsertionPointToStart(&(funcOp.getBody().front()));\n+  for (auto it = map.begin(); it != map.end(); ++it) {\n+    Value v;\n+    if (it->second.front()->srcOp->getParentOfType<scf::ForOp>()) {\n+      v = builder.create<ttng::CreateTokenOp>(funcOp.getLoc(), numStages);\n+    } else {\n+      // No need to pipeline\n+      v = builder.create<ttng::CreateTokenOp>(funcOp.getLoc(), 1);\n+    }\n+    for (auto &c : it->second) {\n+      ret[c] = v;\n+    }\n+  }\n+  return ret;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// createBuffer\n+//===----------------------------------------------------------------------===//\n+\n+DenseMap<Channel *, Value> createBuffer(const SmallVector<Channel *> &channels,\n+                                        triton::FuncOp funcOp, int numStages) {\n+  DenseMap<Channel *, Value> bufferMap;\n+  MLIRContext *context = funcOp.getContext();\n+  OpBuilder builder(funcOp);\n+  builder.setInsertionPointToStart(&(funcOp.getBody().front()));\n+  for (const auto &c : channels) {\n+    auto loadOp = dyn_cast<triton::LoadOp>(c->srcOp);\n+    Value loadResult = loadOp.getResult();\n+    if (auto tensorType = loadResult.getType().dyn_cast<RankedTensorType>()) {\n+      // Get basic information from tensorType\n+      auto order = ttg::getOrder(tensorType.getEncoding());\n+      auto CTALayout = ttg::getCTALayout(tensorType.getEncoding());\n+      auto elemType = tensorType.getElementType();\n+\n+      // Get shape, layout and type of a slice\n+      auto sliceShape = tensorType.getShape();\n+      auto sharedLayout = ttg::SharedEncodingAttr::get(\n+          context, sliceShape, order, CTALayout, elemType);\n+      auto sliceType =\n+          RankedTensorType::get(sliceShape, elemType, sharedLayout);\n+\n+      // Get shape, layout and type of the complete buffer\n+      SmallVector<int64_t> bufferShape(sliceShape.begin(), sliceShape.end());\n+      if (loadOp->getParentOfType<scf::ForOp>()) {\n+        bufferShape.insert(bufferShape.begin(), numStages);\n+      } else {\n+        // No need to pipeline\n+        bufferShape.insert(bufferShape.begin(), 1);\n+      }\n+      auto bufferType =\n+          RankedTensorType::get(bufferShape, elemType, sharedLayout);\n+      Value buffer =\n+          builder.create<ttg::AllocTensorOp>(funcOp.getLoc(), bufferType);\n+      bufferMap[c] = buffer;\n+    } else {\n+      llvm_unreachable(\"Unexpected result type\");\n+    }\n+  }\n+  return bufferMap;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// appendPipelineIdxToLoopArgs\n+//===----------------------------------------------------------------------===//\n+\n+scf::ForOp appendPipelineIdxToLoopArgs(scf::ForOp forOp, int numStages,\n+                                       scf::ForOp &parentForOp) {\n+  auto loc = forOp.getLoc();\n+  Block *body = forOp.getBody();\n+\n+  // The agentId set of pipelineIdx is the union of agentId sets of all ops in\n+  // the for loop\n+  OpBuilderWithAgentIds builder(forOp.getContext());\n+  builder.setAgentIdsFromArray(collectAgentIds(forOp));\n+\n+  builder.setInsertionPoint(forOp);\n+  Value numStagesVal =\n+      builder.createWithAgentIds<arith::ConstantIntOp>(loc, numStages, 32);\n+  // Append pipelineIdx to block arguments\n+  Value pipelineIdx =\n+      body->insertArgument(body->getNumArguments(), builder.getI32Type(), loc);\n+\n+  // pipelineIdx = (pipelineIdx + 1) % numStages\n+  auto yieldOp = llvm::cast<scf::YieldOp>(body->getTerminator());\n+  builder.setInsertionPoint(yieldOp);\n+  Value one = builder.createWithAgentIds<arith::ConstantIntOp>(loc, 1, 32);\n+\n+  Value pipelineIdxPlusOne =\n+      builder.createWithAgentIds<arith::AddIOp>(loc, pipelineIdx, one);\n+\n+  // Append pipelineIdx to yield operands\n+  yieldOp->insertOperands(yieldOp.getNumOperands(), {pipelineIdxPlusOne});\n+\n+  // Copy iter operands of forOp\n+  SmallVector<Value> newLoopArgs;\n+  for (auto operand : forOp.getIterOperands())\n+    newLoopArgs.push_back(operand);\n+\n+  // Append initial value of pipelineIdx to newLoopArgs\n+  builder.setInsertionPoint(forOp);\n+  Value initValue;\n+  if (parentForOp) {\n+    // Make sure prior pipelineIdx is inserted in the end of parentForOp\n+    initValue = parentForOp.getBody()->getArguments().back();\n+    Value numSteps = builder.createWithAgentIds<arith::SubIOp>(\n+        loc, forOp.getUpperBound(), forOp.getLowerBound());\n+    numSteps = builder.createWithAgentIds<arith::DivUIOp>(loc, numSteps,\n+                                                          forOp.getStep());\n+    initValue =\n+        builder.createWithAgentIds<arith::MulIOp>(loc, initValue, numSteps);\n+  } else {\n+    initValue = builder.createWithAgentIds<arith::ConstantIntOp>(loc, 0, 32);\n+  }\n+  newLoopArgs.push_back(initValue);\n+\n+  // Create newForOp and take the region of forOp\n+  auto newForOp = builder.createWithAgentIds<scf::ForOp>(\n+      loc, forOp.getLowerBound(), forOp.getUpperBound(), forOp.getStep(),\n+      newLoopArgs);\n+  newForOp.getRegion().takeBody(forOp.getRegion());\n+\n+  // Replace forOp with newForOp\n+  for (unsigned i = 0; i < forOp.getNumResults(); ++i)\n+    forOp.getResult(i).replaceAllUsesWith(newForOp.getResult(i));\n+  forOp.erase();\n+\n+  return newForOp;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// appendPipelineIdxArgs\n+//===----------------------------------------------------------------------===//\n+\n+void appendPipelineIdxArgs(SmallVector<Operation *> &backbone, int numStages) {\n+\n+  SmallVector<scf::ForOp> orderedForOps;\n+  for (auto &op : backbone) {\n+    op->walk<WalkOrder::PreOrder>([&](Operation *subOp) {\n+      if (auto forOp = dyn_cast<scf::ForOp>(subOp)) {\n+        orderedForOps.push_back(forOp);\n+      }\n+    });\n+  }\n+\n+  for (auto &op : orderedForOps) {\n+    scf::ForOp parentForOp = op->getParentOfType<scf::ForOp>();\n+    auto newForOp = appendPipelineIdxToLoopArgs(op, numStages, parentForOp);\n+    auto backboneForItr =\n+        std::find(backbone.begin(), backbone.end(), op.getOperation());\n+    if (backboneForItr != backbone.end()) {\n+      // Update backbone\n+      *backboneForItr = newForOp.getOperation();\n+    }\n+  }\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// checkDependencyAndCollectUsedArgs\n+//===----------------------------------------------------------------------===//\n+\n+SmallVector<unsigned> checkDependencyAndCollectUsedArgs(\n+    scf::ForOp forOp, AgentId agentId,\n+    DenseMap<BlockArgument, Value> &blockArgToYieldOperand) {\n+\n+  std::unordered_set<Operation *> visited;\n+  SetVector<unsigned> argSet;\n+\n+  // DFS\n+  std::function<void(Operation *)> dfs = [&](Operation *op) {\n+    if (visited.find(op) != visited.end())\n+      return;\n+    visited.insert(op);\n+    for (Value operand : op->getOperands()) {\n+      if (auto blockArg = operand.dyn_cast<BlockArgument>()) {\n+        if (!blockArgToYieldOperand[blockArg])\n+          continue;\n+        argSet.insert(blockArg.getArgNumber() - forOp.getNumInductionVars());\n+        operand = blockArgToYieldOperand[blockArg];\n+      }\n+      Operation *depOp = operand.getDefiningOp();\n+      assert(depOp && \"Unexpected Value with no defining op\");\n+      if (depOp->getBlock() != forOp.getBody())\n+        continue;\n+      assert(hasAgentId(depOp, agentId) && \"Dependency error\");\n+      dfs(depOp);\n+    }\n+  };\n+\n+  // Start from operations that are marked with this agentId explicitly and\n+  // check dependency with DFS traversal\n+  forOp.walk([&](Operation *op) {\n+    if (hasAgentId(op, agentId) && !isa<scf::YieldOp>(op))\n+      dfs(op);\n+  });\n+\n+  // Collect used block args\n+  SmallVector<unsigned> args(argSet.begin(), argSet.end());\n+  llvm::sort(args);\n+  return args;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// createForOpsForEachAgentId\n+//===----------------------------------------------------------------------===//\n+\n+DenseMap<AgentId, scf::ForOp> createForOpsForEachAgentId(scf::ForOp forOp) {\n+  // Collect operation list for each agentId\n+  DenseMap<AgentId, SmallVector<Operation *>> opList;\n+  for (Operation &op : forOp.getBody()->without_terminator())\n+    for (AgentId agentId : getAgentIds(&op))\n+      opList[agentId].push_back(&op);\n+\n+  // Prepare blockArgToYieldOperand mapping\n+  DenseMap<BlockArgument, Value> blockArgToYieldOperand;\n+  auto yieldOp = llvm::cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n+  assert(yieldOp.getNumOperands() == forOp.getNumRegionIterArgs());\n+  for (unsigned i = 0; i < forOp.getNumRegionIterArgs(); ++i)\n+    blockArgToYieldOperand[forOp.getRegionIterArg(i)] = yieldOp.getOperand(i);\n+\n+  auto loc = forOp.getLoc();\n+  OpBuilderWithAgentIds builder(forOp.getContext());\n+  DenseMap<AgentId, scf::ForOp> agentsToForOp;\n+\n+  // Create newForOp for each agent\n+  for (AgentId agentId : collectAgentIds(forOp)) {\n+    auto usedArgs = checkDependencyAndCollectUsedArgs(forOp, agentId,\n+                                                      blockArgToYieldOperand);\n+\n+    // Prepare newLoopArgs\n+    SmallVector<Value> newLoopArgs;\n+    for (unsigned argNumber : usedArgs)\n+      newLoopArgs.push_back(forOp.getIterOperands()[argNumber]);\n+\n+    // Create newForOp\n+    builder.setAgentIdsFromArray({agentId});\n+    builder.setInsertionPoint(forOp);\n+    auto newForOp = builder.createWithAgentIds<scf::ForOp>(\n+        loc, forOp.getLowerBound(), forOp.getUpperBound(), forOp.getStep(),\n+        newLoopArgs);\n+\n+    // Initialize Value mapping from forOp to newForOp\n+    IRMapping mapping;\n+    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n+    for (unsigned i = 0; i < usedArgs.size(); ++i) {\n+      auto oldArg = forOp.getRegionIterArgs()[usedArgs[i]];\n+      auto newArg = newForOp.getRegionIterArgs()[i];\n+      mapping.map(oldArg, newArg);\n+    }\n+\n+    // Clone all operations with this agentId to newForOp\n+    builder.setInsertionPointToStart(newForOp.getBody());\n+    for (Operation *op : opList[agentId]) {\n+      Operation *newOp = builder.clone(*op, mapping);\n+      setAgentIds(newOp, {agentId});\n+      for (unsigned i = 0; i < op->getNumResults(); ++i)\n+        mapping.map(op->getResult(i), newOp->getResult(i));\n+    }\n+\n+    // Create YieldOp for newForOp\n+    SmallVector<Value> newYieldOperands;\n+    for (unsigned i : usedArgs)\n+      newYieldOperands.push_back(mapping.lookup(yieldOp.getOperand(i)));\n+    auto newYieldOp =\n+        builder.create<scf::YieldOp>(yieldOp.getLoc(), newYieldOperands);\n+    setAgentIds(newYieldOp, {agentId});\n+\n+    // Replace results of forOp with results of newForOp\n+    for (unsigned i = 0; i < usedArgs.size(); ++i) {\n+      auto oldResult = forOp.getResult(usedArgs[i]);\n+      auto newResult = newForOp.getResult(i);\n+      oldResult.replaceAllUsesWith(newResult);\n+    }\n+\n+    agentsToForOp[agentId] = newForOp;\n+  }\n+\n+  return agentsToForOp;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// createIfOpsForEachAgentId\n+//===----------------------------------------------------------------------===//\n+\n+DenseMap<AgentId, scf::IfOp> createIfOpsForEachAgentId(scf::IfOp ifOp) {\n+  // TODO: to be implemented\n+  OpBuilderWithAgentIds builder(ifOp.getContext());\n+  DenseMap<AgentId, scf::IfOp> agentsToIfOp;\n+  return agentsToIfOp;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// SpecializeAgentRegion\n+//===----------------------------------------------------------------------===//\n+\n+DenseMap<AgentId, scf::IfOp> SpecializeAgentRegion(triton::FuncOp funcOp) {\n+  MLIRContext *context = funcOp.getContext();\n+  OpBuilder builder(context);\n+  auto loc = funcOp.getLoc();\n+\n+  // Get block from funcOp\n+  Block *block = &funcOp.getBody().front();\n+  auto returnOp = llvm::cast<triton::ReturnOp>(block->getTerminator());\n+\n+  // Collect original operations\n+  SmallVector<Operation *> opList;\n+  for (Operation &op : block->getOperations())\n+    opList.push_back(&op);\n+\n+  // Get curAgentId\n+  builder.setInsertionPoint(returnOp);\n+  Value curAgentId = builder.create<ttng::GetAgentIdOp>(loc);\n+\n+  // Resources for each agentId\n+  DenseMap<AgentId, std::shared_ptr<OpBuilderWithAgentIds>> agentsToBuilders;\n+  DenseMap<AgentId, scf::IfOp> agentsToIfOp;\n+  DenseMap<AgentId, IRMapping> agentsToIRMappings;\n+\n+  for (AgentId agentId : collectAgentIds(funcOp)) {\n+    // Create IfOp for each agentId\n+    Value cond = builder.create<arith::CmpIOp>(\n+        loc, arith::CmpIPredicate::eq, curAgentId,\n+        builder.create<arith::ConstantIntOp>(loc, agentId, 32));\n+\n+    auto ifOp = builder.create<scf::IfOp>(loc, cond);\n+    agentsToIfOp[agentId] = ifOp;\n+    setAgentIds(ifOp, {agentId});\n+\n+    // Create OpBuilderWithAgentIds for each agent\n+    auto agentBuilder = std::make_shared<OpBuilderWithAgentIds>(context);\n+    agentsToBuilders[agentId] = agentBuilder;\n+    agentBuilder->setAgentIdsFromArray({agentId});\n+\n+    // Set insertion point before yieldOp\n+    auto yieldOp = ifOp.thenYield();\n+    setAgentIds(yieldOp, {agentId});\n+    agentBuilder->setInsertionPoint(yieldOp);\n+  }\n+\n+  // Clone all operations into corresponding if blocks\n+  SmallVector<Operation *> cloned;\n+  for (Operation *op : opList) {\n+    auto agentIds = getAgentIds(op);\n+    if (!agentIds.empty()) {\n+      cloned.push_back(op);\n+      for (AgentId agentId : getAgentIds(op)) {\n+        IRMapping &mapping = agentsToIRMappings[agentId];\n+        Operation *newOp = agentsToBuilders[agentId]->clone(*op, mapping);\n+        for (unsigned i = 0; i < op->getNumResults(); ++i)\n+          mapping.map(op->getResult(i), newOp->getResult(i));\n+      }\n+    }\n+  }\n+\n+  // Remove original operations that have been cloned in reverse order\n+  for (auto it = cloned.rbegin(); it != cloned.rend(); ++it) {\n+    Operation *op = *it;\n+    op->erase();\n+  }\n+\n+  return agentsToIfOp;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// collectAsyncChannels\n+//===----------------------------------------------------------------------===//\n+\n+void collectAsyncChannels(SmallVector<std::unique_ptr<Channel>> &channels,\n+                          triton::FuncOp &funcOp) {\n+  funcOp.walk([&](Operation *op) {\n+    for (auto result : op->getResults()) {\n+      if (result.use_empty() || !op->hasAttr(\"async_agent\")) {\n+        continue;\n+      }\n+      auto producerAgent =\n+          op->getAttrOfType<DenseIntElementsAttr>(\"async_agent\");\n+      if (producerAgent.getValues<int>().size() > 1) {\n+        continue;\n+      }\n+      for (Operation *userOp : result.getUsers()) {\n+        if (!userOp->hasAttr(\"async_agent\") ||\n+            userOp->getAttrOfType<DenseIntElementsAttr>(\"async_agent\")\n+                    .getValues<int>()\n+                    .size() > 1) {\n+          continue;\n+        }\n+        auto consumerAgentId =\n+            userOp->getAttrOfType<DenseIntElementsAttr>(\"async_agent\")\n+                .getValues<int>()[0];\n+        auto producerAgentId = producerAgent.getValues<int>()[0];\n+        if (producerAgentId != consumerAgentId) {\n+          channels.push_back(std::make_unique<Channel>(\n+              producerAgentId, consumerAgentId, op, userOp));\n+        }\n+      }\n+    }\n+  });\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// reduceChannels\n+//===----------------------------------------------------------------------===//\n+\n+void reduceChannels(SmallVector<Channel *> &channels,\n+\n+                    DenseMap<Operation *, SmallVector<Channel *>> &map) {\n+  // If producers or their consumers has the same convergent comsumer,\n+  // and those producers, producers' consumers and the convergent comsumer are\n+  // in the same block, They share the same token.\n+  auto checkConverge = [](Operation *op1, Operation *op2) -> Operation * {\n+    // Only check level-0 and level-1 convergence, e.g.\n+    // producer:       load0          load1\n+    //                   |              |\n+    // consumer:  convertLayout0  convertLayout1\n+    //                    \\             /\n+    // consumer:                 dot\n+    // The example above is level-1 convergence.\n+    // If convertLayoutOps converge in deeper depth, this function will\n+    // fail to detect.\n+    // TODO: implement general level-N convergence.\n+    if (op1 == op2) {\n+      return op1;\n+    }\n+    if (op1->getBlock() == op2->getBlock() && op1->hasOneUse() &&\n+        op2->hasOneUse() &&\n+        *(op1->getUsers().begin()) == *(op2->getUsers().begin()) &&\n+        (*(op1->getUsers().begin()))->getBlock() == op1->getBlock()) {\n+      return *(op1->getUsers().begin());\n+    }\n+    return nullptr;\n+  };\n+  assert(channels.size() > 0 && \"channel size is zero\");\n+  // Compare with existing channels in map\n+  for (auto c0 = channels.begin(); c0 != channels.end(); ++c0) {\n+    bool isConvergent = false;\n+    for (auto &kv : map) {\n+      if (kv.second.size() > 0 &&\n+          (*c0)->srcOp->getBlock() == kv.second.front()->srcOp->getBlock()) {\n+        if (auto cvg = checkConverge((*c0)->dstOp, kv.second.front()->dstOp)) {\n+          kv.second.push_back(*c0);\n+          isConvergent = true;\n+          break;\n+        }\n+      }\n+    }\n+    if (!isConvergent) {\n+      map[(*c0)->dstOp].push_back(*c0);\n+    }\n+  }\n+\n+  // Reorder channels and maps based on locations of producers\n+  for (auto &kv : map) {\n+    if (kv.second.size() > 1) {\n+      auto &allOps = kv.second.front()->srcOp->getBlock()->getOperations();\n+      std::sort(\n+          kv.second.begin(), kv.second.end(), [&](Channel *a, Channel *b) {\n+            auto itrA =\n+                std::find_if(allOps.begin(), allOps.end(), [&](Operation &op) {\n+                  Operation *opPointer = &op;\n+                  return opPointer == a->srcOp;\n+                });\n+            auto itrB =\n+                std::find_if(allOps.begin(), allOps.end(), [&](Operation &op) {\n+                  Operation *opPointer = &op;\n+                  return opPointer == b->srcOp;\n+                });\n+            assert(itrA != allOps.end() && itrB != allOps.end());\n+            return std::distance(itrA, itrB) < 0;\n+          });\n+    }\n+  }\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// getBackbone\n+//===----------------------------------------------------------------------===//\n+\n+SmallVector<Operation *> getBackbone(triton::FuncOp funcOp,\n+                                     const SmallVector<Channel *> &channels) {\n+  // Backbone: outermost Ops with regions in funcOp which contain at least one\n+  // relation between producer and consumer. It assumes producer-consumer\n+  // relation going across two outermost Ops in funcOp is forbidden. For\n+  // example, In the example of runOnOperation(), only the outermost ForOp is\n+  // backbone, the inner ForOp is not.\n+  SmallVector<Operation *> backboneOps;\n+  auto isBackbone = [&](Operation *backbone) -> bool {\n+    for (auto c : channels) {\n+      Operation *producer = c->srcOp, *consumer = c->dstOp;\n+      while (producer && !isa<triton::FuncOp>(producer->getParentOp())) {\n+        producer = producer->getParentOp();\n+      }\n+      while (consumer && !isa<triton::FuncOp>(consumer->getParentOp())) {\n+        consumer = consumer->getParentOp();\n+      }\n+      if (producer == backbone && consumer == backbone) {\n+        return true;\n+      }\n+      assert((producer != backbone ||\n+              isa<triton::FuncOp>(producer->getParentOp())) &&\n+             (consumer != backbone ||\n+              isa<triton::FuncOp>(consumer->getParentOp())) &&\n+             \"Error: producer and consumer belongs to different backboneOps\");\n+    }\n+    return false;\n+  };\n+  Operation *op;\n+  for (Operation &bodyOp : funcOp.getBody().front().getOperations()) {\n+    op = &bodyOp;\n+    if (op->getNumRegions() > 0) {\n+      // If this op as a whole is a producer or consumer, continue\n+      if (getAgentIds(op).size() == 1) {\n+        continue;\n+      }\n+      if (isBackbone(op)) {\n+        backboneOps.push_back(op);\n+      }\n+    }\n+  }\n+  return backboneOps;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// buildAsyncComm\n+//===----------------------------------------------------------------------===//\n+\n+void buildAsyncComm(const DenseMap<Operation *, SmallVector<Channel *>> &map,\n+                    const DenseMap<Channel *, Value> &tokenMap,\n+                    const DenseMap<Channel *, Value> &bufferMap,\n+                    int numStages) {\n+\n+  auto getSameLevelOp = [](Operation *p, Operation *c) -> Operation * {\n+    while (!isa<triton::FuncOp>(c)) {\n+      if (c->getParentOp() == p->getParentOp()) {\n+        return c;\n+      }\n+      c = c->getParentOp();\n+    }\n+    llvm_unreachable(\"Falied to find consumer's same level Op with producer\");\n+  };\n+\n+  auto consumerReleaseHeutistic = [&](Operation *p,\n+                                      Operation *c) -> Operation * {\n+    if (c->getBlock() == p->getBlock()) {\n+      auto consumerAgentId =\n+          c->getAttrOfType<DenseIntElementsAttr>(\"async_agent\")\n+              .getValues<int>()[0];\n+      for (auto it = c->getBlock()->rbegin(); it != c->getBlock()->rend();\n+           ++it) {\n+        if (!it->hasAttr(\"async_agent\")) {\n+          continue;\n+        }\n+        auto asyncAttr = it->getAttrOfType<DenseIntElementsAttr>(\"async_agent\")\n+                             .getValues<int>();\n+        if (asyncAttr.size() == 1 && asyncAttr[0] == consumerAgentId) {\n+          return &(*it);\n+        }\n+      }\n+      return nullptr;\n+    } else {\n+      return getSameLevelOp(p, c);\n+    }\n+  };\n+\n+  auto getAgents = [&](Operation *p, Operation *c, SmallVector<AgentId> &agentP,\n+                       SmallVector<AgentId> &agentC,\n+                       SmallVector<AgentId> &agentsPC) -> void {\n+    agentP = collectAgentIds(p);\n+    agentC = collectAgentIds(c);\n+    agentsPC.reserve(agentP.size() + agentC.size());\n+    agentsPC.insert(agentsPC.end(), agentP.begin(), agentP.end());\n+    agentsPC.insert(agentsPC.end(), agentC.begin(), agentC.end());\n+  };\n+  // TODO: try to optimize locations of arriving and waiting token\n+  // for fused-attention\n+  for (auto kv : map) {\n+    /*****************Token related*****************/\n+    auto headProducer = kv.second.front()->srcOp;\n+    auto tailProducer = kv.second.back()->srcOp;\n+    auto headConsumer = kv.second.front()->dstOp;\n+    auto tailConsumer = kv.second.back()->dstOp;\n+    auto token = tokenMap.find(kv.second.front())->second;\n+    SmallVector<AgentId> agentP, agentC, agentsPC;\n+    getAgents(headProducer, headConsumer, agentP, agentC, agentsPC);\n+    OpBuilderWithAgentIds builder(headProducer->getContext());\n+\n+    if (auto funcOp = dyn_cast<triton::FuncOp>(headProducer->getParentOp())) {\n+      builder.setInsertionPointToStart(&(funcOp.getBody().front()));\n+    } else {\n+      builder.setInsertionPoint(headProducer->getParentOp());\n+    }\n+    builder.setAgentIdsFromArray(agentsPC);\n+    Value pipelineIdx;\n+    Value numStagesVal = builder.createWithAgentIds<arith::ConstantIntOp>(\n+        headProducer->getLoc(), numStages, 32);\n+    if (auto forOp = headProducer->getParentOfType<scf::ForOp>()) {\n+      pipelineIdx = forOp.getBody()->getArguments().back();\n+    } else {\n+      // existing\");\n+      pipelineIdx = builder.createWithAgentIds<arith::ConstantIntOp>(\n+          headProducer->getLoc(), 0, 32);\n+    }\n+\n+    // insert ProducerAcquireOp\n+    builder.setInsertionPoint(headProducer);\n+    if (headProducer->getParentOfType<scf::ForOp>()) {\n+      pipelineIdx = builder.createWithAgentIds<arith::RemSIOp>(\n+          headProducer->getLoc(), pipelineIdx, numStagesVal);\n+    }\n+    builder.setAgentIdsFromArray(agentP);\n+    builder.createWithAgentIds<ttng::ProducerAcquireOp>(headProducer->getLoc(),\n+                                                        token, pipelineIdx);\n+\n+    // insert ProducerCommitOp\n+    builder.setInsertionPointAfter(tailProducer);\n+    builder.createWithAgentIds<ttng::ProducerCommitOp>(tailProducer->getLoc(),\n+                                                       token, pipelineIdx);\n+\n+    builder.setAgentIdsFromArray(agentC);\n+    // insert ConsumerWaitOp\n+    auto consumerWaitPoint = getSameLevelOp(headProducer, headConsumer);\n+    builder.setInsertionPoint(consumerWaitPoint);\n+    builder.createWithAgentIds<ttng::ConsumerWaitOp>(headConsumer->getLoc(),\n+                                                     token, pipelineIdx);\n+\n+    // insert ConsumerReleaseOp\n+    auto consumerReleasePoint =\n+        consumerReleaseHeutistic(tailProducer, tailConsumer);\n+    builder.setInsertionPointAfter(consumerReleasePoint);\n+    builder.createWithAgentIds<ttng::ConsumerReleaseOp>(\n+        consumerReleasePoint->getLoc(), token, pipelineIdx);\n+\n+    /*****************Buffer related*****************/\n+    /// splitLoadsInForLoop\n+    for (auto &c : kv.second) {\n+      assert(isa<triton::LoadOp>(c->srcOp) && \"prodcuerOp is not tt.load\");\n+      auto loadOp = cast<triton::LoadOp>(c->srcOp);\n+      auto buffer = bufferMap.find(c)->second;\n+      MLIRContext *context = loadOp->getContext();\n+      OpBuilderWithAgentIds builder(context);\n+      builder.setInsertionPoint(loadOp->getParentOp());\n+      builder.setAgentIdsFromArray(agentsPC);\n+\n+      builder.setInsertionPoint(loadOp);\n+      Value loadResult = loadOp.getResult();\n+      if (auto tensorType = loadResult.getType().dyn_cast<RankedTensorType>()) {\n+        // Get basic information from tensorType\n+        auto order = ttg::getOrder(tensorType.getEncoding());\n+        auto CTALayout = ttg::getCTALayout(tensorType.getEncoding());\n+        auto elemType = tensorType.getElementType();\n+\n+        // Get shape, layout and type of a slice\n+        auto sliceShape = tensorType.getShape();\n+        auto sharedLayout = ttg::SharedEncodingAttr::get(\n+            context, sliceShape, order, CTALayout, elemType);\n+        auto sliceType =\n+            RankedTensorType::get(sliceShape, elemType, sharedLayout);\n+\n+        // Get shape, layout and type of the complete buffer\n+        SmallVector<int64_t> bufferShape(sliceShape.begin(), sliceShape.end());\n+        if (loadOp->getParentOfType<scf::ForOp>()) {\n+          bufferShape.insert(bufferShape.begin(), numStages);\n+        } else {\n+          bufferShape.insert(bufferShape.begin(), 1);\n+        }\n+        auto bufferType =\n+            RankedTensorType::get(bufferShape, elemType, sharedLayout);\n+\n+        // Create InsertSliceOp\n+        builder.setAgentIdsFromOp(loadOp);\n+        builder.setInsertionPointAfter(loadOp);\n+        auto insertSliceOp = builder.createWithAgentIds<ttg::InsertSliceOp>(\n+            /*loc=*/loadOp.getLoc(), /*result=*/bufferType,\n+            /*src=*/loadOp.getPtr(), /*dst=*/buffer, /*index=*/pipelineIdx,\n+            /*mask=*/loadOp.getMask(), /*other=*/loadOp.getOther(),\n+            /*cache=*/loadOp.getCache(), /*evict=*/loadOp.getEvict(),\n+            /*isVolatile=*/loadOp.getIsVolatile(), /*axis=*/0);\n+\n+        // Create ExtractSliceOp\n+        auto attr = [&](int val) { return builder.getI64IntegerAttr(val); };\n+        SmallVector<OpFoldResult> offsets = {pipelineIdx, attr(0), attr(0)};\n+        SmallVector<OpFoldResult> sizes = {attr(1), attr(sliceShape[0]),\n+                                           attr(sliceShape[1])};\n+        SmallVector<OpFoldResult> strides = {attr(1), attr(1), attr(1)};\n+        builder.setAgentIdsFromValueUsers(loadResult);\n+        builder.setInsertionPoint(c->dstOp);\n+        auto extractSliceOp = builder.createWithAgentIds<ttg::ExtractSliceOp>(\n+            loadOp.getLoc(), sliceType, buffer, offsets, sizes, strides);\n+\n+        // Replace all uses of loadResult\n+        loadResult.replaceAllUsesWith(extractSliceOp.getResult());\n+        loadOp.erase();\n+      }\n+    }\n+  }\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// agentDivision\n+//===----------------------------------------------------------------------===//\n+\n+DenseMap<AgentId, Operation *> agentDivision(Operation *backbone) {\n+  // A general agent division in backbone could be:\n+  // *  If opWithRegion has results, e.g. scf.for, this opWithRegion will be\n+  //    splitted into several new operations, each agent has one, which\n+  //    has the part of results related to this agent. One agent could own\n+  //    all orginal results or none of them, but one result must belong to\n+  //    one and only one agent.\n+  // *  if opWithRegions doesn't have result. Simply split for every agent.\n+  // *  So does operands of opWithRegions\n+  // However, current backbones are all ForOps and IfOps. So we customize\n+  // the implementation.\n+  DenseMap<AgentId, Operation *> agentBackbone;\n+  backbone->walk([&](Operation *op) {\n+    auto ids = getAgentIds(op);\n+    if (op->getNumRegions() > 0 && ids.size() > 1) {\n+      // ForOp: change iterArgs and yield results\n+      if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+        auto forOps = createForOpsForEachAgentId(forOp);\n+        if (op == backbone) {\n+          for (auto kv : forOps) {\n+            auto f = kv.second;\n+            auto id = getAgentIds(f.getOperation());\n+            assert(id.size() == 1 &&\n+                   \"generated ForOp doesn't have one and only one agentId\");\n+            agentBackbone[id.front()] = f.getOperation();\n+          }\n+        }\n+        forOp.erase();\n+      } else if (auto ifOp = dyn_cast<scf::IfOp>(op)) {\n+        // TODO: to be implemented\n+        llvm_unreachable(\"If Op is unsupported\");\n+        auto ifOps = createIfOpsForEachAgentId(ifOp);\n+        assert(ifOps.size() > 0);\n+        if (op == backbone) {\n+          for (auto kv : ifOps) {\n+            auto i = kv.second;\n+            auto id = getAgentIds(i.getOperation());\n+            assert(id.size() == 1 &&\n+                   \"generated IfOp doesn't have one and only one agentId\");\n+            agentBackbone[id.front()] = i.getOperation();\n+          }\n+        }\n+      } else {\n+        llvm_unreachable(\"Unexpected Op with regions\");\n+      }\n+    }\n+  });\n+  assert(agentBackbone.size() > 0 && \"Agent division failed\");\n+  return agentBackbone;\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// cloneBackboneForEachAgentId\n+//===----------------------------------------------------------------------===//\n+\n+void cloneBackboneForEachAgentId(SmallVector<Operation *> &backbone) {\n+  SmallVector<Operation *> newBackBone;\n+\n+  for (Operation *op : backbone) {\n+    auto loc = op->getLoc();\n+    OpBuilderWithAgentIds builder(op->getContext());\n+    builder.setInsertionPoint(op);\n+    // First, agent division\n+    DenseMap<AgentId, Operation *> agentBackbone = agentDivision(op);\n+\n+    // Second, remove irrelavant Ops\n+    for (auto kv : agentBackbone) {\n+      SmallVector<Operation *> deleteOps;\n+      AgentId targetId = kv.first;\n+      Operation *newBackbone = kv.second;\n+      newBackbone->walk([&](Operation *subOp) {\n+        auto ids = getAgentIds(subOp);\n+        if (std::find(ids.begin(), ids.end(), targetId) == ids.end()) {\n+          deleteOps.push_back(subOp);\n+        }\n+      });\n+      for (auto it = deleteOps.rbegin(); it != deleteOps.rend(); ++it) {\n+        (*it)->erase();\n+      }\n+    }\n+  }\n+}\n+\n+//===----------------------------------------------------------------------===//\n+// WSPipelinePass\n+//===----------------------------------------------------------------------===//\n+\n+struct WSPipelinePass : public TritonGPUWSPipelineBase<WSPipelinePass> {\n+  WSPipelinePass() = default;\n+  WSPipelinePass(int numStages, int numWarps, int computeCapability) {\n+    this->numStages = numStages;\n+    this->numWarps = numWarps;\n+    this->computeCapability = computeCapability;\n+  }\n+\n+  void runOnOperation() override {\n+    auto mod = getOperation();\n+    if (!ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod))\n+      return signalPassFailure();\n+\n+    mod.walk([&](triton::FuncOp funcOp) {\n+      assert(funcOp.getBody().hasOneBlock() &&\n+             \"FuncOp with more than one blocks is not supported\");\n+      // Maintain all structures between funcOp and producer/consumer Op, for\n+      // example:\n+      /*  +-----------------------------------+\n+       *  | scf.for:                          |\n+       *  |   A = tt.load {agentId = 0}       |\n+       *  |   scf.for:                        |\n+       *  |     B = tt.load {agentId = 0}     |\n+       *  |     C = tt.dot A, B {agentId = 1} |\n+       *  +-----------------------------------+\n+       *                    ||\n+       *                   \\||/\n+       *                    \\/\n+       *  +-----------------------------------------+\n+       *  | token0 = create_token()                 |\n+       *  | token1 = create_token()                 |\n+       *  | buffer0 = alloc_buffer()                |\n+       *  | buffer1 = alloc_buffer()                |\n+       *  | if agent0:                              |\n+       *  |   scf.for:                              |\n+       *  |     producer_aquire token0              |\n+       *  |     buffer0 = tt.load           (load A)|\n+       *  |     producer_commit token0              |\n+       *  |     scf.for:                            |\n+       *  |       producer_aquire token1            |\n+       *  |       buffer1 = tt.load         (load B)|\n+       *  |       producer_commit token1            |\n+       *  | if agent1:                              |\n+       *  |   scf.for:                              |\n+       *  |     consumer_wait token0                |\n+       *  |     scf.for:                            |\n+       *  |       consumer_wait token1              |\n+       *  |       A = extract_slice buffer0         |\n+       *  |       B = extract_slice buffer1         |\n+       *  |       C = tt.dot A, B                   |\n+       *  |       consumer_arrive token1            |\n+       *  |     consumer_arrive token0              |\n+       *  +-----------------------------------------+\n+       */\n+\n+      // First step: collect channels\n+      SmallVector<std::unique_ptr<Channel>> channelsOrigin;\n+      collectAsyncChannels(channelsOrigin, funcOp);\n+      SmallVector<Channel *> channels;\n+      for (const auto &c : channelsOrigin) {\n+        channels.push_back(c.get());\n+      }\n+\n+      // cvgOp-channels map\n+      DenseMap<Operation *, SmallVector<Channel *>> map;\n+      reduceChannels(channels, map);\n+\n+      // Prepare phase, getBackbone, appendPipelineIdxArgs\n+      SmallVector<Operation *> backbone = getBackbone(funcOp, channels);\n+      appendPipelineIdxArgs(backbone, numStages);\n+\n+      // Create token, buffer and data tranfer between async agents\n+      DenseMap<Channel *, Value> tokenMap = createToken(map, funcOp, numStages);\n+      DenseMap<Channel *, Value> bufferMap =\n+          createBuffer(channels, funcOp, numStages);\n+      buildAsyncComm(map, tokenMap, bufferMap, numStages);\n+\n+      // Clone backbone, remove irrelevant blockArgument for {forOp, ifOp}\n+      cloneBackboneForEachAgentId(backbone);\n+\n+      // Specialize agent region\n+      SpecializeAgentRegion(funcOp);\n+    });\n+  }\n+};\n+\n+} // namespace\n+\n+//===----------------------------------------------------------------------===//\n+// createTritonNvidiaGPUWSPipelinePass\n+//===----------------------------------------------------------------------===//\n+\n+std::unique_ptr<Pass>\n+mlir::createTritonNvidiaGPUWSPipelinePass(int numStages, int numWarps,\n+                                          int computeCapability) {\n+  return std::make_unique<WSPipelinePass>(numStages, numWarps,\n+                                          computeCapability);\n+}"}, {"filename": "lib/Hopper/CMakeLists.txt", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "@@ -0,0 +1,24 @@\n+# TODO: re-enable generation of the hopper helper.\n+# For now we just commit a pre-built bc file.\n+#cmake_path(GET LLVM_LIBRARY_DIR PARENT_PATH LLVM_DIR)\n+#set(CUDA_PATH \"/usr/local/cuda\")\n+#if(DEFINED ENV{CUDA_PATH})\n+#        set(CUDA_PATH $ENV{CUDA_PATH})\n+#endif()\n+#\n+#set(outfile \"libhopper_helpers.bc\")\n+#add_custom_target(HopperHelpers ALL\n+#        COMMAND ${LLVM_DIR}/bin/clang -c\n+#        -O3 -emit-llvm\n+#        -fno-unwind-tables -fno-exceptions\n+#        --cuda-gpu-arch=sm_90a\n+#        --cuda-device-only\n+#        -D__CUDACC__ -D__CUDABE__ -D__CUDA_LIBDEVICE__\n+#        -I${CUDA_PATH}/include\n+#        -target nvptx64-nvidia-gpulibs\n+#        ${CMAKE_CURRENT_SOURCE_DIR}/HopperHelpers.c -o ${outfile}\n+#        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/HopperHelpers.c\n+#        BYPRODUCTS ${outfile}\n+#        COMMENT \"Building LLVM bitcode ${outfile}\"\n+#        VERBATIM\n+#)"}, {"filename": "lib/Hopper/HopperHelpers.c", "status": "added", "additions": 503, "deletions": 0, "changes": 503, "file_content_changes": "@@ -0,0 +1,503 @@\n+/*\n+ * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining\n+ * a copy of this software and associated documentation files\n+ * (the \"Software\"), to deal in the Software without restriction,\n+ * including without limitation the rights to use, copy, modify, merge,\n+ * publish, distribute, sublicense, and/or sell copies of the Software,\n+ * and to permit persons to whom the Software is furnished to do so,\n+ * subject to the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be\n+ * included in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+ */\n+\n+#include \"device_launch_parameters.h\"\n+#include <builtin_types.h>\n+#include <cuda_device_runtime_api.h>\n+#include <stdarg.h>\n+#include <stdint.h>\n+#include <stdio.h>\n+\n+#ifdef __NVCC__\n+#define __DEVICE__ __device__ inline\n+#else\n+#define __DEVICE__\n+#endif\n+\n+#ifndef BARRIER_RANDOM_DELAY\n+#define BARRIER_RANDOM_DELAY 0\n+#endif\n+\n+#if BARRIER_RANDOM_DELAY\n+__DEVICE__ uint64_t random_avalanche(uint64_t r) {\n+  r ^= r >> 33;\n+  r *= 0xff51afd7ed558ccd;\n+  r ^= r >> 33;\n+  r *= 0xc4ceb9fe1a85ec53;\n+  r ^= r >> 33;\n+  return r;\n+}\n+\n+__DEVICE__ uint64_t random_stateless_uint64() {\n+  uint64_t r = blockIdx.z << 27 ^ blockIdx.y << 22 ^ blockIdx.x;\n+  r = r << 32 ^ threadIdx.z << 20 ^ threadIdx.y << 10 ^ threadIdx.x;\n+\n+  uint64_t timer;\n+  asm volatile(\"mov.u64 %0, %%globaltimer;\\n\\t\" : \"=l\"(timer) : : \"memory\");\n+\n+  r ^= timer;\n+  return random_avalanche(r);\n+}\n+#endif\n+\n+__DEVICE__ void random_stateless_delay() {\n+#if BARRIER_RANDOM_DELAY\n+  uint64_t r = random_stateless_uint64();\n+\n+  // About 2 milliseconds.\n+  uint32_t ns = r >> (64 - 21);\n+  asm volatile(\"nanosleep.u32 %0;\\n\\t\" : : \"r\"(ns));\n+#endif\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) uint64_t\n+__nv_get_wgmma_desc(uint32_t smem_nvvm_pointer, uint32_t mode,\n+                    uint32_t height) {\n+  uint64_t desc = 0;\n+  uint64_t swizzling = (mode == 1 ? 128 : mode == 2 ? 64 : 32);\n+  uint64_t smem_address_bit = smem_nvvm_pointer;\n+\n+  uint64_t stride_dimension = swizzling << 3 >> 4;\n+  uint64_t leading_dimension = height * swizzling >> 4;\n+  // [benzh] from cutlass\n+  uint64_t base_offset = 0; //(smem_address_bit >> 7) % (swizzling >> 4);\n+  uint64_t start_addr = (smem_address_bit << 46) >> 50;\n+\n+  desc |= ((uint64_t)mode) << 62;\n+  desc |= stride_dimension << 32;\n+  desc |= leading_dimension << 16;\n+  desc |= base_offset << 49;\n+  desc |= start_addr;\n+\n+  return desc;\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_fence() {\n+  asm volatile(\"wgmma.fence.sync.aligned;\\n\");\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_commit_group() {\n+  asm volatile(\"wgmma.commit_group.sync.aligned;\\n\");\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_wait_group() {\n+  asm volatile(\"wgmma.wait_group.sync.aligned 0;\\n\");\n+}\n+\n+// GMMA expects data to be in TN format. if A is column major, transa should be\n+// set GMMA expects data to be in TN format. if B is row major, transb should be\n+// set\n+__DEVICE__ __attribute__((__always_inline__)) float32\n+__nv_wgmma_m64n64k16_f32_f16_f16_row_col(const uint64_t desc_a,\n+                                         const uint64_t desc_b, float32 acc) {\n+  const uint32_t scale_d = 1;\n+  asm volatile(\"{\\n\"\n+               \".reg .pred p;\\n\\t\"\n+               \"setp.eq.u32 p, %34, 1;\\n\\t\"\n+               \"wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16\\n\"\n+               \"{%0, %1, %2, %3, %4, %5, %6, %7, \\n\"\n+               \" %8, %9, %10, %11, %12, %13, %14, %15,\\n\"\n+               \" %16, %17, %18, %19, %20, %21, %22, %23,\\n\"\n+               \" %24, %25, %26, %27, %28, %29, %30, %31},\\n\"\n+               \"%32, \\n\"\n+               \"%33, \\n\"\n+               \"p, 1, 1, 0, 0;\\n\"\n+               \"}\"\n+               : \"+f\"(acc.d0), \"+f\"(acc.d1), \"+f\"(acc.d2), \"+f\"(acc.d3),\n+                 \"+f\"(acc.d4), \"+f\"(acc.d5), \"+f\"(acc.d6), \"+f\"(acc.d7),\n+                 \"+f\"(acc.d8), \"+f\"(acc.d9), \"+f\"(acc.d10), \"+f\"(acc.d11),\n+                 \"+f\"(acc.d12), \"+f\"(acc.d13), \"+f\"(acc.d14), \"+f\"(acc.d15),\n+                 \"+f\"(acc.d16), \"+f\"(acc.d17), \"+f\"(acc.d18), \"+f\"(acc.d19),\n+                 \"+f\"(acc.d20), \"+f\"(acc.d21), \"+f\"(acc.d22), \"+f\"(acc.d23),\n+                 \"+f\"(acc.d24), \"+f\"(acc.d25), \"+f\"(acc.d26), \"+f\"(acc.d27),\n+                 \"+f\"(acc.d28), \"+f\"(acc.d29), \"+f\"(acc.d30), \"+f\"(acc.d31)\n+               : \"l\"(desc_a), \"l\"(desc_b), \"r\"(scale_d));\n+\n+  return acc;\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_mbarrier_init(uint32_t bar, uint32_t expected, uint32_t pred) {\n+  if (pred) {\n+    asm volatile(\"{\\n\\t\"\n+                 \"mbarrier.init.shared.b64 [%0], %1;\\n\\t\"\n+                 \"}\"\n+                 :\n+                 : \"r\"(bar), \"r\"(expected));\n+  }\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_mbarrier_wait(uint32_t bar, uint32_t phase) {\n+  uint32_t large_val = 0x989680;\n+  asm volatile(\"{\\n\\t\"\n+               \".reg .pred                P1; \\n\\t\"\n+               \"LAB_WAIT: \\n\\t\"\n+               \"mbarrier.try_wait.parity.shared.b64 P1, [%0], %1, %2; \\n\\t\"\n+               \"@P1                       bra.uni DONE; \\n\\t\"\n+               \"bra.uni                   LAB_WAIT; \\n\\t\"\n+               \"DONE: \\n\\t\"\n+               \"}\"\n+               :\n+               : \"r\"(bar), \"r\"(phase), \"r\"(large_val));\n+  random_stateless_delay();\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) int\n+__nv_mbarrier_peek(uint32_t bar, uint32_t phase) {\n+  random_stateless_delay();\n+  int ready = 0;\n+  asm volatile(\"{\\n\\t\"\n+               \".reg .pred p;\\n\\t\"\n+               \"mbarrier.try_wait.shared.b64 p, [%1], %2;\\n\\t\"\n+               \"selp.b32 %0, 1, 0, p;\\n\\t\"\n+               \"}\"\n+               : \"=r\"(ready)\n+               : \"r\"(bar), \"l\"((unsigned long long)phase)\n+               : \"memory\");\n+  return ready;\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_mbarrier_arrive_normal(uint32_t bar, uint32_t pred) {\n+  random_stateless_delay();\n+  if (pred) {\n+    asm volatile(\"{\\n\\t\"\n+                 \"mbarrier.arrive.shared.b64 _, [%0];\\n\\t\"\n+                 \"}\"\n+                 :\n+                 : \"r\"(bar));\n+  }\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_mbarrier_arrive_cp_async(uint32_t bar, uint32_t pred) {\n+  random_stateless_delay();\n+  if (pred) {\n+    asm volatile(\"cp.async.mbarrier.arrive.noinc.shared.b64 [%0];\"\n+                 :\n+                 : \"r\"(bar));\n+  }\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_mbarrier_arrive_expect_tx(uint32_t bar, uint32_t txCount, uint32_t pred) {\n+  random_stateless_delay();\n+  if (pred) {\n+    asm volatile(\"{\\n\\t\"\n+                 \"mbarrier.arrive.expect_tx.shared.b64 _, [%0], %1;\\n\\t\"\n+                 \"}\"\n+                 :\n+                 : \"r\"(bar), \"r\"(txCount));\n+  }\n+}\n+\n+// for warp special empty barrier.\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_mbarrier_arrive_remote(uint32_t bar, uint32_t ctaId, uint32_t pred) {\n+  random_stateless_delay();\n+  if (pred) {\n+    asm volatile(\"{\\n\\t\"\n+                 \".reg .b32 remAddr32;\\n\\t\"\n+                 \"mapa.shared::cluster.u32  remAddr32, %0, %1;\\n\\t\"\n+                 \"mbarrier.arrive.shared::cluster.b64  _, [remAddr32];\\n\\t\"\n+                 \"}\"\n+                 :\n+                 : \"r\"(bar), \"r\"(ctaId));\n+  }\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_fence_mbarrier_init() {\n+  asm volatile(\"{\\n\\t\"\n+               \"fence.mbarrier_init.release.cluster; \\n\"\n+               \"}\" ::\n+                   : \"memory\");\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_fence_async_shared_cta() {\n+  asm volatile(\"fence.proxy.async.shared::cta;\\n\");\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_fence_async_shared_cluster() {\n+  asm volatile(\"fence.proxy.async.shared::cluster;\\n\");\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_cp_async_bulk(char *gmem_ptr, unsigned smem_ptr, unsigned barrier,\n+                   int bytes, uint32_t pred) {\n+  if (pred) {\n+    asm volatile(\"cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::\"\n+                 \"bytes [%0], [%1], %2, [%3];\\n\"\n+                 :\n+                 : \"r\"(smem_ptr), \"l\"(gmem_ptr), \"r\"(bytes), \"r\"(barrier)\n+                 : \"memory\");\n+  }\n+}\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_tma_load_tiled_2d(const uint64_t p_tma_desc, uint32_t dst_smem,\n+                       uint32_t barrier, int32_t c0, int32_t c1,\n+                       unsigned long long mem_desc, uint32_t pred) {\n+  if (pred) {\n+    asm volatile(\n+        \"cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx\"\n+        \"::bytes.L2::cache_hint [%0], [%1, {%2, %3}], \"\n+        \"[%4], %5;\\n\"\n+        :\n+        : \"r\"(dst_smem), \"l\"(p_tma_desc), \"r\"(c0), \"r\"(c1), \"r\"(barrier),\n+          \"l\"(mem_desc)\n+        : \"memory\");\n+  }\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_tma_load_tiled_mcast_2d(\n+    const uint64_t p_tma_desc, uint32_t dst_smem, uint32_t barrier, int32_t c0,\n+    int32_t c1, unsigned long long mem_desc, uint16_t mcast, uint32_t pred) {\n+  if (pred) {\n+    asm volatile(\"cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::\"\n+                 \"complete_tx::bytes.multicast::cluster.L2::cache_hint\"\n+                 \" [%0], [%1, {%2, %3}], [%4], %5, %6;\"\n+                 :\n+                 : \"r\"(dst_smem), \"l\"(p_tma_desc), \"r\"(c0), \"r\"(c1),\n+                   \"r\"(barrier), \"h\"(mcast), \"l\"(mem_desc)\n+                 : \"memory\");\n+  }\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_tma_load_tiled_4d(const uint64_t p_tma_desc, uint32_t dst_smem,\n+                       uint32_t barrier, int32_t c0, int32_t c1, int32_t c2,\n+                       int32_t c3, unsigned long long mem_desc, uint32_t pred) {\n+  if (pred) {\n+    asm volatile(\n+        \"cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx\"\n+        \"::bytes.L2::cache_hint [%0], [%1, {%2, %3, %4, %5}], \"\n+        \"[%6], %7;\\n\"\n+        :\n+        : \"r\"(dst_smem), \"l\"(p_tma_desc), \"r\"(c0), \"r\"(c1), \"r\"(c2), \"r\"(c3),\n+          \"r\"(barrier), \"l\"(mem_desc)\n+        : \"memory\");\n+  }\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_stmatrix_x1(uint32_t ptr, const uint32_t d0) {\n+  asm volatile(\n+      \"stmatrix.sync.aligned.m8n8.x1.shared.b16 [%0], {%1};\\n\" ::\"r\"(ptr),\n+      \"r\"(d0));\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_stmatrix_x2(uint32_t ptr, const uint32_t d0, const uint32_t d1) {\n+  asm volatile(\n+      \"stmatrix.sync.aligned.m8n8.x2.shared.b16 [%0], {%1, %2};\\n\" ::\"r\"(ptr),\n+      \"r\"(d0), \"r\"(d1));\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_stmatrix_x4(uint32_t ptr, const uint32_t d0, const uint32_t d1,\n+                 const uint32_t d2, const uint32_t d3) {\n+  asm volatile(\n+      \"stmatrix.sync.aligned.m8n8.x4.shared.b16 [%0], {%1, %2, %3, %4};\\n\" ::\n+          \"r\"(ptr),\n+      \"r\"(d0), \"r\"(d1), \"r\"(d2), \"r\"(d3));\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_async_group_commit() {\n+  asm volatile(\"cp.async.bulk.commit_group;\");\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_async_group_wait0() {\n+  asm volatile(\"cp.async.bulk.wait_group %0;\" : : \"n\"(0) : \"memory\");\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) uint32_t\n+__nv_dsmem_addr(uint32_t buffer_ptr, uint32_t ctaid) {\n+  uint32_t buffer_ptr_;\n+  asm volatile(\"{\\n\\t\"\n+               \"mapa.shared::cluster.u32 %0, %1, %2;\\n\\t\"\n+               \"}\"\n+               : \"=r\"(buffer_ptr_)\n+               : \"r\"(buffer_ptr), \"r\"(ctaid));\n+  return buffer_ptr_;\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_bar_arrive(uint32_t bar, uint32_t numThreads) {\n+  random_stateless_delay();\n+  asm volatile(\"bar.arrive %0, %1;\\n\" ::\"r\"(bar), \"r\"(numThreads));\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_bar_wait(uint32_t bar, uint32_t numThreads) {\n+  asm volatile(\"bar.sync %0, %1;\\n\" ::\"r\"(bar), \"r\"(numThreads));\n+  random_stateless_delay();\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_cga_barrier_sync() {\n+  asm volatile(\"barrier.cluster.sync.aligned;\\n\" ::);\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_cga_barrier_arrive() {\n+  asm volatile(\"barrier.cluster.arrive;\\n\" ::);\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_cga_barrier_wait() {\n+  asm volatile(\"barrier.cluster.wait;\\n\" ::);\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_cluster_arrive_relaxed() {\n+  asm volatile(\"barrier.cluster.arrive.relaxed.aligned;\\n\" : :);\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_cluster_arrive() {\n+  asm volatile(\"barrier.cluster.arrive.aligned;\\n\" : :);\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void __nv_cluster_wait() {\n+  asm volatile(\"barrier.cluster.wait.aligned;\\n\" : :);\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_tma_store_tiled_2d(const uint64_t p_tma_desc, int32_t src_smem, int32_t c0,\n+                        int32_t c1, uint32_t pred) {\n+  if (pred) {\n+    asm volatile(\"cp.async.bulk.tensor.2d.global.shared::cta.bulk_group\"\n+                 \"[%0, {%2, %3}], [%1];\\n\"\n+                 :\n+                 : \"l\"(p_tma_desc), \"r\"(src_smem), \"r\"(c0), \"r\"(c1)\n+                 : \"memory\");\n+  }\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_tma_store_tiled_3d(const uint64_t p_tma_desc, uint32_t src_smem,\n+                        int32_t c0, int32_t c1, int32_t c2, uint32_t pred) {\n+  if (pred) {\n+    asm volatile(\"cp.async.bulk.tensor.3d.global.shared::cta.bulk_group\"\n+                 \"[%0, {%2, %3, %4}], [%1];\\n\"\n+                 :\n+                 : \"l\"(p_tma_desc), \"r\"(src_smem), \"r\"(c0), \"r\"(c1), \"r\"(c2)\n+                 : \"memory\");\n+  }\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_tma_store_tiled_4d(const uint64_t p_tma_desc, uint32_t src_smem,\n+                        int32_t c0, int32_t c1, int32_t c2, int32_t c3,\n+                        uint32_t pred) {\n+  if (pred) {\n+    asm volatile(\"cp.async.bulk.tensor.4d.global.shared::cta.bulk_group\"\n+                 \"[%0, {%2, %3, %4, %5}], [%1];\\n\"\n+                 :\n+                 : \"l\"(p_tma_desc), \"r\"(src_smem), \"r\"(c0), \"r\"(c1), \"r\"(c2),\n+                   \"r\"(c3)\n+                 : \"memory\");\n+  }\n+}\n+__DEVICE__ __attribute__((__always_inline__)) uint32_t\n+__nv_offset_of_stmatrix_v4(uint32_t threadIdx, uint32_t rowOfWarp,\n+                           uint32_t elemIdx, uint32_t leadingDimOffset,\n+                           uint32_t rowStride) {\n+  uint32_t perPhase = 0;\n+  uint32_t maxPhase = 0;\n+  if (rowStride == 64) {\n+    perPhase = 1;\n+    maxPhase = 8;\n+  } else if (rowStride == 32) {\n+    perPhase = 2;\n+    maxPhase = 4;\n+  } else if (rowStride == 16) {\n+    perPhase = 4;\n+    maxPhase = 2;\n+  }\n+\n+  uint32_t iterOfCol = elemIdx / 8;\n+\n+  uint32_t myRow = rowOfWarp + (threadIdx & 0xf);\n+  uint32_t myCol = ((threadIdx >> 4) & 0x1) * 8;\n+  myCol = myCol + iterOfCol * 16;\n+\n+  uint32_t offset0 = (myCol / rowStride) * leadingDimOffset;\n+  myCol = myCol % rowStride;\n+\n+  uint32_t phase = (myRow / perPhase) % maxPhase;\n+\n+  uint32_t lineOffset = (myRow % perPhase) * rowStride + myCol;\n+  uint32_t colOffset = ((lineOffset / 8) ^ phase) * 8 + lineOffset % 8;\n+  uint32_t offset1 = (myRow / perPhase) * 64 + colOffset;\n+\n+  return offset1 + offset0;\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) uint32_t\n+__nv_offset_of_stmatrix_v4_no_swizzle(uint32_t threadIdx, uint32_t rowOfWarp,\n+                                      uint32_t elemIdx, uint32_t rowStride) {\n+  uint32_t iterOfCol = elemIdx / 4;\n+  uint32_t myRow = rowOfWarp + (threadIdx & 0xf);\n+  uint32_t myCol = ((threadIdx >> 4) & 0x1) * 8;\n+\n+  myCol = myCol + iterOfCol * 16;\n+  uint32_t offset = myRow * rowStride + myCol * 2;\n+  return offset;\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) void\n+__nv_sts64(uint32_t ptr, uint32_t d0, uint32_t d1) {\n+  asm volatile(\"st.shared.v2.b32 [%0], {%1, %2};\\n\"\n+               :\n+               : \"r\"(ptr), \"r\"(d0), \"r\"(d1));\n+}\n+\n+__DEVICE__ __attribute__((__always_inline__)) uint32_t\n+__nv_offset_of_sts64(uint32_t threadIdx, uint32_t rowOfWarp, int32_t elemIdx,\n+                     uint32_t rowStride) {\n+  uint32_t perPhase = 0;\n+  uint32_t maxPhase = 0;\n+  if (rowStride == 128) {\n+    perPhase = 1;\n+    maxPhase = 8;\n+  } else if (rowStride == 64) {\n+    perPhase = 2;\n+    maxPhase = 4;\n+  } else if (rowStride == 32) {\n+    perPhase = 4;\n+    maxPhase = 2;\n+  }\n+\n+  uint32_t laneId = threadIdx & 0x1f;\n+\n+  uint32_t myRow = ((elemIdx >> 1) & 0x1) * 8 + laneId / 4;\n+  uint32_t myCol = (elemIdx / 4) * 8 + (laneId % 4) * 2;\n+  myRow += rowOfWarp;\n+\n+  uint32_t phase = (myRow / perPhase) % maxPhase;\n+\n+  uint32_t lineOffset = (myRow % perPhase) * rowStride + myCol * 4;\n+  uint32_t colOffset = ((lineOffset / 16) ^ phase) * 16 + lineOffset % 16;\n+  uint32_t offset = (myRow / perPhase) * 128 + colOffset;\n+\n+  return offset;\n+}"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -23,5 +23,11 @@ add_mlir_translation_library(TritonLLVMIR\n         MLIRSCFToControlFlow\n         MLIRSupport\n         MLIRTargetLLVMIRExport\n+        NVGPUToLLVMIR\n         TritonGPUToLLVM\n         )\n+\n+set_source_files_properties(\n+        LLVMIRTranslation.cpp\n+        PROPERTIES\n+        COMPILE_FLAGS \"-D__BUILD_DIR__=\\\\\\\"${CMAKE_BINARY_DIR}\\\\\\\"\")"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 9, "deletions": 3, "changes": 12, "file_content_changes": "@@ -16,7 +16,9 @@\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+#include \"triton/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.h\"\n #include \"triton/Target/LLVMIR/Passes.h\"\n+#include \"triton/Target/PTX/TmaMetadata.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"triton/Tools/Sys/GetPlatform.hpp\"\n #include \"llvm/ADT/APInt.h\"\n@@ -235,8 +237,8 @@ static void linkLibdevice(llvm::Module &module) {\n   module.addModuleFlag(reflect);\n }\n \n-static bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n-                          llvm::StringRef path, bool isROCM) {\n+bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n+                   llvm::StringRef path, bool isROCM) {\n   llvm::SMDiagnostic err;\n   auto &ctx = module.getContext();\n \n@@ -276,6 +278,8 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n   mlir::registerLLVMDialectTranslation(registry);\n   mlir::registerROCDLDialectTranslation(registry);\n   mlir::registerNVVMDialectTranslation(registry);\n+  mlir::registerNVGPUDialectTranslation(registry);\n+\n   module->getContext()->appendDialectRegistry(registry);\n \n   llvm::DenseMap<llvm::StringRef, NVVMMetadata> nvvmMetadata;\n@@ -322,6 +326,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module, int computeCapability,\n+                           mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n                            bool isROCM) {\n   mlir::PassManager pm(module->getContext());\n   mlir::registerPassManagerCLOptions();\n@@ -344,7 +349,8 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n \n   pm.addPass(mlir::createConvertSCFToCFPass());\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n-  pm.addPass(createConvertTritonGPUToLLVMPass(computeCapability, isROCM));\n+  pm.addPass(\n+      createConvertTritonGPUToLLVMPass(computeCapability, &tmaInfos, isROCM));\n   pm.addPass(mlir::createArithToLLVMConversionPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n   // Simplify the IR"}, {"filename": "lib/Target/PTX/CMakeLists.txt", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -7,3 +7,8 @@ add_mlir_translation_library(TritonPTX\n         LINK_LIBS PUBLIC\n         TritonLLVMIR\n         )\n+\n+set_source_files_properties(\n+        PTXTranslation.cpp\n+        PROPERTIES\n+        COMPILE_FLAGS \"-D__BUILD_DIR__=\\\\\\\"${CMAKE_BINARY_DIR}\\\\\\\"\")"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -10,6 +10,11 @@\n #include \"llvm/Support/CommandLine.h\"\n #include \"llvm/Support/TargetSelect.h\"\n #include \"llvm/Target/TargetMachine.h\"\n+#include \"llvm/Transforms/IPO/AlwaysInliner.h\"\n+#include <filesystem>\n+\n+#include <mutex>\n+#include <optional>\n \n #include <mutex>\n #include <optional>\n@@ -38,7 +43,25 @@ static bool findAndReplace(std::string &str, const std::string &begin,\n   return true;\n }\n \n+static void linkExternal(llvm::Module &module) {\n+  namespace fs = std::filesystem;\n+\n+  // TODO: enable generating bc file from clang.\n+  static const auto this_file_path = std::filesystem::path(__FILE__);\n+  static const auto path =\n+      this_file_path.parent_path().parent_path().parent_path().parent_path() /\n+      \"python\" / \"triton\" / \"hopper_lib\" / \"libhopper_helpers.bc\";\n+\n+  // static const std::filesystem::path path =\n+  //     std::filesystem::path(__BUILD_DIR__) / \"lib\" / \"Hopper\" /\n+  //     \"libhopper_helpers.bc\";\n+  if (mlir::triton::linkExternLib(module, \"libhopper_helpers\", path.string(),\n+                                  /*isROCM*/ false))\n+    llvm::errs() << \"Link failed for: libhopper_helpers.bc\";\n+}\n+\n std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n+  linkExternal(module);\n   // LLVM version in use may not officially support target hardware.\n   // Supported versions for LLVM 14 are here:\n   // https://github.com/llvm/llvm-project/blob/f28c006a5895fc0e329fe15fead81e37457cb1d1/clang/include/clang/Basic/BuiltinsNVPTX.def\n@@ -60,9 +83,14 @@ std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n   std::string layout = \"\";\n   std::string features = \"\";\n   // std::string features = \"+ptx\" + std::to_string(maxPTX);\n+  for (llvm::Function &f : module.functions()) {\n+    if (!f.hasFnAttribute(llvm::Attribute::NoInline))\n+      f.addFnAttr(llvm::Attribute::AlwaysInline);\n+  }\n   initLLVM();\n   // verify and store llvm\n   llvm::legacy::PassManager pm;\n+  pm.add(llvm::createAlwaysInlinerLegacyPass());\n   pm.add(llvm::createVerifierPass());\n   pm.run(module);\n   // module->print(llvm::outs(), nullptr);"}, {"filename": "python/setup.py", "status": "modified", "additions": 11, "deletions": 10, "changes": 21, "file_content_changes": "@@ -71,9 +71,7 @@ def get_llvm_package_info():\n     arch = 'x86_64'\n     if system == \"Darwin\":\n         system_suffix = \"apple-darwin\"\n-        cpu_type = os.popen('sysctl machdep.cpu.brand_string').read()\n-        if \"apple\" in cpu_type.lower():\n-            arch = 'arm64'\n+        arch = platform.machine()\n     elif system == \"Linux\":\n         vglibc = tuple(map(int, platform.libc_ver()[1].split('.')))\n         vglibc = vglibc[0] * 100 + vglibc[1]\n@@ -167,11 +165,15 @@ def __init__(self, name, path, sourcedir=\"\"):\n \n class CMakeBuild(build_ext):\n \n-    user_options = build_ext.user_options + [('base-dir=', None, 'base directory of Triton')]\n+    user_options = build_ext.user_options + \\\n+        [('base-dir=', None, 'base directory of Triton')]\n \n     def initialize_options(self):\n         build_ext.initialize_options(self)\n-        self.base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))\n+        self.base_dir = os.path.abspath(\n+            os.path.join(\n+                os.path.dirname(__file__),\n+                os.pardir))\n \n     def finalize_options(self):\n         build_ext.finalize_options(self)\n@@ -180,9 +182,7 @@ def run(self):\n         try:\n             out = subprocess.check_output([\"cmake\", \"--version\"])\n         except OSError:\n-            raise RuntimeError(\n-                \"CMake must be installed to build the following extensions: \" + \", \".join(e.name for e in self.extensions)\n-            )\n+            raise RuntimeError(\"CMake must be installed to build the following extensions: \" + \", \".join(e.name for e in self.extensions))\n \n         match = re.search(r\"version\\s*(?P<major>\\d+)\\.(?P<minor>\\d+)([\\d.]+)?\", out.decode())\n         cmake_major, cmake_minor = int(match.group(\"major\")), int(match.group(\"minor\"))\n@@ -288,7 +288,7 @@ def build_extension(self, ext):\n         \"triton/tools\",\n     ],\n     install_requires=[\n-        \"filelock\",\n+        \"filelock\"\n     ],\n     include_package_data=True,\n     ext_modules=[CMakeExtension(\"triton\", \"triton/_C/\")],\n@@ -311,7 +311,7 @@ def build_extension(self, ext):\n     test_suite=\"tests\",\n     extras_require={\n         \"build\": [\n-            \"cmake>=3.18\",\n+            \"cmake>=3.20\",\n             \"lit\",\n         ],\n         \"tests\": [\n@@ -321,6 +321,7 @@ def build_extension(self, ext):\n             \"numpy\",\n             \"pytest\",\n             \"scipy>=1.7.1\",\n+            \"torch\",\n         ],\n         \"tutorials\": [\n             \"matplotlib\","}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/__init__.py", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/test_flashattention.py", "status": "added", "additions": 480, "deletions": 0, "changes": 480, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/test_gemm.py", "status": "added", "additions": 469, "deletions": 0, "changes": 469, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/test_gemm_fusion.py", "status": "added", "additions": 166, "deletions": 0, "changes": 166, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/test_mixed_io.py", "status": "added", "additions": 89, "deletions": 0, "changes": 89, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_fused-attention.py", "status": "added", "additions": 395, "deletions": 0, "changes": 395, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "added", "additions": 987, "deletions": 0, "changes": 987, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/test_tma_store_gemm.py", "status": "added", "additions": 123, "deletions": 0, "changes": 123, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/ttgir_tests/test_tma.py", "status": "added", "additions": 75, "deletions": 0, "changes": 75, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/ttgir_tests/test_util.py", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/ttgir_tests/wgmma_64_64_16_f16_NT.ttgir", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/ttgir_tests/wgmma_64_64_16_f16_TN.ttgir", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/ttgir_tests/wgmma_a_ldgsts_64_64_16_f16.ttgir", "status": "added", "additions": 59, "deletions": 0, "changes": 59, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/ttgir_tests/wgmma_a_ldgsts_mbarrier_64_64_16_f16.ttgir", "status": "added", "additions": 63, "deletions": 0, "changes": 63, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/ttgir_tests/wgmma_ldgsts_64_64_16_f16.ttgir", "status": "added", "additions": 63, "deletions": 0, "changes": 63, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/ttgir_tests/wgmma_ldgsts_mbarrier_64_64_16_f16.ttgir", "status": "added", "additions": 67, "deletions": 0, "changes": 67, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/ttgir_tests/wgmma_ldgsts_mbarrier_vec_64_64_16_f16.ttgir", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/ttgir_tests/wgmma_tma_64_64_16_f16.ttgir", "status": "added", "additions": 64, "deletions": 0, "changes": 64, "file_content_changes": "N/A"}, {"filename": "python/test/unit/hopper/utils.py", "status": "added", "additions": 32, "deletions": 0, "changes": 32, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/print_helper.py", "status": "modified", "additions": 10, "deletions": 1, "changes": 11, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 287, "deletions": 129, "changes": 416, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "N/A"}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "N/A"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "N/A"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "N/A"}, {"filename": "python/test/unit/runtime/test_launch.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}, {"filename": "python/test/unit/runtime/test_subproc.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "N/A"}, {"filename": "python/test/unit/tools/test_aot.py", "status": "modified", "additions": 89, "deletions": 49, "changes": 138, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 14, "deletions": 6, "changes": 20, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 133, "deletions": 27, "changes": 160, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 47, "deletions": 14, "changes": 61, "file_content_changes": "N/A"}, {"filename": "python/triton/compiler/utils.py", "status": "added", "additions": 297, "deletions": 0, "changes": 297, "file_content_changes": "N/A"}, {"filename": "python/triton/hopper_lib/libhopper_helpers.bc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "N/A"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "N/A"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 32, "deletions": 10, "changes": 42, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 280, "deletions": 0, "changes": 280, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/driver.py", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "N/A"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 67, "deletions": 20, "changes": 87, "file_content_changes": "N/A"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 16, "deletions": 7, "changes": 23, "file_content_changes": "N/A"}, {"filename": "python/triton/third_party/cuda/include/cuda.h", "status": "modified", "additions": 3421, "deletions": 650, "changes": 4071, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/link.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "N/A"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "file_content_changes": "N/A"}, {"filename": "python/tutorials/09-experimental-tma-matrix-multiplication.py", "status": "added", "additions": 188, "deletions": 0, "changes": 188, "file_content_changes": "N/A"}, {"filename": "python/tutorials/10-experimental-tmastg-matrix-multiplication.py", "status": "added", "additions": 191, "deletions": 0, "changes": 191, "file_content_changes": "N/A"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "N/A"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 79, "deletions": 58, "changes": 137, "file_content_changes": "N/A"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 25, "deletions": 25, "changes": 50, "file_content_changes": "N/A"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "N/A"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "N/A"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 290, "deletions": 163, "changes": 453, "file_content_changes": "N/A"}, {"filename": "test/Conversion/tritongpu_to_llvm_hopper.mlir", "status": "added", "additions": 80, "deletions": 0, "changes": 80, "file_content_changes": "N/A"}, {"filename": "test/NVGPU/test_cga.mlir", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "N/A"}, {"filename": "test/NVGPU/test_mbarrier.mlir", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "N/A"}, {"filename": "test/NVGPU/test_tma.mlir", "status": "added", "additions": 47, "deletions": 0, "changes": 47, "file_content_changes": "N/A"}, {"filename": "test/NVGPU/test_wgmma.mlir", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "N/A"}, {"filename": "test/Triton/reorder-broadcast.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "N/A"}, {"filename": "test/Triton/rewrite-tensor-pointer.mlir", "status": "removed", "additions": 0, "deletions": 83, "changes": 83, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 27, "deletions": 9, "changes": 36, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 56, "deletions": 53, "changes": 109, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/loop-pipeline-hopper.mlir", "status": "added", "additions": 305, "deletions": 0, "changes": 305, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 32, "deletions": 26, "changes": 58, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/materialize-load-store.mlir", "status": "added", "additions": 63, "deletions": 0, "changes": 63, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/rewrite-tensor-pointer.mlir", "status": "added", "additions": 64, "deletions": 0, "changes": 64, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/wsdecomposing.mlir", "status": "added", "additions": 754, "deletions": 0, "changes": 754, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/wsmaterialization.mlir", "status": "added", "additions": 414, "deletions": 0, "changes": 414, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/wsmutex.mlir", "status": "added", "additions": 166, "deletions": 0, "changes": 166, "file_content_changes": "N/A"}, {"filename": "test/TritonGPU/wspipeline.mlir", "status": "added", "additions": 148, "deletions": 0, "changes": 148, "file_content_changes": "N/A"}, {"filename": "test/TritonNvidiaGPU/ws-feasibility-checking.mlir", "status": "added", "additions": 896, "deletions": 0, "changes": 896, "file_content_changes": "N/A"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "N/A"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/DumpLayout.cpp", "status": "added", "additions": 383, "deletions": 0, "changes": 383, "file_content_changes": "N/A"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/DumpLayout.h", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "file_content_changes": "N/A"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/EmitIndicesTest.cpp", "status": "added", "additions": 677, "deletions": 0, "changes": 677, "file_content_changes": "N/A"}, {"filename": "unittest/Dialect/TritonGPU/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "N/A"}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "N/A"}]