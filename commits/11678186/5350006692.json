[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -392,6 +392,7 @@ def TT_DotOp : TT_Op<\"dot\", [Pure,\n     let results = (outs TT_FpIntTensor:$d);\n \n     let assemblyFormat = \"$a`,` $b`,` $c attr-dict `:` type($a) `*` type($b) `->` type($d)\";\n+    let hasVerifier = 1;\n }\n \n //"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n namespace mlir {\n namespace triton {\n@@ -398,6 +399,25 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n   return mlir::success();\n }\n \n+LogicalResult mlir::triton::DotOp::verify() {\n+  auto aTy = getOperand(0).getType().cast<RankedTensorType>();\n+  auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n+  if (aTy.getElementType() != bTy.getElementType())\n+    return emitError(\"element types of operands A and B must match\");\n+  auto aEncoding =\n+      aTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  auto bEncoding =\n+      bTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  if (!aEncoding && !bEncoding)\n+    return mlir::success();\n+  // Verify that the encodings are valid.\n+  if (!aEncoding || !bEncoding)\n+    return emitError(\"mismatching encoding between A and B operands\");\n+  if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n+    return emitError(\"mismatching kWidth between A and B operands\");\n+  return mlir::success();\n+}\n+\n //-- ReduceOp --\n static mlir::LogicalResult\n inferReduceReturnShape(const RankedTensorType &argTy, const Type &retEltTy,"}, {"filename": "test/Conversion/invalid.mlir", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+// RUN: triton-opt %s -split-input-file -verify-diagnostics\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=2}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf32, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{element types of operands A and B must match}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf32, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf16>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{mismatching encoding between A and B operands}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf16> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf16, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{mismatching kWidth between A and B operands}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}"}]