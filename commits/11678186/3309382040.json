[{"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 7, "deletions": 8, "changes": 15, "file_content_changes": "@@ -76,8 +76,8 @@\n #\n #  .. code-block:: python\n #\n-#    a_ptrs += BLOCK_SIZE_K * stride_ak;\n-#    b_ptrs += BLOCK_SIZE_K * stride_bk;\n+#    pa += BLOCK_SIZE_K * stride_ak;\n+#    pb += BLOCK_SIZE_K * stride_bk;\n #\n #\n # L2 Cache Optimizations\n@@ -236,8 +236,8 @@ def matmul_kernel(\n         b_ptrs += BLOCK_SIZE_K * stride_bk\n     # you can fuse arbitrary activation functions here\n     # while the accumulator is still in FP32!\n-    if ACTIVATION == \"leaky_relu\":\n-        accumulator = leaky_relu(accumulator)\n+    if ACTIVATION:\n+        accumulator = ACTIVATION(accumulator)\n     c = accumulator.to(tl.float16)\n \n     # -----------------------------------------------------------\n@@ -252,7 +252,6 @@ def matmul_kernel(\n # we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\n @triton.jit\n def leaky_relu(x):\n-    x = x + 1\n     return tl.where(x >= 0, x, 0.01 * x)\n \n \n@@ -261,7 +260,7 @@ def leaky_relu(x):\n # and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel\n \n \n-def matmul(a, b, activation=\"\"):\n+def matmul(a, b, activation=None):\n     # checks constraints\n     assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n     assert a.is_contiguous(), \"matrix A must be contiguous\"\n@@ -297,7 +296,7 @@ def matmul(a, b, activation=\"\"):\n torch.manual_seed(0)\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-triton_output = matmul(a, b)\n+triton_output = matmul(a, b, activation=None)\n torch_output = torch.matmul(a, b)\n print(f\"triton_output={triton_output}\")\n print(f\"torch_output={torch_output}\")\n@@ -347,7 +346,7 @@ def benchmark(M, N, K, provider):\n         )\n     if provider == 'triton + relu':\n         ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: matmul(a, b, activation=\"leaky_relu\")\n+            lambda: matmul(a, b, activation=leaky_relu)\n         )\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)"}]