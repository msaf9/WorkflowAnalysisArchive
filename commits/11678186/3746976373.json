[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -47,6 +47,8 @@ bool supportMMA(triton::DotOp op, int version);\n \n bool supportMMA(Value value, int version);\n \n+Type getElementType(Value value);\n+\n std::string getValueOperandName(Value value, AsmState &state);\n \n template <typename T_OUT, typename T_IN>"}, {"filename": "include/triton/Conversion/MLIRTypes.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -12,6 +12,7 @@ namespace type {\n // Integer types\n // TODO(Superjomn): may change `static` into better implementations\n static Type i32Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 32); }\n+static Type i16Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 16); }\n static Type i8Ty(MLIRContext *ctx) { return IntegerType::get(ctx, 8); }\n static Type u32Ty(MLIRContext *ctx) {\n   return IntegerType::get(ctx, 32, IntegerType::Unsigned);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -134,6 +134,13 @@ bool supportMMA(Value value, int version) {\n          (elemTy.isInteger(8) && version >= 2);\n }\n \n+Type getElementType(Value value) {\n+  auto type = value.getType();\n+  if (auto tensorType = type.dyn_cast<RankedTensorType>())\n+    return tensorType.getElementType();\n+  return type;\n+}\n+\n std::string getValueOperandName(Value value, AsmState &state) {\n   std::string opName;\n   llvm::raw_string_ostream ss(opName);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -26,13 +26,12 @@ bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n \n void storeBlockedToShared(Value src, Value llSrc, ArrayRef<Value> srcStrides,\n                           ArrayRef<Value> srcIndices, Value dst, Value smemBase,\n-                          Type elemPtrTy, Location loc,\n+                          Type elemTy, Location loc,\n                           ConversionPatternRewriter &rewriter) {\n   auto srcTy = src.getType().cast<RankedTensorType>();\n   auto srcShape = srcTy.getShape();\n   assert(srcShape.size() == 2 && \"Unexpected rank of insertSlice\");\n \n-  auto elemTy = srcTy.getElementType();\n   auto dstTy = dst.getType().cast<RankedTensorType>();\n   auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n   auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n@@ -52,6 +51,7 @@ void storeBlockedToShared(Value src, Value llSrc, ArrayRef<Value> srcStrides,\n   auto srcAccumSizeInThreads =\n       product<unsigned>(srcBlockedLayout.getSizePerThread());\n   auto wordTy = vec_ty(elemTy, minVec);\n+  auto elemPtrTy = ptr_ty(elemTy);\n \n   // TODO: [goostavz] We should make a cache for the calculation of\n   // emitBaseIndexForBlockedLayout in case backend compiler not being able to\n@@ -504,7 +504,7 @@ struct ConvertLayoutOpConversion\n     auto srcIndices = emitBaseIndexForBlockedLayout(loc, rewriter,\n                                                     srcBlockedLayout, srcShape);\n     storeBlockedToShared(src, adaptor.src(), srcStrides, srcIndices, dst,\n-                         smemBase, elemPtrTy, loc, rewriter);\n+                         smemBase, elemTy, loc, rewriter);\n \n     auto smemObj =\n         SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -519,7 +519,7 @@ struct DotOpMmaV2ConversionHelper {\n     case TensorCoreType::FP32_FP16_FP16_FP32:\n       return ptr_ty(type::f16Ty(ctx), 3);\n     case TensorCoreType::FP32_BF16_BF16_FP32:\n-      return ptr_ty(type::bf16Ty(ctx), 3);\n+      return ptr_ty(type::i16Ty(ctx), 3);\n     case TensorCoreType::FP32_TF32_TF32_FP32:\n       return ptr_ty(type::f32Ty(ctx), 3);\n     case TensorCoreType::INT32_INT8_INT8_INT32:\n@@ -534,12 +534,13 @@ struct DotOpMmaV2ConversionHelper {\n   Type getMatType() const {\n     Type fp32Ty = type::f32Ty(ctx);\n     Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n-    Type bf16x2Ty = vec_ty(type::bf16Ty(ctx), 2);\n+    Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n     // floating point types\n     Type fp16x2Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n+    // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n     Type bf16x2Pack4Ty =\n-        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, bf16x2Ty));\n+        LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n     Type fp32Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n     // integer types"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 531, "deletions": 8, "changes": 539, "file_content_changes": "@@ -7,6 +7,352 @@ using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getStructFromElements;\n using ::mlir::triton::gpu::getElemsPerThread;\n \n+struct FpToFpOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::FpToFpOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  static SmallVector<Value>\n+  convertFp8x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    Value fp8x4Vec = undef(fp8x4VecTy);\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n+    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm = \"{                                      \\n\"\n+                   \".reg .b32 a<2>, b<2>;                  \\n\"\n+                   \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n+                   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n+                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"shr.b32  b0, b0, 1;                    \\n\"\n+                   \"shr.b32  b1, b1, 1;                    \\n\"\n+                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n+                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n+                   \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o0 = builder.newOperand(\"=r\");\n+    auto *o1 = builder.newOperand(\"=r\");\n+    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n+    call({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    auto fp16x2x2StructTy =\n+        struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n+    auto fp16x2x2Struct =\n+        builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n+    auto fp16x2Vec0 =\n+        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n+    auto fp16x2Vec1 =\n+        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n+            extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n+            extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n+            extract_element(f16_ty, fp16x2Vec1, i32_val(1))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    Value fp16x2Vec0 = undef(fp16x2VecTy);\n+    Value fp16x2Vec1 = undef(fp16x2VecTy);\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n+    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n+    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm = \"{                                      \\n\"\n+                   \".reg .b32 a<2>, b<2>;                  \\n\"\n+                   \"shl.b32 a0, $1, 1;                     \\n\"\n+                   \"shl.b32 a1, $2, 1;                     \\n\"\n+                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"add.u32 a0, a0, 0x00800080;            \\n\"\n+                   \"add.u32 a1, a1, 0x00800080;            \\n\"\n+                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n+                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n+                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+                   \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n+    call({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    Value fp8x4Vec = undef(fp8x4VecTy);\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n+    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm = \"{                                          \\n\"\n+                   \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>;  \\n\"\n+                   \"prmt.b32 a0, 0, $2, 0x5040;                \\n\"\n+                   \"prmt.b32 a1, 0, $2, 0x7060;                \\n\"\n+                   \"and.b32 sign0, a0, 0x80008000;             \\n\"\n+                   \"and.b32 sign1, a1, 0x80008000;             \\n\"\n+                   \"and.b32 nosign0, a0, 0x7fff7fff;           \\n\"\n+                   \"and.b32 nosign1, a1, 0x7fff7fff;           \\n\"\n+                   \"shr.b32 nosign0, nosign0, 4;               \\n\"\n+                   \"shr.b32 nosign1, nosign1, 4;               \\n\"\n+                   \"add.u32 nosign0, nosign0, 0x38003800;      \\n\"\n+                   \"add.u32 nosign1, nosign1, 0x38003800;      \\n\"\n+                   \"or.b32 $0, sign0, nosign0;                 \\n\"\n+                   \"or.b32 $1, sign1, nosign1;                 \\n\"\n+                   \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o0 = builder.newOperand(\"=r\");\n+    auto *o1 = builder.newOperand(\"=r\");\n+    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n+    call({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto bf16x2VecTy = vec_ty(i16_ty, 2);\n+    auto bf16x2x2StructTy =\n+        struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n+    auto bf16x2x2Struct =\n+        builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n+    auto bf16x2Vec0 =\n+        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n+    auto bf16x2Vec1 =\n+        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n+            extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n+            extract_element(i16_ty, bf16x2Vec1, i32_val(0)),\n+            extract_element(i16_ty, bf16x2Vec1, i32_val(1))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto bf16x2VecTy = vec_ty(i16_ty, 2);\n+    Value bf16x2Vec0 = undef(bf16x2VecTy);\n+    Value bf16x2Vec1 = undef(bf16x2VecTy);\n+    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v0, i32_val(0));\n+    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v1, i32_val(1));\n+    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v2, i32_val(0));\n+    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v3, i32_val(1));\n+    bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);\n+    bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm = \"{                                            \\n\"\n+                   \".reg .u32 sign, sign<2>, nosign, nosign<2>;  \\n\"\n+                   \".reg .u32 fp8_min, fp8_max, rn_, zero;       \\n\"\n+                   \"mov.u32 fp8_min, 0x38003800;                 \\n\"\n+                   \"mov.u32 fp8_max, 0x3ff03ff0;                 \\n\"\n+                   \"mov.u32 rn_, 0x80008;                        \\n\"\n+                   \"mov.u32 zero, 0;                             \\n\"\n+                   \"and.b32 sign0, $1, 0x80008000;               \\n\"\n+                   \"and.b32 sign1, $2, 0x80008000;               \\n\"\n+                   \"prmt.b32 sign, sign0, sign1, 0x7531;         \\n\"\n+                   \"and.b32 nosign0, $1, 0x7fff7fff;             \\n\"\n+                   \"and.b32 nosign1, $2, 0x7fff7fff;             \\n\"\n+                   \".reg .u32 nosign_0_<2>, nosign_1_<2>;        \\n\"\n+                   \"and.b32 nosign_0_0, nosign0, 0xffff0000;     \\n\"\n+                   \"max.u32 nosign_0_0, nosign_0_0, 0x38000000;  \\n\"\n+                   \"min.u32 nosign_0_0, nosign_0_0, 0x3ff00000;  \\n\"\n+                   \"and.b32 nosign_0_1, nosign0, 0x0000ffff;     \\n\"\n+                   \"max.u32 nosign_0_1, nosign_0_1, 0x3800;      \\n\"\n+                   \"min.u32 nosign_0_1, nosign_0_1, 0x3ff0;      \\n\"\n+                   \"or.b32 nosign0, nosign_0_0, nosign_0_1;      \\n\"\n+                   \"and.b32 nosign_1_0, nosign1, 0xffff0000;     \\n\"\n+                   \"max.u32 nosign_1_0, nosign_1_0, 0x38000000;  \\n\"\n+                   \"min.u32 nosign_1_0, nosign_1_0, 0x3ff00000;  \\n\"\n+                   \"and.b32 nosign_1_1, nosign1, 0x0000ffff;     \\n\"\n+                   \"max.u32 nosign_1_1, nosign_1_1, 0x3800;      \\n\"\n+                   \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0;      \\n\"\n+                   \"or.b32 nosign1, nosign_1_0, nosign_1_1;      \\n\"\n+                   \"add.u32 nosign0, nosign0, rn_;               \\n\"\n+                   \"add.u32 nosign1, nosign1, rn_;               \\n\"\n+                   \"sub.u32 nosign0, nosign0, 0x38003800;        \\n\"\n+                   \"sub.u32 nosign1, nosign1, 0x38003800;        \\n\"\n+                   \"shr.u32 nosign0, nosign0, 4;                 \\n\"\n+                   \"shr.u32 nosign1, nosign1, 4;                 \\n\"\n+                   \"prmt.b32 nosign, nosign0, nosign1, 0x6420;   \\n\"\n+                   \"or.b32 $0, nosign, sign;                     \\n\"\n+                   \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n+    call({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp32x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp64x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  static Value convertBf16ToFp32(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.rn.f32.bf16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(v, \"h\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, f32_ty, false);\n+  }\n+\n+  static Value convertFp32ToBf16(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.rn.bf16.f32\");\n+    auto res = builder.newOperand(\"=h\");\n+    auto operand = builder.newOperand(v, \"r\");\n+    cvt(res, operand);\n+    // TODO: This is a hack to get the right type. We should be able to invoke\n+    // the type converter\n+    return builder.launch(rewriter, loc, i16_ty, false);\n+  }\n+\n+  LogicalResult\n+  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto srcTensorType = op.from().getType().cast<mlir::RankedTensorType>();\n+    auto dstTensorType = op.result().getType().cast<mlir::RankedTensorType>();\n+    auto srcEltType = srcTensorType.getElementType();\n+    auto dstEltType = dstTensorType.getElementType();\n+    auto loc = op->getLoc();\n+    auto elems = getElemsPerThread(dstTensorType);\n+    SmallVector<Value> resultVals;\n+\n+    // Select convertor\n+    if (srcEltType.isa<triton::Float8Type>() ||\n+        dstEltType.isa<triton::Float8Type>()) {\n+      std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n+                                       const Value &, const Value &,\n+                                       const Value &, const Value &)>\n+          convertor;\n+      if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF16()) {\n+        convertor = convertFp8x4ToFp16x4;\n+      } else if (srcEltType.isF16() && dstEltType.isa<triton::Float8Type>()) {\n+        convertor = convertFp16x4ToFp8x4;\n+      } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isBF16()) {\n+        convertor = convertFp8x4ToBf16x4;\n+      } else if (srcEltType.isBF16() && dstEltType.isa<triton::Float8Type>()) {\n+        convertor = convertBf16x4ToFp8x4;\n+      } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF32()) {\n+        convertor = convertFp8x4ToFp32x4;\n+      } else if (srcEltType.isF32() && dstEltType.isa<triton::Float8Type>()) {\n+        convertor = convertFp32x4ToFp8x4;\n+      } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF64()) {\n+        convertor = convertFp8x4ToFp64x4;\n+      } else if (srcEltType.isF64() && dstEltType.isa<triton::Float8Type>()) {\n+        convertor = convertFp64x4ToFp8x4;\n+      } else {\n+        assert(false && \"unsupported fp8 casting\");\n+      }\n+\n+      // Vectorized casting\n+      assert(elems % 4 == 0 &&\n+             \"FP8 casting only support tensors with 4-aligned sizes\");\n+      auto elements = getElementsFromStruct(loc, adaptor.from(), rewriter);\n+      for (size_t i = 0; i < elems; i += 4) {\n+        auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n+                                   elements[i + 2], elements[i + 3]);\n+        resultVals.append(converted);\n+      }\n+    } else if (srcEltType.isBF16() && dstEltType.isF32()) {\n+      resultVals.emplace_back(convertBf16ToFp32(loc, rewriter, adaptor.from()));\n+    } else if (srcEltType.isF32() && dstEltType.isBF16()) {\n+      resultVals.emplace_back(convertFp32ToBf16(loc, rewriter, adaptor.from()));\n+    } else {\n+      assert(false && \"unsupported type casting\");\n+    }\n+\n+    assert(resultVals.size() == elems);\n+    auto convertedDstTensorType =\n+        this->getTypeConverter()->convertType(dstTensorType);\n+    auto result = getStructFromElements(loc, resultVals, rewriter,\n+                                        convertedDstTensorType);\n+    rewriter.replaceOp(op, result);\n+    return success();\n+  }\n+};\n+\n template <typename SourceOp, typename ConcreteT>\n class ElementwiseOpConversionBase\n     : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n@@ -250,6 +596,179 @@ struct FDivOpConversion\n   }\n };\n \n+struct FMulOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::MulFOp, FMulOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::MulFOp, FMulOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::MulFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto lhsElemTy = getElementType(op.getLhs());\n+    auto rhsElemTy = getElementType(op.getRhs());\n+    if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n+      PTXBuilder builder;\n+      auto ptxAsm = \" { .reg .b16 c;        \\n\"\n+                    \"    mov.b16 c, 0x8000U; \\n\" // 0.0\n+                    \"    fma.rn.bf16 $0, $1, $2, c; } \\n\";\n+      auto &fMul = *builder.create<PTXInstr>(ptxAsm);\n+      auto res = builder.newOperand(\"=h\");\n+      auto lhs = builder.newOperand(operands[0], \"h\");\n+      auto rhs = builder.newOperand(operands[1], \"h\");\n+      fMul({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n+      return builder.launch(rewriter, loc, i16_ty, false);\n+    } else {\n+      return rewriter.create<LLVM::FMulOp>(loc, elemTy, operands[0],\n+                                           operands[1]);\n+    }\n+  }\n+};\n+\n+struct FAddOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::AddFOp, FAddOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::AddFOp, FAddOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::AddFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto lhsElemTy = getElementType(op.getLhs());\n+    auto rhsElemTy = getElementType(op.getRhs());\n+    if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n+      PTXBuilder builder;\n+      auto ptxAsm = \"{ .reg .b16 c;         \\n\"\n+                    \"   mov.b16 c, 0x3f80U; \\n\" // 1.0\n+                    \"   fma.rn.bf16 $0, $1, c, $2; } \\n\";\n+      auto &fAdd = *builder.create<PTXInstr>(ptxAsm);\n+      auto res = builder.newOperand(\"=h\");\n+      auto lhs = builder.newOperand(operands[0], \"h\");\n+      auto rhs = builder.newOperand(operands[1], \"h\");\n+      fAdd({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n+      return builder.launch(rewriter, loc, i16_ty, false);\n+    } else {\n+      return rewriter.create<LLVM::FAddOp>(loc, elemTy, operands[0],\n+                                           operands[1]);\n+    }\n+  }\n+};\n+\n+struct FSubOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::SubFOp, FSubOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::SubFOp, FSubOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::SubFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto lhsElemTy = getElementType(op.getLhs());\n+    auto rhsElemTy = getElementType(op.getRhs());\n+    if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n+      PTXBuilder builder;\n+      auto ptxAsm = \" { .reg .b16 c;         \\n\"\n+                    \"    mov.b16 c, 0xbf80U; \\n\" // -1.0\n+                    \"    fma.rn.bf16 $0, $2, c, $1;} \\n\";\n+      auto &fSub = *builder.create<PTXInstr>(ptxAsm);\n+      auto res = builder.newOperand(\"=h\");\n+      auto lhs = builder.newOperand(operands[0], \"h\");\n+      auto rhs = builder.newOperand(operands[1], \"h\");\n+      fSub({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n+      return builder.launch(rewriter, loc, i16_ty, false);\n+    } else {\n+      return rewriter.create<LLVM::FSubOp>(loc, elemTy, operands[0],\n+                                           operands[1]);\n+    }\n+  }\n+};\n+\n+struct SIToFPOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::SIToFPOp, SIToFPOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::SIToFPOp, SIToFPOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::SIToFPOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto outElemTy = getElementType(op.getOut());\n+    if (outElemTy.isBF16()) {\n+      auto value = rewriter.create<LLVM::SIToFPOp>(loc, f32_ty, operands[0]);\n+      return FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, value);\n+    } else {\n+      return rewriter.create<LLVM::SIToFPOp>(loc, elemTy, operands[0]);\n+    }\n+  }\n+};\n+\n+struct FPToSIOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::FPToSIOp, FPToSIOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::FPToSIOp, FPToSIOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::FPToSIOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto inElemTy = getElementType(op.getIn());\n+    if (inElemTy.isBF16()) {\n+      auto value =\n+          FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0]);\n+      return rewriter.create<LLVM::FPToSIOp>(loc, elemTy, value);\n+    } else {\n+      return rewriter.create<LLVM::FPToSIOp>(loc, elemTy, operands[0]);\n+    }\n+  }\n+};\n+\n+struct ExtFOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::ExtFOp, ExtFOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::ExtFOp, ExtFOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::ExtFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto inElemTy = getElementType(op.getIn());\n+    if (inElemTy.isBF16()) {\n+      auto outElemTy = getElementType(op.getOut());\n+      assert(outElemTy.isF32() && \"unsupported conversion\");\n+      return FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0]);\n+    } else {\n+      return rewriter.create<LLVM::FPExtOp>(loc, elemTy, operands[0]);\n+    }\n+  }\n+};\n+\n+struct TruncFOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::TruncFOp, TruncFOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<mlir::arith::TruncFOp, TruncFOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::TruncFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    auto outElemTy = getElementType(op.getOut());\n+    if (outElemTy.isBF16()) {\n+      auto inElemTy = getElementType(op.getIn());\n+      assert(inElemTy.isF32() && \"unsupported conversion\");\n+      return FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, operands[0]);\n+    } else {\n+      return rewriter.create<LLVM::FPTruncOp>(loc, elemTy, operands[0]);\n+    }\n+  }\n+};\n+\n struct ExpOpConversionApprox\n     : ElementwiseOpConversionBase<mlir::math::ExpOp, ExpOpConversionApprox> {\n   using Base =\n@@ -290,12 +809,8 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n #define POPULATE_BINARY_OP(SRC_OP, DST_OP)                                     \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_BINARY_OP(arith::SubIOp, LLVM::SubOp) // -\n-  POPULATE_BINARY_OP(arith::SubFOp, LLVM::FSubOp)\n   POPULATE_BINARY_OP(arith::AddIOp, LLVM::AddOp) // +\n-  POPULATE_BINARY_OP(arith::AddFOp, LLVM::FAddOp)\n   POPULATE_BINARY_OP(arith::MulIOp, LLVM::MulOp) // *\n-  POPULATE_BINARY_OP(arith::MulFOp, LLVM::FMulOp)\n-  POPULATE_BINARY_OP(arith::DivFOp, LLVM::FDivOp) // /\n   POPULATE_BINARY_OP(arith::DivSIOp, LLVM::SDivOp)\n   POPULATE_BINARY_OP(arith::DivUIOp, LLVM::UDivOp)\n   POPULATE_BINARY_OP(arith::RemFOp, LLVM::FRemOp) // %\n@@ -312,14 +827,10 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_UNARY_OP(arith::TruncIOp, LLVM::TruncOp)\n-  POPULATE_UNARY_OP(arith::TruncFOp, LLVM::FPTruncOp)\n   POPULATE_UNARY_OP(arith::ExtSIOp, LLVM::SExtOp)\n   POPULATE_UNARY_OP(arith::ExtUIOp, LLVM::ZExtOp)\n   POPULATE_UNARY_OP(arith::FPToUIOp, LLVM::FPToUIOp)\n-  POPULATE_UNARY_OP(arith::FPToSIOp, LLVM::FPToSIOp)\n   POPULATE_UNARY_OP(arith::UIToFPOp, LLVM::UIToFPOp)\n-  POPULATE_UNARY_OP(arith::SIToFPOp, LLVM::SIToFPOp)\n-  POPULATE_UNARY_OP(arith::ExtFOp, LLVM::FPExtOp)\n   POPULATE_UNARY_OP(math::LogOp, math::LogOp)\n   POPULATE_UNARY_OP(math::CosOp, math::CosOp)\n   POPULATE_UNARY_OP(math::SinOp, math::SinOp)\n@@ -332,7 +843,19 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n \n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n+\n   patterns.add<FDivOpConversion>(typeConverter, benefit);\n+  patterns.add<FSubOpConversion>(typeConverter, benefit);\n+  patterns.add<FAddOpConversion>(typeConverter, benefit);\n+  patterns.add<FMulOpConversion>(typeConverter, benefit);\n+\n+  patterns.add<ExtFOpConversion>(typeConverter, benefit);\n+  patterns.add<TruncFOpConversion>(typeConverter, benefit);\n+  patterns.add<FPToSIOpConversion>(typeConverter, benefit);\n+  patterns.add<SIToFPOpConversion>(typeConverter, benefit);\n+\n+  patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n+\n   patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n   // ExpOpConversionApprox will try using ex2.approx if the input type is FP32.\n   // For FP64 input type, ExpOpConversionApprox will return failure and"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -634,15 +634,15 @@ struct InsertSliceOpConversion\n     // Compute the offset based on the original strides of the shared memory\n     // object\n     auto offset = dot(rewriter, loc, offsets, smemObj.strides);\n-    auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n-    auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+    auto elemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+    auto elemPtrTy = ptr_ty(elemTy, 3);\n     auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n \n     auto llSrc = adaptor.source();\n     auto srcIndices =\n         emitBaseIndexForBlockedLayout(loc, rewriter, srcLayout, srcShape);\n     storeBlockedToShared(src, llSrc, srcStrides, srcIndices, dst, smemBase,\n-                         elemPtrTy, loc, rewriter);\n+                         elemTy, loc, rewriter);\n     // Barrier is not necessary.\n     // The membar pass knows that it writes to shared memory and will handle it\n     // properly.\n@@ -714,8 +714,7 @@ struct InsertSliceAsyncOpConversion\n     // Compute the offset based on the original dimensions of the shared\n     // memory object\n     auto dstOffset = dot(rewriter, loc, offsetVals, smemObj.strides);\n-    auto dstPtrTy =\n-        ptr_ty(getTypeConverter()->convertType(resTy.getElementType()), 3);\n+    auto dstPtrTy = ptr_ty(resElemTy, 3);\n     Value dstPtrBase = gep(dstPtrTy, smemObj.base, dstOffset);\n \n     // %mask"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 317, "changes": 317, "file_content_changes": "@@ -476,322 +476,6 @@ struct ExtractSliceOpConversion\n   }\n };\n \n-struct FpToFpOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::FpToFpOp> {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  static SmallVector<Value>\n-  convertFp8x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto ctx = rewriter.getContext();\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    Value fp8x4Vec = undef(fp8x4VecTy);\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n-    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n-\n-    PTXBuilder builder;\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n-                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"shr.b32  b0, b0, 1;                    \\n\"\n-                   \"shr.b32  b1, b1, 1;                    \\n\"\n-                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n-                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n-                   \"}\";\n-    auto &call = *builder.create(ptxAsm);\n-\n-    auto *o0 = builder.newOperand(\"=r\");\n-    auto *o1 = builder.newOperand(\"=r\");\n-    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-    call({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n-    auto fp16x2x2StructTy =\n-        struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n-    auto fp16x2x2Struct =\n-        builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n-    auto fp16x2Vec0 =\n-        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-    auto fp16x2Vec1 =\n-        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n-    return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n-            extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n-            extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n-            extract_element(f16_ty, fp16x2Vec1, i32_val(1))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto ctx = rewriter.getContext();\n-    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n-    Value fp16x2Vec0 = undef(fp16x2VecTy);\n-    Value fp16x2Vec1 = undef(fp16x2VecTy);\n-    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n-    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n-    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n-    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n-    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n-    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n-\n-    PTXBuilder builder;\n-    auto *ptxAsm = \"{                                      \\n\"\n-                   \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"shl.b32 a0, $1, 1;                     \\n\"\n-                   \"shl.b32 a1, $2, 1;                     \\n\"\n-                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"add.u32 a0, a0, 0x00800080;            \\n\"\n-                   \"add.u32 a1, a1, 0x00800080;            \\n\"\n-                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n-                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n-                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n-                   \"}\";\n-    auto &call = *builder.create(ptxAsm);\n-\n-    auto *o = builder.newOperand(\"=r\");\n-    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n-    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n-    call({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp8x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto ctx = rewriter.getContext();\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    Value fp8x4Vec = undef(fp8x4VecTy);\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n-    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n-\n-    PTXBuilder builder;\n-    auto *ptxAsm = \"{                                          \\n\"\n-                   \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>;  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5040;                \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7060;                \\n\"\n-                   \"and.b32 sign0, a0, 0x80008000;             \\n\"\n-                   \"and.b32 sign1, a1, 0x80008000;             \\n\"\n-                   \"and.b32 nosign0, a0, 0x7fff7fff;           \\n\"\n-                   \"and.b32 nosign1, a1, 0x7fff7fff;           \\n\"\n-                   \"shr.b32 nosign0, nosign0, 4;               \\n\"\n-                   \"shr.b32 nosign1, nosign1, 4;               \\n\"\n-                   \"add.u32 nosign0, nosign0, 0x38003800;      \\n\"\n-                   \"add.u32 nosign1, nosign1, 0x38003800;      \\n\"\n-                   \"or.b32 $0, sign0, nosign0;                 \\n\"\n-                   \"or.b32 $1, sign1, nosign1;                 \\n\"\n-                   \"}\";\n-    auto &call = *builder.create(ptxAsm);\n-\n-    auto *o0 = builder.newOperand(\"=r\");\n-    auto *o1 = builder.newOperand(\"=r\");\n-    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-    call({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n-\n-    auto bf16x2VecTy = vec_ty(bf16_ty, 2);\n-    auto bf16x2x2StructTy =\n-        struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n-    auto bf16x2x2Struct =\n-        builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n-    auto bf16x2Vec0 =\n-        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n-    auto bf16x2Vec1 =\n-        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n-    return {extract_element(bf16_ty, bf16x2Vec0, i32_val(0)),\n-            extract_element(bf16_ty, bf16x2Vec0, i32_val(1)),\n-            extract_element(bf16_ty, bf16x2Vec1, i32_val(0)),\n-            extract_element(bf16_ty, bf16x2Vec1, i32_val(1))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertBf16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto ctx = rewriter.getContext();\n-    auto bf16x2VecTy = vec_ty(bf16_ty, 2);\n-    Value bf16x2Vec0 = undef(bf16x2VecTy);\n-    Value bf16x2Vec1 = undef(bf16x2VecTy);\n-    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v0, i32_val(0));\n-    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v1, i32_val(1));\n-    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v2, i32_val(0));\n-    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v3, i32_val(1));\n-    bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);\n-    bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n-\n-    PTXBuilder builder;\n-    auto *ptxAsm = \"{                                            \\n\"\n-                   \".reg .u32 sign, sign<2>, nosign, nosign<2>;  \\n\"\n-                   \".reg .u32 fp8_min, fp8_max, rn_, zero;       \\n\"\n-                   \"mov.u32 fp8_min, 0x38003800;                 \\n\"\n-                   \"mov.u32 fp8_max, 0x3ff03ff0;                 \\n\"\n-                   \"mov.u32 rn_, 0x80008;                        \\n\"\n-                   \"mov.u32 zero, 0;                             \\n\"\n-                   \"and.b32 sign0, $1, 0x80008000;               \\n\"\n-                   \"and.b32 sign1, $2, 0x80008000;               \\n\"\n-                   \"prmt.b32 sign, sign0, sign1, 0x7531;         \\n\"\n-                   \"and.b32 nosign0, $1, 0x7fff7fff;             \\n\"\n-                   \"and.b32 nosign1, $2, 0x7fff7fff;             \\n\"\n-                   \".reg .u32 nosign_0_<2>, nosign_1_<2>;        \\n\"\n-                   \"and.b32 nosign_0_0, nosign0, 0xffff0000;     \\n\"\n-                   \"max.u32 nosign_0_0, nosign_0_0, 0x38000000;  \\n\"\n-                   \"min.u32 nosign_0_0, nosign_0_0, 0x3ff00000;  \\n\"\n-                   \"and.b32 nosign_0_1, nosign0, 0x0000ffff;     \\n\"\n-                   \"max.u32 nosign_0_1, nosign_0_1, 0x3800;      \\n\"\n-                   \"min.u32 nosign_0_1, nosign_0_1, 0x3ff0;      \\n\"\n-                   \"or.b32 nosign0, nosign_0_0, nosign_0_1;      \\n\"\n-                   \"and.b32 nosign_1_0, nosign1, 0xffff0000;     \\n\"\n-                   \"max.u32 nosign_1_0, nosign_1_0, 0x38000000;  \\n\"\n-                   \"min.u32 nosign_1_0, nosign_1_0, 0x3ff00000;  \\n\"\n-                   \"and.b32 nosign_1_1, nosign1, 0x0000ffff;     \\n\"\n-                   \"max.u32 nosign_1_1, nosign_1_1, 0x3800;      \\n\"\n-                   \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0;      \\n\"\n-                   \"or.b32 nosign1, nosign_1_0, nosign_1_1;      \\n\"\n-                   \"add.u32 nosign0, nosign0, rn_;               \\n\"\n-                   \"add.u32 nosign1, nosign1, rn_;               \\n\"\n-                   \"sub.u32 nosign0, nosign0, 0x38003800;        \\n\"\n-                   \"sub.u32 nosign1, nosign1, 0x38003800;        \\n\"\n-                   \"shr.u32 nosign0, nosign0, 4;                 \\n\"\n-                   \"shr.u32 nosign1, nosign1, 4;                 \\n\"\n-                   \"prmt.b32 nosign, nosign0, nosign1, 0x6420;   \\n\"\n-                   \"or.b32 $0, nosign, sign;                     \\n\"\n-                   \"}\";\n-    auto &call = *builder.create(ptxAsm);\n-\n-    auto *o = builder.newOperand(\"=r\");\n-    auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n-    auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n-    call({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp8x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-    return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp32x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp8x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-    return {rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp64x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const Value &v0, const Value &v1, const Value &v2,\n-                       const Value &v3) {\n-    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n-  }\n-\n-  LogicalResult\n-  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto srcTensorType = op.from().getType().cast<mlir::RankedTensorType>();\n-    auto dstTensorType = op.result().getType().cast<mlir::RankedTensorType>();\n-    auto srcEltType = srcTensorType.getElementType();\n-    auto dstEltType = dstTensorType.getElementType();\n-    assert(srcEltType.isa<triton::Float8Type>() ||\n-           dstEltType.isa<triton::Float8Type>());\n-    auto convertedDstTensorType =\n-        this->getTypeConverter()->convertType(dstTensorType);\n-    auto convertedDstEleType =\n-        this->getTypeConverter()->convertType(dstEltType);\n-\n-    // Select convertor\n-    std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n-                                     const Value &, const Value &,\n-                                     const Value &, const Value &)>\n-        convertor;\n-    if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF16()) {\n-      convertor = convertFp8x4ToFp16x4;\n-    } else if (srcEltType.isF16() && dstEltType.isa<triton::Float8Type>()) {\n-      convertor = convertFp16x4ToFp8x4;\n-    } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isBF16()) {\n-      convertor = convertFp8x4ToBf16x4;\n-    } else if (srcEltType.isBF16() && dstEltType.isa<triton::Float8Type>()) {\n-      convertor = convertBf16x4ToFp8x4;\n-    } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF32()) {\n-      convertor = convertFp8x4ToFp32x4;\n-    } else if (srcEltType.isF32() && dstEltType.isa<triton::Float8Type>()) {\n-      convertor = convertFp32x4ToFp8x4;\n-    } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF64()) {\n-      convertor = convertFp8x4ToFp64x4;\n-    } else if (srcEltType.isF64() && dstEltType.isa<triton::Float8Type>()) {\n-      convertor = convertFp64x4ToFp8x4;\n-    } else {\n-      assert(false && \"unsupported type casting\");\n-    }\n-\n-    // Vectorized casting\n-    auto loc = op->getLoc();\n-    auto elems = getElemsPerThread(dstTensorType);\n-    assert(elems % 4 == 0 &&\n-           \"FP8 casting only support tensors with 4-aligned sizes\");\n-    auto elements = getElementsFromStruct(loc, adaptor.from(), rewriter);\n-    SmallVector<Value> resultVals;\n-    for (size_t i = 0; i < elems; i += 4) {\n-      auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n-                                 elements[i + 2], elements[i + 3]);\n-      resultVals.append(converted);\n-    }\n-    assert(resultVals.size() == elems);\n-    auto result = getStructFromElements(loc, resultVals, rewriter,\n-                                        convertedDstTensorType);\n-    rewriter.replaceOp(op, result);\n-    return success();\n-  }\n-};\n-\n struct AsyncWaitOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::AsyncWaitOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n@@ -825,7 +509,6 @@ void populateTritonGPUToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<AllocTensorOpConversion>(typeConverter, allocation, smem,\n                                         benefit);\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n-  patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n \n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -119,13 +119,14 @@ class ConvertTritonGPUToLLVM\n     // Step 3: Allocate shared memories and insert barriers\n     // Step 4: Convert SCF to CFG\n     // Step 5: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // Step 6: Convert the rest of ops via partial conversion\n+    // Step 6: Get axis and shared memory info\n+    // Step 7: Convert the rest of ops via partial conversion\n     //\n-    // The reason for putting step 1 before step 2 is that the membar\n+    // The reason for putting step 3 before step 4 is that the membar\n     // analysis currently only supports SCF but not CFG. The reason for a\n-    // separation between 1/4 is that, step 3 is out of the scope of Dialect\n+    // separation between 5/7 is that, step 6 is out of the scope of Dialect\n     // Conversion, thus we need to make sure the smem is not revised during the\n-    // conversion of step 4.\n+    // conversion of step 7.\n \n     // Step 1\n     decomposeMmaToDotOperand(mod, numWarps);\n@@ -165,7 +166,7 @@ class ConvertTritonGPUToLLVM\n                  mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n                                         allocation.getSharedMemorySize()));\n \n-    // Step 6 - rewrite rest of ops\n+    // Step 7 - rewrite rest of ops\n     // We set a higher benefit here to ensure triton's patterns runs before\n     // arith patterns for some encoding not supported by the community\n     // patterns."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -37,6 +37,10 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n     addConversion([&](triton::Float8Type type) -> llvm::Optional<Type> {\n       return IntegerType::get(type.getContext(), 8);\n     });\n+    // Internally store bfloat16 as int16\n+    addConversion([&](BFloat16Type type) -> llvm::Optional<Type> {\n+      return IntegerType::get(type.getContext(), 16);\n+    });\n   }\n \n   Type convertTritonPointerType(triton::PointerType type) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -72,6 +72,7 @@\n \n // Types\n #define i32_ty rewriter.getIntegerType(32)\n+#define i16_ty rewriter.getIntegerType(16)\n #define ui32_ty rewriter.getIntegerType(32, false)\n #define f16_ty rewriter.getF16Type()\n #define bf16_ty rewriter.getBF16Type()"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -113,10 +113,10 @@ struct ArithConstantSplatOpConversion\n     auto elemType = values.getElementType();\n \n     Attribute val;\n-    if (type::isInt(elemType)) {\n-      val = values.getValues<IntegerAttr>()[0];\n-    } else if (type::isFloat(elemType)) {\n+    if (elemType.isBF16() || type::isFloat(elemType)) {\n       val = values.getValues<FloatAttr>()[0];\n+    } else if (type::isInt(elemType)) {\n+      val = values.getValues<IntegerAttr>()[0];\n     } else {\n       llvm::errs() << \"ArithConstantSplatOpConversion get unsupported type: \"\n                    << value.getType() << \"\\n\";"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 128, "deletions": 129, "changes": 257, "file_content_changes": "@@ -18,9 +18,8 @@\n uint_dtypes = ['uint8', 'uint16', 'uint32', 'uint64']\n float_dtypes = ['float16', 'float32', 'float64']\n dtypes = int_dtypes + uint_dtypes + float_dtypes\n-# TODO: handle bfloat16\n-dtypes_with_bfloat16 = dtypes  # + ['bfloat16']\n-torch_dtypes = ['bool'] + int_dtypes + ['uint8'] + float_dtypes  # + ['bfloat16']\n+dtypes_with_bfloat16 = dtypes + ['bfloat16']\n+torch_dtypes = ['bool'] + int_dtypes + ['uint8'] + float_dtypes + ['bfloat16']\n \n \n def _bitwidth(dtype: str) -> int:\n@@ -448,9 +447,9 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n     z = np.where(0, x, 0)\n     assert (z == to_numpy(z_tri)).all()\n \n-# # ---------------\n-# # test unary ops\n-# # ---------------\n+# ---------------\n+# test unary ops\n+# ---------------\n \n \n @pytest.mark.parametrize(\"dtype_x, expr\", [\n@@ -461,9 +460,9 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n def test_unary_op(dtype_x, expr, device='cuda'):\n     _test_unary(dtype_x, expr, device=device)\n \n-# # ----------------\n-# # test math ops\n-# # ----------------\n+# ----------------\n+# test math ops\n+# ----------------\n \n \n @pytest.mark.parametrize(\"expr\", [\n@@ -473,9 +472,9 @@ def test_math_op(expr, device='cuda'):\n     _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n \n-# # ----------------\n-# # test indexing\n-# # ----------------\n+# ----------------\n+# test indexing\n+# ----------------\n \n \n def make_ptr_str(name, shape):\n@@ -492,11 +491,10 @@ def make_ptr_str(name, shape):\n # TODO: handle `%4 = triton_gpu.convert_layout %3 : (tensor<32xi32, #blocked0>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>``\n @pytest.mark.parametrize(\"expr, dtype_str\", [\n     (f'x[{s}]', d)\n-    for s in ['None, :', ':, None',\n-              # TODO: 3D\n-              #  'None, :, :',\n-              #  ':, :, None'\n-              ]\n+    for s in ['None, :', ':, None']\n+    # FIXME: 3d indexing doesn't work\n+    #'None, :, :',\n+    # ':, :, None']\n     for d in ['int32', 'uint32', 'uint16']\n ])\n def test_index1d(expr, dtype_str, device='cuda'):\n@@ -551,9 +549,9 @@ def catch_compilation_error(kernel):\n     catch_compilation_error(kernel_rank_mismatch)\n \n \n-# # ---------------\n-# # test tuples\n-# # ---------------\n+# ---------------\n+# test tuples\n+# ---------------\n \n \n @triton.jit\n@@ -713,28 +711,30 @@ def serialized_add(data, Lock):\n     triton.testing.assert_almost_equal(data, ref)\n \n \n-# # ---------------\n-# # test cast\n-# # ---------------\n+# ---------------\n+# test cast\n+# ---------------\n \n \n @pytest.mark.parametrize(\"dtype_x, dtype_z, bitcast\", [\n     (dtype_x, dtype_z, False)\n     for dtype_x in dtypes\n     for dtype_z in dtypes\n ] + [\n-    # TODO:\n-    # ('float32', 'bfloat16', False),\n-    # ('bfloat16', 'float32', False),\n+    ('float32', 'bfloat16', False),\n+    ('bfloat16', 'float32', False),\n     ('float32', 'int32', True),\n-    # TODO:\n     ('float32', 'int1', False),\n ] + [\n     (f'uint{x}', f'int{x}', True) for x in [8, 16, 32, 64]\n ] + [\n     (f'int{x}', f'uint{x}', True) for x in [8, 16, 32, 64]\n ])\n def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n+    # bfloat16 on cc < 80 will not be tested\n+    check_type_supported(dtype_x)\n+    check_type_supported(dtype_z)\n+\n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     x0 = 43 if dtype_x in int_dtypes else 43.5\n     if dtype_x in float_dtypes and dtype_z == 'int1':\n@@ -877,9 +877,9 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n \n \n-# # ---------------\n-# # test reduce\n-# # ---------------\n+# ---------------\n+# test reduce\n+# ---------------\n \n \n def get_reduced_dtype(dtype_str, op):\n@@ -892,7 +892,6 @@ def get_reduced_dtype(dtype_str, op):\n     return dtype_str\n \n \n-# TODO: [Qingyi] Fix argmin / argmax\n @pytest.mark.parametrize(\"op, dtype_str, shape\",\n                          [(op, dtype, shape)\n                           for op in ['min', 'max', 'sum']\n@@ -970,6 +969,8 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n+    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+\n     # triton kernel\n     @triton.jit\n     def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n@@ -1021,9 +1022,9 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n         else:\n             np.testing.assert_equal(z_ref, z_tri)\n \n-# # ---------------\n-# # test permute\n-# # ---------------\n+# ---------------\n+# test permute\n+# ---------------\n \n \n @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n@@ -1070,9 +1071,9 @@ def kernel(X, stride_xm, stride_xn,\n     assert 'ld.global.v4' in ptx\n     assert 'st.global.v4' in ptx\n \n-# # ---------------\n-# # test dot\n-# # ---------------\n+# ---------------\n+# test dot\n+# ---------------\n \n \n @pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype\",\n@@ -1097,9 +1098,6 @@ def kernel(X, stride_xm, stride_xn,\n                           for col_b in [True, False]\n                           for dtype in ['int8', 'float16', 'float32']])\n def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, device='cuda'):\n-    # TODO: fma bug for some shapes and transposition modes?\n-    # if dtype == 'float32' and not allow_tf32:\n-    #     pytest.skip(\"Seems to have bugs\")\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n@@ -1230,24 +1228,24 @@ def kernel(X, stride_xm, stride_xk,\n     elif dtype == 'int8':\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n-\n+# FIXME: Unsupported layout found in ConvertSplatLikeOp\n # def test_dot_without_load():\n-#     @triton.jit\n-#     def kernel(out):\n-#         pid = tl.program_id(axis=0)\n-#         a = tl.zeros((32, 32), tl.float32)\n-#         b = tl.zeros((32, 32), tl.float32)\n-#         c = tl.zeros((32, 32), tl.float32)\n-#         c = tl.dot(a, b)\n-#         pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-#         tl.store(pout, c)\n-\n-#     out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n-#     kernel[(1,)](out)\n+#    @triton.jit\n+#    def kernel(out):\n+#        pid = tl.program_id(axis=0)\n+#        a = tl.zeros((32, 32), tl.float32)\n+#        b = tl.zeros((32, 32), tl.float32)\n+#        c = tl.zeros((32, 32), tl.float32)\n+#        c = tl.dot(a, b)\n+#        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+#        tl.store(pout, c)\n+#\n+#    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n+#    kernel[(1,)](out)\n \n-# # ---------------\n-# # test arange\n-# # ---------------\n+# ---------------\n+# test arange\n+# ---------------\n \n \n @pytest.mark.parametrize(\"start\", [0, 1, 7, 16])\n@@ -1300,57 +1298,57 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n     reference_out = torch.cat((reference_out, torch.ones((size_diff,), dtype=dtype, device=device)))\n     triton.testing.allclose(output, reference_out)\n \n-# # 'bfloat16': torch.bfloat16,\n-# # Testing masked loads with an intermate copy to shared memory run.\n+# 'bfloat16': torch.bfloat16,\n+# Testing masked loads with an intermate copy to shared memory run.\n \n \n-# @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n-# def test_masked_load_shared_memory(dtype, device='cuda'):\n-#     check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+@pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n+def test_masked_load_shared_memory(dtype, device='cuda'):\n+    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n \n-#     M = 32\n-#     N = 32\n-#     K = 16\n+    M = 32\n+    N = 32\n+    K = 16\n \n-#     in1 = torch.rand((M, K), dtype=dtype, device=device)\n-#     in2 = torch.rand((K, N), dtype=dtype, device=device)\n-#     out = torch.zeros((M, N), dtype=dtype, device=device)\n+    in1 = torch.rand((M, K), dtype=dtype, device=device)\n+    in2 = torch.rand((K, N), dtype=dtype, device=device)\n+    out = torch.zeros((M, N), dtype=dtype, device=device)\n \n-#     @triton.jit\n-#     def _kernel(in1_ptr, in2_ptr, output_ptr,\n-#                 in_stride, in2_stride, out_stride,\n-#                 in_numel, in2_numel, out_numel,\n-#                 M: tl.constexpr, N: tl.constexpr, K: tl.constexpr):\n+    @triton.jit\n+    def _kernel(in1_ptr, in2_ptr, output_ptr,\n+                in_stride, in2_stride, out_stride,\n+                in_numel, in2_numel, out_numel,\n+                M: tl.constexpr, N: tl.constexpr, K: tl.constexpr):\n \n-#         M_offsets = tl.arange(0, M)\n-#         N_offsets = tl.arange(0, N)\n-#         K_offsets = tl.arange(0, K)\n+        M_offsets = tl.arange(0, M)\n+        N_offsets = tl.arange(0, N)\n+        K_offsets = tl.arange(0, K)\n \n-#         in_offsets = M_offsets[:, None] * in_stride + K_offsets[None, :]\n-#         in2_offsets = K_offsets[:, None] * in2_stride + N_offsets[None, :]\n+        in_offsets = M_offsets[:, None] * in_stride + K_offsets[None, :]\n+        in2_offsets = K_offsets[:, None] * in2_stride + N_offsets[None, :]\n \n-#         # Load inputs.\n-#         x = tl.load(in1_ptr + in_offsets, mask=in_offsets < in_numel)\n-#         w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < in2_numel)\n+        # Load inputs.\n+        x = tl.load(in1_ptr + in_offsets, mask=in_offsets < in_numel)\n+        w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < in2_numel)\n \n-#         # Without a dot product the memory doesn't get promoted to shared.\n-#         o = tl.dot(x, w)\n+        # Without a dot product the memory doesn't get promoted to shared.\n+        o = tl.dot(x, w)\n \n-#         # Store output\n-#         output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]\n-#         tl.store(output_ptr + output_offsets, o, mask=output_offsets < in2_numel)\n+        # Store output\n+        output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]\n+        tl.store(output_ptr + output_offsets, o, mask=output_offsets < in2_numel)\n \n-#     pgm = _kernel[(1,)](in1, in2, out,\n-#                         in1.stride()[0],\n-#                         in2.stride()[0],\n-#                         out.stride()[0],\n-#                         in1.numel(),\n-#                         in2.numel(),\n-#                         out.numel(),\n-#                         M=M, N=N, K=K)\n+    pgm = _kernel[(1,)](in1, in2, out,\n+                        in1.stride()[0],\n+                        in2.stride()[0],\n+                        out.stride()[0],\n+                        in1.numel(),\n+                        in2.numel(),\n+                        out.numel(),\n+                        M=M, N=N, K=K)\n \n-#     reference_out = torch.matmul(in1, in2)\n-#     triton.testing.allclose(out, reference_out)\n+    reference_out = torch.matmul(in1, in2)\n+    triton.testing.allclose(out, reference_out)\n \n \n @pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])\n@@ -1394,26 +1392,27 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n     else:\n         assert \"ld.global.b32\" in ptx\n     # triton.testing.assert_almost_equal(dst, src[:N])\n-# # ---------------\n-# # test store\n-# # ---------------\n \n-# # ---------------\n-# # test if\n-# # ---------------\n+# ---------------\n+# test store\n+# ---------------\n+\n+# ---------------\n+# test if\n+# ---------------\n \n-# # ---------------\n-# # test for\n-# # ---------------\n+# ---------------\n+# test for\n+# ---------------\n \n-# # ---------------\n-# # test while\n-# # ---------------\n+# ---------------\n+# test while\n+# ---------------\n \n-# # ---------------\n-# # test default\n-# # ---------------\n-# # TODO: can't be local to test_default\n+# ---------------\n+# test default\n+# ---------------\n+# TODO: can't be local to test_default\n \n \n @triton.jit\n@@ -1435,9 +1434,9 @@ def _kernel(ret0, ret1, value):\n     assert ret0.item() == 10\n     assert ret1.item() == value\n \n-# # ---------------\n-# # test noop\n-# # ----------------\n+# ---------------\n+# test noop\n+# ----------------\n \n \n def test_noop(device='cuda'):\n@@ -1471,9 +1470,9 @@ def kernel(VALUE, X):\n     JITFunction.cache_hook = None\n     assert spec_type == value_type\n \n-# # --------------------\n-# # value specialization\n-# # --------------------\n+# --------------------\n+# value specialization\n+# --------------------\n \n \n @pytest.mark.parametrize(\n@@ -1495,9 +1494,9 @@ def kernel(VALUE, X):\n         kernel[(1, )](value, x)\n \n \n-# # ----------------\n-# # test constexpr\n-# # ----------------\n+# ----------------\n+# test constexpr\n+# ----------------\n \n @pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>'])\n @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n@@ -1548,9 +1547,9 @@ def kernel(X, s):\n     kernel[(1,)](x_tri, 32)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256) % 8)\n \n-# # -------------\n-# # test call\n-# # -------------\n+# -------------\n+# test call\n+# -------------\n \n \n @triton.jit\n@@ -1584,9 +1583,9 @@ def kernel(ptr, n_elements, num1, num2):\n     ans = rand_val * 1 * 2 * 1 * 2 * 3 * 4\n     np.testing.assert_equal(to_numpy(rand_val_tri), ans)\n \n-# # -------------\n-# # test if\n-# # -------------\n+# -------------\n+# test if\n+# -------------\n \n \n def test_if():\n@@ -1620,9 +1619,9 @@ def _kernel(dst):\n     _kernel[(1,)](dst=dst, num_warps=2)\n     _kernel[(1,)](dst=dst, num_warps=4)\n \n-# # -------------\n-# # test extern\n-# # -------------\n+# -------------\n+# test extern\n+# -------------\n \n \n def system_libdevice_path() -> str:"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -71,10 +71,8 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8 and DTYPE == \"bfloat16\":\n         pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n-    # if DTYPE == \"bfloat16\" and SPLIT_K != 1:\n-    #    pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n-    if DTYPE == \"bfloat16\":\n-        pytest.skip(\"bfloat16 matmuls doesn't support for now\")\n+    if DTYPE == \"bfloat16\" and SPLIT_K != 1:\n+        pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n     torch.manual_seed(0)\n     # nuke kernel decorators -- will set meta-parameters manually\n     kwargs = {'BLOCK_M': BLOCK_M, 'BLOCK_N': BLOCK_N, 'BLOCK_K': BLOCK_K, 'SPLIT_K': SPLIT_K}"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -637,9 +637,9 @@ def cast(input: tl.tensor,\n         return tl.tensor(builder.create_fp_to_fp(input.handle, dst_ty.to_ir(builder)),\n                          dst_ty)\n \n-    # Casting types of the same bit width: fp16 <=> bf16\n-    if (src_sca_ty.is_fp16() and dst_sca_ty.is_bf16()) or \\\n-       (src_sca_ty.is_bf16() and dst_sca_ty.is_fp16()):\n+    # bf16 <=> (not fp32)\n+    if (src_sca_ty.is_fp16() and not dst_sca_ty.is_fp32()) or \\\n+       (src_sca_ty.is_bf16() and not dst_sca_ty.is_fp32()):\n         return cast(cast(input, tl.float32, builder), dst_sca_ty, builder)\n \n     # Standard floating types' casting: truncation"}]