[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -152,7 +152,7 @@ jobs:\n         if: ${{ matrix.runner != 'macos-10.15' && (matrix.runner[1] != 'arc770') }}\n         run: |\n           python3 -m pip install --upgrade pre-commit\n-          python3 -m pre_commit run --all-files\n+          python3 -m pre_commit run --all-files --verbose\n \n       - name: Check pre-commit arc770\n         if: ${{ matrix.runner != 'macos-10.15' && (matrix.runner[1] == 'arc770') }}"}, {"filename": "CONTRIBUTING.md", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -106,7 +106,7 @@ triton\n \u2502   \u2502\t\u251c\u2500\u2500language: core of triton language, load tensors to SRAM, language logic, etc.\n \u2502   \u2502\t\u2502\n \u2502   \u2502\t\u251c\u2500\u2500ops: contains functions for flash-attn, softmax, cross-entropy and other torch.nn.F functions\n-\u2502   \u2502\t\u251c\u2500\u2500runtime: contains impl jit compilation, autotuning, backend drivers,cahcing, error handles, etc.\n+\u2502   \u2502\t\u251c\u2500\u2500runtime: contains impl jit compilation, autotuning, backend drivers, caching, error handles, etc.\n \u2502   \u2502\t\u251c\u2500\u2500third_party\n \u2502   \u2502\t\u251c\u2500\u2500tools\n \u2502   \u251c\u2500\u2500 triton.egg-info"}, {"filename": "docs/meetups/07-18-2023.md", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "file_content_changes": "@@ -1,4 +1,9 @@\n #### Agenda:\n+\n+##### Announcements:\n+1. Triton conference planned mid September in the Microsoft Silicon Valley Campus.\n+\n+##### Items:\n 1. Alternative backend development approach (e.g. AMD, Intel)\n 2. State of the documentation, is there a planned effort? If yes, what do you think is the priority?\n 3. Mechanisms for smaller technical discussions: Slack channel per topic? Dedicated meetings for some topics?\n@@ -12,3 +17,28 @@\n    - Sm75.\n    - device functions. How hard is this to support while Triton frontend traverses AST?\n    - remove torch dependencies from the frontend. (it sounds like there is already progress on this but could be worth discussing)\n+\n+##### Minutes\n+Recording link [here](https://drive.google.com/file/d/1uMlIvih_E5FITwPnNHwTYzo-UKqtey2c/view)\n+\n+1. Backend plans/broader roadmap:\n+   - Plan is for major updates to come in the Triton development meetup which will happen mid-September. For major design changes, currently the plan is to not upstream them directly but have a staging state and different backends can be integrated through a plugin mechanism where Triton provides a layer at the Triton IR layer that is generic and other backends can plug into that.\n+   - Short term roadmap plans are very focused on things like improving all FP8 things on Ampere and Hopper support (end of August). After Hopper support lands, priorities will include refactoring codebase to increase maintainability.\n+   - Linalg \u2013 upstreaming on hold due to limited dev bandwidth. Want to build an ecosystem where others can leverage Linalg like passes developed in their backend.\n+   - For now, peak performance on Nvidia GPUs needs Nvidia specific things, but the convergence of programming models for different backends will allow convergence of hardware backend support in Triton.\n+2. Documentation:\n+   - OpenAI has included comments in the backend code.\n+   - Seek community involvement to improve tutorials, based on new users knowing what is missing.\n+   - Seek community involvement for signature changes and doc updates.\n+   - Thread created in slack for suggestions on areas needing doc updates. Ian Bearman and his team may have bandwidth to update certain documentation.\n+3. Discussion channels:\n+   - Preferred #dev channel in slack for technical discussions.\n+   - Between GitHub and Slack it would be good to post links into places so folks know discussions are happening elsewhere\n+4. CI/testing:\n+   - Pretty liberal in terms of accepting regression tests and integration tests for Nvidia.\n+   - Plugin interface tested like everything else, and regressions there would block merges into main.\n+   - Correctness/Performance of external backends are tested nightly, but regressions do not prevent wheels from being built.\n+5. Language improvements:\n+   - Have added location information support into Triton codegen.\n+   - Feel free to bring up pain points in slack.\n+7. Windows Support: Technically not difficult to get a preliminary version. Most of the maintenance burden would come from having to support it when it breaks."}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -37,6 +37,8 @@ class ReduceOpHelper {\n \n   bool isFastReduction();\n \n+  bool isWarpSynchronous();\n+\n   unsigned getInterWarpSize();\n \n   unsigned getIntraWarpSize();"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -469,6 +469,36 @@ class SplatOpAxisInfoVisitor final\n   }\n };\n \n+class LoadOpAxisInfoVisitor final : public AxisInfoVisitorImpl<triton::LoadOp> {\n+public:\n+  using AxisInfoVisitorImpl<triton::LoadOp>::AxisInfoVisitorImpl;\n+\n+  AxisInfo\n+  getAxisInfo(triton::LoadOp op,\n+              ArrayRef<const dataflow::Lattice<AxisInfo> *> operands) override {\n+    // If pointers and mask both have constancy properties, those properties\n+    // will also extend to output.\n+    AxisInfo ptrInfo = operands[0]->getValue();\n+    std::optional<AxisInfo> maskInfo;\n+    if (operands.size() > 1) {\n+      maskInfo = operands[1]->getValue();\n+    }\n+    AxisInfo::DimVectorT contiguity;\n+    AxisInfo::DimVectorT divisibility;\n+    AxisInfo::DimVectorT constancy;\n+\n+    for (int d = 0; d < ptrInfo.getRank(); ++d) {\n+      contiguity.push_back(1);\n+      divisibility.push_back(1);\n+      constancy.push_back(\n+          gcd(ptrInfo.getConstancy(d),\n+              maskInfo.has_value() ? maskInfo->getConstancy(d) : 0));\n+    }\n+\n+    return AxisInfo(contiguity, divisibility, constancy);\n+  }\n+};\n+\n class ExpandDimsOpAxisInfoVisitor final\n     : public AxisInfoVisitorImpl<triton::ExpandDimsOp> {\n public:\n@@ -871,6 +901,7 @@ AxisInfoAnalysis::AxisInfoAnalysis(DataFlowSolver &solver)\n                   MaxMinOpAxisInfoVisitor<arith::MaxUIOp>,\n                   MaxMinOpAxisInfoVisitor<arith::MinSIOp>,\n                   MaxMinOpAxisInfoVisitor<arith::MinUIOp>>();\n+  visitors.append<LoadOpAxisInfoVisitor>();\n }\n \n void AxisInfoAnalysis::visitOperation("}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "file_content_changes": "@@ -81,14 +81,20 @@ SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {\n   return smemShape;\n }\n \n+bool ReduceOpHelper::isWarpSynchronous() {\n+  auto argsLayout = getSrcLayout();\n+  return isFastReduction() &&\n+         (triton::gpu::getWarpsPerCTA(argsLayout)[axis] == 1);\n+}\n+\n SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n   SmallVector<SmallVector<unsigned>> smemShapes(3);\n \n   auto argLayout = getSrcLayout();\n   auto argLayoutMma = argLayout.dyn_cast<triton::gpu::MmaEncodingAttr>();\n \n   // that case doesn't need inter-warp communication\n-  if (isFastReduction() && triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n+  if (isWarpSynchronous())\n     return {{0, 0}, {0, 0}};\n \n   /// shared memory block0"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -439,12 +439,15 @@ struct ConvertLayoutOpConversion\n     }\n     // Potentially we need to store for multiple CTAs in this replication\n     auto accumNumReplicates = product<unsigned>(numReplicates);\n-    // unsigned elems = getTotalElemsPerThread(srcTy);\n     auto vals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n                                                      rewriter, srcTy);\n     unsigned inVec = 0;\n     unsigned outVec = 0;\n     auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n+    if (getElementTypeOrSelf(op.getType()).isa<mlir::Float8E4M3B11FNUZType>()) {\n+      assert(inVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n+      assert(outVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n+    }\n \n     unsigned outElems = getTotalElemsPerThread(dstTy);\n     auto outOrd = getOrder(dstLayout);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -335,7 +335,9 @@ struct ReduceOpConversion\n     unsigned sizeInterWarps = helper.getInterWarpSizeWithUniqueData();\n \n     SmallVector<Value> smemBases(op.getNumOperands());\n-    if (sizeInterWarps > 1) {\n+    bool isWarpSync = helper.isWarpSynchronous();\n+\n+    if (!isWarpSync) {\n       smemBases[0] = bitcast(\n           getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys[0]);\n       for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n@@ -403,7 +405,7 @@ struct ReduceOpConversion\n         accumulate(rewriter, *combineOp, acc, shfl, false);\n       }\n \n-      if (sizeInterWarps == 1) {\n+      if (isWarpSync) {\n         finalAccs[key] = acc;\n         continue;\n       }\n@@ -418,7 +420,7 @@ struct ReduceOpConversion\n       }\n     }\n \n-    if (sizeInterWarps == 1) {\n+    if (isWarpSync) {\n       SmallVector<Value> results(op.getNumOperands());\n       for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n         if (auto resultTy ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -313,6 +313,7 @@ class ConvertTritonGPUToLLVM\n     int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n \n     // Preprocess\n+    decomposeFp8e4b15Convert(mod);\n     decomposeMmaToDotOperand(mod, numWarps, threadsPerWarp);\n     decomposeBlockedToDotOperand(mod);\n     decomposeInsertSliceAsyncOp(mod);\n@@ -442,6 +443,33 @@ class ConvertTritonGPUToLLVM\n                                         allocation.getSharedMemorySize()));\n   }\n \n+  void decomposeFp8e4b15Convert(ModuleOp mod) const {\n+    mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n+      OpBuilder builder(cvtOp);\n+      if (!getElementTypeOrSelf(cvtOp).isa<mlir::Float8E4M3B11FNUZType>())\n+        return;\n+      auto shape = cvtOp.getType().cast<RankedTensorType>().getShape();\n+      auto argEncoding =\n+          cvtOp.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+      auto cvtEncoding = cvtOp.getType().cast<RankedTensorType>().getEncoding();\n+      if (argEncoding.isa<triton::gpu::DotOperandEncodingAttr>() ||\n+          cvtEncoding.isa<triton::gpu::DotOperandEncodingAttr>())\n+        return;\n+      auto F16Ty = builder.getF16Type();\n+\n+      auto newArgType = RankedTensorType::get(shape, F16Ty, argEncoding);\n+      auto newCvtType = RankedTensorType::get(shape, F16Ty, cvtEncoding);\n+      auto newArg = builder.create<mlir::triton::FpToFpOp>(\n+          cvtOp.getLoc(), newArgType, cvtOp.getOperand());\n+      auto newCvt = builder.create<mlir::triton::gpu::ConvertLayoutOp>(\n+          cvtOp.getLoc(), newCvtType, newArg);\n+      auto newRet = builder.create<mlir::triton::FpToFpOp>(\n+          cvtOp.getLoc(), cvtOp.getType(), newCvt.getResult());\n+      cvtOp.replaceAllUsesWith(newRet.getResult());\n+      cvtOp.erase();\n+    });\n+  }\n+\n   void decomposeMmaToDotOperand(ModuleOp mod, int numWarps,\n                                 int threadsPerWarp) const {\n     // Replace `mma -> dot_op` with `mma -> blocked -> dot_op`"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -809,11 +809,11 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n                                     nextIV, newForOp.getUpperBound());\n \n   pipelineIterIdx = newForOp.getRegionIterArgs()[ivIndex + 1];\n-  Value insertSliceIndex = builder.create<arith::RemUIOp>(\n+  Value insertSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n   loopIterIdx = newForOp.getRegionIterArgs()[ivIndex + 2];\n-  Value extractSliceIndex = builder.create<arith::RemUIOp>(\n+  Value extractSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n "}, {"filename": "python/README.md", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/setup.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -125,8 +125,8 @@ def download_and_copy_ptxas():\n \n     base_dir = os.path.dirname(__file__)\n     src_path = \"bin/ptxas\"\n-    version = \"12.2.91\"\n-    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.2.0/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n+    version = \"12.1.105\"\n+    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n     dst_prefix = os.path.join(base_dir, \"triton\")\n     dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n     dst_path = os.path.join(dst_prefix, dst_suffix)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 35, "deletions": 21, "changes": 56, "file_content_changes": "@@ -44,6 +44,9 @@ def numpy_random(shape, dtype_str, rs: Optional[RandomState] = None, low=None, h\n         x = rs.randint(low, high, shape, dtype=dtype)\n         x[x == 0] = 1  # Hack. Never return zero so tests of division don't error out.\n         return x\n+    elif dtype_str and 'float8' in dtype_str:\n+        x = rs.randint(20, 40, shape, dtype=np.int8)\n+        return x\n     elif dtype_str in float_dtypes:\n         return rs.normal(0, 1, shape).astype(dtype_str)\n     elif dtype_str == 'bfloat16':\n@@ -67,6 +70,8 @@ def to_triton(x: np.ndarray, device='cuda', dst_type=None) -> Union[TensorWrappe\n         x_signed = x.astype(getattr(np, signed_type_name))\n         return reinterpret(torch.tensor(x_signed, device=device), getattr(tl, t))\n     else:\n+        if dst_type and 'float8' in dst_type:\n+            return reinterpret(torch.tensor(x, device=device), getattr(tl, dst_type))\n         if t == 'float32' and dst_type == 'bfloat16':\n             return torch.tensor(x, device=device).bfloat16()\n         return torch.tensor(x, device=device)\n@@ -1276,9 +1281,23 @@ def serialize_fp8(np_data, in_dtype):\n     else:\n         return np_data\n \n+# inverse of `serialize_fp8`\n+\n+\n+def deserialize_fp8(np_data, in_dtype):\n+    if in_dtype == tl.float8e4b15:\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n+        signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n+        bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n+\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n-@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"\n     For all possible float8 values (ref_fp8 = range(0, 256)), test that:\n@@ -1287,13 +1306,6 @@ def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     this is only possible if both conversions are correct\n     \"\"\"\n     check_type_supported(out_dtype, device)\n-    from contextlib import nullcontext as does_not_raise\n-    expectation = does_not_raise()\n-    err_msg = None\n-    if (in_dtype == tl.float8e4b15 and out_dtype != torch.float16) or\\\n-       (in_dtype != torch.float16 and out_dtype == tl.float8e4b15):\n-        expectation = pytest.raises(triton.CompilationError)\n-        err_msg = \"fp8e4b15 can only be converted to/from fp16\"\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -1312,19 +1324,15 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     ref_fp8[is_subnormal] = 0\n     tri_fp8 = torch.from_numpy(serialize_fp8(ref_fp8, in_dtype)).cuda()\n     tri_fp16 = torch.empty(256, dtype=out_dtype, device=\"cuda\")\n-    with expectation as e:\n-        copy_kernel[(1,)](triton.reinterpret(tri_fp8, in_dtype), tri_fp16, tri_fp16.shape[0], BLOCK_SIZE=1024)\n+    copy_kernel[(1,)](triton.reinterpret(tri_fp8, in_dtype), tri_fp16, tri_fp16.shape[0], BLOCK_SIZE=1024)\n \n-        ref_fp8 = torch.from_numpy(ref_fp8).cuda()\n-        ref_fp16 = convert_float_to_float32(ref_fp8, in_dtype)\n-        assert torch.all(tri_fp16[~is_subnormal] == ref_fp16[~is_subnormal])\n+    ref_fp8 = torch.from_numpy(ref_fp8).cuda()\n+    ref_fp16 = convert_float_to_float32(ref_fp8, in_dtype)\n+    assert torch.all(tri_fp16[~is_subnormal] == ref_fp16[~is_subnormal])\n \n-        ref_fp8 = torch.empty_like(tri_fp16, dtype=torch.int8)\n-        copy_kernel[(1,)](tri_fp16, triton.reinterpret(ref_fp8, in_dtype), tri_fp16.shape[0], BLOCK_SIZE=1024)\n-        assert torch.all(tri_fp8 == ref_fp8)\n-\n-    if err_msg is not None:\n-        assert err_msg in str(e)\n+    ref_fp8 = torch.empty_like(tri_fp16, dtype=torch.int8)\n+    copy_kernel[(1,)](tri_fp16, triton.reinterpret(ref_fp8, in_dtype), tri_fp16.shape[0], BLOCK_SIZE=1024)\n+    assert torch.all(tri_fp8 == ref_fp8)\n \n # ---------------\n # test reduce\n@@ -1901,7 +1909,7 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n                          [(dtype, shape, perm)\n                           # TODO: bfloat16\n-                          for dtype in ['float16', 'float32']\n+                          for dtype in ['float8e4b15', 'float16', 'float32']\n                              for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n def test_permute(dtype_str, shape, perm, device):\n@@ -1930,7 +1938,13 @@ def kernel(X, stride_xm, stride_xn,\n                                     z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n                                     BLOCK_M=shape[0], BLOCK_N=shape[1])\n     # numpy result\n-    z_ref = x.transpose(*perm)\n+    if dtype_str == 'float8e4b15':\n+        ty = tl.float8e4b15\n+        z_ref = serialize_fp8(deserialize_fp8(x, ty).T.copy(), ty)\n+        z_tri = z_tri.base\n+        z_tri_contiguous = z_tri_contiguous.base\n+    else:\n+        z_ref = x.transpose(*perm)\n     # compare\n     np.testing.assert_allclose(to_numpy(z_tri), z_ref)\n     np.testing.assert_allclose(to_numpy(z_tri_contiguous), z_ref)"}, {"filename": "python/triton/common/backend.py", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -1,6 +1,10 @@\n \n+import functools\n import importlib\n import importlib.util\n+import os\n+import re\n+import subprocess\n from typing import Dict\n \n from ..runtime.driver import DriverBase\n@@ -94,3 +98,22 @@ def get_backend(device_type: str):\n         else:\n             return None\n     return _backends[device_type] if device_type in _backends else None\n+\n+\n+@functools.lru_cache()\n+def path_to_ptxas():\n+    base_dir = os.path.join(os.path.dirname(__file__), os.pardir)\n+    paths = [\n+        os.environ.get(\"TRITON_PTXAS_PATH\", \"\"),\n+        os.path.join(base_dir, \"third_party\", \"cuda\", \"bin\", \"ptxas\")\n+    ]\n+\n+    for ptxas in paths:\n+        ptxas_bin = ptxas.split(\" \")[0]\n+        if os.path.exists(ptxas_bin) and os.path.isfile(ptxas_bin):\n+            result = subprocess.check_output([ptxas_bin, \"--version\"], stderr=subprocess.STDOUT)\n+            if result is not None:\n+                version = re.search(r\".*release (\\d+\\.\\d+).*\", result.decode(\"utf-8\"), flags=re.MULTILINE)\n+                if version is not None:\n+                    return ptxas, version.group(1)\n+    raise RuntimeError(\"Cannot find ptxas\")"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 1, "deletions": 20, "changes": 21, "file_content_changes": "@@ -15,7 +15,7 @@\n                                    get_shared_memory_size, ir,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n-from ..common.backend import get_backend\n+from ..common.backend import get_backend, path_to_ptxas\n # from ..runtime import driver, jit, JITFunction\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n@@ -127,25 +127,6 @@ def ptx_get_version(cuda_version) -> int:\n     raise RuntimeError(\"Triton only support CUDA 10.0 or higher\")\n \n \n-@functools.lru_cache()\n-def path_to_ptxas():\n-    base_dir = os.path.join(os.path.dirname(__file__), os.pardir)\n-    paths = [\n-        os.environ.get(\"TRITON_PTXAS_PATH\", \"\"),\n-        os.path.join(base_dir, \"third_party\", \"cuda\", \"bin\", \"ptxas\")\n-    ]\n-\n-    for ptxas in paths:\n-        ptxas_bin = ptxas.split(\" \")[0]\n-        if os.path.exists(ptxas_bin) and os.path.isfile(ptxas_bin):\n-            result = subprocess.check_output([ptxas_bin, \"--version\"], stderr=subprocess.STDOUT)\n-            if result is not None:\n-                version = re.search(r\".*release (\\d+\\.\\d+).*\", result.decode(\"utf-8\"), flags=re.MULTILINE)\n-                if version is not None:\n-                    return ptxas, version.group(1)\n-    raise RuntimeError(\"Cannot find ptxas\")\n-\n-\n def llir_to_ptx(mod: Any, arch: int, ptx_version: int = None) -> str:\n     '''\n     Translate TritonGPU module to PTX code."}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -687,11 +687,6 @@ def cast(input: tl.tensor,\n         warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n                       \"Please use tl.float8e4b15.\", DeprecationWarning)\n \n-    # Unsupported conversion:\n-    if (src_sca_ty.is_fp8e4b15() and not dst_sca_ty.is_fp16()) or \\\n-       (dst_sca_ty.is_fp8e4b15() and not src_sca_ty.is_fp16()):\n-        raise ValueError('fp8e4b15 can only be converted to/from fp16')\n-\n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n     if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n        (src_sca_ty.is_floating() and dst_sca_ty.is_fp8()):"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -11,7 +11,7 @@\n from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n                     overload)\n \n-from ..common.backend import get_backend\n+from ..common.backend import get_backend, path_to_ptxas\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n TRITON_VERSION = \"2.1.0\"\n@@ -117,10 +117,8 @@ def version_key():\n         with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n             contents += [hashlib.md5(f.read()).hexdigest()]\n     # ptxas version\n-    try:\n-        ptxas_version = hashlib.md5(subprocess.check_output([\"ptxas\", \"--version\"])).hexdigest()\n-    except Exception:\n-        ptxas_version = ''\n+    ptxas = path_to_ptxas()[0]\n+    ptxas_version = hashlib.md5(subprocess.check_output([ptxas, \"--version\"])).hexdigest()\n     return '-'.join(TRITON_VERSION) + '-' + ptxas_version + '-' + '-'.join(contents)\n \n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 60, "deletions": 100, "changes": 160, "file_content_changes": "@@ -26,7 +26,7 @@ def max_fn(x, y):\n @triton.jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,\n-    L, M,\n+    L,\n     Out,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n@@ -35,7 +35,7 @@ def _fwd_kernel(\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n-    MODE: tl.constexpr,\n+    IS_CAUSAL: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n@@ -64,90 +64,65 @@ def _fwd_kernel(\n         block_shape=(BLOCK_N, BLOCK_DMODEL),\n         order=(1, 0)\n     )\n-    O_block_ptr = tl.make_block_ptr(\n-        base=Out + qvk_offset,\n-        shape=(N_CTX, BLOCK_DMODEL),\n-        strides=(stride_om, stride_on),\n-        offsets=(start_m * BLOCK_M, 0),\n-        block_shape=(BLOCK_M, BLOCK_DMODEL),\n-        order=(1, 0)\n-    )\n     # initialize offsets\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     offs_n = tl.arange(0, BLOCK_N)\n     # initialize pointer to m and l\n     m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n     l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-    # causal check on every loop iteration can be expensive\n-    # and peeling the last iteration of the loop does not work well with ptxas\n-    # so we have a mode to do the causal check in a separate kernel entirely\n-    if MODE == 0:  # entire non-causal attention\n-        lo, hi = 0, N_CTX\n-    if MODE == 1:  # entire causal attention\n-        lo, hi = 0, (start_m + 1) * BLOCK_M\n-    if MODE == 2:  # off band-diagonal\n-        lo, hi = 0, start_m * BLOCK_M\n-    if MODE == 3:  # on band-diagonal\n-        l_ptrs = L + off_hz * N_CTX + offs_m\n-        m_ptrs = M + off_hz * N_CTX + offs_m\n-        m_i = tl.load(m_ptrs)\n-        l_i = tl.load(l_ptrs)\n-        acc += tl.load(O_block_ptr).to(tl.float32)\n-        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n     # scale sm_scale by log_2(e) and use\n     # 2^x instead of exp in the loop because CSE and LICM\n     # don't work as expected with `exp` in the loop\n     qk_scale = sm_scale * 1.44269504\n     # load q: it will stay in SRAM throughout\n     q = tl.load(Q_block_ptr)\n     q = (q * qk_scale).to(tl.float16)\n-    # advance block pointers to first iteration of the loop\n-    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n-    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n+    lo = 0\n+    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n     for start_n in range(lo, hi, BLOCK_N):\n-        start_n = tl.multiple_of(start_n, BLOCK_N)\n-        # -- compute qk ----\n+        # -- load k, v --\n         k = tl.load(K_block_ptr)\n+        v = tl.load(V_block_ptr)\n+        # -- compute qk ---\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, k)\n-        if MODE == 1 or MODE == 3:\n+        if IS_CAUSAL:\n             qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        # -- compute m_ij, p, l_ij\n-        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n-        p = tl.math.exp2(qk - m_ij[:, None])\n-        l_ij = tl.sum(p, 1)\n-        # -- update m_i and l_i\n-        alpha = tl.math.exp2(m_i - m_ij)\n-        l_i *= alpha\n-        l_i_new = l_i + l_ij\n-        # scale acc\n-        acc_scale = l_i * 0 + alpha\n-        acc = acc * acc_scale[:, None]\n-        # update acc\n-        v = tl.load(V_block_ptr)\n-        p = p.to(tl.float16)\n-        acc += tl.dot(p, v)\n-        # update m_i and l_i\n-        l_i = l_i_new\n-        m_i = m_ij\n+        qk += tl.dot(q, k)\n+        # -- compute scaling constant ---\n+        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n+        alpha = tl.math.exp2(m_i - m_i_new)\n+        p = tl.math.exp2(qk - m_i_new[:, None])\n+        # -- scale and update acc --\n+        acc_scale = l_i * 0 + alpha  # workaround some compiler bug\n+        acc *= acc_scale[:, None]\n+        acc += tl.dot(p.to(tl.float16), v)\n+        # -- update m_i and l_i --\n+        l_i = l_i * alpha + tl.sum(p, 1)\n+        m_i = m_i_new\n         # update pointers\n         K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n         V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     acc = acc / l_i[:, None]\n     l_ptrs = L + off_hz * N_CTX + offs_m\n-    m_ptrs = M + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, l_i)\n-    tl.store(m_ptrs, m_i)\n+    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n     # write back O\n+    O_block_ptr = tl.make_block_ptr(\n+        base=Out + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_om, stride_on),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n     tl.store(O_block_ptr, acc.to(tl.float16))\n \n \n @triton.jit\n def _bwd_preprocess(\n-    Out, DO, L,\n+    Out, DO,\n     NewDO, Delta,\n     BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n ):\n@@ -156,9 +131,7 @@ def _bwd_preprocess(\n     # load\n     o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n     do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n-    denom = tl.load(L + off_m).to(tl.float32)\n     # compute\n-    do = do / denom[:, None]\n     delta = tl.sum(o * do, axis=1)\n     # write-back\n     tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n@@ -169,7 +142,7 @@ def _bwd_preprocess(\n def _bwd_kernel(\n     Q, K, V, sm_scale, Out, DO,\n     DQ, DK, DV,\n-    L, M,\n+    L,\n     D,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n@@ -178,7 +151,7 @@ def _bwd_kernel(\n     num_block,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n-    MODE: tl.constexpr,\n+    CAUSAL: tl.constexpr,\n ):\n     off_hz = tl.program_id(0)\n     off_z = off_hz // H\n@@ -193,10 +166,10 @@ def _bwd_kernel(\n     DK += off_z * stride_qz + off_h * stride_qh\n     DV += off_z * stride_qz + off_h * stride_qh\n     for start_n in range(0, num_block):\n-        if MODE == 0:\n-            lo = 0\n-        else:\n+        if CAUSAL:\n             lo = start_n * BLOCK_M\n+        else:\n+            lo = 0\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n         offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n@@ -210,7 +183,7 @@ def _bwd_kernel(\n         dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n         # pointer to row-wise quantities in value-like data\n         D_ptrs = D + off_hz * N_CTX\n-        m_ptrs = M + off_hz * N_CTX\n+        l_ptrs = L + off_hz * N_CTX\n         # initialize dv amd dk\n         dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n         dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n@@ -223,16 +196,14 @@ def _bwd_kernel(\n             # load q, k, v, do on-chip\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n-            # NOTE: `do` is pre-divided by `l`; no normalization here\n-            # if MODE == 1:\n-            if MODE == 1:\n+            if CAUSAL:\n                 qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n             else:\n                 qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n             qk += tl.dot(q, tl.trans(k))\n             qk *= qk_scale\n-            m = tl.load(m_ptrs + offs_m_curr)\n-            p = tl.math.exp2(qk - m[:, None])\n+            l_i = tl.load(l_ptrs + offs_m_curr)\n+            p = tl.math.exp2(qk - l_i[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n             dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n@@ -275,29 +246,23 @@ def forward(ctx, q, k, v, causal, sm_scale):\n         BLOCK_N = 64\n         grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n \n         num_warps = 4 if Lk <= 64 else 8\n-        if causal:\n-            modes = [1] if q.shape[2] <= 2048 else [2, 3]\n-        else:\n-            modes = [0]\n-        for mode in modes:\n-            _fwd_kernel[grid](\n-                q, k, v, sm_scale,\n-                L, m,\n-                o,\n-                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-                q.shape[0], q.shape[1], q.shape[2],\n-                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n-                MODE=mode,\n-                num_warps=num_warps,\n-                num_stages=4)\n-\n-        ctx.save_for_backward(q, k, v, o, L, m)\n+        _fwd_kernel[grid](\n+            q, k, v, sm_scale,\n+            L,\n+            o,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n+            IS_CAUSAL=causal,\n+            num_warps=num_warps,\n+            num_stages=4)\n+\n+        ctx.save_for_backward(q, k, v, o, L)\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n@@ -307,36 +272,31 @@ def forward(ctx, q, k, v, causal, sm_scale):\n     @staticmethod\n     def backward(ctx, do):\n         BLOCK = 128\n-        q, k, v, o, l, m = ctx.saved_tensors\n+        q, k, v, o, L = ctx.saved_tensors\n         do = do.contiguous()\n         dq = torch.zeros_like(q, dtype=torch.float32)\n         dk = torch.empty_like(k)\n         dv = torch.empty_like(v)\n         do_scaled = torch.empty_like(do)\n-        delta = torch.empty_like(l)\n-        if ctx.causal:\n-            mode = 1\n-        else:\n-            mode = 0\n+        delta = torch.empty_like(L)\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n-            o, do, l,\n+            o, do,\n             do_scaled, delta,\n             BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n         _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n             dq, dk, dv,\n-            l, m,\n-            delta,\n+            L, delta,\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             q.shape[0], q.shape[1], q.shape[2],\n             ctx.grid[0],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n-            MODE=mode,\n+            CAUSAL=ctx.causal,\n             num_stages=1,\n         )\n         return dq, dk, dv, None, None\n@@ -397,7 +357,7 @@ def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n     args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n-) for mode in ['fwd'] for causal in [False]]\n+) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n \n \n @triton.testing.perf_report(configs)"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 29, "deletions": 0, "changes": 29, "file_content_changes": "@@ -402,6 +402,35 @@ tt.func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32\n \n // -----\n \n+// CHECK-LABEL: @load_constancy\n+tt.func @load_constancy(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 1 : i32}) {\n+  // CHECK: divisibility = [16]\n+  %sixteen = arith.constant dense<16> : tensor<1024xi32>\n+  // CHECK-NEXT: divisibility = [8]\n+  %eight = arith.constant dense<8> : tensor<1024xi32>\n+  // CHECK-NEXT: contiguity = [1024], divisibility = [1073741824], constancy = [1]\n+  %1 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [16]\n+  %2 = arith.divsi %1, %sixteen : tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [1024]\n+  %3 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n+  // CHECK-NEXT: constancy = [1024]\n+  %4 = tt.splat %arg1 : (i32) -> tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [8]\n+  %5 = arith.divsi %1, %eight : tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [8]\n+  %6 = arith.cmpi slt, %5, %4 : tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [16]\n+  %7 = tt.addptr %3, %2 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n+  // CHECK-NEXT: constancy = [16]\n+  %8 = tt.load %7 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+  // CHECK-NEXT: constancy = [8]\n+  %9 = tt.load %7, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n+  tt.return\n+}\n+\n+// -----\n+\n // This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n // CHECK-LABEL: @store_constant_align\n tt.func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,4 +1,3 @@\n-// RUN: triton-opt %s -split-input-file -canonicalize -triton-combine\n // RUN: triton-opt %s -split-input-file -canonicalize -triton-combine | FileCheck %s\n \n // CHECK-LABEL: @test_combine_dot_add_pattern"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -37,8 +37,8 @@\n // CHECK:   %[[arg_b0_dot_op_0:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   %[[arg_b0_dot_op_1:.*]] = arith.mulf %[[arg_b0_dot_op_0]]\n // CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op_1]], {{.*}}\n-// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remui %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remui %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n@@ -110,8 +110,8 @@ tt.func @matmul_loop(%lb : index, %ub : index, %step : index,\n // CHECK:     %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n // CHECK:     %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:     tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n-// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remui %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remui %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:     %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     triton_gpu.async_wait {num = 2 : i32}\n@@ -179,8 +179,8 @@ tt.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   tt.dot {{.*}}, %[[arg_b0_dot_op]], {{.*}}\n-// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remui %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remui %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 1 : i32}\n // CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]"}]