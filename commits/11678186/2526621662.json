[{"filename": "lib/codegen/analysis/layout.cc", "status": "modified", "additions": 1, "deletions": 10, "changes": 11, "file_content_changes": "@@ -212,18 +212,9 @@ mma_layout::mma_layout(size_t num_warps,\n     order_ = {0, 1};\n   }\n   else{\n+    spw_ = mma_instr_shape_.at(tensor_core_type_); // e.g., {16, 8, 16} for f32.f16.f16.f32\n     contig_per_thread_ = {1, 2};\n     order_ = {1, 0};\n-    // axis 1 is repeated 2 times so each warp has a 16x16 tile\n-    // that can be directly reused as the LHS of another dot\n-    bool is_bf16fp16 = tensor_core_type_ == mma_layout::FP32_FP16_FP16_FP32 ||\n-                       tensor_core_type_ == mma_layout::FP32_BF16_BF16_FP32;\n-    rep_ = {1, 1};\n-    if(is_bf16fp16) \n-      rep_ = {1,  2};\n-    // e.g., {16, 8, 16} for f32.f16.f16.f32\n-    auto instr_shape = mma_instr_shape_.at(tensor_core_type_); \n-    spw_ = {instr_shape[0]*rep_[0], instr_shape[1]*rep_[1]};\n   }\n \n   /* warps per tile */"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 14, "deletions": 17, "changes": 31, "file_content_changes": "@@ -1627,12 +1627,12 @@ void generator::visit_mma884(ir::dot_inst* C, ir::value *A, ir::value *B, ir::va\n namespace {\n class mma16816_smem_loader {\n public:\n-  mma16816_smem_loader(int wpt, int rep, std::vector<int> order, int k_order, \n+  mma16816_smem_loader(int wpt, std::vector<int> order, int k_order, \n                        std::vector<unsigned> tile_shape, \n                        std::vector<int> instr_shape, std::vector<int> mat_shape, \n                        int per_phase, int max_phase, int dtsize, Builder *builder, \n                        adder add, multiplier mul, geper gep)\n-                      : wpt_(wpt), rep_(rep), order_(order), k_order_(k_order), tile_shape_(tile_shape),\n+                      : wpt_(wpt), order_(order), k_order_(k_order), tile_shape_(tile_shape),\n                         instr_shape_(instr_shape), mat_shape_(mat_shape), \n                         per_phase_(per_phase), max_phase_(max_phase), dtsize_(dtsize), builder_(builder),\n                         add(add), mul(mul), gep(gep) {\n@@ -1669,8 +1669,8 @@ class mma16816_smem_loader {\n \n   std::vector<Value*> compute_offs(Value *warp_off, Value *lane) {\n     // TODO: this needs to be moved to constructor (and extracted to arr_order)\n-    mat_arr_stride_  = (k_order_ == 1 || dtsize_ == 2) ? 1 : wpt_;\n-    warp_off_stride_ = rep_ * instr_shape_[k_order_^1] / mat_shape_[k_order_^1];\n+    mat_arr_stride_  = (k_order_ == 1) ? 1 : wpt_;\n+    warp_off_stride_ = instr_shape_[k_order_^1] / mat_shape_[k_order_^1];\n     // start matrix logic offset (rename it as base_mat_off?)\n     Value *mat_off[2] = {nullptr, nullptr};\n \n@@ -1949,7 +1949,6 @@ class mma16816_smem_loader {\n \n private:\n   int wpt_;\n-  int rep_;\n   std::vector<int> order_;\n   int k_order_;\n   std::vector<unsigned> tile_shape_;\n@@ -2015,8 +2014,8 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n   const int mat_shape_k = mat_shape[2];\n \n \n-  const int num_rep_m = shapes[0] / layout->shape_per_cta(0) * layout->rep(0);\n-  const int num_rep_n = shapes[1] / layout->shape_per_cta(1) * layout->rep(1);\n+  const int num_rep_m = shapes[0] / layout->shape_per_cta(0);\n+  const int num_rep_n = shapes[1] / layout->shape_per_cta(1);\n   const int num_rep_k = std::max<int>(NK/mma_instr_k, 1);\n \n   // floating point types\n@@ -2114,7 +2113,7 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n   if(is_a_shared) {\n     const int per_phase_a = swizzle_->get_per_phase(layout_a);\n     const int max_phase_a = swizzle_->get_max_phase(layout_a);\n-    mma16816_smem_loader a_loader(layout->wpt(0), layout->rep(0), ord_a, /*k_order*/1, shape_a, \n+    mma16816_smem_loader a_loader(layout->wpt(0), ord_a, /*k_order*/1, shape_a, \n                                   {mma_instr_m, mma_instr_k}, {mat_shape_m, mat_shape_k}, \n                                   per_phase_a, max_phase_a, dtsize_a, builder_, add, mul, gep);\n     std::vector<Value*> off_a = a_loader.compute_offs(warp_m, lane);\n@@ -2182,7 +2181,7 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n     // std::swap(ord_b[0], ord_b[1]);\n     // std::swap(shape_b[0], shape_b[1]);\n   // }\n-  mma16816_smem_loader b_loader(layout->wpt(1), layout->rep(1), ord_b, k_order_b, shape_b,\n+  mma16816_smem_loader b_loader(layout->wpt(1), ord_b, k_order_b, shape_b,\n                                 mma_instr_b, mat_shape_b,\n                                 per_phase_b, max_phase_b, dtsize_b, builder_, add, mul, gep);\n   std::vector<Value*> off_b = b_loader.compute_offs(warp_n, lane);\n@@ -3341,15 +3340,13 @@ void generator::visit_layout_mma(analysis::mma_layout* layout) {\n     // c offset\n     Value *off_c_m = add(udiv(lane, _4), off_warp_m);\n     Value *off_c_n = add(mul(_2, urem(lane, _4)), off_warp_n);\n-    for(unsigned m = 0; m < shape[0]; m+=layout->shape_per_cta(0))\n-    for(unsigned mm = 0; mm < layout->rep(0); mm++){\n-      idx_m.push_back(add(off_c_m, i32(m + mm*layout->spw(0)/layout->rep(0) + 0)));\n-      idx_m.push_back(add(off_c_m, i32(m + mm*layout->spw(0)/layout->rep(0) + 8)));\n+    for(unsigned m = 0; m < shape[0]; m+=layout->shape_per_cta(0)){\n+      idx_m.push_back(add(off_c_m, i32(m)));\n+      idx_m.push_back(add(off_c_m, i32(m + 8)));\n     }\n-    for(unsigned n = 0; n < shape[1]; n+=layout->shape_per_cta(1))\n-    for(unsigned nn = 0; nn < layout->rep(1); nn++){\n-      idx_n.push_back(add(off_c_n, i32(n + nn*layout->spw(1)/layout->rep(1) + 0)));\n-      idx_n.push_back(add(off_c_n, i32(n + nn*layout->spw(1)/layout->rep(1) + 1)));\n+    for(unsigned n = 0; n < shape[1]; n+=layout->shape_per_cta(1)){\n+      idx_n.push_back(add(off_c_n, i32(n)));\n+      idx_n.push_back(add(off_c_n, i32(n + 1)));\n     }\n     /* axes */\n     axes_[layout->get_axis(0)] = distributed_axis{1, idx_m, warp_0};"}]