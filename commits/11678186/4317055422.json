[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -1242,6 +1242,9 @@ def kernel(X, stride_xm, stride_xk,\n \n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n def test_full(dtype_str):\n+    dtype = getattr(torch, dtype_str)\n+    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+\n     @triton.jit\n     def kernel_static(out):\n         a = GENERATE_TEST_HERE\n@@ -1255,9 +1258,9 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n         tl.store(out_ptr, a)\n \n     kernel_static_patched = patch_kernel(kernel_static, {'GENERATE_TEST_HERE': f\"tl.full((128,), 2, tl.{dtype_str})\"})\n-    out_static = torch.zeros((128), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+    out_static = torch.zeros((128), dtype=dtype, device=\"cuda\")\n     kernel_static_patched[(1,)](out_static)\n-    out_dynamic = torch.zeros((128), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+    out_dynamic = torch.zeros((128), dtype=dtype, device=\"cuda\")\n     kernel_dynamic[(1,)](out_dynamic, 2, getattr(triton.language, dtype_str))\n     assert torch.all(out_static == 2)\n     assert torch.all(out_dynamic == 2)"}]