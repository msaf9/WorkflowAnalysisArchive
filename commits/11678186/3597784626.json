[{"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -929,9 +929,16 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: matmul_tf32dot\n   func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    // CHECK-SAME: (f32, f32, f32, f32)\n     %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n     %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n "}]