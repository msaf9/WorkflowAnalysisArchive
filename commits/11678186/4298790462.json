[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 27, "deletions": 106, "changes": 133, "file_content_changes": "@@ -106,43 +106,23 @@ class ConvertTritonGPUToLLVM\n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n     ModuleOp mod = getOperation();\n-\n     mlir::LowerToLLVMOptions option(context);\n     option.overrideIndexBitwidth(32);\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n     TritonLLVMFunctionConversionTarget funcTarget(*context);\n     TritonLLVMConversionTarget target(*context);\n-\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    // Step 1: Decompose unoptimized layout conversions to use shared memory\n-    // Step 2: Decompose insert_slice_async to use load + insert_slice for\n-    //   pre-Ampere architectures or unsupported vectorized load sizes\n-    // Step 3: Allocate shared memories and insert barriers\n-    // Step 4: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // Step 5: Get axis and shared memory info\n-    // Step 6: Convert the rest of ops via partial conversion\n-    //\n-    // The reason for a separation between 4/6 is that, step 5 is out of the\n-    // scope of Dialect Conversion, thus we need to make sure the smem is not\n-    // revised during the conversion of step 6.\n-\n-    // Step 1\n+    /* preprocess */\n     decomposeMmaToDotOperand(mod, numWarps);\n     decomposeBlockedToDotOperand(mod);\n-\n-    // Step 2\n     if (failed(decomposeInsertSliceAsyncOp(mod)))\n       return signalPassFailure();\n \n-    // Step 3\n+    /* allocate shared memory and set barrier */\n     Allocation allocation(mod);\n     MembarAnalysis membarPass(&allocation);\n     membarPass.run();\n-\n-    // Step 4\n-\n-    // Step 5 - get axis and shared memory info\n     std::unique_ptr<DataFlowSolver> solver = createDataFlowSolver();\n     AxisInfoAnalysis *axisInfoAnalysis = solver->load<AxisInfoAnalysis>();\n     if (failed(solver->initializeAndRun(mod)))\n@@ -152,53 +132,32 @@ class ConvertTritonGPUToLLVM\n                  mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n                                         allocation.getSharedMemorySize()));\n \n-    // Step 6 - rewrite rest of ops\n-    // We set a higher benefit here to ensure triton's patterns runs before\n-    // arith patterns for some encoding not supported by the community\n-    // patterns.\n+    /* rewrite ops */\n+    RewritePatternSet patterns(context);\n+    // TritonGPU lowering patterns\n     OpBuilder::InsertPoint indexInsertPoint;\n     ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo indexCacheInfo{\n         &baseIndexCache, &indexCache, &indexInsertPoint};\n-\n-    RewritePatternSet patterns(context);\n-\n-    // Normal conversions\n-    populateTritonGPUToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                    *axisInfoAnalysis, &allocation, smem,\n-                                    indexCacheInfo, /*benefit=*/10);\n-    // ConvertLayoutOp\n-    populateConvertLayoutOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                          *axisInfoAnalysis, &allocation, smem,\n-                                          indexCacheInfo, /*benefit=*/10);\n-    // DotOp\n-    populateDotOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                *axisInfoAnalysis, &allocation, smem,\n-                                /*benefit=*/10);\n-    // ElementwiseOp\n-    populateElementwiseOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                        *axisInfoAnalysis, &allocation, smem,\n-                                        /*benefit=*/10);\n-    // LoadStoreOp\n-    populateLoadStoreOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                      *axisInfoAnalysis, &allocation, smem,\n-                                      indexCacheInfo, /*benefit=*/10);\n-    // ReduceOp\n-    populateReduceOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                   *axisInfoAnalysis, &allocation, smem,\n-                                   indexCacheInfo, /*benefit=*/10);\n-    // ViewOp\n-    populateViewOpToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                 *axisInfoAnalysis, &allocation, smem,\n-                                 /*benefit=*/10);\n-\n-    // Add arith/math's patterns to help convert scalar expression to LLVM.\n+    auto populatePatterns1 = [&](auto populateFunc) {\n+      populateFunc(typeConverter, patterns, numWarps, *axisInfoAnalysis,\n+                   &allocation, smem, indexCacheInfo, /*benefit*/ 1);\n+    };\n+    auto populatePatterns2 = [&](auto populateFunc) {\n+      populateFunc(typeConverter, patterns, numWarps, *axisInfoAnalysis,\n+                   &allocation, smem, /*benefit*/ 1);\n+    };\n+    populatePatterns1(populateTritonGPUToLLVMPatterns);\n+    populatePatterns1(populateConvertLayoutOpToLLVMPatterns);\n+    populatePatterns2(populateDotOpToLLVMPatterns);\n+    populatePatterns2(populateElementwiseOpToLLVMPatterns);\n+    populatePatterns1(populateLoadStoreOpToLLVMPatterns);\n+    populatePatterns1(populateReduceOpToLLVMPatterns);\n+    populatePatterns2(populateViewOpToLLVMPatterns);\n+    // Native lowering patterns\n     mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n                                                           patterns);\n     mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n-    // mlir::arith::populateArithToLLVMConversionPatterns(typeConverter,\n-    // patterns);\n     mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n-\n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();\n   }\n@@ -419,42 +378,6 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n   }\n };\n \n-class CFBranchPattern : public OpConversionPattern<cf::BranchOp> {\n-public:\n-  using OpConversionPattern<cf::BranchOp>::OpConversionPattern;\n-\n-  LogicalResult\n-  matchAndRewrite(cf::BranchOp op, cf::BranchOp::Adaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<cf::BranchOp>(op, op.getSuccessor(),\n-                                              adaptor.getOperands());\n-    return success();\n-  }\n-};\n-\n-class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n-public:\n-  using OpConversionPattern<cf::CondBranchOp>::OpConversionPattern;\n-\n-  LogicalResult\n-  matchAndRewrite(cf::CondBranchOp op, cf::CondBranchOp::Adaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto converter = getTypeConverter();\n-    auto newOp = rewriter.replaceOpWithNewOp<cf::CondBranchOp>(\n-        op, adaptor.getCondition(), op.getTrueDest(),\n-        adaptor.getTrueDestOperands(), op.getFalseDest(),\n-        adaptor.getFalseDestOperands());\n-\n-    if (failed(rewriter.convertRegionTypes(newOp.getTrueDest()->getParent(),\n-                                           *converter)))\n-      return failure();\n-    if (failed(rewriter.convertRegionTypes(newOp.getFalseDest()->getParent(),\n-                                           *converter)))\n-      return failure();\n-    return success();\n-  }\n-};\n-\n class ConvertTritonFuncToLLVM\n     : public ConvertTritonFuncToLLVMBase<ConvertTritonFuncToLLVM> {\n public:\n@@ -466,15 +389,13 @@ class ConvertTritonFuncToLLVM\n \n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    RewritePatternSet funcPatterns(context);\n-    TritonLLVMFunctionConversionTarget funcTarget(*context);\n-    funcPatterns.add<FuncOpConversion>(typeConverter, numWarps, 1);\n-    funcPatterns.add<ReturnOpConversion>(typeConverter);\n+    RewritePatternSet patterns(context);\n+    TritonLLVMFunctionConversionTarget target(*context);\n+    patterns.add<FuncOpConversion>(typeConverter, numWarps, 1);\n+    patterns.add<ReturnOpConversion>(typeConverter);\n     mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n-                                                          funcPatterns);\n-\n-    if (failed(\n-            applyPartialConversion(mod, funcTarget, std::move(funcPatterns))))\n+                                                          patterns);\n+    if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();\n   }\n };"}]