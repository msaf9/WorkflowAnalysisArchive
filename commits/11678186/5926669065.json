[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 14, "deletions": 10, "changes": 24, "file_content_changes": "@@ -128,16 +128,16 @@ def check_type_supported(dtype, device):\n \n \n class MmaLayout:\n-    def __init__(self, version, warps_per_cta, ctas_per_cga, cta_split_num, cta_order, instr_shape):\n+    def __init__(self, version, warps_per_cta, ctas_per_cga, cta_split_num, cta_order, is_int8_input):\n         self.version = version\n         self.warps_per_cta = str(warps_per_cta)\n         self.ctas_per_cga = str(ctas_per_cga)\n         self.cta_split_num = str(cta_split_num)\n         self.cta_order = str(cta_order)\n-        self.instr_shape = str(instr_shape)\n+        self.is_int8_input = str(is_int8_input).lower()\n \n     def __str__(self):\n-        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}, instrShape={self.instr_shape}}}>\"\n+        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}, isInt8Input={self.is_int8_input}}}>\"\n \n \n class BlockedLayout:\n@@ -1774,8 +1774,8 @@ def test_scan_layouts(M, N, src_layout, axis, device):\n     BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0], [1, 1], [1, 1], [0, 1]),\n     BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1], [1, 1], [1, 1], [0, 1]),\n     BlockedLayout([4, 4], [2, 16], [4, 1], [1, 0], [1, 1], [1, 1], [0, 1]),\n-    MmaLayout(version=(2, 0), warps_per_cta=[4, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[0, 1], instr_shape=[16, 8]),\n-    MmaLayout(version=(2, 0), warps_per_cta=[2, 2], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[0, 1], instr_shape=[16, 8])\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[0, 1], is_int8_input=False),\n+    MmaLayout(version=(2, 0), warps_per_cta=[2, 2], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[0, 1], is_int8_input=False)\n ]\n \n \n@@ -1847,7 +1847,7 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n layouts = [\n     BlockedLayout([1, 4], [1, 32], [4, 1], [1, 0], [1, 1], [1, 1], [0, 1]),\n     BlockedLayout([1, 4], [1, 32], [2, 2], [1, 0], [1, 1], [1, 1], [0, 1]),\n-    MmaLayout(version=(2, 0), warps_per_cta=[4, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[0, 1], instr_shape=[16, 8])\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[0, 1], is_int8_input=False)\n ]\n \n \n@@ -1893,7 +1893,7 @@ def test_store_op(M, src_layout, device):\n layouts = [\n     BlockedLayout([1, 4], [1, 32], [4, 1], [1, 0], [1, 1], [1, 1], [0, 1]),\n     BlockedLayout([1, 4], [1, 32], [2, 2], [1, 0], [1, 1], [1, 1], [0, 1]),\n-    MmaLayout(version=(2, 0), warps_per_cta=[4, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[0, 1], instr_shape=[16, 8])\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1], ctas_per_cga=[1, 1], cta_split_num=[1, 1], cta_order=[0, 1], is_int8_input=False)\n ]\n \n \n@@ -2313,9 +2313,9 @@ def kernel(X, stride_xm, stride_xk,\n         assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.tf32.tf32', ptx)\n     elif in_dtype == 'float16' and out_dtype == tl.float32:\n         if capability[0] == 7 and capability[1] == 5:  # Turing\n-          assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.f16.f16', ptx)\n+            assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.f16.f16', ptx)\n         else:\n-          assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f32.f16.f16', ptx)\n+            assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f32.f16.f16', ptx)\n     elif in_dtype == 'float16' and out_dtype == tl.float16:\n         if capability[0] == 7 and capability[1] == 5:  # Turing\n             assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f16.f16.f16', ptx)\n@@ -2331,6 +2331,7 @@ def test_dot_mulbroadcastred(in_dtype, device):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         pytest.skip(\"Requires sm >= 80 to run\")\n+\n     @triton.jit\n     def kernel(Z, X, Y,\n                M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n@@ -2430,6 +2431,7 @@ def kernel(out_ptr):\n def test_dot_without_load(dtype_str, device):\n     capability = torch.cuda.get_device_capability()\n     allow_tf32 = capability[0] > 7\n+\n     @triton.jit\n     def _kernel(out, ALLOW_TF32: tl.constexpr):\n         a = GENERATE_TEST_HERE\n@@ -2900,6 +2902,8 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n # TODO(Keren): if_exp_dynamic\n+\n+\n @pytest.mark.parametrize(\"if_type\", [\"if\", \"if_and_dynamic\", \"if_exp_static\", \"if_and_static\"])\n def test_if(if_type, device):\n \n@@ -3084,7 +3088,7 @@ def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n def test_inline_asm_packed(num_ctas, device):\n     check_cuda_only(device)\n-    \n+\n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))"}]