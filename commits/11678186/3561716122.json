[{"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 18, "deletions": 7, "changes": 25, "file_content_changes": "@@ -1334,9 +1334,19 @@ Value DotOpMmaV1ConversionHelper::loadA(\n \n   auto *ctx = rewriter.getContext();\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-  auto shape = tensorTy.getShape();\n   auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto order = sharedLayout.getOrder();\n+  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                             tensorTy.getShape().end());\n+  SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n+                              sharedLayout.getOrder().end());\n+\n+  // TODO [Superjomn]: transA cannot be accessed in ConvertLayoutOp.\n+  bool transA = false;\n+  if (transA) {\n+    std::swap(shape[0], shape[1]);\n+    std::swap(order[0], order[1]);\n+  }\n+\n   Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n \n   bool isARow = order[0] != 0;\n@@ -1381,7 +1391,7 @@ Value DotOpMmaV1ConversionHelper::loadA(\n     Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n     offA0I = udiv(offA0I, i32_val(vecA));\n     offA0I = xor_(offA0I, phaseA);\n-    offA0I = xor_(offA0I, i32_val(vecA));\n+    offA0I = mul(offA0I, i32_val(vecA));\n     offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n   }\n \n@@ -1399,7 +1409,7 @@ Value DotOpMmaV1ConversionHelper::loadA(\n     ptrA[i] = gep(ptr_ty(f16_ty), smem, offA[i]);\n \n   auto instrShape = getMmaInstrShape();\n-  unsigned numM = rep[0] * shape[0] / (spw[0] * wpt[0]);\n+  unsigned numM = std::max<int>(rep[0] * shape[0] / (spw[0] * wpt[0]), 1);\n \n   Type f16PtrTy = ptr_ty(f16_ty);\n \n@@ -1461,13 +1471,14 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n \n-  // TODO [Superjomn]: transB cannot be accessed here.\n-  bool transB = false;\n-\n   SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n                              tensorTy.getShape().end());\n   SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n                               sharedLayout.getOrder().end());\n+\n+  // TODO [Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n+  bool transB = false;\n+\n   if (transB) {\n     std::swap(order[0], order[1]);\n     std::swap(shape[0], shape[1]);"}]