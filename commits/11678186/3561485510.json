[{"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -107,7 +107,8 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   }\n \n   llvm::LLVMContext llvmContext;\n-  auto llvmir = translateTritonGPUToLLVMIR(&llvmContext, *module);\n+  auto llvmir =\n+      translateTritonGPUToLLVMIR(&llvmContext, *module, SMArch.getValue());\n   if (!llvmir) {\n     llvm::errs() << \"Translate to LLVM IR failed\";\n   }"}, {"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -56,8 +56,12 @@ class MembarAnalysis {\n     bool isIntersected(const RegionInfo &other, Allocation *allocation) const {\n       return /*RAW*/ isIntersected(syncWriteBuffers, other.syncReadBuffers,\n                                    allocation) ||\n-             /*WAR*/ isIntersected(syncReadBuffers, other.syncWriteBuffers,\n-                                   allocation);\n+             /*WAR*/\n+             isIntersected(syncReadBuffers, other.syncWriteBuffers,\n+                           allocation) ||\n+             /*WAW*/\n+             isIntersected(syncWriteBuffers, other.syncWriteBuffers,\n+                           allocation);\n     }\n \n     /// Clears the buffers because a barrier is inserted."}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "file_content_changes": "@@ -8,10 +8,35 @@\n \n namespace mlir {\n \n+class ReduceOpHelper {\n+public:\n+  explicit ReduceOpHelper(triton::ReduceOp op) : op(op) {\n+    srcTy = op.operand().getType().cast<RankedTensorType>();\n+  }\n+\n+  ArrayRef<int64_t> getSrcShape() { return srcTy.getShape(); }\n+\n+  Attribute getSrcLayout() { return srcTy.getEncoding(); }\n+\n+  bool isFastReduction();\n+\n+  unsigned getInterWarpSize();\n+\n+  unsigned getIntraWarpSize();\n+\n+  unsigned getThreadsReductionAxis();\n+\n+private:\n+  triton::ReduceOp op;\n+  RankedTensorType srcTy{};\n+};\n+\n bool isSharedEncoding(Value value);\n \n bool maybeSharedAllocationOp(Operation *op);\n \n+bool maybeAliasOp(Operation *op);\n+\n std::string getValueOperandName(Value value, AsmState &state);\n \n template <typename Int> Int product(llvm::ArrayRef<Int> arr) {"}, {"filename": "include/triton/Conversion/Passes.td", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -43,6 +43,12 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n                              \"mlir::triton::gpu::TritonGPUDialect\",\n                              \"mlir::NVVM::NVVMDialect\",\n                              \"mlir::StandardOpsDialect\"];\n+\n+    let options = [\n+        Option<\"computeCapability\", \"compute-capability\",\n+               \"int32_t\", /*default*/\"80\",\n+               \"device compute capability\">\n+    ];\n }\n \n #endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -33,7 +33,8 @@ struct NVVMMetadataField {\n   static constexpr char Kernel[] = \"nvvm.kernel\";\n };\n \n-std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonGPUToLLVMPass();\n+std::unique_ptr<OperationPass<ModuleOp>>\n+createConvertTritonGPUToLLVMPass(int computeCapability = 80);\n \n } // namespace triton\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -59,7 +59,8 @@ def TT_AtomicRMWAttr : I32EnumAttr<\n         I32EnumAttrCase<\"MAX\", 6, \"max\">,\n         I32EnumAttrCase<\"MIN\", 7, \"min\">,\n         I32EnumAttrCase<\"UMAX\", 8, \"umax\">,\n-        I32EnumAttrCase<\"UMIN\", 9, \"umin\">\n+        I32EnumAttrCase<\"UMIN\", 9, \"umin\">,\n+        I32EnumAttrCase<\"XCHG\", 10, \"exch\">\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -187,6 +187,7 @@ def TT_StoreOp : TT_Op<\"store\",\n //\n def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n                                           SameOperandsAndResultEncoding,\n+                                          MemoryEffects<[MemRead]>,\n                                           MemoryEffects<[MemWrite]>,\n                                           TypesMatchWith<\"infer ptr type from value type\",\n                                                          \"val\", \"ptr\",\n@@ -208,7 +209,9 @@ def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n     let results = (outs TT_Type:$result);\n }\n \n-def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [SameOperandsAndResultShape,\n+def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n+                                          MemoryEffects<[MemWrite]>,\n+                                          SameOperandsAndResultShape,\n                                           SameOperandsAndResultEncoding]> {\n     let summary = \"atomic cas\";\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -228,17 +228,24 @@ for\n       unsigned remainingLanes = 32;\n       unsigned remainingThreads = numWarps*32;\n       unsigned remainingWarps = numWarps;\n+      unsigned prevLanes = 1;\n+      unsigned prevWarps = 1;\n       SmallVector<unsigned, 4> threadsPerWarp(rank);\n       SmallVector<unsigned, 4> warpsPerCTA(rank);\n-      for (int _dim = 0; _dim < rank; ++_dim) {\n+      for (int _dim = 0; _dim < rank - 1; ++_dim) {\n         int i = order[_dim];\n         unsigned threadsPerCTA = std::clamp<unsigned>(remainingThreads, 1, shape[i] / sizePerThread[i]);\n         threadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n         warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / threadsPerWarp[i], 1, remainingWarps);\n         remainingWarps /= warpsPerCTA[i];\n         remainingLanes /= threadsPerWarp[i];\n         remainingThreads /= threadsPerCTA;\n+        prevLanes *= threadsPerWarp[i];\n+        prevWarps *= warpsPerCTA[i];\n       }\n+      // Expand the last dimension to fill the remaining lanes and warps\n+      threadsPerWarp[order[rank-1]] = 32 / prevLanes;\n+      warpsPerCTA[order[rank-1]] = numWarps / prevWarps;\n \n       return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order);\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -79,8 +79,7 @@ def TTG_SelectOp : TTG_Op<\"select\", [NoSideEffect]> {\n def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n                                     [AttrSizedOperandSegments,\n                                      ResultsAreSharedEncoding,\n-                                     // MemoryEffects<[MemRead]>, doesn't work with CSE but seems like it should?\n-                                     NoSideEffect,\n+                                     MemoryEffects<[MemRead]>,\n                                      TypesMatchWith<\"infer mask type from src type\",\n                                                     \"src\", \"mask\", \"getI1SameShape($_self)\",\n                                                     \"($_op.getOperands().size() <= 3) || std::equal_to<>()\">,\n@@ -158,7 +157,8 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n   let printer = [{ return printInsertSliceAsyncOp(p, *this); }];\n }\n \n-def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [NoSideEffect, ResultsAreSharedEncoding]> {\n+def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [MemoryEffects<[MemAlloc]>,  // Allocate shared memory\n+                                                ResultsAreSharedEncoding]> {\n   let summary = \"allocate tensor\";\n \n   let description = [{"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -25,7 +25,8 @@ void addExternalLibs(mlir::ModuleOp &module,\n // Translate TritonGPU dialect to LLVMIR, return null if failed.\n std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n-                           mlir::ModuleOp module);\n+                           mlir::ModuleOp module,\n+                           int computeCapability);\n \n // Translate mlir LLVM dialect to LLVMIR, return null if failed.\n std::unique_ptr<llvm::Module>"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -26,13 +26,14 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n     // FIXME(Keren): extract and insert are always alias for now\n-    if (auto extractSliceOp = dyn_cast<tensor::ExtractSliceOp>(op)) {\n+    if (isa<tensor::ExtractSliceOp>(op)) {\n       // extract_slice %src\n       aliasInfo = AliasInfo(operands[0]->getValue());\n       pessimistic = false;\n-    } else if (auto insertSliceOp =\n-                   dyn_cast<triton::gpu::InsertSliceAsyncOp>(op)) {\n+    } else if (isa<tensor::InsertSliceOp>(op) ||\n+               isa<triton::gpu::InsertSliceAsyncOp>(op)) {\n       // insert_slice_async %src, %dst, %index\n+      // insert_slice %src into %dst[%offsets]\n       aliasInfo = AliasInfo(operands[1]->getValue());\n       pessimistic = false;\n     } else if (isSharedEncoding(result)) {"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 59, "deletions": 20, "changes": 79, "file_content_changes": "@@ -28,7 +28,7 @@ namespace mlir {\n namespace triton {\n \n // Bitwidth of pointers\n-constexpr int kPtrBitWidth = 64; \n+constexpr int kPtrBitWidth = 64;\n \n static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n@@ -89,29 +89,42 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n }\n \n SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n-  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding();\n-  auto srcShape = srcTy.getShape();\n-  auto axis = op.axis();\n-\n-  bool fastReduce = axis == getOrder(srcLayout)[0];\n+  ReduceOpHelper helper(op);\n \n   SmallVector<unsigned> smemShape;\n+  auto srcShape = helper.getSrcShape();\n   for (auto d : srcShape)\n     smemShape.push_back(d);\n \n-  if (fastReduce) {\n-    unsigned sizeInterWarps = gpu::getWarpsPerCTA(srcLayout)[axis];\n-    smemShape[axis] = sizeInterWarps;\n+  auto axis = op.axis();\n+  if (helper.isFastReduction()) {\n+    smemShape[axis] = helper.getInterWarpSize();\n   } else {\n-    unsigned threadsPerCTAAxis = gpu::getThreadsPerWarp(srcLayout)[axis] *\n-                                 gpu::getWarpsPerCTA(srcLayout)[axis];\n-    smemShape[axis] = threadsPerCTAAxis;\n+    smemShape[axis] =\n+        std::min(smemShape[axis], helper.getThreadsReductionAxis());\n   }\n \n   return smemShape;\n }\n \n+// TODO: extend beyond scalars\n+SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n+  SmallVector<unsigned> smemShape;\n+  if (op.ptr().getType().isa<RankedTensorType>()) {\n+    // do nothing or just assert because shared memory is not used in tensor up\n+    // to now\n+  } else {\n+    // need only bytes for scalar\n+    // always vec = 1 and elemsPerThread = 1 for scalar?\n+    smemShape.push_back(1);\n+  }\n+  return smemShape;\n+}\n+\n+SmallVector<unsigned> getScratchConfigForAtomicCAS(triton::AtomicCASOp op) {\n+  return SmallVector<unsigned>{1};\n+}\n+\n class AllocationAnalysis {\n public:\n   AllocationAnalysis(Operation *operation, Allocation *allocation)\n@@ -141,8 +154,7 @@ class AllocationAnalysis {\n     // For example: %a = scf.if -> yield\n     // %a must be allocated elsewhere by other operations.\n     // FIXME(Keren): extract and insert are always alias for now\n-    if (!maybeSharedAllocationOp(op) || isa<tensor::ExtractSliceOp>(op) ||\n-        isa<triton::gpu::InsertSliceAsyncOp>(op)) {\n+    if (!maybeSharedAllocationOp(op) || maybeAliasOp(op)) {\n       return;\n     }\n \n@@ -164,8 +176,7 @@ class AllocationAnalysis {\n       // TODO(Keren): Reduce with index is not supported yet.\n       auto value = op->getOperand(0);\n       if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n-        auto srcLayout = tensorType.getEncoding();\n-        bool fastReduce = reduceOp.axis() == getOrder(srcLayout)[0];\n+        bool fastReduce = ReduceOpHelper(reduceOp).isFastReduction();\n         auto smemShape = getScratchConfigForReduce(reduceOp);\n         unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                          std::multiplies{});\n@@ -196,9 +207,37 @@ class AllocationAnalysis {\n       auto smemShape = getScratchConfigForCvtLayout(cvtLayout, inVec, outVec);\n       unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                        std::multiplies{});\n-      auto bytes = srcTy.getElementType().isa<triton::PointerType>()? \n-                   elems * kPtrBitWidth / 8 :\n-                   elems * srcTy.getElementTypeBitWidth() / 8;\n+      auto bytes = srcTy.getElementType().isa<triton::PointerType>()\n+                       ? elems * kPtrBitWidth / 8\n+                       : elems * srcTy.getElementTypeBitWidth() / 8;\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    } else if (auto atomicRMWOp = dyn_cast<triton::AtomicRMWOp>(op)) {\n+      auto value = op->getOperand(0);\n+      // only scalar requires scratch memory\n+      // make it explicit for readability\n+      if (value.getType().dyn_cast<RankedTensorType>()) {\n+        // nothing to do\n+      } else {\n+        auto smemShape = getScratchConfigForAtomicRMW(atomicRMWOp);\n+        unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                         std::multiplies{});\n+        auto elemTy =\n+            value.getType().cast<triton::PointerType>().getPointeeType();\n+        auto bytes = elemTy.isa<triton::PointerType>()\n+                         ? elems * kPtrBitWidth / 8\n+                         : elems * elemTy.getIntOrFloatBitWidth() / 8;\n+        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      }\n+    } else if (auto atomicCASOp = dyn_cast<triton::AtomicCASOp>(op)) {\n+      auto value = op->getOperand(0);\n+      auto smemShape = getScratchConfigForAtomicCAS(atomicCASOp);\n+      unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                       std::multiplies{});\n+      auto elemTy =\n+          value.getType().cast<triton::PointerType>().getPointeeType();\n+      auto bytes = elemTy.isa<triton::PointerType>()\n+                       ? elems * kPtrBitWidth / 8\n+                       : elems * elemTy.getIntOrFloatBitWidth() / 8;\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     }\n   }"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 10, "deletions": 3, "changes": 13, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"triton/Analysis/Membar.h\"\n+#include \"triton/Analysis/Alias.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n #include \"mlir/Dialect/GPU/GPUDialect.h\"\n@@ -71,11 +72,17 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n \n   RegionInfo curRegionInfo;\n   for (Value value : op->getOperands()) {\n-    // ConvertLayoutOp: shared memory -> registers\n-    // Need to consider all alias buffers\n     for (auto bufferId : allocation->getBufferIds(value)) {\n       if (bufferId != Allocation::InvalidBufferId) {\n-        curRegionInfo.syncReadBuffers.insert(bufferId);\n+        if (isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n+            isa<tensor::InsertSliceOp>(op)) {\n+          // FIXME(Keren): insert_slice and insert_slice_async are always alias\n+          // for now\n+          curRegionInfo.syncWriteBuffers.insert(bufferId);\n+        } else {\n+          // ConvertLayoutOp: shared memory -> registers\n+          curRegionInfo.syncReadBuffers.insert(bufferId);\n+        }\n       }\n     }\n   }"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 38, "deletions": 0, "changes": 38, "file_content_changes": "@@ -5,6 +5,38 @@\n \n namespace mlir {\n \n+bool ReduceOpHelper::isFastReduction() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto axis = op.axis();\n+  return axis == triton::gpu::getOrder(srcLayout)[0];\n+}\n+\n+unsigned ReduceOpHelper::getInterWarpSize() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto srcShape = srcTy.getShape();\n+  auto axis = op.axis();\n+  auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n+  unsigned sizeIntraWarps = getIntraWarpSize();\n+  return std::min(srcReduceDimSize / sizeIntraWarps,\n+                  triton::gpu::getWarpsPerCTA(srcLayout)[axis]);\n+}\n+\n+unsigned ReduceOpHelper::getIntraWarpSize() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto srcShape = srcTy.getShape();\n+  auto axis = op.axis();\n+  auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n+  return std::min(srcReduceDimSize,\n+                  triton::gpu::getThreadsPerWarp(srcLayout)[axis]);\n+}\n+\n+unsigned ReduceOpHelper::getThreadsReductionAxis() {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto axis = op.axis();\n+  return triton::gpu::getThreadsPerWarp(srcLayout)[axis] *\n+         triton::gpu::getWarpsPerCTA(srcLayout)[axis];\n+}\n+\n bool isSharedEncoding(Value value) {\n   auto type = value.getType();\n   if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n@@ -28,6 +60,12 @@ bool maybeSharedAllocationOp(Operation *op) {\n           dialect->getTypeID() == mlir::TypeID::get<tensor::TensorDialect>());\n }\n \n+bool maybeAliasOp(Operation *op) {\n+  return isa<tensor::ExtractSliceOp>(op) ||\n+         isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n+         isa<tensor::InsertSliceOp>(op);\n+}\n+\n std::string getValueOperandName(Value value, AsmState &state) {\n   std::string opName;\n   llvm::raw_string_ostream ss(opName);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 396, "deletions": 148, "changes": 544, "file_content_changes": "@@ -39,6 +39,7 @@ using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n using ::mlir::LLVM::DotOpMmaV2ConversionHelper;\n using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::LLVM::getStructFromElements;\n using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::LLVM::SharedMemoryObject;\n@@ -1353,9 +1354,7 @@ struct ReduceOpConversion\n LogicalResult\n ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n-  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n-  if (op.axis() == srcLayout.getOrder()[0])\n+  if (ReduceOpHelper(op).isFastReduction())\n     return matchAndRewriteFast(op, adaptor, rewriter);\n   return matchAndRewriteBasic(op, adaptor, rewriter);\n }\n@@ -1553,10 +1552,11 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n   smemBase = bitcast(smemBase, elemPtrTy);\n \n-  auto order = getOrder(srcLayout);\n-  unsigned sizeIntraWarps = threadsPerWarp[axis];\n-  unsigned sizeInterWarps = warpsPerCTA[axis];\n+  ReduceOpHelper helper(op);\n+  unsigned sizeIntraWarps = helper.getIntraWarpSize();\n+  unsigned sizeInterWarps = helper.getInterWarpSize();\n \n+  auto order = getOrder(srcLayout);\n   unsigned srcElems = getElemsPerThread(srcTy);\n   auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n   auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n@@ -2636,6 +2636,112 @@ struct ConvertLayoutOpConversion\n     return failure();\n   }\n \n+  static void storeBlockedToShared(Value src, Value llSrc,\n+                                   ArrayRef<Value> srcStrides,\n+                                   ArrayRef<Value> srcIndices, Value dst,\n+                                   Value smemBase, Type elemPtrTy, Location loc,\n+                                   ConversionPatternRewriter &rewriter) {\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    assert(srcShape.size() == 2 && \"Unexpected rank of insertSlice\");\n+\n+    auto elemTy = srcTy.getElementType();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto inOrd = srcBlockedLayout.getOrder();\n+    auto outOrd = dstSharedLayout.getOrder();\n+    unsigned inVec =\n+        inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n+    unsigned outVec = dstSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned perPhase = dstSharedLayout.getPerPhase();\n+    unsigned maxPhase = dstSharedLayout.getMaxPhase();\n+    unsigned numElems = getElemsPerThread(srcTy);\n+    auto inVals = getElementsFromStruct(loc, llSrc, rewriter);\n+    auto srcAccumSizeInThreads =\n+        product<unsigned>(srcBlockedLayout.getSizePerThread());\n+    auto wordTy = vec_ty(elemTy, minVec);\n+\n+    // TODO: [goostavz] We should make a cache for the calculation of\n+    // emitBaseIndexForBlockedLayout in case backend compiler not being able to\n+    // optimize that\n+    SmallVector<unsigned> srcShapePerCTA = getShapePerCTA(srcBlockedLayout);\n+    SmallVector<unsigned> reps{ceil<unsigned>(srcShape[0], srcShapePerCTA[0]),\n+                               ceil<unsigned>(srcShape[1], srcShapePerCTA[1])};\n+\n+    // Visit each input value in the order they are placed in inVals\n+    //\n+    // Please note that the order was not awaring of blockLayout.getOrder(),\n+    // thus the adjacent elems may not belong to a same word. This could be\n+    // improved if we update the elements order by emitIndicesForBlockedLayout()\n+    SmallVector<unsigned> wordsInEachRep(2);\n+    wordsInEachRep[0] = inOrd[0] == 0\n+                            ? srcBlockedLayout.getSizePerThread()[0] / minVec\n+                            : srcBlockedLayout.getSizePerThread()[0];\n+    wordsInEachRep[1] = inOrd[0] == 0\n+                            ? srcBlockedLayout.getSizePerThread()[1]\n+                            : srcBlockedLayout.getSizePerThread()[1] / minVec;\n+    Value outVecVal = i32_val(outVec);\n+    Value minVecVal = i32_val(minVec);\n+    auto numWordsEachRep = product<unsigned>(wordsInEachRep);\n+    SmallVector<Value> wordVecs(numWordsEachRep);\n+    for (unsigned i = 0; i < numElems; ++i) {\n+      if (i % srcAccumSizeInThreads == 0) {\n+        // start of a replication\n+        for (unsigned w = 0; w < numWordsEachRep; ++w) {\n+          wordVecs[w] = undef(wordTy);\n+        }\n+      }\n+      unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n+      auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n+          linearIdxInNanoTile, srcBlockedLayout.getSizePerThread(), inOrd);\n+      unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n+      multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n+      auto wordVecIdx = getLinearIndex<unsigned>(multiDimIdxInNanoTile,\n+                                                 wordsInEachRep, inOrd);\n+      wordVecs[wordVecIdx] =\n+          insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], i32_val(pos));\n+\n+      if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n+        // end of replication, store the vectors into shared memory\n+        unsigned linearRepIdx = i / srcAccumSizeInThreads;\n+        auto multiDimRepIdx =\n+            getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n+        for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n+             ++linearWordIdx) {\n+          // step 1: recover the multidim_index from the index of input_elements\n+          auto multiDimWordIdx =\n+              getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n+          SmallVector<Value> multiDimIdx(2);\n+          auto wordOffset0 = multiDimRepIdx[0] * srcShapePerCTA[0] +\n+                             multiDimWordIdx[0] * (inOrd[0] == 0 ? minVec : 1);\n+          auto wordOffset1 = multiDimRepIdx[1] * srcShapePerCTA[1] +\n+                             multiDimWordIdx[1] * (inOrd[0] == 1 ? minVec : 1);\n+          multiDimIdx[0] = add(srcIndices[0], i32_val(wordOffset0));\n+          multiDimIdx[1] = add(srcIndices[1], i32_val(wordOffset1));\n+\n+          // step 2: do swizzling\n+          Value remained = urem(multiDimIdx[outOrd[0]], outVecVal);\n+          multiDimIdx[outOrd[0]] = udiv(multiDimIdx[outOrd[0]], outVecVal);\n+          Value off_1 = mul(multiDimIdx[outOrd[1]], srcStrides[outOrd[1]]);\n+          Value phaseId = udiv(multiDimIdx[outOrd[1]], i32_val(perPhase));\n+          phaseId = urem(phaseId, i32_val(maxPhase));\n+          Value off_0 = xor_(multiDimIdx[outOrd[0]], phaseId);\n+          off_0 = mul(off_0, outVecVal);\n+          remained = udiv(remained, minVecVal);\n+          off_0 = add(off_0, mul(remained, minVecVal));\n+          Value offset = add(off_1, off_0);\n+\n+          // step 3: store\n+          Value smemAddr = gep(elemPtrTy, smemBase, offset);\n+          smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n+          store(wordVecs[linearWordIdx], smemAddr);\n+        }\n+      }\n+    }\n+  }\n+\n private:\n   SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,\n                                        ConversionPatternRewriter &rewriter,\n@@ -2930,109 +3036,90 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n   auto inOrd = srcBlockedLayout.getOrder();\n   auto outOrd = dstSharedLayout.getOrder();\n-  unsigned inVec =\n-      inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n-  unsigned outVec = dstSharedLayout.getVec();\n-  unsigned minVec = std::min(outVec, inVec);\n-  unsigned perPhase = dstSharedLayout.getPerPhase();\n-  unsigned maxPhase = dstSharedLayout.getMaxPhase();\n-  unsigned numElems = getElemsPerThread(srcTy);\n-  auto inVals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n-  auto srcAccumSizeInThreads =\n-      product<unsigned>(srcBlockedLayout.getSizePerThread());\n-  auto elemTy = srcTy.getElementType();\n-  auto wordTy = vec_ty(elemTy, minVec);\n-\n-  // TODO: [goostavz] We should make a cache for the calculation of\n-  // emitBaseIndexForBlockedLayout in case backend compiler not being able to\n-  // optimize that\n-  SmallVector<Value> multiDimOffsetFirstElem =\n-      emitBaseIndexForBlockedLayout(loc, rewriter, srcBlockedLayout, srcShape);\n-  SmallVector<unsigned> srcShapePerCTA = getShapePerCTA(srcBlockedLayout);\n-  SmallVector<unsigned> reps{ceil<unsigned>(srcShape[0], srcShapePerCTA[0]),\n-                             ceil<unsigned>(srcShape[1], srcShapePerCTA[1])};\n-\n-  // Visit each input value in the order they are placed in inVals\n-  //\n-  // Please note that the order was not awaring of blockLayout.getOrder(),\n-  // thus the adjacent elems may not belong to a same word. This could be\n-  // improved if we update the elements order by emitIndicesForBlockedLayout()\n-  SmallVector<unsigned> wordsInEachRep(2);\n-  wordsInEachRep[0] = inOrd[0] == 0\n-                          ? srcBlockedLayout.getSizePerThread()[0] / minVec\n-                          : srcBlockedLayout.getSizePerThread()[0];\n-  wordsInEachRep[1] = inOrd[0] == 0\n-                          ? srcBlockedLayout.getSizePerThread()[1]\n-                          : srcBlockedLayout.getSizePerThread()[1] / minVec;\n-  Value outVecVal = idx_val(outVec);\n-  Value minVecVal = idx_val(minVec);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n+  auto elemTy = getTypeConverter()->convertType(srcTy.getElementType());\n   auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n   smemBase = bitcast(smemBase, elemPtrTy);\n+\n+  auto srcStrides = getStridesFromShapeAndOrder(srcShape, inOrd, loc, rewriter);\n+  auto srcIndices =\n+      emitBaseIndexForBlockedLayout(loc, rewriter, srcBlockedLayout, srcShape);\n+  storeBlockedToShared(src, adaptor.src(), srcStrides, srcIndices, dst,\n+                       smemBase, elemPtrTy, loc, rewriter);\n+\n   auto smemObj = SharedMemoryObject(smemBase, dstShape, outOrd, loc, rewriter);\n   auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n-  auto numWordsEachRep = product<unsigned>(wordsInEachRep);\n-  SmallVector<Value> wordVecs(numWordsEachRep);\n-  for (unsigned i = 0; i < numElems; ++i) {\n-    if (i % srcAccumSizeInThreads == 0) {\n-      // start of a replication\n-      for (unsigned w = 0; w < numWordsEachRep; ++w) {\n-        wordVecs[w] = undef(wordTy);\n+  rewriter.replaceOp(op, retVal);\n+  return success();\n+}\n+\n+struct InsertSliceOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<tensor::InsertSliceOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      tensor::InsertSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(tensor::InsertSliceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // %dst = insert_slice %src into %dst[%offsets]\n+    Location loc = op->getLoc();\n+    Value dst = op.dest();\n+    Value src = op.source();\n+    Value res = op.result();\n+    assert(allocation->getBufferId(res) == Allocation::InvalidBufferId &&\n+           \"Only support in-place insert_slice for now\");\n+\n+    auto srcTy = src.getType().dyn_cast<RankedTensorType>();\n+    auto srcLayout = srcTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    auto srcShape = srcTy.getShape();\n+    assert(srcLayout && \"Unexpected srcLayout in InsertSliceOpConversion\");\n+\n+    auto dstTy = dst.getType().dyn_cast<RankedTensorType>();\n+    auto dstLayout = dstTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n+    auto llDst = adaptor.dest();\n+    assert(dstLayout && \"Unexpected dstLayout in InsertSliceOpConversion\");\n+    assert(op.hasUnitStride() &&\n+           \"Only unit stride supported by InsertSliceOpConversion\");\n+\n+    // newBase = base + offset\n+    // Triton support either static and dynamic offsets\n+    auto smemObj = getSharedMemoryObjectFromStruct(loc, llDst, rewriter);\n+    SmallVector<Value, 4> offsets;\n+    SmallVector<Value, 4> srcStrides;\n+    auto mixedOffsets = op.getMixedOffsets();\n+    for (auto i = 0; i < mixedOffsets.size(); ++i) {\n+      if (op.isDynamicOffset(i)) {\n+        offsets.emplace_back(adaptor.offsets()[i]);\n+      } else {\n+        offsets.emplace_back(i32_val(op.getStaticOffset(i)));\n       }\n-    }\n-    unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n-    auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n-        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread(), inOrd);\n-    unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n-    multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n-    auto wordVecIdx =\n-        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep, inOrd);\n-    wordVecs[wordVecIdx] =\n-        insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], idx_val(pos));\n-\n-    if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n-      // end of replication, store the vectors into shared memory\n-      unsigned linearRepIdx = i / srcAccumSizeInThreads;\n-      auto multiDimRepIdx =\n-          getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n-      for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n-           ++linearWordIdx) {\n-        // step 1: recover the multidim_index from the index of input_elements\n-        auto multiDimWordIdx =\n-            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n-        SmallVector<Value> multiDimIdx(2);\n-        auto wordOffset0 = multiDimRepIdx[0] * srcShapePerCTA[0] +\n-                           multiDimWordIdx[0] * (inOrd[0] == 0 ? minVec : 1);\n-        auto wordOffset1 = multiDimRepIdx[1] * srcShapePerCTA[1] +\n-                           multiDimWordIdx[1] * (inOrd[0] == 1 ? minVec : 1);\n-        multiDimIdx[0] = add(multiDimOffsetFirstElem[0], idx_val(wordOffset0));\n-        multiDimIdx[1] = add(multiDimOffsetFirstElem[1], idx_val(wordOffset1));\n-\n-        // step 2: do swizzling\n-        Value remained = urem(multiDimIdx[outOrd[0]], outVecVal);\n-        multiDimIdx[outOrd[0]] = udiv(multiDimIdx[outOrd[0]], outVecVal);\n-        Value off_1 = mul(multiDimIdx[outOrd[1]], idx_val(srcShape[outOrd[0]]));\n-        Value phaseId = udiv(multiDimIdx[outOrd[1]], idx_val(perPhase));\n-        phaseId = urem(phaseId, idx_val(maxPhase));\n-        Value off_0 = xor_(multiDimIdx[outOrd[0]], phaseId);\n-        off_0 = mul(off_0, outVecVal);\n-        remained = udiv(remained, minVecVal);\n-        off_0 = add(off_0, mul(remained, minVecVal));\n-        Value offset = add(off_1, off_0);\n-\n-        // step 3: store\n-        Value smemAddr = gep(elemPtrTy, smemBase, offset);\n-        smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n-        store(wordVecs[linearWordIdx], smemAddr);\n+      // Like insert_slice_async, we only support slice from one dimension,\n+      // which has a slice size of 1\n+      if (op.getStaticSize(i) != 1) {\n+        srcStrides.emplace_back(smemObj.strides[i]);\n       }\n     }\n+\n+    // Compute the offset based on the original strides of the shared memory\n+    // object\n+    auto offset = dot(rewriter, loc, offsets, smemObj.strides);\n+    auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+    auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+    auto smemBase = gep(elemPtrTy, smemObj.base, offset);\n+\n+    auto llSrc = adaptor.source();\n+    auto srcIndices =\n+        emitBaseIndexForBlockedLayout(loc, rewriter, srcLayout, srcShape);\n+    ConvertLayoutOpConversion::storeBlockedToShared(src, llSrc, srcStrides,\n+                                                    srcIndices, dst, smemBase,\n+                                                    elemPtrTy, loc, rewriter);\n+    // Barrier is not necessary.\n+    // The membar pass knows that it writes to shared memory and will handle it\n+    // properly.\n+    rewriter.replaceOp(op, llDst);\n+    return success();\n   }\n-  // Barrier is not necessary.\n-  // The membar pass knows that it writes to shared memory and will handle it\n-  // properly.\n-  rewriter.replaceOp(op, retVal);\n-  return success();\n-}\n+};\n \n /// ====================== dot codegen begin ==========================\n \n@@ -3983,6 +4070,82 @@ struct ExpOpConversionApprox\n     return ptxBuilder.launch(rewriter, loc, f32_ty, false);\n   }\n };\n+/// ====================== atomic_cas codegen begin ==========================\n+struct AtomicCASOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::AtomicCASOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  AtomicCASOpConversion(LLVMTypeConverter &converter,\n+                        const Allocation *allocation, Value smem,\n+                        AxisInfoAnalysis &axisAnalysisPass,\n+                        PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::AtomicCASOp>(\n+            converter, allocation, smem, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+    Value ptr = op.ptr();\n+\n+    Value llPtr = adaptor.ptr();\n+    Value llCmp = adaptor.cmp();\n+    Value llVal = adaptor.val();\n+\n+    auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n+    auto cmpElements = getElementsFromStruct(loc, llCmp, rewriter);\n+    auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n+\n+    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    Type valueElemTy =\n+        valueTy ? getTypeConverter()->convertType(valueTy.getElementType())\n+                : op.getResult().getType();\n+    auto tid = tid_val();\n+    Value pred = icmp_eq(tid, i32_val(0));\n+    PTXBuilder ptxBuilderMemfence;\n+    auto memfenc = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n+    memfenc();\n+    auto ASMReturnTy = void_ty(ctx);\n+    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n+\n+    Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+    atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n+\n+    Value casPtr = ptrElements[0];\n+    Value casCmp = cmpElements[0];\n+    Value casVal = valElements[0];\n+\n+    PTXBuilder ptxBuilderAtomicCAS;\n+    auto *dstOpr = ptxBuilderAtomicCAS.newOperand(\"=r\");\n+    auto *ptrOpr = ptxBuilderAtomicCAS.newAddrOperand(casPtr, \"l\");\n+    auto *cmpOpr = ptxBuilderAtomicCAS.newOperand(casCmp, \"r\");\n+    auto *valOpr = ptxBuilderAtomicCAS.newOperand(casVal, \"r\");\n+    auto &atom = *ptxBuilderAtomicCAS.create<PTXInstr>(\"atom\");\n+    atom.global().o(\"cas\").o(\"b32\");\n+    atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(pred);\n+    auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n+    barrier();\n+\n+    PTXBuilder ptxBuilderStore;\n+    auto *dstOprStore = ptxBuilderStore.newAddrOperand(atomPtr, \"l\");\n+    auto *valOprStore = ptxBuilderStore.newOperand(old, \"r\");\n+    auto &st = *ptxBuilderStore.create<PTXInstr>(\"st\");\n+    st.shared().o(\"b32\");\n+    st(dstOprStore, valOprStore).predicate(pred);\n+    ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n+    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n+    barrier();\n+    Value ret = load(atomPtr);\n+    barrier();\n+    rewriter.replaceOp(op, {ret});\n+    return success();\n+  }\n+};\n+/// ====================== atomic_cas codegen end ==========================\n \n /// ====================== atomic_rmw codegen begin ==========================\n struct AtomicRMWOpConversion\n@@ -3992,10 +4155,11 @@ struct AtomicRMWOpConversion\n       triton::AtomicRMWOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   AtomicRMWOpConversion(LLVMTypeConverter &converter,\n+                        const Allocation *allocation, Value smem,\n                         AxisInfoAnalysis &axisAnalysisPass,\n                         PatternBenefit benefit)\n-      : ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>(converter,\n-                                                             benefit),\n+      : ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>(\n+            converter, allocation, smem, benefit),\n         LoadStoreConversionBase(axisAnalysisPass) {}\n \n   LogicalResult\n@@ -4017,29 +4181,28 @@ struct AtomicRMWOpConversion\n     auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n     auto maskElements = getElementsFromStruct(loc, llMask, rewriter);\n \n-    // TODO[dongdongl]: Support scalar\n-\n     auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n-    if (!valueTy)\n-      return failure();\n     Type valueElemTy =\n-        getTypeConverter()->convertType(valueTy.getElementType());\n-\n-    auto valTy = val.getType().cast<RankedTensorType>();\n+        valueTy ? getTypeConverter()->convertType(valueTy.getElementType())\n+                : op.getResult().getType();\n     const size_t valueElemNbits = valueElemTy.getIntOrFloatBitWidth();\n-    auto vec = getVectorSize(ptr);\n-    vec = std::min<unsigned>(vec, valTy.getElementType().isF16() ? 2 : 1);\n-\n-    auto vecTy = vec_ty(valueElemTy, vec);\n     auto elemsPerThread = getElemsPerThread(val.getType());\n-    // mask\n+    // vec = 1 for scalar\n+    auto vec = getVectorSize(ptr);\n     Value mask = int_val(1, 1);\n-    auto shape = valueTy.getShape();\n-    auto numElements = product(shape);\n     auto tid = tid_val();\n-    mask = and_(mask, icmp_slt(mul(tid, i32_val(elemsPerThread)),\n-                               i32_val(numElements)));\n+    // tensor\n+    if (valueTy) {\n+      auto valTy = val.getType().cast<RankedTensorType>();\n+      vec = std::min<unsigned>(vec, valTy.getElementType().isF16() ? 2 : 1);\n+      // mask\n+      auto shape = valueTy.getShape();\n+      auto numElements = product(shape);\n+      mask = and_(mask, icmp_slt(mul(tid, i32_val(elemsPerThread)),\n+                                 i32_val(numElements)));\n+    }\n \n+    auto vecTy = vec_ty(valueElemTy, vec);\n     SmallVector<Value> resultVals(elemsPerThread);\n     for (size_t i = 0; i < elemsPerThread; i += vec) {\n       Value rmwVal = undef(vecTy);\n@@ -4052,13 +4215,15 @@ struct AtomicRMWOpConversion\n       Value rmwMask = maskElements[i];\n       rmwMask = and_(rmwMask, mask);\n       std::string sTy;\n-      PTXBuilder ptxBuilder;\n-\n-      auto *dstOpr = ptxBuilder.newOperand(\"=r\");\n-      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"l\");\n-      auto *valOpr = ptxBuilder.newOperand(rmwVal, \"r\");\n-\n-      auto &atom = ptxBuilder.create<>(\"atom\")->global().o(\"gpu\");\n+      PTXBuilder ptxBuilderAtomicRMW;\n+      std::string tyId = valueElemNbits * vec == 64\n+                             ? \"l\"\n+                             : (valueElemNbits * vec == 32 ? \"r\" : \"h\");\n+      auto *dstOpr = ptxBuilderAtomicRMW.newOperand(\"=\" + tyId);\n+      auto *ptrOpr = ptxBuilderAtomicRMW.newAddrOperand(rmwPtr, \"l\");\n+      auto *valOpr = ptxBuilderAtomicRMW.newOperand(rmwVal, tyId);\n+\n+      auto &atom = ptxBuilderAtomicRMW.create<>(\"atom\")->global().o(\"gpu\");\n       auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n       auto sBits = std::to_string(valueElemNbits);\n       switch (atomicRmwAttr) {\n@@ -4094,22 +4259,44 @@ struct AtomicRMWOpConversion\n         rmwOp = \"min\";\n         sTy = \"u\" + sBits;\n         break;\n+      case RMWOp::XCHG:\n+        sTy = \"b\" + sBits;\n+        break;\n       default:\n         return failure();\n       }\n       atom.o(rmwOp).o(sTy);\n-      atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n-\n-      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy);\n-      for (int ii = 0; ii < vec; ++ii) {\n-        resultVals[i * vec + ii] =\n-            vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n+      if (valueTy) {\n+        atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n+        auto ret = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n+        for (int ii = 0; ii < vec; ++ii) {\n+          resultVals[i * vec + ii] =\n+              vec == 1 ? ret : extract_element(valueElemTy, ret, idx_val(ii));\n+        }\n+      } else {\n+        PTXBuilder ptxBuilderMemfence;\n+        auto memfenc = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n+        memfenc();\n+        auto ASMReturnTy = void_ty(ctx);\n+        ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n+        rmwMask = and_(rmwMask, icmp_eq(tid, i32_val(0)));\n+        atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n+        auto old = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n+        Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+        atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n+        store(old, atomPtr);\n+        barrier();\n+        Value ret = load(atomPtr);\n+        barrier();\n+        rewriter.replaceOp(op, {ret});\n       }\n     }\n-    Type structTy = getTypeConverter()->convertType(valueTy);\n-    Value resultStruct =\n-        getStructFromElements(loc, resultVals, rewriter, structTy);\n-    rewriter.replaceOp(op, {resultStruct});\n+    if (valueTy) {\n+      Type structTy = getTypeConverter()->convertType(valueTy);\n+      Value resultStruct =\n+          getStructFromElements(loc, resultVals, rewriter, structTy);\n+      rewriter.replaceOp(op, {resultStruct});\n+    }\n     return success();\n   }\n };\n@@ -4195,11 +4382,16 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);\n-  patterns.add<AtomicRMWOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n+  patterns.add<AtomicCASOpConversion>(typeConverter, allocation, smem,\n+                                      axisInfoAnalysis, benefit);\n+  patterns.add<AtomicRMWOpConversion>(typeConverter, allocation, smem,\n+                                      axisInfoAnalysis, benefit);\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n   patterns.add<GetNumProgramsOpConversion>(typeConverter, benefit);\n+  patterns.add<InsertSliceOpConversion>(typeConverter, allocation, smem,\n+                                        benefit);\n   patterns.add<InsertSliceAsyncOpConversion>(typeConverter, allocation, smem,\n                                              axisInfoAnalysis, benefit);\n   patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n@@ -4245,8 +4437,57 @@ class ConvertTritonGPUToLLVM\n     });\n   }\n \n+  void decomposeInsertSliceAsyncOp(ModuleOp mod,\n+                                   TritonGPUToLLVMTypeConverter &converter) {\n+    // cp.async is supported in Ampere and later\n+    if (computeCapability >= 80)\n+      return;\n+\n+    // insert_slice_async %src, %dst, %idx, %mask, %other\n+    // =>\n+    // %tmp = load %src, %mask, %other\n+    // %res = insert_slice %tmp into %dst[%idx]\n+    mod.walk([&](triton::gpu::InsertSliceAsyncOp insertSliceAsyncOp) -> void {\n+      OpBuilder builder(insertSliceAsyncOp);\n+      // load\n+      auto srcTy = insertSliceAsyncOp.src().getType().cast<RankedTensorType>();\n+      auto dstTy = insertSliceAsyncOp.getType().cast<RankedTensorType>();\n+      auto srcBlocked =\n+          srcTy.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n+      auto elemTy = converter.convertType(dstTy.getElementType());\n+      auto tmpTy = RankedTensorType::get(srcTy.getShape(), elemTy, srcBlocked);\n+      auto loadOp = builder.create<triton::LoadOp>(\n+          insertSliceAsyncOp.getLoc(), tmpTy, insertSliceAsyncOp.src(),\n+          insertSliceAsyncOp.mask(), insertSliceAsyncOp.other(),\n+          insertSliceAsyncOp.cache(), insertSliceAsyncOp.evict(),\n+          insertSliceAsyncOp.isVolatile());\n+      // insert_slice\n+      auto axis = insertSliceAsyncOp.axis();\n+      auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n+      auto offsets = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(0));\n+      auto sizes = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(1));\n+      auto strides = SmallVector<OpFoldResult>(dstTy.getRank(), intAttr(1));\n+      offsets[axis] = insertSliceAsyncOp.index();\n+      for (size_t i = 0; i < dstTy.getRank(); i++) {\n+        if (i != axis)\n+          sizes[i] = intAttr(dstTy.getShape()[i]);\n+      }\n+      auto insertSliceOp = builder.create<tensor::InsertSliceOp>(\n+          insertSliceAsyncOp.getLoc(), loadOp, insertSliceAsyncOp.dst(),\n+          offsets, sizes, strides);\n+      // Replace\n+      insertSliceAsyncOp.replaceAllUsesWith(insertSliceOp.getResult());\n+      insertSliceAsyncOp.erase();\n+    });\n+\n+    mod.walk([&](triton::gpu::AsyncWaitOp asyncWaitOp) -> void {\n+      asyncWaitOp.erase();\n+    });\n+  }\n+\n public:\n-  ConvertTritonGPUToLLVM() = default;\n+  explicit ConvertTritonGPUToLLVM(int computeCapability)\n+      : computeCapability(computeCapability) {}\n \n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n@@ -4262,18 +4503,22 @@ class ConvertTritonGPUToLLVM\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n     // step 1: Decompose unoptimized layout conversions to use shared memory\n-    // step 2: Allocate shared memories and insert barriers\n-    // step 3: Convert SCF to CFG\n-    // step 4: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // step 5: Convert the rest of ops via partial conversion\n-    // The reason for putting step 1 before step 2 is that the membar analysis\n-    // currently only supports SCF but not CFG.\n-    // The reason for a separation between 1/4 is that, step 3 is out of\n-    // the scope of Dialect Conversion, thus we need to make sure the smem\n-    // is not revised during the conversion of step 4.\n+    // step 2: Decompose insert_slice_async to use load + insert_slice for\n+    // pre-Ampere architectures\n+    // step 3: Allocate shared memories and insert barriers\n+    // step 4: Convert SCF to CFG\n+    // step 5: Convert FuncOp to LLVMFuncOp via partial conversion\n+    // step 6: Convert the rest of ops via partial\n+    // conversion The reason for putting step 1 before step 2 is that the membar\n+    // analysis currently only supports SCF but not CFG. The reason for a\n+    // separation between 1/4 is that, step 3 is out of the scope of Dialect\n+    // Conversion, thus we need to make sure the smem is not revised during the\n+    // conversion of step 4.\n \n     decomposeBlockedToDotOperand(mod);\n \n+    decomposeInsertSliceAsyncOp(mod, typeConverter);\n+\n     Allocation allocation(mod);\n     MembarAnalysis membar(&allocation);\n \n@@ -4332,6 +4577,8 @@ class ConvertTritonGPUToLLVM\n                         TritonGPUToLLVMTypeConverter &typeConverter);\n \n   Value smem;\n+\n+  int computeCapability{};\n };\n \n void ConvertTritonGPUToLLVM::initSharedMemory(\n@@ -4394,8 +4641,9 @@ TritonLLVMFunctionConversionTarget::TritonLLVMFunctionConversionTarget(\n \n namespace triton {\n \n-std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonGPUToLLVMPass() {\n-  return std::make_unique<::ConvertTritonGPUToLLVM>();\n+std::unique_ptr<OperationPass<ModuleOp>>\n+createConvertTritonGPUToLLVMPass(int computeCapability) {\n+  return std::make_unique<::ConvertTritonGPUToLLVM>(computeCapability);\n }\n \n } // namespace triton"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 17, "deletions": 6, "changes": 23, "file_content_changes": "@@ -167,6 +167,20 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n \n } // namespace\n \n+/// Helper function to get strides from a given shape and its order\n+static SmallVector<Value>\n+getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,\n+                            Location loc, ConversionPatternRewriter &rewriter) {\n+  auto rank = shape.size();\n+  SmallVector<Value> strides(rank);\n+  auto stride = 1;\n+  for (auto idx : order) {\n+    strides[idx] = i32_val(stride);\n+    stride *= shape[idx];\n+  }\n+  return strides;\n+}\n+\n struct SharedMemoryObject {\n   Value base; // i32 ptr. The start address of the shared memory object.\n   // We need to store strides as Values but not integers because the\n@@ -193,13 +207,10 @@ struct SharedMemoryObject {\n                      ArrayRef<unsigned> order, Location loc,\n                      ConversionPatternRewriter &rewriter)\n       : base(base) {\n-    auto rank = shape.size();\n-    auto stride = 1;\n-    strides.resize(rank);\n+    strides = getStridesFromShapeAndOrder(shape, order, loc, rewriter);\n+\n     for (auto idx : order) {\n-      strides[idx] = i32_val(stride);\n       offsets.emplace_back(i32_val(0));\n-      stride *= shape[idx];\n     }\n   }\n \n@@ -233,7 +244,7 @@ struct SharedMemoryObject {\n   }\n };\n \n-static mlir::LLVM::SharedMemoryObject\n+static SharedMemoryObject\n getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n                                 ConversionPatternRewriter &rewriter) {\n   auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -278,6 +278,20 @@ struct TritonStorePattern : public OpConversionPattern<triton::StoreOp> {\n   }\n };\n \n+struct TritonAtomicCASPattern\n+    : public OpConversionPattern<triton::AtomicCASOp> {\n+  using OpConversionPattern<triton::AtomicCASOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n+        op, typeConverter->convertType(op.getType()), \n+        adaptor.ptr(), adaptor.cmp(), adaptor.val());\n+    return success();\n+  }\n+};\n+\n struct TritonAtomicRMWPattern\n     : public OpConversionPattern<triton::AtomicRMWOp> {\n   using OpConversionPattern<triton::AtomicRMWOp>::OpConversionPattern;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 67, "deletions": 31, "changes": 98, "file_content_changes": "@@ -561,6 +561,7 @@ class RematerializeForward : public mlir::RewritePattern {\n // -----------------------------------------------------------------------------\n //\n // -----------------------------------------------------------------------------\n+namespace {\n static int computeCapabilityToMMAVersion(int computeCapability) {\n   if (computeCapability < 80) {\n     return 1;\n@@ -575,16 +576,68 @@ static int computeCapabilityToMMAVersion(int computeCapability) {\n static SmallVector<int64_t, 2>\n mmaVersionToShapePerWarp(int version, const ArrayRef<int64_t> &shape,\n                          int numWarps) {\n-  if (version == 1) {\n+  if (version == 1)\n     return {16, 16};\n-  } else if (version == 2) {\n+  else if (version == 2)\n     return {16, 8};\n-  } else {\n+  else {\n     assert(false && \"version not supported\");\n     return {0, 0};\n   }\n }\n \n+template <int version>\n+SmallVector<unsigned, 2> warpsPerTile(const ArrayRef<int64_t> shape,\n+                                      int numWarps);\n+\n+template <>\n+SmallVector<unsigned, 2> warpsPerTile<1>(const ArrayRef<int64_t> shape,\n+                                         int numWarps) {\n+  SmallVector<unsigned, 2> ret = {1, 1};\n+  SmallVector<int64_t, 2> shapePerWarp =\n+      mmaVersionToShapePerWarp(1, shape, numWarps);\n+  bool changed = false;\n+  do {\n+    changed = false;\n+    if (ret[0] * ret[1] < numWarps) {\n+      ret[0] = std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n+      changed = true;\n+    }\n+    if (ret[0] * ret[1] < numWarps) {\n+      ret[1] = std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n+      changed = true;\n+    }\n+  } while (changed);\n+  return ret;\n+}\n+\n+template <>\n+SmallVector<unsigned, 2> warpsPerTile<2>(const ArrayRef<int64_t> shape,\n+                                         int numWarps) {\n+  SmallVector<unsigned, 2> ret = {1, 1};\n+  SmallVector<int64_t, 2> shapePerWarp =\n+      mmaVersionToShapePerWarp(2, shape, numWarps);\n+  // TODO (@daadaada): double-check.\n+  // original logic in\n+  // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n+  // seems buggy for shape = [32, 16] ?\n+  do {\n+    if (ret[0] * ret[1] >= numWarps)\n+      break;\n+    if (shape[0] / shapePerWarp[0] / ret[0] >=\n+        shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n+      if (ret[0] < shape[0] / shapePerWarp[0]) {\n+        ret[0] *= 2;\n+      } else\n+        ret[1] *= 2;\n+    } else {\n+      ret[1] *= 2;\n+    }\n+  } while (true);\n+  return ret;\n+}\n+\n+} // namespace\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n \n@@ -593,34 +646,17 @@ class BlockedToMMA : public mlir::RewritePattern {\n       : mlir::RewritePattern(triton::DotOp::getOperationName(), 2, context),\n         computeCapability(computeCapability) {}\n \n-  static SmallVector<unsigned, 2>\n-  getWarpsPerTile(const ArrayRef<int64_t> &shape, int version, int numWarps) {\n-    assert(version == 2);\n-    // TODO: Handle one warp per row for fused matmuls\n-    // TODO: unsigned -> int64_t to keep things uniform\n-    SmallVector<unsigned, 2> ret = {1, 1};\n-    SmallVector<int64_t, 2> shapePerWarp =\n-        mmaVersionToShapePerWarp(version, shape, numWarps);\n-    bool changed = false;\n-    // TODO (@daadaada): double-check.\n-    // original logic in\n-    // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n-    // seems buggy for shape = [32, 16] ?\n-    do {\n-      changed = false;\n-      if (ret[0] * ret[1] >= numWarps)\n-        break;\n-      if (shape[0] / shapePerWarp[0] / ret[0] >=\n-          shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n-        if (ret[0] < shape[0] / shapePerWarp[0]) {\n-          ret[0] *= 2;\n-        } else\n-          ret[1] *= 2;\n-      } else {\n-        ret[1] *= 2;\n-      }\n-    } while (true);\n-    return ret;\n+  static SmallVector<unsigned, 2> getWarpsPerTile(const ArrayRef<int64_t> shape,\n+                                                  int version, int numWarps) {\n+    switch (version) {\n+    case 1:\n+      return warpsPerTile<1>(shape, numWarps);\n+    case 2:\n+      return warpsPerTile<2>(shape, numWarps);\n+    default:\n+      assert(false && \"not supported version\");\n+      return {0, 0};\n+    }\n   }\n \n   mlir::LogicalResult"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -202,8 +202,7 @@ LogicalResult LoopPipeliner::initialize() {\n             bufferShape.insert(bufferShape.begin(), numStages);\n             auto sharedEnc = ttg::SharedEncodingAttr::get(\n                 ty.getContext(), dotOpEnc, ty.getShape(),\n-                triton::gpu::getOrder(ty.getEncoding()),\n-                ty.getElementType());\n+                triton::gpu::getOrder(ty.getEncoding()), ty.getElementType());\n             loadsBufferType[loadOp] = RankedTensorType::get(\n                 bufferShape, ty.getElementType(), sharedEnc);\n           }"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -119,7 +119,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module) {\n \n std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n-                           mlir::ModuleOp module) {\n+                           mlir::ModuleOp module, int computeCapability) {\n   mlir::PassManager pm(module->getContext());\n   applyPassManagerCLOptions(pm);\n   auto printingFlags = mlir::OpPrintingFlags();"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 28, "deletions": 7, "changes": 35, "file_content_changes": "@@ -105,7 +105,7 @@ void init_triton_ir(py::module &&m) {\n       .value(\"AND\", mlir::triton::RMWOp::AND)\n       .value(\"OR\", mlir::triton::RMWOp::OR)\n       .value(\"XOR\", mlir::triton::RMWOp::XOR)\n-      // .value(\"XCHG\", mlir::triton::RMWOp::Xchg)\n+      .value(\"XCHG\", mlir::triton::RMWOp::XCHG)\n       .value(\"MAX\", mlir::triton::RMWOp::MAX)\n       .value(\"MIN\", mlir::triton::RMWOp::MIN)\n       .value(\"UMIN\", mlir::triton::RMWOp::UMIN)\n@@ -1095,9 +1095,18 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n               mlir::Value &val) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n-                                .cast<mlir::triton::PointerType>();\n-             mlir::Type dstType = ptrType.getPointeeType();\n+             mlir::Type dstType;\n+             if (auto srcTensorType = ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n+               mlir::Type dstElemType = srcTensorType.getElementType()\n+                                            .cast<mlir::triton::PointerType>()\n+                                            .getPointeeType();\n+               dstType = mlir::RankedTensorType::get(srcTensorType.getShape(),\n+                                                     dstElemType);\n+             } else {\n+               auto ptrType = mlir::getElementTypeOrSelf(ptr)\n+                                  .cast<mlir::triton::PointerType>();\n+               dstType = ptrType.getPointeeType();\n+             }\n              return self.create<mlir::triton::AtomicCASOp>(loc, dstType, ptr,\n                                                            cmp, val);\n            })\n@@ -1106,7 +1115,19 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &ptr, mlir::Value &val,\n               mlir::Value &mask) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             mlir::Type dstType = val.getType();\n+             mlir::Type dstType;\n+             if (auto srcTensorType =\n+                     ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n+               mlir::Type dstElemType = srcTensorType.getElementType()\n+                                            .cast<mlir::triton::PointerType>()\n+                                            .getPointeeType();\n+               dstType = mlir::RankedTensorType::get(srcTensorType.getShape(),\n+                                                     dstElemType);\n+             } else {\n+               auto ptrType = mlir::getElementTypeOrSelf(ptr)\n+                                  .cast<mlir::triton::PointerType>();\n+               dstType = ptrType.getPointeeType();\n+             }\n              return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n                                                            ptr, val, mask);\n            })\n@@ -1304,8 +1325,8 @@ void init_triton_translation(py::module &m) {\n       \"translate_triton_gpu_to_llvmir\",\n       [](mlir::ModuleOp op, int computeCapability) {\n         llvm::LLVMContext llvmContext;\n-        auto llvmModule =\n-            ::mlir::triton::translateTritonGPUToLLVMIR(&llvmContext, op);\n+        auto llvmModule = ::mlir::triton::translateTritonGPUToLLVMIR(\n+            &llvmContext, op, computeCapability);\n         if (!llvmModule)\n           llvm::report_fatal_error(\"Failed to translate TritonGPU to LLVM IR.\");\n "}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 76, "deletions": 80, "changes": 156, "file_content_changes": "@@ -595,100 +595,86 @@ def without_fn(X, Y, A, B, C):\n         assert c_tri == c_ref\n \n \n-# # ---------------\n-# # test atomics\n-# # ---------------\n-# @pytest.mark.parametrize(\"op, dtype_x_str, mode\", itertools.chain.from_iterable([\n-#     [\n-#         ('add', 'float16', mode),\n-#         ('add', 'uint32', mode), ('add', 'int32', mode), ('add', 'float32', mode),\n-#         ('max', 'uint32', mode), ('max', 'int32', mode), ('max', 'float32', mode),\n-#         ('min', 'uint32', mode), ('min', 'int32', mode), ('min', 'float32', mode),\n-#     ]\n-#     for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']]))\n-# def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n-#     n_programs = 5\n-\n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, Z):\n-#         pid = tl.program_id(0)\n-#         x = tl.load(X + pid)\n-#         old = GENERATE_TEST_HERE\n+# ---------------\n+# test atomics\n+# ---------------\n+@pytest.mark.parametrize(\"op, dtype_x_str, mode\", itertools.chain.from_iterable([\n+    [\n+        ('add', 'float16', mode),\n+        ('add', 'uint32', mode), ('add', 'int32', mode), ('add', 'float32', mode),\n+        ('max', 'uint32', mode), ('max', 'int32', mode), ('max', 'float32', mode),\n+        ('min', 'uint32', mode), ('min', 'int32', mode), ('min', 'float32', mode),\n+    ]\n+    for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']]))\n+def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n+    n_programs = 5\n \n-#     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x)'})\n-#     numpy_op = {'add': np.sum, 'max': np.max, 'min': np.min}[op]\n-#     max_neutral = float('-inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).min\n-#     min_neutral = float('inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).max\n-#     neutral = {'add': 0, 'max': max_neutral, 'min': min_neutral}[op]\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z):\n+        pid = tl.program_id(0)\n+        x = tl.load(X + pid)\n+        old = GENERATE_TEST_HERE\n \n-#     # triton result\n-#     rs = RandomState(17)\n-#     x = numpy_random((n_programs, ), dtype_str=dtype_x_str, rs=rs)\n-#     if mode == 'all_neg':\n-#         x = -np.abs(x)\n-#     if mode == 'all_pos':\n-#         x = np.abs(x)\n-#     if mode == 'min_neg':\n-#         idx = rs.randint(n_programs, size=(1, )).item()\n-#         x[idx] = -np.max(np.abs(x)) - 1\n-#     if mode == 'max_pos':\n-#         idx = rs.randint(n_programs, size=(1, )).item()\n-#         x[idx] = np.max(np.abs(x)) + 1\n-#     x_tri = to_triton(x, device=device)\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x)'})\n+    numpy_op = {'add': np.sum, 'max': np.max, 'min': np.min}[op]\n+    max_neutral = float('-inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).min\n+    min_neutral = float('inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).max\n+    neutral = {'add': 0, 'max': max_neutral, 'min': min_neutral}[op]\n \n-#     z_tri = to_triton(np.array([neutral], dtype=getattr(np, dtype_x_str)), device=device)\n-#     kernel[(n_programs, )](x_tri, z_tri)\n-#     # torch result\n-#     z_ref = numpy_op(x).astype(getattr(np, dtype_x_str))\n-#     # compare\n-#     exact = op not in ['add']\n-#     if exact:\n-#         assert z_ref.item() == to_numpy(z_tri).item()\n-#     else:\n-#         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    # triton result\n+    rs = RandomState(17)\n+    x = numpy_random((n_programs, ), dtype_str=dtype_x_str, rs=rs)\n+    if mode == 'all_neg':\n+        x = -np.abs(x)\n+    if mode == 'all_pos':\n+        x = np.abs(x)\n+    if mode == 'min_neg':\n+        idx = rs.randint(n_programs, size=(1, )).item()\n+        x[idx] = -np.max(np.abs(x)) - 1\n+    if mode == 'max_pos':\n+        idx = rs.randint(n_programs, size=(1, )).item()\n+        x[idx] = np.max(np.abs(x)) + 1\n+    x_tri = to_triton(x, device=device)\n \n+    z_tri = to_triton(np.array([neutral], dtype=getattr(np, dtype_x_str)), device=device)\n+    kernel[(n_programs, )](x_tri, z_tri)\n+    # torch result\n+    z_ref = numpy_op(x).astype(getattr(np, dtype_x_str))\n+    # compare\n+    exact = op not in ['add']\n+    if exact:\n+        assert z_ref.item() == to_numpy(z_tri).item()\n+    else:\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n-# @pytest.mark.parametrize(\"axis\", [0, 1])\n-# def test_tensor_atomic_rmw(axis, device=\"cuda\"):\n-#     shape0, shape1 = 8, 8\n-#     # triton kernel\n \n-#     @triton.jit\n-#     def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n-#         off0 = tl.arange(0, SHAPE0)\n-#         off1 = tl.arange(0, SHAPE1)\n-#         x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n-#         z = tl.sum(x, axis=AXIS)\n-#         tl.atomic_add(Z + off0, z)\n-#     rs = RandomState(17)\n-#     x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n-#     # reference result\n-#     z_ref = np.sum(x, axis=axis)\n-#     # triton result\n-#     x_tri = to_triton(x, device=device)\n-#     z_tri = to_triton(np.zeros((shape0,), dtype=\"float32\"), device=device)\n-#     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n-#     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n+@pytest.mark.parametrize(\"shape, axis\",\n+                         [(shape, axis) for shape in [(2, 2), (2, 8), (8, 2), (8, 8), (32, 32)] for axis in [0, 1]])\n+def test_tensor_atomic_rmw(shape, axis, device=\"cuda\"):\n+    shape0, shape1 = shape\n+    # triton kernel\n \n-def test_tensor_atomic_rmw_add_elementwise(device=\"cuda\"):\n-    shape0, shape1 = 2, 8\n     @triton.jit\n-    def kernel(Z, X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+    def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n         off0 = tl.arange(0, SHAPE0)\n         off1 = tl.arange(0, SHAPE1)\n         x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n-        tl.atomic_add(Z + off0[:, None] * SHAPE1 + off1[None, :], x)\n-\n+        z = tl.sum(x, axis=AXIS)\n+        if AXIS == 1:\n+            tl.atomic_add(Z + off0, z)\n+        else:\n+            tl.atomic_add(Z + off1, z)\n     rs = RandomState(17)\n     x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n-    z = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n-    # reference\n-    z_ref = z + x\n+    print(x)\n+    # reference result\n+    z_ref = np.sum(x, axis=axis, keepdims=False)\n     # triton result\n-    x_tri = torch.from_numpy(x).to(device=device)\n-    z_tri = torch.from_numpy(z).to(device=device)\n-    kernel[(1,)](z_tri, x_tri, shape0, shape1)\n+    x_tri = to_triton(x, device=device)\n+    z_shape = (shape0, ) if axis == 1 else (shape1, )\n+    z_tri = to_triton(np.zeros(z_shape, dtype=\"float32\"), device=device)\n+    kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n # def test_atomic_cas():\n@@ -720,6 +706,16 @@ def kernel(Z, X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n #     serialized_add[(64,)](data, Lock)\n #     triton.testing.assert_almost_equal(data, ref)\n \n+def test_simple_atomic_cas():\n+    # 1. make sure that atomic_cas changes the original value (Lock)\n+    @triton.jit\n+    def change_value(Lock):\n+        tl.atomic_cas(Lock, 0, 1)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    change_value[(1,)](Lock)\n+\n+    assert (Lock[0] == 1)\n \n # # ---------------\n # # test cast"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -172,6 +172,8 @@ def get_proper_err(a, b, golden):\n     [128, 64, 128, 4, 128, 64, 128, False, False],\n     [16, 16, 16, 16, 16, 16, 16, False, False],  # wpt overflow issue\n     # K-Forloop\n+    [32, 32, 64, 4, 32, 32, 32, False, False], # Single shared encoding\n+    [16, 16, 128, 4, 16, 16, 16, False, False], # Single shared encoding and small k\n     [64, 32, 128, 4, 64, 32, 64, False, False],\n     [128, 16, 128, 4, 128, 16, 32, False, False],\n     [32, 16, 128, 4, 32, 16, 32, False, False],"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -977,7 +977,12 @@ def ptx_get_version(cuda_version) -> int:\n \n \n def path_to_ptxas():\n-    prefixes = [os.environ.get(\"TRITON_PTXAS_PATH\", \"\"), \"\", os.environ.get('CUDA_PATH', default_cuda_dir())]\n+    prefixes = [\n+        os.environ.get(\"TRITON_PTXAS_PATH\", \"\"),\n+        \"\",\n+        \"/usr\",\n+        os.environ.get('CUDA_PATH', default_cuda_dir())\n+    ]\n     for prefix in prefixes:\n         ptxas = os.path.join(prefix, \"bin\", \"ptxas\")\n         if os.path.exists(ptxas):\n@@ -1416,7 +1421,8 @@ def compile(fn, **kwargs):\n         path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n         if ir == ext:\n             next_module = parse(fn)\n-        elif os.path.exists(path) and\\\n+        elif os.path.exists(path) and \\\n+                ir in metadata[\"ctime\"] and \\\n                 os.path.getctime(path) == metadata[\"ctime\"][ir]:\n             next_module = parse(path)\n         else:"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -875,8 +875,8 @@ def atomic_max(ptr: tl.tensor,\n     # return atomic_umin(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n@@ -907,8 +907,8 @@ def atomic_min(ptr: tl.tensor,\n     # return atomic_umax(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n-    pos = greater_equal(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n-    neg = less_than(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n+    pos = greater_equal(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n+    neg = less_than(val, tl.tensor(builder.get_float32(0), sca_ty), builder)\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                   i_ptr.handle,\n                                                   i_val.handle,"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -65,6 +65,20 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   return\n }\n \n+// CHECK-LABEL: insert_slice\n+func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n+  %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n+  %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n+  %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  // CHECK: %cst_0 -> %cst_0\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n+  %index = arith.constant 0 : index\n+  %a = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n+  // CHECK: %3 -> %cst_0\n+  %b = tensor.insert_slice %a into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n+  return\n+}\n+\n // CHECK-LABEL: extract_slice\n func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "file_content_changes": "@@ -119,8 +119,26 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %tensor = triton_gpu.alloc_tensor : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n+  // CHECK: Membar 6\n   %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n-  // CHECK: Membar 7\n+  // CHECK: Membar 8\n+  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n+  return\n+}\n+\n+// CHECK-LABEL: insert_slice\n+func @insert_slice(%A : !tt.ptr<f16>, %i1 : i1) {\n+  %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n+  %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n+  %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n+  %index = arith.constant 0 : index\n+  %al = tt.load %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #AL>\n+  // CHECK: Membar 6\n+  %a = tensor.insert_slice %al into %tensor[%index, 0, 0][1, 16, 16][1, 1, 1]: tensor<16x16xf16, #AL> into tensor<1x16x16xf16, #A_SHARED>\n+  // CHECK: Membar 8\n+  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n+  // CHECK: Membar 10\n   %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n   return\n }"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 27, "deletions": 1, "changes": 28, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -convert-triton-to-tritongpu=num-warps=2 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu=num-warps=2 | FileCheck %s\n \n func @ops() {\n   // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n@@ -9,6 +9,8 @@ func @ops() {\n   return\n }\n \n+// -----\n+\n func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   // Test if LoadOp is lowered properly (see #771)\n   %ptrs = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n@@ -25,3 +27,27 @@ func @load_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   tt.store %ptrs, %c : tensor<128xf32>\n   return\n }\n+\n+// -----\n+\n+func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+  // Test if the total number of threadsPerWarp is 32\n+  // Test if the total number of warps is 2\n+  // CHECK: #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 2], order = [0, 1]}>\n+  // CHECK: module attributes {\"triton_gpu.num-warps\" = 2 : i32} {{.*}}\n+  %c0 = arith.constant dense<1.00e+00> : tensor<4x4xf32>\n+  %c1 = arith.constant dense<2.00e+00> : tensor<8x2xf32>\n+  %c2 = arith.constant dense<3.00e+00> : tensor<16x16xf32>\n+  // CHECK: tensor<4x4xf32, #blocked0> -> tensor<4xf32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>\n+  %c0_ = tt.reduce %c0 {redOp = 1 : i32, axis = 0 : i32} : tensor<4x4xf32> -> tensor<4xf32>\n+  // CHECK: tensor<8x2xf32, #blocked1> -> tensor<2xf32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>\n+  %c1_ = tt.reduce %c1 {redOp = 1 : i32, axis = 0 : i32} : tensor<8x2xf32> -> tensor<2xf32>\n+  // CHECK: tensor<8x2xf32, #blocked1> -> tensor<8xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %c2_ = tt.reduce %c1 {redOp = 1 : i32, axis = 1 : i32} : tensor<8x2xf32> -> tensor<8xf32>\n+  // CHECK: tensor<16x16xf32, #blocked2> -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+  %c3_ = tt.reduce %c2 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf32> -> tensor<16xf32>\n+\n+  return\n+}"}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -34,7 +34,8 @@ TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n \n   // create element type\n   Type eltType = IntegerType::get(&ctx, params.typeWidth);\n-  auto layout = SharedEncodingAttr::get(&ctx, encoding, params.shape, {1, 0}, eltType);\n+  auto layout =\n+      SharedEncodingAttr::get(&ctx, encoding, params.shape, {1, 0}, eltType);\n \n   ASSERT_EQ(layout.getVec(), params.refSwizzle.vec);\n   ASSERT_EQ(layout.getPerPhase(), params.refSwizzle.perPhase);"}]