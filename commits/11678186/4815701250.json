[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 126, "deletions": 81, "changes": 207, "file_content_changes": "@@ -19,10 +19,10 @@ using ::mlir::triton::gpu::SharedEncodingAttr;\n class MMA16816SmemLoader {\n public:\n   MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n-                     uint32_t kWidth, ArrayRef<Value> smemStrides,\n-                     ArrayRef<int64_t> tileShape, ArrayRef<int> instrShape,\n-                     ArrayRef<int> matShape, int perPhase, int maxPhase,\n-                     int elemBytes, ConversionPatternRewriter &rewriter,\n+                     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+                     ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                     int perPhase, int maxPhase, int elemBytes,\n+                     ConversionPatternRewriter &rewriter,\n                      TritonGPUToLLVMTypeConverter *typeConverter,\n                      const Location &loc);\n \n@@ -82,8 +82,6 @@ class MMA16816SmemLoader {\n   int warpOffStride;\n };\n \n-Type getMatType(Type argType);\n-\n SmallVector<Value>\n MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n                                            Value cSwizzleOffset) {\n@@ -181,7 +179,6 @@ SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value warpOff,\n                                                          Value cSwizzleOffset,\n                                                          int elemBytes) {\n   assert(elemBytes <= 4);\n-\n   int cTileShape = tileShape[order[0]];\n   int sTileShape = tileShape[order[1]];\n   if (!needTrans) {\n@@ -307,6 +304,7 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n     return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n             extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n   } else {\n+    elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n     // base pointers\n     std::array<std::array<Value, 4>, 2> ptrs;\n     int vecWidth = 4 / elemBytes;\n@@ -334,23 +332,26 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n     if ((needTrans && kOrder == 1) || (!needTrans && kOrder == 0))\n       std::swap(vals[1], vals[2]);\n     // pack loaded vectors into 4 32-bit values\n-    elemTy = getMatType(vals[0][0].getType())\n-                 .cast<LLVM::LLVMStructType>()\n-                 .getBody()[0];\n     std::array<Value, 4> retElems;\n     retElems.fill(undef(elemTy));\n     for (int m = 0; m < 4; ++m) {\n       for (int e = 0; e < vecWidth; ++e)\n         retElems[m] = insert_element(retElems[m].getType(), retElems[m],\n                                      vals[m][e], i32_val(e));\n     }\n-    return {bitcast(retElems[0], i32_ty), bitcast(retElems[1], i32_ty),\n-            bitcast(retElems[2], i32_ty), bitcast(retElems[3], i32_ty)};\n+    if (elemBytes == 1)\n+      return {bitcast(retElems[0], i32_ty), bitcast(retElems[1], i32_ty),\n+              bitcast(retElems[2], i32_ty), bitcast(retElems[3], i32_ty)};\n+    else\n+      return {retElems[0], retElems[1], retElems[2], retElems[3]};\n   }\n+\n+  assert(false && \"Invalid smem load\");\n+  return {Value{}, Value{}, Value{}, Value{}};\n }\n \n MMA16816SmemLoader::MMA16816SmemLoader(\n-    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder, uint32_t kWidth,\n+    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n     ArrayRef<int> instrShape, ArrayRef<int> matShape, int perPhase,\n     int maxPhase, int elemBytes, ConversionPatternRewriter &rewriter,\n@@ -369,7 +370,6 @@ MMA16816SmemLoader::MMA16816SmemLoader(\n   // rule: k must be the fast-changing axis.\n   needTrans = kOrder != order[0];\n   canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n-  canUseLdmatrix = canUseLdmatrix && (kWidth == 4 / elemBytes);\n \n   if (canUseLdmatrix) {\n     // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n@@ -468,12 +468,13 @@ Value composeValuesToDotOperandLayoutStruct(\n   return result;\n }\n \n-std::function<void(int, int)> getLoadMatrixFn(\n-    Value tensor, const SharedMemoryObject &smemObj, MmaEncodingAttr mmaLayout,\n-    int wpt, uint32_t kOrder, uint32_t kWidth, SmallVector<int> instrShape,\n-    SmallVector<int> matShape, Value warpId, Value lane, ValueTable &vals,\n-    bool isA, TritonGPUToLLVMTypeConverter *typeConverter,\n-    ConversionPatternRewriter &rewriter, Location loc) {\n+std::function<void(int, int)>\n+getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n+                MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n+                SmallVector<int> instrShape, SmallVector<int> matShape,\n+                Value warpId, Value lane, ValueTable &vals, bool isA,\n+                TritonGPUToLLVMTypeConverter *typeConverter,\n+                ConversionPatternRewriter &rewriter, Location loc) {\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   Type eltTy = tensorTy.getElementType();\n   // We assumes that the input operand of Dot should be from shared layout.\n@@ -484,99 +485,143 @@ std::function<void(int, int)> getLoadMatrixFn(\n   const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n   auto order = sharedLayout.getOrder();\n \n+  // the original register_lds2, but discard the prefetch logic.\n+  auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n+    vals[{mn, k}] = val;\n+  };\n+\n   // (a, b) is the coordinate.\n-  auto load = [=, &rewriter, &vals](int a, int b) {\n+  auto load = [=, &rewriter, &vals, &ld2](int a, int b) {\n     MMA16816SmemLoader loader(\n-        wpt, sharedLayout.getOrder(), kOrder, kWidth, smemObj.strides,\n+        wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n         tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n         maxPhase, elemBytes, rewriter, typeConverter, loc);\n     Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n     SmallVector<Value> offs =\n         loader.computeOffsets(warpId, lane, cSwizzleOffset);\n-    // initialize pointers\n     const int numPtrs = loader.getNumPtrs();\n     SmallVector<Value> ptrs(numPtrs);\n+\n     Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n     Type smemPtrTy = getShemPtrTy(eltTy);\n-    for (int i = 0; i < numPtrs; ++i)\n-      ptrs[i] = bitcast(gep(smemPtrTy, smemBase, offs[i]), smemPtrTy);\n-    // actually load from shared memory\n-    auto matTy = LLVM::LLVMStructType::getLiteral(eltTy.getContext(),\n-                                                  SmallVector<Type>(4, i32_ty));\n+    for (int i = 0; i < numPtrs; ++i) {\n+      ptrs[i] =\n+          bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n+    }\n+\n     auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n         (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n-        ptrs, matTy, getShemPtrTy(eltTy));\n-    if (!isA)\n-      std::swap(ha1, ha2);\n-    // update user-provided values in-place\n-    vals[{a, b}] = ha0;\n-    vals[{a + 1, b}] = ha1;\n-    vals[{a, b + 1}] = ha2;\n-    vals[{a + 1, b + 1}] = ha3;\n+        ptrs, getMatType(eltTy), getShemPtrTy(eltTy));\n+\n+    if (isA) {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha1);\n+      ld2(vals, a, b + 1, ha2);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    } else {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha2);\n+      ld2(vals, a, b + 1, ha1);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    }\n   };\n \n   return load;\n }\n \n-Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n-              DotOperandEncodingAttr encoding,\n-              const SharedMemoryObject &smemObj,\n-              TritonGPUToLLVMTypeConverter *typeConverter, Value thread,\n-              bool isA) {\n+Value loadA(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n+            DotOperandEncodingAttr aEncoding, const SharedMemoryObject &smemObj,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n+  int bitwidth = aTensorTy.getElementTypeBitWidth();\n+  auto mmaLayout = aEncoding.getParent().cast<MmaEncodingAttr>();\n+\n+  SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n+                             aTensorTy.getShape().end());\n+\n+  ValueTable ha;\n+  std::function<void(int, int)> loadFn;\n+  int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n+  int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n+\n+  auto numRep = aEncoding.getMMAv2Rep(aTensorTy.getShape(), bitwidth);\n+  int numRepM = numRep[0];\n+  int numRepK = numRep[1];\n+\n+  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+    int wpt0 = mmaLayout.getWarpsPerCTA()[0];\n+    Value warp = udiv(thread, i32_val(32));\n+    Value lane = urem(thread, i32_val(32));\n+    Value warpM = urem(urem(warp, i32_val(wpt0)), i32_val(shape[0] / 16));\n+    // load from smem\n+    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+    int wpt = std::min<int>(wpt0, shape[0] / 16);\n+    loadFn = getLoadMatrixFn(\n+        tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n+        {mmaInstrM, mmaInstrK} /*instrShape*/,\n+        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, lane /*laneId*/,\n+        ha /*vals*/, true /*isA*/, typeConverter /* typeConverter */,\n+        rewriter /*rewriter*/, loc /*loc*/);\n+  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+    // load from registers, used in gemm fuse\n+    // TODO(Superjomn) Port the logic.\n+    assert(false && \"Loading A from register is not supported yet.\");\n+  } else {\n+    assert(false && \"A's layout is not supported.\");\n+  }\n+\n+  // step1. Perform loading.\n+  for (int m = 0; m < numRepM; ++m)\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * m, 2 * k);\n+\n+  // step2. Format the values to LLVM::Struct to passing to mma codegen.\n+  return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK,\n+                                               typeConverter, loc, rewriter);\n+}\n+\n+Value loadB(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n+            DotOperandEncodingAttr bEncoding, const SharedMemoryObject &smemObj,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  ValueTable hb;\n   auto tensorTy = tensor.getType().cast<RankedTensorType>();\n   int bitwidth = tensorTy.getElementTypeBitWidth();\n-  auto mmaLayout = encoding.getParent().cast<MmaEncodingAttr>();\n+  auto mmaLayout = bEncoding.getParent().cast<MmaEncodingAttr>();\n \n   SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n                              tensorTy.getShape().end());\n \n-  ValueTable vals;\n   int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n   int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n \n-  auto numRep = encoding.getMMAv2Rep(tensorTy.getShape(), bitwidth);\n-  int kWidth = encoding.getMMAv2kWidth();\n+  auto numRep = bEncoding.getMMAv2Rep(tensorTy.getShape(), bitwidth);\n+  int numRepK = numRep[0];\n+  int numRepN = numRep[1];\n \n   int wpt0 = mmaLayout.getWarpsPerCTA()[0];\n   int wpt1 = mmaLayout.getWarpsPerCTA()[1];\n   Value warp = udiv(thread, i32_val(32));\n   Value lane = urem(thread, i32_val(32));\n-  Value warpM = urem(urem(warp, i32_val(wpt0)), i32_val(shape[0] / 16));\n   Value warpMN = udiv(warp, i32_val(wpt0));\n   Value warpN = urem(urem(warpMN, i32_val(wpt1)), i32_val(shape[1] / 8));\n-\n-  int wpt;\n-  if (isA)\n-    wpt = std::min<int>(wpt0, shape[0] / 16);\n-  else\n-    wpt = std::min<int>(wpt1, shape[1] / 16);\n-\n-  std::function<void(int, int)> loadFn;\n-  if (isA)\n-    loadFn = getLoadMatrixFn(\n-        tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/, kWidth,\n-        {mmaInstrM, mmaInstrK} /*instrShape*/,\n-        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, lane /*laneId*/,\n-        vals /*vals*/, isA /*isA*/, typeConverter /* typeConverter */,\n-        rewriter /*rewriter*/, loc /*loc*/);\n-  else\n-    loadFn = getLoadMatrixFn(\n-        tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/, kWidth,\n-        {mmaInstrK, mmaInstrN} /*instrShape*/,\n-        {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, lane /*laneId*/,\n-        vals /*vals*/, isA /*isA*/, typeConverter /* typeConverter */,\n-        rewriter /*rewriter*/, loc /*loc*/);\n-\n-  // Perform loading.\n-  int numRepOuter = isA ? numRep[0] : std::max<int>(numRep[1] / 2, 1);\n-  int numRepK = isA ? numRep[1] : numRep[0];\n-  for (int m = 0; m < numRepOuter; ++m)\n+  // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+  int wpt = std::min<int>(wpt1, shape[1] / 16);\n+  auto loadFn = getLoadMatrixFn(\n+      tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n+      {mmaInstrK, mmaInstrN} /*instrShape*/,\n+      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, lane /*laneId*/,\n+      hb /*vals*/, false /*isA*/, typeConverter /* typeConverter */,\n+      rewriter /*rewriter*/, loc /*loc*/);\n+\n+  for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n     for (int k = 0; k < numRepK; ++k)\n-      loadFn(2 * m, 2 * k);\n+      loadFn(2 * n, 2 * k);\n+  }\n \n-  // Format the values to LLVM::Struct to passing to mma codegen.\n-  return composeValuesToDotOperandLayoutStruct(vals, numRepOuter, numRepK,\n-                                               typeConverter, loc, rewriter);\n+  Value result = composeValuesToDotOperandLayoutStruct(\n+      hb, std::max(numRepN / 2, 1), numRepK, typeConverter, loc, rewriter);\n+  return result;\n }\n \n namespace SharedToDotOperandMMAv2 {\n@@ -585,12 +630,12 @@ Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,\n                     const SharedMemoryObject &smemObj,\n                     TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n   if (opIdx == 0)\n-    return loadArg(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n-                   thread, true);\n+    return loadA(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                 thread);\n   else {\n     assert(opIdx == 1);\n-    return loadArg(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n-                   thread, false);\n+    return loadB(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                 thread);\n   }\n }\n } // namespace SharedToDotOperandMMAv2"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 66, "deletions": 188, "changes": 254, "file_content_changes": "@@ -4,140 +4,11 @@ using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n \n-static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n-                                        Type inType, Type ouType) {\n-  auto inTensorTy = inType.dyn_cast<RankedTensorType>();\n-  auto ouTensorTy = ouType.dyn_cast<RankedTensorType>();\n-  if (!inTensorTy || !ouTensorTy)\n-    return values;\n-  auto inEncoding =\n-      dyn_cast<triton::gpu::DotOperandEncodingAttr>(inTensorTy.getEncoding());\n-  auto ouEncoding =\n-      dyn_cast<triton::gpu::DotOperandEncodingAttr>(ouTensorTy.getEncoding());\n-  assert(inEncoding == ouEncoding);\n-  if (!inEncoding)\n-    return values;\n-  size_t inBitWidth = inTensorTy.getElementType().getIntOrFloatBitWidth();\n-  size_t ouBitWidth = ouTensorTy.getElementType().getIntOrFloatBitWidth();\n-  auto ouEltTy = ouTensorTy.getElementType();\n-  if (inBitWidth == ouBitWidth)\n-    return values;\n-  if (inBitWidth == 16 && ouBitWidth == 32) {\n-    SmallVector<Value> ret;\n-    for (unsigned i = 0; i < values.size(); i += 8) {\n-      ret.push_back(values[i]);\n-      ret.push_back(values[i + 1]);\n-      ret.push_back(values[i + 4]);\n-      ret.push_back(values[i + 5]);\n-      ret.push_back(values[i + 2]);\n-      ret.push_back(values[i + 3]);\n-      ret.push_back(values[i + 6]);\n-      ret.push_back(values[i + 7]);\n-    }\n-    return ret;\n-  }\n-  if (inBitWidth == 8 && ouBitWidth == 16) {\n-    SmallVector<Value> ret;\n-    for (unsigned i = 0; i < values.size(); i += 16) {\n-      ret.push_back(values[i + 0]);\n-      ret.push_back(values[i + 1]);\n-      ret.push_back(values[i + 2]);\n-      ret.push_back(values[i + 3]);\n-      ret.push_back(values[i + 8]);\n-      ret.push_back(values[i + 9]);\n-      ret.push_back(values[i + 10]);\n-      ret.push_back(values[i + 11]);\n-      ret.push_back(values[i + 4]);\n-      ret.push_back(values[i + 5]);\n-      ret.push_back(values[i + 6]);\n-      ret.push_back(values[i + 7]);\n-      ret.push_back(values[i + 12]);\n-      ret.push_back(values[i + 13]);\n-      ret.push_back(values[i + 14]);\n-      ret.push_back(values[i + 15]);\n-    }\n-    return ret;\n-    // for (unsigned i = 0; i < values.size(); i += 16) {\n-    //   ret.push_back(values[i]);\n-    //   ret.push_back(values[i + 1]);\n-    //   ret.push_back(values[i + 4]);\n-    //   ret.push_back(values[i + 5]);\n-    //   ret.push_back(values[i + 8]);\n-    //   ret.push_back(values[i + 9]);\n-    //   ret.push_back(values[i + 12]);\n-    //   ret.push_back(values[i + 13]);\n-\n-    //   ret.push_back(values[i + 2]);\n-    //   ret.push_back(values[i + 3]);\n-    //   ret.push_back(values[i + 6]);\n-    //   ret.push_back(values[i + 7]);\n-    //   ret.push_back(values[i + 10]);\n-    //   ret.push_back(values[i + 11]);\n-    //   ret.push_back(values[i + 14]);\n-    //   ret.push_back(values[i + 15]);\n-    // }\n-    return values;\n-  }\n-  llvm_unreachable(\"unimplemented code path\");\n-}\n-\n-inline SmallVector<Value> unpackI32(const SmallVector<Value> &inValues,\n-                                    Type srcTy,\n-                                    ConversionPatternRewriter &rewriter,\n-                                    Location loc,\n-                                    TypeConverter *typeConverter) {\n-  auto tensorTy = srcTy.dyn_cast<RankedTensorType>();\n-  if (!tensorTy)\n-    return inValues;\n-  auto encoding = tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n-  if (!(encoding && encoding.getParent().isa<MmaEncodingAttr>()))\n-    return inValues;\n-  SmallVector<Value> outValues;\n-  for (auto v : inValues) {\n-    // cast i32 to appropriate eltType vector and extract elements\n-    auto eltType = typeConverter->convertType(tensorTy.getElementType());\n-    auto vecType = vec_ty(eltType, 32 / eltType.getIntOrFloatBitWidth());\n-    auto vec = bitcast(v, vecType);\n-    for (int i = 0; i < 32 / eltType.getIntOrFloatBitWidth(); i++) {\n-      outValues.push_back(extract_element(vec, i32_val(i)));\n-    }\n-  }\n-  return outValues;\n-}\n-\n-inline SmallVector<Value> packI32(const SmallVector<Value> &inValues,\n-                                  Type srcTy,\n-                                  ConversionPatternRewriter &rewriter,\n-                                  Location loc, TypeConverter *typeConverter) {\n-  auto tensorTy = srcTy.dyn_cast<RankedTensorType>();\n-  if (!tensorTy)\n-    return inValues;\n-  auto encoding = tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>();\n-  if (!(encoding && encoding.getParent().isa<MmaEncodingAttr>()))\n-    return inValues;\n-  SmallVector<Value> outValues;\n-  auto eltType = typeConverter->convertType(tensorTy.getElementType());\n-  int vecWidth = 32 / eltType.getIntOrFloatBitWidth();\n-  auto vecType = vec_ty(eltType, vecWidth);\n-  for (int i = 0; i < inValues.size(); i += vecWidth) {\n-    Value vec = undef(vecType);\n-    for (int j = 0; j < vecWidth; j++) {\n-      vec = insert_element(vec, inValues[i + j], i32_val(j));\n-    }\n-    outValues.push_back(bitcast(vec, i32_ty));\n-  }\n-  return outValues;\n-}\n-\n struct FpToFpOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::FpToFpOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n \n-  typedef std::function<SmallVector<Value>(\n-      Location, ConversionPatternRewriter &, const Value &, const Value &,\n-      const Value &, const Value &)>\n-      ConvertorT;\n   /* ------------------ */\n   // FP8 -> FP16\n   /* ------------------ */\n@@ -619,14 +490,35 @@ struct FpToFpOpConversion\n     return builder.launch(rewriter, loc, f16_ty, false);\n   }\n \n-  ConvertorT getConversionFunc(Type srcTy, Type dstTy) const {\n+  LogicalResult\n+  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n+    auto dstTensorType =\n+        op.getResult().getType().cast<mlir::RankedTensorType>();\n+    auto srcEltType = srcTensorType.getElementType();\n+    auto dstEltType = dstTensorType.getElementType();\n+    auto loc = op->getLoc();\n+    auto elems = getTotalElemsPerThread(dstTensorType);\n+    SmallVector<Value> resultVals;\n+    bool isSrcFP8 =\n+        srcEltType.isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>();\n+    bool isDstFP8 =\n+        dstEltType.isa<mlir::Float8E4M3FNType, mlir::Float8E5M2Type>();\n+\n+    // Select convertor\n+    typedef std::function<SmallVector<Value>(\n+        Location, ConversionPatternRewriter &, const Value &, const Value &,\n+        const Value &, const Value &)>\n+        ConvertorT;\n+\n     auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNType>();\n     auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n     auto F16TyID = TypeID::get<mlir::Float16Type>();\n     auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n     auto F32TyID = TypeID::get<mlir::Float32Type>();\n     auto F64TyID = TypeID::get<mlir::Float64Type>();\n-    static DenseMap<std::pair<TypeID, TypeID>, ConvertorT> convertorMap = {\n+    DenseMap<std::pair<TypeID, TypeID>, ConvertorT> convertorMap = {\n         // F8 -> F16\n         {{F8E4M3TyID, F16TyID}, convertFp8E4M3x4ToFp16x4},\n         {{F8E5M2TyID, F16TyID}, convertFp8E5M2x4ToFp16x4},\n@@ -647,46 +539,28 @@ struct FpToFpOpConversion\n         {{F32TyID, F8E5M2TyID}, convertFp32x4ToFp8E5M2x4},\n     };\n \n-    std::pair<TypeID, TypeID> key = {srcTy.getTypeID(), dstTy.getTypeID()};\n+    std::pair<TypeID, TypeID> key = {srcEltType.getTypeID(),\n+                                     dstEltType.getTypeID()};\n     if (convertorMap.count(key) == 0) {\n-      llvm::errs() << \"Unsupported conversion from \" << srcTy << \" to \" << dstTy\n-                   << \"\\n\";\n+      llvm::errs() << \"Unsupported conversion from \" << srcEltType << \" to \"\n+                   << dstEltType << \"\\n\";\n       llvm_unreachable(\"\");\n     }\n-    return convertorMap.lookup(key);\n-  }\n+    auto convertor = convertorMap.lookup(key);\n \n-  LogicalResult\n-  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    // llvm::outs() << 0 << \"\\n\";\n-    auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n-    auto dstTensorType =\n-        op.getResult().getType().cast<mlir::RankedTensorType>();\n-    auto loc = op->getLoc();\n-    // check that the number of elements is divisible by 4\n-    // Get convertor\n-    auto cvtFunc = getConversionFunc(srcTensorType.getElementType(),\n-                                     dstTensorType.getElementType());\n-    // Unpack value\n-    auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getFrom(),\n-                                                       rewriter, srcTensorType);\n-    inVals =\n-        unpackI32(inVals, srcTensorType, rewriter, loc, getTypeConverter());\n-    // Cast\n-    SmallVector<Value> outVals;\n-    auto elems = inVals.size();\n+    // Vectorized casting\n     assert(elems % 4 == 0 &&\n            \"FP8 casting only support tensors with 4-aligned sizes\");\n-    for (size_t i = 0; i < elems; i += 4)\n-      outVals.append(cvtFunc(loc, rewriter, inVals[i], inVals[i + 1],\n-                             inVals[i + 2], inVals[i + 3]));\n-    // Pack values\n-    assert(outVals.size() == elems);\n-    outVals = reorderValues(outVals, srcTensorType, dstTensorType);\n-    outVals =\n-        packI32(outVals, dstTensorType, rewriter, loc, getTypeConverter());\n-    auto result = getTypeConverter()->packLLElements(loc, outVals, rewriter,\n+    auto elements = getTypeConverter()->unpackLLElements(\n+        loc, adaptor.getFrom(), rewriter, srcTensorType);\n+    for (size_t i = 0; i < elems; i += 4) {\n+      auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n+                                 elements[i + 2], elements[i + 3]);\n+      resultVals.append(converted);\n+    }\n+\n+    assert(resultVals.size() == elems);\n+    auto result = getTypeConverter()->packLLElements(loc, resultVals, rewriter,\n                                                      dstTensorType);\n     rewriter.replaceOp(op, result);\n     return success();\n@@ -706,41 +580,45 @@ class ElementwiseOpConversionBase\n   LogicalResult\n   matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto argTy = op->getOperand(0).getType();\n     auto resultTy = op.getType();\n     Location loc = op->getLoc();\n-    // element type\n+\n+    unsigned elems = getTotalElemsPerThread(resultTy);\n     auto resultElementTy = getElementTypeOrSelf(resultTy);\n     Type elemTy = this->getTypeConverter()->convertType(resultElementTy);\n-    SmallVector<Value> resultVals;\n-    //\n-    SmallVector<SmallVector<Value>> allOperands;\n-    for (auto operand : adaptor.getOperands()) {\n-      auto sub_operands = this->getTypeConverter()->unpackLLElements(\n-          loc, operand, rewriter, argTy);\n-      sub_operands = unpackI32(sub_operands, argTy, rewriter, loc,\n-                               this->getTypeConverter());\n-      allOperands.resize(sub_operands.size());\n-      for (auto v : llvm::enumerate(sub_operands))\n-        allOperands[v.index()].push_back(v.value());\n-    }\n-    for (const SmallVector<Value> &operands : allOperands) {\n-      Value curr =\n-          ((ConcreteT *)(this))\n-              ->createDestOp(op, adaptor, rewriter, elemTy, operands, loc);\n-      if (!bool(curr))\n+    SmallVector<Type> types(elems, elemTy);\n+    Type structTy = this->getTypeConverter()->convertType(resultTy);\n+\n+    auto *concreteThis = static_cast<const ConcreteT *>(this);\n+    auto operands = getOperands(rewriter, adaptor, resultTy, elems, loc);\n+    SmallVector<Value> resultVals(elems);\n+    for (unsigned i = 0; i < elems; ++i) {\n+      resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n+                                                 operands[i], loc);\n+      if (!bool(resultVals[i]))\n         return failure();\n-      resultVals.push_back(curr);\n     }\n-    resultVals = reorderValues(resultVals, argTy, resultTy);\n-    resultVals =\n-        packI32(resultVals, resultTy, rewriter, loc, this->getTypeConverter());\n     Value view = this->getTypeConverter()->packLLElements(loc, resultVals,\n                                                           rewriter, resultTy);\n     rewriter.replaceOp(op, view);\n \n     return success();\n   }\n+\n+protected:\n+  SmallVector<SmallVector<Value>>\n+  getOperands(ConversionPatternRewriter &rewriter, OpAdaptor adaptor,\n+              Type operandTy, const unsigned elems, Location loc) const {\n+    SmallVector<SmallVector<Value>> operands(elems);\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getTypeConverter()->unpackLLElements(\n+          loc, operand, rewriter, operandTy);\n+      for (size_t i = 0; i < elems; ++i) {\n+        operands[i].push_back(sub_operands[i]);\n+      }\n+    }\n+    return operands;\n+  }\n };\n \n template <typename SourceOp, typename DestOp>"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "file_content_changes": "@@ -106,8 +106,17 @@ Type TritonGPUToLLVMTypeConverter::getElementTypeForStruct(\n     return elemTy;\n   if (mmaParent.isAmpere()) {\n     int bitwidth = elemTy.getIntOrFloatBitWidth();\n-    assert(bitwidth <= 32);\n-    return IntegerType::get(ctx, 32);\n+    // sub-word integer types need to be packed for perf reasons\n+    if (elemTy.isa<IntegerType>() && bitwidth < 32)\n+      return IntegerType::get(ctx, 32);\n+    // TODO: unify everything to use packed integer-types\n+    // otherwise, vector types are ok\n+    const llvm::DenseMap<int, Type> elemTyMap = {\n+        {32, vec_ty(elemTy, 1)},\n+        {16, vec_ty(elemTy, 2)},\n+        {8, vec_ty(elemTy, 4)},\n+    };\n+    return elemTyMap.lookup(bitwidth);\n   } else {\n     assert(mmaParent.isVolta());\n     return vec_ty(elemTy, 2);"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -978,10 +978,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (i32, i32, i32, i32)\n+    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    // CHECK-SAME: (i32, i32, i32, i32)\n+    // CHECK-SAME: (vector<1xf32>, vector<1xf32>, vector<1xf32>, vector<1xf32>)\n     %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n     %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n "}]