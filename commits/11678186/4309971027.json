[{"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 179, "deletions": 0, "changes": 179, "file_content_changes": "@@ -269,6 +269,17 @@ struct FpToFpOpConversion\n     return builder.launch(rewriter, loc, f32_ty, false);\n   }\n \n+  static Value convertFp16ToFp32(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.f32.f16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(v, \"h\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, f32_ty, false);\n+  }\n+\n   static Value convertFp32ToBf16(Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  const Value &v) {\n@@ -282,6 +293,17 @@ struct FpToFpOpConversion\n     return builder.launch(rewriter, loc, i16_ty, false);\n   }\n \n+  static Value convertFp32ToFp16(Location loc,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 const Value &v) {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.rn.f16.f32\");\n+    auto res = builder.newOperand(\"=h\");\n+    auto operand = builder.newOperand(v, \"r\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, f16_ty, false);\n+  }\n+\n   LogicalResult\n   matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n@@ -336,6 +358,12 @@ struct FpToFpOpConversion\n     } else if (srcEltType.isF32() && dstEltType.isBF16()) {\n       resultVals.emplace_back(\n           convertFp32ToBf16(loc, rewriter, adaptor.getFrom()));\n+    } else if (srcEltType.isF16() && dstEltType.isF32()) {\n+      resultVals.emplace_back(\n+          convertFp16ToFp32(loc, rewriter, adaptor.getFrom()));\n+    } else if (srcEltType.isF32() && dstEltType.isF16()) {\n+      resultVals.emplace_back(\n+          convertFp32ToFp16(loc, rewriter, adaptor.getFrom()));\n     } else {\n       assert(false && \"unsupported type casting\");\n     }\n@@ -860,3 +888,154 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   // __nv_expf for higher-precision calculation\n   patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n }\n+\n+struct FPExtOpConversion\n+    : ElementwiseOpConversionBase<LLVM::FPExtOp, FPExtOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::FPExtOp, FPExtOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::FPExtOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isF32() && srcTy.isF16()) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  Value createDestOp(LLVM::FPExtOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    return FpToFpOpConversion::convertFp16ToFp32(loc, rewriter, operands[0]);\n+  }\n+};\n+\n+struct FPTruncOpConversion\n+    : ElementwiseOpConversionBase<LLVM::FPTruncOp, FPTruncOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<LLVM::FPTruncOp, FPTruncOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::FPTruncOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isF16() && srcTy.isF32()) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  Value createDestOp(LLVM::FPTruncOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    return FpToFpOpConversion::convertFp32ToFp16(loc, rewriter, operands[0]);\n+  }\n+};\n+\n+struct TruncOpConversion\n+    : ElementwiseOpConversionBase<LLVM::TruncOp, TruncOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::TruncOp, TruncOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::TruncOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isInteger(16) && srcTy.isInteger(32)) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  Value createDestOp(LLVM::TruncOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.u16.u32\");\n+    auto res = builder.newOperand(\"=h\");\n+    auto operand = builder.newOperand(operands[0], \"r\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, i16_ty, false);\n+  }\n+};\n+\n+struct SExtOpConversion\n+    : ElementwiseOpConversionBase<LLVM::SExtOp, SExtOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::SExtOp, SExtOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::SExtOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isInteger(32) && srcTy.isInteger(16)) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  Value createDestOp(LLVM::SExtOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.s32.s16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(operands[0], \"h\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, i32_ty, false);\n+  }\n+};\n+\n+struct ZExtOpConversion\n+    : ElementwiseOpConversionBase<LLVM::ZExtOp, ZExtOpConversion> {\n+  using Base = ElementwiseOpConversionBase<LLVM::ZExtOp, ZExtOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  static bool isLegalOp(LLVM::ZExtOp op) {\n+    auto retTy = op.getResult().getType();\n+    auto srcTy = op.getOperand().getType();\n+    if (retTy.isInteger(32) && srcTy.isInteger(16)) {\n+      return false;\n+    }\n+    return true;\n+  }\n+\n+  Value createDestOp(LLVM::ZExtOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    PTXBuilder builder;\n+    auto &cvt = *builder.create(\"cvt.u32.u16\");\n+    auto res = builder.newOperand(\"=r\");\n+    auto operand = builder.newOperand(operands[0], \"h\");\n+    cvt(res, operand);\n+    return builder.launch(rewriter, loc, i32_ty, false);\n+  }\n+};\n+\n+bool isLegalElementwiseOp(Operation *op) {\n+  if (isa<LLVM::FPExtOp>(op)) {\n+    return FPExtOpConversion::isLegalOp(cast<LLVM::FPExtOp>(op));\n+  } else if (isa<LLVM::FPTruncOp>(op)) {\n+    return FPTruncOpConversion::isLegalOp(cast<LLVM::FPTruncOp>(op));\n+  } else if (isa<LLVM::TruncOp>(op)) {\n+    return TruncOpConversion::isLegalOp(cast<LLVM::TruncOp>(op));\n+  } else if (isa<LLVM::SExtOp>(op)) {\n+    return SExtOpConversion::isLegalOp(cast<LLVM::SExtOp>(op));\n+  } else if (isa<LLVM::ZExtOp>(op)) {\n+    return ZExtOpConversion::isLegalOp(cast<LLVM::ZExtOp>(op));\n+  }\n+  return true;\n+}\n+\n+void populateElementwiseOpToPTXPatterns(mlir::LLVMTypeConverter &typeConverter,\n+                                        RewritePatternSet &patterns,\n+                                        PatternBenefit benefit) {\n+  patterns.add<FPExtOpConversion>(typeConverter, benefit);\n+  patterns.add<FPTruncOpConversion>(typeConverter, benefit);\n+  patterns.add<TruncOpConversion>(typeConverter, benefit);\n+  patterns.add<SExtOpConversion>(typeConverter, benefit);\n+  patterns.add<ZExtOpConversion>(typeConverter, benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -13,4 +13,10 @@ void populateElementwiseOpToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                          const Allocation *allocation,\n                                          Value smem, PatternBenefit benefit);\n \n+bool isLegalElementwiseOp(Operation *op);\n+\n+void populateElementwiseOpToPTXPatterns(mlir::LLVMTypeConverter &typeConverter,\n+                                        RewritePatternSet &patterns,\n+                                        PatternBenefit benefit);\n+\n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -49,6 +49,16 @@ class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n   }\n };\n \n+class TritonPTXConversionTarget : public ConversionTarget {\n+public:\n+  explicit TritonPTXConversionTarget(MLIRContext &ctx) : ConversionTarget(ctx) {\n+    addDynamicallyLegalDialect<LLVM::LLVMDialect>(\n+        [&](Operation *op) { return isLegalElementwiseOp(op); });\n+    addLegalDialect<NVVM::NVVMDialect>();\n+    addLegalOp<mlir::UnrealizedConversionCastOp>();\n+  }\n+};\n+\n struct ReturnOpConversion : public ConvertOpToLLVMPattern<func::ReturnOp> {\n   using ConvertOpToLLVMPattern<func::ReturnOp>::ConvertOpToLLVMPattern;\n \n@@ -197,6 +207,19 @@ class ConvertTritonGPUToLLVM\n     mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n     if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n       return signalPassFailure();\n+\n+    // Use our custom converters to convert some operations to PTX to avoid\n+    // using NVPTX for two reasons:\n+    // 1. NVPTX backend is flaky on data types like float16 and bfloat16\n+    // 2. In some cases, we may generate faster PTX code than NVPTX backend\n+    TritonPTXConversionTarget ptxTarget(*context);\n+    RewritePatternSet ptxPatterns(context);\n+    // Add patterns to convert LLVM to PTX\n+    populateElementwiseOpToPTXPatterns(typeConverter, ptxPatterns,\n+                                       /*benefits=*/10);\n+\n+    if (failed(applyPartialConversion(mod, ptxTarget, std::move(ptxPatterns))))\n+      return signalPassFailure();\n   }\n \n private:"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 80, "deletions": 42, "changes": 122, "file_content_changes": "@@ -21,6 +21,13 @@ using namespace mlir::triton;\n \n namespace {\n \n+// pass named attrs (e.g., tt.contiguity) from Triton to Triton\n+static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n+  for (const NamedAttribute attr : dictAttrs.getValue())\n+    if (!op->hasAttr(attr.getName()))\n+      op->setAttr(attr.getName(), attr.getValue());\n+}\n+\n template <class Op> class GenericOpPattern : public OpConversionPattern<Op> {\n public:\n   using OpConversionPattern<Op>::OpConversionPattern;\n@@ -29,7 +36,9 @@ template <class Op> class GenericOpPattern : public OpConversionPattern<Op> {\n   matchAndRewrite(Op op, typename Op::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands());\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -43,8 +52,10 @@ class ArithCmpPattern : public OpConversionPattern<SrcOp> {\n   matchAndRewrite(SrcOp op, typename SrcOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<DstOp>(op, retType, adaptor.getPredicate(),\n-                                       adaptor.getLhs(), adaptor.getRhs());\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<DstOp>(op, retType, adaptor.getPredicate(),\n+                                           adaptor.getLhs(), adaptor.getRhs()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -65,7 +76,9 @@ class ArithConstantPattern : public OpConversionPattern<arith::ConstantOp> {\n     else\n       // This is a hack. We just want to add encoding\n       value = value.reshape(retType);\n-    rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, retType, value);\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, retType, value),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -137,9 +150,10 @@ class StdSelectPattern : public OpConversionPattern<arith::SelectOp> {\n   matchAndRewrite(arith::SelectOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n-        op, retType, adaptor.getCondition(), adaptor.getTrueValue(),\n-        adaptor.getFalseValue());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n+                      op, retType, adaptor.getCondition(),\n+                      adaptor.getTrueValue(), adaptor.getFalseValue()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -176,8 +190,9 @@ struct TritonMakeRangePattern\n   matchAndRewrite(triton::MakeRangeOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<triton::MakeRangeOp>(\n-        op, retType, adaptor.getStart(), adaptor.getEnd());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::MakeRangeOp>(\n+                      op, retType, adaptor.getStart(), adaptor.getEnd()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -220,8 +235,9 @@ struct TritonExpandDimsPattern\n     // construct new op\n     auto newSrc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op.getLoc(), newArgType, adaptor.getSrc());\n-    rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(op, newSrc,\n-                                                      adaptor.getAxis());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(\n+                      op, newSrc, adaptor.getAxis()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -273,8 +289,9 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n     }\n     c = rewriter.create<triton::gpu::ConvertLayoutOp>(c.getLoc(), retType, c);\n \n-    rewriter.replaceOpWithNewOp<triton::DotOp>(op, retType, a, b, c,\n-                                               adaptor.getAllowTF32());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::DotOp>(\n+                      op, retType, a, b, c, adaptor.getAllowTF32()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -289,8 +306,9 @@ struct TritonCatPattern : public OpConversionPattern<triton::CatOp> {\n     // For now, this behaves like generic, but this will evolve when\n     // we add support for `can_reorder=False`\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<triton::CatOp>(op, retType,\n-                                               adaptor.getOperands());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::CatOp>(\n+                      op, retType, adaptor.getOperands()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -322,7 +340,8 @@ struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n       src = rewriter.create<triton::gpu::ConvertLayoutOp>(src.getLoc(), srcType,\n                                                           src);\n     }\n-    rewriter.replaceOpWithNewOp<triton::TransOp>(op, src);\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::TransOp>(op, src),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -333,10 +352,12 @@ struct TritonLoadPattern : public OpConversionPattern<triton::LoadOp> {\n   LogicalResult\n   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::LoadOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.getPtr(),\n-        adaptor.getMask(), adaptor.getOther(), adaptor.getCache(),\n-        adaptor.getEvict(), adaptor.getIsVolatile());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::LoadOp>(\n+                      op, typeConverter->convertType(op.getType()),\n+                      adaptor.getPtr(), adaptor.getMask(), adaptor.getOther(),\n+                      adaptor.getCache(), adaptor.getEvict(),\n+                      adaptor.getIsVolatile()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -347,9 +368,11 @@ struct TritonStorePattern : public OpConversionPattern<triton::StoreOp> {\n   LogicalResult\n   matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::StoreOp>(\n-        op, adaptor.getPtr(), adaptor.getValue(), adaptor.getMask(),\n-        adaptor.getCache(), adaptor.getEvict());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::StoreOp>(\n+                      op, adaptor.getPtr(), adaptor.getValue(),\n+                      adaptor.getMask(), adaptor.getCache(),\n+                      adaptor.getEvict()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -361,9 +384,10 @@ struct TritonAtomicCASPattern\n   LogicalResult\n   matchAndRewrite(triton::AtomicCASOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.getPtr(),\n-        adaptor.getCmp(), adaptor.getVal());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n+                      op, typeConverter->convertType(op.getType()),\n+                      adaptor.getPtr(), adaptor.getCmp(), adaptor.getVal()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -375,9 +399,11 @@ struct TritonAtomicRMWPattern\n   LogicalResult\n   matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::AtomicRMWOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.getAtomicRmwOp(),\n-        adaptor.getPtr(), adaptor.getVal(), adaptor.getMask());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::AtomicRMWOp>(\n+                      op, typeConverter->convertType(op.getType()),\n+                      adaptor.getAtomicRmwOp(), adaptor.getPtr(),\n+                      adaptor.getVal(), adaptor.getMask()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -389,9 +415,11 @@ struct TritonExtElemwisePattern\n   LogicalResult\n   matchAndRewrite(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::ExtElemwiseOp>(\n-        op, typeConverter->convertType(op.getType()), adaptor.getArgs(),\n-        adaptor.getLibname(), adaptor.getLibpath(), adaptor.getSymbol());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::ExtElemwiseOp>(\n+                      op, typeConverter->convertType(op.getType()),\n+                      adaptor.getArgs(), adaptor.getLibname(),\n+                      adaptor.getLibpath(), adaptor.getSymbol()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -404,7 +432,9 @@ struct TritonGenericPattern : public OpConversionPattern<Op> {\n   matchAndRewrite(Op op, typename Op::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands());\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -425,8 +455,9 @@ struct TritonBroadcastPattern\n     Type retType = RankedTensorType::get(opType.getShape(),\n                                          opType.getElementType(), srcEncoding);\n     // Type retType = this->getTypeConverter()->convertType(op.getType());\n-    rewriter.replaceOpWithNewOp<triton::BroadcastOp>(op, retType,\n-                                                     adaptor.getOperands());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::BroadcastOp>(\n+                      op, retType, adaptor.getOperands()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -437,8 +468,10 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   LogicalResult\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n-        op, adaptor.getRedOp(), adaptor.getOperand(), adaptor.getAxis());\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n+            op, adaptor.getRedOp(), adaptor.getOperand(), adaptor.getAxis()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -449,8 +482,9 @@ struct TritonPrintfPattern : public OpConversionPattern<triton::PrintfOp> {\n   LogicalResult\n   matchAndRewrite(PrintfOp op, typename PrintfOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<triton::PrintfOp>(op, op.getPrefixAttr(),\n-                                                  adaptor.getOperands());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::PrintfOp>(\n+                      op, op.getPrefixAttr(), adaptor.getOperands()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -529,7 +563,9 @@ struct SCFYieldPattern : public OpConversionPattern<scf::YieldOp> {\n     // rewriter.setInsertionPointToEnd(rewriter.getInsertionBlock());\n     // rewriter.create<scf::YieldOp>(op.getLoc(), adaptor.getOperands());\n     // op.erase();\n-    rewriter.replaceOpWithNewOp<scf::YieldOp>(op, adaptor.getOperands());\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<scf::YieldOp>(op, adaptor.getOperands()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -634,8 +670,9 @@ class CFBranchPattern : public OpConversionPattern<cf::BranchOp> {\n   LogicalResult\n   matchAndRewrite(cf::BranchOp op, cf::BranchOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    rewriter.replaceOpWithNewOp<cf::BranchOp>(op, op.getSuccessor(),\n-                                              adaptor.getOperands());\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<cf::BranchOp>(\n+                      op, op.getSuccessor(), adaptor.getOperands()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -652,6 +689,7 @@ class CFCondBranchPattern : public OpConversionPattern<cf::CondBranchOp> {\n         op, adaptor.getCondition(), op.getTrueDest(),\n         adaptor.getTrueDestOperands(), op.getFalseDest(),\n         adaptor.getFalseDestOperands());\n+    addNamedAttrs(newOp, adaptor.getAttributes());\n \n     if (failed(rewriter.convertRegionTypes(newOp.getTrueDest()->getParent(),\n                                            *converter)))"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 22, "deletions": 8, "changes": 30, "file_content_changes": "@@ -46,6 +46,7 @@\n #include <pybind11/stl.h>\n #include <pybind11/stl_bind.h>\n #include <regex>\n+#include <signal.h>\n #include <sstream>\n #include <stdexcept>\n #include <string>\n@@ -1512,7 +1513,6 @@ void init_triton_translation(py::module &m) {\n           llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n           llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n           std::string fbin = std::string(fsrc) + \".o\";\n-          llvm::FileRemover srcRemover(fsrc);\n           llvm::FileRemover logRemover(flog);\n           llvm::FileRemover binRemover(fbin);\n           const char *_fsrc = fsrc.c_str();\n@@ -1529,16 +1529,30 @@ void init_triton_translation(py::module &m) {\n \n           err = system(cmd.c_str());\n           if (err != 0) {\n+            err >>= 8;\n             std::ifstream _log(_flog);\n             std::string log(std::istreambuf_iterator<char>(_log), {});\n-            throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n-                                     log);\n+            if (err == 255) {\n+              throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n+                                       log);\n+            } else if (err == 128 + SIGSEGV) {\n+              throw std::runtime_error(\"Please run `ptxas \" + fsrc.str().str() +\n+                                       \"` to confirm that this is a \"\n+                                       \"bug in `ptxas`\\n\" +\n+                                       log);\n+            } else {\n+              throw std::runtime_error(\"`ptxas` failed with error code \" +\n+                                       std::to_string(err) + \": \\n\" + log);\n+            }\n+            return {};\n+          } else {\n+            llvm::FileRemover srcRemover(fsrc);\n+            std::ifstream _cubin(_fbin, std::ios::binary);\n+            std::string cubin(std::istreambuf_iterator<char>(_cubin), {});\n+            _cubin.close();\n+            py::bytes bytes(cubin);\n+            return std::move(bytes);\n           }\n-          std::ifstream _cubin(_fbin, std::ios::binary);\n-          std::string cubin(std::istreambuf_iterator<char>(_cubin), {});\n-          _cubin.close();\n-          py::bytes bytes(cubin);\n-          return std::move(bytes);\n         });\n \n   m.def(\"add_external_libs\","}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 61, "deletions": 0, "changes": 61, "file_content_changes": "@@ -1409,6 +1409,28 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n         assert \"ld.global.b32\" in ptx\n     # triton.testing.assert_almost_equal(dst, src[:N])\n \n+\n+@pytest.mark.parametrize(\"has_hints\", [False, True])\n+def test_vectorization_hints(has_hints):\n+    src = torch.empty(1024, device='cuda')\n+    dst = torch.empty(1024, device='cuda')\n+    off = torch.zeros(1, device='cuda', dtype=torch.int32)\n+\n+    @triton.jit\n+    def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n+        offsets = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        offsets = offsets + tl.load(off)\n+        if HINT:\n+            tl.max_contiguous(tl.multiple_of(offsets, 1024), 1024)\n+        x = tl.load(src + offsets, mask=offsets < N)\n+        tl.store(dst + offsets, x, mask=offsets < N)\n+    pgm = _kernel[(1,)](dst, src, off, N=1024, BLOCK_SIZE=src.shape[0], HINT=has_hints)\n+    ptx = pgm.asm[\"ptx\"]\n+    if has_hints:\n+        assert \"ld.global.v4.b32\" in ptx\n+    else:\n+        assert \"ld.global.v4.b32\" not in ptx\n+\n # ---------------\n # test store\n # ---------------\n@@ -1989,3 +2011,42 @@ def kernel(Input, Index, Out, N: int):\n     Out = torch.empty_like(Index, device='cuda')\n     kernel[(1,)](Input, Index, Out, Index.numel())\n     assert Out.data[0] == 0\n+\n+\n+# This test is used to test our own PTX codegen for float16 and int16 conversions\n+# maybe delete it later after ptxas has been fixed\n+@pytest.mark.parametrize(\"dtype_str\", ['float16', 'int16'])\n+def test_ptx_cast(dtype_str):\n+    @triton.jit\n+    def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n+        xoffset = tl.program_id(0) * XBLOCK\n+        xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n+        xmask = xindex < xnumel\n+        rbase = tl.arange(0, RBLOCK)[None, :]\n+        x0 = xindex\n+        _tmp4 = (tl.zeros([XBLOCK, RBLOCK], dtype) - 10000).to(dtype)\n+        for roffset in range(0, rnumel, RBLOCK):\n+            rindex = roffset + rbase\n+            rmask = rindex < rnumel\n+            r1 = rindex\n+            tmp0 = tl.load(in_ptr0 + (r1 + (197 * x0)), rmask & xmask).to(dtype)\n+            tmp1 = 2\n+            tmp2 = tmp0 * tmp1\n+            tmp3 = tmp2.to(dtype)\n+            tmp5 = _tmp4 < tmp3\n+            _tmp4 = tl.where(rmask & xmask & tmp5, tmp3, _tmp4)\n+            tl.store(out_ptr2 + (r1 + (197 * x0) + tl.zeros([XBLOCK, RBLOCK], tl.int32)), _tmp4, rmask & xmask)\n+\n+    torch.manual_seed(123)\n+    if dtype_str == 'int16':\n+        torch_dtype = torch.int16\n+        triton_dtype = tl.int32\n+    else:\n+        torch_dtype = torch.float16\n+        triton_dtype = tl.float32\n+\n+    s0 = 4\n+    buf11 = -torch.ones((6 * s0, 197, 197), device='cuda', dtype=torch_dtype)\n+    buf14 = -torch.ones((s0, 6, 197, 197), device='cuda', dtype=torch_dtype)\n+    kernel[(4728,)](buf11, buf14, 1182 * s0, 197, triton_dtype, 1, 256, num_warps=2)\n+    assert buf14.to(torch.float32).mean() == -2.0"}]