[{"filename": "include/triton/codegen/analysis/layout.h", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -211,7 +211,8 @@ class mma_layout: public distributed_layout {\n   TensorCoreType tensor_core_type_ = FP32_FP16_FP16_FP32;\n };\n \n-struct scanline_layout: public distributed_layout {\n+class scanline_layout: public distributed_layout {\n+public:\n   scanline_layout(size_t num_warps,\n                     const std::vector<int>& axes,\n                     const std::vector<unsigned>& shape,\n@@ -225,7 +226,7 @@ struct scanline_layout: public distributed_layout {\n   int contig_per_thread(size_t k) { return nts_.at(k); }\n \n   int per_thread(size_t k) { return contig_per_thread(k) * shape_[k] / shape_per_cta(k);}\n-public:\n+private:\n   // micro tile size. The size of a tile held by a thread block.\n   std::vector<int> mts_;\n   // nano tile size. The size of a tile held by a thread."}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 8, "deletions": 13, "changes": 21, "file_content_changes": "@@ -2234,15 +2234,10 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n     ldmatrix_ty = FunctionType::get(fp16x2_pack4_ty, std::vector<llvm::Type*>{smem_ptr_ty}, false);\n     phi_ty = fp16x2_ty;    \n   } else if (A_ir_ty->is_bf16_ty() && B_ir_ty->is_bf16_ty()) {\n-    // FIXME: We should use bf16 here.\n-    mma_ty = FunctionType::get(fp32_pack4_ty, std::vector<llvm::Type*>{fp16x2_ty, fp16x2_ty, fp16x2_ty, fp16x2_ty, fp16x2_ty, fp16x2_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty}, false);\n-    smem_ptr_ty = ptr_ty(f16_ty, 3);\n-    ldmatrix_ty = FunctionType::get(fp16x2_pack4_ty, std::vector<llvm::Type*>{smem_ptr_ty}, false);\n-    phi_ty = fp16x2_ty;\n-    // mma_ty = FunctionType::get(fp32_pack4_ty, std::vector<llvm::Type*>{bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty}, false);\n-    // smem_ptr_ty = ptr_ty(bf16_ty, 3);\n-    // ldmatrix_ty = FunctionType::get(bf16x2_pack4_ty, std::vector<llvm::Type*>{smem_ptr_ty}, false);\n-    // phi_ty = bf16x2_ty;\n+    mma_ty = FunctionType::get(fp32_pack4_ty, std::vector<llvm::Type*>{bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, bf16x2_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty}, false);\n+    smem_ptr_ty = ptr_ty(bf16_ty, 3);\n+    ldmatrix_ty = FunctionType::get(bf16x2_pack4_ty, std::vector<llvm::Type*>{smem_ptr_ty}, false);\n+    phi_ty = bf16x2_ty;\n   } else if (A_ir_ty->is_fp32_ty() && B_ir_ty->is_fp32_ty()) {\n     mma_ty = FunctionType::get(fp32_pack4_ty, std::vector<llvm::Type*>{fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty, fp32_ty}, false);\n     smem_ptr_ty = ptr_ty(fp32_ty, 3);\n@@ -2342,10 +2337,10 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n       // std::cout << idxs_[A].size() << std::endl;\n       // std::cout << (m+1)*ldm + k*2 + 3 << std::endl;\n       // int ldm = num_rep_k*4;\n-      Value* ha0 = UndefValue::get(fp16x2_ty);\n-      Value* ha1 = UndefValue::get(fp16x2_ty);\n-      Value* ha2 = UndefValue::get(fp16x2_ty);\n-      Value* ha3 = UndefValue::get(fp16x2_ty);\n+      Value* ha0 = UndefValue::get(phi_ty);  // e.g., fp16x2\n+      Value* ha1 = UndefValue::get(phi_ty);\n+      Value* ha2 = UndefValue::get(phi_ty);\n+      Value* ha3 = UndefValue::get(phi_ty);\n       ha0 = builder_->CreateInsertElement(ha0, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 0]], i32(0));\n       ha0 = builder_->CreateInsertElement(ha0, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 1]], i32(1));\n       ha1 = builder_->CreateInsertElement(ha1, vals_[A][idxs_[A][(m+1)*ldm + k*2 + 0]], i32(0));"}]