[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 7, "deletions": 33, "changes": 40, "file_content_changes": "@@ -37,6 +37,8 @@ getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n   auto dstDotLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>();\n   assert(!(srcMmaLayout && dstMmaLayout) &&\n          \"Unexpected mma -> mma layout conversion\");\n+  // mma or dot layout does not have an order, so the order depends on the\n+  // layout of the other operand.\n   auto inOrd = (srcMmaLayout || srcDotLayout) ? getOrder(dstLayout)\n                                               : getOrder(srcLayout);\n   auto outOrd = (dstMmaLayout || dstDotLayout) ? getOrder(srcLayout)\n@@ -45,33 +47,6 @@ getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n   return {inOrd, outOrd};\n }\n \n-static std::pair<unsigned, unsigned> getCvtContigPerThread(\n-    const Attribute &srcLayout, const ArrayRef<unsigned> inOrd,\n-    const Attribute &dstLayout, const ArrayRef<unsigned> outOrd) {\n-  auto getContigFn = [](const Attribute &layout, const Attribute &otherLayout,\n-                        const ArrayRef<unsigned> ord) {\n-    auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>();\n-    if (dotLayout) {\n-      auto parentLayout = dotLayout.getParent();\n-      auto blockedLayout = otherLayout.dyn_cast<BlockedEncodingAttr>();\n-      assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n-      assert(blockedLayout && \"DotOperandEncodingAttr must be converted to a \"\n-                              \"BlockedEncodingAttr\");\n-      auto idx = dotLayout.getOpIdx();\n-      if (idx == 0)\n-        return static_cast<unsigned>(\n-            std::ceil(16.0 / blockedLayout.getThreadsPerWarp()[1]));\n-      else\n-        return getSizePerThread(blockedLayout)[1];\n-    } else {\n-      return getSizePerThread(layout)[ord[0]];\n-    }\n-  };\n-  unsigned srcContigPerThread = getContigFn(srcLayout, dstLayout, inOrd);\n-  unsigned dstContigPerThread = getContigFn(dstLayout, srcLayout, outOrd);\n-  return {srcContigPerThread, dstContigPerThread};\n-}\n-\n SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n                              unsigned &outVec) {\n@@ -81,11 +56,9 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   Attribute dstLayout = dstTy.getEncoding();\n   assert(srcLayout && dstLayout &&\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n-  unsigned rank = dstTy.getRank();\n-  SmallVector<unsigned> paddedRepShape(rank);\n   auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n-  auto [srcContigPerThread, dstContigPerThread] =\n-      getCvtContigPerThread(srcLayout, inOrd, dstLayout, outOrd);\n+  unsigned srcContigPerThread = getSizePerThread(srcLayout)[inOrd[0]];\n+  unsigned dstContigPerThread = getSizePerThread(dstLayout)[outOrd[0]];\n   // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n   //       that we cannot do vectorization.\n   inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;\n@@ -94,6 +67,8 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   auto srcShapePerCTA = getShapePerCTA(srcLayout);\n   auto dstShapePerCTA = getShapePerCTA(dstLayout);\n \n+  unsigned rank = dstTy.getRank();\n+  SmallVector<unsigned> paddedRepShape(rank);\n   unsigned pad = std::max(inVec, outVec);\n   for (unsigned d = 0; d < rank; ++d) {\n     paddedRepShape[d] =\n@@ -204,8 +179,7 @@ class AllocationAnalysis {\n       auto dstEncoding = dstTy.getEncoding();\n       if (srcEncoding.isa<SharedEncodingAttr>() ||\n           dstEncoding.isa<SharedEncodingAttr>()) {\n-        // Only conversions between the blocked and mma layouts require scratch\n-        // allocation.\n+        // Conversions from/to shared memory do not need scratch memory.\n         return;\n       }\n       // ConvertLayoutOp with both input/output non-shared_layout"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 22, "deletions": 2, "changes": 24, "file_content_changes": "@@ -75,8 +75,28 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n     assert(mmaLayout.getVersion() == 2 &&\n            \"mmaLayout version = 1 is not implemented yet\");\n     return SmallVector<unsigned>{2, 2};\n-  } else {\n-    llvm::errs() << \"layout: \" << layout << \"\\n\";\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    auto parentLayout = dotLayout.getParent();\n+    assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n+    if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert(parentMmaLayout.getVersion() == 2 &&\n+             \"mmaLayout version = 1 is not implemented yet\");\n+      auto parentShapePerCTA = getShapePerCTA(parentLayout);\n+      auto opIdx = dotLayout.getOpIdx();\n+      if (opIdx == 0) {\n+        return {2, 4};\n+      } else if (opIdx == 1) {\n+        return {4, 1};\n+      } else {\n+        assert(0 && \"DotOperandEncodingAttr opIdx must be 0 or 1\");\n+        return {};\n+      }\n+    } else {\n+      assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n+                  \"supported yet\");\n+      return {};\n+    }\n+   } else {\n     assert(0 && \"getSizePerThread not implemented\");\n     return {};\n   }"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1259,6 +1259,10 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self, int numStages) {\n              self.addPass(mlir::createTritonGPUPipelinePass(numStages));\n            })\n+      .def(\"add_tritongpu_prefetch_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUPrefetchPass());\n+           })\n       .def(\"add_triton_gpu_combine_pass\",\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUCombineOpsPass());"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -876,6 +876,8 @@ def ttir_to_ttgir(mod, num_warps, num_stages):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n     pm.enable_debug()\n+    # Prefetch must be run before pipeline\n+    #pm.add_tritongpu_prefetch_pass()\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()"}]