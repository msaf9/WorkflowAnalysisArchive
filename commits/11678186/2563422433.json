[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -21,7 +21,7 @@ jobs:\n \n       - name: Clear cache\n         run: |\n-          rm -r /tmp/triton/\n+          rm -r ~/.triton/\n         continue-on-error: true\n \n       - name: Install Triton"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -106,9 +106,13 @@ Atomic Ops\n     :nosignatures:\n \n     atomic_cas\n+    atomic_xchg\n     atomic_add\n     atomic_max\n     atomic_min\n+    atomic_and\n+    atomic_or\n+    atomic_xor\n \n \n Comparison ops"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -2811,8 +2811,6 @@ void generator::visit_layout_convert(ir::value *out, ir::value *in){\n   // Orders\n   analysis::distributed_layout* in_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(in));\n   analysis::distributed_layout* out_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(out));\n-  auto in_ord = in_layout->get_order();\n-  auto out_ord = out_layout->get_order();\n   Value *base;\n   int off = alloc_->offset(layouts_->get(layouts_->tmp(out)));\n    // std::cout << off << std::endl;\n@@ -2831,9 +2829,16 @@ void generator::visit_layout_convert(ir::value *out, ir::value *in){\n     in_ax.push_back(axes_.at(a_axes_->get(in, d)).values);\n     out_ax.push_back(axes_.at(a_axes_->get(out, d)).values);\n   }\n-  in_ord = in_layout->to_mma() ? out_ord : in_ord;\n-  out_ord = out_layout->to_mma() ? in_ord : out_ord;\n-  int in_vec = out_ord[0] == 0 ? 1 : in_layout->contig_per_thread(in_ord[0]);\n+  auto in_ord =\n+      in_layout->to_mma() ? out_layout->get_order() : in_layout->get_order();\n+  auto out_ord =\n+      out_layout->to_mma() ? in_layout->get_order() : out_layout->get_order();\n+  // out_ord[0] == 0 or in_order[0] == 0 means the first dimension is\n+  // non-contiguous. in_vec can be greater than 0 only if both out_ord[0] and\n+  // and in_ord[0] are contiguous.\n+  int in_vec = out_ord[0] == 0  ? 1\n+               : in_ord[0] == 0 ? 1\n+                                : in_layout->contig_per_thread(in_ord[0]);\n   int out_vec = out_ord[0] == 0 ? 1 : out_layout->contig_per_thread(out_ord[0]);\n   int pad = std::max(in_vec, out_vec);\n   Value *in_ld = i32(shape[in_ord[0]] + pad);"}, {"filename": "python/bench/bench_matmul.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n \n def rounded_linspace(low, high, steps, div):\n     ret = torch.linspace(low, high, steps)\n-    ret = (ret.int() + div - 1) // div * div\n+    ret = torch.div(ret.int() + div - 1, div, rounding_mode='trunc') * div\n     ret = torch.unique(ret)\n     return list(map(int, ret))\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -793,8 +793,8 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n \n @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n                          [(dtype, shape, perm)\n-                          for dtype in ['float32']\n-                             for shape in [(128, 128)]\n+                          for dtype in ['float16', 'float32']\n+                             for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n def test_permute(dtype_str, shape, perm, device='cuda'):\n \n@@ -812,18 +812,26 @@ def kernel(X, stride_xm, stride_xn,\n     x = numpy_random(shape, dtype_str=dtype_str)\n     # triton result\n     z_tri = to_triton(np.empty_like(x), device=device)\n+    z_tri_contiguous = to_triton(np.empty_like(x), device=device)\n     x_tri = to_triton(x, device=device)\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          z_tri, z_tri.stride(1), z_tri.stride(0),\n                          BLOCK_M=shape[0], BLOCK_N=shape[1])\n+    pgm_contiguous = kernel[(1, 1)](x_tri, x_tri.stride(1), x_tri.stride(0),\n+                                    z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n+                                    BLOCK_M=shape[0], BLOCK_N=shape[1])\n     # torch result\n     z_ref = x.transpose(*perm)\n     # compare\n     triton.testing.assert_almost_equal(z_tri, z_ref)\n+    triton.testing.assert_almost_equal(z_tri_contiguous, z_ref)\n     # parse ptx to make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n     assert 'ld.global.v4' in ptx\n     assert 'st.global.v4' in ptx\n+    ptx = pgm_contiguous.asm['ptx']\n+    assert 'ld.global.v4' in ptx\n+    assert 'st.global.v4' in ptx\n \n # ---------------\n # test dot"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 23, "deletions": 12, "changes": 35, "file_content_changes": "@@ -806,6 +806,25 @@ def store(pointer, value, eviction_policy=\"\", mask=None, _builder=None):\n # Atomic Memory Operations\n # -----------------------\n \n+@builtin\n+def atomic_cas(pointer, cmp, val, _builder=None):\n+    \"\"\"\n+        Performs an atomic compare-and-swap at the memory location specified by :code:`pointer`.\n+\n+        Return the data stored at :code:`pointer` before the atomic operation.\n+\n+        :param pointer: The memory locations to compare-and-swap.\n+        :type pointer: Block of dtype=triton.PointerDType\n+        :param cmp: The values expected to be found in the atomic object\n+        :type cmp: Block of dtype=`pointer.dtype.element_ty`\n+        :param val: The values to copy in case the expected value matches the contained value.\n+        :type val: Block of dtype=`pointer.dtype.element_ty`\n+    \"\"\"\n+    cmp = _to_tensor(cmp, _builder)\n+    val = _to_tensor(val, _builder)\n+    return semantic.atomic_cas(pointer, cmp, val, _builder)\n+\n+\n def _add_atomic_docstr(name):\n \n     def _decorator(func):\n@@ -814,27 +833,19 @@ def _decorator(func):\n \n     Return the data stored at :code:`pointer` before the atomic operation.\n \n-    :param pointer: The memory locations to compare-and-swap.\n+    :param pointer: The memory locations to apply {name}.\n     :type pointer: Block of dtype=triton.PointerDType\n-    :param cmp: The values expected to be found in the atomic object\n-    :type cmp: Block of dtype=`pointer.dtype.element_ty`\n-    :param val: The values to copy in case the expected value matches the contained value.\n+    :param val: The values to {name} in the atomic object.\n     :type val: Block of dtype=`pointer.dtype.element_ty`\n+    :param mask: If mask[idx] is false, do not apply {name}.\n+    :type mask: Block of triton.int1, optional\n     \"\"\"\n         func.__doc__ = docstr.format(name=name)\n         return func\n \n     return _decorator\n \n \n-@builtin\n-@_add_atomic_docstr(\"compare-and-swap\")\n-def atomic_cas(pointer, cmp, val, _builder=None):\n-    cmp = _to_tensor(cmp, _builder)\n-    val = _to_tensor(val, _builder)\n-    return semantic.atomic_cas(pointer, cmp, val, _builder)\n-\n-\n @builtin\n @_add_atomic_docstr(\"exchange\")\n def atomic_xchg(pointer, val, mask=None, _builder=None):"}]