[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -7,6 +7,10 @@ on:\n       - main\n       - triton-mlir\n \n+concurrency:\n+  group: ${{ github.ref }}\n+  cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}\n+\n jobs:\n   Runner-Preparation:\n     runs-on: ubuntu-latest\n@@ -55,8 +59,8 @@ jobs:\n         if: ${{ matrix.runner != 'macos-10.15' }}\n         run: |\n           pip install clang-format\n-          find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n-          (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n+          find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/triton/*\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n+          (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/triton/*\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n \n       - name: Flake8\n         if: ${{ matrix.runner != 'macos-10.15' }}"}, {"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "file_content_changes": "@@ -1,8 +1,8 @@\n name: Wheels\n on:\n   workflow_dispatch:\n-  schedule:    \n-    - cron: \"0 0 * * *\"\n+  #schedule:    \n+  #  - cron: \"0 0 * * *\"\n \n jobs:\n \n@@ -26,15 +26,14 @@ jobs:\n \n       - name: Build wheels\n         run: |\n-          export CIBW_MANYLINUX_X86_64_IMAGE=\"manylinux2014\"\n-          export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"manylinux2014\"\n-          export CIBW_BEFORE_BUILD=\"pip install cmake;\\\n-                                    yum install -y llvm11 llvm11-devel llvm11-static llvm11-libs zlib-devel;\"\n+          export CIBW_MANYLINUX_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n+          #export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n+          export CIBW_BEFORE_BUILD=\"pip install cmake;\"\n           export CIBW_SKIP=\"{cp,pp}35-*\"\n           export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n           python3 -m cibuildwheel python --output-dir wheelhouse\n \n \n       - name: Upload wheels to PyPI\n         run: |\n-          python3 -m twine upload wheelhouse/* --skip-existing\n\\ No newline at end of file\n+          python3 -m twine upload wheelhouse/* -u __token__ -p ${{ secrets.PYPY_API_TOKEN }}\n\\ No newline at end of file"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -17,7 +17,6 @@ option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n \n # Ensure Python3 vars are set correctly\n #  used conditionally in this file and by lit tests\n-find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n \n # Customized release build type with assertions: TritonRelBuildWithAsserts\n set(CMAKE_C_FLAGS_TRITONRELBUILDWITHASSERTS \"-O2 -g\")\n@@ -148,6 +147,7 @@ if(TRITON_BUILD_PYTHON_MODULE)\n     if (PYTHON_INCLUDE_DIRS)\n       include_directories(${PYTHON_INCLUDE_DIRS})\n     else()\n+      find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n       include_directories(${Python3_INCLUDE_DIRS})\n       link_directories(${Python3_LIBRARY_DIRS})\n       link_libraries(${Python3_LIBRARIES})"}, {"filename": "include/triton/Dialect/Triton/IR/Types.h", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1,10 +1,17 @@\n #ifndef TRITON_IR_TYPES_H_\n #define TRITON_IR_TYPES_H_\n \n+#include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/TypeSupport.h\"\n #include \"mlir/IR/Types.h\"\n \n #define GET_TYPEDEF_CLASSES\n #include \"triton/Dialect/Triton/IR/Types.h.inc\"\n \n+namespace mlir {\n+\n+unsigned getPointeeBitWidth(RankedTensorType tensorTy);\n+\n+}\n+\n #endif // TRITON_IR_TYPES_H_"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -850,8 +850,11 @@ unsigned AxisInfoAnalysis::getPtrAlignment(Value ptr) {\n   auto axisInfo = lookupLatticeElement(ptr)->getValue();\n   auto layout = tensorTy.getEncoding();\n   auto order = triton::gpu::getOrder(layout);\n-  auto maxMultiple = axisInfo.getDivisibility(order[0]);\n+  auto maxMultipleBytes = axisInfo.getDivisibility(order[0]);\n   auto maxContig = axisInfo.getContiguity(order[0]);\n+  auto elemNumBits = getPointeeBitWidth(tensorTy);\n+  auto elemNumBytes = std::max<unsigned>(elemNumBits / 8, 1);\n+  auto maxMultiple = std::max<int64_t>(maxMultipleBytes / elemNumBytes, 1);\n   unsigned alignment = std::min(maxMultiple, maxContig);\n   return alignment;\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 8, "changes": 10, "file_content_changes": "@@ -43,14 +43,9 @@ struct LoadStoreConversionBase {\n     if (!tensorTy)\n       return 1;\n     auto contiguity = getContiguity(ptr);\n-    unsigned numElemBits = 0;\n-    auto ptrTy = tensorTy.getElementType().cast<triton::PointerType>();\n-    auto pointeeType = ptrTy.getPointeeType();\n-    numElemBits = pointeeType.isa<triton::Float8Type>()\n-                      ? 8\n-                      : pointeeType.getIntOrFloatBitWidth();\n+    auto pointeeBitWidth = getPointeeBitWidth(tensorTy);\n     // The maximum vector size is 128 bits on NVIDIA GPUs.\n-    return std::min<unsigned>(128 / numElemBits, contiguity);\n+    return std::min<unsigned>(128 / pointeeBitWidth, contiguity);\n   }\n \n   unsigned getMaskAlignment(Value mask) const {\n@@ -781,7 +776,6 @@ struct InsertSliceAsyncOpConversion\n     auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n \n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n-\n       // 16 * 8 = 128bits\n       auto maxBitWidth =\n           std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());"}, {"filename": "lib/Dialect/Triton/IR/Types.cpp", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -37,3 +37,15 @@ Type PointerType::parse(AsmParser &parser) {\n void PointerType::print(AsmPrinter &printer) const {\n   printer << \"<\" << getPointeeType() << \">\";\n }\n+\n+namespace mlir {\n+\n+unsigned getPointeeBitWidth(RankedTensorType tensorTy) {\n+  auto ptrTy = tensorTy.getElementType().cast<triton::PointerType>();\n+  auto pointeeType = ptrTy.getPointeeType();\n+  return pointeeType.isa<triton::Float8Type>()\n+             ? 8\n+             : pointeeType.getIntOrFloatBitWidth();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 21, "deletions": 15, "changes": 36, "file_content_changes": "@@ -31,15 +31,13 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n \n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n-    PointerType ptrType = origType.getElementType().cast<PointerType>();\n-    auto pointeeType = ptrType.getPointeeType();\n-    unsigned numBits = pointeeType.isa<triton::Float8Type>()\n-                           ? 8\n-                           : pointeeType.getIntOrFloatBitWidth();\n-    unsigned maxMultiple = info.getDivisibility(order[0]);\n+    unsigned elemNumBits = getPointeeBitWidth(origType);\n+    unsigned elemNumBytes = std::max(elemNumBits / 8, 1u);\n+    unsigned maxMultipleBytes = info.getDivisibility(order[0]);\n+    unsigned maxMultiple = std::max(maxMultipleBytes / elemNumBytes, 1u);\n     unsigned maxContig = info.getContiguity(order[0]);\n     unsigned alignment = std::min(maxMultiple, maxContig);\n-    unsigned perThread = std::min(alignment, 128 / numBits);\n+    unsigned perThread = std::min(alignment, 128 / elemNumBits);\n     sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n \n     SmallVector<unsigned> dims(rank);\n@@ -107,7 +105,6 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // Run axis info analysis\n     AxisInfoAnalysis axisInfo(&getContext());\n     axisInfo.run(op);\n-    OpBuilder builder(op);\n \n     // For each memory op that has a layout L1:\n     // 1. Create a coalesced memory layout L2 of the pointer operands\n@@ -117,19 +114,28 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // 4. Convert the output of this new memory op back to L1\n     // 5. Replace all the uses of the original memory op by the new one\n     op->walk([&](Operation *curr) {\n-      OpBuilder::InsertionGuard g(builder);\n-      builder.setInsertionPoint(curr);\n-      if (auto load = dyn_cast<triton::LoadOp>(curr))\n+      OpBuilder builder(curr);\n+      if (auto load = dyn_cast<triton::LoadOp>(curr)) {\n         coalesceOp<triton::LoadOp>(axisInfo, curr, load.ptr(), builder);\n-      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr))\n+        return;\n+      }\n+      if (auto op = dyn_cast<triton::AtomicRMWOp>(curr)) {\n         coalesceOp<triton::AtomicRMWOp>(axisInfo, curr, op.ptr(), builder);\n-      if (auto op = dyn_cast<triton::AtomicCASOp>(curr))\n+        return;\n+      }\n+      if (auto op = dyn_cast<triton::AtomicCASOp>(curr)) {\n         coalesceOp<triton::AtomicCASOp>(axisInfo, curr, op.ptr(), builder);\n-      if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr))\n+        return;\n+      }\n+      if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr)) {\n         coalesceOp<triton::gpu::InsertSliceAsyncOp>(axisInfo, curr, load.src(),\n                                                     builder);\n-      if (auto store = dyn_cast<triton::StoreOp>(curr))\n+        return;\n+      }\n+      if (auto store = dyn_cast<triton::StoreOp>(curr)) {\n         coalesceOp<triton::StoreOp>(axisInfo, curr, store.ptr(), builder);\n+        return;\n+      }\n     });\n   }\n };"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -128,7 +128,8 @@ static std::map<std::string, std::string> getExternLibs(mlir::ModuleOp module) {\n                                                   .parent_path()\n                                                   .parent_path()\n                                                   .parent_path() /\n-                                              \"python\" / \"triton\" / \"language\" /\n+                                              \"python\" / \"triton\" /\n+                                              \"third_party\" / \"cuda\" / \"lib\" /\n                                               \"libdevice.10.bc\";\n     externLibs.try_emplace(libdevice, path.string());\n   }"}, {"filename": "python/setup.py", "status": "modified", "additions": 46, "deletions": 22, "changes": 68, "file_content_changes": "@@ -6,6 +6,7 @@\n import subprocess\n import sys\n import tarfile\n+import tempfile\n import urllib.request\n from distutils.version import LooseVersion\n from typing import NamedTuple\n@@ -30,9 +31,9 @@ def get_build_type():\n         # TODO: change to release when stable enough\n         return \"TritonRelBuildWithAsserts\"\n \n-\n # --- third party packages -----\n \n+\n class Package(NamedTuple):\n     package: str\n     name: str\n@@ -42,24 +43,33 @@ class Package(NamedTuple):\n     lib_flag: str\n     syspath_var_name: str\n \n+# pybind11\n+\n \n def get_pybind11_package_info():\n     name = \"pybind11-2.10.0\"\n     url = \"https://github.com/pybind/pybind11/archive/refs/tags/v2.10.0.tar.gz\"\n     return Package(\"pybind11\", name, url, \"include/pybind11/pybind11.h\", \"PYBIND11_INCLUDE_DIR\", \"\", \"PYBIND11_SYSPATH\")\n \n+# llvm\n+\n \n def get_llvm_package_info():\n     # download if nothing is installed\n     system = platform.system()\n-    system_suffix = {\"Linux\": \"linux-gnu-ubuntu-18.04\", \"Darwin\": \"apple-darwin\"}[system]\n-    use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n-    if use_assert_enabled_llvm:\n-        name = 'llvm+mlir-14.0.0-x86_64-{}-assert'.format(system_suffix)\n-        url = \"https://github.com/shintaro-iwasaki/llvm-releases/releases/download/llvm-14.0.0-329fda39c507/{}.tar.xz\".format(name)\n+    if system == \"Darwin\":\n+        system_suffix = \"apple-darwin\"\n+    elif system == \"Linux\":\n+        vglibc = tuple(map(int, platform.libc_ver()[1].split('.')))\n+        vglibc = vglibc[0] * 100 + vglibc[1]\n+        linux_suffix = 'ubuntu-18.04' if vglibc > 217 else 'centos-7'\n+        system_suffix = f\"linux-gnu-{linux_suffix}\"\n     else:\n-        name = 'clang+llvm-14.0.0-x86_64-{}'.format(system_suffix)\n-        url = \"https://github.com/llvm/llvm-project/releases/download/llvmorg-14.0.0/{}.tar.xz\".format(name)\n+        raise RuntimeError(f\"unsupported system: {system}\")\n+    use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n+    release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n+    name = f'llvm+mlir-14.0.6-x86_64-{system_suffix}-{release_suffix}'\n+    url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/llvm-14.0.6-f28c006a5895/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"lib\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n \n \n@@ -78,16 +88,38 @@ def get_thirdparty_packages(triton_cache_path):\n             except Exception:\n                 pass\n             os.makedirs(package_root_dir, exist_ok=True)\n-            print('downloading and extracting {} ...'.format(p.url))\n+            print(f'downloading and extracting {p.url} ...')\n             ftpstream = urllib.request.urlopen(p.url)\n             file = tarfile.open(fileobj=ftpstream, mode=\"r|*\")\n             file.extractall(path=package_root_dir)\n         if p.include_flag:\n-            thirdparty_cmake_args.append(\"-D{}={}/include\".format(p.include_flag, package_dir))\n+            thirdparty_cmake_args.append(f\"-D{p.include_flag}={package_dir}/include\")\n         if p.lib_flag:\n-            thirdparty_cmake_args.append(\"-D{}={}/lib\".format(p.lib_flag, package_dir))\n+            thirdparty_cmake_args.append(f\"-D{p.lib_flag}={package_dir}/lib\")\n     return thirdparty_cmake_args\n \n+# ---- package data ---\n+\n+\n+def download_and_copy_ptxas():\n+    base_dir = os.path.dirname(__file__)\n+    src_path = \"bin/ptxas\"\n+    url = \"https://conda.anaconda.org/nvidia/label/cuda-12.0.0/linux-64/cuda-nvcc-12.0.76-0.tar.bz2\"\n+    dst_prefix = os.path.join(base_dir, \"triton\")\n+    dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n+    dst_path = os.path.join(dst_prefix, dst_suffix)\n+    if not os.path.exists(dst_path):\n+        print(f'downloading and extracting {url} ...')\n+        ftpstream = urllib.request.urlopen(url)\n+        file = tarfile.open(fileobj=ftpstream, mode=\"r|*\")\n+        with tempfile.TemporaryDirectory() as temp_dir:\n+            file.extractall(path=temp_dir)\n+            src_path = os.path.join(temp_dir, src_path)\n+            os.makedirs(os.path.split(dst_path)[0], exist_ok=True)\n+            shutil.copy(src_path, dst_path)\n+    return dst_suffix\n+\n+\n # ---- cmake extension ----\n \n \n@@ -154,7 +186,7 @@ def build_extension(self, ext):\n         build_args = [\"--config\", cfg]\n \n         if platform.system() == \"Windows\":\n-            cmake_args += [\"-DCMAKE_RUNTIME_OUTPUT_DIRECTORY_{}={}\".format(cfg.upper(), extdir)]\n+            cmake_args += [f\"-DCMAKE_RUNTIME_OUTPUT_DIRECTORY_{cfg.upper()}={extdir}\"]\n             if sys.maxsize > 2**32:\n                 cmake_args += [\"-A\", \"x64\"]\n             build_args += [\"--\", \"/m\"]\n@@ -174,15 +206,7 @@ def build_extension(self, ext):\n     \"triton/language\": [\"*.bc\"],\n }\n \n-if os.getenv(\"TRITION_PACKAGE_CUDA_DEPS\"):\n-    base_dir = os.path.dirname(__file__)\n-    cuda_dir = os.getenv(\"CUDA_HOME\", \"/usr/local/cuda\")\n-    triton_dir = os.path.join(base_dir, \"triton\")\n-    os.makedirs(os.path.join(triton_dir, \"include\"), exist_ok=True)\n-    os.makedirs(os.path.join(triton_dir, \"bin\"), exist_ok=True)\n-    shutil.copy(os.path.join(cuda_dir, \"include\", \"cuda.h\"), os.path.join(triton_dir, \"include\"))\n-    shutil.copy(os.path.join(cuda_dir, \"bin\", \"ptxas\"), os.path.join(triton_dir, \"bin\"))\n-    package_data[\"triton\"] = [\"include/cuda.h\", \"bin/ptxas\"]\n+download_and_copy_ptxas()\n \n setup(\n     name=\"triton\",\n@@ -198,7 +222,7 @@ def build_extension(self, ext):\n         \"torch\",\n         \"lit\",\n     ],\n-    package_data=package_data,\n+    package_data={\"triton\": [\"third_party/*\"]},\n     include_package_data=True,\n     ext_modules=[CMakeExtension(\"triton\", \"triton/_C/\")],\n     cmdclass={\"build_ext\": CMakeBuild},"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 2, "deletions": 16, "changes": 18, "file_content_changes": "@@ -627,7 +627,7 @@ def kernel(X, Z):\n \n     # triton result\n     rs = RandomState(17)\n-    x = numpy_random((n_programs, ), dtype_str=dtype_x_str, rs=rs)\n+    x = np.array([2**i for i in range(n_programs)], dtype=getattr(np, dtype_x_str))\n     if mode == 'all_neg':\n         x = -np.abs(x)\n     if mode == 'all_pos':\n@@ -1648,24 +1648,10 @@ def _kernel(dst):\n # -------------\n \n \n-def system_libdevice_path() -> str:\n-    _SYSTEM_LIBDEVICE_SEARCH_PATHS = [\n-        '/usr/lib/cuda/nvvm/libdevice/libdevice.10.bc',\n-        '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc',\n-    ]\n-    SYSTEM_LIBDEVICE_PATH: Optional[str] = None\n-    for _p in _SYSTEM_LIBDEVICE_SEARCH_PATHS:\n-        if os.path.exists(_p):\n-            SYSTEM_LIBDEVICE_PATH = _p\n-    assert SYSTEM_LIBDEVICE_PATH is not None, \\\n-        \"Could not find libdevice.10.bc path\"\n-    return SYSTEM_LIBDEVICE_PATH\n-\n-\n @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n                          [('int32', 'libdevice.ffs', ''),\n                           ('float32', 'libdevice.log2', ''),\n-                          ('float32', 'libdevice.pow', system_libdevice_path()),\n+                          ('float32', 'libdevice.pow', tl.libdevice.LIBDEVICE_PATH),\n                           ('float64', 'libdevice.norm4d', '')])\n def test_libdevice_tensor(dtype_str, expr, lib_path):\n "}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 4, "deletions": 8, "changes": 12, "file_content_changes": "@@ -1061,17 +1061,13 @@ def ptx_get_version(cuda_version) -> int:\n \n \n def path_to_ptxas():\n-    prefixes = [\n+    base_dir = os.path.dirname(__file__)\n+    paths = [\n         os.environ.get(\"TRITON_PTXAS_PATH\", \"\"),\n-        \"\",\n-        \"/usr\",\n-        os.environ.get('CUDA_PATH', default_cuda_dir())\n+        os.path.join(base_dir, \"third_party\", \"cuda\", \"bin\", \"ptxas\")\n     ]\n-    if not os.getenv(\"TRITON_IGNORE_BUNDLED_PTXAS\"):\n-        prefixes.insert(0, os.path.dirname(__file__))\n \n-    for prefix in prefixes:\n-        ptxas = os.path.join(prefix, \"bin\", \"ptxas\")\n+    for ptxas in paths:\n         if os.path.exists(ptxas):\n             result = subprocess.check_output([ptxas, \"--version\"], stderr=subprocess.STDOUT)\n             if result is not None:"}, {"filename": "python/triton/language/libdevice.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -3,8 +3,7 @@\n from .. import impl\n from . import core, extern\n \n-LIBDEVICE_PATH = os.path.dirname(\n-    os.path.abspath(__file__)) + \"/libdevice.10.bc\"\n+LIBDEVICE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\", \"cuda\", \"lib\", \"libdevice.10.bc\")\n \n \n @impl.extern"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -231,7 +231,7 @@ def __init__(self, layout, block, device, is_dense=False):\n \n     def __call__(self, a, *, scale=1.0, rel_logits=None, is_causal=False):\n         if rel_logits is not None and rel_logits.dtype != a.dtype:\n-            raise ValueError(\"relative position embedding must be %s\" % a.dtype)\n+            raise ValueError(f\"relative position embedding must be {a.dtype}\")\n         a = _softmax.apply(\n             a, scale, rel_logits, is_causal,\n             self.spdims, self.block, self.lut, self.maxlut, self.is_dense,"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -115,7 +115,7 @@ def _kernel(A, B, C, M, N, K,\n class _matmul(torch.autograd.Function):\n     kernel = _kernel\n \n-    _locks = dict()\n+    _locks = {}\n \n     @staticmethod\n     def _call(a, b):"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -18,11 +18,11 @@ def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by:\n             'prune_num_stages_by'(optional): a function used to prune num_stages. It take configs:List[Config] as its input, and returns pruned configs.\n         '''\n         if not configs:\n-            self.configs = [Config(dict(), num_warps=4, num_stages=2)]\n+            self.configs = [Config({}, num_warps=4, num_stages=2)]\n         else:\n             self.configs = configs\n         self.key_idx = [arg_names.index(k) for k in key]\n-        self.cache = dict()\n+        self.cache = {}\n         # hook to reset all required tensor to zeros before relaunching a kernel\n         self.hook = lambda args: 0\n         if reset_to_zero is not None:"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -241,8 +241,8 @@ def _make_launcher(self):\n         src = f\"\"\"\n def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False):\n     sig_key =  {sig_keys},\n-    constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else tuple()}\n-    spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else tuple()}\n+    constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n+    spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n     key = (version_key, sig_key, constexpr_key, spec_key)\n     if not extern_libs is None:\n       key = (key, tuple(extern_libs.items()))"}, {"filename": "python/triton/third_party/cuda/include/cuda.h", "status": "added", "additions": 19348, "deletions": 0, "changes": 19348, "file_content_changes": "N/A"}, {"filename": "python/triton/third_party/cuda/lib/libdevice.10.bc", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "python/triton/language/libdevice.10.bc"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -289,7 +289,7 @@ def _output_stubs(self) -> str:\n         #   return extern.dispatch(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n         import_str = \"from . import core, extern\\n\"\n         import_str += \"import os\\n\"\n-        header_str = \"LIBDEVICE_PATH = os.path.dirname(\\n\\tos.path.abspath(__file__)) + \\\"/libdevice.10.bc\\\"\\n\"\n+        header_str = \"LIBDEVICE_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\", \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\"\n         func_str = \"\"\n         for symbols in self._symbol_groups.values():\n             func_str += \"@extern.extern\\n\""}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 48, "deletions": 3, "changes": 51, "file_content_changes": "@@ -128,7 +128,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n   // CHECK-LABEL: global_load_store_vec4\n-  func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+  func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -192,10 +192,55 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: global_load_store_vec2\n+    func @global_load_store_vec2(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 8 : i32}, %arg3: i32) {\n+    %c256_i32 = arith.constant 256 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c256_i32 : i32\n+    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n+    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n+\n+    // Load 8 elements from A with four vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 8 elements from B with four vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v2.b32 { ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n+    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n+\n+    // Store 8 elements to global with four vectorized store instruction\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    // CHECK: @${{.*}} st.global.v2.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}} };\n+    tt.store %13, %11 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec8\n-    func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+    func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n@@ -436,7 +481,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v4\n-  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}) {\n+  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 32 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>"}]