[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -23,23 +23,23 @@ namespace gpu {\n \n unsigned getElemsPerThread(Type type);\n \n-SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout);\n+SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n \n-SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout);\n+SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n-SmallVector<unsigned> getSizePerThread(const Attribute &layout);\n+SmallVector<unsigned> getSizePerThread(Attribute layout);\n \n-SmallVector<unsigned> getContigPerThread(const Attribute &layout);\n+SmallVector<unsigned> getContigPerThread(Attribute layout);\n \n-SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n+SmallVector<unsigned> getThreadsPerCTA(Attribute layout);\n \n SmallVector<unsigned>\n-getShapePerCTA(const Attribute &layout,\n+getShapePerCTA(Attribute layout,\n                ArrayRef<int64_t> tensorShape = ArrayRef<int64_t>());\n \n-SmallVector<unsigned> getOrder(const Attribute &layout);\n+SmallVector<unsigned> getOrder(Attribute layout);\n \n-bool isaDistributedLayout(const Attribute &layout);\n+bool isaDistributedLayout(Attribute layout);\n \n } // namespace gpu\n } // namespace triton"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -33,7 +33,7 @@ namespace triton {\n constexpr int kPtrBitWidth = 64;\n \n static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n-getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n+getCvtOrder(Attribute srcLayout, Attribute dstLayout) {\n   auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n   auto srcDotLayout = srcLayout.dyn_cast<DotOperandEncodingAttr>();\n   auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -502,7 +502,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n \n   SmallVector<Value> emitBaseIndexForLayout(Location loc,\n                                             ConversionPatternRewriter &rewriter,\n-                                            const Attribute &layout,\n+                                            Attribute layout,\n                                             RankedTensorType type) const {\n     IndexCacheKeyT key = std::make_pair(layout, type);\n     auto cache = indexCacheInfo.baseIndexCache;\n@@ -532,7 +532,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   }\n \n   SmallVector<SmallVector<unsigned>>\n-  emitOffsetForLayout(const Attribute &layout, RankedTensorType type) const {\n+  emitOffsetForLayout(Attribute layout, RankedTensorType type) const {\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n       return emitOffsetForBlockedLayout(blockedLayout, type);\n     if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n@@ -549,7 +549,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n   SmallVector<SmallVector<Value>> emitIndices(Location loc,\n                                               ConversionPatternRewriter &b,\n-                                              const Attribute &layout,\n+                                              Attribute layout,\n                                               RankedTensorType type) const {\n     IndexCacheKeyT key(layout, type);\n     auto cache = indexCacheInfo.indexCache;\n@@ -861,8 +861,8 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n   SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n-      Location loc, ConversionPatternRewriter &rewriter,\n-      const Attribute &layout, RankedTensorType type) const {\n+      Location loc, ConversionPatternRewriter &rewriter, Attribute layout,\n+      RankedTensorType type) const {\n     // step 1, delinearize threadId to get the base index\n     auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, type);\n     // step 2, get offset of each element"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -48,7 +48,7 @@ unsigned getElemsPerThread(Type type) {\n                            tensorType.getElementType());\n }\n \n-SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout) {\n+SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getThreadsPerWarp().begin(),\n                                  blockedLayout.getThreadsPerWarp().end());\n@@ -63,7 +63,7 @@ SmallVector<unsigned> getThreadsPerWarp(const Attribute &layout) {\n   return {};\n }\n \n-SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout) {\n+SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getWarpsPerCTA().begin(),\n                                  blockedLayout.getWarpsPerCTA().end());\n@@ -76,7 +76,7 @@ SmallVector<unsigned> getWarpsPerCTA(const Attribute &layout) {\n   return {};\n }\n \n-SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n+SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n@@ -120,7 +120,7 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n   }\n }\n \n-SmallVector<unsigned> getContigPerThread(const Attribute &layout) {\n+SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.isVolta() || mmaLayout.isAmpere());\n     return {1, 2};\n@@ -129,7 +129,7 @@ SmallVector<unsigned> getContigPerThread(const Attribute &layout) {\n   }\n }\n \n-SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n+SmallVector<unsigned> getThreadsPerCTA(Attribute layout) {\n   SmallVector<unsigned> threads;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     for (int d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n@@ -148,7 +148,7 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n   return threads;\n }\n \n-SmallVector<unsigned> getShapePerCTA(const Attribute &layout,\n+SmallVector<unsigned> getShapePerCTA(Attribute layout,\n                                      ArrayRef<int64_t> tensorShape) {\n   SmallVector<unsigned> shape;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n@@ -202,7 +202,7 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout,\n   return shape;\n }\n \n-SmallVector<unsigned> getOrder(const Attribute &layout) {\n+SmallVector<unsigned> getOrder(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getOrder().begin(),\n                                  blockedLayout.getOrder().end());\n@@ -232,7 +232,7 @@ SmallVector<unsigned> getOrder(const Attribute &layout) {\n   }\n };\n \n-bool isaDistributedLayout(const Attribute &layout) {\n+bool isaDistributedLayout(Attribute layout) {\n   return layout.isa<BlockedEncodingAttr>() || layout.isa<MmaEncodingAttr>() ||\n          layout.isa<SliceEncodingAttr>();\n }\n@@ -241,7 +241,7 @@ bool isaDistributedLayout(const Attribute &layout) {\n } // namespace triton\n } // namespace mlir\n \n-static LogicalResult parseIntAttrValue(AsmParser &parser, const Attribute &attr,\n+static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,\n                                        unsigned &value, StringRef desc) {\n   auto intAttr = attr.dyn_cast<IntegerAttr>();\n   if (!intAttr) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -134,7 +134,7 @@ bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n int simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    const Attribute &targetEncoding) {\n+    Attribute targetEncoding) {\n   // DFS\n   std::vector<std::pair<Operation *, Attribute>> queue;\n   queue.emplace_back(initOp, targetEncoding);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -19,7 +19,7 @@ bool expensiveToRemat(Operation *op, Attribute &targetEncoding);\n int simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    const Attribute &targetEncoding);\n+    Attribute targetEncoding);\n \n Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n                               IRMapping &mapping);"}]