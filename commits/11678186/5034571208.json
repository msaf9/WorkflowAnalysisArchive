[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -1969,6 +1969,20 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n     assert torch.all(out_dynamic == 2)\n \n \n+@pytest.mark.parametrize(\"literal, dtype_str\",\n+                         [(1e+50, \"f64\"), (1e+10, \"f32\"), (1.0, \"f32\"),\n+                          (5, \"i32\"), (2**40, \"i64\"),])\n+def test_constexpr(literal, dtype_str):\n+    @triton.jit\n+    def kernel(out_ptr):\n+        val = GENERATE_TEST_HERE\n+        tl.store(out_ptr.to(tl.pointer_type(val.dtype)), val)\n+\n+    kernel_patched = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{literal}\"})\n+    out = torch.zeros((1,), dtype=torch.float32, device=\"cuda\")\n+    h = kernel_patched[(1,)](out)\n+    assert re.search(r\"arith.constant .* : \" + dtype_str, h.asm[\"ttir\"]) is not None\n+\n # TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n # @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n # def test_dot_without_load(dtype_str):"}]