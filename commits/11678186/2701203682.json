[{"filename": "python/src/triton.cc", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -238,6 +238,11 @@ void parse_args(py::list& args, py::list do_not_specialize, const std::string& f\n       // argument is `constexpr`\n       if(py::hasattr(arg, \"value\")){\n         py::object value = arg.attr(\"value\");\n+        std::string ty_str = value.attr(\"__class__\").attr(\"__name__\").cast<std::string>();\n+        if (ty_str == \"JITFunction\") {\n+          throw std::runtime_error(\n+              \"JITFunction is not supported as a constexpr argument\");\n+        }\n         py::object name = arg_names[i];\n         constants[name] = value;\n         py::object repr = py::repr(value);"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -236,8 +236,8 @@ def matmul_kernel(\n         b_ptrs += BLOCK_SIZE_K * stride_bk\n     # you can fuse arbitrary activation functions here\n     # while the accumulator is still in FP32!\n-    if ACTIVATION:\n-        accumulator = ACTIVATION(accumulator)\n+    if ACTIVATION == \"leaky_relu\":\n+        accumulator = leaky_relu(accumulator)\n     c = accumulator.to(tl.float16)\n \n     # -----------------------------------------------------------\n@@ -347,7 +347,7 @@ def benchmark(M, N, K, provider):\n         )\n     if provider == 'triton + relu':\n         ms, min_ms, max_ms = triton.testing.do_bench(\n-            lambda: matmul(a, b, activation=leaky_relu)\n+            lambda: matmul(a, b, activation=\"leaky_relu\")\n         )\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)"}]