[{"filename": "python/tests/test_backend.py", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -89,7 +89,3 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n \n     assert torch.equal(z, x)\n \n-\n-\n-def test_reduce2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n-    pass\n\\ No newline at end of file"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 116, "deletions": 127, "changes": 243, "file_content_changes": "@@ -1077,133 +1077,122 @@ def kernel(X, stride_xm, stride_xn,\n # # ---------------\n \n \n-@pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n-                         [(epilogue, allow_tf32, dtype)\n-                          for epilogue in ['softmax']\n-                          for allow_tf32 in [True, False]\n-                          for dtype in ['float16']\n-                          if not (allow_tf32 and (dtype in ['float16']))])\n-def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n-    capability = torch.cuda.get_device_capability()\n-    if capability[0] < 8:\n-        if dtype == 'int8':\n-            pytest.skip(\"Only test int8 on devices with sm >= 80\")\n-        elif dtype == 'float32' and allow_tf32:\n-            pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n-\n-    M, N, K = 128, 128, 64\n-    num_warps = 4\n-    trans_a, trans_b = False, False\n-\n-    # triton kernel\n-    @triton.jit\n-    def kernel(X, stride_xm, stride_xk,\n-               Y, stride_yk, stride_yn,\n-               W, stride_wn, stride_wl,\n-               Z, stride_zm, stride_zn,\n-               BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n-               ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n-               ALLOW_TF32: tl.constexpr,\n-               DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n-               TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n-        off_m = tl.arange(0, BLOCK_M)\n-        off_n = tl.arange(0, BLOCK_N)\n-        off_l = tl.arange(0, BLOCK_N)\n-        off_k = tl.arange(0, BLOCK_K)\n-        Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n-        Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n-        Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n-        Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n-        z = tl.dot(tl.load(Xs), tl.load(Ys), trans_a=TRANS_A, trans_b=TRANS_B, allow_tf32=ALLOW_TF32)\n-        if ADD_MATRIX:\n-            z += tl.load(Zs)\n-        if ADD_ROWS:\n-            ZRs = Z + off_m * stride_zm\n-            z += tl.load(ZRs)[:, None]\n-        if ADD_COLS:\n-            ZCs = Z + off_n * stride_zn\n-            z += tl.load(ZCs)[None, :]\n-        if DO_SOFTMAX:\n-            mask = off_m[:, None] >= off_n[None, :]\n-            z += tl.where(mask, 0, float(\"-inf\"))\n-            num = tl.exp(z - tl.max(z, 1)[:, None])\n-            denom = tl.sum(num, 1)\n-            z = num / denom[:, None]\n-        if CHAIN_DOT:\n-            # tl.store(Zs, z)\n-            # tl.debug_barrier()\n-            z = tl.dot(z.to(tl.float16), tl.load(Ws), trans_a=TRANS_A)\n-        tl.store(Zs, z)\n-    # input\n-    rs = RandomState(17)\n-    x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n-    y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n-    w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n-    if allow_tf32:\n-        x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-        y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-        w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-    x_tri = to_triton(x, device=device)\n-    y_tri = to_triton(y, device=device)\n-    w_tri = to_triton(w, device=device)\n-    # triton result\n-    z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n-    z_tri = to_triton(z, device=device)\n-    if epilogue == 'trans':\n-        z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n-    pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n-                         y_tri, y_tri.stride(0), y_tri.stride(1),\n-                         w_tri, w_tri.stride(0), w_tri.stride(1),\n-                         z_tri, z_tri.stride(0), z_tri.stride(1),\n-                         TRANS_A=trans_a, TRANS_B=trans_b,\n-                         BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n-                         ADD_MATRIX=epilogue == 'add-matrix',\n-                         ADD_ROWS=epilogue == 'add-rows',\n-                         ADD_COLS=epilogue == 'add-cols',\n-                         DO_SOFTMAX=epilogue == 'softmax',\n-                         CHAIN_DOT=epilogue == 'chain-dot',\n-                         ALLOW_TF32=allow_tf32,\n-                         num_warps=num_warps)\n-    # torch result\n-    # kernel = triton.compile(\"./matmul-debug.ttgir\")\n-    # kernel[(1,1,1)](x_tri.data_ptr(), x_tri.stride(0),\n-    #                 y_tri.data_ptr(), y_tri.stride(0),\n-    #                 w_tri.data_ptr(), w_tri.stride(0),\n-    #                 z_tri.data_ptr(), z_tri.stride(0))\n-    # print(z_tri)\n-    # print((z_tri[:]==16).nonzero())\n-    # exit(1)\n-    x_ref = x.T if trans_a else x\n-    y_ref = y.T if trans_b else y\n-    z_ref = np.matmul(x_ref, y_ref)\n-    if epilogue == 'add-matrix':\n-        z_ref += z\n-    if epilogue == 'add-rows':\n-        z_ref += z[:, 0][:, None]\n-    if epilogue == 'add-cols':\n-        z_ref += z[0, :][None, :]\n-    if epilogue == 'softmax':\n-        z_ref += np.triu(np.full_like(z_ref, float(\"-inf\")), k=1)\n-        num = np.exp(z_ref - np.max(z_ref, 1)[:, None])\n-        denom = np.sum(num, 1)\n-        z_ref = num / denom[:, None]\n-\n-    if epilogue == 'chain-dot':\n-        z_ref = np.matmul(z_ref.T if trans_a else z_ref, w)\n-    # compare\n-    # print(z_ref[:,0], z_tri[:,0])\n-    # print(z_tri[0,:])\n-    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n-    # make sure ld/st are vectorized\n-    ptx = pgm.asm['ptx']\n-    assert 'ld.global.v4' in ptx\n-    assert 'st.global.v4' in ptx\n-    if allow_tf32:\n-        assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n-    elif dtype == 'float32':\n-        assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n-    elif dtype == 'int8':\n-        assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n+# @pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n+#                          [(epilogue, allow_tf32, dtype)\n+#                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n+#                           for allow_tf32 in [True, False]\n+#                           for dtype in ['float16']\n+#                           if not (allow_tf32 and (dtype in ['float16']))])\n+# def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n+#     cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+#     if cc < 80:\n+#         if dtype == 'int8':\n+#             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n+#         elif dtype == 'float32' and allow_tf32:\n+#             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n+\n+#     M, N, K = 128, 128, 64\n+#     num_warps = 8\n+#     trans_a, trans_b = False, False\n+\n+#     # triton kernel\n+#     @triton.jit\n+#     def kernel(X, stride_xm, stride_xk,\n+#                Y, stride_yk, stride_yn,\n+#                W, stride_wn, stride_wl,\n+#                Z, stride_zm, stride_zn,\n+#                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n+#                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n+#                ALLOW_TF32: tl.constexpr,\n+#                DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n+#                TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n+#         off_m = tl.arange(0, BLOCK_M)\n+#         off_n = tl.arange(0, BLOCK_N)\n+#         off_l = tl.arange(0, BLOCK_N)\n+#         off_k = tl.arange(0, BLOCK_K)\n+#         Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n+#         Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n+#         Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n+#         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n+#         z = tl.dot(tl.load(Xs), tl.load(Ys), trans_a=TRANS_A, trans_b=TRANS_B, allow_tf32=ALLOW_TF32)\n+#         if ADD_MATRIX:\n+#             z += tl.load(Zs)\n+#         if ADD_ROWS:\n+#             ZRs = Z + off_m * stride_zm\n+#             z += tl.load(ZRs)[:, None]\n+#         if ADD_COLS:\n+#             ZCs = Z + off_n * stride_zn\n+#             z += tl.load(ZCs)[None, :]\n+#         if DO_SOFTMAX:\n+#             max = tl.max(z, 1)\n+#             z = z - max[:, None]\n+#             num = tl.exp(z)\n+#             den = tl.sum(num, 1)\n+#             z = num / den[:, None]\n+#         if CHAIN_DOT:\n+#             # tl.store(Zs, z)\n+#             # tl.debug_barrier()\n+#             z = tl.dot(z.to(tl.float16), tl.load(Ws), trans_a=TRANS_A)\n+#         tl.store(Zs, z)\n+#     # input\n+#     rs = RandomState(17)\n+#     x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n+#     y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n+#     w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n+#     if allow_tf32:\n+#         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+#         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+#         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+#     x_tri = to_triton(x, device=device)\n+#     y_tri = to_triton(y, device=device)\n+#     w_tri = to_triton(w, device=device)\n+#     # triton result\n+#     z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n+#     z_tri = to_triton(z, device=device)\n+#     if epilogue == 'trans':\n+#         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n+#     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n+#                          y_tri, y_tri.stride(0), y_tri.stride(1),\n+#                          w_tri, w_tri.stride(0), w_tri.stride(1),\n+#                          z_tri, z_tri.stride(0), z_tri.stride(1),\n+#                          TRANS_A=trans_a, TRANS_B=trans_b,\n+#                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n+#                          ADD_MATRIX=epilogue == 'add-matrix',\n+#                          ADD_ROWS=epilogue == 'add-rows',\n+#                          ADD_COLS=epilogue == 'add-cols',\n+#                          DO_SOFTMAX=epilogue == 'softmax',\n+#                          CHAIN_DOT=epilogue == 'chain-dot',\n+#                          ALLOW_TF32=allow_tf32,\n+#                          num_warps=num_warps)\n+#     # torch result\n+#     x_ref = x.T if trans_a else x\n+#     y_ref = y.T if trans_b else y\n+#     z_ref = np.matmul(x_ref, y_ref)\n+#     if epilogue == 'add-matrix':\n+#         z_ref += z\n+#     if epilogue == 'add-rows':\n+#         z_ref += z[:, 0][:, None]\n+#     if epilogue == 'add-cols':\n+#         z_ref += z[0, :][None, :]\n+#     if epilogue == 'softmax':\n+#         num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n+#         denom = np.sum(num, axis=-1, keepdims=True)\n+#         z_ref = num / denom\n+#     if epilogue == 'chain-dot':\n+#         z_ref = np.matmul(z_ref.T if trans_a else z_ref, w)\n+#     # compare\n+#     # print(z_ref[:,0], z_tri[:,0])\n+#     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+#     # make sure ld/st are vectorized\n+#     ptx = pgm.asm['ptx']\n+#     assert 'ld.global.v4' in ptx\n+#     assert 'st.global.v4' in ptx\n+#     if allow_tf32:\n+#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n+#     elif dtype == 'float32':\n+#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n+#     elif dtype == 'int8':\n+#         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n # def test_dot_without_load():"}]