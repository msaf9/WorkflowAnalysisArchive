[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -91,7 +91,7 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '1' && env.ENABLE_MMA_V3 == '1'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime\n+          python3 -m pytest -n 8 --ignore=runtime --dist loadgroup\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n "}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp", "status": "modified", "additions": 14, "deletions": 7, "changes": 21, "file_content_changes": "@@ -3,6 +3,7 @@\n #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"llvm/Support/Debug.h\"\n \n //===----------------------------------------------------------------------===//\n@@ -32,16 +33,22 @@ struct FenceInsertionPass\n   FenceInsertionPass(int computeCapability) {\n     this->computeCapability = computeCapability;\n   }\n-  // TODO: support more patterns to insert fences\n-  // only support insertion between convert layout ops and dot ops to protect\n-  // flashattention\n+  // TODO: support more general patterns to insert fences.\n+  // generic + fence + async\n+  // eg. any op(generic) to shared in use-def chain which refers by async proxy\n   void runOnOperation() override {\n     // Only insert fences for compute capability 9.0\n     if (computeCapability < 90)\n       return;\n+    // ENABLE_TMA and ENABLE_MMA_V3\n+    if (!::triton::tools::getBoolEnv(\"ENABLE_TMA\") or\n+        !::triton::tools::getBoolEnv(\"ENABLE_MMA_V3\"))\n+      return;\n     ModuleOp mod = getOperation();\n     mod.walk([&](Operation *op) {\n       if (isa<tt::DotOp>(op)) {\n+        OpBuilder builder(op);\n+        // after convertlayout/trans to shared before dot\n         auto a = op->getOperand(0);\n         auto b = op->getOperand(1);\n         auto mmaEncoding = op->getResult(0)\n@@ -50,13 +57,13 @@ struct FenceInsertionPass\n                                .getEncoding()\n                                .dyn_cast<ttg::MmaEncodingAttr>();\n         auto isHopperEncoding = mmaEncoding && mmaEncoding.isHopper();\n-        if (isHopperEncoding && (isa<ttg::ConvertLayoutOp>(a.getDefiningOp()) &&\n-                                 ttg::isSharedEncoding(a)) ||\n-            (isa<ttg::ConvertLayoutOp>(b.getDefiningOp()) &&\n+        if (isHopperEncoding &&\n+                (isa<ttg::ConvertLayoutOp, tt::TransOp>(a.getDefiningOp()) &&\n+                 ttg::isSharedEncoding(a)) ||\n+            (isa<ttg::ConvertLayoutOp, tt::TransOp>(b.getDefiningOp()) &&\n              ttg::isSharedEncoding(b))) {\n \n           // TODO: check whether cluster fence is needed\n-          OpBuilder builder(op);\n           builder.create<ttng::FenceAsyncSharedOp>(op->getLoc(),\n                                                    false /*bCluster*/);\n         }"}, {"filename": "python/test/unit/hopper/test_flashattention.py", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "file_content_changes": "@@ -368,15 +368,17 @@ def backward(ctx, do):\n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 128, 64),\n-                                                 #  (4, 48, 256, 64),\n-                                                 #  (4, 48, 512, 64),\n-                                                 #  (4, 48, 1024, 64),\n-                                                 #  (4, 48, 2048, 64),\n-                                                 #  (4, 48, 4096, 64),\n-                                                 #  (4, 48, 8192, 64), out of memory\n-                                                 ])\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [\n+    (4, 48, 128, 64),\n+    (4, 48, 256, 64),\n+    (4, 48, 512, 64),\n+    (4, 48, 1024, 64),\n+    (4, 48, 2048, 64),\n+    (4, 48, 4096, 64),\n+    #  (4, 48, 8192, 64), out of memory\n+])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"requires arch 9+\")\n+@pytest.mark.xdist_group(name=\"flash\")\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     torch.manual_seed(20)\n     q = torch.empty("}]