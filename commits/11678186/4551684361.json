[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n-#include \"ConvertLayoutOpToLLVM.h\"\n-#include \"Utility.h\"\n+#include \"../ConvertLayoutOpToLLVM.h\"\n+#include \"../Utility.h\"\n \n using ValueTable = std::map<std::pair<int, int>, Value>;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n-#include \"ConvertLayoutOpToLLVM.h\"\n-#include \"Utility.h\"\n+#include \"../ConvertLayoutOpToLLVM.h\"\n+#include \"../Utility.h\"\n \n using CoordTy = SmallVector<Value>;\n using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1,5 +1,5 @@\n-#include \"ConvertLayoutOpToLLVM.h\"\n-#include \"Utility.h\"\n+#include \"../ConvertLayoutOpToLLVM.h\"\n+#include \"../Utility.h\"\n \n using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n@@ -585,7 +585,8 @@ Value loadA(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n   int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n \n   auto numRep = aEncoding.getMMAv2Rep(aTensorTy.getShape(), bitwidth);\n-  int numRepM = numRep[0], numRepK = numRep[1];\n+  int numRepM = numRep[0];\n+  int numRepK = numRep[1];\n \n   if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n     int wpt0 = mmaLayout.getWarpsPerCTA()[0];"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/FMA.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n-#include \"DotOpToLLVM.h\"\n-#include \"Utility.h\"\n+#include \"../DotOpToLLVM.h\"\n+#include \"../Utility.h\"\n \n using namespace mlir;\n using namespace mlir::triton;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv1.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n-#include \"DotOpToLLVM.h\"\n-#include \"Utility.h\"\n+#include \"../DotOpToLLVM.h\"\n+#include \"../Utility.h\"\n \n using namespace mlir;\n using namespace mlir::triton;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n-#include \"DotOpToLLVM.h\"\n-#include \"Utility.h\"\n+#include \"../DotOpToLLVM.h\"\n+#include \"../Utility.h\"\n \n using namespace mlir;\n using namespace mlir::triton;"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -420,11 +420,11 @@ DotOperandEncodingAttr::getMMAv2Rep(ArrayRef<int64_t> shape,\n   assert(mmaParent.isAmpere());\n   if (getOpIdx() == 0)\n     return {std::max<int64_t>(1, shape[0] / (shapePerWarp[0] * warpsPerCTA[0])),\n-            shape[1] / shapePerWarp[2]};\n+            std::max<int64_t>(1, shape[1] / shapePerWarp[2])};\n   else {\n     assert(getOpIdx() == 1);\n     return {\n-        shape[0] / shapePerWarp[2],\n+        std::max<int64_t>(1, shape[0] / shapePerWarp[2]),\n         std::max<int64_t>(1, shape[1] / (shapePerWarp[1] * warpsPerCTA[1]))};\n   }\n }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -287,7 +287,8 @@ scf::ForOp Prefetcher::createNewForOp() {\n                                            true, dotEncoding, builder));\n   }\n   // Update ops of yield\n-  builder.create<scf::YieldOp>(yieldOp.getLoc(), yieldValues);\n+  if (!yieldValues.empty())\n+    builder.create<scf::YieldOp>(yieldOp.getLoc(), yieldValues);\n   return newForOp;\n }\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1284,8 +1284,8 @@ def kernel(X, stride_xm, stride_xn,\n                                            [128, 128, 64, 4],\n                                            [64, 128, 128, 4],\n                                            [32, 128, 64, 2],\n-                                           # triggers nvptx/ptxas bug on V100 currently\n-                                           # [128, 128, 64, 2],\n+                                           [64, 64, 32, 4],\n+                                           [128, 128, 64, 2],\n                                            [64, 128, 128, 2]]\n                           for allow_tf32 in [True]\n                           for col_a in [True, False]"}, {"filename": "python/test/unit/runtime/test_launch.py", "status": "modified", "additions": 70, "deletions": 0, "changes": 70, "file_content_changes": "@@ -1,11 +1,20 @@\n import gc\n+import importlib\n+import os\n+import sys\n+import tempfile\n+import textwrap\n+import time\n import tracemalloc\n+from typing import Tuple\n \n import torch\n \n import triton\n import triton.language as tl\n \n+LATENCY_THRESHOLD_US = 43\n+\n \n def test_memory_leak() -> None:\n \n@@ -33,3 +42,64 @@ def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n         assert end - begin < 1000\n     finally:\n         tracemalloc.stop()\n+\n+\n+def test_kernel_launch_latency() -> None:\n+    def define_kernel(kernel_name: str, num_tensor_args: int) -> str:\n+        arg_str = \",\".join([f\"arg{i}: torch.Tensor\" for i in range(num_tensor_args)])\n+        arg_str += \", n_elements: int, BLOCK_SIZE: tl.constexpr\"\n+        func_str = f\"\"\"\n+        import torch\n+\n+        import triton\n+        import triton.language as tl\n+\n+        @triton.jit\n+        def {kernel_name}({arg_str}):\n+            pass\n+        \"\"\"\n+        with tempfile.NamedTemporaryFile(mode=\"w+t\", suffix=\".py\", delete=False) as temp_file:\n+            temp_file.write(textwrap.dedent(func_str))\n+            temp_file_path = temp_file.name\n+\n+        return temp_file_path\n+\n+    def import_kernel(file_path, kernel_name):\n+        directory, filename = os.path.split(file_path)\n+        module_name, _ = os.path.splitext(filename)\n+        sys.path.insert(0, directory)\n+\n+        module = importlib.import_module(module_name)\n+        kernel = getattr(module, kernel_name)\n+        return kernel\n+\n+    def empty(*kernel_args: Tuple[torch.Tensor]):\n+        first_arg = kernel_args[0]\n+        n_elements = first_arg.numel()\n+        grid = (triton.cdiv(n_elements, 1024),)\n+        device = torch.cuda.current_device()\n+        # Warmup\n+        empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n+        torch.cuda.synchronize()\n+        # Measure launch overhead at steady state\n+        num_runs = 1000\n+        start_time = time.time()\n+        for i in range(num_runs):\n+            empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n+        end_time = time.time()\n+        latency_us = (end_time - start_time) / num_runs * 1e6\n+\n+        assert latency_us < LATENCY_THRESHOLD_US, \"Kernel launch time has increased!\"\n+\n+    num_tensor_args = 40\n+    kernel_name = 'empty_kernel'\n+    file_path = define_kernel(kernel_name, num_tensor_args)\n+    empty_kernel = import_kernel(file_path, kernel_name)\n+\n+    # Initialize random tensors for the empty_kernel\n+    torch.manual_seed(0)\n+    size = 1024\n+    kernel_args = (torch.rand(size, device='cuda') for i in range(num_tensor_args))\n+\n+    # Run empty, which would run empty_kernel internally\n+    empty(*kernel_args)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -109,7 +109,7 @@ def _unwrap_if_constexpr(o: Any):\n     return o.value if isinstance(o, triton.language.constexpr) else o\n \n \n-_condition_types = {bool, int}  # Python types accepted for conditionals inside kernels\n+_condition_types = {bool, int, type(None)}  # Python types accepted for conditionals inside kernels\n \n \n class enter_sub_region:"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -1183,6 +1183,9 @@ def dot(lhs: tl.tensor,\n         and rhs.shape[1].value >= 16,\\\n         \"small blocks not supported!\"\n     if lhs.type.scalar.is_int():\n+        assert lhs.type.scalar == tl.int8, \"only int8 supported!\"\n+        # TODO: This is CUDA specific, check if ROCm has the same limitation\n+        assert lhs.shape[1].value >= 32, \"small blocks not supported!\"\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n     elif lhs.type.scalar.is_fp32() or lhs.type.scalar.is_bf16():"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 17, "deletions": 7, "changes": 24, "file_content_changes": "@@ -78,7 +78,7 @@ def visit_Call(self, node):\n             return\n         if func.__module__ and func.__module__.startswith('triton.'):\n             return\n-        assert isinstance(func, JITFunction)\n+        assert isinstance(func, JITFunction), f\"Function \\\"{func.__name__}\\\" is being called from a Triton function but is not a Triton function itself. Decorate it with @triton.jit to fix this\"\n         if func.hash is None:\n             tree = ast.parse(func.src)\n             finder = DependenciesFinder(func.__globals__, func.src)\n@@ -247,14 +247,23 @@ def _make_launcher(self):\n         for i, arg in enumerate(regular_args):\n             if i in self.do_not_specialize:\n                 continue\n-            specializations += [f'({arg}.data_ptr() % {JITFunction.divisibility} == 0) if hasattr({arg}, \"data_ptr\") '\n-                                f'else ({arg} % {JITFunction.divisibility} == 0, {arg} == 1) if isinstance({arg}, int) '\n-                                f'else (False,)']\n+            arg_annotation = self.__annotations__.get(arg, None)\n+            if not arg_annotation:\n+                specializations += [f'({arg}.data_ptr() % {JITFunction.divisibility} == 0) if hasattr({arg}, \"data_ptr\") '\n+                                    f'else ({arg} % {JITFunction.divisibility} == 0, {arg} == 1) if isinstance({arg}, int) '\n+                                    f'else (False,)']\n+            elif arg_annotation == 'torch.Tensor':\n+                specializations += [f'({arg}.data_ptr() % {JITFunction.divisibility} == 0)']\n+            elif arg_annotation == 'int':\n+                specializations += [f'({arg} % {JITFunction.divisibility} == 0, {arg} == 1)']\n+            else:\n+                specializations += ['(False,)']\n+\n         spec_keys = ', '.join(specializations)\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n \n         src = f\"\"\"\n-def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False):\n+def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None):\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n@@ -268,8 +277,9 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     grid_0 = grid[0]\n     grid_1 = grid[1] if grid_size > 1 else 1\n     grid_2 = grid[2] if grid_size > 2 else 1\n-    device = get_current_device()\n-    set_current_device(device)\n+    if device is None:\n+        device = get_current_device()\n+        set_current_device(device)\n     if stream is None and not warmup:\n       stream = get_cuda_stream(device)\n     try:"}]