[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -25,9 +25,9 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"gfx908\"], \"macos-10.15\"]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"gfx908\"]]'\n           else\n-            echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-10.15\"]'\n+            echo '::set-output name=matrix::[\"ubuntu-latest\"]'\n           fi\n \n   Integration-Tests:"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 18, "deletions": 2, "changes": 20, "file_content_changes": "@@ -72,8 +72,24 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<triton::ReturnOp> {\n                                                   op->getAttrs());\n     } else {\n       // A device function\n-      auto newOp =\n-          rewriter.create<LLVM::ReturnOp>(op.getLoc(), adaptor.getOperands());\n+      LLVM::ReturnOp newOp;\n+      if (adaptor.getOperands().size() < 2) {\n+        // Single or no return value.\n+        newOp =\n+            rewriter.create<LLVM::ReturnOp>(op.getLoc(), adaptor.getOperands());\n+      } else {\n+        // Pack the results into a struct.\n+        auto packedResultsTy = this->getTypeConverter()->packFunctionResults(\n+            funcOp.getResultTypes());\n+        Value packedResults =\n+            rewriter.create<LLVM::UndefOp>(op.getLoc(), packedResultsTy);\n+        auto loc = op.getLoc();\n+        for (auto it : llvm::enumerate(adaptor.getOperands())) {\n+          packedResults = insert_val(packedResultsTy, packedResults, it.value(),\n+                                     it.index());\n+        }\n+        newOp = rewriter.create<LLVM::ReturnOp>(op.getLoc(), packedResults);\n+      }\n       newOp->setAttrs(op->getAttrs());\n       rewriter.replaceOp(op, newOp->getResults());\n     }"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 14, "deletions": 2, "changes": 16, "file_content_changes": "@@ -854,7 +854,19 @@ def noinline_dynamic_fn(x, y, Z):\n     tl.store(Z, z)\n \n \n-@pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\"])\n+@triton.jit(noinline=True)\n+def noinline_call_multi_values_fn(x, y):\n+    return x + 1, y + 2\n+\n+\n+@triton.jit(noinline=True)\n+def noinline_multi_values_fn(x, y, Z):\n+    x, y = noinline_call_multi_values_fn(x, y)\n+    z = x + y\n+    tl.store(Z, z)\n+\n+\n+@pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\", \"multi_values\"])\n def test_noinline(mode):\n     device = 'cuda'\n \n@@ -875,7 +887,7 @@ def kernel(X, Y, Z):\n     kernel[(1,)](x, y, z, num_warps=1)\n     if mode == \"simple\":\n         assert torch.equal(z, x + y)\n-    elif mode == \"call_graph\" or mode == \"dynamic\":\n+    elif mode == \"call_graph\" or mode == \"dynamic\" or mode == \"multi_values\":\n         assert torch.equal(z, x + 1 + y + 2)\n     elif mode == \"shared\":\n         ref = torch.full((16, 16), 16, device=device, dtype=torch.float32)"}]