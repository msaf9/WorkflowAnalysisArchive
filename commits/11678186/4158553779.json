[{"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 35, "deletions": 35, "changes": 70, "file_content_changes": "@@ -306,42 +306,42 @@ def matmul(a, b, bias, activation=None):\n else:\n     print(\"\u274c Triton and Torch differ\")\n \n-# # %%\n-# # Benchmark\n-# # --------------\n-# #\n-# # Square Matrix Performance\n-# # ~~~~~~~~~~~~~~~~~~~~~~~~~~\n-# # We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\n+# %%\n+# Benchmark\n+# --------------\n+#\n+# Square Matrix Performance\n+# ~~~~~~~~~~~~~~~~~~~~~~~~~~\n+# We can now compare the performance of our kernel against that of cuBLAS. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape.\n \n \n-# @triton.testing.perf_report(\n-#     triton.testing.Benchmark(\n-#         x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n-#         x_vals=[\n-#             8192\n-#         ],  # different possible values for `x_name`\n-#         line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n-#         # possible values for `line_arg``\n-#         line_vals=['cublas', 'triton'],\n-#         # label name for the lines\n-#         line_names=[\"cuBLAS\", \"Triton\"],\n-#         # line styles\n-#         styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n-#         ylabel=\"TFLOPS\",  # label name for the y-axis\n-#         plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n-#         args={},\n-#     )\n-# )\n-# def benchmark(M, N, K, provider):\n-#     a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n-#     b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n-#     if provider == 'cublas':\n-#         ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=100)\n-#     if provider == 'triton':\n-#         ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=100)\n-#     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n-#     return perf(ms), perf(max_ms), perf(min_ms)\n+@triton.testing.perf_report(\n+    triton.testing.Benchmark(\n+        x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n+        x_vals=[\n+            8192\n+        ],  # different possible values for `x_name`\n+        line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n+        # possible values for `line_arg``\n+        line_vals=['cublas', 'triton'],\n+        # label name for the lines\n+        line_names=[\"cuBLAS\", \"Triton\"],\n+        # line styles\n+        styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n+        ylabel=\"TFLOPS\",  # label name for the y-axis\n+        plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n+        args={},\n+    )\n+)\n+def benchmark(M, N, K, provider):\n+    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n+    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n+    if provider == 'cublas':\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=100)\n+    if provider == 'triton':\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=100)\n+    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n+    return perf(ms), perf(max_ms), perf(min_ms)\n \n \n-# benchmark.run(show_plots=True, print_data=True)\n+benchmark.run(show_plots=True, print_data=True)"}]