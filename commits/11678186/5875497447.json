[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 27, "deletions": 10, "changes": 37, "file_content_changes": "@@ -3,6 +3,7 @@\n \n #include \"mlir/Analysis/DataFlowFramework.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n+#include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include <algorithm>\n@@ -11,11 +12,34 @@\n \n namespace mlir {\n \n+struct BlockToCFOpsHelper {\n+  BlockToCFOpsHelper(Value v) {\n+    auto moduleOp =\n+        v.getParentBlock()->getParentOp()->getParentOfType<ModuleOp>();\n+    moduleOp.walk([&](Operation *op) {\n+      if (auto br = dyn_cast<cf::BranchOp>(op)) {\n+        Block *block = br.getDest();\n+        blockToCFOps[block].insert({op, -1});\n+      }\n+      if (auto condBr = dyn_cast<cf::CondBranchOp>(op)) {\n+        Block *blockT = condBr.getTrueDest();\n+        Block *blockF = condBr.getFalseDest();\n+        blockToCFOps[blockT].insert({condBr, 1});\n+        blockToCFOps[blockF].insert({condBr, 0});\n+      }\n+    });\n+  }\n+\n+  using BranchOps = llvm::SetVector<std::pair<Operation *, int>>;\n+  llvm::DenseMap<Block *, BranchOps> blockToCFOps;\n+};\n+\n class ReduceOpHelper {\n public:\n   explicit ReduceOpHelper(triton::ReduceOp op)\n       : op(op.getOperation()), axis(op.getAxis()) {\n-    auto firstTy = op.getOperands()[0].getType().cast<RankedTensorType>();\n+    srcValue = op.getOperands()[0];\n+    auto firstTy = srcValue.getType().cast<RankedTensorType>();\n     srcShape = firstTy.getShape();\n     srcEncoding = firstTy.getEncoding();\n     srcElementTypes = op.getElementTypes();\n@@ -33,6 +57,7 @@ class ReduceOpHelper {\n   ArrayRef<int64_t> getSrcShape() { return srcShape; }\n \n   Attribute getSrcLayout() { return srcEncoding; }\n+  Value getSrcValue() { return srcValue; }\n \n   triton::ReduceOp getOperation() { return op; }\n \n@@ -60,6 +85,7 @@ class ReduceOpHelper {\n \n private:\n   triton::ReduceOp op;\n+  Value srcValue;\n   ArrayRef<int64_t> srcShape;\n   Attribute srcEncoding;\n   SmallVector<Type> srcElementTypes;\n@@ -341,18 +367,9 @@ SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n                                                 StringRef inputType);\n SmallVector<unsigned, 3>\n mmaVersionToInstrShape(int version, ArrayRef<int64_t> shape, Type type);\n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                ArrayRef<int64_t> shape,\n-                                                Type type, int opIdx);\n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                ArrayRef<int64_t> shape,\n-                                                StringRef inputType, int opIdx);\n SmallVector<unsigned, 3>\n mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n                        ArrayRef<int64_t> shape);\n-SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       ArrayRef<int64_t> shape, int opIdx);\n \n } // namespace mlir\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "file_content_changes": "@@ -516,16 +516,14 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     \"unsigned\":$versionMinor,\n     ArrayRefParameter<\"unsigned\">:$warpsPerCTA,\n     \"CTALayoutAttr\":$CTALayout,\n-    StringRefParameter<\"\">:$inputType,\n-    ArrayRefParameter<\"unsigned\">:$instrShape\n+    StringRefParameter<\"\">:$inputType\n   );\n \n   let builders = [\n     // Specially for MMAV1(Volta)\n     AttrBuilder<(ins \"int\":$versionMajor,\n                      \"int\":$numWarps,\n                      \"CTALayoutAttr\":$CTALayout,\n-                     \"ArrayRef<unsigned>\":$instrShape,\n                      \"ArrayRef<int64_t>\":$shapeC,\n                      \"bool\":$isARow,\n                      \"bool\":$isBRow,\n@@ -563,14 +561,13 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n           wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shapeC[1] / spw[1]);\n       } while (wpt_nm1 != wpt);\n \n-      return $_get(context, versionMajor, versionMinor, wpt, CTALayout, \"\", instrShape);\n+      return $_get(context, versionMajor, versionMinor, wpt, CTALayout, \"\");\n     }]>,\n \n \n     AttrBuilder<(ins \"int\":$versionMajor,\n                      \"int\":$numWarps,\n                      \"CTALayoutAttr\":$CTALayout,\n-                     \"ArrayRef<unsigned>\":$instrShape,\n                      \"ArrayRef<int64_t>\":$shapeA,\n                      \"ArrayRef<int64_t>\":$shapeB,\n                      \"ArrayRef<int64_t>\":$shapeC,\n@@ -580,7 +577,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n       assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n       bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n       bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n-      return get(context, versionMajor, numWarps, CTALayout, instrShape, shapeC, isARow, isBRow, isAVec4, isBVec4, id);\n+      return get(context, versionMajor, numWarps, CTALayout, shapeC, isARow, isBRow, isAVec4, isBVec4, id);\n     }]>\n   ];\n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -17,7 +17,6 @@ using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getShapePerCTATile;\n-using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 3, "deletions": 34, "changes": 37, "file_content_changes": "@@ -663,23 +663,7 @@ static triton::MakeTensorPtrOp getMakeTensorPtrOpImpl(Operation *op, Value v) {\n }\n \n triton::MakeTensorPtrOp getMakeTensorPtrOp(Value v) {\n-  using BranchOps = llvm::SetVector<std::pair<Operation *, int>>;\n-  llvm::DenseMap<Block *, BranchOps> blockToCFOps;\n-  auto moduleOp =\n-      v.getParentBlock()->getParentOp()->getParentOfType<ModuleOp>();\n-\n-  moduleOp.walk([&](Operation *op) {\n-    if (auto br = dyn_cast<cf::BranchOp>(op)) {\n-      Block *block = br.getDest();\n-      blockToCFOps[block].insert({op, -1});\n-    }\n-    if (auto condBr = dyn_cast<cf::CondBranchOp>(op)) {\n-      Block *blockT = condBr.getTrueDest();\n-      Block *blockF = condBr.getFalseDest();\n-      blockToCFOps[blockT].insert({condBr, 1});\n-      blockToCFOps[blockF].insert({condBr, 0});\n-    }\n-  });\n+  BlockToCFOpsHelper helper(v);\n \n   if (Operation *definingOp = v.getDefiningOp()) {\n     return getMakeTensorPtrOpImpl(definingOp, v);\n@@ -694,7 +678,7 @@ triton::MakeTensorPtrOp getMakeTensorPtrOp(Value v) {\n       Block *block = arg.getOwner();\n       Operation *op;\n       int tOrF;\n-      std::tie(op, tOrF) = blockToCFOps[block][0];\n+      std::tie(op, tOrF) = helper.blockToCFOps[block][0];\n       if (auto br = dyn_cast<cf::BranchOp>(op)) {\n         return getMakeTensorPtrOp(br.getDestOperands()[argNum]);\n       }\n@@ -760,26 +744,11 @@ SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n                                                 StringRef inputType) {\n   return {0};\n }\n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                ArrayRef<int64_t> shape,\n-                                                Type type, int opIdx) {\n-  return {0};\n-}\n-SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n-                                                ArrayRef<int64_t> shape,\n-                                                StringRef inputType,\n-                                                int opIdx) {\n-  return {0};\n-}\n+\n SmallVector<unsigned, 3>\n mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n                        ArrayRef<int64_t> shape) {\n   return {0};\n }\n \n-SmallVector<unsigned, 3>\n-mmaVersionToInstrShape(triton::gpu::MmaEncodingAttr mma,\n-                       ArrayRef<int64_t> shape, int opIdx) {\n-  return {0};\n-}\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -127,8 +127,6 @@ struct ConvertLayoutOpConversion\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n       unsigned dim = sliceLayout.getDim();\n       auto parentEncoding = sliceLayout.getParent();\n-      // [benzh] how about slice of mma\n-      auto parentSizePerThread = getSizePerThread(parentEncoding, {});\n       auto parentShape = sliceLayout.paddedShape(shape);\n       auto parentTy = RankedTensorType::get(parentShape, type.getElementType(),\n                                             parentEncoding);\n@@ -249,10 +247,10 @@ struct ConvertLayoutOpConversion\n     auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n     auto layout = type.getEncoding();\n     auto rank = type.getRank();\n-    auto sizePerThread = getSizePerThread(layout, {});\n+    auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n+    auto sizePerThread = getSizePerThread(layout, shapePerCTA);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTATiles(rank);\n-    auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n     auto shapePerCTATile = getShapePerCTATile(layout, shapePerCTA);\n     auto order = getOrder(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n@@ -537,7 +535,7 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> outNumCTAsEachRep(rank);\n     SmallVector<unsigned> inNumCTAs(rank);\n     SmallVector<unsigned> outNumCTAs(rank);\n-    // [benzh] here logic need more check\n+    // [benzh] here logic need more check: src and dst using same shape???\n     auto srcShapePerCTATile = getShapePerCTATile(srcLayout, srcTy.getShape());\n     auto dstShapePerCTATile = getShapePerCTATile(dstLayout, shape);\n     auto shapePerCTA = getShapePerCTA(srcLayout, shape);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -9,7 +9,6 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n-using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -11,7 +11,6 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n-using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -161,7 +161,7 @@ DotOpMmaV3SmemLoader loadA(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto aSharedLayout = aTensorTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n   assert(aSharedLayout && \"only support load dot operand from shared.\");\n   auto shapePerCTA = getShapePerCTA(aTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA, 0);\n+  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto aOrd = aSharedLayout.getOrder();\n   bool transA = aOrd[0] == 0;\n@@ -192,7 +192,7 @@ DotOpMmaV3SmemLoader loadB(TritonGPUToLLVMTypeConverter *typeConverter,\n   auto bSharedLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n   assert(bSharedLayout && \"only support load B from shared.\");\n   auto shapePerCTA = triton::gpu::getShapePerCTA(bTensorTy);\n-  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA, 1);\n+  auto instrShape = mmaVersionToInstrShape(mmaEncoding, shapePerCTA);\n \n   auto wpt = mmaEncoding.getWarpsPerCTA();\n   auto bOrd = bSharedLayout.getOrder();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 54, "deletions": 6, "changes": 60, "file_content_changes": "@@ -87,12 +87,53 @@ struct ReduceOpConversion\n     return srcValues;\n   }\n \n+  Value getParentValueWithSameLayout(BlockToCFOpsHelper &helper, Value value,\n+                                     Attribute attr) const {\n+    if (Operation *defOp = value.getDefiningOp()) {\n+      for (Value operand : defOp->getOperands()) {\n+        if (auto type = operand.getType().cast<RankedTensorType>()) {\n+          if (type.getEncoding() == attr)\n+            return operand;\n+        }\n+      }\n+      llvm_unreachable(\"no operand has same encoding\");\n+    } else if (BlockArgument arg = value.cast<BlockArgument>()) {\n+      unsigned argNum = arg.getArgNumber();\n+      Operation *argOwner = arg.getOwner()->getParentOp();\n+      Value newValue;\n+      if (auto forOp = dyn_cast<scf::ForOp>(argOwner))\n+        Value newValue =\n+            forOp.getOperand(argNum + forOp.getNumControlOperands() - 1);\n+      else if (auto funcOp = dyn_cast<mlir::triton::FuncOp>(argOwner)) {\n+        Block *block = arg.getOwner();\n+        Operation *op;\n+        int tOrF = 0;\n+        std::tie(op, tOrF) = helper.blockToCFOps[block][0];\n+        Value newValue;\n+        if (auto br = dyn_cast<cf::BranchOp>(op)) {\n+          newValue = br.getDestOperands()[argNum];\n+        }\n+        if (auto condBr = dyn_cast<cf::CondBranchOp>(op)) {\n+          if (tOrF)\n+            newValue = condBr.getTrueDestOperands()[argNum];\n+          else\n+            newValue = condBr.getFalseDestOperands()[argNum];\n+        }\n+      } else\n+        newValue = argOwner->getOperand(argNum);\n+      return getParentValueWithSameLayout(helper, value, attr);\n+    }\n+    llvm_unreachable(\"cannot find defOp and not block argument\");\n+    return {};\n+  }\n+\n   // Calculates the write index in the shared memory where we would be writing\n   // the within-thread accumulations before we start doing across-threads\n   // accumulations. `index` is the index of the within-thread accumulations in\n   // the full tensor, whereas `writeIdx` is the mapped-to index in the shared\n   // memory\n   void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n+                          BlockToCFOpsHelper &helper, Value value,\n                           Attribute layout, SmallVector<Value> &index,\n                           SmallVector<Value> &writeIdx,\n                           std::map<int, Value> &ints, unsigned originalAxis,\n@@ -101,13 +142,17 @@ struct ReduceOpConversion\n       // Recover the axis in the parent layout\n       auto parentAxis = axis < sliceLayout.getDim() ? axis : axis + 1;\n       auto parentLayout = sliceLayout.getParent();\n-      getWriteIndexBasic(rewriter, loc, parentLayout, index, writeIdx, ints,\n-                         originalAxis, parentAxis);\n+      auto parentValue =\n+          getParentValueWithSameLayout(helper, value, parentLayout);\n+\n+      getWriteIndexBasic(rewriter, loc, helper, parentValue, parentLayout,\n+                         index, writeIdx, ints, originalAxis, parentAxis);\n       return;\n     }\n \n     writeIdx = index;\n-    auto sizePerThread = triton::gpu::getSizePerThread(layout, {});\n+    auto sizePerThread = triton::gpu::getSizePerThread(\n+        layout, triton::gpu::getShapePerCTA(value.getType()));\n     Value axisSizePerThread = ints[sizePerThread[axis]];\n     Value _8 = ints[8];\n     Value _16 = ints[16];\n@@ -153,8 +198,9 @@ struct ReduceOpConversion\n     }\n     // The order of the axes for the the threads within the warp\n     auto srcOrd = triton::gpu::getOrder(srcLayout);\n-    auto sizePerThread = triton::gpu::getSizePerThread(srcLayout, {});\n     auto srcShape = helper.getSrcShape();\n+    auto sizePerThread = triton::gpu::getSizePerThread(\n+        srcLayout, triton::gpu::getShapePerCTA(srcLayout, srcShape));\n \n     SmallVector<Type> elemPtrTys(srcTys.size());\n     for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n@@ -191,13 +237,15 @@ struct ReduceOpConversion\n     ints[16] = i32_val(16);\n \n     // reduce across threads\n+    BlockToCFOpsHelper cfHelper(helper.getSrcValue());\n+\n     for (auto it : accs) {\n       const SmallVector<unsigned> &key = it.first;\n       auto &acc = it.second;\n       // get the writeIdx at which to write in smem\n       SmallVector<Value> writeIdx;\n-      getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n-                         axis, axis);\n+      getWriteIndexBasic(rewriter, loc, cfHelper, helper.getSrcValue(),\n+                         srcLayout, indices[key], writeIdx, ints, axis, axis);\n \n       // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -545,11 +545,13 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       auto layout = tensorTy.getEncoding();\n       auto shape = tensorTy.getShape();\n       unsigned rank = shape.size();\n-      auto sizePerThread = triton::gpu::getSizePerThread(layout, {});\n+      auto shapePerCTA = triton::gpu::getShapePerCTA(tensorTy);\n+      auto sizePerThread = triton::gpu::getSizePerThread(layout, shapePerCTA);\n       auto threadsPerWarp = triton::gpu::getThreadsPerWarp(layout);\n       auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n       auto order = triton::gpu::getOrder(layout);\n-      auto shapePerCTATile = triton::gpu::getShapePerCTATile(layout, shape);\n+      auto shapePerCTATile =\n+          triton::gpu::getShapePerCTATile(layout, shapePerCTA);\n       Value warpSize = i32_val(32);\n       Value laneId = urem(tid, warpSize);\n       Value warpId = udiv(tid, warpSize);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -655,8 +655,8 @@ class ConvertTritonGPUToLLVM\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::BlockedEncodingAttr::get(\n                 mod.getContext(), srcType.getShape(),\n-                getSizePerThread(srcMma, {}), getOrder(srcMma), numWarps,\n-                threadsPerWarp, numCTAs));\n+                getSizePerThread(srcMma, triton::gpu::getShapePerCTA(srcType)),\n+                getOrder(srcMma), numWarps, threadsPerWarp, numCTAs));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n         addWSNamedAttrs(tmp, cvtOp->getAttrs());"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -190,7 +190,7 @@ SmallVector<unsigned> getSizePerThread(Attribute layout,\n                                  blockedLayout.getSizePerThread().end());\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     // [benzh] should here provide parent shape???\n-    auto sizePerThread = getSizePerThread(sliceLayout.getParent(), {});\n+    auto sizePerThread = getSizePerThread(sliceLayout.getParent(), shapePerCTA);\n     sizePerThread.erase(sizePerThread.begin() + sliceLayout.getDim());\n     return sizePerThread;\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n@@ -199,7 +199,7 @@ SmallVector<unsigned> getSizePerThread(Attribute layout,\n     } else if (mmaLayout.isVolta()) {\n       return {1, 2};\n     } else if (mmaLayout.isHopper()) {\n-      auto instrShape = mmaLayout.getInstrShape();\n+      auto instrShape = mmaVersionToInstrShape(mmaLayout, shapePerCTA);\n       // TODO(thomas): what are those magic numbers?\n       return SmallVector<unsigned>{instrShape[0] * 4 / 32, instrShape[1] / 4};\n     } else {\n@@ -803,8 +803,8 @@ MmaEncodingAttr::getElemsPerThreadOfOperand(int opIdx,\n         \"getElemsPerThreadOfOperand() not supported for version 2\");\n   } else if (isHopper()) {\n     auto wpt = getWarpsPerCTA();\n-    auto instrMNK = mmaVersionToInstrShape(getVersionMajor(), shapePerCTA,\n-                                           getInputType(), opIdx);\n+    auto instrMNK =\n+        mmaVersionToInstrShape(getVersionMajor(), shapePerCTA, getInputType());\n     if (opIdx == 0) {\n       int repM = ceil<unsigned>(shapePerCTA[0], instrMNK[0] * wpt[0]);\n       int repK = ceil<unsigned>(shapePerCTA[1], instrMNK[2]);\n@@ -1098,7 +1098,7 @@ Attribute MmaEncodingAttr::parse(AsmParser &parser, Type type) {\n \n   return parser.getChecked<MmaEncodingAttr>(parser.getContext(), versionMajor,\n                                             versionMinor, warpsPerCTA,\n-                                            CTALayout, inputType, instrShape);\n+                                            CTALayout, inputType);\n }\n \n void MmaEncodingAttr::print(AsmPrinter &printer) const {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -252,14 +252,14 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n       mmaEnc = ttg::MmaEncodingAttr::get(\n           oldRetType.getContext(), versionMajor, numWarps, CTALayout,\n-          instrShape, oldAType.getShape(), oldBType.getShape(), retShapePerCTA,\n-          isARow, isBRow, mmaV1Counter++);\n+          oldAType.getShape(), oldBType.getShape(), retShapePerCTA, isARow,\n+          isBRow, mmaV1Counter++);\n     } else if (versionMajor == 2 || versionMajor == 3) {\n       auto warpsPerTile = getWarpsPerTile(dotOp, retShapePerCTA, versionMajor,\n                                           numWarps, instrShape);\n       mmaEnc = ttg::MmaEncodingAttr::get(oldRetType.getContext(), versionMajor,\n                                          0 /*versionMinor*/, warpsPerTile,\n-                                         CTALayout, \"\", instrShape);\n+                                         CTALayout, \"\");\n     }\n     auto newRetType = RankedTensorType::get(\n         oldRetType.getShape(), oldRetType.getElementType(), mmaEnc);"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -301,7 +301,8 @@ bool CTAPlanner::processReduce(triton::FuncOp &funcOp) {\n \n     auto rank = srcShape.size();\n     auto order = ttg::getOrder(srcLayout);\n-    auto sizePerThread = ttg::getSizePerThread(srcLayout, {});\n+    auto sizePerThread =\n+        ttg::getSizePerThread(srcLayout, ttg::getShapePerCTA(srcTy));\n     auto CTAOrder = ttg::getCTAOrder(srcLayout);\n \n     llvm::SmallVector<unsigned> CTAsPerCGA(rank, 0);"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/EmitIndicesTest.cpp", "status": "modified", "additions": 5, "deletions": 8, "changes": 13, "file_content_changes": "@@ -120,9 +120,8 @@ class EmitIndicesTest : public ::testing::Test {\n                        llvm::ArrayRef<unsigned> warpsPerCTA,\n                        llvm::ArrayRef<unsigned> instrShape,\n                        const std::string &refStr) {\n-    auto layout =\n-        MmaEncodingAttr::get(&context, versionMajor, versionMinor, warpsPerCTA,\n-                             getSingleCTALayout2d(), \"\", instrShape);\n+    auto layout = MmaEncodingAttr::get(&context, versionMajor, versionMinor,\n+                                       warpsPerCTA, getSingleCTALayout2d(), \"\");\n     runDistributed2d(row, col, layout, /*multiCTA=*/false, refStr);\n   }\n \n@@ -131,9 +130,8 @@ class EmitIndicesTest : public ::testing::Test {\n                          llvm::ArrayRef<unsigned> warpsPerCTA,\n                          llvm::ArrayRef<unsigned> instrShape, unsigned opIdx,\n                          const std::string &refStr) {\n-    auto parent =\n-        MmaEncodingAttr::get(&context, versionMajor, versionMinor, warpsPerCTA,\n-                             getSingleCTALayout2d(), \"\", instrShape);\n+    auto parent = MmaEncodingAttr::get(&context, versionMajor, versionMinor,\n+                                       warpsPerCTA, getSingleCTALayout2d(), \"\");\n     auto layout = DotOperandEncodingAttr::get(&context, opIdx, parent, 0);\n     runDistributed2d(row, col, layout, /*multiCTA=*/false, refStr);\n   }\n@@ -638,8 +636,7 @@ TEST_F(EmitIndicesTest, LayoutVisualizer_Mma) {\n \n   Attribute mmaLayout = MmaEncodingAttr::get(\n       /*context=*/&context, /*versionMajor=*/2, /*versionMinor=*/1,\n-      /*warpsPerCTA=*/{1, 1}, /*CTALayout=*/CTALayout, \"\",\n-      /*instrShape=*/{16, 8});\n+      /*warpsPerCTA=*/{1, 1}, /*CTALayout=*/CTALayout, \"\");\n \n   llvm::SmallVector<int64_t> shape = {/*row=*/16, /*col=*/8};\n "}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -32,8 +32,8 @@ TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n       triton::gpu::CTALayoutAttr::get(&ctx, {1, 1}, {1, 1}, {0, 1});\n \n   // create encoding\n-  auto parent = triton::gpu::MmaEncodingAttr::get(&ctx, 2, 0, {1, 1}, CTALayout,\n-                                                  \"\", {16, 64, 16});\n+  auto parent =\n+      triton::gpu::MmaEncodingAttr::get(&ctx, 2, 0, {1, 1}, CTALayout, \"\");\n   auto encoding = triton::gpu::DotOperandEncodingAttr::get(\n       &ctx, params.opIdx, parent, 32 / params.typeWidth);\n "}]