[{"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -49,7 +49,7 @@\n                 (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE),\n-            ] for DTYPE in [\"int8\",\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n+            ] for DTYPE in [\"int8\", \"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n         *[\n@@ -62,16 +62,16 @@\n                 # split-k\n                 (64, 64, 16, 8, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE),\n                 (64, 64, 16, 8, 4, STAGES, 1024, 1024, 32, AT, BT, DTYPE),\n-            ] for DTYPE in [\"int8\",\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [2, 3, 4]\n+            ] for DTYPE in [\"int8\", \"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [2, 3, 4]\n         ]\n     ),\n )\n def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, DTYPE):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n-    if capability[0] < 8 and DTYPE == \"bfloat16\":\n-        pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n+    if capability[0] < 8 and DTYPE in [\"bfloat16\", \"int8\"]:\n+        pytest.skip(\"Only test bfloat16 and int8 on devices with sm >= 80\")\n     if DTYPE == \"bfloat16\" and SPLIT_K != 1:\n         pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n     torch.manual_seed(0)\n@@ -102,7 +102,7 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     # run test\n     th_c = torch.matmul(a.to(torch.float32), b.to(torch.float32)).to(out_dtype)\n     try:\n-        tt_c = triton.ops.matmul(a, b, gemm_out_dtype = out_dtype)\n+        tt_c = triton.ops.matmul(a, b, gemm_out_dtype=out_dtype)\n         torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}]