[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 6, "deletions": 11, "changes": 17, "file_content_changes": "@@ -903,26 +903,21 @@ def ttgir_to_llir(mod, extern_libs):\n     return _triton.translate_triton_gpu_to_llvmir(mod)\n \n \n-def llir_to_ptx(mod: Any, **kwargs) -> Tuple[str, int]:\n+def llir_to_ptx(mod: Any, compute_capability: int = None, ptx_version: int = None) -> Tuple[str, int]:\n     '''\n     Translate TritonGPU module to PTX code.\n     :param mod: a TritonGPU dialect module\n     :return:\n         - PTX code\n         - shared memory alloaction size\n     '''\n-    if \"device\" in kwargs:\n-        assert \"compute_capability\" not in kwargs\n-        assert \"ptx_version\" not in kwargs\n-        device = int(kwargs[\"device\"])\n-        assert device >= 0, \"device should be provided.\"\n-        _, cuda_version = path_to_ptxas()\n+    if compute_capability is None:\n+        device = torch.cuda.current_device()\n         compute_capability = torch.cuda.get_device_capability(device)\n         compute_capability = compute_capability[0] * 10 + compute_capability[1]\n+    if ptx_version is None:\n+        _, cuda_version = path_to_ptxas()\n         ptx_version = ptx_get_version(cuda_version)\n-    else:\n-        compute_capability = kwargs[\"compute_capability\"]\n-        ptx_version = kwargs[\"ptx_version\"]\n     return _triton.translate_llvmir_to_ptx(mod, compute_capability, ptx_version)\n \n \n@@ -1354,7 +1349,7 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n     # llvm-ir -> ptx (or read from cache)\n     ptx, ptx_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.ptx\", metadata,\n                           run_if_found = lambda path: Path(path).read_text(),\n-                          run_if_not_found = lambda: llir_to_ptx(llir, device=device))\n+                          run_if_not_found = lambda: llir_to_ptx(llir))\n     # ptx -> cubin (or read from cache)\n     cubin, cubin_md5, force_compile, _ = read_or_execute(fn_cache_manager, force_compile, f\"{name}.cubin\", metadata,\n                             run_if_found = lambda path: Path(path).read_bytes(),      "}]