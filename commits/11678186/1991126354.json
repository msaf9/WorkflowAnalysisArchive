[{"filename": "README.md", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -18,6 +18,21 @@ The foundations of this project are described in the following MAPL2019 publicat\n \n The [official documentation](https://triton-lang.org) contains installation instructions and tutorials.\n \n+# Quick Installation\n+\n+You can install the latest stable release of Triton from pip:\n+\n+```bash\n+pip install triton\n+```\n+Binary wheels are available for CPython 3.6-3.9 and PyPy 3.6-3.7.\n+\n+And the latest nightly release:\n+\n+```bash\n+pip install -U --pre triton\n+```\n+\n # Changelog\n \n Version 1.1 is out! New features include:"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "file_content_changes": "@@ -300,8 +300,12 @@ void init_triton_runtime(py::module &&m) {\n \n     // get cached binary\n     py::str key(cache_key);\n-    if(!bin_cache.contains(key))\n-      add_to_cache(key, args, device, num_warps, num_stages);\n+    py::bool_ noop = false;\n+    if(!bin_cache.contains(key)) {\n+      noop = add_to_cache(key, args, device, num_warps, num_stages);\n+    }\n+    if (noop)\n+      return (py::object)py::none();\n     py::object bin = bin_cache[key];\n \n     // get grid\n@@ -530,6 +534,7 @@ void init_triton_codegen(py::module &&m) {\n           return hip_compile_ttir(name, ir, device, num_warps, num_stages, asm_map);\n       }, py::return_value_policy::take_ownership);\n   m.def(\"load_binary\", [](backend_t backend, const std::string& name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev){\n+\tpy::gil_scoped_release allow_threads;\n         if(backend == CUDA)\n           return cu_load_binary(name, asm_map, n_shared_bytes, dev);\n         if(backend == ROCM)"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -77,7 +77,7 @@ def reset_tmp_dir():\n def test_reuse():\n     counter = 0\n \n-    def inc_counter(key, binary, repr):\n+    def inc_counter(*args, **kwargs):\n         nonlocal counter\n         counter += 1\n     JITFunction.cache_hook = inc_counter\n@@ -92,7 +92,7 @@ def inc_counter(key, binary, repr):\n def test_specialize(mode):\n     counter = 0\n \n-    def inc_counter(key, binary, repr):\n+    def inc_counter(*args, **kwargs):\n         nonlocal counter\n         counter += 1\n     JITFunction.cache_hook = inc_counter\n@@ -118,9 +118,9 @@ def kernel(VALUE, X):\n \n     cache_str = None\n \n-    def get_cache_str(key, binary, repr):\n+    def get_cache_str(*args, **kwargs):\n         nonlocal cache_str\n-        cache_str = key.split('-')\n+        cache_str = kwargs['key'].split('-')\n     triton.code_gen.JITFunction.cache_hook = get_cache_str\n     reset_tmp_dir()\n     x = torch.tensor([3.14159], device='cuda')"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 98, "deletions": 81, "changes": 179, "file_content_changes": "@@ -710,8 +710,19 @@ def _type_name(obj):\n             return 'str'\n         raise NotImplementedError(f'could not compute type name for {obj}')\n \n+    @staticmethod\n+    def _to_python_ir(obj):\n+        # convert torch.Tensor to Triton IR pointers\n+        if hasattr(obj, 'data_ptr'):\n+            name = Kernel._type_name(obj)\n+            return 'ptr', name\n+        # default path returns triton.ir.type directly\n+        name = Kernel._type_name(obj)\n+        return 'scalar', name\n+\n     @staticmethod\n     def _to_triton_ir(obj):\n+        which, name = obj\n         type_map = {\n             'I': triton.language.int32,\n             'L': triton.language.int64,\n@@ -733,12 +744,10 @@ def _to_triton_ir(obj):\n             'u64': triton.language.uint64,\n         }\n         # convert torch.Tensor to Triton IR pointers\n-        if hasattr(obj, 'data_ptr'):\n-            name = Kernel._type_name(obj)\n+        if which == 'ptr':\n             elt_ty = type_map[name]\n             return triton.language.pointer_type(elt_ty, 1)\n-        # default path returns triton.language.type directly\n-        name = Kernel._type_name(obj)\n+        # default path returns triton.ir.type directly\n         return type_map[name]\n \n     @staticmethod\n@@ -756,36 +765,6 @@ def pow2_divisor(N):\n     def __init__(self, fn):\n         self.fn = fn\n \n-    def _compile(self, *wargs, device, attributes, constants, num_warps, num_stages):\n-        # create IR module\n-        context = _triton.ir.context()\n-        # get just-in-time proto-type of kernel\n-        fn_args = [arg for i, arg in enumerate(wargs) if i not in constants]\n-        arg_types = [Kernel._to_triton_ir(arg) for arg in fn_args]\n-        ret_type = triton.language.void\n-        prototype = triton.language.function_type(ret_type, arg_types)\n-        # generate Triton-IR\n-        # export symbols visible from self.fn into code-generator object\n-        gscope = self.fn.__globals__\n-        generator = CodeGenerator(context, prototype, gscope=gscope, attributes=attributes, constants=constants, kwargs=dict())\n-        try:\n-            generator.visit(self.fn.parse())\n-        except Exception as e:\n-            node = generator.last_node\n-            if node is None or isinstance(e, (NotImplementedError, CompilationError)):\n-                raise e\n-            raise CompilationError(self.fn.src, node) from e\n-        # Compile to machine code\n-        if torch.version.hip is None:\n-            backend = _triton.runtime.backend.CUDA\n-        else:\n-            backend = _triton.runtime.backend.ROCM\n-        name, asm, shared_mem = _triton.code_gen.compile_ttir(backend, generator.module, device, num_warps, num_stages)\n-        max_shared_memory = _triton.runtime.max_shared_memory(backend, device)\n-        if shared_mem > max_shared_memory:\n-            raise OutOfResources(shared_mem, max_shared_memory, \"shared memory\")\n-        return Binary(backend, name, asm, shared_mem, num_warps)\n-\n     def add_to_cache(self, key, wargs, device_idx, num_warps, num_stages):\n         tensor_idxs = [i for i, arg in enumerate(wargs) if hasattr(arg, 'data_ptr')]\n         # attributes\n@@ -800,57 +779,12 @@ def add_to_cache(self, key, wargs, device_idx, num_warps, num_stages):\n                 range_size = _triton.runtime.get_pointer_range_size(addr)\n                 attributes[i] = min(Kernel.pow2_divisor(addr),\n                                     Kernel.pow2_divisor(range_size))\n-\n         # transforms ints whose value is one into constants for just-in-time compilation\n         constants = {i: arg for i, arg in enumerate(wargs) if isinstance(arg, int) and arg == 1 and i not in self.fn.do_not_specialize}\n         constants.update({i: arg.value for i, arg in enumerate(wargs) if isinstance(arg, triton.language.constexpr)})\n         constants.update({i: None for i, arg in enumerate(wargs) if arg is None})\n-        hashed_key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n-\n-        # create cache directory\n-        cache_dir = os.environ.get('TRITON_CACHE_DIR', '/tmp/triton/')\n-        if cache_dir and not os.path.exists(cache_dir):\n-            os.makedirs(cache_dir, exist_ok=True)\n-\n-        if cache_dir:\n-            bin_cache_path = os.path.join(cache_dir, hashed_key)\n-            bin_lock_path = bin_cache_path + \".lock\"\n-        else:\n-            bin_cache_path = None\n-            bin_lock_path = None\n-\n-        binary = None\n-        if bin_cache_path and os.path.exists(bin_cache_path):\n-            assert bin_lock_path is not None\n-            with FileLock(bin_lock_path):\n-                with open(bin_cache_path, 'rb') as f:\n-                    binary = pickle.load(f)[\"binary\"]\n-        if binary is None:\n-            binary = self._compile(\n-                *wargs, device=device_idx, attributes=attributes,\n-                num_warps=num_warps, num_stages=num_stages,\n-                constants=constants,\n-            )\n-            if bin_cache_path:\n-                assert bin_lock_path is not None\n-                with FileLock(bin_lock_path):\n-                    with open(bin_cache_path + \".tmp\", \"wb\") as f:\n-                        pickle.dump({\"binary\": binary, \"key\": key}, f)\n-                    os.rename(bin_cache_path + \".tmp\", bin_cache_path)\n-                    if JITFunction.cache_hook is not None:\n-                        name = self.fn.__name__\n-                        info = key.split('-')[-3:]\n-                        num_warps, num_stages, sig = info[0], info[1], info[2].split('_')[1:]\n-                        # make signature human-readable\n-                        arg_reprs = []\n-                        for arg_name, arg_sig in zip(self.fn.arg_names, sig):\n-                            arg_reprs.append(f'{arg_name}: {arg_sig}')\n-                        # assemble the repr\n-                        arg_reprs = \", \".join(arg_reprs)\n-                        repr = f\"{name}[num_warps={num_warps}, num_stages={num_stages}]({arg_reprs})\"\n-                        JITFunction.cache_hook(key=key, binary=binary, repr=repr)\n-\n-        self.fn.bin_cache[key] = LoadedBinary(device_idx, binary)\n+        arg_types = [Kernel._to_python_ir(arg) for i, arg in enumerate(wargs) if i not in constants]\n+        return self.fn._warmup(key, arg_types=arg_types, device=device_idx, attributes=attributes, constants=constants, num_warps=num_warps, num_stages=num_stages, is_manual_warmup=False)\n \n     def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n         # handle arguments passed by name\n@@ -1141,6 +1075,89 @@ def _init_kernel(self):\n                 self.kernel = decorator(self.kernel)\n         return self.kernel\n \n+    def warmup(self, compile):\n+        return self._warmup(**compile, is_manual_warmup=True)\n+\n+    def _warmup(self, key, arg_types, device, attributes, constants, num_warps, num_stages, is_manual_warmup):\n+        hashed_key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n+\n+        # create cache directory\n+        cache_dir = os.environ.get('TRITON_CACHE_DIR', '/tmp/triton/')\n+        if cache_dir:\n+            os.makedirs(cache_dir, exist_ok=True)\n+\n+        if cache_dir:\n+            bin_cache_path = os.path.join(cache_dir, hashed_key)\n+            bin_lock_path = bin_cache_path + \".lock\"\n+        else:\n+            bin_cache_path = None\n+            bin_lock_path = None\n+\n+        binary = None\n+        if bin_cache_path and os.path.exists(bin_cache_path):\n+            assert bin_lock_path is not None\n+            with FileLock(bin_lock_path):\n+                with open(bin_cache_path, 'rb') as f:\n+                    binary = pickle.load(f)[\"binary\"]\n+\n+        compile = dict(arg_types=arg_types, device=device, attributes=attributes, constants=constants, num_warps=num_warps, num_stages=num_stages)\n+        if JITFunction.cache_hook is not None:\n+            name = self.__name__\n+            info = key.split('-')[-3:]\n+            num_warps, num_stages, sig = info[0], info[1], info[2].split('_')[1:]\n+            # make signature human-readable\n+            arg_reprs = []\n+            for arg_name, arg_sig in zip(self.arg_names, sig):\n+                arg_reprs.append(f'{arg_name}: {arg_sig}')\n+            # assemble the repr\n+            arg_reprs = \", \".join(arg_reprs)\n+            repr = f\"{name}[num_warps={num_warps}, num_stages={num_stages}]({arg_reprs})\"\n+            noop = JITFunction.cache_hook(key=key, repr=repr, fn=self, compile={\"key\": key, **compile}, is_manual_warmup=is_manual_warmup, already_compiled=binary is not None)\n+            if noop:\n+                return True\n+\n+        if binary is None:\n+            binary = self._compile(**compile)\n+\n+        if bin_cache_path:\n+            assert bin_lock_path is not None\n+            with FileLock(bin_lock_path):\n+                with open(bin_cache_path + \".tmp\", \"wb\") as f:\n+                    pickle.dump({\"binary\": binary, \"key\": key}, f)\n+                os.rename(bin_cache_path + \".tmp\", bin_cache_path)\n+\n+        self.bin_cache[key] = LoadedBinary(device, binary)\n+        return False\n+\n+    def _compile(self, arg_types, device, attributes, constants, num_warps, num_stages):\n+        # create IR module\n+        context = _triton.ir.context()\n+        # get just-in-time proto-type of kernel\n+        arg_types = [Kernel._to_triton_ir(arg) for arg in arg_types]\n+        ret_type = triton.language.void\n+        prototype = triton.language.function_type(ret_type, arg_types)\n+        # generate Triton-IR\n+        # export symbols visible from self into code-generator object\n+        gscope = self.__globals__\n+        generator = CodeGenerator(context, prototype, gscope=gscope, attributes=attributes, constants=constants, kwargs=dict())\n+        try:\n+            generator.visit(self.parse())\n+        except Exception as e:\n+            node = generator.last_node\n+            if node is None or isinstance(e, (NotImplementedError, CompilationError)):\n+                raise e\n+            raise CompilationError(self.src, node) from e\n+        # Compile to machine code\n+        if torch.version.hip is None:\n+            backend = _triton.runtime.backend.CUDA\n+        else:\n+            backend = _triton.runtime.backend.ROCM\n+        name, asm, shared_mem = _triton.code_gen.compile_ttir(backend, generator.module, device, num_warps, num_stages)\n+        max_shared_memory = _triton.runtime.max_shared_memory(backend, device)\n+        if shared_mem > max_shared_memory:\n+            raise OutOfResources(shared_mem, max_shared_memory, \"shared memory\")\n+        return Binary(backend, name, asm, shared_mem, num_warps)\n+\n     def __getitem__(self, grid):\n         return Launcher(self._init_kernel(), grid)\n "}]