[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -163,18 +163,19 @@ for\n                      \"ArrayRef<unsigned>\":$order,\n                      \"unsigned\":$numWarps), [{\n       int rank = sizePerThread.size();\n-      int remainingWarps = numWarps;\n-      int remainingLanes = 32;\n+      unsigned remainingLanes = 32;\n+      unsigned remainingThreads = numWarps*32;\n+      unsigned remainingWarps = numWarps;\n       SmallVector<unsigned, 4> threadsPerWarp(rank);\n       SmallVector<unsigned, 4> warpsPerCTA(rank);\n       for (int _dim = 0; _dim < rank; ++_dim) {\n-        int dim = order[_dim];\n-        int maxNumThreads = int(shape[dim]) / sizePerThread[dim];\n-        warpsPerCTA[dim] = std::clamp(remainingWarps, 1, maxNumThreads);\n-        maxNumThreads = maxNumThreads / warpsPerCTA[dim];\n-        threadsPerWarp[dim] = std::clamp(remainingLanes, 1, maxNumThreads);\n-        remainingWarps /= warpsPerCTA[dim];\n-        remainingLanes /= threadsPerWarp[dim];\n+        int i = order[_dim];\n+        unsigned threadsPerCTA = std::clamp<unsigned>(remainingThreads, 1, shape[i] / sizePerThread[i]);\n+        threadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n+        warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / threadsPerWarp[i], 1, remainingWarps);\n+        remainingWarps /= warpsPerCTA[i];\n+        remainingLanes /= threadsPerWarp[i];\n+        remainingThreads /= threadsPerCTA;\n       }\n \n       return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order);"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "file_content_changes": "@@ -284,17 +284,17 @@ def matmul(a, b, activation=None):\n #\n # We can test our custom matrix multiplication operation against a native torch implementation (i.e., cuBLAS)\n \n-torch.manual_seed(0)\n-a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n-triton_output = matmul(a, b, activation=None)\n-torch_output = torch.matmul(a, b)\n-print(f\"triton_output={triton_output}\")\n-print(f\"torch_output={torch_output}\")\n-if triton.testing.allclose(triton_output, torch_output):\n-    print(\"\u2705 Triton and Torch match\")\n-else:\n-    print(\"\u274c Triton and Torch differ\")\n+# torch.manual_seed(0)\n+# a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+# b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n+# triton_output = matmul(a, b, activation=None)\n+# torch_output = torch.matmul(a, b)\n+# print(f\"triton_output={triton_output}\")\n+# print(f\"torch_output={torch_output}\")\n+# if triton.testing.allclose(triton_output, torch_output):\n+#     print(\"\u2705 Triton and Torch match\")\n+# else:\n+#     print(\"\u274c Triton and Torch differ\")\n \n # %%\n # Benchmark\n@@ -334,4 +334,4 @@ def benchmark(M, N, K, provider):\n     return perf(ms), perf(max_ms), perf(min_ms)\n \n \n-benchmark.run(show_plots=True, print_data=True)\n+benchmark.run(show_plots=False, print_data=True)"}]