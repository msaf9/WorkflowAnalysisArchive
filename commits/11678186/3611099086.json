[{"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -81,9 +81,11 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n     return;\n   }\n \n-  if (isa<triton::gpu::AsyncWaitOp>(op)) {\n-    // If the current op is an async wait, we insert a barrier op and sync\n-    // previous reads and writes.\n+  if (isa<triton::gpu::AsyncWaitOp>(op) &&\n+      !isa<gpu::BarrierOp>(op->getNextNode())) {\n+    // If the current op is an async wait and the next op is not a barrier we\n+    // insert a barrier op and sync\n+    regionInfo->sync();\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPointAfter(op);\n     builder->create<gpu::BarrierOp>(op->getLoc());"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "file_content_changes": "@@ -708,19 +708,19 @@ class MMA16816SmemLoader {\n       Type elemTy = type::f32Ty(ctx);\n       Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n-        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n-        elems[1] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n+        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n+        elems[1] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n         elems[2] =\n-            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n         elems[3] =\n-            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n       } else {\n-        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n-        elems[2] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n+        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n+        elems[2] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n         elems[1] =\n-            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n         elems[3] =\n-            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+            load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n       }\n       return {elems[0], elems[1], elems[2], elems[3]};\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 30, "deletions": 48, "changes": 78, "file_content_changes": "@@ -3327,10 +3327,10 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   // We cannot get both the operand types(in TypeConverter), here we assume the\n   // types of both the operands are identical here.\n   // TODO[Superjomn]: Find a better way to implement it.\n-  static bool isDotHMMA(TensorType operand, bool allowTF32, int mmaVersion) {\n+  static bool isDotHMMA(TensorType operand, int mmaVersion) {\n     auto elemTy = operand.getElementType();\n     return elemTy.isF16() || elemTy.isBF16() ||\n-           (elemTy.isF32() && allowTF32 && mmaVersion >= 2) ||\n+           (elemTy.isF32() && mmaVersion >= 2) ||\n            (elemTy.isInteger(8) && mmaVersion >= 2);\n   }\n \n@@ -3354,11 +3354,7 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   Value src = op.src();\n   Value dst = op.result();\n   auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n-  // TODO[Superjomn]: allowTF32 is not accessible here for it is an attribute of\n-  // an Op instance.\n-  bool allowTF32 = false;\n-  bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, allowTF32,\n-                                           mmaLayout.getVersion());\n+  bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, mmaLayout.getVersion());\n \n   auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n   Value res;\n@@ -3421,25 +3417,16 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   } else if (auto blockedLayout =\n                  dotOperandLayout.getParent()\n                      .dyn_cast_or_null<BlockedEncodingAttr>()) {\n-    // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n-    // is an attribute of DotOp.\n-    bool allowTF32 = false;\n-    bool isFMADot = dstTensorTy.getElementType().isF32() && !allowTF32;\n-    if (isFMADot) {\n-      auto dotOpLayout =\n-          dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-      auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n-      DotOpFMAConversionHelper helper(blockedLayout);\n-      auto thread = getThreadId(rewriter, loc);\n-      if (dotOpLayout.getOpIdx() == 0) { // $a\n-        res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n-                           rewriter);\n-      } else { // $b\n-        res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n-                           rewriter);\n-      }\n-    } else\n-      assert(false && \"Unsupported dot operand layout found\");\n+    auto dotOpLayout = dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+    DotOpFMAConversionHelper helper(blockedLayout);\n+    auto thread = getThreadId(rewriter, loc);\n+    if (dotOpLayout.getOpIdx() == 0) { // $a\n+      res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n+                         rewriter);\n+    } else { // $b\n+      res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n+                         rewriter);\n+    }\n   } else {\n     assert(false && \"Unsupported dot operand layout found\");\n   }\n@@ -3805,13 +3792,6 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n     auto ctx = type.getContext();\n     Attribute layout = type.getEncoding();\n     auto shape = type.getShape();\n-\n-    // TODO[Keren, Superjomn]: fix it, allowTF32 is not accessible here for it\n-    // is bound to an Op instance.\n-    bool allowTF32 = false;\n-    bool isFMADot = type.getElementType().isF32() && !allowTF32 &&\n-                    layout.dyn_cast_or_null<DotOperandEncodingAttr>();\n-\n     if (layout &&\n         (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n          layout.isa<MmaEncodingAttr>())) {\n@@ -3835,37 +3815,39 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n       return LLVM::LLVMStructType::getLiteral(ctx, types);\n     } else if (auto dotOpLayout =\n                    layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n-      if (isFMADot) { // for parent is blocked layout\n+      if (dotOpLayout.getParent()\n+              .isa<BlockedEncodingAttr>()) { // for parent is blocked layout\n         int numElemsPerThread =\n             DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n \n         return LLVM::LLVMStructType::getLiteral(\n             ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n-\n       } else { // for parent is MMA layout\n         auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n         auto wpt = mmaLayout.getWarpsPerCTA();\n         Type elemTy = convertType(type.getElementType());\n-        auto vecSize = 1;\n-        if (elemTy.getIntOrFloatBitWidth() == 16) {\n-          vecSize = 2;\n-        } else if (elemTy.getIntOrFloatBitWidth() == 8) {\n-          vecSize = 4;\n-        } else {\n-          assert(false && \"Unsupported element type\");\n-        }\n-        Type vecTy = vec_ty(elemTy, vecSize);\n         if (mmaLayout.getVersion() == 2) {\n+          const llvm::DenseMap<int, Type> targetTyMap = {\n+              {32, elemTy},\n+              {16, vec_ty(elemTy, 2)},\n+              {8, vec_ty(elemTy, 4)},\n+          };\n+          Type targetTy;\n+          if (targetTyMap.count(elemTy.getIntOrFloatBitWidth())) {\n+            targetTy = targetTyMap.lookup(elemTy.getIntOrFloatBitWidth());\n+          } else {\n+            assert(false && \"Unsupported element type\");\n+          }\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n             int elems =\n                 MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n             return LLVM::LLVMStructType::getLiteral(\n-                ctx, SmallVector<Type>(elems, vecTy));\n+                ctx, SmallVector<Type>(elems, targetTy));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n             int elems =\n                 MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt[1]);\n-            return struct_ty(SmallVector<Type>(elems, vecTy));\n+            return struct_ty(SmallVector<Type>(elems, targetTy));\n           }\n         }\n \n@@ -3995,10 +3977,10 @@ struct InsertSliceAsyncOpConversion\n     // %other\n     SmallVector<Value> otherElems;\n     if (llOther) {\n-      // TODO(Keren): support \"other\" tensor.\n+      // FIXME(Keren): always assume other is 0 for now\n       // It's not necessary for now because the pipeline pass will skip\n       // generating insert_slice_async if the load op has any \"other\" tensor.\n-      assert(false && \"insert_slice_async: Other value not supported yet\");\n+      // assert(false && \"insert_slice_async: Other value not supported yet\");\n       otherElems = getLLVMElems(other, llOther, rewriter, loc);\n       assert(srcElems.size() == otherElems.size());\n     }"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 23, "deletions": 13, "changes": 36, "file_content_changes": "@@ -220,14 +220,17 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n \n \n-@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n-    [32, 32, 16, 4, 32, 32, 16],\n-    [32, 16, 16, 4, 32, 32, 16],\n-    [128, 8, 8, 4, 32, 32, 16],\n-    # TODO[Superjomn]: fix it later\n-    # [127, 41, 43, 4, 32, 32, 16],\n+@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K,allow_tf32', [\n+    [32, 32, 16, 4, 32, 32, 16, False],\n+    [32, 32, 16, 4, 32, 32, 16, True],\n+    [32, 16, 16, 4, 32, 32, 16, False],\n+    [32, 16, 16, 4, 32, 32, 16, True],\n+    [127, 41, 43, 4, 32, 32, 16, False],\n+    [127, 41, 43, 4, 32, 32, 16, True],\n+    [128, 8, 8, 4, 32, 32, 16, False],\n+    [128, 8, 8, 4, 32, 32, 16, True]\n ])\n-def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+def test_gemm_fp32(M, N, K, num_warps, block_M, block_N, block_K, allow_tf32):\n     @triton.jit\n     def matmul_kernel(\n         a_ptr, b_ptr, c_ptr,\n@@ -236,6 +239,7 @@ def matmul_kernel(\n         stride_bk, stride_bn,\n         stride_cm, stride_cn,\n         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+        ALLOW_TF32: tl.constexpr\n     ):\n         pid = tl.program_id(axis=0)\n         # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n@@ -253,10 +257,9 @@ def matmul_kernel(\n         for k in range(0, K, BLOCK_SIZE_K):\n             a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n             b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n-            a = tl.load(a_ptrs, a_mask)\n-            b = tl.load(b_ptrs, b_mask)\n-            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n-            accumulator += tl.dot(a, b, allow_tf32=False)\n+            a = tl.load(a_ptrs, a_mask, other=0.0)\n+            b = tl.load(b_ptrs, b_mask, other=0.0)\n+            accumulator += tl.dot(a, b, allow_tf32=ALLOW_TF32)\n             a_ptrs += BLOCK_SIZE_K * stride_ak\n             b_ptrs += BLOCK_SIZE_K * stride_bk\n             offs_k += BLOCK_SIZE_K\n@@ -267,6 +270,9 @@ def matmul_kernel(\n         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n         tl.store(c_ptrs, accumulator, c_mask)\n \n+    # Configure the pytorch counterpart\n+    torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n+\n     a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n     c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n@@ -277,8 +283,12 @@ def matmul_kernel(\n                         stride_am=a.stride(0), stride_ak=a.stride(1),\n                         stride_bk=b.stride(0), stride_bn=b.stride(1),\n                         stride_cm=c.stride(0), stride_cn=c.stride(1),\n-                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n+                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K, ALLOW_TF32=allow_tf32)\n \n     golden = torch.matmul(a, b)\n     golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n-    torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))\n+    if allow_tf32:\n+        # TF32 is not accurate enough\n+        torch.testing.assert_close(c, golden, rtol=max(1e-2, 1.5 * golden_rel_err), atol=max(1e-2, 1.5 * golden_abs_err))\n+    else:\n+        torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -923,6 +923,45 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+#mma = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: matmul_tf32dot\n+  func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    // CHECK-SAME: (f32, f32, f32, f32)\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n+\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<32x16xf32, #dot_operand_a> * tensor<16x32xf32, #dot_operand_b> -> tensor<32x32xf32, #mma>\n+    %38 = triton_gpu.convert_layout %28 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %38 : tensor<32x32xf32, #blocked>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32"}]