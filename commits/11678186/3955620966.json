[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -77,6 +77,10 @@ SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   return result;\n }\n \n+bool isMmaToDotShortcut(\n+    triton::gpu::MmaEncodingAttr &mmaLayout,\n+    triton::gpu::DotOperandEncodingAttr &dotOperandLayout);\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 1, "deletions": 9, "changes": 10, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"llvm/ADT/SmallVector.h\"\n+#include \"triton/Analysis/Utility.h\"\n \n #include <algorithm>\n #include <limits>\n@@ -51,15 +52,6 @@ getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n   return {inOrd, outOrd};\n }\n \n-bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n-                        DotOperandEncodingAttr &dotOperandLayout) {\n-  // dot_op<opIdx=0, parent=#mma> = #mma\n-  // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n-  return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n-         dotOperandLayout.getOpIdx() == 0 &&\n-         dotOperandLayout.getParent() == mmaLayout;\n-}\n-\n SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n                              unsigned &outVec) {"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -152,4 +152,14 @@ std::string getValueOperandName(Value value, AsmState &state) {\n   return opName;\n }\n \n+bool isMmaToDotShortcut(\n+    triton::gpu::MmaEncodingAttr &mmaLayout,\n+    triton::gpu::DotOperandEncodingAttr &dotOperandLayout) {\n+  // dot_op<opIdx=0, parent=#mma> = #mma\n+  // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+  return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n+         dotOperandLayout.getOpIdx() == 0 &&\n+         dotOperandLayout.getParent() == mmaLayout;\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 8, "changes": 9, "file_content_changes": "@@ -1,5 +1,6 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"DotOpHelpers.h\"\n+#include \"Utility.h\"\n \n using ::mlir::LLVM::DotOpFMAConversionHelper;\n using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n@@ -17,14 +18,6 @@ using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n-bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n-                        DotOperandEncodingAttr &dotOperandLayout) {\n-  // dot_op<opIdx=0, parent=#mma> = #mma\n-  // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n-  return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n-         dotOperandLayout.getOpIdx() == 0 &&\n-         dotOperandLayout.getParent() == mmaLayout;\n-}\n \n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -8,9 +8,6 @@ using namespace mlir::triton;\n \n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n \n-bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n-                        DotOperandEncodingAttr &dotOperandLayout);\n-\n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 0, "deletions": 10, "changes": 10, "file_content_changes": "@@ -330,16 +330,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return ret;\n   }\n \n-  bool isMmaToDotShortcut(\n-      MmaEncodingAttr &mmaLayout,\n-      triton::gpu::DotOperandEncodingAttr &dotOperandLayout) const {\n-    // dot_op<opIdx=0, parent=#mma> = #mma\n-    // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n-    return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n-           dotOperandLayout.getOpIdx() == 0 &&\n-           dotOperandLayout.getParent() == mmaLayout;\n-  }\n-\n   void storeDistributedToShared(Value src, Value llSrc,\n                                 ArrayRef<Value> dstStrides,\n                                 ArrayRef<SmallVector<Value>> srcIndices,"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1580,7 +1580,7 @@ def _init_handles(self):\n         if self.shared > max_shared:\n             raise OutOfResources(self.shared, max_shared, \"shared memory\")\n         mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n-        print(self.shared, n_regs, n_spills)\n+        # print(self.shared, n_regs, n_spills)\n         self.cu_module = mod\n         self.cu_function = func\n "}, {"filename": "test/lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -6,4 +6,5 @@ add_mlir_library(TritonTestAnalysis\n \n   LINK_LIBS PUBLIC\n   TritonAnalysis\n+  ${dialect_libs}\n )\n\\ No newline at end of file"}]