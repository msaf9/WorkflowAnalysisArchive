[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -96,7 +96,7 @@ def test_matmul(M, N, K, dtype_str):\n     ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.02)\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, tol=0.05)\n \n \n #######################\n@@ -152,7 +152,7 @@ def test_elementwise(N):\n     ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.02)\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, tol=0.05)\n \n #######################\n # Flash-Attention\n@@ -200,4 +200,4 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n-    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, atol=0.02)\n+    assert triton.testing.allclose(cur_gpu_util, ref_gpu_util, tol=0.05)"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -94,6 +94,10 @@ def assert_almost_equal(x, y, decimal=2, err_msg=''):\n \n \n def allclose(x, y, tol=1e-2):\n+    if not isinstance(x, torch.Tensor):\n+        x = torch.tensor(x)\n+    if not isinstance(y, torch.Tensor):\n+        y = torch.tensor(y)\n     if x.dtype != y.dtype:\n         raise RuntimeError(f'{x.dtype} did not match with {x.dtype}')\n     if x.shape != y.shape:"}]