[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1309,7 +1309,6 @@ class TritonGPUCombineOpsPass\n     if (fixupLoops(m).failed()) {\n       signalPassFailure();\n     }\n-\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "file_content_changes": "@@ -215,16 +215,16 @@ LogicalResult LoopPipeliner::initialize() {\n       // advance to the first conversion as long\n       // as the use resides in shared memory and it has\n       // a single use itself\n-      while(use){\n-        if(use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n+      while (use) {\n+        if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n           break;\n-        auto tensorType = use->getResult(0).getType().dyn_cast<RankedTensorType>();\n-        if(!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n+        auto tensorType =\n+            use->getResult(0).getType().dyn_cast<RankedTensorType>();\n+        if (!tensorType.getEncoding().isa<ttg::SharedEncodingAttr>())\n           break;\n         use = *use->getResult(0).getUsers().begin();\n       }\n \n-\n       if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use)) {\n         if (auto tensorType = convertLayout.getResult()\n                                   .getType()\n@@ -252,7 +252,6 @@ LogicalResult LoopPipeliner::initialize() {\n       loads.insert(loadOp);\n   }\n \n-\n   // We have some loads to pipeline\n   if (!loads.empty()) {\n     // Update depArgs & depOps\n@@ -383,13 +382,14 @@ void LoopPipeliner::emitPrologue() {\n                                    loads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (Value loadOp : loads) {\n-    auto bufferType = loadStageBuffer[loadOp][numStages - 1].getType()\n+    auto bufferType = loadStageBuffer[loadOp][numStages - 1]\n+                          .getType()\n                           .cast<RankedTensorType>();\n     auto bufferShape = bufferType.getShape();\n     auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n-    sliceType =\n-        RankedTensorType::get({bufferShape[1], bufferShape[2]}, sliceType.getElementType(),\n-                              loadsBufferType[loadOp].getEncoding());\n+    sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n+                                      sliceType.getElementType(),\n+                                      loadsBufferType[loadOp].getEncoding());\n     Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n@@ -583,8 +583,9 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       auto bufferType = insertAsyncOp.getType().cast<RankedTensorType>();\n       auto bufferShape = bufferType.getShape();\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n-      sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]}, sliceType.getElementType(),\n-                              loadsBufferType[loadOp].getEncoding());\n+      sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n+                                        sliceType.getElementType(),\n+                                        loadsBufferType[loadOp].getEncoding());\n \n       nextOp = builder.create<tensor::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n@@ -710,7 +711,6 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n         forOp->getResult(i).replaceAllUsesWith(newForOp->getResult(i));\n       forOp->erase();\n     });\n-\n   }\n };\n } // anonymous namespace"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 0, "deletions": 63, "changes": 63, "file_content_changes": "@@ -1952,69 +1952,6 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     assert torch.equal(z, x)\n \n \n-# layouts = [\n-#     BlockedLayout([1, 8], [2, 16], [4, 1], [1, 0]),\n-#     BlockedLayout([1, 4], [4, 8], [2, 2], [1, 0]),\n-#     BlockedLayout([1, 1], [1, 32], [2, 2], [1, 0]),\n-#     BlockedLayout([8, 1], [16, 2], [1, 4], [0, 1]),\n-#     BlockedLayout([4, 1], [8, 4], [2, 2], [0, 1]),\n-#     BlockedLayout([1, 1], [32, 1], [2, 2], [0, 1]),\n-#     BlockedLayout([4, 4], [1, 32], [4, 1], [1, 0])\n-# ]\n-\n-\n-# @pytest.mark.parametrize(\"shape\", [(128, 128)])\n-# @pytest.mark.parametrize(\"dtype\", ['float16'])\n-# @pytest.mark.parametrize(\"src_layout\", layouts)\n-# @pytest.mark.parametrize(\"dst_layout\", layouts)\n-# def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n-#     if str(src_layout) == str(dst_layout):\n-#         pytest.skip()\n-#     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n-#         pytest.skip()\n-\n-#     ir = f\"\"\"\n-# #src = {src_layout}\n-# #dst = {dst_layout}\n-# \"\"\" + \"\"\"\n-# module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-#   func public @kernel_0d1d(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}) {\n-#     %cst = arith.constant dense<128> : tensor<128x1xi32, #src>\n-#     %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n-#     %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>\n-#     %2 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #src>\n-#     %4 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #src}>>) -> tensor<128x1xi32, #src>\n-#     %5 = arith.muli %4, %cst : tensor<128x1xi32, #src>\n-#     %6 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #src}>>) -> tensor<1x128xi32, #src>\n-#     %7 = tt.broadcast %6 : (tensor<1x128xi32, #src>) -> tensor<128x128xi32, #src>\n-#     %8 = tt.broadcast %5 : (tensor<128x1xi32, #src>) -> tensor<128x128xi32, #src>\n-#     %9 = arith.addi %8, %7 : tensor<128x128xi32, #src>\n-#     %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>, tensor<128x128xi32, #src>\n-#     %11 = tt.load %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf16, #src>\n-#     %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #dst>\n-#     %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n-#     %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n-#     %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n-#     tt.store %14, %13 : tensor<128x128xf16, #dst>\n-#     return\n-#   }\n-# }\n-# \"\"\"\n-\n-#     x = to_triton(numpy_random(shape, dtype_str=dtype))\n-#     z = torch.empty_like(x)\n-\n-#     # write the IR to a temporary file using mkstemp\n-#     import tempfile\n-#     with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n-#         f.write(ir)\n-#         f.flush()\n-#         kernel = triton.compile(f.name)\n-#     kernel[(1, 1, 1)](x.data_ptr(), z.data_ptr())\n-\n-#     assert torch.equal(z, x)\n-\n-\n def test_load_scalar_with_mask():\n     @triton.jit\n     def kernel(Input, Index, Out, N: int):"}]