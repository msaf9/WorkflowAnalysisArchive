[{"filename": ".github/workflows/documentation.yml", "status": "modified", "additions": 20, "deletions": 19, "changes": 39, "file_content_changes": "@@ -5,37 +5,39 @@ on:\n     - cron: \"0 0 * * *\"\n \n jobs:\n-\n   Build-Documentation:\n-\n-    runs-on: [self-hosted, V100]\n+    runs-on: [self-hosted, A100]\n \n     steps:\n-\n-      - name: Checkout gh-pages\n+      - name: Checkout branch\n         uses: actions/checkout@v2\n         with:\n-          ref: 'gh-pages'\n+          token: ${{ secrets.CI_PAT }}\n+          fetch-depth: 0\n \n       - name: Clear docs\n         run: |\n           rm -r /tmp/triton-docs\n         continue-on-error: true\n \n-      - name: Checkout branch\n-        uses: actions/checkout@v2\n+      - name: Install dependent packages\n+        run: |\n+          pip3 install tabulate\n+          pip3 install cmake\n+\n+      #- name: Fetch dependent branches\n+      #  run: |\n+      #    git fetch origin main:main\n \n       - name: Build docs\n         run: |\n-          git fetch origin main\n           cd docs\n-          sphinx-multiversion . _build/html/\n+          export PATH=$(python3 -c \"import cmake; print(cmake.CMAKE_BIN_DIR)\"):$PATH\n+          python3 -m sphinx_multiversion . _build/html/\n \n-      - name: Publish docs\n+      - name: Update docs\n         run: |\n-          git branch\n-          # update docs\n-          mkdir /tmp/triton-docs;\n+          mkdir /tmp/triton-docs\n           mv docs/_build/html/* /tmp/triton-docs/\n           git checkout gh-pages\n           cp -r CNAME /tmp/triton-docs/\n@@ -45,8 +47,7 @@ jobs:\n           cp -r /tmp/triton-docs/* .\n           git add .\n           git commit -am \"[GH-PAGES] Updated website\"\n-          # publish docs\n-          eval `ssh-agent -s`\n-          DISPLAY=:0 SSH_ASKPASS=~/.ssh/give_pass.sh ssh-add ${{ secrets.SSH_KEY }} <<< ${{ secrets.SSH_PASS }}\n-          git remote set-url origin git@github.com:openai/triton.git\n-          git push\n+\n+      - name: Publish docs\n+        run: |\n+          git push origin gh-pages"}, {"filename": "docs/conf.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -86,8 +86,8 @@ def documenter(app, obj, parent):\n autosummary_generate = True\n \n # versioning config\n-smv_tag_whitelist = r'^(v1.1.2)$'\n-smv_branch_whitelist = r'^master$'\n+smv_tag_whitelist = r'^(v2.1.0)$'\n+smv_branch_whitelist = r'^main$'\n smv_remote_whitelist = None\n smv_released_pattern = r'^tags/.*$'\n smv_outputdir_format = '{ref.name}'\n@@ -100,6 +100,7 @@ def documenter(app, obj, parent):\n     'examples_dirs': '../python/tutorials/',\n     'gallery_dirs': 'getting-started/tutorials',\n     'filename_pattern': '',\n+    # XXX: Temporarily disable fused attention tutorial on V100\n     'ignore_pattern': r'__init__\\.py',\n     'within_subsection_order': FileNameSortKey,\n     'reference_url': {"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -25,8 +25,6 @@ std::unique_ptr<Pass> createTritonGPUVerifier();\n \n std::unique_ptr<Pass> createTritonGPUOptimizeDotOperandsPass();\n \n-std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass();\n-\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\""}, {"filename": "python/MANIFEST.in", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n graft src\n graft triton/third_party\n+graft triton/runtime/backends/"}, {"filename": "python/setup.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -227,7 +227,6 @@ def build_extension(self, ext):\n         \"triton/ops\",\n         \"triton/ops/blocksparse\",\n         \"triton/runtime\",\n-        \"triton/runtime/driver\",\n         \"triton/tools\",\n     ],\n     install_requires=["}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 9, "deletions": 14, "changes": 23, "file_content_changes": "@@ -15,10 +15,10 @@\n \n import triton\n import triton._C.libtriton.triton as _triton\n+from ..runtime import driver\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n from ..runtime.cache import get_cache_manager\n-from ..runtime.driver import get_cuda_utils, get_hip_utils\n from ..tools.disasm import extract\n from .code_generator import ast_to_ttir\n from .make_launcher import make_stub\n@@ -519,24 +519,19 @@ def __init__(self, fn, so_path, metadata, asm):\n         self.metadata = metadata\n         self.cu_module = None\n         self.cu_function = None\n-        self.is_hip = \"amdgcn\" in asm\n \n     def _init_handles(self):\n         if self.cu_module is not None:\n             return\n         device = triton.runtime.jit.get_current_device()\n-        if self.is_hip:\n-            hip_utils = get_hip_utils()\n-            max_shared = hip_utils.get_device_properties(device)[\"max_shared_mem\"]\n-            if self.shared > max_shared:\n-                raise OutOfResources(self.shared, max_shared, \"shared memory\")\n-            mod, func, n_regs, n_spills = hip_utils.load_binary(self.metadata[\"name\"], self.asm[\"hsaco_path\"], self.shared, device)\n-        else:\n-            cuda_utils = get_cuda_utils()\n-            max_shared = cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n-            if self.shared > max_shared:\n-                raise OutOfResources(self.shared, max_shared, \"shared memory\")\n-            mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n+        bin_path = {\n+            driver.HIP: \"hsaco_path\",\n+            driver.CUDA: \"cubin\"\n+        }[driver.backend]\n+        max_shared = driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n+        if self.shared > max_shared:\n+            raise OutOfResources(self.shared, max_shared, \"shared memory\")\n+        mod, func, n_regs, n_spills = driver.utils.load_binary(self.metadata[\"name\"], self.asm[bin_path], self.shared, device)\n \n         self.n_spills = n_spills\n         self.n_regs = n_regs"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1255,9 +1255,10 @@ def _argreduce(input, axis, combine_fn, _builder=None, _generator=None):\n     index = arange(0, n, _builder=_builder)\n \n     if len(input.shape) > 1:\n-        new_shape = [constexpr(1)] * len(input.shape)\n-        new_shape[axis] = constexpr(n)\n-        index = view(index, new_shape, _builder=_builder)\n+        # Broadcast index across the non-reduced axes\n+        expand_dims_index = [None] * len(input.shape)\n+        expand_dims_index[axis] = slice(None)\n+        index = index.__getitem__(expand_dims_index, _builder=_builder)\n         index = broadcast_to(index, input.shape, _builder=_builder)\n \n     rvalue, rindices = reduce((input, index), axis, combine_fn,"}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 2, "deletions": 7, "changes": 9, "file_content_changes": "@@ -1,14 +1,9 @@\n import os\n \n-import torch\n-\n+from ..runtime import driver\n from . import core, extern\n \n-if torch.version.hip is not None:\n-    LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"cuda2gcn.bc\")\n-else:\n-    LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\", \"cuda\", \"lib\", \"libdevice.10.bc\")\n-LIBDEVICE_PATH = os.getenv(\"TRITON_LIBDEVICE_PATH\", LOCAL_PATH)\n+LIBDEVICE_PATH = os.getenv(\"TRITON_LIBDEVICE_PATH\", driver.libdevice_path)\n \n \n @extern.extern"}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 5, "deletions": 10, "changes": 15, "file_content_changes": "@@ -4,24 +4,22 @@\n \n import triton\n import triton._C.libtriton.triton as _triton\n-from triton.runtime.driver.cuda import get_cuda_utils\n+from triton.runtime import driver\n from triton.testing import get_dram_gbps, get_max_simd_tflops, get_max_tensorcore_tflops\n \n \n def get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype):\n     ''' return compute throughput in TOPS '''\n     total_warps = num_ctas * min(num_warps, 4)\n-    cuda_utils = get_cuda_utils()\n-    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n+    num_subcores = driver.utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n     tflops = min(num_subcores, total_warps) / num_subcores * get_max_tensorcore_tflops(dtype, backend, device)\n     return tflops\n \n \n def get_simd_tflops(backend, device, num_ctas, num_warps, dtype):\n     ''' return compute throughput in TOPS '''\n     total_warps = num_ctas * min(num_warps, 4)\n-    cuda_utils = get_cuda_utils()\n-    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n+    num_subcores = driver.utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n     tflops = min(num_subcores, total_warps) / num_subcores * get_max_simd_tflops(dtype, backend, device)\n     return tflops\n \n@@ -62,8 +60,7 @@ def estimate_matmul_time(\n     compute_ms = total_ops / tput\n \n     # time to load data\n-    cuda_utils = get_cuda_utils()\n-    num_sm = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"]\n+    num_sm = driver.utils.get_device_properties(device)[\"multiprocessor_count\"]\n     active_cta_ratio = min(1, num_ctas / num_sm)\n     active_cta_ratio_bw1 = min(1, num_ctas / 32)  # 32 active ctas are enough to saturate\n     active_cta_ratio_bw2 = max(min(1, (num_ctas - 32) / (108 - 32)), 0)  # 32-108, remaining 5%\n@@ -114,9 +111,7 @@ def early_config_prune(configs, named_args):\n         BLOCK_M, BLOCK_N, BLOCK_K, num_stages = \\\n             kw['BLOCK_M'], kw['BLOCK_N'], kw['BLOCK_K'], config.num_stages\n \n-        # TODO: move to `cuda_utils` submodule\n-        cuda_utils = get_cuda_utils()\n-        max_shared_memory = cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n+        max_shared_memory = driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n         required_shared_memory = (BLOCK_M + BLOCK_N) * BLOCK_K * num_stages * dtsize\n         if required_shared_memory <= max_shared_memory:\n             pruned_configs.append(config)"}, {"filename": "python/triton/runtime/__init__.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,6 +1,6 @@\n-from . import driver\n from .autotuner import (Autotuner, Config, Heuristics, OutOfResources, autotune,\n                         heuristics)\n+from .driver import driver\n from .jit import (JITFunction, KernelInterface, MockTensor, TensorWrapper, reinterpret,\n                   version_key)\n "}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -79,7 +79,7 @@ def kernel_call():\n         try:\n             return do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n         except OutOfResources:\n-            return (float('inf'), float('inf'), float('inf'))\n+            return [float('inf'), float('inf'), float('inf')]\n \n     def run(self, *args, **kwargs):\n         self.nargs = dict(zip(self.arg_names, args))"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "added", "additions": 124, "deletions": 0, "changes": 124, "file_content_changes": "@@ -0,0 +1,124 @@\n+#include \"cuda.h\"\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+\n+static inline void gpuAssert(CUresult code, const char *file, int line) {\n+  if (code != CUDA_SUCCESS) {\n+    const char *prefix = \"Triton Error [CUDA]: \";\n+    const char *str;\n+    cuGetErrorString(code, &str);\n+    char err[1024] = {0};\n+    strcat(err, prefix);\n+    strcat(err, str);\n+    PyErr_SetString(PyExc_RuntimeError, err);\n+  }\n+}\n+\n+#define CUDA_CHECK(ans)                                                        \\\n+  {                                                                            \\\n+    gpuAssert((ans), __FILE__, __LINE__);                                      \\\n+    if (PyErr_Occurred())                                                      \\\n+      return NULL;                                                             \\\n+  }\n+\n+static PyObject *getDeviceProperties(PyObject *self, PyObject *args) {\n+  int device_id;\n+  if (!PyArg_ParseTuple(args, \"i\", &device_id))\n+    return NULL;\n+  // Get device handle\n+  CUdevice device;\n+  cuDeviceGet(&device, device_id);\n+\n+  // create a struct to hold device properties\n+  int max_shared_mem;\n+  int multiprocessor_count;\n+  int sm_clock_rate;\n+  int mem_clock_rate;\n+  int mem_bus_width;\n+  CUDA_CHECK(cuDeviceGetAttribute(\n+      &max_shared_mem, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN,\n+      device));\n+  CUDA_CHECK(cuDeviceGetAttribute(\n+      &multiprocessor_count, CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, device));\n+  CUDA_CHECK(cuDeviceGetAttribute(&sm_clock_rate,\n+                                  CU_DEVICE_ATTRIBUTE_CLOCK_RATE, device));\n+  CUDA_CHECK(cuDeviceGetAttribute(\n+      &mem_clock_rate, CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE, device));\n+  CUDA_CHECK(cuDeviceGetAttribute(\n+      &mem_bus_width, CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH, device));\n+\n+  return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\",\n+                       max_shared_mem, \"multiprocessor_count\",\n+                       multiprocessor_count, \"sm_clock_rate\", sm_clock_rate,\n+                       \"mem_clock_rate\", mem_clock_rate, \"mem_bus_width\",\n+                       mem_bus_width);\n+}\n+\n+static PyObject *loadBinary(PyObject *self, PyObject *args) {\n+  const char *name;\n+  const char *data;\n+  Py_ssize_t data_size;\n+  int shared;\n+  int device;\n+  if (!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared,\n+                        &device)) {\n+    return NULL;\n+  }\n+  CUfunction fun;\n+  CUmodule mod;\n+  int32_t n_regs = 0;\n+  int32_t n_spills = 0;\n+  // create driver handles\n+  CUDA_CHECK(cuModuleLoadData(&mod, data));\n+  CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n+  // get allocated registers and spilled registers from the function\n+  CUDA_CHECK(cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS, fun));\n+  CUDA_CHECK(\n+      cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun));\n+  n_spills /= 4;\n+  // set dynamic shared memory if necessary\n+  int shared_optin;\n+  CUDA_CHECK(cuDeviceGetAttribute(\n+      &shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN,\n+      device));\n+  if (shared > 49152 && shared_optin > 49152) {\n+    CUDA_CHECK(cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED));\n+    int shared_total, shared_static;\n+    CUDA_CHECK(cuDeviceGetAttribute(\n+        &shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR,\n+        device));\n+    CUDA_CHECK(cuFuncGetAttribute(&shared_static,\n+                                  CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n+    CUDA_CHECK(\n+        cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n+                           shared_optin - shared_static));\n+  }\n+\n+  if (PyErr_Occurred()) {\n+    return NULL;\n+  }\n+  return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs,\n+                       n_spills);\n+}\n+\n+static PyMethodDef ModuleMethods[] = {\n+    {\"load_binary\", loadBinary, METH_VARARGS,\n+     \"Load provided cubin into CUDA driver\"},\n+    {\"get_device_properties\", getDeviceProperties, METH_VARARGS,\n+     \"Get the properties for a given device\"},\n+    {NULL, NULL, 0, NULL} // sentinel\n+};\n+\n+static struct PyModuleDef ModuleDef = {PyModuleDef_HEAD_INIT, \"cuda_utils\",\n+                                       NULL, // documentation\n+                                       -1,   // size\n+                                       ModuleMethods};\n+\n+PyMODINIT_FUNC PyInit_cuda_utils(void) {\n+  PyObject *m = PyModule_Create(&ModuleDef);\n+  if (m == NULL) {\n+    return NULL;\n+  }\n+  PyModule_AddFunctions(m, ModuleMethods);\n+  return m;\n+}"}, {"filename": "python/triton/runtime/backends/hip.c", "status": "added", "additions": 120, "deletions": 0, "changes": 120, "file_content_changes": "@@ -0,0 +1,120 @@\n+#define __HIP_PLATFORM_AMD__\n+#include <hip/hip_runtime.h>\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+#include <stdio.h>\n+#include <stdlib.h>\n+\n+static inline void gpuAssert(hipError_t code, const char *file, int line) {\n+  {\n+    if (code != HIP_SUCCESS) {\n+      {\n+        const char *prefix = \"Triton Error [HIP]: \";\n+        const char *str = hipGetErrorString(code);\n+        char err[1024] = {0};\n+        snprintf(err, 1024, \"%s Code: %d, Messsage: %s\", prefix, code, str);\n+        PyErr_SetString(PyExc_RuntimeError, err);\n+      }\n+    }\n+  }\n+}\n+\n+#define HIP_CHECK(ans)                                                         \\\n+  {                                                                            \\\n+    gpuAssert((ans), __FILE__, __LINE__);                                      \\\n+    if (PyErr_Occurred())                                                      \\\n+      return NULL;                                                             \\\n+  }\n+\n+static PyObject *getDeviceProperties(PyObject *self, PyObject *args) {\n+  int device_id;\n+  if (!PyArg_ParseTuple(args, \"i\", &device_id))\n+    return NULL;\n+\n+  hipDeviceProp_t props;\n+  HIP_CHECK(hipGetDeviceProperties(&props, device_id));\n+\n+  // create a struct to hold device properties\n+  return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\",\n+                       props.sharedMemPerBlock, \"multiprocessor_count\",\n+                       props.multiProcessorCount, \"sm_clock_rate\",\n+                       props.clockRate, \"mem_clock_rate\", props.memoryClockRate,\n+                       \"mem_bus_width\", props.memoryBusWidth);\n+}\n+\n+static PyObject *loadBinary(PyObject *self, PyObject *args) {\n+  const char *name;\n+  const char *data;\n+  Py_ssize_t data_size;\n+  int shared;\n+  int device;\n+  if (!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared,\n+                        &device)) {\n+    return NULL;\n+  }\n+\n+  // Open HSACO file\n+  FILE *hsaco_file;\n+  if ((hsaco_file = fopen(data, \"rb\")) == NULL) {\n+    return NULL;\n+  }\n+\n+  // Read HSCAO file into Buffer\n+  fseek(hsaco_file, 0L, SEEK_END);\n+  size_t hsaco_file_size = ftell(hsaco_file);\n+  unsigned char *hsaco =\n+      (unsigned char *)malloc(hsaco_file_size * sizeof(unsigned char));\n+  rewind(hsaco_file);\n+  fread(hsaco, sizeof(unsigned char), hsaco_file_size, hsaco_file);\n+  fclose(hsaco_file);\n+\n+  // set HIP options\n+  hipJitOption opt[] = {hipJitOptionErrorLogBufferSizeBytes,\n+                        hipJitOptionErrorLogBuffer,\n+                        hipJitOptionInfoLogBufferSizeBytes,\n+                        hipJitOptionInfoLogBuffer, hipJitOptionLogVerbose};\n+  const unsigned int errbufsize = 8192;\n+  const unsigned int logbufsize = 8192;\n+  char _err[errbufsize];\n+  char _log[logbufsize];\n+  void *optval[] = {(void *)(uintptr_t)errbufsize, (void *)_err,\n+                    (void *)(uintptr_t)logbufsize, (void *)_log, (void *)1};\n+\n+  // launch HIP Binary\n+  hipModule_t mod;\n+  hipFunction_t fun;\n+  hipModuleLoadDataEx(&mod, hsaco, 5, opt, optval);\n+  hipModuleGetFunction(&fun, mod, name);\n+  free(hsaco);\n+\n+  // get allocated registers and spilled registers from the function\n+  int n_regs = 0;\n+  int n_spills = 0;\n+  if (PyErr_Occurred()) {\n+    return NULL;\n+  }\n+  return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs,\n+                       n_spills);\n+}\n+\n+static PyMethodDef ModuleMethods[] = {\n+    {\"load_binary\", loadBinary, METH_VARARGS,\n+     \"Load provided hsaco into HIP driver\"},\n+    {\"get_device_properties\", getDeviceProperties, METH_VARARGS,\n+     \"Get the properties for a given device\"},\n+    {NULL, NULL, 0, NULL} // sentinel\n+};\n+\n+static struct PyModuleDef ModuleDef = {PyModuleDef_HEAD_INIT, \"hip_utils\",\n+                                       NULL, // documentation\n+                                       -1,   // size\n+                                       ModuleMethods};\n+\n+PyMODINIT_FUNC PyInit_hip_utils(void) {\n+  PyObject *m = PyModule_Create(&ModuleDef);\n+  if (m == NULL) {\n+    return NULL;\n+  }\n+  PyModule_AddFunctions(m, ModuleMethods);\n+  return m;\n+}"}, {"filename": "python/triton/runtime/driver.py", "status": "added", "additions": 149, "deletions": 0, "changes": 149, "file_content_changes": "@@ -0,0 +1,149 @@\n+import abc\n+import hashlib\n+import os\n+import tempfile\n+from pathlib import Path\n+\n+from ..common.build import _build\n+from .cache import get_cache_manager\n+\n+\n+class DriverBase(metaclass=abc.ABCMeta):\n+\n+    CUDA = 0\n+    HIP = 1\n+\n+    @staticmethod\n+    def third_party_dir():\n+        return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\")\n+\n+    def __init__(self) -> None:\n+        pass\n+# -----------------------------\n+# CUDA\n+# -----------------------------\n+\n+\n+class CudaUtils(object):\n+\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(CudaUtils, cls).__new__(cls)\n+        return cls.instance\n+\n+    def __init__(self):\n+        dirname = os.path.dirname(os.path.realpath(__file__))\n+        src = Path(os.path.join(dirname, \"backends\", \"cuda.c\")).read_text()\n+        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n+        cache = get_cache_manager(key)\n+        fname = \"cuda_utils.so\"\n+        cache_path = cache.get_file(fname)\n+        if cache_path is None:\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                src_path = os.path.join(tmpdir, \"main.c\")\n+                with open(src_path, \"w\") as f:\n+                    f.write(src)\n+                so = _build(\"cuda_utils\", src_path, tmpdir)\n+                with open(so, \"rb\") as f:\n+                    cache_path = cache.put(f.read(), fname, binary=True)\n+        import importlib.util\n+        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache_path)\n+        mod = importlib.util.module_from_spec(spec)\n+        spec.loader.exec_module(mod)\n+        self.load_binary = mod.load_binary\n+        self.get_device_properties = mod.get_device_properties\n+\n+\n+class CudaDriver(DriverBase):\n+\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(CudaDriver, cls).__new__(cls)\n+        return cls.instance\n+\n+    def get_libdevice_path(self):\n+        return os.path.join(self.third_party_dir(), \"cuda\", \"lib\", \"libdevice.10.bc\")\n+\n+    def __init__(self):\n+        self.utils = CudaUtils()\n+        self.backend = self.CUDA\n+        self.libdevice_path = self.get_libdevice_path()\n+\n+# -----------------------------\n+# HIP\n+# -----------------------------\n+\n+\n+class HIPUtils(object):\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(HIPUtils, cls).__new__(cls)\n+        return cls.instance\n+\n+    def __init__(self):\n+        dirname = os.path.dirname(os.path.realpath(__file__))\n+        src = Path(os.path.join(dirname, \"backends\", \"hip.c\")).read_text()\n+        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n+        cache = get_cache_manager(key)\n+        fname = \"hip_utils.so\"\n+        cache_path = cache.get_file(fname)\n+        if cache_path is None:\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                src_path = os.path.join(tmpdir, \"main.c\")\n+                with open(src_path, \"w\") as f:\n+                    f.write(src)\n+                so = _build(\"hip_utils\", src_path, tmpdir)\n+                with open(so, \"rb\") as f:\n+                    cache_path = cache.put(f.read(), fname, binary=True)\n+        import importlib.util\n+        spec = importlib.util.spec_from_file_location(\"hip_utils\", cache_path)\n+        mod = importlib.util.module_from_spec(spec)\n+        spec.loader.exec_module(mod)\n+        self.load_binary = mod.load_binary\n+        self.get_device_properties = mod.get_device_properties\n+\n+\n+class HIPDriver(DriverBase):\n+\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(HIPDriver, cls).__new__(cls)\n+        return cls.instance\n+\n+    def get_libdevice_path(self):\n+        return os.path.join(self.third_party_dir(), \"third_party\", \"rocm\", \"lib\", \"libdevice.10.bc\")\n+\n+    def __init__(self):\n+        self.utils = HIPUtils()\n+        self.backend = self.HIP\n+        self.libdevice_path = self.get_libdevice_path()\n+\n+\n+class UnsupportedDriver(DriverBase):\n+\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(UnsupportedDriver, cls).__new__(cls)\n+        return cls.instance\n+\n+    def __init__(self):\n+        self.utils = None\n+        self.backend = None\n+        self.libdevice_path = ''\n+\n+# -----------------------------\n+# Driver\n+# -----------------------------\n+\n+\n+def create_driver():\n+    import torch\n+    if torch.version.hip is not None:\n+        return HIPDriver()\n+    elif torch.cuda.is_available():\n+        return CudaDriver()\n+    else:\n+        return UnsupportedDriver()\n+\n+\n+driver = create_driver()"}, {"filename": "python/triton/runtime/driver/__init__.py", "status": "removed", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -1,4 +0,0 @@\n-from .cuda import get_cuda_utils\n-from .hip import get_hip_utils\n-\n-__all__ = ['get_cuda_utils', 'get_hip_utils']"}, {"filename": "python/triton/runtime/driver/cuda.py", "status": "removed", "additions": 0, "deletions": 159, "changes": 159, "file_content_changes": "@@ -1,159 +0,0 @@\n-import hashlib\n-import os\n-import tempfile\n-\n-from ...common.build import _build\n-from ..cache import get_cache_manager\n-\n-\n-def get_cuda_utils():\n-    global _cuda_utils\n-    if _cuda_utils is None:\n-        _cuda_utils = CudaUtils()\n-    return _cuda_utils\n-\n-\n-_cuda_utils = None\n-\n-\n-class CudaUtils(object):\n-\n-    def __new__(cls):\n-        if not hasattr(cls, 'instance'):\n-            cls.instance = super(CudaUtils, cls).__new__(cls)\n-        return cls.instance\n-\n-    @staticmethod\n-    def _generate_src():\n-        return \"\"\"\n-        #include <cuda.h>\n-\n-        #include \\\"cuda.h\\\"\n-        #define PY_SSIZE_T_CLEAN\n-        #include <Python.h>\n-\n-        static inline void gpuAssert(CUresult code, const char *file, int line)\n-        {\n-           if (code != CUDA_SUCCESS)\n-           {\n-              const char* prefix = \"Triton Error [CUDA]: \";\n-              const char* str;\n-              cuGetErrorString(code, &str);\n-              char err[1024] = {0};\n-              strcat(err, prefix);\n-              strcat(err, str);\n-              PyErr_SetString(PyExc_RuntimeError, err);\n-           }\n-        }\n-\n-        #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); if(PyErr_Occurred()) return NULL; }\n-\n-        static PyObject* getDeviceProperties(PyObject* self, PyObject* args){\n-            int device_id;\n-            if(!PyArg_ParseTuple(args, \"i\", &device_id))\n-                return NULL;\n-            // Get device handle\n-            CUdevice device;\n-            cuDeviceGet(&device, device_id);\n-\n-            // create a struct to hold device properties\n-            int max_shared_mem;\n-            int multiprocessor_count;\n-            int sm_clock_rate;\n-            int mem_clock_rate;\n-            int mem_bus_width;\n-            CUDA_CHECK(cuDeviceGetAttribute(&max_shared_mem, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device));\n-            CUDA_CHECK(cuDeviceGetAttribute(&multiprocessor_count, CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, device));\n-            CUDA_CHECK(cuDeviceGetAttribute(&sm_clock_rate, CU_DEVICE_ATTRIBUTE_CLOCK_RATE, device));\n-            CUDA_CHECK(cuDeviceGetAttribute(&mem_clock_rate, CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE, device));\n-            CUDA_CHECK(cuDeviceGetAttribute(&mem_bus_width, CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH, device));\n-\n-\n-            return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\", max_shared_mem,\n-                                       \"multiprocessor_count\", multiprocessor_count,\n-                                       \"sm_clock_rate\", sm_clock_rate,\n-                                       \"mem_clock_rate\", mem_clock_rate,\n-                                       \"mem_bus_width\", mem_bus_width);\n-        }\n-\n-        static PyObject* loadBinary(PyObject* self, PyObject* args) {\n-            const char* name;\n-            const char* data;\n-            Py_ssize_t data_size;\n-            int shared;\n-            int device;\n-            if(!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared, &device)) {\n-                return NULL;\n-            }\n-            CUfunction fun;\n-            CUmodule mod;\n-            int32_t n_regs = 0;\n-            int32_t n_spills = 0;\n-            // create driver handles\n-            CUDA_CHECK(cuModuleLoadData(&mod, data));\n-            CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n-            // get allocated registers and spilled registers from the function\n-            CUDA_CHECK(cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS, fun));\n-            CUDA_CHECK(cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun));\n-            n_spills /= 4;\n-            // set dynamic shared memory if necessary\n-            int shared_optin;\n-            CUDA_CHECK(cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device));\n-            if (shared > 49152 && shared_optin > 49152) {\n-              CUDA_CHECK(cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED));\n-              int shared_total, shared_static;\n-              CUDA_CHECK(cuDeviceGetAttribute(&shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, device));\n-              CUDA_CHECK(cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n-              CUDA_CHECK(cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static));\n-            }\n-\n-            if(PyErr_Occurred()) {\n-              return NULL;\n-            }\n-            return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs, n_spills);\n-        }\n-\n-        static PyMethodDef ModuleMethods[] = {\n-          {\"load_binary\", loadBinary, METH_VARARGS, \"Load provided cubin into CUDA driver\"},\n-          {\"get_device_properties\", getDeviceProperties, METH_VARARGS, \"Get the properties for a given device\"},\n-          {NULL, NULL, 0, NULL} // sentinel\n-        };\n-\n-        static struct PyModuleDef ModuleDef = {\n-          PyModuleDef_HEAD_INIT,\n-          \\\"cuda_utils\\\",\n-          NULL, //documentation\n-          -1, //size\n-          ModuleMethods\n-        };\n-\n-        PyMODINIT_FUNC PyInit_cuda_utils(void) {\n-          PyObject *m = PyModule_Create(&ModuleDef);\n-          if(m == NULL) {\n-            return NULL;\n-          }\n-          PyModule_AddFunctions(m, ModuleMethods);\n-          return m;\n-        }\n-        \"\"\"\n-\n-    def __init__(self):\n-        src = self._generate_src()\n-        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n-        cache = get_cache_manager(key)\n-        fname = \"cuda_utils.so\"\n-        cache_path = cache.get_file(fname)\n-        if cache_path is None:\n-            with tempfile.TemporaryDirectory() as tmpdir:\n-                src_path = os.path.join(tmpdir, \"main.c\")\n-                with open(src_path, \"w\") as f:\n-                    f.write(src)\n-                so = _build(\"cuda_utils\", src_path, tmpdir)\n-                with open(so, \"rb\") as f:\n-                    cache_path = cache.put(f.read(), fname, binary=True)\n-        import importlib.util\n-        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache_path)\n-        mod = importlib.util.module_from_spec(spec)\n-        spec.loader.exec_module(mod)\n-        self.load_binary = mod.load_binary\n-        self.get_device_properties = mod.get_device_properties"}, {"filename": "python/triton/runtime/driver/hip.py", "status": "removed", "additions": 0, "deletions": 158, "changes": 158, "file_content_changes": "@@ -1,158 +0,0 @@\n-import hashlib\n-import os\n-import tempfile\n-\n-from ...common.build import _build\n-from ..cache import get_cache_manager\n-\n-\n-def get_hip_utils():\n-    global _hip_utils\n-    if _hip_utils is None:\n-        _hip_utils = HIPUtils()\n-    return _hip_utils\n-\n-\n-_hip_utils = None\n-\n-\n-class HIPUtils(object):\n-    def __new__(cls):\n-        if not hasattr(cls, 'instance'):\n-            cls.instance = super(HIPUtils, cls).__new__(cls)\n-        return cls.instance\n-\n-    def _generate_src(self):\n-        return \"\"\"\n-        #define __HIP_PLATFORM_AMD__\n-        #include <hip/hip_runtime.h>\n-        #define PY_SSIZE_T_CLEAN\n-        #include <Python.h>\n-        #include <stdio.h>\n-        #include <stdlib.h>\n-        static inline void gpuAssert(hipError_t code, const char *file, int line)\n-        {{\n-          if (code != HIP_SUCCESS)\n-          {{\n-             const char* prefix = \"Triton Error [HIP]: \";\n-             const char* str = hipGetErrorString(code);\n-             char err[1024] = {0};\n-             snprintf(err, 1024, \"%s Code: %d, Messsage: %s\", prefix, code, str );\n-             PyErr_SetString(PyExc_RuntimeError, err);\n-          }}\n-        }}\n-\n-        #define HIP_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); if (PyErr_Occurred()) return NULL; }\n-\n-        static PyObject* getDeviceProperties(PyObject* self, PyObject* args){\n-            int device_id;\n-            if (!PyArg_ParseTuple(args, \"i\", &device_id))\n-                return NULL;\n-\n-            hipDeviceProp_t props;\n-            HIP_CHECK(hipGetDeviceProperties(&props, device_id));\n-\n-            // create a struct to hold device properties\n-            return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\", props.sharedMemPerBlock,\n-                                       \"multiprocessor_count\", props.multiProcessorCount,\n-                                       \"sm_clock_rate\", props.clockRate,\n-                                       \"mem_clock_rate\", props.memoryClockRate,\n-                                       \"mem_bus_width\", props.memoryBusWidth);\n-        }\n-\n-        static PyObject* loadBinary(PyObject* self, PyObject* args) {\n-            const char* name;\n-            const char* data;\n-            Py_ssize_t data_size;\n-            int shared;\n-            int device;\n-            if (!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared, &device)) {\n-                return NULL;\n-            }\n-\n-            // Open HSACO file\n-            FILE* hsaco_file;\n-            if ((hsaco_file = fopen(data, \"rb\")) == NULL) {\n-                return NULL;\n-            }\n-\n-            // Read HSCAO file into Buffer\n-            fseek(hsaco_file, 0L, SEEK_END);\n-            size_t hsaco_file_size = ftell(hsaco_file);\n-            unsigned char* hsaco = (unsigned char*) malloc(hsaco_file_size * sizeof(unsigned char));\n-            rewind(hsaco_file);\n-            fread(hsaco, sizeof(unsigned char), hsaco_file_size, hsaco_file);\n-            fclose(hsaco_file);\n-\n-            // set HIP options\n-            hipJitOption opt[] = {hipJitOptionErrorLogBufferSizeBytes, hipJitOptionErrorLogBuffer,\n-                                  hipJitOptionInfoLogBufferSizeBytes, hipJitOptionInfoLogBuffer,\n-                                  hipJitOptionLogVerbose};\n-            const unsigned int errbufsize = 8192;\n-            const unsigned int logbufsize = 8192;\n-            char _err[errbufsize];\n-            char _log[logbufsize];\n-            void *optval[] = {(void *)(uintptr_t)errbufsize,\n-                              (void *)_err, (void *)(uintptr_t)logbufsize,\n-                              (void *)_log, (void *)1};\n-\n-            // launch HIP Binary\n-            hipModule_t mod;\n-            hipFunction_t fun;\n-            hipModuleLoadDataEx(&mod, hsaco, 5, opt, optval);\n-            hipModuleGetFunction(&fun, mod, name);\n-            free(hsaco);\n-\n-            // get allocated registers and spilled registers from the function\n-            int n_regs = 0;\n-            int n_spills = 0;\n-            if (PyErr_Occurred()) {\n-              return NULL;\n-            }\n-            return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs, n_spills);\n-        }\n-\n-        static PyMethodDef ModuleMethods[] = {\n-          {\"load_binary\", loadBinary, METH_VARARGS, \"Load provided hsaco into HIP driver\"},\n-          {\"get_device_properties\", getDeviceProperties, METH_VARARGS, \"Get the properties for a given device\"},\n-          {NULL, NULL, 0, NULL} // sentinel\n-        };\n-\n-        static struct PyModuleDef ModuleDef = {\n-          PyModuleDef_HEAD_INIT,\n-          \"hip_utils\",\n-          NULL, //documentation\n-          -1, //size\n-          ModuleMethods\n-        };\n-\n-        PyMODINIT_FUNC PyInit_hip_utils(void) {\n-          PyObject *m = PyModule_Create(&ModuleDef);\n-          if (m == NULL) {\n-            return NULL;\n-          }\n-          PyModule_AddFunctions(m, ModuleMethods);\n-          return m;\n-        }\n-        \"\"\"\n-\n-    def __init__(self):\n-        src = self._generate_src()\n-        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n-        cache = get_cache_manager(key)\n-        fname = \"hip_utils.so\"\n-        cache_path = cache.get_file(fname)\n-        if cache_path is None:\n-            with tempfile.TemporaryDirectory() as tmpdir:\n-                src_path = os.path.join(tmpdir, \"main.c\")\n-                with open(src_path, \"w\") as f:\n-                    f.write(src)\n-                so = _build(\"hip_utils\", src_path, tmpdir)\n-                with open(so, \"rb\") as f:\n-                    cache_path = cache.put(f.read(), fname, binary=True)\n-        import importlib.util\n-        spec = importlib.util.spec_from_file_location(\"hip_utils\", cache_path)\n-        mod = importlib.util.module_from_spec(spec)\n-        spec.loader.exec_module(mod)\n-        self.load_binary = mod.load_binary\n-        self.get_device_properties = mod.get_device_properties"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 12, "deletions": 10, "changes": 22, "file_content_changes": "@@ -5,7 +5,6 @@\n from contextlib import contextmanager\n \n import triton._C.libtriton.triton as _triton\n-from .runtime.driver.cuda import get_cuda_utils\n \n \n def nvsmi(attrs):\n@@ -275,28 +274,30 @@ def perf_report(benchmarks):\n def get_dram_gbps(backend=None, device=None):\n     ''' return DRAM bandwidth in GB/s '''\n     import torch\n+\n+    from .runtime import driver\n     if not backend:\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n-    cuda_utils = get_cuda_utils()\n-    mem_clock_khz = cuda_utils.get_device_properties(device)[\"mem_clock_rate\"]  # in kHz\n-    bus_width = cuda_utils.get_device_properties(device)[\"mem_bus_width\"]\n+    mem_clock_khz = driver.utils.get_device_properties(device)[\"mem_clock_rate\"]  # in kHz\n+    bus_width = driver.utils.get_device_properties(device)[\"mem_bus_width\"]\n     bw_gbps = mem_clock_khz * bus_width * 2 / 1e6 / 8  # In GB/s\n     return bw_gbps\n \n \n def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None):\n     import torch\n+\n+    from .runtime import driver\n     if not backend:\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n \n-    cuda_utils = get_cuda_utils()\n-    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n+    num_subcores = driver.utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n     if not clock_rate:\n-        clock_rate = cuda_utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n+        clock_rate = driver.utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n     capability = torch.cuda.get_device_capability(device)\n     if capability[0] < 8:\n         assert dtype == torch.float16\n@@ -390,14 +391,15 @@ def set_gpu_clock(ref_sm_clock=1350, ref_mem_clock=1215):\n \n def get_max_simd_tflops(dtype, backend=None, device=None):\n     import torch\n+\n+    from .runtime import driver\n     if not backend:\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n \n-    cuda_utils = get_cuda_utils()\n-    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n-    clock_rate = cuda_utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n+    num_subcores = driver.utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n+    clock_rate = driver.utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         if dtype == torch.float32:"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -288,12 +288,14 @@ def _output_stubs(self) -> str:\n         #   arg_type_symbol_dict = {[arg_type]: {(symbol, ret_type)}}\n         #   return extern.dispatch(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n         import_str = \"from . import core, extern\\n\"\n+        import_str += \"from ..runtime import driver\\n\"\n         import_str += \"import os\\n\"\n-        header_str = \"LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\", \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\\n\"\n-        header_str += \"LIBDEVICE_PATH = os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", LOCAL_PATH)\\n\"\n+\n+        header_str = \"\"\n+        header_str += \"LIBDEVICE_PATH = os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", driver.libdevice_path)\\n\"\n         func_str = \"\"\n         for symbols in self._symbol_groups.values():\n-            func_str += \"@language.extern\\n\"\n+            func_str += \"@extern.extern\\n\"\n             func_name_str = f\"def {symbols[0].op_name}(\"\n             for arg_name in symbols[0].arg_names:\n                 func_name_str += f\"{arg_name}, \""}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 10, "deletions": 3, "changes": 13, "file_content_changes": "@@ -157,7 +157,14 @@\n \n @triton.autotune(\n     configs=[\n-        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n+        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n+        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n     ],\n     key=['M', 'N', 'K'],\n )\n@@ -312,15 +319,15 @@ def matmul(a, b, activation=None):\n     triton.testing.Benchmark(\n         x_names=['M', 'N', 'K'],  # argument names to use as an x-axis for the plot\n         x_vals=[\n-            8192\n+            128 * i for i in range(2, 33)\n         ],  # different possible values for `x_name`\n         line_arg='provider',  # argument name whose value corresponds to a different line in the plot\n         # possible values for `line_arg``\n         line_vals=['cublas', 'triton'],\n         # label name for the lines\n         line_names=[\"cuBLAS\", \"Triton\"],\n         # line styles\n-        styles=[('green', '-'), ('green', '--'), ('blue', '-'), ('blue', '--')],\n+        styles=[('green', '-'), ('blue', '-')],\n         ylabel=\"TFLOPS\",  # label name for the y-axis\n         plot_name=\"matmul-performance\",  # name for the plot. Used also as a file name for saving the plot.\n         args={},"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -33,7 +33,7 @@ def _fwd_kernel(\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n     off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n+    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n     off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n@@ -50,7 +50,7 @@ def _fwd_kernel(\n         # -- compute qk ----\n         k = tl.load(k_ptrs)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, tl.trans(k))\n+        qk += tl.dot(q, k)\n         qk *= sm_scale\n         qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n         # compute new m\n@@ -62,10 +62,10 @@ def _fwd_kernel(\n         l_curr = tl.sum(p, 1) + l_prev\n         # rescale operands of matmuls\n         l_rcp = 1. / l_curr\n-        p *= l_rcp\n+        p *= l_rcp[:, None]\n         acc *= (l_prev * l_rcp)[:, None]\n         # update acc\n-        p = p.to(tl.float16)\n+        p = p.to(Q.dtype.element_ty)\n         v = tl.load(v_ptrs)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n@@ -169,18 +169,18 @@ def _bwd_kernel(\n             p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n-            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n+            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n             # compute dp = dot(v, do)\n             Di = tl.load(D_ptrs + offs_m_curr)\n             dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n             dp += tl.dot(do, tl.trans(v))\n             # compute ds = p * (dp - delta[:, None])\n             ds = p * dp * sm_scale\n             # compute dk = dot(ds.T, q)\n-            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n             # compute dq\n             dq = tl.load(dq_ptrs)\n-            dq += tl.dot(ds.to(tl.float16), k)\n+            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n             tl.store(dq_ptrs, dq)\n             # increment pointers\n             dq_ptrs += BLOCK_M * stride_qm"}]