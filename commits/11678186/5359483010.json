[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 109, "deletions": 2, "changes": 111, "file_content_changes": "@@ -25,7 +25,7 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"H100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"H100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n           else\n             echo '::set-output name=matrix::[\"ubuntu-latest\"]'\n           fi\n@@ -140,7 +140,7 @@ jobs:\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n         uses: actions/upload-artifact@v2\n         with:\n-          name: artifacts\n+          name: artifacts ${{ matrix.runner[1] }}\n           path: ~/.triton/artifacts.tar.gz\n \n       - name: Run CXX unittests\n@@ -173,3 +173,110 @@ jobs:\n           sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n           python3 -m pytest -vs .\n           sudo nvidia-smi -i 0 -rgc\n+\n+  Compare-artifacts:\n+    needs: Integration-Tests\n+\n+    runs-on: ubuntu-latest\n+\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+\n+      - name: Install gh CLI\n+        run: |\n+          sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key 23F3D4EA75716059\n+          echo \"deb [arch=$(dpkg --print-architecture)] https://cli.github.com/packages focal main\" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null\n+          sudo apt update\n+          sudo apt install gh\n+\n+      - name: Download latest main artifacts\n+        env:\n+          ARTIFACT_NAME: artifacts A100\n+          ARTIFACT_JOB_NAME: Integration-Tests\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+        run: |\n+          OWNER_REPO=\"${{ github.repository }}\"\n+          PR_NUMBER=$(gh api --method GET repos/$OWNER_REPO/pulls -f state=closed | jq \".[] | select(.merged_at != null) | .number\" | head -1)\n+          echo \"Last merged PR number: $PR_NUMBER\"\n+\n+          BRANCH_NAME=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.head.ref')\n+          echo \"BRANCH_NAME: $BRANCH_NAME\"\n+          WORKFLOW_RUN_ID=$(gh api --method GET repos/$OWNER_REPO/actions/runs | jq --arg branch_name \"$BRANCH_NAME\" '.workflow_runs[] | select(.head_branch == $branch_name)' | jq '.id' | head -1)\n+          echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n+          ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n+          echo \"ARTIFACT_URL: $ARTIFACT_URL\"\n+\n+          if [ -n \"$ARTIFACT_URL\" ]; then\n+            echo \"Downloading artifact: $ARTIFACT_URL\"\n+            curl --location --remote-header-name -H \"Authorization: token $GH_TOKEN\" -o reference.zip \"$ARTIFACT_URL\"\n+            # Print the size of the downloaded artifact\n+            echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" reference.zip)\"\n+            echo \"Artifact size (du): $(du -sh reference.zip)\"\n+            unzip reference.zip\n+            tar -xzf artifacts.tar.gz\n+            rm reference.zip\n+            rm artifacts.tar.gz\n+            mv cache reference\n+          else\n+            echo \"No artifact found with the name: $ARTIFACT_NAME\"\n+          fi\n+      - name: Download current job artifacts\n+        uses: actions/download-artifact@v2\n+        with:\n+          name: artifacts A100\n+      - name: Unzip current job artifacts\n+        run: |\n+          # Print the size of the downloaded artifact\n+          echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" artifacts.tar.gz)\"\n+          echo \"Artifact size (du): $(du -sh artifacts.tar.gz)\"\n+          tar -xzf artifacts.tar.gz\n+          rm artifacts.tar.gz\n+          mv cache current\n+      - name: Compare artifacts\n+        run: |\n+          set +e\n+          python3 python/test/tools/compare_files.py --path1 reference --path2 current --kernels python/test/kernel_comparison/kernels.yml\n+          exit_code=$?\n+          set -e\n+          echo $exit_code\n+          if [ $exit_code -eq 0 ]; then\n+            echo \"Artifacts are identical\"\n+            echo \"COMPARISON_RESULT=true\" >> $GITHUB_ENV\n+          elif [ $exit_code -eq 1 ]; then\n+            echo \"Artifacts are different\"\n+            echo \"COMPARISON_RESULT=false\" >> $GITHUB_ENV\n+          else\n+            echo \"Error while comparing artifacts\"\n+            echo \"COMPARISON_RESULT=error\" >> $GITHUB_ENV\n+          fi\n+          echo \"COMPARISON_RESULT=env.COMPARISON_RESULT\"\n+      - name: Check exit code and handle failure\n+        if: ${{ env.COMPARISON_RESULT == 'error' }}\n+        run: |\n+          echo \"Error while comparing artifacts\"\n+          exit 1\n+      - name: Fetch Run ID\n+        id: get_run_id\n+        run: echo \"RUN_ID=${{ github.run_id }}\" >> $GITHUB_ENV\n+\n+      - name: Upload results as artifact\n+        uses: actions/upload-artifact@v2\n+        with:\n+          name: kernels-reference-check\n+          path: kernels_reference_check.txt\n+\n+      - name: Check output and comment on PR\n+        if: ${{ env.COMPARISON_RESULT == 'false' }}\n+        uses: actions/github-script@v5\n+        with:\n+          script: |\n+            const run_id = ${{ env.RUN_ID }};\n+            const issue_number = context.payload.pull_request.number;\n+            const message = `:warning: **This PR does not produce bitwise identical kernels as the branch it's merged against.** Please check artifacts for details. [Download the output file here](https://github.com/${{ github.repository }}/actions/runs/${run_id}).`;\n+            await github.rest.issues.createComment({\n+                owner: context.repo.owner,\n+                repo: context.repo.repo,\n+                issue_number: issue_number,\n+                body: message\n+            });"}, {"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 42, "deletions": 17, "changes": 59, "file_content_changes": "@@ -2,7 +2,7 @@ name: Wheels\n on:\n   workflow_dispatch:\n   schedule:\n-    - cron: \"0 2 * * *\"\n+    - cron: \"20 2 * * *\"\n \n jobs:\n \n@@ -18,25 +18,20 @@ jobs:\n       - name: Checkout\n         uses: actions/checkout@v3\n \n-      - name: Install Azure CLI\n+      # The LATEST_DATE here should be kept in sync with the one in Patch setup.py\n+      - id: check-version\n+        name: Check latest version\n         run: |\n-          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n-\n-      - name: Azure login\n-        uses: azure/login@v1\n-        with:\n-          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n-          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n-          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n-\n-      - id: generate-token\n-        name: Generate token\n-        run: |\n-          AZ_TOKEN=$(az account get-access-token --query accessToken)\n-          echo \"::add-mask::$AZ_TOKEN\"\n-          echo \"access_token=$AZ_TOKEN\" >> \"$GITHUB_OUTPUT\"\n+          export PACKAGE_DATE=$(python3 -m pip install --user --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ --dry-run triton-nightly== |& grep -oP '(?<=, )[0-9\\.]+dev[0-9]+(?=\\))' | grep -oP '(?<=dev)[0-9]+')\n+          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d%H%M%S' --format=\"%cd\")\n+          if cmp -s <(echo $PACKAGE_DATE) <(echo $LATEST_DATE); then\n+            echo \"new_commit=false\" >> \"$GITHUB_OUTPUT\"\n+          else\n+            echo \"new_commit=true\" >> \"$GITHUB_OUTPUT\"\n+          fi\n \n       - name: Patch setup.py\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n         run: |\n           sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n           export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d%H%M%S' --format=\"%cd\")\n@@ -46,6 +41,7 @@ jobs:\n           echo \"base-dir=/project\" >> python/setup.cfg\n \n       - name: Build wheels\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n         run: |\n           export CIBW_MANYLINUX_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n           #export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n@@ -54,6 +50,35 @@ jobs:\n           export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n           python3 -m cibuildwheel python --output-dir wheelhouse\n \n+      - name: Install Azure CLI\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n+        run: |\n+          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n+\n+      - name: Azure login\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n+        uses: azure/login@v1\n+        with:\n+          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n+          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n+          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n+\n+      - id: generate-token\n+        name: Generate token\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n+        run: |\n+          AZ_TOKEN=$(az account get-access-token --query accessToken)\n+          echo \"::add-mask::$AZ_TOKEN\"\n+          echo \"access_token=$AZ_TOKEN\" >> \"$GITHUB_OUTPUT\"\n+\n       - name: Publish wheels to Azure DevOps\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n         run: |\n           python3 -m twine upload -r Triton-Nightly -u TritonArtifactsSP -p ${{ steps.generate-token.outputs.access_token }} --config-file utils/nightly.pypirc --non-interactive --verbose wheelhouse/*\n+\n+      - name: Azure Logout\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' && (success() || failure()) }}\n+        run: |\n+          az logout\n+          az cache purge\n+          az account clear"}, {"filename": "README.md", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -25,7 +25,7 @@ You can install the latest stable release of Triton from pip:\n ```bash\n pip install triton\n ```\n-Binary wheels are available for CPython 3.8-3.11 and PyPy 3.8-3.9.\n+Binary wheels are available for CPython 3.7-3.11 and PyPy 3.8-3.9.\n \n And the latest nightly release:\n "}, {"filename": "docs/getting-started/installation.rst", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -12,7 +12,7 @@ You can install the latest stable release of Triton from pip:\n \n       pip install triton\n \n-Binary wheels are available for CPython 3.8-3.11 and PyPy 3.8-3.9.\n+Binary wheels are available for CPython 3.7-3.11 and PyPy 3.8-3.9.\n \n And the latest nightly release:\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -3,13 +3,15 @@\n \n include \"mlir/IR/EnumAttr.td\"\n \n-// Attributes for LoadOp\n+// Attributes for LoadOp and StoreOp\n def TT_CacheModifierAttr : I32EnumAttr<\n     \"CacheModifier\", \"\",\n     [\n         I32EnumAttrCase<\"NONE\", 1, \"none\">,\n         I32EnumAttrCase<\"CA\", 2, \"ca\">,\n         I32EnumAttrCase<\"CG\", 3, \"cg\">,\n+        I32EnumAttrCase<\"WB\", 4, \"wb\">,\n+        I32EnumAttrCase<\"CS\", 5, \"cs\">,\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -397,6 +397,7 @@ def TT_DotOp : TT_Op<\"dot\", [Pure,\n     let results = (outs TT_FpIntTensor:$d);\n \n     let assemblyFormat = \"$a`,` $b`,` $c attr-dict `:` type($a) `*` type($b) `->` type($d)\";\n+    let hasVerifier = 1;\n }\n \n //"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 12, "deletions": 4, "changes": 16, "file_content_changes": "@@ -77,7 +77,7 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n     AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n                      \"ArrayRef<int64_t>\":$shape,\n                      \"ArrayRef<unsigned>\":$order,\n-                     \"Type\":$eltTy), [{\n+                     \"unsigned\":$typeWidthInBit), [{\n         auto mmaEnc = dotOpEnc.getParent().dyn_cast<MmaEncodingAttr>();\n \n         if(!mmaEnc)\n@@ -87,7 +87,7 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         int opIdx = dotOpEnc.getOpIdx();\n \n         // number of rows per phase\n-        int perPhase = 128 / (shape[order[0]] * (eltTy.getIntOrFloatBitWidth() / 8));\n+        int perPhase = 128 / (shape[order[0]] * (typeWidthInBit / 8));\n         perPhase = std::max<int>(perPhase, 1);\n \n         // index of the inner dimension in `order`\n@@ -109,9 +109,9 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         // ---- begin Ampere ----\n         if (mmaEnc.isAmpere()) {\n           std::vector<size_t> matShape = {8, 8,\n-                                          2 * 64 / eltTy.getIntOrFloatBitWidth()};\n+                                          2 * 64 / typeWidthInBit};\n           // for now, disable swizzle when using transposed int8 tensor cores\n-          if (eltTy.isInteger(8) && order[0] == inner)\n+          if (typeWidthInBit == 8 && order[0] == inner)\n             return $_get(context, 1, 1, 1, order);\n \n           // --- handle A operand ---\n@@ -135,6 +135,14 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n \n         // ---- not implemented ----\n         llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n+    }]>,\n+\n+    AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n+                     \"ArrayRef<int64_t>\":$shape,\n+                     \"ArrayRef<unsigned>\":$order,\n+                     \"Type\":$eltTy), [{\n+      unsigned bitwidth = eltTy.getIntOrFloatBitWidth();\n+      return get(context, dotOpEnc, shape, order, bitwidth);\n     }]>\n   ];\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -354,6 +354,9 @@ struct StoreOpConversion\n       auto &ptxStoreInstr =\n           ptxBuilder.create<>(\"st\")\n               ->global()\n+              .o(\"wb\", op.getCache() == triton::CacheModifier::WB)\n+              .o(\"cg\", op.getCache() == triton::CacheModifier::CG)\n+              .o(\"cs\", op.getCache() == triton::CacheModifier::CS)\n               .o(\"L1::evict_first\",\n                  op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)\n               .o(\"L1::evict_last\","}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n namespace mlir {\n namespace triton {\n@@ -398,6 +399,25 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n   return mlir::success();\n }\n \n+LogicalResult mlir::triton::DotOp::verify() {\n+  auto aTy = getOperand(0).getType().cast<RankedTensorType>();\n+  auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n+  if (aTy.getElementType() != bTy.getElementType())\n+    return emitError(\"element types of operands A and B must match\");\n+  auto aEncoding =\n+      aTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  auto bEncoding =\n+      bTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  if (!aEncoding && !bEncoding)\n+    return mlir::success();\n+  // Verify that the encodings are valid.\n+  if (!aEncoding || !bEncoding)\n+    return emitError(\"mismatching encoding between A and B operands\");\n+  if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n+    return emitError(\"mismatching kWidth between A and B operands\");\n+  return mlir::success();\n+}\n+\n //-- ReduceOp --\n static mlir::LogicalResult\n inferReduceReturnShape(const RankedTensorType &argTy, const Type &retEltTy,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 176, "deletions": 50, "changes": 226, "file_content_changes": "@@ -14,6 +14,7 @@ using triton::DotOp;\n using triton::gpu::ConvertLayoutOp;\n using triton::gpu::DotOperandEncodingAttr;\n using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SharedEncodingAttr;\n using triton::gpu::SliceEncodingAttr;\n \n // convert(trans(convert(arg)))\n@@ -78,16 +79,15 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n \n public:\n   MoveOpAfterLayoutConversion(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, context) {}\n \n   static mlir::LogicalResult\n   isBlockedToDotOperand(mlir::Operation *op,\n                         triton::gpu::DotOperandEncodingAttr &retEncoding,\n                         triton::gpu::BlockedEncodingAttr &srcEncoding) {\n-    if (!op)\n+    auto cvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(op);\n+    if (!cvt)\n       return failure();\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n     auto srcTy = cvt.getOperand().getType().cast<RankedTensorType>();\n     auto retTy = cvt.getResult().getType().dyn_cast<RankedTensorType>();\n     retEncoding =\n@@ -135,63 +135,185 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n-    triton::gpu::DotOperandEncodingAttr retEncoding;\n-    triton::gpu::BlockedEncodingAttr srcEncoding;\n-    if (isBlockedToDotOperand(op, retEncoding, srcEncoding).failed())\n-      return mlir::failure();\n-\n+    auto dotOp = cast<triton::DotOp>(op);\n     // only supports dot NT\n-    auto users = cvt->getUsers();\n-    auto dotOp = dyn_cast_or_null<DotOp>(*users.begin());\n-    if (!dotOp)\n-      return failure();\n     if (!isDotNT(dotOp))\n       return failure();\n+    bool changed = false;\n+    for (Value operand : {dotOp.getOperand(0), dotOp.getOperand(1)}) {\n+      auto cvt = operand.getDefiningOp<triton::gpu::ConvertLayoutOp>();\n+      triton::gpu::DotOperandEncodingAttr retEncoding;\n+      triton::gpu::BlockedEncodingAttr srcEncoding;\n+      bool failed =\n+          isBlockedToDotOperand(cvt, retEncoding, srcEncoding).failed();\n+      assert(!failed);\n \n-    // don't move things around when cvt operand is a block arg\n-    Operation *argOp = cvt.getOperand().getDefiningOp();\n-    if (!argOp)\n-      return failure();\n-    //\n-    SetVector<Operation *> processed;\n-    SetVector<Attribute> layout;\n-    llvm::MapVector<Value, Attribute> toConvert;\n-    int numCvts = simulateBackwardRematerialization(cvt, processed, layout,\n-                                                    toConvert, retEncoding);\n-    if (numCvts > 1 || toConvert.size() == 1)\n-      return failure();\n-    for (Operation *op : processed) {\n-      if (op->getNumOperands() != 1)\n+      // don't move things around when cvt operand is a block arg\n+      Operation *argOp = cvt.getOperand().getDefiningOp();\n+      if (!argOp)\n         continue;\n-      auto srcTy = op->getOperand(0).getType().cast<RankedTensorType>();\n-      auto dstTy = op->getResult(0).getType().cast<RankedTensorType>();\n-      // we don't want to push conversions backward if there is a downcast\n-      // since it would result in more shared memory traffic\n-      if (srcTy.getElementType().getIntOrFloatBitWidth() >\n-          dstTy.getElementType().getIntOrFloatBitWidth())\n-        return failure();\n-      // we only push back when the first op in the chain has a load operand\n-      if ((op == processed.back()) &&\n-          !isa<triton::LoadOp>(op->getOperand(0).getDefiningOp()))\n-        return failure();\n-      // we don't want to use ldmatrix for 8-bit data that requires trans\n-      // since Nvidia GPUs can't do it efficiently\n-      int kOrder = retEncoding.getOpIdx() ^ 1;\n-      bool isTrans = kOrder != srcEncoding.getOrder()[0];\n-      bool isInt8 = srcTy.getElementType().getIntOrFloatBitWidth() == 8;\n-      if (isTrans && isInt8)\n-        return failure();\n+      SetVector<Operation *> processed;\n+      SetVector<Attribute> layout;\n+      llvm::MapVector<Value, Attribute> toConvert;\n+      int numCvts = simulateBackwardRematerialization(cvt, processed, layout,\n+                                                      toConvert, retEncoding);\n+      if (numCvts > 1 || toConvert.size() == 1)\n+        continue;\n+      bool replaceOperand = true;\n+      for (Operation *op : processed) {\n+        if (op->getNumOperands() != 1)\n+          continue;\n+        auto srcTy = op->getOperand(0).getType().cast<RankedTensorType>();\n+        auto dstTy = op->getResult(0).getType().cast<RankedTensorType>();\n+        // we don't want to push conversions backward if there is a downcast\n+        // since it would result in more shared memory traffic\n+        if (srcTy.getElementType().getIntOrFloatBitWidth() >\n+            dstTy.getElementType().getIntOrFloatBitWidth()) {\n+          replaceOperand = false;\n+          break;\n+        }\n+        // we only push back when the first op in the chain has a load operand\n+        if ((op == processed.back()) &&\n+            !isa<triton::LoadOp>(op->getOperand(0).getDefiningOp())) {\n+          replaceOperand = false;\n+          break;\n+        }\n+        // we don't want to use ldmatrix for 8-bit data that requires trans\n+        // since Nvidia GPUs can't do it efficiently\n+        int kOrder = retEncoding.getOpIdx() ^ 1;\n+        bool isTrans = kOrder != srcEncoding.getOrder()[0];\n+        bool isInt8 = srcTy.getElementType().getIntOrFloatBitWidth() == 8;\n+        if (isTrans && isInt8) {\n+          replaceOperand = false;\n+          break;\n+        }\n+      }\n+      if (!replaceOperand)\n+        continue;\n+      IRMapping mapping;\n+      rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n+      rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n+      changed = true;\n     }\n-    IRMapping mapping;\n-    rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n-    rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n-    return mlir::success();\n+    return mlir::success(changed);\n   }\n };\n \n } // namespace\n \n+static bool isConvertToDotEncoding(Operation *op) {\n+  auto convertLayout = llvm::dyn_cast<ConvertLayoutOp>(op);\n+  if (!convertLayout)\n+    return false;\n+  auto tensorType =\n+      convertLayout.getResult().getType().cast<RankedTensorType>();\n+  return tensorType.getEncoding().isa<DotOperandEncodingAttr>();\n+}\n+\n+static ConvertLayoutOp updateConvert(OpBuilder &builder, ConvertLayoutOp cvt,\n+                                     IRMapping &mapping, Type smallestType) {\n+  auto cvtDstTy = cvt.getResult().getType().cast<RankedTensorType>();\n+  auto cvtDstEnc = cvtDstTy.getEncoding().cast<DotOperandEncodingAttr>();\n+  Value operand = cvt.getOperand();\n+  if (mapping.contains(operand))\n+    operand = mapping.lookup(operand);\n+  auto newDstTy = RankedTensorType::get(\n+      cvtDstTy.getShape(), cvtDstTy.getElementType(),\n+      DotOperandEncodingAttr::get(cvtDstEnc.getContext(), cvtDstEnc.getOpIdx(),\n+                                  cvtDstEnc.getParent(), smallestType));\n+  auto newCvt =\n+      builder.create<ConvertLayoutOp>(cvt.getLoc(), newDstTy, operand);\n+  mapping.map(cvt.getResult(), newCvt.getResult());\n+  return newCvt;\n+}\n+\n+// Update kWidth based on the smallestType found in the given convert ops and\n+// propagate the type change.\n+static void\n+updateDotEncodingLayout(SmallVector<ConvertLayoutOp> &convertsToDotEncoding,\n+                        Type smallestType) {\n+  IRMapping mapping;\n+  OpBuilder builder(smallestType.getContext());\n+  SetVector<Operation *> slices(convertsToDotEncoding.begin(),\n+                                convertsToDotEncoding.end());\n+  // Collect all the operations where the type needs to be propagated.\n+  for (auto cvt : convertsToDotEncoding) {\n+    auto filter = [&](Operation *op) {\n+      for (Value operand : op->getOperands()) {\n+        auto tensorType = operand.getType().dyn_cast<RankedTensorType>();\n+        if (tensorType &&\n+            tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n+          return true;\n+      }\n+      return false;\n+    };\n+    mlir::getForwardSlice(cvt.getResult(), &slices, {filter});\n+  }\n+  // Apply the type change by walking ops in topological order.\n+  slices = mlir::topologicalSort(slices);\n+  for (Operation *op : slices) {\n+    builder.setInsertionPoint(op);\n+    if (isConvertToDotEncoding(op)) {\n+      auto cvt = cast<ConvertLayoutOp>(op);\n+      ConvertLayoutOp newCvt =\n+          updateConvert(builder, cvt, mapping, smallestType);\n+      continue;\n+    }\n+    auto *newOp = cloneWithInferType(builder, op, mapping);\n+    for (auto [result, newResult] :\n+         llvm::zip(op->getResults(), newOp->getResults())) {\n+      result.replaceUsesWithIf(newResult, [&](OpOperand &operand) {\n+        return slices.count(operand.getOwner()) == 0;\n+      });\n+    }\n+  }\n+  for (Operation *op : llvm::reverse(slices))\n+    op->erase();\n+}\n+\n+// Change the layout of dotOperand layout to use the kWidth from the smallest\n+// loaded type. This allows better code generation for mixed-mode matmul.\n+static void optimizeKWidth(triton::FuncOp func) {\n+  SmallVector<ConvertLayoutOp> convertsToDotEncoding;\n+  Type smallestType;\n+  func->walk([&](triton::LoadOp loadOp) {\n+    if (!loadOp.getResult().hasOneUse())\n+      return;\n+    Operation *use = *loadOp.getResult().getUsers().begin();\n+\n+    // Advance to the first conversion as long as the use resides in shared\n+    // memory and it has a single use itself\n+    while (use) {\n+      if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n+        break;\n+      auto tensorType =\n+          use->getResult(0).getType().dyn_cast<RankedTensorType>();\n+      if (!tensorType || !tensorType.getEncoding().isa<SharedEncodingAttr>())\n+        break;\n+      use = *use->getResult(0).getUsers().begin();\n+    }\n+\n+    auto convertLayout = llvm::dyn_cast<ConvertLayoutOp>(use);\n+    if (!convertLayout)\n+      return;\n+    auto tensorType =\n+        convertLayout.getResult().getType().cast<RankedTensorType>();\n+    if (!tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n+      return;\n+    convertsToDotEncoding.push_back(convertLayout);\n+\n+    // Update the smallest type.\n+    auto ty = loadOp.getType().cast<RankedTensorType>();\n+    Type eltTy = ty.getElementType();\n+    if (!smallestType ||\n+        (eltTy.getIntOrFloatBitWidth() < smallestType.getIntOrFloatBitWidth()))\n+      smallestType = eltTy;\n+  });\n+  if (!smallestType)\n+    return;\n+  updateDotEncodingLayout(convertsToDotEncoding, smallestType);\n+}\n+\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n@@ -216,6 +338,10 @@ class TritonGPUOptimizeDotOperandsPass\n       signalPassFailure();\n     if (fixupLoops(m).failed())\n       signalPassFailure();\n+\n+    // Change the layout of dotOperand layout to use the kWidth from the\n+    // smallest loaded type.\n+    m->walk([](triton::FuncOp func) { optimizeKWidth(func); });\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 8, "deletions": 30, "changes": 38, "file_content_changes": "@@ -111,9 +111,6 @@ class LoopPipeliner {\n \n   /// Loads to be pipelined\n   SetVector<Value> validLoads;\n-  /// Smallest data-type for each load (used to optimize swizzle and\n-  /// (create DotOpEncoding layout)\n-  DenseMap<Value, Type> loadsSmallestType;\n   /// The value that each load will be mapped to (after layout conversion)\n   DenseMap<Value, Value> loadsMapping;\n   /// load => buffer\n@@ -485,21 +482,6 @@ Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n }\n \n void LoopPipeliner::createBufferTypes() {\n-  // We need to find the smallest common dtype since this determines the layout\n-  // of `mma.sync` operands in mixed-precision mode\n-  Type smallestType;\n-  for (auto loadCvt : loadsMapping) {\n-    auto loadOp = loadCvt.first;\n-    auto ty = loadOp.getType().cast<RankedTensorType>();\n-    Type eltTy = ty.getElementType();\n-    if (!smallestType ||\n-        (eltTy.getIntOrFloatBitWidth() < smallestType.getIntOrFloatBitWidth()))\n-      smallestType = eltTy;\n-  }\n-\n-  for (auto loadCvt : loadsMapping)\n-    loadsSmallestType[loadCvt.first] = smallestType;\n-\n   for (auto loadCvt : loadsMapping) {\n     auto loadOp = loadCvt.first;\n     Value cvt = loadCvt.second;\n@@ -511,9 +493,12 @@ void LoopPipeliner::createBufferTypes() {\n     SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n                                      ty.getShape().end());\n     bufferShape.insert(bufferShape.begin(), numStages);\n-    auto sharedEnc = ttg::SharedEncodingAttr::get(\n-        ty.getContext(), dotOpEnc, ty.getShape(),\n-        ttg::getOrder(ty.getEncoding()), loadsSmallestType[loadOp]);\n+    unsigned bitWidth = dotOpEnc.getMMAv2kWidth()\n+                            ? 32 / dotOpEnc.getMMAv2kWidth()\n+                            : ty.getElementType().getIntOrFloatBitWidth();\n+    auto sharedEnc =\n+        ttg::SharedEncodingAttr::get(ty.getContext(), dotOpEnc, ty.getShape(),\n+                                     ttg::getOrder(ty.getEncoding()), bitWidth);\n     loadsBufferType[loadOp] =\n         RankedTensorType::get(bufferShape, ty.getElementType(), sharedEnc);\n   }\n@@ -789,19 +774,12 @@ scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n     // we replace the use new load use with a convert layout\n     size_t i = std::distance(validLoads.begin(), it);\n     auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n-    auto cvtDstEnc =\n-        cvtDstTy.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n-    if (!cvtDstEnc) {\n+    if (!cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n       builder.clone(op, mapping);\n       continue;\n     }\n-    auto newDstTy = RankedTensorType::get(\n-        cvtDstTy.getShape(), cvtDstTy.getElementType(),\n-        ttg::DotOperandEncodingAttr::get(\n-            cvtDstEnc.getContext(), cvtDstEnc.getOpIdx(), cvtDstEnc.getParent(),\n-            loadsSmallestType[op.getOperand(0)]));\n     auto cvt = builder.create<ttg::ConvertLayoutOp>(\n-        op.getResult(0).getLoc(), newDstTy,\n+        op.getResult(0).getLoc(), cvtDstTy,\n         newForOp.getRegionIterArgs()[loadIdx + i]);\n     mapping.map(op.getResult(0), cvt.getResult());\n   }"}, {"filename": "python/setup.py", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "file_content_changes": "@@ -13,6 +13,7 @@\n \n from setuptools import Extension, setup\n from setuptools.command.build_ext import build_ext\n+from setuptools.command.build_py import build_py\n \n \n # Taken from https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/env.py\n@@ -146,6 +147,11 @@ def download_and_copy_ptxas():\n \n # ---- cmake extension ----\n \n+class CMakeBuildPy(build_py):\n+    def run(self) -> None:\n+        self.run_command('build_ext')\n+        return super().run()\n+\n \n class CMakeExtension(Extension):\n     def __init__(self, name, path, sourcedir=\"\"):\n@@ -280,7 +286,7 @@ def build_extension(self, ext):\n     ],\n     include_package_data=True,\n     ext_modules=[CMakeExtension(\"triton\", \"triton/_C/\")],\n-    cmdclass={\"build_ext\": CMakeBuild},\n+    cmdclass={\"build_ext\": CMakeBuild, \"build_py\": CMakeBuildPy},\n     zip_safe=False,\n     # for PyPI\n     keywords=[\"Compiler\", \"Deep Learning\"],"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -89,6 +89,8 @@ void init_triton_ir(py::module &&m) {\n       .value(\"NONE\", mlir::triton::CacheModifier::NONE)\n       .value(\"CA\", mlir::triton::CacheModifier::CA)\n       .value(\"CG\", mlir::triton::CacheModifier::CG)\n+      .value(\"WB\", mlir::triton::CacheModifier::WB)\n+      .value(\"CS\", mlir::triton::CacheModifier::CS)\n       .export_values();\n \n   py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\")"}, {"filename": "python/test/kernel_comparison/kernels.yml", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -0,0 +1,31 @@\n+name_and_extension:\n+  - name: _kernel_0d1d2d34567c89c1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6d7c8d9c10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6d7c8c9d10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3456c789c1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6d7c8c9d1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d34567c8c91011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3456c78c91011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6c78c9d1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6c789c1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6c7d8d9c10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6c7d8c9d10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6d7c89c1011c\n+    extension: ptx\n+  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15c16d17d18d19c20d21d22d23c2425d26d27\n+    extension: ptx\n+  - name: _fwd_kernel_0d1d2d34d5d6d7d8d9d10c11d12d13d14c15d16d17d18c19d20d21d22c2324d25d\n+    extension: ptx\n+  - name: _bwd_preprocess_0d1d2d3d4d\n+    extension: ptx"}, {"filename": "python/test/tools/compare_files.py", "status": "added", "additions": 261, "deletions": 0, "changes": 261, "file_content_changes": "@@ -0,0 +1,261 @@\n+import argparse\n+import difflib\n+import glob\n+import os\n+import sys\n+from typing import Dict, List, Optional, Tuple\n+\n+import yaml\n+\n+\n+class ComparisonResult:\n+    def __init__(self, name: str, extension: str, numComparisons: int, diffs: List[str] = None, errors: List[str] = None):\n+        self.name = name\n+        self.extension = extension\n+        self.numComparisons = numComparisons\n+        self.diffs = [] if diffs is None else diffs\n+        self.errors = [] if errors is None else errors\n+\n+    def isSuccess(self) -> bool:\n+        return len(self.diffs) == 0 and len(self.errors) == 0\n+\n+    def __str__(self) -> str:\n+        return f\"name={self.name}, extension={self.extension}, numComparisons={self.numComparisons}, success={self.isSuccess()}\"\n+\n+\n+def listFilesWithExtension(path: str, extension: str) -> List[str]:\n+    \"\"\"\n+        Returns a list of files in the given path with the given extension\n+        The files are returned with their full path\n+    \"\"\"\n+    files = glob.glob(os.path.join(path, f'*.{extension}'))\n+    return files\n+\n+\n+def getFileWithExtension(path: str, ext: str) -> Optional[str]:\n+    \"\"\"\n+        Returns a single file in the given path with the given extension\n+    \"\"\"\n+    # get all files in directory with extension\n+    files = listFilesWithExtension(path, ext)\n+    if len(files) == 0:\n+        return None\n+    # filter out files with grp in their name\n+    files = [f for f in files if \"__grp__\" not in f]\n+    if len(files) != 1:\n+        print(f\"Found {len(files)} files in {path} with extension {ext}!\")\n+        sys.exit(2)\n+    return files[0]\n+\n+\n+def loadYamlFile(filePath: str) -> List[Dict[str, str]]:\n+    \"\"\"\n+        Loads a yaml file and returns its content as a list of dictionaries\n+    \"\"\"\n+    with open(filePath, 'r') as file:\n+        content = yaml.safe_load(file)\n+    return content\n+\n+\n+def compareFiles(file1: str, file2: str) -> bool:\n+    \"\"\"\n+        Compares two files and returns True if they are the same, False otherwise\n+    \"\"\"\n+    with open(file1, 'rb') as f1, open(file2, 'rb') as f2:\n+        content1 = f1.read()\n+        content2 = f2.read()\n+\n+    return content1 == content2\n+\n+\n+def diffFiles(file1, file2):\n+    with open(file1, 'r') as f1:\n+        file1_lines = f1.readlines()\n+    with open(file2, 'r') as f2:\n+        file2_lines = f2.readlines()\n+\n+    diff = list(difflib.unified_diff(file1_lines, file2_lines, file1, file2))\n+    return diff\n+\n+\n+def getFileVec(path: str) -> List[Tuple[str, str]]:\n+    \"\"\"\n+        Returns a list of tuples (extension, file) for the given path (note: the path includes the hash)\n+        The returned list must have extensions (json, ttir, ttgir)\n+        in this particular order, unless a file with a certain extension does not exist\n+    \"\"\"\n+    vec = []\n+    for ext in [\"json\", \"ttir\", \"ttgir\"]:\n+        file = getFileWithExtension(path, ext)\n+        if file is not None:\n+            vec.append((ext, file))\n+    return vec\n+\n+\n+def getNameToHashesDict(path: str) -> Dict[str, List[str]]:\n+    \"\"\"\n+        Returns a dictionary that maps kernel names to a list of hashes that have the same kernel name\n+        in the given path\n+        Note: the hashes must have a json file and either a ttir or ttgir file, otherwise they are ignored\n+    \"\"\"\n+    nameToHashes = {}\n+    for hash in os.listdir(path):\n+        fullPath = os.path.join(path, hash)\n+        if not os.path.isdir(fullPath):\n+            print(f\"Path {fullPath} is not a directory!\")\n+            sys.exit(2)\n+        fileVec = getFileVec(fullPath)\n+        if len(fileVec) < 2 or fileVec[0][0] != \"json\":\n+            continue\n+        jsonFile = fileVec[0][1]\n+        # load json file\n+        with open(jsonFile, 'r') as file:\n+            content = yaml.safe_load(file)\n+            # get name\n+            name = content[\"name\"]\n+            nameToHashes.setdefault(name, []).append(hash)\n+    return nameToHashes\n+\n+\n+def doFilesMatch(path1: str, path2: str) -> bool:\n+    \"\"\"\n+        Returns True if the files in the given paths match, False otherwise\n+        The files are considered to match if:\n+        1. The number of files in both paths match\n+        2. The json files match\n+        3. Both paths have a ttir that match, if a ttir does not exist, the ttgir file must exist and match\n+    \"\"\"\n+    filesVec1 = getFileVec(path1)\n+    filesVec2 = getFileVec(path2)\n+    # The number of files must match\n+    if len(filesVec1) != len(filesVec2):\n+        return False\n+\n+    for (ext1, file1), (ext2, file2) in zip(filesVec1, filesVec2):\n+        if ext1 != ext2:\n+            return False\n+        if not compareFiles(file1, file2):\n+            return False\n+        else:\n+            # once we actually compared a ttir or ttgir file, we can break\n+            if ext1 in (\"ttir\", \"ttgir\"):\n+                break\n+    return True\n+\n+\n+def compareMatchingFiles(name: str, extension: str, nameToHashes1: Dict[str, List[str]], nameToHashes2: Dict[str, List[str]], args) -> ComparisonResult:\n+    \"\"\"\n+        Compare files with the given name/extension in all hashes in both paths\n+        Return the first mismatching files as a tuple (file1, file2), otherwise, return an empty tuple\n+    \"\"\"\n+    hashes1 = nameToHashes1.get(name, [])\n+    hashes2 = nameToHashes2.get(name, [])\n+    diffs = []\n+    errors = []\n+    numComparisons = 0\n+    for hash1 in hashes1:\n+        path1 = os.path.join(args.path1, hash1)\n+        for hash2 in hashes2:\n+            path2 = os.path.join(args.path2, hash2)\n+            # check whether both paths have:\n+            # 1. json files that match\n+            # 2. ttir files that match (if they exist), otherwise ttgir files that match (if they exist)\n+            # if any of these contraints is not met, then we can skip this pair of hashes since they are not a match\n+            if not doFilesMatch(path1, path2):\n+                continue\n+            numComparisons += 1\n+            extFile1 = listFilesWithExtension(path1, extension)[0]\n+            extFile2 = listFilesWithExtension(path2, extension)[0]\n+            diff = diffFiles(extFile1, extFile2)\n+            if len(diff) > 0:\n+                diffs.append(diffFiles(extFile2, extFile1))\n+    if numComparisons == 0:\n+        errors.append(f\"Did not find any matching files for {name}\")\n+    return ComparisonResult(name=name, extension=extension, numComparisons=numComparisons, diffs=diffs, errors=errors)\n+\n+\n+def dumpResults(results: List[ComparisonResult], fileName: str):\n+    \"\"\"\n+        Dumps the results to the given file\n+    \"\"\"\n+    with open(fileName, 'w') as file:\n+        for result in results:\n+            file.write(str(result) + \"\\n\")\n+            file.write(\"Diffs:\\n\")\n+            for diff in result.diffs:\n+                for line in diff:\n+                    file.write(line)\n+            file.write(\"Errors:\\n\")\n+            for error in result.errors:\n+                file.write(error)\n+            file.write(\"\\n\\n\")\n+\n+\n+def main(args) -> bool:\n+    \"\"\"\n+        Iterates over all kernels in the given yaml file and compares them\n+        in the given paths\n+    \"\"\"\n+    if args.path1 == args.path2:\n+        print(\"Cannot compare files in the same directory!\")\n+        sys.exit(2)\n+    # Get kernel name to hashes dict, these hashes would have the same kernel name\n+    nameToHashes1 = getNameToHashesDict(args.path1)\n+    nameToHashes2 = getNameToHashesDict(args.path2)\n+\n+    yamlFilePath = args.kernels\n+    if not os.path.exists(yamlFilePath):\n+        print(f\"Path {yamlFilePath} does not exist!\")\n+        sys.exit(2)\n+    nameAndExtension = loadYamlFile(yamlFilePath)[\"name_and_extension\"]\n+\n+    results = []\n+    # iterate over the kernels that need to be checked\n+    for d in nameAndExtension:\n+        name = d[\"name\"]  # kernel name\n+        extension = d[\"extension\"]  # extension of the file to be compared (e.g. ptx)\n+        # Compare all hashes on path 1 with all hashes on path 2\n+        # result is either the mismatching (file1, file2) with \"extension\" or empty tuple if no mismatch\n+        result = compareMatchingFiles(name, extension, nameToHashes1, nameToHashes2, args)\n+        print(result)\n+        # Otherwise, add it to the mismatches\n+        results.append(result)\n+\n+    # Dump results\n+    dumpResults(results, \"kernels_reference_check.txt\")\n+\n+    success = all(result.isSuccess() for result in results)\n+\n+    if not success:\n+        print(\"Failed!\")\n+        sys.exit(1)\n+\n+    print(\"Passed!\")\n+    sys.exit(0)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--path1\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=(\"Path to first cache directory\"),\n+    )\n+    parser.add_argument(\n+        \"--path2\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=(\"Path to second cache directory\"),\n+    )\n+    parser.add_argument(\n+        \"--kernels\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=(\"Path to kernels yaml file\"),\n+    )\n+    args = parser.parse_args()\n+    main(args)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 43, "deletions": 3, "changes": 46, "file_content_changes": "@@ -1370,7 +1370,8 @@ def get_reduced_dtype(dtype_str, op):\n                           for op in ['min', 'max',\n                                      'min-with-indices',\n                                      'max-with-indices',\n-                                     'argmin', 'argmax',\n+                                     'argmin-tie-break-left',\n+                                     'argmax-tie-break-left',\n                                      'sum']\n                           for dtype in dtypes_with_bfloat16\n                           for shape in [32, 64, 128, 512]])\n@@ -1386,18 +1387,26 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n     if 'with-indices' in op:\n         patch = f'z, _ = tl.{op.split(\"-\")[0]}(x, axis=0, return_indices=True)'\n+    elif 'arg' in op:\n+        tie_break_left = 'tie-break-left' in op\n+        patch = f'z = tl.{op.split(\"-\")[0]}(x, axis=0, tie_break_left={tie_break_left})'\n     else:\n         patch = f'z = tl.{op}(x, axis=0)'\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': patch})\n     # input\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n-    x_tri = to_triton(x, device=device)\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'max-with-indices': np.max,\n                 'min-with-indices': np.min,\n-                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+                'argmin-tie-break-fast': np.argmin,\n+                'argmin-tie-break-left': np.argmin,\n+                'argmax-tie-break-fast': np.argmax,\n+                'argmax-tie-break-left': np.argmax}[op]\n+    if 'tie-break-left' in op:\n+        x[3:10] = numpy_op(x)\n+    x_tri = to_triton(x, device=device)\n     # numpy result\n     z_dtype_str = 'int32' if op in ('argmin', 'argmax') else dtype_str\n     z_tri_dtype_str = z_dtype_str\n@@ -2257,6 +2266,37 @@ def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n # test store\n # ---------------\n \n+\n+@pytest.mark.parametrize(\"cache\", [\"\", \".wb\", \".cg\", \".cs\"])\n+def test_store_cache_modifier(cache):\n+    src = torch.empty(128, device='cuda')\n+    dst = torch.empty(128, device='cuda')\n+\n+    @triton.jit\n+    def _kernel(dst, src, CACHE: tl.constexpr):\n+        offsets = tl.arange(0, 128)\n+        x = tl.load(src + offsets)\n+        tl.store(dst + offsets, x, cache_modifier=CACHE)\n+\n+    pgm = _kernel[(1,)](dst, src, CACHE=cache)\n+    ptx = pgm.asm['ptx']\n+    if cache == '':\n+        assert 'st.global.wb' not in ptx\n+        assert 'st.global.cg' not in ptx\n+        assert 'st.global.cs' not in ptx\n+    if cache == '.wb':\n+        assert 'st.global.wb' in ptx\n+        assert 'st.global.cg' not in ptx\n+        assert 'st.global.cs' not in ptx\n+    if cache == '.cg':\n+        assert 'st.global.wb' not in ptx\n+        assert 'st.global.cg' in ptx\n+        assert 'st.global.cs' not in ptx\n+    if cache == '.cs':\n+        assert 'st.global.wb' not in ptx\n+        assert 'st.global.cg' not in ptx\n+        assert 'st.global.cs' in ptx\n+\n # ---------------\n # test if\n # ---------------"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -67,6 +67,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n+                (128, 256, 64, 1, 8, 3, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n@@ -131,6 +132,9 @@ def get_input(n, m, t, dtype):\n     th_c = torch.matmul(a, b)\n     try:\n         tt_c = triton.ops.matmul(a, b)\n-        torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n+        atol, rtol = 1e-2, 0\n+        if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:\n+            atol, rtol = 3.5e-2, 0\n+        torch.testing.assert_allclose(th_c, tt_c, atol=atol, rtol=rtol)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 66, "deletions": 32, "changes": 98, "file_content_changes": "@@ -1255,15 +1255,21 @@ def abs(x, _builder=None):\n # Reductions\n # -----------------------\n \n-def _add_reduction_docstr(name: str) -> Callable[[T], T]:\n+def _add_reduction_docstr(name: str, return_indices_arg: str = None, tie_break_arg: str = None) -> Callable[[T], T]:\n \n     def _decorator(func: T) -> T:\n         docstr = \"\"\"\n     Returns the {name} of all elements in the :code:`input` tensor along the provided :code:`axis`\n \n     :param input: the input values\n-    :param axis: the dimension along which the reduction should be done\n-    \"\"\"\n+    :param axis: the dimension along which the reduction should be done\"\"\"\n+        if return_indices_arg is not None:\n+            docstr += f\"\"\"\n+    :param {return_indices_arg}: if true, return index corresponding to the {name} value\"\"\"\n+        if tie_break_arg is not None:\n+            docstr += f\"\"\"\n+    :param {tie_break_arg}: if true, return the left-most indices in case of ties for values that aren't NaN\"\"\"\n+\n         func.__doc__ = docstr.format(name=name)\n         return func\n \n@@ -1374,65 +1380,93 @@ def maximum(x, y):\n \n \n @jit\n-def _max_combine(a, b):\n-    return maximum(a, b)\n+def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    gt = value1 > value2 or tie\n+    v_ret = where(gt, value1, value2)\n+    i_ret = where(gt, index1, index2)\n+    return v_ret, i_ret\n \n \n @jit\n-def _argmax_combine(value1, index1, value2, index2):\n-    gt = value1 > value2\n-    value_ret = where(gt, value1, value2)\n-    index_ret = where(gt, index1, index2)\n-    return value_ret, index_ret\n+def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, True)\n \n \n @jit\n-@_add_reduction_docstr(\"maximum\")\n-def max(input, axis=None, return_indices=False):\n+def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@_add_reduction_docstr(\"maximum\",\n+                       return_indices_arg=\"return_indices\",\n+                       tie_break_arg=\"return_indices_tie_break_left\")\n+def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n     input = _promote_reduction_input(input)\n     if return_indices:\n-        return _reduce_with_indices(input, axis, _argmax_combine)\n+        if return_indices_tie_break_left:\n+            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n+        else:\n+            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, _max_combine)\n+        return reduce(input, axis, maximum)\n \n \n @jit\n-@_add_reduction_docstr(\"maximum index\")\n-def argmax(input, axis):\n-    (_, ret) = max(input, axis, return_indices=True)\n+@_add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n+def argmax(input, axis, tie_break_left=True):\n+    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n     return ret\n \n # min and argmin\n \n \n @jit\n-def _min_combine(a, b):\n-    # TODO: minimum/maximum doesn't get lowered to fmin/fmax...\n-    return minimum(a, b)\n-\n-\n-@jit\n-def _argmin_combine(value1, index1, value2, index2):\n-    lt = value1 < value2\n+def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    lt = value1 < value2 or tie\n     value_ret = where(lt, value1, value2)\n     index_ret = where(lt, index1, index2)\n     return value_ret, index_ret\n \n \n @jit\n-@_add_reduction_docstr(\"minimum\")\n-def min(input, axis=None, return_indices=False):\n+def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@_add_reduction_docstr(\"minimum\",\n+                       return_indices_arg=\"return_indices\",\n+                       tie_break_arg=\"return_indices_tie_break_left\")\n+def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n     input = _promote_reduction_input(input)\n     if return_indices:\n-        return _reduce_with_indices(input, axis, _argmin_combine)\n+        if return_indices_tie_break_left:\n+            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n+        else:\n+            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, _min_combine)\n+        return reduce(input, axis, minimum)\n \n \n @jit\n-@_add_reduction_docstr(\"minimum index\")\n-def argmin(input, axis):\n-    _, ret = min(input, axis, return_indices=True)\n+@_add_reduction_docstr(\"minimum index\",\n+                       tie_break_arg=\"tie_break_left\")\n+def argmin(input, axis, tie_break_left=True):\n+    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n     return ret\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 17, "deletions": 3, "changes": 20, "file_content_changes": "@@ -775,7 +775,7 @@ def cast(input: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n-def _str_to_cache_modifier(cache_modifier):\n+def _str_to_load_cache_modifier(cache_modifier):\n     cache = ir.CACHE_MODIFIER.NONE  # default\n     if cache_modifier:\n         if cache_modifier == \".ca\":\n@@ -787,6 +787,20 @@ def _str_to_cache_modifier(cache_modifier):\n     return cache\n \n \n+def _str_to_store_cache_modifier(cache_modifier):\n+    cache = ir.CACHE_MODIFIER.NONE  # default\n+    if cache_modifier:\n+        if cache_modifier == \".wb\":\n+            cache = ir.CACHE_MODIFIER.WB\n+        elif cache_modifier == \".cg\":\n+            cache = ir.CACHE_MODIFIER.CG\n+        elif cache_modifier == \".cs\":\n+            cache = ir.CACHE_MODIFIER.CS\n+        else:\n+            raise ValueError(f\"Cache modifier {cache_modifier} not supported\")\n+    return cache\n+\n+\n def _str_to_eviction_policy(eviction_policy):\n     eviction = ir.EVICTION_POLICY.NORMAL  # default\n     if eviction_policy:\n@@ -929,7 +943,7 @@ def load(ptr: tl.tensor,\n          is_volatile: bool,\n          builder: ir.builder) -> tl.tensor:\n     # Cache, eviction and padding options\n-    cache = _str_to_cache_modifier(cache_modifier)\n+    cache = _str_to_load_cache_modifier(cache_modifier)\n     eviction = _str_to_eviction_policy(eviction_policy)\n     padding = _str_to_padding_option(padding_option)\n \n@@ -1018,7 +1032,7 @@ def store(ptr: tl.tensor,\n           eviction_policy: str,\n           builder: ir.builder) -> tl.tensor:\n     # Cache and eviction options\n-    cache = _str_to_cache_modifier(cache_modifier)\n+    cache = _str_to_store_cache_modifier(cache_modifier)\n     eviction = _str_to_eviction_policy(eviction_policy)\n \n     if ptr.type.is_ptr() and ptr.type.element_ty.is_block():"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -70,10 +70,11 @@ def _bench(self, *args, config, **meta):\n             )\n         # augment meta-parameters with tunable ones\n         current = dict(meta, **config.kwargs)\n+        full_nargs = {**self.nargs, **current}\n \n         def kernel_call():\n             if config.pre_hook:\n-                config.pre_hook(self.nargs)\n+                config.pre_hook(full_nargs)\n             self.hook(args)\n             self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n         try:\n@@ -106,7 +107,8 @@ def run(self, *args, **kwargs):\n             config = self.configs[0]\n         self.best_config = config\n         if config.pre_hook is not None:\n-            config.pre_hook(self.nargs)\n+            full_nargs = {**self.nargs, **kwargs, **self.best_config.kwargs}\n+            config.pre_hook(full_nargs)\n         return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n \n     def prune_configs(self, kwargs):"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -71,7 +71,7 @@ def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n         while isinstance(lhs, ast.Attribute):\n             lhs = self.visit(lhs.value)\n-        if lhs is None or lhs.__name__ == \"triton\":\n+        if lhs is None or getattr(lhs, \"__name__\", \"\") == \"triton\":\n             return None\n         return getattr(lhs, node.attr)\n "}, {"filename": "test/Conversion/invalid.mlir", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+// RUN: triton-opt %s -split-input-file -verify-diagnostics\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=2}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf32, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{element types of operands A and B must match}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf32, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf16>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{mismatching encoding between A and B operands}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf16> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf16, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{mismatching kWidth between A and B operands}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 62, "deletions": 4, "changes": 66, "file_content_changes": "@@ -15,10 +15,12 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // CHECK: tt.func @push_elementwise1\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n-// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n-// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]]\n-// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n-// CHECK: %[[C:.*]] = tt.dot %[[AF16]]\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %{{.*}} : {{.*}} tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[C:.*]] = tt.dot %[[AF16]], %[[BCVT]]\n+// CHECK-SAME: tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<16x16xf32, #mma>\n // CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n tt.func @push_elementwise1(\n                    %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n@@ -125,4 +127,60 @@ tt.func @push_elementwise5(\n   tt.return %newc : tensor<16x16xf32, #Cv1>\n }\n \n+// CHECK: tt.func @succeeds_if_arg_is_not_convert_layout\n+// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]]\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n+// CHECK: %[[C:.*]] = tt.dot %[[AF16]]\n+// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n+tt.func @succeeds_if_arg_is_not_convert_layout(\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n+  %dotai8 = triton_gpu.convert_layout %ai8 : (tensor<16x16xi8, #ALR>) -> tensor<16x16xi8, #Av2>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n+  %dotaf8 = tt.bitcast %dotai8 : tensor<16x16xi8, #Av2> -> tensor<16x16xf8E5M2, #Av2>\n+  %dota = tt.fp_to_fp %dotaf8 : tensor<16x16xf8E5M2, #Av2> -> tensor<16x16xf16, #Av2>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n+  tt.return %newc : tensor<16x16xf32, #Cv2>\n+}\n+\n+}\n+\n+// -----\n+\n+#blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+// CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+// CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+// CHECK: #[[MMA:.*]] = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+\n+// CHECK: tt.func @push_convert_both_operands\n+// CHECK: %[[ALOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BA]]>\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] : (tensor<16x16xf16, #[[BA]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BLOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BB]]>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %[[BLOAD]] : (tensor<16x16xf16, #[[BB]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[AEXT:.*]] = arith.extf %[[ACVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BEXT:.*]] = arith.extf %[[BCVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: tt.dot %[[AEXT]], %[[BEXT]], %{{.*}} {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> -> tensor<16x16xf32, #mma>\n+tt.func @push_convert_both_operands(\n+                   %pa: tensor<16x16x!tt.ptr<f16>, #blockedA> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #blockedB> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #mma>) -> tensor<16x16xf32, #mma>{\n+  %a = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedA>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedB>\n+  %ae = arith.extf %a : tensor<16x16xf16, #blockedA> to tensor<16x16xf32, #blockedA>\n+  %be = arith.extf %b : tensor<16x16xf16, #blockedB> to tensor<16x16xf32, #blockedB>\n+  %al = triton_gpu.convert_layout %ae : (tensor<16x16xf32, #blockedA>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+  %bl = triton_gpu.convert_layout %be : (tensor<16x16xf32, #blockedB>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+  %r = tt.dot %al, %bl, %c {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<16x16xf32, #mma>\n+  tt.return %r : tensor<16x16xf32, #mma>\n+}\n+\n }"}]