[{"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "file_content_changes": "@@ -195,13 +195,15 @@ struct FpToFpOpConversion\n                            const Value &v3) {\n     auto *ptxAsm = \"{                                      \\n\"\n                    \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"shl.b32  b0, $1, 1;                    \\n\"\n-                   \"shl.b32  b1, $2, 1;                    \\n\"\n-                   \"add.u32   b0, b0, 128;                  \\n\"\n-                   \"add.u32   b1, b1, 128;                  \\n\"\n-                   \"lop3.b32  b0, b0, 0x80008000, $1, 0xf8;\\n\"\n-                   \"lop3.b32  b1, b1, 0x80008000, $2, 0xf8;\\n\"\n-                   \"prmt.b32  $0, b0, b1, 0x7531;          \\n\"\n+                   \"shl.b32 a0, $1, 1;                     \\n\"\n+                   \"shl.b32 a1, $2, 1;                     \\n\"\n+                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"add.u32 a0, a0, 0x00800080;            \\n\"\n+                   \"add.u32 a1, a1, 0x00800080;            \\n\"\n+                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n+                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n+                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n                    \"}\";\n     return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   }"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -917,7 +917,6 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         ),\n         dim=1,\n     )[0]\n-    print(min_error)\n     # 1.9375 is float8 max\n     mismatch = torch.logical_and(\n         abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.abs(f16_input) < 1.9375)"}]