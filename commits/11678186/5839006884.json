[{"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -674,7 +674,7 @@ def __getattribute__(self, name):\n         return super().__getattribute__(name)\n \n     # capture args and expand args with cutensormap*\n-    def assemble_tensormap_to_arg(self, args, constants):\n+    def assemble_tensormap_to_arg(self, args):\n         args_with_tma = list(args)\n         if hasattr(self, 'tensormaps_info'):\n             # tuple for hashable\n@@ -687,7 +687,7 @@ def __getitem__(self, grid):\n         self._init_handles()\n \n         def runner(*args, stream=None):\n-            args_expand = self.assemble_tensormap_to_arg(args, self.constants)\n+            args_expand = self.assemble_tensormap_to_arg(args)\n             if stream is None:\n                 if self.device_type in [\"cuda\", \"hip\"]:\n                     stream = get_cuda_stream()"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 7, "changes": 9, "file_content_changes": "@@ -407,13 +407,8 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_s\n     if bin is not None:\n       # build dict of constant values\n       args = [{args}]\n-      all_args = {', '.join([f'{arg}' for arg in self.arg_names]) + ', ' if len(self.arg_names) > 0 else ()}\n-      configs = self._get_config(*all_args),\n-      constants = self._make_constants(constexpr_key)\n-      constants.update({{i: None for i, arg in enumerate(all_args) if arg is None}})\n-      constants.update({{i: 1 for i in configs[0].equal_to_1}})\n       # Create tensormaps and append to args\n-      args = bin.assemble_tensormap_to_arg(args, constants)\n+      args = bin.assemble_tensormap_to_arg(args)\n       if not warmup:\n           bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.num_ctas, bin.clusterDims[0], bin.clusterDims[1], bin.clusterDims[2], bin.shared, stream, bin.cu_function, CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, bin, *args)\n       return bin\n@@ -435,7 +430,7 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_s\n       if not self._call_hook(key, signature, device, constants, num_warps, num_ctas, num_stages, enable_warp_specialization, extern_libs, configs):\n         bin = compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_ctas=num_ctas, num_stages=num_stages, enable_warp_specialization=enable_warp_specialization, extern_libs=extern_libs, configs=configs, debug=self.debug, device_type=device_type)\n         # Create tensormaps and append to args\n-        args = bin.assemble_tensormap_to_arg(args, constants)\n+        args = bin.assemble_tensormap_to_arg(args)\n         if not warmup:\n             bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.num_ctas, bin.clusterDims[0], bin.clusterDims[1], bin.clusterDims[2], bin.shared, stream, bin.cu_function, CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, bin, *args)\n         self.cache[device][key] = bin"}]