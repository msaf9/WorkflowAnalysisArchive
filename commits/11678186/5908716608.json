[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -585,6 +585,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n \n   let extraClassDeclaration = extraBaseClassDeclaration # [{\n     bool isVolta() const;\n+    bool isTuring() const;\n     bool isAmpere() const;\n     bool isHopper() const;\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -17,6 +17,10 @@ LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n                             TritonGPUToLLVMTypeConverter *typeConverter,\n                             ConversionPatternRewriter &rewriter);\n \n+LogicalResult convertMMA1688(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                             TritonGPUToLLVMTypeConverter *typeConverter,\n+                             ConversionPatternRewriter &rewriter);\n+\n LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n                               TritonGPUToLLVMTypeConverter *typeConverter,\n                               ConversionPatternRewriter &rewriter);\n@@ -56,6 +60,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     if (!isOuter && mmaLayout && supportMMA(op, mmaLayout.getVersionMajor())) {\n       if (mmaLayout.isVolta())\n         return convertMMA884(op, adaptor, getTypeConverter(), rewriter);\n+      if (mmaLayout.isTuring())\n+        return convertMMA1688(op, adaptor, getTypeConverter(), rewriter);\n       if (mmaLayout.isAmpere())\n         return convertMMA16816(op, adaptor, getTypeConverter(), rewriter);\n       if (mmaLayout.isHopper())"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "modified", "additions": 61, "deletions": 17, "changes": 78, "file_content_changes": "@@ -141,7 +141,15 @@ TensorCoreType getMmaType(triton::DotOp op) {\n   return TensorCoreType::NOT_APPLICABLE;\n }\n \n-inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n+inline static const std::map<TensorCoreType, std::string> mmaInstrPtxTuring = {\n+    {TensorCoreType::FP32_FP16_FP16_FP32,\n+     \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"},\n+\n+    {TensorCoreType::FP16_FP16_FP16_FP16,\n+     \"mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16\"},\n+};\n+\n+inline static const std::map<TensorCoreType, std::string> mmaInstrPtxAmpere = {\n     {TensorCoreType::FP32_FP16_FP16_FP32,\n      \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n     {TensorCoreType::FP32_BF16_BF16_FP32,\n@@ -164,7 +172,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n                          ConversionPatternRewriter &rewriter, Location loc,\n                          Value a, Value b, Value c, Value d, Value loadedA,\n                          Value loadedB, Value loadedC, DotOp op,\n-                         DotOpAdaptor adaptor) {\n+                         DotOpAdaptor adaptor, bool isTuring) {\n   MLIRContext *ctx = c.getContext();\n   auto aTensorTy = a.getType().cast<RankedTensorType>();\n   auto bTensorTy = b.getType().cast<RankedTensorType>();\n@@ -197,6 +205,9 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n \n   auto mmaType = getMmaType(op);\n \n+  const auto &mmaInstructions =\n+      isTuring ? mmaInstrPtxTuring : mmaInstrPtxAmpere;\n+\n   auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n     unsigned colsPerThread = repN * 2;\n     PTXBuilder builder;\n@@ -206,14 +217,6 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n     bool isAccF16 = dTensorTy.getElementType().isF16();\n     auto retArgs =\n         builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n-    auto aArgs = builder.newListOperand({\n-        {ha[{m, k}], \"r\"},\n-        {ha[{m + 1, k}], \"r\"},\n-        {ha[{m, k + 1}], \"r\"},\n-        {ha[{m + 1, k + 1}], \"r\"},\n-    });\n-    auto bArgs =\n-        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n     auto cArgs = builder.newListOperand();\n     for (int i = 0; i < numMmaRets; ++i) {\n       cArgs->listAppend(builder.newOperand(\n@@ -222,9 +225,36 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n       // reuse the output registers\n     }\n \n-    mma(retArgs, aArgs, bArgs, cArgs);\n+    if (isTuring) {\n+      auto aArgs1 = builder.newListOperand({\n+          {ha[{m, k}], \"r\"},\n+          {ha[{m + 1, k}], \"r\"},\n+      });\n+      auto bArgs1 = builder.newListOperand({\n+          {hb[{n, k}], \"r\"},\n+      });\n+      auto aArgs2 = builder.newListOperand({\n+          {ha[{m, k + 1}], \"r\"},\n+          {ha[{m + 1, k + 1}], \"r\"},\n+      });\n+      auto bArgs2 = builder.newListOperand({\n+          {hb[{n, k + 1}], \"r\"}\n+      });\n+      mma(retArgs, aArgs1, bArgs1, cArgs);\n+      mma(retArgs, aArgs2, bArgs2, cArgs);\n+    } else {\n+      auto aArgs = builder.newListOperand({\n+          {ha[{m, k}], \"r\"},\n+          {ha[{m + 1, k}], \"r\"},\n+          {ha[{m, k + 1}], \"r\"},\n+          {ha[{m + 1, k + 1}], \"r\"},\n+      });\n+      auto bArgs =\n+          builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+      mma(retArgs, aArgs, bArgs, cArgs);\n+    }\n     Value mmaOut =\n-        builder.launch(rewriter, loc, getMmaRetType(mmaType, op.getContext()));\n+      builder.launch(rewriter, loc, getMmaRetType(mmaType, op.getContext()));\n \n     Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n     for (int i = 0; i < numMmaRets; ++i) {\n@@ -259,10 +289,9 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   return success();\n }\n \n-// Convert to mma.m16n8k16\n-LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n-                              TritonGPUToLLVMTypeConverter *typeConverter,\n-                              ConversionPatternRewriter &rewriter) {\n+LogicalResult convertMMA(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                         TritonGPUToLLVMTypeConverter *typeConverter,\n+                         ConversionPatternRewriter &rewriter, bool isTuring) {\n   auto loc = op.getLoc();\n   auto mmaLayout = op.getResult()\n                        .getType()\n@@ -288,5 +317,20 @@ LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n       loadC(op.getC(), adaptor.getC(), typeConverter, op.getLoc(), rewriter);\n \n   return convertDot(typeConverter, rewriter, op.getLoc(), A, B, C, op.getD(),\n-                    loadedA, loadedB, loadedC, op, adaptor);\n+                    loadedA, loadedB, loadedC, op, adaptor, isTuring);\n+}\n+\n+// Convert to mma.m16n8k8\n+LogicalResult convertMMA1688(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                             TritonGPUToLLVMTypeConverter *typeConverter,\n+                             ConversionPatternRewriter &rewriter) {\n+  return convertMMA(op, adaptor, typeConverter, rewriter, true /*isTuring*/);\n+}\n+\n+// Convert to mma.m16n8k16\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter) {\n+  return convertMMA(op, adaptor, typeConverter, rewriter, false /*isTuring*/);\n }\n+"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1205,6 +1205,10 @@ void SharedEncodingAttr::print(AsmPrinter &printer) const {\n \n bool MmaEncodingAttr::isVolta() const { return getVersionMajor() == 1; }\n \n+bool MmaEncodingAttr::isTuring() const {\n+  return getVersionMajor() == 2 && getVersionMinor() == 1;\n+}\n+\n bool MmaEncodingAttr::isAmpere() const { return getVersionMajor() == 2; }\n \n bool MmaEncodingAttr::isHopper() const { return getVersionMajor() == 3; }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -24,7 +24,7 @@ using ttg::SliceEncodingAttr;\n // supported\n static int getMMAVersionSafe(int computeCapability, tt::DotOp op) {\n   int baseVersion = 0;\n-  if (computeCapability < 80) {\n+  if (computeCapability < 75) {\n     baseVersion = 1;\n   } else if (computeCapability < 90) {\n     baseVersion = 2;\n@@ -255,10 +255,11 @@ class BlockedToMMA : public mlir::RewritePattern {\n           instrShape, oldAType.getShape(), oldBType.getShape(), retShapePerCTA,\n           isARow, isBRow, mmaV1Counter++);\n     } else if (versionMajor == 2 || versionMajor == 3) {\n+      int versionMinor = computeCapability == 75 ? 1 : 0;\n       auto warpsPerTile = getWarpsPerTile(dotOp, retShapePerCTA, versionMajor,\n                                           numWarps, instrShape);\n       mmaEnc = ttg::MmaEncodingAttr::get(oldRetType.getContext(), versionMajor,\n-                                         0 /*versionMinor*/, warpsPerTile,\n+                                         versionMinor, warpsPerTile,\n                                          CTALayout, instrShape);\n     }\n     auto newRetType = RankedTensorType::get("}]