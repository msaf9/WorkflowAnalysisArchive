[{"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 48, "deletions": 43, "changes": 91, "file_content_changes": "@@ -148,46 +148,51 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n \n # Precession regression for FMADot is not done yet due to some issue on the optimizer failed to give a blocked layout to dot op.\n # TODO[Superjomn]: Uncomment this test and continue to finish precession regression latter.\n-# @pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n-#     [128, 256, 128, 4, 128, 256, 32],\n-#     [256, 128, 64, 4, 256, 128, 16],\n-#     [128, 64, 128, 4, 128, 64, 32],\n-# ])\n-# def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n-#     @triton.jit\n-#     def matmul_kernel(\n-#         a_ptr, b_ptr, c_ptr,\n-#         stride_am, stride_ak,\n-#         stride_bk, stride_bn,\n-#         stride_cm, stride_cn,\n-#         K: tl.constexpr,\n-#         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-#     ):\n-#         offs_m = tl.arange(0, BLOCK_SIZE_M)\n-#         offs_n = tl.arange(0, BLOCK_SIZE_N)\n-#         offs_k = tl.arange(0, BLOCK_SIZE_K)\n-#         a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n-#         b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n-#         accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-#         for k in range(0, K, BLOCK_SIZE_K):\n-#             a = tl.load(a_ptrs)\n-#             b = tl.load(b_ptrs)\n-#             accumulator += tl.dot(a, b, allow_tf32=True)\n-#             a_ptrs += BLOCK_SIZE_K * stride_ak\n-#             b_ptrs += BLOCK_SIZE_K * stride_bk\n-\n-#         c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n-#         tl.store(c_ptrs, accumulator)\n-\n-#     a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n-#     b = torch.randn((K, N), device='cuda', dtype=torch.float)\n-#     c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n-#     grid = lambda META: (1, )\n-#     matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-#                         stride_am=a.stride(0), stride_ak=a.stride(1),\n-#                         stride_bk=b.stride(0), stride_bn=b.stride(1),\n-#                         stride_cm=c.stride(0), stride_cn=c.stride(1),\n-#                         K=a.shape[1], BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N,\n-#                         BLOCK_SIZE_K=block_K, num_warps=num_warps)\n-#     golden = torch.matmul(a, b)\n-#     torch.testing.assert_close(c, golden)\n+@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n+    [128, 256, 128, 4, 128, 256, 32],\n+    [256, 128, 64, 4, 256, 128, 16],\n+    [128, 64, 128, 4, 128, 64, 32],\n+])\n+def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+    @triton.jit\n+    def matmul_kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        K: tl.constexpr,\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+    ):\n+        offs_m = tl.arange(0, BLOCK_SIZE_M)\n+        offs_n = tl.arange(0, BLOCK_SIZE_N)\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n+        b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, K, BLOCK_SIZE_K):\n+            a = tl.load(a_ptrs)\n+            b = tl.load(b_ptrs)\n+            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n+            accumulator += tl.dot(a, b, allow_tf32=False)\n+            a_ptrs += BLOCK_SIZE_K * stride_ak\n+            b_ptrs += BLOCK_SIZE_K * stride_bk\n+\n+        c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n+        tl.store(c_ptrs, accumulator)\n+\n+    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n+    b = torch.randn((K, N), device='cuda', dtype=torch.float)\n+    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n+    grid = lambda META: (1, )\n+    matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                        K=a.shape[1], BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N,\n+                        BLOCK_SIZE_K=block_K, num_warps=num_warps)\n+    golden = torch.matmul(a, b)\n+    torch.testing.assert_close(c, golden)\n+\n+\n+#test_gemm_no_scf(*[64, 128, 128, 2])\n+test_gemm_fmadot(*[128, 64, 128, 4, 128, 64, 32])"}]