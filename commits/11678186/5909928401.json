[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 27, "deletions": 15, "changes": 42, "file_content_changes": "@@ -463,33 +463,23 @@ def TT_ScanReturnOp: TT_Op<\"scan.return\",\n //\n // External Elementwise op\n //\n-class TT_ExternElementwiseOpBase<string mnemonic, list<Trait> traits = []> :\n-    TT_Op<mnemonic,\n-         traits # [Elementwise,\n-                   SameOperandsAndResultEncoding,\n-                   SameVariadicOperandSize]> {\n+def TT_ExternElementwiseOp : TT_Op<\"extern_elementwise\", [Elementwise,\n+                                                            SameOperandsAndResultEncoding,\n+                                                            SameVariadicOperandSize,\n+                                                            DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {\n \n     let description = [{\n         call an external function $symbol implemented in $libpath/$libname with $args\n         return $libpath/$libname:$symbol($args...)\n     }];\n \n-    let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n+    let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol, BoolAttr:$pure);\n \n     let results = (outs TT_Type:$result);\n \n     let assemblyFormat = \"operands attr-dict `:` functional-type(operands, $result)\";\n }\n \n-def TT_PureExternElementwiseOp : TT_ExternElementwiseOpBase<\"pure_extern_elementwise\", [Pure, Elementwise]> {\n-    let summary = \"FFI for pure element-wise extern LLVM bitcode functions\";\n-}\n-\n-def TT_ImpureExternElementwiseOp : TT_ExternElementwiseOpBase<\"impure_extern_elementwise\", [MemoryEffects<[MemRead]>,\n-                                                                                            MemoryEffects<[MemWrite]>]> {\n-    let summary = \"FFI for impure element-wise extern LLVM bitcode functions\";\n-}\n-\n //\n // Make Range Op\n //\n@@ -510,6 +500,28 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n     let assemblyFormat = \"attr-dict `:` type($result)\";\n }\n \n+//\n+// ElementwiseInlineAsm Op\n+//\n+def TT_ElementwiseInlineAsmOp : TT_Op<\"elementwise_inline_asm\", [Elementwise,\n+                                                                 SameOperandsAndResultEncoding,\n+                                                                 DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {\n+  let summary = \"inline assembly applying elementwise operation to a group of packed element.\";\n+  let description = [{\n+   This will apply the given in inline assembly to `packed_element` number of\n+   elements of the inputs. The elements packed together is unknown and will\n+   depend on the backend implementation.\n+  }];\n+\n+  let arguments = (ins StrAttr:$asm_string, StrAttr:$constraints, BoolAttr:$pure, I32Attr:$packed_element, Variadic<AnyTypeOf<[TT_Type]>>:$args);\n+  let results = (outs TT_Tensor:$result);\n+\n+\n+  let assemblyFormat = [{\n+    $asm_string attr-dict ($args^ `:` type($args))? `->` type($result)\n+  }];\n+}\n+\n //\n // Print Op\n //"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -585,6 +585,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n \n   let extraClassDeclaration = extraBaseClassDeclaration # [{\n     bool isVolta() const;\n+    bool isTuring() const;\n     bool isAmpere() const;\n     bool isHopper() const;\n "}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -361,7 +361,7 @@ bool supportMMA(triton::DotOp op, int version) {\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n     if (!(numWarps % 4 == 0 && retShapePerCTA[0] % 64 == 0 &&\n           retShapePerCTA[1] % 8 == 0 &&\n-          (aElemTy.isFloat8E5M2() || aElemTy.isFloat8E4M3FN() ||\n+          (aElemTy.isFloat8E5M2() || aElemTy.isFloat8E4M3FNUZ() ||\n            aElemTy.isInteger(8) || aElemTy.isF16() || aElemTy.isBF16() ||\n            aElemTy.isF32()))) {\n       return false;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -17,6 +17,10 @@ LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n                             TritonGPUToLLVMTypeConverter *typeConverter,\n                             ConversionPatternRewriter &rewriter);\n \n+LogicalResult convertMMA1688(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                             TritonGPUToLLVMTypeConverter *typeConverter,\n+                             ConversionPatternRewriter &rewriter);\n+\n LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n                               TritonGPUToLLVMTypeConverter *typeConverter,\n                               ConversionPatternRewriter &rewriter);\n@@ -56,6 +60,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     if (!isOuter && mmaLayout && supportMMA(op, mmaLayout.getVersionMajor())) {\n       if (mmaLayout.isVolta())\n         return convertMMA884(op, adaptor, getTypeConverter(), rewriter);\n+      if (mmaLayout.isTuring())\n+        return convertMMA1688(op, adaptor, getTypeConverter(), rewriter);\n       if (mmaLayout.isAmpere())\n         return convertMMA16816(op, adaptor, getTypeConverter(), rewriter);\n       if (mmaLayout.isHopper())"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "modified", "additions": 61, "deletions": 17, "changes": 78, "file_content_changes": "@@ -141,7 +141,15 @@ TensorCoreType getMmaType(triton::DotOp op) {\n   return TensorCoreType::NOT_APPLICABLE;\n }\n \n-inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n+inline static const std::map<TensorCoreType, std::string> mmaInstrPtxTuring = {\n+    {TensorCoreType::FP32_FP16_FP16_FP32,\n+     \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"},\n+\n+    {TensorCoreType::FP16_FP16_FP16_FP16,\n+     \"mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16\"},\n+};\n+\n+inline static const std::map<TensorCoreType, std::string> mmaInstrPtxAmpere = {\n     {TensorCoreType::FP32_FP16_FP16_FP32,\n      \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n     {TensorCoreType::FP32_BF16_BF16_FP32,\n@@ -164,7 +172,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n                          ConversionPatternRewriter &rewriter, Location loc,\n                          Value a, Value b, Value c, Value d, Value loadedA,\n                          Value loadedB, Value loadedC, DotOp op,\n-                         DotOpAdaptor adaptor) {\n+                         DotOpAdaptor adaptor, bool isTuring) {\n   MLIRContext *ctx = c.getContext();\n   auto aTensorTy = a.getType().cast<RankedTensorType>();\n   auto bTensorTy = b.getType().cast<RankedTensorType>();\n@@ -197,23 +205,18 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n \n   auto mmaType = getMmaType(op);\n \n+  const auto &mmaInstructions =\n+      isTuring ? mmaInstrPtxTuring : mmaInstrPtxAmpere;\n+\n   auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n     unsigned colsPerThread = repN * 2;\n     PTXBuilder builder;\n-    auto &mma = *builder.create(mmaInstrPtx.at(mmaType));\n+    auto &mma = *builder.create(mmaInstructions.at(mmaType));\n     // using =r for float32 works but leads to less readable ptx.\n     bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n     bool isAccF16 = dTensorTy.getElementType().isF16();\n     auto retArgs =\n         builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n-    auto aArgs = builder.newListOperand({\n-        {ha[{m, k}], \"r\"},\n-        {ha[{m + 1, k}], \"r\"},\n-        {ha[{m, k + 1}], \"r\"},\n-        {ha[{m + 1, k + 1}], \"r\"},\n-    });\n-    auto bArgs =\n-        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n     auto cArgs = builder.newListOperand();\n     for (int i = 0; i < numMmaRets; ++i) {\n       cArgs->listAppend(builder.newOperand(\n@@ -222,7 +225,34 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n       // reuse the output registers\n     }\n \n-    mma(retArgs, aArgs, bArgs, cArgs);\n+    if (isTuring) {\n+      auto aArgs1 = builder.newListOperand({\n+          {ha[{m, k}], \"r\"},\n+          {ha[{m + 1, k}], \"r\"},\n+      });\n+      auto bArgs1 = builder.newListOperand({\n+          {hb[{n, k}], \"r\"},\n+      });\n+      auto aArgs2 = builder.newListOperand({\n+          {ha[{m, k + 1}], \"r\"},\n+          {ha[{m + 1, k + 1}], \"r\"},\n+      });\n+      auto bArgs2 = builder.newListOperand({\n+          {hb[{n, k + 1}], \"r\"}\n+      });\n+      mma(retArgs, aArgs1, bArgs1, cArgs);\n+      mma(retArgs, aArgs2, bArgs2, cArgs);\n+    } else {\n+      auto aArgs = builder.newListOperand({\n+          {ha[{m, k}], \"r\"},\n+          {ha[{m + 1, k}], \"r\"},\n+          {ha[{m, k + 1}], \"r\"},\n+          {ha[{m + 1, k + 1}], \"r\"},\n+      });\n+      auto bArgs =\n+          builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+      mma(retArgs, aArgs, bArgs, cArgs);\n+    }\n     Value mmaOut =\n         builder.launch(rewriter, loc, getMmaRetType(mmaType, op.getContext()));\n \n@@ -259,10 +289,9 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   return success();\n }\n \n-// Convert to mma.m16n8k16\n-LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n-                              TritonGPUToLLVMTypeConverter *typeConverter,\n-                              ConversionPatternRewriter &rewriter) {\n+LogicalResult convertMMA(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                         TritonGPUToLLVMTypeConverter *typeConverter,\n+                         ConversionPatternRewriter &rewriter, bool isTuring) {\n   auto loc = op.getLoc();\n   auto mmaLayout = op.getResult()\n                        .getType()\n@@ -288,5 +317,20 @@ LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n       loadC(op.getC(), adaptor.getC(), typeConverter, op.getLoc(), rewriter);\n \n   return convertDot(typeConverter, rewriter, op.getLoc(), A, B, C, op.getD(),\n-                    loadedA, loadedB, loadedC, op, adaptor);\n+                    loadedA, loadedB, loadedC, op, adaptor, isTuring);\n+}\n+\n+// Convert to mma.m16n8k8\n+LogicalResult convertMMA1688(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                             TritonGPUToLLVMTypeConverter *typeConverter,\n+                             ConversionPatternRewriter &rewriter) {\n+  return convertMMA(op, adaptor, typeConverter, rewriter, true /*isTuring*/);\n+}\n+\n+// Convert to mma.m16n8k16\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter) {\n+  return convertMMA(op, adaptor, typeConverter, rewriter, false /*isTuring*/);\n }\n+"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -58,7 +58,7 @@ triton::nvgpu::WGMMAEltType getMmaOperandType(Value a, bool allowTF32) {\n     return triton::nvgpu::WGMMAEltType::s8;\n   } else if (aTy.isFloat8E5M2()) {\n     return triton::nvgpu::WGMMAEltType::e5m2;\n-  } else if (aTy.isFloat8E4M3FN()) {\n+  } else if (aTy.isFloat8E4M3FNUZ()) {\n     return triton::nvgpu::WGMMAEltType::e4m3;\n   } else {\n     llvm::report_fatal_error(\"Unsupported mma operand type found\");\n@@ -298,7 +298,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n \n   triton::nvgpu::WGMMAEltType eltTypeC = getMmaRetType(d);\n   triton::nvgpu::WGMMAEltType eltTypeA = getMmaOperandType(a, allowTF32);\n-  triton::nvgpu::WGMMAEltType eltTypeB = eltTypeA;\n+  triton::nvgpu::WGMMAEltType eltTypeB = getMmaOperandType(b, allowTF32);\n \n   triton::nvgpu::WGMMALayout layoutA = transA ? triton::nvgpu::WGMMALayout::col\n                                               : triton::nvgpu::WGMMALayout::row;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 177, "deletions": 140, "changes": 317, "file_content_changes": "@@ -154,96 +154,14 @@ const std::string Fp16_to_Fp8E4M3B15x4 =\n     \"lop3.b32 $0, $0, $2, 0xbf80bf80, 0xf8;  \\n\"\n     \"}\";\n \n-/* ----- FP8E4M3 ------ */\n-// Note: when handled by software, this format\n-// does not handle denormals and has\n-// more than a single NaN values.\n-\n-// Fp8E4M3 -> Fp16 (packed)\n-const std::string Fp8E4M3_to_Fp16 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x0504;            \\n\" // a0 = 0x00f300f4\n-    \"prmt.b32 a1, 0, $2, 0x0706;            \\n\" // a1 = 0x00f100f2\n-    \"and.b32  b0, a0, 0x00800080;           \\n\" // b0 = a0 & 0x00800080\n-    \"and.b32  b1, a1, 0x00800080;           \\n\" // (extract sign)\n-    \"add.u32  b0, b0, a0;                   \\n\" // b0 = b0 + a0\n-    \"add.u32  b1, b1, a1;                   \\n\" // (move sign to the left)\n-    \"mad.lo.u32 $0, b0, 128, 0x20002000;    \\n\" // out0 = (b0 << 7) + 0x20002000\n-    \"mad.lo.u32 $1, b1, 128, 0x20002000;    \\n\" // (shift into position and bias\n-                                                // exponent)\n-    \"}\";\n-\n-// Fp16 -> Fp8E4M3 (packed)\n-const std::string Fp16_to_Fp8E4M3 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n-    \"and.b32 a0, $1, 0x7fff7fff;            \\n\" // a0 = input0 & 0x7fff7fff\n-    \"and.b32 a1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-    \"mad.lo.u32 a0, a0, 2, 0x40804080;      \\n\" // shift exponent (<< 1),\n-    \"mad.lo.u32 a1, a1, 2, 0x40804080;      \\n\" // correct bias (0x40004000),\n-                                                // and round to nearest\n-    \"lop3.b32 b0, $1, 0x80008000, a0, 0xe2; \\n\" // b0 = 0x80008000 ? in0 : a0\n-    \"lop3.b32 b1, $2, 0x80008000, a1, 0xe2; \\n\" // (restore sign)\n-    \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n-    \"}\";\n-\n-// WARN: subnormal (0bs0000xxx) are not handled\n-const std::string Fp8E4M3_to_Bf16 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x0504;            \\n\" // a0 = 0x00f300f4\n-    \"prmt.b32 a1, 0, $2, 0x0706;            \\n\" // a1 = 0x00f100f2\n-    \"and.b32  b0, a0, 0x00800080;           \\n\" // b0 = a0 & 0x00800080\n-    \"and.b32  b1, a1, 0x00800080;           \\n\" // (extract sign)\n-    \"mad.lo.u32 b0, b0, 15, a0;             \\n\" // b0 = b0 * 15 + a0\n-    \"mad.lo.u32 b1, b1, 15, a1;             \\n\" // (move sign to the left)\n-    \"mad.lo.u32 $0, b0, 16, 0x3c003c00;     \\n\" // out0 = (b0 << 4) + 0x3c003c00\n-    \"mad.lo.u32 $1, b1, 16, 0x3c003c00;     \\n\" // (shift into position and bias\n-                                                // exponent)\n-    \"}\";\n-\n-const std::string Bf16_to_Fp8E4M3 =\n-    \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n-    \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n-    \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n-    \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n-    \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n-    \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n-    \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n-    \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n-    \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n-    \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n-    \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-\n-    // nosign = clamp(nosign, min, max)\n-    \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n-    \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n-    \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n-    \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n-    \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n-    \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n-    \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n-    \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n-    \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n-    \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n-    \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n-    \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n-    \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n-    \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n-    \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n-\n-    \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n-    \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n-    \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n-    \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n-    \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n-    \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n-    \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n-                                                     // nosign1 = 0x00f300f4\n-                                                     // nosign = 0xf3f4f1f2\n-    \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n-    \"}\";\n+// Fp8E4M3 (x2) -> Fp16 (x2) (packed)\n+const std::string Fp8E4M3Nv_to_Fp16 = \"{ \\n\"\n+                                      \"cvt.rn.f16x2.e4m3x2 $0, $1; \\n\"\n+                                      \"}\";\n+// Fp16 (x2) -> Fp8E4M3 (x2) (packed)\n+const std::string Fp16_to_Fp8E4M3Nv = \"{ \\n\"\n+                                      \"cvt.rn.satfinite.e4m3x2.f16x2 $0, $1; \\n\"\n+                                      \"}\";\n \n /* ----- Packed integer to BF16 ------ */\n const std::string S8_to_Bf16 =\n@@ -391,40 +309,49 @@ inline SmallVector<Value> packI32(const SmallVector<Value> &inValues,\n }\n \n typedef std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n-                                         const Value &, const Value &,\n-                                         const Value &, const Value &)>\n+                                         const SmallVector<Value> &)>\n     ConverterT;\n \n static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n-                                       Type outType) {\n+                                       Type outType,\n+                                       const int inVecWidthBits = 32,\n+                                       const int outVecWidthBits = 32) {\n+\n+  ConverterT converter =\n+      [ptxAsm, inType, outType, inVecWidthBits,\n+       outVecWidthBits](Location loc, ConversionPatternRewriter &rewriter,\n+                        const SmallVector<Value> &v) -> SmallVector<Value> {\n+    int numElements = v.size();\n+    assert(numElements == 4 || numElements == 2 && \"invalid vector size\");\n \n-  ConverterT converter = [ptxAsm, inType, outType](\n-                             Location loc, ConversionPatternRewriter &rewriter,\n-                             const Value &v0, const Value &v1, const Value &v2,\n-                             const Value &v3) -> SmallVector<Value> {\n-    SmallVector<Value> v = {v0, v1, v2, v3};\n     auto ctx = rewriter.getContext();\n     int inBitwidth = inType.getIntOrFloatBitWidth();\n     int outBitwidth = outType.getIntOrFloatBitWidth();\n     // first, we pack `v` into 32-bit ints\n-    int inVecWidth = 32 / inBitwidth;\n+    int inVecWidth = inVecWidthBits / inBitwidth;\n     auto inVecTy = vec_ty(inType, inVecWidth);\n-    SmallVector<Value> inPacked(4 / inVecWidth, undef(inVecTy));\n-    for (size_t i = 0; i < 4; i++)\n+    SmallVector<Value> inPacked(numElements / inVecWidth, undef(inVecTy));\n+    for (size_t i = 0; i < numElements; i++)\n       inPacked[i / inVecWidth] = insert_element(\n           inVecTy, inPacked[i / inVecWidth], v[i], i32_val(i % inVecWidth));\n     for (size_t i = 0; i < inPacked.size(); i++)\n-      inPacked[i] = bitcast(inPacked[i], i32_ty);\n+      inPacked[i] = bitcast(inPacked[i], int_ty(inVecWidthBits));\n \n     // then, we run the provided inline PTX\n-    int outVecWidth = 32 / outBitwidth;\n-    int outNums = 4 / outVecWidth;\n+    int outVecWidth = outVecWidthBits / outBitwidth;\n+    int outNums = numElements / outVecWidth;\n     PTXBuilder builder;\n     SmallVector<PTXBuilder::Operand *> operands;\n-    for (int i = 0; i < outNums; i++)\n-      operands.push_back(builder.newOperand(\"=r\"));\n-    for (Value inVal : inPacked)\n-      operands.push_back(builder.newOperand(inVal, \"r\"));\n+    auto outConstriant = outVecWidthBits == 16 ? \"=h\" : \"=r\";\n+    auto inConstraint = inVecWidthBits == 16 ? \"h\" : \"r\";\n+    for (int i = 0; i < outNums; i++) {\n+      operands.push_back(builder.newOperand(outConstriant));\n+    }\n+\n+    for (Value inVal : inPacked) {\n+      operands.push_back(builder.newOperand(inVal, inConstraint));\n+    }\n+\n     auto &ptxOp = *builder.create(ptxAsm);\n     ptxOp(operands, /*onlyAttachMLIRArgs=*/true);\n     auto outVecTy = vec_ty(outType, outVecWidth);\n@@ -439,7 +366,7 @@ static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n     }\n     // unpack the output\n     SmallVector<Value> ret;\n-    for (size_t i = 0; i < 4; i++)\n+    for (size_t i = 0; i < numElements; i++)\n       ret.push_back(extract_element(outType, outPacked[i / outVecWidth],\n                                     i32_val(i % outVecWidth)));\n     return ret;\n@@ -526,6 +453,9 @@ class ElementwiseOpConversionBase\n \n     return success();\n   }\n+\n+private:\n+  int computeCapability;\n };\n \n template <typename SourceOp, typename DestOp>\n@@ -538,11 +468,6 @@ struct ElementwiseOpConversion\n   using Base::Base;\n   using OpAdaptor = typename Base::OpAdaptor;\n \n-  explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n-                                   PatternBenefit benefit = 1)\n-      : ElementwiseOpConversionBase<SourceOp, ElementwiseOpConversion>(\n-            typeConverter, benefit) {}\n-\n   // An interface to support variant DestOp builder.\n   SmallVector<DestOp> createDestOps(SourceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter,\n@@ -559,6 +484,11 @@ struct FpToFpOpConversion\n   using ElementwiseOpConversionBase<\n       triton::FpToFpOp, FpToFpOpConversion>::ElementwiseOpConversionBase;\n \n+  explicit FpToFpOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n+                              int computeCapability, PatternBenefit benefit = 1)\n+      : ElementwiseOpConversionBase(typeConverter, benefit),\n+        computeCapability(computeCapability) {}\n+\n   static Value convertBf16ToFp32(Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  const Value &v) {\n@@ -618,58 +548,83 @@ struct FpToFpOpConversion\n         // F8 -> F16\n         {{F8E4M3B15TyID, F16TyID}, Fp8E4M3B15_to_Fp16},\n         {{F8E4M3FNTyID, F16TyID}, Fp8E4M3B15x4_to_Fp16},\n-        {{F8E4M3TyID, F16TyID}, Fp8E4M3_to_Fp16},\n+        {{F8E4M3TyID, F16TyID}, Fp8E4M3Nv_to_Fp16},\n         {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n         // F16 -> F8\n         {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n         {{F16TyID, F8E4M3FNTyID}, Fp16_to_Fp8E4M3B15x4},\n-        {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3},\n+        {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3Nv},\n         {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},\n         // F8 -> BF16\n-        {{F8E4M3TyID, BF16TyID}, Fp8E4M3_to_Bf16},\n         {{F8E5M2TyID, BF16TyID}, Fp8E5M2_to_Bf16},\n         // BF16 -> F8\n-        {{BF16TyID, F8E4M3TyID}, Bf16_to_Fp8E4M3},\n         {{BF16TyID, F8E5M2TyID}, Bf16_to_Fp8E5M2},\n     };\n+    int inVecWidthBits = 32;\n+    int outVecWidthBits = 32;\n+    if (srcTy.isFloat8E4M3FNUZ()) {\n+      inVecWidthBits = 16;\n+      outVecWidthBits = 32;\n+    }\n+    if (dstTy.isFloat8E4M3FNUZ()) {\n+      inVecWidthBits = 32;\n+      outVecWidthBits = 16;\n+    }\n \n     std::pair<TypeID, TypeID> key = {srcTy.getTypeID(), dstTy.getTypeID()};\n     if (srcMap.count(key) == 0) {\n       llvm::errs() << \"Unsupported conversion from \" << srcTy << \" to \" << dstTy\n                    << \"\\n\";\n       llvm_unreachable(\"\");\n     }\n+    if (computeCapability < 90 &&\n+        (srcTy.isFloat8E4M3FNUZ() || dstTy.isFloat8E4M3FNUZ())) {\n+      llvm::errs() << \"Conversion from/to f8e4m3nv is only supported on \"\n+                      \"compute capability >= 90\"\n+                   << \"\\n\";\n+      llvm_unreachable(\"\");\n+    }\n     return makeConverterFromPtx(srcMap.lookup(key),\n                                 getTypeConverter()->convertType(srcTy),\n-                                getTypeConverter()->convertType(dstTy));\n+                                getTypeConverter()->convertType(dstTy),\n+                                inVecWidthBits, outVecWidthBits);\n   }\n \n   SmallVector<Value> createDestOps(triton::FpToFpOp op, OpAdaptor adaptor,\n                                    ConversionPatternRewriter &rewriter,\n                                    Type elemTy, MultipleOperandsRange operands,\n                                    Location loc) const {\n-    assert(operands.size() % 4 == 0 &&\n-           \"FP8 casting only support tensors with 4-aligned sizes\");\n     auto srcElementType = getElementType(op.getFrom());\n     auto dstElementType = getElementType(op.getResult());\n+    int numElements = 4;\n+    if (srcElementType.isFloat8E4M3FNUZ() ||\n+        dstElementType.isFloat8E4M3FNUZ()) {\n+      numElements = 2;\n+    }\n+    assert(operands.size() % numElements == 0 &&\n+           \"FP8 casting only support tensors with aligned sizes\");\n     bool isSrcFP32 = srcElementType.isF32();\n     bool isDstFP32 = dstElementType.isF32();\n     auto cvtFunc = getConversionFunc(isSrcFP32 ? f16_ty : srcElementType,\n                                      isDstFP32 ? f16_ty : dstElementType);\n-    SmallVector<Value> inVals = {operands[0][0], operands[1][0], operands[2][0],\n-                                 operands[3][0]};\n+    SmallVector<Value> inVals;\n+    for (unsigned i = 0; i < numElements; i++) {\n+      inVals.push_back(operands[i][0]);\n+    }\n     if (isSrcFP32)\n       for (Value &v : inVals)\n         v = convertFp32ToFp16(loc, rewriter, v);\n-    SmallVector<Value> outVals =\n-        cvtFunc(loc, rewriter, inVals[0], inVals[1], inVals[2], inVals[3]);\n+    SmallVector<Value> outVals = cvtFunc(loc, rewriter, inVals);\n     assert(outVals.size() == inVals.size());\n     if (isDstFP32)\n       for (Value &v : outVals)\n         v = convertFp16ToFp32(loc, rewriter, v);\n     // Pack values\n     return outVals;\n   }\n+\n+private:\n+  int computeCapability;\n };\n \n struct CmpIOpConversion\n@@ -762,15 +717,16 @@ struct CmpFOpConversion\n   }\n };\n \n-template <class T>\n struct ExternElementwiseOpConversion\n-    : public ElementwiseOpConversionBase<T, ExternElementwiseOpConversion<T>> {\n-  using Base = ElementwiseOpConversionBase<T, ExternElementwiseOpConversion<T>>;\n+    : public ElementwiseOpConversionBase<ExternElementwiseOp,\n+                                         ExternElementwiseOpConversion> {\n+  using Base = ElementwiseOpConversionBase<ExternElementwiseOp,\n+                                           ExternElementwiseOpConversion>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n   typedef typename Base::OpAdaptor OpAdaptor;\n \n-  SmallVector<Value> createDestOps(T op, OpAdaptor adaptor,\n+  SmallVector<Value> createDestOps(ExternElementwiseOp op, OpAdaptor adaptor,\n                                    ConversionPatternRewriter &rewriter,\n                                    Type elemTy, MultipleOperandsRange operands,\n                                    Location loc) const {\n@@ -791,8 +747,9 @@ struct ExternElementwiseOpConversion\n     return LLVM::LLVMFunctionType::get(resultType, operandTypes);\n   }\n \n-  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter, T op,\n-                                     StringRef funcName, Type funcType) const {\n+  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter,\n+                                     ExternElementwiseOp op, StringRef funcName,\n+                                     Type funcType) const {\n     using LLVM::LLVMFuncOp;\n \n     auto funcAttr = StringAttr::get(op->getContext(), funcName);\n@@ -811,6 +768,86 @@ struct ExternElementwiseOpConversion\n   }\n };\n \n+struct ElementwiseInlineAsmOpConversion\n+    : public ElementwiseOpConversionBase<ElementwiseInlineAsmOp,\n+                                         ElementwiseInlineAsmOpConversion> {\n+  using Base = ElementwiseOpConversionBase<ElementwiseInlineAsmOp,\n+                                           ElementwiseInlineAsmOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+  typedef typename Base::OpAdaptor OpAdaptor;\n+\n+  // If operand size is smaller than 32bits pack by groups of 32bits.\n+  // Otherwise have separate inputs.\n+  SmallVector<Value> packOperands(ElementwiseInlineAsmOp op,\n+                                  MultipleOperandsRange operands,\n+                                  ConversionPatternRewriter &rewriter,\n+                                  Location loc) const {\n+    SmallVector<Value> packedOperands;\n+    unsigned numPackedElements = op.getPackedElement();\n+    for (int i = 0, e = op.getNumOperands(); i < e; i++) {\n+      unsigned bitWidth =\n+          getElementType(op.getOperand(i)).getIntOrFloatBitWidth();\n+      unsigned numElementPerReg = bitWidth < 32 ? 32 / bitWidth : 1;\n+      numElementPerReg = std::min(numElementPerReg, numPackedElements);\n+      for (int j = 0; j < numPackedElements; j += numElementPerReg) {\n+        if (numElementPerReg == 1) {\n+          packedOperands.push_back(operands[j][i]);\n+          continue;\n+        }\n+        Type t = vec_ty(\n+            getTypeConverter()->convertType(getElementType(op.getOperand(i))),\n+            numElementPerReg);\n+        Value packed = undef(t);\n+        for (int k = 0; k < numElementPerReg; k++) {\n+          packed = insert_element(packed, operands[j + k][i], i32_val(k));\n+        }\n+        packedOperands.push_back(packed);\n+      }\n+    }\n+    return packedOperands;\n+  }\n+\n+  SmallVector<Value> createDestOps(ElementwiseInlineAsmOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    int numPackedElements = op.getPackedElement();\n+    if (operands.size() % numPackedElements != 0)\n+      llvm::report_fatal_error(\"Inline asm op has more packed elements than \"\n+                               \"number of elements per thread.\");\n+    SmallVector<Value> packedOperands =\n+        packOperands(op, operands, rewriter, loc);\n+    Type dstType =\n+        getTypeConverter()->convertType(getElementType(op.getResult()));\n+    Type retType = dstType;\n+    if (numPackedElements > 1)\n+      retType = vec_ty(retType, numPackedElements);\n+    Value result = rewriter\n+                       .create<LLVM::InlineAsmOp>(\n+                           loc, retType,\n+                           packedOperands,      // operands\n+                           op.getAsmString(),   // asm_string\n+                           op.getConstraints(), // constraints\n+                           !op.getPure(),       // has_side_effects\n+                           false,               // is_align_stack\n+                           LLVM::AsmDialectAttr::get(\n+                               rewriter.getContext(),\n+                               LLVM::AsmDialect::AD_ATT), // asm_dialect\n+                           ArrayAttr()                    // operand_attrs\n+                           )\n+                       ->getResult(0);\n+    SmallVector<Value> results;\n+    if (numPackedElements > 1) {\n+      for (int i = 0; i < numPackedElements; i++)\n+        results.push_back(extract_element(result, i32_val(i)));\n+    } else {\n+      results = {result};\n+    }\n+    return results;\n+  }\n+};\n+\n struct FDivOpConversion\n     : ElementwiseOpConversionBase<mlir::arith::DivFOp, FDivOpConversion> {\n   using Base =\n@@ -956,8 +993,9 @@ struct SIToFPOpConversion\n       auto cvtFunc = makeConverterFromPtx(\n           S8_to_Bf16, getTypeConverter()->convertType(inElemTy),\n           getTypeConverter()->convertType(outElemTy));\n-      auto outVals = cvtFunc(loc, rewriter, operands[0][0], operands[1][0],\n-                             operands[2][0], operands[3][0]);\n+      SmallVector<Value> inVals = {operands[0][0], operands[1][0],\n+                                   operands[2][0], operands[3][0]};\n+      auto outVals = cvtFunc(loc, rewriter, inVals);\n       assert(outVals.size() == 4);\n       return outVals;\n     } else if (outElemTy.isBF16()) {\n@@ -1142,7 +1180,9 @@ struct IndexCastOpLowering\n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-    ModuleAllocation &allocation, PatternBenefit benefit) {\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    int computeCapability, PatternBenefit benefit) {\n #define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp)\n@@ -1206,13 +1246,10 @@ void populateElementwiseOpToLLVMPatterns(\n   patterns.add<SIToFPOpConversion>(typeConverter, benefit);\n   patterns.add<IndexCastOpLowering>(typeConverter, benefit);\n \n-  patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n+  patterns.add<FpToFpOpConversion>(typeConverter, computeCapability, benefit);\n \n-  patterns.add<ExternElementwiseOpConversion<triton::PureExternElementwiseOp>>(\n-      typeConverter, benefit);\n-  patterns\n-      .add<ExternElementwiseOpConversion<triton::ImpureExternElementwiseOp>>(\n-          typeConverter, benefit);\n+  patterns.add<ExternElementwiseOpConversion>(typeConverter, benefit);\n+  patterns.add<ElementwiseInlineAsmOpConversion>(typeConverter, benefit);\n   // ExpOpConversionApprox will try using ex2.approx if the input type is\n   // FP32. For other input types, ExpOpConversionApprox will return failure and\n   // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -9,7 +9,9 @@ using namespace mlir::triton;\n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-    ModuleAllocation &allocation, PatternBenefit benefit);\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    int computeCapability, PatternBenefit benefit);\n \n bool isLegalElementwiseOp(Operation *op);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -528,7 +528,7 @@ struct ConvertTritonGPUToLLVM\n     populatePatterns1(populateTritonGPUToLLVMPatterns);\n     populatePatterns1(populateConvertLayoutOpToLLVMPatterns);\n     populatePatterns2(populateDotOpToLLVMPatterns);\n-    populatePatterns2(populateElementwiseOpToLLVMPatterns);\n+    populatePatterns4(populateElementwiseOpToLLVMPatterns);\n     populatePatterns3(populateLoadStoreOpToLLVMPatterns);\n     populatePatterns4(populateReduceOpToLLVMPatterns);\n     populatePatterns1(populateScanOpToLLVMPatterns);\n@@ -837,9 +837,9 @@ struct ConvertTritonGPUToLLVM\n                                       .dyn_cast<MmaEncodingAttr>();\n       if (mmaLayout) {\n         bool isNativeHopperFP8 =\n-            AElType.isFloat8E5M2() || AElType.isFloat8E4M3FN();\n+            AElType.isFloat8E5M2() || AElType.isFloat8E4M3FNUZ();\n         bool isFP8 = isNativeHopperFP8 || AElType.isFloat8E5M2FNUZ() ||\n-                     AElType.isFloat8E4M3FNUZ();\n+                     AElType.isFloat8E4M3FN();\n         if (!isFP8 || (isNativeHopperFP8 && mmaLayout.isHopper()))\n           return;\n         promoteType = builder.getF16Type();"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 18, "deletions": 38, "changes": 56, "file_content_changes": "@@ -498,24 +498,6 @@ struct TritonAtomicRMWPattern\n   }\n };\n \n-template <class T>\n-struct TritonExternElementwisePattern : public OpConversionPattern<T> {\n-  using OpConversionPattern<T>::OpConversionPattern;\n-  using OpConversionPattern<T>::typeConverter;\n-  typedef typename OpConversionPattern<T>::OpAdaptor OpAdaptor;\n-\n-  LogicalResult\n-  matchAndRewrite(T op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<T>(\n-                      op, typeConverter->convertType(op.getType()),\n-                      adaptor.getArgs(), adaptor.getLibname(),\n-                      adaptor.getLibpath(), adaptor.getSymbol()),\n-                  adaptor.getAttributes());\n-    return success();\n-  }\n-};\n-\n template <class Op>\n struct TritonGenericPattern : public OpConversionPattern<Op> {\n   using OpConversionPattern<Op>::OpConversionPattern;\n@@ -695,26 +677,24 @@ class TritonReturnOpPattern : public OpConversionPattern<ReturnOp> {\n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n                             RewritePatternSet &patterns, unsigned numCTAs) {\n   MLIRContext *context = patterns.getContext();\n-  patterns\n-      .insert< // TODO: view should have custom pattern that views the layout\n-          TritonGenericPattern<triton::AdvanceOp>,\n-          TritonGenericPattern<triton::MakeTensorPtrOp>,\n-          TritonGenericPattern<triton::ViewOp>,\n-          TritonGenericPattern<triton::BitcastOp>,\n-          TritonGenericPattern<triton::FpToFpOp>,\n-          TritonGenericPattern<triton::IntToPtrOp>,\n-          TritonGenericPattern<triton::PtrToIntOp>,\n-          TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n-          TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n-          TritonReducePattern, TritonReduceReturnPattern, TritonScanPattern,\n-          TritonScanReturnPattern, TritonTransPattern, TritonExpandDimsPattern,\n-          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n-          TritonStorePattern,\n-          TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n-          TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n-          TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,\n-          TritonFuncOpPattern, TritonReturnOpPattern, TritonCallOpPattern>(\n-          typeConverter, context);\n+  patterns.insert< // TODO: view should have custom pattern that views the\n+                   // layout\n+      TritonGenericPattern<triton::AdvanceOp>,\n+      TritonGenericPattern<triton::MakeTensorPtrOp>,\n+      TritonGenericPattern<triton::ViewOp>,\n+      TritonGenericPattern<triton::BitcastOp>,\n+      TritonGenericPattern<triton::FpToFpOp>,\n+      TritonGenericPattern<triton::IntToPtrOp>,\n+      TritonGenericPattern<triton::PtrToIntOp>,\n+      TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n+      TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n+      TritonGenericPattern<triton::ElementwiseInlineAsmOp>, TritonReducePattern,\n+      TritonReduceReturnPattern, TritonScanPattern, TritonScanReturnPattern,\n+      TritonTransPattern, TritonExpandDimsPattern, TritonMakeRangePattern,\n+      TritonDotPattern, TritonLoadPattern, TritonStorePattern,\n+      TritonGenericPattern<triton::ExternElementwiseOp>, TritonPrintPattern,\n+      TritonAssertPattern, TritonAtomicRMWPattern, TritonFuncOpPattern,\n+      TritonReturnOpPattern, TritonCallOpPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 28, "deletions": 2, "changes": 30, "file_content_changes": "@@ -401,8 +401,10 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n LogicalResult mlir::triton::DotOp::verify() {\n   auto aTy = getOperand(0).getType().cast<RankedTensorType>();\n   auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n-  if (aTy.getElementType() != bTy.getElementType())\n-    return emitError(\"element types of operands A and B must match\");\n+  if (aTy.getElementType().getIntOrFloatBitWidth() !=\n+      bTy.getElementType().getIntOrFloatBitWidth())\n+    return emitError(\n+        \"element types of operands A and B must have same bit width\");\n   auto aEncoding = aTy.getEncoding();\n   auto bEncoding = bTy.getEncoding();\n   if (!aEncoding && !bEncoding)\n@@ -870,5 +872,29 @@ LogicalResult triton::ReturnOp::verify() {\n   return success();\n }\n \n+// -- ElementwiseInlineAsmOp --\n+void ElementwiseInlineAsmOp::getEffects(\n+    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>\n+        &effects) {\n+  if (getPure())\n+    return;\n+  effects.emplace_back(MemoryEffects::Write::get(),\n+                       SideEffects::DefaultResource::get());\n+  effects.emplace_back(MemoryEffects::Read::get(),\n+                       SideEffects::DefaultResource::get());\n+}\n+\n+// -- ExternElementwiseOp --\n+void ExternElementwiseOp::getEffects(\n+    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>\n+        &effects) {\n+  if (getPure())\n+    return;\n+  effects.emplace_back(MemoryEffects::Write::get(),\n+                       SideEffects::DefaultResource::get());\n+  effects.emplace_back(MemoryEffects::Read::get(),\n+                       SideEffects::DefaultResource::get());\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1205,6 +1205,10 @@ void SharedEncodingAttr::print(AsmPrinter &printer) const {\n \n bool MmaEncodingAttr::isVolta() const { return getVersionMajor() == 1; }\n \n+bool MmaEncodingAttr::isTuring() const {\n+  return getVersionMajor() == 2 && getVersionMinor() == 1;\n+}\n+\n bool MmaEncodingAttr::isAmpere() const { return getVersionMajor() == 2; }\n \n bool MmaEncodingAttr::isHopper() const { return getVersionMajor() == 3; }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -24,7 +24,7 @@ using ttg::SliceEncodingAttr;\n // supported\n static int getMMAVersionSafe(int computeCapability, tt::DotOp op) {\n   int baseVersion = 0;\n-  if (computeCapability < 80) {\n+  if (computeCapability < 75) {\n     baseVersion = 1;\n   } else if (computeCapability < 90) {\n     baseVersion = 2;\n@@ -255,10 +255,11 @@ class BlockedToMMA : public mlir::RewritePattern {\n           instrShape, oldAType.getShape(), oldBType.getShape(), retShapePerCTA,\n           isARow, isBRow, mmaV1Counter++);\n     } else if (versionMajor == 2 || versionMajor == 3) {\n+      int versionMinor = computeCapability == 75 ? 1 : 0;\n       auto warpsPerTile = getWarpsPerTile(dotOp, retShapePerCTA, versionMajor,\n                                           numWarps, instrShape);\n       mmaEnc = ttg::MmaEncodingAttr::get(oldRetType.getContext(), versionMajor,\n-                                         0 /*versionMinor*/, warpsPerTile,\n+                                         versionMinor, warpsPerTile,\n                                          CTALayout, instrShape);\n     }\n     auto newRetType = RankedTensorType::get("}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -26,8 +26,8 @@ SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n     SmallVector<unsigned> validN;\n \n     // MMAv3 with larger instruction shape is preferred.\n-    if (eltType.isFloat8E5M2() || eltType.isFloat8E4M3FN() || eltType.isF16() ||\n-        eltType.isBF16() || eltType.isF32()) {\n+    if (eltType.isFloat8E5M2() || eltType.isFloat8E4M3FNUZ() ||\n+        eltType.isF16() || eltType.isBF16() || eltType.isF32()) {\n       validN.assign({256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176,\n                      168, 160, 152, 144, 136, 128, 120, 112, 104, 96,  88,\n                      80,  72,  64,  56,  48,  40,  32,  24,  16,  8});"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -644,9 +644,10 @@ bool CTAPlanner::isElementwiseOp(Operation *op) const {\n                 math::RsqrtOp, math::SqrtOp, math::TanhOp>(op))\n     return true;\n   if (llvm::isa<triton::IntToPtrOp, triton::PtrToIntOp, triton::BitcastOp,\n-                triton::FpToFpOp, triton::AddPtrOp,\n-                triton::PureExternElementwiseOp>(op))\n+                triton::FpToFpOp, triton::AddPtrOp>(op))\n     return true;\n+  if (auto externElementwiseOp = dyn_cast<triton::ExternElementwiseOp>(op))\n+    return externElementwiseOp.getPure();\n   if (llvm::isa<ttg::CmpIOp, ttg::CmpFOp, ttg::SelectOp>(op))\n     return true;\n   return false;"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 11, "deletions": 7, "changes": 18, "file_content_changes": "@@ -760,7 +760,7 @@ void init_triton_ir(py::module &&m) {\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getI64Type();\n            })\n-      .def(\"get_fp8e4_ty\",\n+      .def(\"get_fp8e4nv_ty\",\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getType<mlir::Float8E4M3FNUZType>();\n            })\n@@ -1424,12 +1424,8 @@ void init_triton_ir(py::module &&m) {\n               const std::string &libPath, const std::string &symbol,\n               std::vector<mlir::Value> &argList, mlir::Type retType,\n               bool isPure) -> mlir::Value {\n-             if (isPure)\n-               return self.create<mlir::triton::PureExternElementwiseOp>(\n-                   retType, argList, libName, libPath, symbol);\n-             else\n-               return self.create<mlir::triton::ImpureExternElementwiseOp>(\n-                   retType, argList, libName, libPath, symbol);\n+             return self.create<mlir::triton::ExternElementwiseOp>(\n+                 retType, argList, libName, libPath, symbol, isPure);\n            })\n       // Built-in instruction\n       .def(\"create_get_program_id\",\n@@ -1524,6 +1520,14 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::arith::SelectOp>(condition, trueValue,\n                                                        falseValue);\n            })\n+      .def(\"create_inline_asm\",\n+           [](TritonOpBuilder &self, const std::string &inlineAsm,\n+              const std::string &constraints,\n+              const std::vector<mlir::Value> &values, mlir::Type &type,\n+              bool isPure, int pack) -> mlir::Value {\n+             return self.create<mlir::triton::ElementwiseInlineAsmOp>(\n+                 type, inlineAsm, constraints, isPure, pack, values);\n+           })\n       .def(\"create_print\",\n            [](TritonOpBuilder &self, const std::string &prefix,\n               const std::vector<mlir::Value> &values) -> void {"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 61, "deletions": 4, "changes": 65, "file_content_changes": "@@ -123,6 +123,8 @@ def check_type_supported(dtype, device):\n         cc = torch.cuda.get_device_capability()\n         if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n             pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+        if cc[0] < 9 and (dtype is tl.float8e4nv or dtype == \"float8e4\"):\n+            pytest.skip(\"float8e4 is only supported on NVGPU with cc >= 90\")\n \n \n class MmaLayout:\n@@ -847,7 +849,7 @@ def test_abs(dtype_x, device):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4nv, tl.float8e5])\n def test_abs_fp8(in_dtype, device):\n \n     @triton.jit\n@@ -1379,8 +1381,8 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     extended_exp = ((1 << (tl.float32.primitive_bitwidth - tl.float32.fp_mantissa_width - 1)) - 1) << tl.float32.fp_mantissa_width\n     # special cases, exp is 0b11..1\n-    if dtype in [tl.float8e4, tl.float8e4b15]:\n-        # float8e4m3 does not have infinities\n+    if dtype in [tl.float8e4nv, tl.float8e4b15]:\n+        # float8e4m3nv does not have infinities\n         output[fp == 0b01111111] = torch.nan\n         output[fp == 0b11111111] = torch.nan\n     else:\n@@ -1439,7 +1441,7 @@ def deserialize_fp8(np_data, in_dtype):\n         return np_data\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4nv, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"\n@@ -1448,6 +1450,7 @@ def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n         - conversion tri_fp8 = convert(input=tri_fp16, out=out_dtype) matches the original\n     this is only possible if both conversions are correct\n     \"\"\"\n+    check_type_supported(in_dtype, device)\n     check_type_supported(out_dtype, device)\n \n     @triton.jit\n@@ -3034,6 +3037,60 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n+\n+# -----------------------\n+# test inline asm\n+# -----------------------\n+\n+@pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n+def test_inline_asm(num_ctas, device):\n+    check_cuda_only(device)\n+\n+    @triton.jit\n+    def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        y = tl.load(Y + tl.arange(0, BLOCK))\n+        s = tl.full([BLOCK], n, tl.int32)\n+        z = tl.inline_asm_elementwise(\"shf.l.wrap.b32 $0, $1, $2, $3;\", \"=r,r, r, r\", [x, y, s], dtype=tl.int32, is_pure=True, pack=1)\n+        tl.store(Z + tl.arange(0, BLOCK), z)\n+\n+    shape = (128, )\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str='uint32', rs=rs)\n+    y = numpy_random(shape, dtype_str='uint32', rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    n = 17\n+    z_tri = to_triton(numpy_random(shape, dtype_str='uint32', rs=rs), device=device)\n+    kernel[(1,)](x_tri, y_tri, z_tri, n, BLOCK=shape[0], num_ctas=num_ctas)\n+    y_ref = (y << n) | (x >> (32 - n))\n+    # compare\n+    np.testing.assert_equal(y_ref, to_numpy(z_tri))\n+\n+\n+@pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n+def test_inline_asm_packed(num_ctas, device):\n+    check_cuda_only(device)\n+    \n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        # shift 4x8bits values together.\n+        y = tl.inline_asm_elementwise(\"and.b32 $0, $1, 0x1F1F1F1F; \\\n+                                       shl.b32 $0, $0, 3;\",\n+                                      \"=r,r\", [x,], dtype=tl.int8, is_pure=True, pack=4)\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+    shape = (512, )\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str='uint8', rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(numpy_random(shape, dtype_str='uint8', rs=rs), device=device)\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], num_ctas=num_ctas)\n+    y_ref = x << 3\n+    # compare\n+    np.testing.assert_equal(y_ref, to_numpy(y_tri))\n+\n # -----------------------\n # test control flow\n # -----------------------"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -81,8 +81,11 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8e4\", \"float8e5\"),\n-                                     (\"float8e4\", \"float16\"),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float8e5\"),\n+                                     (\"float8e4nv\", \"float8e4nv\"),\n+                                     (\"float8e5\", \"float8e4nv\"),\n+                                     (\"float8e5\", \"float8e5\"),\n+                                     (\"float8e4nv\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n                                      (\"float16\", \"float32\"),\n                                      (\"float32\", \"float16\"),\n@@ -97,7 +100,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                                      (\"float8e4b15\", \"float16\"),\n                                      (\"float16\", \"float8e4b15\"),\n                                      (\"float8e5\", \"float8e5\"),\n-                                     (\"float8e4\", \"float8e4\"),\n+                                     (\"float8e4nv\", \"float8e4nv\"),\n                                      (\"int8\", \"int8\")]\n         ]\n     ),\n@@ -108,6 +111,8 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8 and (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\"):\n         pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n+    if capability[0] < 9 and (ADTYPE == \"float8e4nv\" or BDTYPE == \"float8e4nv\"):\n+        pytest.skip(\"Only test float8e4nv on devices with sm >= 90\")\n     if (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\") and SPLIT_K != 1:\n         pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n     torch.manual_seed(0)\n@@ -131,7 +136,7 @@ def maybe_upcast(x, dtype, is_float8):\n \n     def init_input(m, n, dtype):\n         if 'float8' in dtype:\n-            ewidth = {'float8e4b15': 4, 'float8e4': 4, 'float8e5': 5}[dtype]\n+            ewidth = {'float8e4b15': 4, 'float8e4nv': 4, 'float8e5': 5}[dtype]\n             sign = torch.randint(2, size=(m, n), device=\"cuda\", dtype=torch.int8) * 128\n             val = torch.randint(2**3 - 1, size=(m, n), device=\"cuda\", dtype=torch.int8) << 7 - ewidth\n             return sign | val"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1062,7 +1062,7 @@ def str_to_ty(name):\n         ty = str_to_ty(name[1:])\n         return language.pointer_type(ty)\n     tys = {\n-        \"fp8e4\": language.float8e4,\n+        \"fp8e4nv\": language.float8e4nv,\n         \"fp8e5\": language.float8e5,\n         \"fp8e4b15\": language.float8e4b15,\n         \"fp8e4b15x4\": language.float8e4b15x4,"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -51,9 +51,10 @@\n     float64,\n     float8e4b15,\n     float8e4b15x4,\n-    float8e4,\n+    float8e4nv,\n     float8e5,\n     function_type,\n+    inline_asm_elementwise,\n     int1,\n     int16,\n     int32,\n@@ -150,10 +151,11 @@\n     \"float64\",\n     \"float8e4b15\",\n     \"float8e4b15x4\",\n-    \"float8e4\",\n+    \"float8e4nv\",\n     \"float8e5\",\n     \"full\",\n     \"function_type\",\n+    \"inline_asm_elementwise\",\n     \"int1\",\n     \"int16\",\n     \"int32\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 50, "deletions": 7, "changes": 57, "file_content_changes": "@@ -76,7 +76,7 @@ def _to_tensor(x, builder):\n class dtype:\n     SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n+    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4nv', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -104,7 +104,7 @@ def __init__(self, name):\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 15\n-            elif name == 'fp8e4':\n+            elif name == 'fp8e4nv':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 7\n@@ -136,15 +136,18 @@ def __init__(self, name):\n     def is_fp8(self):\n         return 'fp8' in self.name\n \n-    def is_fp8e4(self):\n-        return self.name == 'fp8e4'\n+    def is_fp8e4nv(self):\n+        return self.name == 'fp8e4nv'\n \n     def is_fp8e4b15(self):\n         return self.name == 'fp8e4b15'\n \n     def is_fp8e4b15x4(self):\n         return self.name == 'fp8e4b15x4'\n \n+    def is_fp8e5(self):\n+        return self.name == 'fp8e5'\n+\n     def is_fp16(self):\n         return self.name == 'fp16'\n \n@@ -244,8 +247,8 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_int64_ty()\n         elif self.name == 'fp8e5':\n             return builder.get_fp8e5_ty()\n-        elif self.name == 'fp8e4':\n-            return builder.get_fp8e4_ty()\n+        elif self.name == 'fp8e4nv':\n+            return builder.get_fp8e4nv_ty()\n         elif self.name == 'fp8e4b15':\n             return builder.get_fp8e4b15_ty()\n         elif self.name == 'fp8e4b15x4':\n@@ -382,7 +385,7 @@ def to_ir(self, builder: ir.builder):\n uint32 = dtype('uint32')\n uint64 = dtype('uint64')\n float8e5 = dtype('fp8e5')\n-float8e4 = dtype('fp8e4')\n+float8e4nv = dtype('fp8e4nv')\n float8e4b15 = dtype('fp8e4b15')\n float8e4b15x4 = dtype('fp8e4b15x4')\n float16 = dtype('fp16')\n@@ -1788,6 +1791,46 @@ def device_assert(cond, msg=\"\", _builder=None):\n     return semantic.device_assert(_to_tensor(cond, _builder), msg, file_name, func_name, lineno, _builder)\n \n \n+@builtin\n+def inline_asm_elementwise(asm: str, constraints: str, args: list, dtype, is_pure: bool, pack: int, _builder=None):\n+    '''\n+        Execute the inline assembly to a packed of elements of the tensor\n+        :param asm: assembly to be inlined, it has to match the target assembly format\n+        :param constraints: string representing the mapping of operands to register\n+        :param args: the arguments of the operation\n+        :param dtype: the element type of the returned variable\n+        :param is_pure: whether the operation is pure\n+        :param pack: the number of elements to be processed by one instance of inline assembly\n+        :param _builder: the builder\n+        :return: the return value of the function\n+    '''\n+    dispatch_args = args.copy()\n+    asm = _constexpr_to_value(asm)\n+    constraints = _constexpr_to_value(constraints)\n+    pack = _constexpr_to_value(pack)\n+    is_pure = _constexpr_to_value(is_pure)\n+    ret_shape = None\n+    arg_types = []\n+    for i in range(len(dispatch_args)):\n+        dispatch_args[i] = _to_tensor(dispatch_args[i], _builder)\n+        arg_types.append(dispatch_args[i].dtype)\n+    if len(arg_types) > 0:\n+        arg_types = tuple(arg_types)\n+        broadcast_arg = dispatch_args[0]\n+        # Get the broadcast shape over all the arguments\n+        for i, item in enumerate(dispatch_args):\n+            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n+                item, broadcast_arg, _builder, arithmetic_check=False)\n+        # Change the shape of each argument based on the broadcast shape\n+        for i in range(len(dispatch_args)):\n+            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n+                dispatch_args[i], broadcast_arg, _builder, arithmetic_check=False)\n+    ret_shape = broadcast_arg.shape\n+    res_ty = block_type(dtype, ret_shape).to_ir(_builder)\n+    call = _builder.create_inline_asm(asm, constraints, [t.handle for t in args], res_ty, is_pure, pack)\n+    return tensor(call, block_type(dtype, ret_shape))\n+\n+\n # -----------------------\n # Iterators\n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 28, "deletions": 5, "changes": 33, "file_content_changes": "@@ -1,6 +1,5 @@\n from __future__ import annotations  # remove after python 3.11\n \n-import warnings\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n@@ -685,9 +684,8 @@ def cast(input: tl.tensor,\n     dst_sca_ty = dst_ty.scalar\n \n     if _is_cuda(builder.arch) and builder.arch < 89 and \\\n-       (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n-        warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n-                      \"Please use tl.float8e4b15.\", DeprecationWarning)\n+       (src_sca_ty.is_fp8e4nv() or dst_sca_ty.is_fp8e4nv()):\n+        assert False, \"fp8e4nv data type is not supported on CUDA arch < 89\"\n \n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n     if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n@@ -1246,8 +1244,33 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n+    def assert_dtypes_valid(lhs_dtype, rhs_dtype, arch):\n+        # Checks for non-cuda archs\n+        if not _is_cuda(builder.arch):\n+            assert lhs_dtype == rhs_dtype, f\"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!\"\n+            return\n+        # Checks for cuda arch\n+        if arch < 90:\n+            assert not lhs_dtype.is_fp8e4nv() and not rhs_dtype.is_fp8e4nv(), \"Dot op does not support fp8e4nv on CUDA arch < 90\"\n+            assert lhs_dtype == rhs_dtype, f\"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!\"\n+        else:\n+            assert not lhs_dtype.is_fp8e4b15() and not rhs_dtype.is_fp8e4b15(), \"Dot op does not support fp8e4b15 on CUDA arch >= 90\"\n+            assert not lhs_dtype.is_fp8e4b15x4() and not rhs_dtype.is_fp8e4b15x4(), \"Dot op does not support fp8e4b15x4 on CUDA arch >= 90\"\n+            if lhs_dtype.is_int() or rhs_dtype.is_int():\n+                assert lhs_dtype == rhs_dtype, f\"Both operands must be same type. First operand ({lhs_dtype}) and second operand ({rhs_dtype})\"\n+                assert lhs_dtype.is_int8() or lhs_dtype.is_uint8(), f\"Both operands must be either int8 or uint8. Operand type ({lhs_dtype})\"\n+            elif lhs_dtype.is_fp8() or rhs_dtype.is_fp8():\n+                assert lhs_dtype.is_fp8e4nv() or lhs_dtype.is_fp8e5(), f\"Only supports fp8e4nv or fp8e5. First operand ({lhs_dtype})\"\n+                assert rhs_dtype.is_fp8e4nv() or rhs_dtype.is_fp8e5(), f\"Only supports fp8e4nv or fp8e5. Second operand ({rhs_dtype})\"\n+            else:\n+                assert lhs_dtype.is_fp16() or lhs_dtype.is_bf16() or lhs_dtype.is_fp32() or lhs_dtype.is_int1(), f\"Unsupported dtype {lhs_dtype}\"\n+                assert rhs_dtype.is_fp16() or rhs_dtype.is_bf16() or rhs_dtype.is_fp32() or rhs_dtype.is_int1(), f\"Unsupported dtype {rhs_dtype}\"\n+                assert lhs_dtype == rhs_dtype, f\"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!\"\n+\n     assert lhs.type.is_block() and rhs.type.is_block()\n-    assert lhs.dtype == rhs.dtype, f\"First input ({lhs.dtype}) and second input ({rhs.dtype}) must have the same dtype!\"\n+\n+    assert_dtypes_valid(lhs.dtype, rhs.dtype, builder.arch)\n+\n     assert len(lhs.shape) == 2, f\"First input shape ({lhs.shape}) is not two dimensional!\"\n     assert len(rhs.shape) == 2, f\"Second input shape ({rhs.shape}) is not two dimensional!\"\n     assert lhs.shape[1].value == rhs.shape[0].value, f\"First input shape ({lhs.shape}) and second input shape {rhs.shape} are not compatible for matmul (second index of first shape ({lhs.shape[1].value}) must be equal to first index of second shape ({rhs.shape[0].value})\""}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 8, "deletions": 5, "changes": 13, "file_content_changes": "@@ -82,7 +82,7 @@ def _kernel(A, B, C, M, N, K,\n             stride_cm, stride_cn,\n             dot_out_dtype: tl.constexpr,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n-            GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n+            GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr, AB_DTYPE: tl.constexpr\n             ):\n     # matrix multiplication\n     pid = tl.program_id(0)\n@@ -114,7 +114,7 @@ def _kernel(A, B, C, M, N, K,\n             _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n             a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n             b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n-        if a.dtype != b.dtype:\n+        if AB_DTYPE:\n             a = a.to(C.dtype.element_ty)\n             b = b.to(C.dtype.element_ty)\n         acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n@@ -151,8 +151,8 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        if a.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5] or\\\n-           b.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+        if a.dtype in [tl.float8e4nv, tl.float8e4b15, tl.float8e5] or\\\n+           b.dtype in [tl.float8e4nv, tl.float8e4b15, tl.float8e5]:\n             c_dtype = torch.float16\n         else:\n             c_dtype = get_higher_dtype(a.dtype, b.dtype)\n@@ -170,14 +170,17 @@ def _call(a, b, dot_out_dtype):\n                 dot_out_dtype = tl.float32\n             else:\n                 dot_out_dtype = tl.int32\n+        ab_dtype = True\n+        if a.dtype in [tl.float8e4nv, tl.float8e5] and b.dtype in [tl.float8e4nv, tl.float8e5]:\n+            ab_dtype = False\n         # launch kernel\n         grid = lambda META: (cdiv(M, META['BLOCK_M']) * cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n         _kernel[grid](a, b, c, M, N, K,\n                       a.stride(0), a.stride(1),\n                       b.stride(0), b.stride(1),\n                       c.stride(0), c.stride(1),\n                       dot_out_dtype=dot_out_dtype,\n-                      GROUP_M=8)\n+                      GROUP_M=8, AB_DTYPE=ab_dtype)\n         return c\n \n     @staticmethod"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -245,7 +245,7 @@ def _type_of(key):\n         dtype_str = str(key).split(\".\")[-1]\n         tys = {\n             \"bool\": \"i1\",\n-            \"float8e4\": \"fp8e4\",\n+            \"float8e4nv\": \"fp8e4nv\",\n             \"float8e5\": \"fp8e5\",\n             \"float8e4b15\": \"fp8e4b15\",\n             \"float8e4b15x4\": \"fp8e4b15x4\","}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -381,7 +381,7 @@ def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None)\n             ops_per_sub_core = 256\n         elif dtype in [torch.float16, torch.bfloat16, torch.int16]:\n             ops_per_sub_core = 512\n-        elif dtype in [torch.int8, tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+        elif dtype in [torch.int8, tl.float8e4nv, tl.float8e4b15, tl.float8e5]:\n             ops_per_sub_core = 1024\n         else:\n             raise RuntimeError(\"dtype not supported\")"}, {"filename": "test/Conversion/invalid.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   tt.func @convert_dot(%A: tensor<16x16xf32, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n-    // expected-error@+1 {{element types of operands A and B must match}}\n+    // expected-error@+1 {{element types of operands A and B must have same bit width}}\n     %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n         tensor<16x16xf32, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n     tt.return"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -201,5 +201,12 @@ tt.func @scan_op(%ptr: tensor<1x2x4x!tt.ptr<f32>>, %v : tensor<1x2x4xf32>) {\n   }) : (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n   tt.store %ptr, %a : tensor<1x2x4xf32>\n   tt.return\n+}\n \n+// CHECK-LABEL: inline_asm\n+// CHECK: tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\"\n+tt.func @inline_asm(%0: tensor<512xi8>) {\n+  %1 = tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\"\n+    {constraints = \"=r,r\", packed_element = 4 : i32, pure = true} %0 : tensor<512xi8> -> tensor<512xi8>\n+  tt.return\n }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -1396,3 +1396,42 @@ module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 2 :\n     tt.return\n   }\n }\n+\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 80 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: inline_asm\n+  tt.func public @inline_asm(%arg0: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked>\n+    %1 = tt.splat %arg0 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %2 = tt.addptr %1, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512xi8, #blocked>\n+// CHECK: %{{.*}} = llvm.inline_asm asm_dialect = att \"shl.b32 $0, $0, 3;\", \"=r,r\" %{{.*}} : (vector<4xi8>) -> vector<4xi8>\n+    %4 = tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\" {constraints = \"=r,r\", packed_element = 4 : i32, pure = true} %3 : tensor<512xi8, #blocked> -> tensor<512xi8, #blocked>\n+    %5 = tt.splat %arg1 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %6 = tt.addptr %5, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<512xi8, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 80 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: inline_asm_pack_16bit\n+  tt.func public @inline_asm_pack_16bit(%arg0: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked>\n+    %1 = tt.splat %arg0 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %2 = tt.addptr %1, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512xi8, #blocked>\n+// CHECK: %{{.*}} = llvm.inline_asm asm_dialect = att \"shl.b16 $0, $0, 3;\", \"=h,h\" %{{.*}} : (vector<2xi8>) -> vector<2xi8>\n+    %4 = tt.elementwise_inline_asm \"shl.b16 $0, $0, 3;\" {constraints = \"=h,h\", packed_element = 2 : i32, pure = true} %3 : tensor<512xi8, #blocked> -> tensor<512xi8, #blocked>\n+    %5 = tt.splat %arg1 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %6 = tt.addptr %5, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<512xi8, #blocked>\n+    tt.return\n+  }\n+}"}]