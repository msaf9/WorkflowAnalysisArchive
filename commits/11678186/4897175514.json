[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 46, "deletions": 0, "changes": 46, "file_content_changes": "@@ -1543,7 +1543,53 @@ def test_store_op(M, src_layout, device='cuda'):\n \n     pgm = store_kernel[(1, 1, 1)](x_tri, y_tri)\n     y_ref = x\n+    np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n+layouts = [\n+    BlockedLayout([1, 4], [1, 32], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [1, 32], [2, 2], [1, 0]),\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1])\n+]\n+\n+\n+@pytest.mark.parametrize(\"M\", [64, 128, 256])\n+@pytest.mark.parametrize(\"src_layout\", layouts)\n+@pytest.mark.parametrize(\"dst_layout\", layouts)\n+@pytest.mark.parametrize(\"src_dim\", [0, 1])\n+@pytest.mark.parametrize(\"dst_dim\", [0, 1])\n+def test_convert_layout(M, src_layout, dst_layout, src_dim, dst_dim, device='cuda'):\n+    ir = f\"\"\"\n+    #dst = {dst_layout}\n+    #src = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n+        tt.func public @kernel(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n+            %0 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %1 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %2 = tt.addptr %0, %1 : tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>\n+            %4 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %6 = tt.addptr %4, %5 : tensor<{M}x!tt.ptr<i32>, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            %7 = triton_gpu.convert_layout %3 : (tensor<{M}xi32, #triton_gpu.slice<{{dim = {src_dim}, parent = #src}}>>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            tt.store %6, %7 : tensor<{M}xi32, #triton_gpu.slice<{{dim = {dst_dim}, parent = #dst}}>>\n+            tt.return\n+        }}\n+    }}\n+    \"\"\"\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n \n+    rs = RandomState(17)\n+    x = rs.randint(0, 4, (M, )).astype('int32')\n+    y = np.zeros((M, ), dtype='int32')\n+    x_tri = torch.tensor(x, device=device)\n+    y_tri = torch.tensor(y, device=device)\n+    pgm = kernel[(1, 1, 1)](x_tri, y_tri)\n+    y_ref = x\n     np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n "}]