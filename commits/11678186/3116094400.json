[{"filename": "python/tests/test_math_ops.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -23,10 +23,11 @@ def math_kernel(x1_ptr, x2_ptr, x3_ptr, x4_ptr, n, BLOCK_SIZE: tl.constexpr):\n \n \n def test_empty_kernel_cubin_compile():\n-    kernel = triton.compile(math_kernel,\n-                            \"*fp32,*fp32,*fp32,*fp32,i32\",\n-                            device=0,\n-                            constants={\"BLOCK_SIZE\": 256})  # \"cubin\"\n+    kernel = triton.compiler._compile(math_kernel,\n+                                     \"*fp32,*fp32,*fp32,*fp32,i32\",\n+                                     device=0,\n+                                     constants={\"BLOCK_SIZE\": 256},\n+                                     output=\"ttgir\")  # \"cubin\"\n     assert kernel\n     # TODO: Check if the values are correct.\n     # TODO: Cover all the math operators"}, {"filename": "python/tests/test_transpose.py", "status": "modified", "additions": 1, "deletions": 21, "changes": 22, "file_content_changes": "@@ -40,29 +40,9 @@ def kernel(x_ptr, stride_xm,\n     [2, 128, 64]\n ])\n def test_convert_layout_impl(NUM_WARPS, SIZE_M, SIZE_N):\n-    # TODO: this is to initialize the cuda context since it is not properly\n-    #       dealed with in the existing runtime, remove this when the runtime\n-    #       is updated\n-    torch.zeros([10], device=torch.device('cuda'))\n-    device = torch.cuda.current_device()\n-    binary = runtime.build_kernel(kernel,\n-                                  \"*fp32,i32,*fp32,i32\",\n-                                  constants={\"SIZE_M\": SIZE_M,\n-                                             \"SIZE_N\": SIZE_N},\n-                                  num_warps=NUM_WARPS,\n-                                  num_stages=3)\n     grid = lambda META: (1, )\n-\n     x = torch.randn((SIZE_M, SIZE_N), device='cuda', dtype=torch.float32)\n     z = torch.empty((SIZE_N, SIZE_M), device=x.device, dtype=x.dtype)\n-    runtime.launch_kernel(kernel=binary,\n-                          device=device,\n-                          grid=grid,\n-                          x_ptr=x,\n-                          stride_xm=x.stride(0),\n-                          z_ptr=z,\n-                          stride_zn=z.stride(0),\n-                          SIZE_M=tl.constexpr(SIZE_M),\n-                          SIZE_N=tl.constexpr(SIZE_N))\n+    kernel[grid](x_ptr=x, stride_xm=x.stride(0), z_ptr=z, stride_zn=z.stride(0), SIZE_M=SIZE_M, SIZE_N=SIZE_N, num_warps=NUM_WARPS)\n     golden_z = torch.t(x)\n     assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)"}, {"filename": "python/tests/test_vecadd_no_scf.py", "status": "modified", "additions": 3, "deletions": 15, "changes": 18, "file_content_changes": "@@ -22,27 +22,15 @@ def kernel(x_ptr,\n         z_ptrs = z_ptr + offset\n         tl.store(z_ptrs, z)\n \n-    torch.zeros([10], device=torch.device('cuda'))\n-    device = torch.cuda.current_device()\n-    binary = runtime.build_kernel(kernel, \"*fp32,*fp32,*fp32,i32\",\n-                                  constants={\"BLOCK_SIZE_N\": block_size},\n-                                  num_warps=num_warps,\n-                                  num_stages=3)\n+    \n \n     x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n     y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n     z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n \n-    assert x.shape.numel() % block_size == 0, \"Only test load without mask here\"\n     grid = lambda EA: (x.shape.numel() // block_size,)\n-\n-    runtime.launch_kernel(kernel=binary,\n-                          grid=grid,\n-                          device=device,\n-                          x_ptr=x,\n-                          y_ptr=y,\n-                          z_ptr=z,\n-                          BLOCK_SIZE_N=tl.constexpr(block_size))\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, BLOCK_SIZE_N=block_size, num_warps=num_warps)\n+    \n     golden_z = x + y\n     assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n "}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -877,6 +877,8 @@ def ptx_get_kernel_name(ptx: str) -> str:\n instance_descriptor = namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"], defaults=[set(), set()])\n \n def _compile(fn, signature: str, device: int = -1, constants=dict(), specialization=instance_descriptor(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, output: str = \"ttgir\") -> Tuple[str, int, str]:\n+    if isinstance(signature, str):\n+      signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n     valid_outputs = (\"ttir\", \"ttgir\", \"ptx\", \"cubin\")\n     assert output in valid_outputs, \"output should be one of [%s], but get \\\"%s\\\"\" % (','.join(valid_outputs), output)\n \n@@ -885,6 +887,7 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     module = optimize_triton_ir(module)\n     if output == \"ttir\":\n         return module.str()\n+    \n \n     # tritongpu-ir\n     module = make_tritongpu_ir(module, num_warps)"}]