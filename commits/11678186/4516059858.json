[{"filename": ".pre-commit-config.yaml", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -4,7 +4,8 @@ repos:\n       - id: isort\n         name: isort\n         entry: isort\n-        args: [\"./python\"]\n+        files: \"python/.*\"\n+        exclude: \"python/build/.*\"\n         language: python\n         types: [python]\n         additional_dependencies:\n@@ -13,7 +14,9 @@ repos:\n       - id: autopep8\n         name: autopep8\n         entry: autopep8\n-        args: [\"-a\", \"-r\", \"-i\", \"./python/\"]\n+        args: [\"-a\", \"-r\", \"-i\"]\n+        files: \"python/.*\"\n+        exclude: \"python/build/.*\"\n         language: python\n         types: [python]\n         additional_dependencies:"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -23,8 +23,9 @@ void MembarAnalysis::resolve(Operation *operation, OpBuilder *builder) {\n       // Check if the operation belongs to scf dialect, if so, we need to\n       // throw an error\n       if (op.getDialect()->getNamespace() == \"scf\") {\n-        op.emitError(\"scf dialect is not supported in membar. Please lower it \"\n-                     \"to cf dialect first.\");\n+        llvm::report_fatal_error(\n+            \"scf dialect is not supported in membar. Please lower it \"\n+            \"to cf dialect first.\");\n         return;\n       }\n     }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file --convert-scf-to-cf --convert-triton-gpu-to-llvm | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm | FileCheck %s\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.func @test_empty_kernel(%arg0: i64, %arg1: !llvm.ptr<f16, 1>)\n@@ -1136,11 +1136,13 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: test_index_cache_different_block\n   func.func @test_index_cache_different_block(%arg0: tensor<128x32xf32, #blocked0>, %arg1: i1) {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n+    // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n-    scf.if %arg1 {\n-      // CHECK-NOT: nvvm.read.ptx.sreg.tid.x\n+    cf.cond_br %arg1, ^bb1, ^bb2\n+    ^bb1:  // pred: ^bb0\n       %1 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n-    }\n-    return\n+      cf.br ^bb2\n+    ^bb2:  // 2 preds: ^bb0, ^bb1\n+      return\n   }\n }"}]