[{"filename": ".github/workflows/compare-artifacts.yml", "status": "modified", "additions": 72, "deletions": 2, "changes": 74, "file_content_changes": "@@ -9,7 +9,77 @@ on:\n jobs:\n   Compare-artifacts:\n     runs-on: ubuntu-latest\n+    if: ${{ github.event.workflow_run.conclusion == 'success' }}\n \n     steps:\n-      - name: Echo\n-        run: echo \"Testing new workflow\"\n+      - name: Download PR number artifact\n+        uses: actions/github-script@v6\n+        with:\n+          script: |\n+            let allArtifacts = await github.rest.actions.listWorkflowRunArtifacts({\n+               owner: context.repo.owner,\n+               repo: context.repo.repo,\n+               run_id: context.payload.workflow_run.id,\n+            });\n+            let matchArtifact = allArtifacts.data.artifacts.filter((artifact) => {\n+              return artifact.name == \"pr_number\"\n+            })[0];\n+            let download = await github.rest.actions.downloadArtifact({\n+               owner: context.repo.owner,\n+               repo: context.repo.repo,\n+               artifact_id: matchArtifact.id,\n+               archive_format: 'zip',\n+            });\n+            let fs = require('fs');\n+            fs.writeFileSync(`${process.env.GITHUB_WORKSPACE}/pr_number.zip`, Buffer.from(download.data));\n+      - name: Download comparison result artifact\n+        uses: actions/github-script@v6\n+        with:\n+          script: |\n+            let allArtifacts = await github.rest.actions.listWorkflowRunArtifacts({\n+               owner: context.repo.owner,\n+               repo: context.repo.repo,\n+               run_id: context.payload.workflow_run.id,\n+            });\n+            let matchArtifact = allArtifacts.data.artifacts.filter((artifact) => {\n+              return artifact.name == \"comparison_result\"\n+            })[0];\n+            let download = await github.rest.actions.downloadArtifact({\n+               owner: context.repo.owner,\n+               repo: context.repo.repo,\n+               artifact_id: matchArtifact.id,\n+               archive_format: 'zip',\n+            });\n+            let fs = require('fs');\n+            fs.writeFileSync(`${process.env.GITHUB_WORKSPACE}/comparison_result.zip`, Buffer.from(download.data));\n+      - name: Unzip artifacts\n+        run: |\n+          unzip pr_number.zip\n+          unzip comparison_result.zip\n+      - name: Print artifacts\n+        uses: actions/github-script@v6\n+        with:\n+          script: |\n+            let fs = require('fs');\n+            let pr_number = Number(fs.readFileSync('./pr_number'));\n+            let comparison_result = fs.readFileSync('./comparison_result', 'utf8');\n+            console.log(\"PR number = \", pr_number);\n+            console.log(\"Comparison result = \", comparison_result);\n+      - name: Comment on PR\n+        uses: actions/github-script@v6\n+        with:\n+          github-token: ${{ secrets.GITHUB_TOKEN }}\n+          script: |\n+            let fs = require('fs');\n+            let run_id = context.payload.workflow_run.id;\n+            let issue_number = Number(fs.readFileSync('./pr_number'));\n+            let comparison_result = fs.readFileSync('./comparison_result', 'utf8');\n+            const message = `:warning: **This PR does not produce bitwise identical kernels as the branch it's merged against.** Please check artifacts for details. [Download the output file here](https://github.com/${{ github.repository }}/actions/runs/${run_id}).`;\n+            if (comparison_result.trim() !== 'SUCCESS') {\n+              await github.rest.issues.createComment({\n+                owner: context.repo.owner,\n+                repo: context.repo.repo,\n+                issue_number: issue_number,\n+                body: message\n+              });\n+            }"}, {"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 27, "deletions": 24, "changes": 51, "file_content_changes": "@@ -62,6 +62,11 @@ jobs:\n         run: |\n           echo \"PATH=${HOME}/.local/bin:${PATH}\" >> \"${GITHUB_ENV}\"\n \n+      - name: Check pre-commit\n+        run: |\n+          python3 -m pip install --upgrade pre-commit\n+          python3 -m pre_commit run --all-files --verbose\n+\n       - name: Install Triton\n         if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n@@ -258,6 +263,17 @@ jobs:\n           sudo apt update\n           sudo apt install gh\n \n+      - name: Save PR number to a file\n+        env:\n+          PR_NUMBER: ${{ github.event.number }}\n+        run: |\n+          echo $PR_NUMBER > pr_number\n+      - name: Upload PR number to artifacts\n+        uses: actions/upload-artifact@v3\n+        with:\n+          name: pr_number\n+          path: pr_number\n+\n       - name: Download latest main artifacts\n         env:\n           ARTIFACT_NAME: artifacts A100\n@@ -353,33 +369,20 @@ jobs:\n             echo \"Error while comparing artifacts\"\n             echo \"COMPARISON_RESULT=error\" >> $GITHUB_ENV\n           fi\n-      - name: Check exit code and handle failure\n-        if: ${{ env.COMPARISON_RESULT == 'error' }}\n+      - name: Check comparison result and write to file\n         run: |\n-          echo \"Error while comparing artifacts\"\n-          exit 1\n-      - name: Fetch Run ID\n-        id: get_run_id\n-        run: echo \"RUN_ID=${{ github.run_id }}\" >> $GITHUB_ENV\n-\n+          if [ \"${{ env.COMPARISON_RESULT }}\" = \"true\" ]; then\n+            echo \"SUCCESS\" > comparison_result\n+          else\n+            echo \"FAILED\" > comparison_result\n+          fi\n+      - name: Upload comparison result to artifacts\n+        uses: actions/upload-artifact@v3\n+        with:\n+          name: comparison_result\n+          path: comparison_result\n       - name: Upload results as artifact\n         uses: actions/upload-artifact@v2\n         with:\n           name: kernels-reference-check\n           path: kernels_reference_check.txt\n-\n-      - name: Check output and comment on PR\n-        if: ${{ env.COMPARISON_RESULT == 'false' }}\n-        uses: actions/github-script@v5\n-        with:\n-          github-token: ${{ secrets.CI_ACCESS_TOKEN }}\n-          script: |\n-            const run_id = ${{ env.RUN_ID }};\n-            const issue_number = context.payload.pull_request.number;\n-            const message = `:warning: **This PR does not produce bitwise identical kernels as the branch it's merged against.** Please check artifacts for details. [Download the output file here](https://github.com/${{ github.repository }}/actions/runs/${run_id}).`;\n-            await github.rest.issues.createComment({\n-                owner: context.repo.owner,\n-                repo: context.repo.repo,\n-                issue_number: issue_number,\n-                body: message\n-            });"}, {"filename": "README.md", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "file_content_changes": "@@ -9,6 +9,32 @@\n ------------------- |\n [![Documentation](https://github.com/openai/triton/actions/workflows/documentation.yml/badge.svg)](https://triton-lang.org/)\n \n+# Triton Developer Conference Registration Open\n+The Triton Developer Conference will be held in a hybrid mode at the Microsoft Silicon Valley Campus in Mountain View, California. The conference will be held on September 20th from 10am to 4pm, followed by a reception till 5:30 pm. Please use the link below to register to attend either in-person or virtually online.\n+\n+Registration Link for Triton Developer Conference is [here](https://forms.office.com/r/m4jQXShDts)\n+\n+Tentative Agenda for the conference (subject to change):\n+\n+|Time    |Title  |Speaker\n+|--------|-------|-------|\n+|10:00 AM|Welcome|Kevin Scott (Microsoft)|\n+|10:20 AM|The Triton Compiler: Past, Present and Future|Phil Tillet (OpenAI)|\n+|11:00 AM|**Break**||\n+|11:20 AM|Hopper support in Triton|Gustav Zhu (Nvidia)|\n+|11:40 AM|Bringing Triton to AMD GPUs|Jason Furmanek, Lixun Zhang (AMD)|\n+|12:00 PM|Intel XPU Backend for Triton|Eikan Wang (Intel)|\n+|12:20 PM|Vectorization of Triton Kernels for Qualcomm Hexagon Backend|Javed Absar (Qualcomm)|\n+|12:30 PM|**Lunch**||\n+|1:40 PM |Triton for MTIA|Roman Levenstein et al, (Meta)|\n+|2:00 PM |Using Triton IR for high-performance fusions in XLA|George Karpenkov (Google)|\n+|2:20 PM |Triton for All: Triton as a device-independent language|Ian Bearman (Microsoft)|\n+|2:40 PM|**Break**||\n+|3:00 PM|PyTorch 2.0 and TorchInductor|Jason Ansel, Horace He (Meta)|\n+|3:20 PM|Pallas: A JAX Kernel Language|Sharad Vikram (Google)|\n+|3:40 PM|Writing Grouped GEMMs in Triton|Vinod Grover (Nvidia)|\n+|4:00 PM|**Reception**||\n+\n \n # Triton\n "}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -124,7 +124,7 @@ LogicalResult tritonTranslateMain(int argc, char **argv,\n   llvm::LLVMContext llvmContext;\n   mlir::triton::gpu::TMAMetadataTy tmaInfos;\n   auto llvmir = translateTritonGPUToLLVMIR(\n-      &llvmContext, *module, SMArch.getValue(), tmaInfos, false /*isRocm*/);\n+      &llvmContext, *module, SMArch.getValue(), tmaInfos, Target::Default);\n \n   if (!llvmir) {\n     llvm::errs() << \"Translate to LLVM IR failed\";"}, {"filename": "docs/conf.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -101,11 +101,12 @@ def documenter(app, obj, parent):\n     'gallery_dirs': 'getting-started/tutorials',\n     'filename_pattern': '',\n     # XXX: Temporarily disable fused attention tutorial on V100\n-    'ignore_pattern': r'__init__\\.py',\n+    'ignore_pattern': r'(__init__\\.py|09.*\\.py|10.*\\.py)',\n     'within_subsection_order': FileNameSortKey,\n     'reference_url': {\n         'sphinx_gallery': None,\n-    }\n+    },\n+    'abort_on_example_error': True,\n }\n \n # Add any paths that contain templates here, relative to this directory.\n@@ -144,7 +145,7 @@ def documenter(app, obj, parent):\n #\n # This is also used if you do content translation via gettext catalogs.\n # Usually you set \"language\" from the command line for these cases.\n-language = None\n+language = 'en'\n \n # List of patterns, relative to source directory, that match files and\n # directories to ignore when looking for source files."}, {"filename": "docs/meetups/08-22-2023.md", "status": "modified", "additions": 31, "deletions": 2, "changes": 33, "file_content_changes": "@@ -8,5 +8,34 @@\n 2. Triton release plan update\n 3. Linalg updates\n 4. Intel GPU Backend status update.\n-2. Intel working on the CPU backend for Triton.\n-4. Open discussion\n+5. Intel working on the CPU backend for Triton.\n+6. AMD updates\n+7. Open discussion\n+\n+##### Minutes:\n+Recording link [here](https://drive.google.com/file/d/19Nnc0i7zUyn-ni2RSFHbPHHiPkYU96Mz/view)\n+\n+1. H100 updates:\n+   - Preliminary support is merged, disabled by default, can be enabled with env variables\n+   - Supports latest tensor cores, FP8s. Support for Flash Attention on the main branch coming soon.\n+   - Performance is very good on Matmuls, 80-90% of cublas on large Matmuls right now, will eventually reach parity with cublas. Above 600 teraflops on fp16 on xxm card, cublas is 670 on random input data. FP8 is twice that, around 1.2 petaflops.\n+   - Hopper support includes the full FP8 support for compute.\n+2. Triton release plan update\n+   - No specific dates for now, plan is to release before end of 2023.\n+   - Will move to 3.0 release due to minor backward compatibility breaking changes. For eg. Will move compiler options in the indexing operators as hardcoded operators in the kernel, will bump the major version.\n+   - Functionally the main goal will be to have 3rd party plugins for Intel and AMD gpus.\n+   - May synchronise with a PyTorch release so that PyTorch can benefit from the latest features, however continuous integration workflow is the default release cadence expected.\n+   - Will switch the default behavior to optimized mode for the release, needs more discussion with Nvidia.\n+   - Will expose flags for a user to enable kernel selection themselves.\n+   - Open question: Pytorch hasn\u2019t rebased to latest triton, it is close to PyTorch code freeze \u2013 will PyTorch still sync with Triton 2.0? Will we have another release to support triton 2.0?\n+   - Community can start with the latest stable branch and rebase 3rd party plugin on top of that. OAI has no resources to commit to, but community can contribute.\n+3. Linalg updates\n+   - Discussion on Github for Linalg as a middle layer between the language and target hardware. Includes support for block pointers and modulo operators.\n+   - Please join the conversation [here](https://github.com/openai/triton/discussions/1842)\n+   - Branch pushed is behind the tip, will work on getting it caught up on the tip.\n+4. Intel GPU Backend status update.\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Intel%20XPU%20Backend%20for%20Triton%20-%20Update%20-%200823.pptx)\n+5. Intel working on the CPU backend for Triton.\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Intel%20XPU%20Backend%20for%20Triton%20-%20Update%20-%200823.pptx)\n+6. AMD updates\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Triton_AMD_update_0823.pdf)."}, {"filename": "docs/meetups/Intel XPU Backend for Triton - Update - 0823.pptx", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "docs/meetups/Triton_AMD_update_0823.pdf", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/Passes.td", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -30,9 +30,13 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n         Option<\"tmaMetadata\", \"tma-metadata\",\n                \"mlir::triton::gpu::TMAMetadataTy*\", /*default*/\"nullptr\",\n                \"tma metadata to the runtime\">,\n-        Option<\"isROCM\", \"is-rocm\",\n-               \"bool\", /*default*/\"false\",\n-               \"compile for ROCM-compatible LLVM\">,\n+        Option<\"target\", \"target\", \"enum Target\", \"mlir::triton::Target::Default\",\n+               \"compile for target compatible LLVM\",\n+               \"llvm::cl::values(\"\n+               \"clEnumValN(mlir::triton::Target::NVVM, \\\"nvvm\\\", \\\"compile for \"\n+               \"NVVM-compatible LLVM\\\"), \"\n+               \"clEnumValN(mlir::triton::Target::ROCDL, \\\"rocdl\\\", \\\"compile for \"\n+               \"ROCDL-compatible LLVM\\\"))\">,\n     ];\n }\n "}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -14,6 +14,8 @@ template <typename T> class OperationPass;\n \n namespace triton {\n \n+enum Target { NVVM, ROCDL, Default = NVVM };\n+\n #define GEN_PASS_DECL\n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h.inc\"\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 30, "deletions": 16, "changes": 46, "file_content_changes": "@@ -137,7 +137,7 @@ def TT_LoadOp : TT_Op<\"load\",\n                       [SameLoadStoreOperandsAndResultShape,\n                        SameLoadStoreOperandsAndResultEncoding,\n                        AttrSizedOperandSegments,\n-                       MemoryEffects<[MemRead]>,\n+                       DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,\n                        TypesMatchWith<\"infer ptr type from result type\",\n                                       \"result\", \"ptr\", \"$_self\",\n                                       \"mlir::OpTrait::impl::verifyLoadStorePointerAndValueType\">,\n@@ -463,33 +463,23 @@ def TT_ScanReturnOp: TT_Op<\"scan.return\",\n //\n // External Elementwise op\n //\n-class TT_ExternElementwiseOpBase<string mnemonic, list<Trait> traits = []> :\n-    TT_Op<mnemonic,\n-         traits # [Elementwise,\n-                   SameOperandsAndResultEncoding,\n-                   SameVariadicOperandSize]> {\n+def TT_ExternElementwiseOp : TT_Op<\"extern_elementwise\", [Elementwise,\n+                                                            SameOperandsAndResultEncoding,\n+                                                            SameVariadicOperandSize,\n+                                                            DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {\n \n     let description = [{\n         call an external function $symbol implemented in $libpath/$libname with $args\n         return $libpath/$libname:$symbol($args...)\n     }];\n \n-    let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol);\n+    let arguments = (ins Variadic<TT_Type>:$args, StrAttr:$libname, StrAttr:$libpath, StrAttr:$symbol, BoolAttr:$pure);\n \n     let results = (outs TT_Type:$result);\n \n     let assemblyFormat = \"operands attr-dict `:` functional-type(operands, $result)\";\n }\n \n-def TT_PureExternElementwiseOp : TT_ExternElementwiseOpBase<\"pure_extern_elementwise\", [Pure, Elementwise]> {\n-    let summary = \"FFI for pure element-wise extern LLVM bitcode functions\";\n-}\n-\n-def TT_ImpureExternElementwiseOp : TT_ExternElementwiseOpBase<\"impure_extern_elementwise\", [MemoryEffects<[MemRead]>,\n-                                                                                            MemoryEffects<[MemWrite]>]> {\n-    let summary = \"FFI for impure element-wise extern LLVM bitcode functions\";\n-}\n-\n //\n // Make Range Op\n //\n@@ -508,6 +498,30 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n     let results = (outs TT_IntTensor:$result);\n \n     let assemblyFormat = \"attr-dict `:` type($result)\";\n+\n+    let hasFolder = 1;\n+}\n+\n+//\n+// ElementwiseInlineAsm Op\n+//\n+def TT_ElementwiseInlineAsmOp : TT_Op<\"elementwise_inline_asm\", [Elementwise,\n+                                                                 SameOperandsAndResultEncoding,\n+                                                                 DeclareOpInterfaceMethods<MemoryEffectsOpInterface>]> {\n+  let summary = \"inline assembly applying elementwise operation to a group of packed element.\";\n+  let description = [{\n+   This will apply the given in inline assembly to `packed_element` number of\n+   elements of the inputs. The elements packed together is unknown and will\n+   depend on the backend implementation.\n+  }];\n+\n+  let arguments = (ins StrAttr:$asm_string, StrAttr:$constraints, BoolAttr:$pure, I32Attr:$packed_element, Variadic<AnyTypeOf<[TT_Type]>>:$args);\n+  let results = (outs TT_Type:$result);\n+\n+\n+  let assemblyFormat = [{\n+    $asm_string attr-dict ($args^ `:` type($args))? `->` type($result)\n+  }];\n }\n \n //"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -585,6 +585,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n \n   let extraClassDeclaration = extraBaseClassDeclaration # [{\n     bool isVolta() const;\n+    bool isTuring() const;\n     bool isAmpere() const;\n     bool isHopper() const;\n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 14, "deletions": 19, "changes": 33, "file_content_changes": "@@ -19,8 +19,6 @@ class SharedEncodingAttr;\n }\n } // namespace triton\n \n-LogicalResult fixupLoops(ModuleOp mod);\n-\n SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n                                                 const ArrayRef<int64_t> &shape,\n                                                 RankedTensorType type);\n@@ -103,31 +101,28 @@ class GraphLayoutMarker : public GraphDumper {\n   std::string getColor(const Type &type) const;\n };\n \n-// TODO: Interface\n-LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n-                             Attribute &ret);\n+// Infers the encoding of the result of op given the source encoding.\n+std::optional<Attribute> inferDstEncoding(Operation *op, Attribute encoding);\n \n-bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+// Infers the encoding of the source of op given the result encoding.\n+std::optional<Attribute> inferSrcEncoding(Operation *op, Attribute encoding);\n \n-bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n+bool isExpensiveLoadOrStore(Operation *op);\n \n-// skipInit is True when we only consider the operands of the initOp but\n-// not the initOp itself.\n-int simulateBackwardRematerialization(\n-    Operation *initOp, SetVector<Operation *> &processed,\n-    SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    Attribute targetEncoding);\n+bool canFoldIntoConversion(Operation *op, Attribute targetEncoding);\n \n Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n                               IRMapping &mapping);\n \n-void rematerializeConversionChain(\n-    const llvm::MapVector<Value, Attribute> &toConvert,\n-    mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n-    IRMapping &mapping);\n+// Get backward slice of tensor values starting from the root node along with\n+// encoding propagation.\n+LogicalResult getConvertBackwardSlice(\n+    Value root, SetVector<Value> &slice, Attribute rootEncoding,\n+    DenseMap<Value, Attribute> &layout,\n+    std::function<bool(Operation *)> stopPropagation = nullptr);\n \n-LogicalResult canMoveOutOfLoop(BlockArgument arg,\n-                               SmallVector<Operation *> &cvts);\n+// Populate pattern to remove dead cycles in ForOp.\n+void populateForOpDeadArgumentElimination(RewritePatternSet &patterns);\n \n // Convert an \\param index to a multi-dim coordinate given \\param shape and\n // \\param order."}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUOps.td", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -279,8 +279,7 @@ def TTNG_DotWaitOp : TTNG_Op<\"dot_wait\", []> {\n }\n \n def TTNG_StoreAsyncOp : TTNG_Op<\"store_async\",\n-                              [Source1IsSharedEncoding,\n-                               MemoryEffects<[MemWrite]>]> {\n+                              [MemoryEffects<[MemWrite]>]> {\n   let summary = \"store asynchronous by a tensor pointer\";\n   let arguments = (ins TT_TensorPtr:$dst, TT_Tensor:$src,\n                        DefaultValuedAttr<TT_CacheModifierAttr, \"triton::CacheModifier::NONE\">:$cache);"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1,5 +1,6 @@\n #ifndef TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n #define TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n+#include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n #include \"llvm/ADT/StringRef.h\"\n #include <memory>\n@@ -28,15 +29,15 @@ std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module, int computeCapability,\n                            mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n-                           bool isROCM);\n+                           Target target);\n \n // Translate mlir LLVM dialect to LLVMIR, return null if failed.\n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n-                      bool isROCM);\n+                      Target target);\n \n bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n-                   llvm::StringRef path, bool isROCM);\n+                   llvm::StringRef path, Target target);\n \n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -109,6 +109,12 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   return paddedRepShape;\n }\n \n+SmallVector<unsigned>\n+getScratchConfigForStoreAsync(triton::nvidia_gpu::StoreAsyncOp op) {\n+  auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+  return convertType<unsigned, int64_t>(getShapePerCTA(srcTy));\n+}\n+\n // TODO: extend beyond scalars\n SmallVector<unsigned> getScratchConfigForAtomicRMW(triton::AtomicRMWOp op) {\n   SmallVector<unsigned> smemShape;\n@@ -244,6 +250,18 @@ class AllocationAnalysis {\n               : elems * std::max<int>(8, srcTy.getElementTypeBitWidth()) / 8;\n       maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes,\n                                                           scratchAlignment);\n+    } else if (auto storeAsyncOp =\n+                   dyn_cast<triton::nvidia_gpu::StoreAsyncOp>(op)) {\n+      auto srcTy = storeAsyncOp.getSrc().getType().cast<RankedTensorType>();\n+      auto srcEncoding = srcTy.getEncoding();\n+      if (!srcEncoding.isa<MmaEncodingAttr>()) {\n+        return;\n+      }\n+      auto smemShape = getScratchConfigForStoreAsync(storeAsyncOp);\n+      unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                       std::multiplies{});\n+      auto bytes = elems * std::max<int>(8, srcTy.getElementTypeBitWidth()) / 8;\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes, 1024);\n     } else if (auto atomicRMWOp = dyn_cast<triton::AtomicRMWOp>(op)) {\n       auto value = op->getOperand(0);\n       // only scalar requires scratch memory"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 14, "deletions": 4, "changes": 18, "file_content_changes": "@@ -361,7 +361,7 @@ bool supportMMA(triton::DotOp op, int version) {\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n     if (!(numWarps % 4 == 0 && retShapePerCTA[0] % 64 == 0 &&\n           retShapePerCTA[1] % 8 == 0 &&\n-          (aElemTy.isFloat8E5M2() || aElemTy.isFloat8E4M3FN() ||\n+          (aElemTy.isFloat8E5M2() || aElemTy.isFloat8E4M3FNUZ() ||\n            aElemTy.isInteger(8) || aElemTy.isF16() || aElemTy.isBF16() ||\n            aElemTy.isF32()))) {\n       return false;\n@@ -473,9 +473,11 @@ struct DFSState {\n   SmallVector<Operation *, 16> topologicalCounts;\n   DenseSet<Operation *> seen;\n \n-  /// We mark each op as ready if all its operands are seen. If an op is ready,\n-  /// we add it to the queue. Otherwise, we keep adding its operands to the\n-  /// ancestors set.\n+  /// We mark each op as ready if all its operands and parents ops are seen. If\n+  /// an op is ready, we add it to the queue. Otherwise, we keep adding its\n+  /// operands to the ancestors set.\n+  /// We always want an op to be scheduled after all its parents to handle\n+  /// correctly cases with scf operations.\n   void addToReadyQueue(Operation *op, DFSSubgraphState &subGraph,\n                        SmallVector<Operation *, 4> &readyQueue) {\n     bool ready = true;\n@@ -486,6 +488,14 @@ struct DFSState {\n         ready = false;\n       }\n     }\n+    Operation *parent = op->getParentOp();\n+    while (parent) {\n+      if (!seen.count(parent)) {\n+        subGraph.push_back(parent);\n+        ready = false;\n+      }\n+      parent = parent->getParentOp();\n+    }\n     if (ready)\n       readyQueue.push_back(op);\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -699,15 +699,6 @@ struct ConvertLayoutOpConversion\n     return success();\n   }\n \n-  // Pack two 16-bit values into a 32-bit register.\n-  static Value pack16bitsTo32(ConversionPatternRewriter &rewriter, Location loc,\n-                              Value hb, Value lb) {\n-    hb = zext(i32_ty, bitcast(hb, i16_ty));\n-    lb = zext(i32_ty, bitcast(lb, i16_ty));\n-    Value pack = or_(lb, shl(hb, i32_val(16)));\n-    return pack;\n-  }\n-\n   // blocked -> shared.\n   // Swizzling in shared memory to avoid bank conflict. Normally used for\n   // A/B operands of dots.\n@@ -766,6 +757,14 @@ struct ConvertLayoutOpConversion\n       Value warpId0 = urem(urem(warpId, i32_val(warpsPerCTA[0])),\n                            i32_val(srcShape[0] / instrShape[0]));\n \n+      unsigned inVec =\n+          inOrd == outOrd ? triton::gpu::getContigPerThread(mmaLayout)[inOrd[0]]\n+                          : 1;\n+      unsigned outVec = dstSharedLayout.getVec();\n+      unsigned minVec = std::min(outVec, inVec);\n+      assert(minVec == 2);\n+      auto wordTy = vec_ty(elemTy, minVec);\n+\n       for (int rep = 0; rep < repM; ++rep) {\n         Value rowOfWarp = add(mul(warpId0, i32_val(instrShape[0])),\n                               i32_val(rep * rowsPerRep));\n@@ -779,18 +778,19 @@ struct ConvertLayoutOpConversion\n               numElemsPerSwizzlingRow, true);\n \n           Value addr = gep(elemPtrTy, smemBase, offset);\n-          Value data0 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 1],\n-                                       inVals[elemIdx + 0]);\n-          Value data1 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 3],\n-                                       inVals[elemIdx + 2]);\n-          Value data2 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 5],\n-                                       inVals[elemIdx + 4]);\n-          Value data3 = pack16bitsTo32(rewriter, loc, inVals[elemIdx + 7],\n-                                       inVals[elemIdx + 6]);\n+\n+          Value words[4];\n+          for (unsigned i = 0; i < 8; ++i) {\n+            if (i % minVec == 0)\n+              words[i / 2] = undef(wordTy);\n+            words[i / 2] = insert_element(\n+                wordTy, words[i / 2], inVals[elemIdx + i], i32_val(i % minVec));\n+          }\n \n           rewriter.create<triton::nvgpu::StoreMatrixOp>(\n               loc, bitcast(addr, ptrI8SharedTy),\n-              ValueRange{data0, data1, data2, data3});\n+              ValueRange{bitcast(words[0], i32_ty), bitcast(words[1], i32_ty),\n+                         bitcast(words[2], i32_ty), bitcast(words[3], i32_ty)});\n         }\n       }\n       // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -128,7 +128,8 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n   for (int i = 0; i < aNumPtr; ++i) {\n     aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n   }\n-  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n+  auto elemTy = typeConverter->convertType(\n+      A.getType().cast<RankedTensorType>().getElementType());\n \n   Type ptrTy = ptr_ty(elemTy, 3);\n   SmallVector<Value> aPtrs(aNumPtr);\n@@ -192,7 +193,8 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n   for (int i = 0; i < bNumPtr; ++i) {\n     bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n   }\n-  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n+  auto elemTy = typeConverter->convertType(\n+      B.getType().cast<RankedTensorType>().getElementType());\n \n   Type ptrTy = ptr_ty(elemTy, 3);\n   SmallVector<Value> bPtrs(bNumPtr);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -17,6 +17,10 @@ LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n                             TritonGPUToLLVMTypeConverter *typeConverter,\n                             ConversionPatternRewriter &rewriter);\n \n+LogicalResult convertMMA1688(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                             TritonGPUToLLVMTypeConverter *typeConverter,\n+                             ConversionPatternRewriter &rewriter);\n+\n LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n                               TritonGPUToLLVMTypeConverter *typeConverter,\n                               ConversionPatternRewriter &rewriter);\n@@ -56,6 +60,8 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     if (!isOuter && mmaLayout && supportMMA(op, mmaLayout.getVersionMajor())) {\n       if (mmaLayout.isVolta())\n         return convertMMA884(op, adaptor, getTypeConverter(), rewriter);\n+      if (mmaLayout.isTuring())\n+        return convertMMA1688(op, adaptor, getTypeConverter(), rewriter);\n       if (mmaLayout.isAmpere())\n         return convertMMA16816(op, adaptor, getTypeConverter(), rewriter);\n       if (mmaLayout.isHopper())"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "modified", "additions": 58, "deletions": 17, "changes": 75, "file_content_changes": "@@ -141,7 +141,15 @@ TensorCoreType getMmaType(triton::DotOp op) {\n   return TensorCoreType::NOT_APPLICABLE;\n }\n \n-inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n+inline static const std::map<TensorCoreType, std::string> mmaInstrPtxTuring = {\n+    {TensorCoreType::FP32_FP16_FP16_FP32,\n+     \"mma.sync.aligned.m16n8k8.row.col.f32.f16.f16.f32\"},\n+\n+    {TensorCoreType::FP16_FP16_FP16_FP16,\n+     \"mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16\"},\n+};\n+\n+inline static const std::map<TensorCoreType, std::string> mmaInstrPtxAmpere = {\n     {TensorCoreType::FP32_FP16_FP16_FP32,\n      \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n     {TensorCoreType::FP32_BF16_BF16_FP32,\n@@ -164,7 +172,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n                          ConversionPatternRewriter &rewriter, Location loc,\n                          Value a, Value b, Value c, Value d, Value loadedA,\n                          Value loadedB, Value loadedC, DotOp op,\n-                         DotOpAdaptor adaptor) {\n+                         DotOpAdaptor adaptor, bool isTuring) {\n   MLIRContext *ctx = c.getContext();\n   auto aTensorTy = a.getType().cast<RankedTensorType>();\n   auto bTensorTy = b.getType().cast<RankedTensorType>();\n@@ -197,23 +205,18 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n \n   auto mmaType = getMmaType(op);\n \n+  const auto &mmaInstructions =\n+      isTuring ? mmaInstrPtxTuring : mmaInstrPtxAmpere;\n+\n   auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n     unsigned colsPerThread = repN * 2;\n     PTXBuilder builder;\n-    auto &mma = *builder.create(mmaInstrPtx.at(mmaType));\n+    auto &mma = *builder.create(mmaInstructions.at(mmaType));\n     // using =r for float32 works but leads to less readable ptx.\n     bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n     bool isAccF16 = dTensorTy.getElementType().isF16();\n     auto retArgs =\n         builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n-    auto aArgs = builder.newListOperand({\n-        {ha[{m, k}], \"r\"},\n-        {ha[{m + 1, k}], \"r\"},\n-        {ha[{m, k + 1}], \"r\"},\n-        {ha[{m + 1, k + 1}], \"r\"},\n-    });\n-    auto bArgs =\n-        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n     auto cArgs = builder.newListOperand();\n     for (int i = 0; i < numMmaRets; ++i) {\n       cArgs->listAppend(builder.newOperand(\n@@ -222,7 +225,32 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n       // reuse the output registers\n     }\n \n-    mma(retArgs, aArgs, bArgs, cArgs);\n+    if (isTuring) {\n+      auto aArgs1 = builder.newListOperand({\n+          {ha[{m, k}], \"r\"},\n+          {ha[{m + 1, k}], \"r\"},\n+      });\n+      auto bArgs1 = builder.newListOperand({\n+          {hb[{n, k}], \"r\"},\n+      });\n+      auto aArgs2 = builder.newListOperand({\n+          {ha[{m, k + 1}], \"r\"},\n+          {ha[{m + 1, k + 1}], \"r\"},\n+      });\n+      auto bArgs2 = builder.newListOperand({{hb[{n, k + 1}], \"r\"}});\n+      mma(retArgs, aArgs1, bArgs1, cArgs);\n+      mma(retArgs, aArgs2, bArgs2, cArgs);\n+    } else {\n+      auto aArgs = builder.newListOperand({\n+          {ha[{m, k}], \"r\"},\n+          {ha[{m + 1, k}], \"r\"},\n+          {ha[{m, k + 1}], \"r\"},\n+          {ha[{m + 1, k + 1}], \"r\"},\n+      });\n+      auto bArgs =\n+          builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+      mma(retArgs, aArgs, bArgs, cArgs);\n+    }\n     Value mmaOut =\n         builder.launch(rewriter, loc, getMmaRetType(mmaType, op.getContext()));\n \n@@ -259,10 +287,9 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   return success();\n }\n \n-// Convert to mma.m16n8k16\n-LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n-                              TritonGPUToLLVMTypeConverter *typeConverter,\n-                              ConversionPatternRewriter &rewriter) {\n+LogicalResult convertMMA(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                         TritonGPUToLLVMTypeConverter *typeConverter,\n+                         ConversionPatternRewriter &rewriter, bool isTuring) {\n   auto loc = op.getLoc();\n   auto mmaLayout = op.getResult()\n                        .getType()\n@@ -288,5 +315,19 @@ LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n       loadC(op.getC(), adaptor.getC(), typeConverter, op.getLoc(), rewriter);\n \n   return convertDot(typeConverter, rewriter, op.getLoc(), A, B, C, op.getD(),\n-                    loadedA, loadedB, loadedC, op, adaptor);\n+                    loadedA, loadedB, loadedC, op, adaptor, isTuring);\n+}\n+\n+// Convert to mma.m16n8k8\n+LogicalResult convertMMA1688(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                             TritonGPUToLLVMTypeConverter *typeConverter,\n+                             ConversionPatternRewriter &rewriter) {\n+  return convertMMA(op, adaptor, typeConverter, rewriter, true /*isTuring*/);\n+}\n+\n+// Convert to mma.m16n8k16\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter) {\n+  return convertMMA(op, adaptor, typeConverter, rewriter, false /*isTuring*/);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -58,7 +58,7 @@ triton::nvgpu::WGMMAEltType getMmaOperandType(Value a, bool allowTF32) {\n     return triton::nvgpu::WGMMAEltType::s8;\n   } else if (aTy.isFloat8E5M2()) {\n     return triton::nvgpu::WGMMAEltType::e5m2;\n-  } else if (aTy.isFloat8E4M3FN()) {\n+  } else if (aTy.isFloat8E4M3FNUZ()) {\n     return triton::nvgpu::WGMMAEltType::e4m3;\n   } else {\n     llvm::report_fatal_error(\"Unsupported mma operand type found\");\n@@ -298,7 +298,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n \n   triton::nvgpu::WGMMAEltType eltTypeC = getMmaRetType(d);\n   triton::nvgpu::WGMMAEltType eltTypeA = getMmaOperandType(a, allowTF32);\n-  triton::nvgpu::WGMMAEltType eltTypeB = eltTypeA;\n+  triton::nvgpu::WGMMAEltType eltTypeB = getMmaOperandType(b, allowTF32);\n \n   triton::nvgpu::WGMMALayout layoutA = transA ? triton::nvgpu::WGMMALayout::col\n                                               : triton::nvgpu::WGMMALayout::row;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 183, "deletions": 140, "changes": 323, "file_content_changes": "@@ -154,96 +154,14 @@ const std::string Fp16_to_Fp8E4M3B15x4 =\n     \"lop3.b32 $0, $0, $2, 0xbf80bf80, 0xf8;  \\n\"\n     \"}\";\n \n-/* ----- FP8E4M3 ------ */\n-// Note: when handled by software, this format\n-// does not handle denormals and has\n-// more than a single NaN values.\n-\n-// Fp8E4M3 -> Fp16 (packed)\n-const std::string Fp8E4M3_to_Fp16 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x0504;            \\n\" // a0 = 0x00f300f4\n-    \"prmt.b32 a1, 0, $2, 0x0706;            \\n\" // a1 = 0x00f100f2\n-    \"and.b32  b0, a0, 0x00800080;           \\n\" // b0 = a0 & 0x00800080\n-    \"and.b32  b1, a1, 0x00800080;           \\n\" // (extract sign)\n-    \"add.u32  b0, b0, a0;                   \\n\" // b0 = b0 + a0\n-    \"add.u32  b1, b1, a1;                   \\n\" // (move sign to the left)\n-    \"mad.lo.u32 $0, b0, 128, 0x20002000;    \\n\" // out0 = (b0 << 7) + 0x20002000\n-    \"mad.lo.u32 $1, b1, 128, 0x20002000;    \\n\" // (shift into position and bias\n-                                                // exponent)\n-    \"}\";\n-\n-// Fp16 -> Fp8E4M3 (packed)\n-const std::string Fp16_to_Fp8E4M3 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n-    \"and.b32 a0, $1, 0x7fff7fff;            \\n\" // a0 = input0 & 0x7fff7fff\n-    \"and.b32 a1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-    \"mad.lo.u32 a0, a0, 2, 0x40804080;      \\n\" // shift exponent (<< 1),\n-    \"mad.lo.u32 a1, a1, 2, 0x40804080;      \\n\" // correct bias (0x40004000),\n-                                                // and round to nearest\n-    \"lop3.b32 b0, $1, 0x80008000, a0, 0xe2; \\n\" // b0 = 0x80008000 ? in0 : a0\n-    \"lop3.b32 b1, $2, 0x80008000, a1, 0xe2; \\n\" // (restore sign)\n-    \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n-    \"}\";\n-\n-// WARN: subnormal (0bs0000xxx) are not handled\n-const std::string Fp8E4M3_to_Bf16 =\n-    \"{                                      \\n\"\n-    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-    \"prmt.b32 a0, 0, $2, 0x0504;            \\n\" // a0 = 0x00f300f4\n-    \"prmt.b32 a1, 0, $2, 0x0706;            \\n\" // a1 = 0x00f100f2\n-    \"and.b32  b0, a0, 0x00800080;           \\n\" // b0 = a0 & 0x00800080\n-    \"and.b32  b1, a1, 0x00800080;           \\n\" // (extract sign)\n-    \"mad.lo.u32 b0, b0, 15, a0;             \\n\" // b0 = b0 * 15 + a0\n-    \"mad.lo.u32 b1, b1, 15, a1;             \\n\" // (move sign to the left)\n-    \"mad.lo.u32 $0, b0, 16, 0x3c003c00;     \\n\" // out0 = (b0 << 4) + 0x3c003c00\n-    \"mad.lo.u32 $1, b1, 16, 0x3c003c00;     \\n\" // (shift into position and bias\n-                                                // exponent)\n-    \"}\";\n-\n-const std::string Bf16_to_Fp8E4M3 =\n-    \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n-    \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n-    \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n-    \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n-    \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n-    \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n-    \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n-    \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n-    \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n-    \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n-    \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-\n-    // nosign = clamp(nosign, min, max)\n-    \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n-    \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n-    \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n-    \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n-    \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n-    \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n-    \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n-    \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n-    \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n-    \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n-    \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n-    \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n-    \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n-    \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n-    \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n-\n-    \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n-    \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n-    \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n-    \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n-    \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n-    \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n-    \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n-                                                     // nosign1 = 0x00f300f4\n-                                                     // nosign = 0xf3f4f1f2\n-    \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n-    \"}\";\n+// Fp8E4M3 (x2) -> Fp16 (x2) (packed)\n+const std::string Fp8E4M3Nv_to_Fp16 = \"{ \\n\"\n+                                      \"cvt.rn.f16x2.e4m3x2 $0, $1; \\n\"\n+                                      \"}\";\n+// Fp16 (x2) -> Fp8E4M3 (x2) (packed)\n+const std::string Fp16_to_Fp8E4M3Nv = \"{ \\n\"\n+                                      \"cvt.rn.satfinite.e4m3x2.f16x2 $0, $1; \\n\"\n+                                      \"}\";\n \n /* ----- Packed integer to BF16 ------ */\n const std::string S8_to_Bf16 =\n@@ -272,6 +190,12 @@ static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n   assert(inEncoding == ouEncoding);\n   if (!inEncoding)\n     return values;\n+  // If the parent of the dot operand is in block encoding, we don't need to\n+  // reorder elements\n+  auto parentEncoding =\n+      dyn_cast<triton::gpu::MmaEncodingAttr>(ouEncoding.getParent());\n+  if (!parentEncoding)\n+    return values;\n   size_t inBitWidth = inTensorTy.getElementType().getIntOrFloatBitWidth();\n   size_t ouBitWidth = ouTensorTy.getElementType().getIntOrFloatBitWidth();\n   auto ouEltTy = ouTensorTy.getElementType();\n@@ -391,40 +315,49 @@ inline SmallVector<Value> packI32(const SmallVector<Value> &inValues,\n }\n \n typedef std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n-                                         const Value &, const Value &,\n-                                         const Value &, const Value &)>\n+                                         const SmallVector<Value> &)>\n     ConverterT;\n \n static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n-                                       Type outType) {\n+                                       Type outType,\n+                                       const int inVecWidthBits = 32,\n+                                       const int outVecWidthBits = 32) {\n+\n+  ConverterT converter =\n+      [ptxAsm, inType, outType, inVecWidthBits,\n+       outVecWidthBits](Location loc, ConversionPatternRewriter &rewriter,\n+                        const SmallVector<Value> &v) -> SmallVector<Value> {\n+    int numElements = v.size();\n+    assert(numElements == 4 || numElements == 2 && \"invalid vector size\");\n \n-  ConverterT converter = [ptxAsm, inType, outType](\n-                             Location loc, ConversionPatternRewriter &rewriter,\n-                             const Value &v0, const Value &v1, const Value &v2,\n-                             const Value &v3) -> SmallVector<Value> {\n-    SmallVector<Value> v = {v0, v1, v2, v3};\n     auto ctx = rewriter.getContext();\n     int inBitwidth = inType.getIntOrFloatBitWidth();\n     int outBitwidth = outType.getIntOrFloatBitWidth();\n     // first, we pack `v` into 32-bit ints\n-    int inVecWidth = 32 / inBitwidth;\n+    int inVecWidth = inVecWidthBits / inBitwidth;\n     auto inVecTy = vec_ty(inType, inVecWidth);\n-    SmallVector<Value> inPacked(4 / inVecWidth, undef(inVecTy));\n-    for (size_t i = 0; i < 4; i++)\n+    SmallVector<Value> inPacked(numElements / inVecWidth, undef(inVecTy));\n+    for (size_t i = 0; i < numElements; i++)\n       inPacked[i / inVecWidth] = insert_element(\n           inVecTy, inPacked[i / inVecWidth], v[i], i32_val(i % inVecWidth));\n     for (size_t i = 0; i < inPacked.size(); i++)\n-      inPacked[i] = bitcast(inPacked[i], i32_ty);\n+      inPacked[i] = bitcast(inPacked[i], int_ty(inVecWidthBits));\n \n     // then, we run the provided inline PTX\n-    int outVecWidth = 32 / outBitwidth;\n-    int outNums = 4 / outVecWidth;\n+    int outVecWidth = outVecWidthBits / outBitwidth;\n+    int outNums = numElements / outVecWidth;\n     PTXBuilder builder;\n     SmallVector<PTXBuilder::Operand *> operands;\n-    for (int i = 0; i < outNums; i++)\n-      operands.push_back(builder.newOperand(\"=r\"));\n-    for (Value inVal : inPacked)\n-      operands.push_back(builder.newOperand(inVal, \"r\"));\n+    auto outConstriant = outVecWidthBits == 16 ? \"=h\" : \"=r\";\n+    auto inConstraint = inVecWidthBits == 16 ? \"h\" : \"r\";\n+    for (int i = 0; i < outNums; i++) {\n+      operands.push_back(builder.newOperand(outConstriant));\n+    }\n+\n+    for (Value inVal : inPacked) {\n+      operands.push_back(builder.newOperand(inVal, inConstraint));\n+    }\n+\n     auto &ptxOp = *builder.create(ptxAsm);\n     ptxOp(operands, /*onlyAttachMLIRArgs=*/true);\n     auto outVecTy = vec_ty(outType, outVecWidth);\n@@ -439,7 +372,7 @@ static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n     }\n     // unpack the output\n     SmallVector<Value> ret;\n-    for (size_t i = 0; i < 4; i++)\n+    for (size_t i = 0; i < numElements; i++)\n       ret.push_back(extract_element(outType, outPacked[i / outVecWidth],\n                                     i32_val(i % outVecWidth)));\n     return ret;\n@@ -526,6 +459,9 @@ class ElementwiseOpConversionBase\n \n     return success();\n   }\n+\n+private:\n+  int computeCapability;\n };\n \n template <typename SourceOp, typename DestOp>\n@@ -538,11 +474,6 @@ struct ElementwiseOpConversion\n   using Base::Base;\n   using OpAdaptor = typename Base::OpAdaptor;\n \n-  explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n-                                   PatternBenefit benefit = 1)\n-      : ElementwiseOpConversionBase<SourceOp, ElementwiseOpConversion>(\n-            typeConverter, benefit) {}\n-\n   // An interface to support variant DestOp builder.\n   SmallVector<DestOp> createDestOps(SourceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter,\n@@ -559,6 +490,11 @@ struct FpToFpOpConversion\n   using ElementwiseOpConversionBase<\n       triton::FpToFpOp, FpToFpOpConversion>::ElementwiseOpConversionBase;\n \n+  explicit FpToFpOpConversion(TritonGPUToLLVMTypeConverter &typeConverter,\n+                              int computeCapability, PatternBenefit benefit = 1)\n+      : ElementwiseOpConversionBase(typeConverter, benefit),\n+        computeCapability(computeCapability) {}\n+\n   static Value convertBf16ToFp32(Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  const Value &v) {\n@@ -618,58 +554,83 @@ struct FpToFpOpConversion\n         // F8 -> F16\n         {{F8E4M3B15TyID, F16TyID}, Fp8E4M3B15_to_Fp16},\n         {{F8E4M3FNTyID, F16TyID}, Fp8E4M3B15x4_to_Fp16},\n-        {{F8E4M3TyID, F16TyID}, Fp8E4M3_to_Fp16},\n+        {{F8E4M3TyID, F16TyID}, Fp8E4M3Nv_to_Fp16},\n         {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n         // F16 -> F8\n         {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n         {{F16TyID, F8E4M3FNTyID}, Fp16_to_Fp8E4M3B15x4},\n-        {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3},\n+        {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3Nv},\n         {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},\n         // F8 -> BF16\n-        {{F8E4M3TyID, BF16TyID}, Fp8E4M3_to_Bf16},\n         {{F8E5M2TyID, BF16TyID}, Fp8E5M2_to_Bf16},\n         // BF16 -> F8\n-        {{BF16TyID, F8E4M3TyID}, Bf16_to_Fp8E4M3},\n         {{BF16TyID, F8E5M2TyID}, Bf16_to_Fp8E5M2},\n     };\n+    int inVecWidthBits = 32;\n+    int outVecWidthBits = 32;\n+    if (srcTy.isFloat8E4M3FNUZ()) {\n+      inVecWidthBits = 16;\n+      outVecWidthBits = 32;\n+    }\n+    if (dstTy.isFloat8E4M3FNUZ()) {\n+      inVecWidthBits = 32;\n+      outVecWidthBits = 16;\n+    }\n \n     std::pair<TypeID, TypeID> key = {srcTy.getTypeID(), dstTy.getTypeID()};\n     if (srcMap.count(key) == 0) {\n       llvm::errs() << \"Unsupported conversion from \" << srcTy << \" to \" << dstTy\n                    << \"\\n\";\n       llvm_unreachable(\"\");\n     }\n+    if (computeCapability < 90 &&\n+        (srcTy.isFloat8E4M3FNUZ() || dstTy.isFloat8E4M3FNUZ())) {\n+      llvm::errs() << \"Conversion from/to f8e4m3nv is only supported on \"\n+                      \"compute capability >= 90\"\n+                   << \"\\n\";\n+      llvm_unreachable(\"\");\n+    }\n     return makeConverterFromPtx(srcMap.lookup(key),\n                                 getTypeConverter()->convertType(srcTy),\n-                                getTypeConverter()->convertType(dstTy));\n+                                getTypeConverter()->convertType(dstTy),\n+                                inVecWidthBits, outVecWidthBits);\n   }\n \n   SmallVector<Value> createDestOps(triton::FpToFpOp op, OpAdaptor adaptor,\n                                    ConversionPatternRewriter &rewriter,\n                                    Type elemTy, MultipleOperandsRange operands,\n                                    Location loc) const {\n-    assert(operands.size() % 4 == 0 &&\n-           \"FP8 casting only support tensors with 4-aligned sizes\");\n     auto srcElementType = getElementType(op.getFrom());\n     auto dstElementType = getElementType(op.getResult());\n+    int numElements = 4;\n+    if (srcElementType.isFloat8E4M3FNUZ() ||\n+        dstElementType.isFloat8E4M3FNUZ()) {\n+      numElements = 2;\n+    }\n+    assert(operands.size() % numElements == 0 &&\n+           \"FP8 casting only support tensors with aligned sizes\");\n     bool isSrcFP32 = srcElementType.isF32();\n     bool isDstFP32 = dstElementType.isF32();\n     auto cvtFunc = getConversionFunc(isSrcFP32 ? f16_ty : srcElementType,\n                                      isDstFP32 ? f16_ty : dstElementType);\n-    SmallVector<Value> inVals = {operands[0][0], operands[1][0], operands[2][0],\n-                                 operands[3][0]};\n+    SmallVector<Value> inVals;\n+    for (unsigned i = 0; i < numElements; i++) {\n+      inVals.push_back(operands[i][0]);\n+    }\n     if (isSrcFP32)\n       for (Value &v : inVals)\n         v = convertFp32ToFp16(loc, rewriter, v);\n-    SmallVector<Value> outVals =\n-        cvtFunc(loc, rewriter, inVals[0], inVals[1], inVals[2], inVals[3]);\n+    SmallVector<Value> outVals = cvtFunc(loc, rewriter, inVals);\n     assert(outVals.size() == inVals.size());\n     if (isDstFP32)\n       for (Value &v : outVals)\n         v = convertFp16ToFp32(loc, rewriter, v);\n     // Pack values\n     return outVals;\n   }\n+\n+private:\n+  int computeCapability;\n };\n \n struct CmpIOpConversion\n@@ -762,15 +723,16 @@ struct CmpFOpConversion\n   }\n };\n \n-template <class T>\n struct ExternElementwiseOpConversion\n-    : public ElementwiseOpConversionBase<T, ExternElementwiseOpConversion<T>> {\n-  using Base = ElementwiseOpConversionBase<T, ExternElementwiseOpConversion<T>>;\n+    : public ElementwiseOpConversionBase<ExternElementwiseOp,\n+                                         ExternElementwiseOpConversion> {\n+  using Base = ElementwiseOpConversionBase<ExternElementwiseOp,\n+                                           ExternElementwiseOpConversion>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n   typedef typename Base::OpAdaptor OpAdaptor;\n \n-  SmallVector<Value> createDestOps(T op, OpAdaptor adaptor,\n+  SmallVector<Value> createDestOps(ExternElementwiseOp op, OpAdaptor adaptor,\n                                    ConversionPatternRewriter &rewriter,\n                                    Type elemTy, MultipleOperandsRange operands,\n                                    Location loc) const {\n@@ -791,8 +753,9 @@ struct ExternElementwiseOpConversion\n     return LLVM::LLVMFunctionType::get(resultType, operandTypes);\n   }\n \n-  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter, T op,\n-                                     StringRef funcName, Type funcType) const {\n+  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter,\n+                                     ExternElementwiseOp op, StringRef funcName,\n+                                     Type funcType) const {\n     using LLVM::LLVMFuncOp;\n \n     auto funcAttr = StringAttr::get(op->getContext(), funcName);\n@@ -811,6 +774,86 @@ struct ExternElementwiseOpConversion\n   }\n };\n \n+struct ElementwiseInlineAsmOpConversion\n+    : public ElementwiseOpConversionBase<ElementwiseInlineAsmOp,\n+                                         ElementwiseInlineAsmOpConversion> {\n+  using Base = ElementwiseOpConversionBase<ElementwiseInlineAsmOp,\n+                                           ElementwiseInlineAsmOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+  typedef typename Base::OpAdaptor OpAdaptor;\n+\n+  // If operand size is smaller than 32bits pack by groups of 32bits.\n+  // Otherwise have separate inputs.\n+  SmallVector<Value> packOperands(ElementwiseInlineAsmOp op,\n+                                  MultipleOperandsRange operands,\n+                                  ConversionPatternRewriter &rewriter,\n+                                  Location loc) const {\n+    SmallVector<Value> packedOperands;\n+    unsigned numPackedElements = op.getPackedElement();\n+    for (int i = 0, e = op.getNumOperands(); i < e; i++) {\n+      unsigned bitWidth =\n+          getElementType(op.getOperand(i)).getIntOrFloatBitWidth();\n+      unsigned numElementPerReg = bitWidth < 32 ? 32 / bitWidth : 1;\n+      numElementPerReg = std::min(numElementPerReg, numPackedElements);\n+      for (int j = 0; j < numPackedElements; j += numElementPerReg) {\n+        if (numElementPerReg == 1) {\n+          packedOperands.push_back(operands[j][i]);\n+          continue;\n+        }\n+        Type t = vec_ty(\n+            getTypeConverter()->convertType(getElementType(op.getOperand(i))),\n+            numElementPerReg);\n+        Value packed = undef(t);\n+        for (int k = 0; k < numElementPerReg; k++) {\n+          packed = insert_element(packed, operands[j + k][i], i32_val(k));\n+        }\n+        packedOperands.push_back(packed);\n+      }\n+    }\n+    return packedOperands;\n+  }\n+\n+  SmallVector<Value> createDestOps(ElementwiseInlineAsmOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    int numPackedElements = op.getPackedElement();\n+    if (operands.size() % numPackedElements != 0)\n+      llvm::report_fatal_error(\"Inline asm op has more packed elements than \"\n+                               \"number of elements per thread.\");\n+    SmallVector<Value> packedOperands =\n+        packOperands(op, operands, rewriter, loc);\n+    Type dstType =\n+        getTypeConverter()->convertType(getElementType(op.getResult()));\n+    Type retType = dstType;\n+    if (numPackedElements > 1)\n+      retType = vec_ty(retType, numPackedElements);\n+    Value result = rewriter\n+                       .create<LLVM::InlineAsmOp>(\n+                           loc, retType,\n+                           packedOperands,      // operands\n+                           op.getAsmString(),   // asm_string\n+                           op.getConstraints(), // constraints\n+                           !op.getPure(),       // has_side_effects\n+                           false,               // is_align_stack\n+                           LLVM::AsmDialectAttr::get(\n+                               rewriter.getContext(),\n+                               LLVM::AsmDialect::AD_ATT), // asm_dialect\n+                           ArrayAttr()                    // operand_attrs\n+                           )\n+                       ->getResult(0);\n+    SmallVector<Value> results;\n+    if (numPackedElements > 1) {\n+      for (int i = 0; i < numPackedElements; i++)\n+        results.push_back(extract_element(result, i32_val(i)));\n+    } else {\n+      results = {result};\n+    }\n+    return results;\n+  }\n+};\n+\n struct FDivOpConversion\n     : ElementwiseOpConversionBase<mlir::arith::DivFOp, FDivOpConversion> {\n   using Base =\n@@ -956,8 +999,9 @@ struct SIToFPOpConversion\n       auto cvtFunc = makeConverterFromPtx(\n           S8_to_Bf16, getTypeConverter()->convertType(inElemTy),\n           getTypeConverter()->convertType(outElemTy));\n-      auto outVals = cvtFunc(loc, rewriter, operands[0][0], operands[1][0],\n-                             operands[2][0], operands[3][0]);\n+      SmallVector<Value> inVals = {operands[0][0], operands[1][0],\n+                                   operands[2][0], operands[3][0]};\n+      auto outVals = cvtFunc(loc, rewriter, inVals);\n       assert(outVals.size() == 4);\n       return outVals;\n     } else if (outElemTy.isBF16()) {\n@@ -1142,7 +1186,9 @@ struct IndexCastOpLowering\n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-    ModuleAllocation &allocation, PatternBenefit benefit) {\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    int computeCapability, PatternBenefit benefit) {\n #define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp)\n@@ -1206,13 +1252,10 @@ void populateElementwiseOpToLLVMPatterns(\n   patterns.add<SIToFPOpConversion>(typeConverter, benefit);\n   patterns.add<IndexCastOpLowering>(typeConverter, benefit);\n \n-  patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n+  patterns.add<FpToFpOpConversion>(typeConverter, computeCapability, benefit);\n \n-  patterns.add<ExternElementwiseOpConversion<triton::PureExternElementwiseOp>>(\n-      typeConverter, benefit);\n-  patterns\n-      .add<ExternElementwiseOpConversion<triton::ImpureExternElementwiseOp>>(\n-          typeConverter, benefit);\n+  patterns.add<ExternElementwiseOpConversion>(typeConverter, benefit);\n+  patterns.add<ElementwiseInlineAsmOpConversion>(typeConverter, benefit);\n   // ExpOpConversionApprox will try using ex2.approx if the input type is\n   // FP32. For other input types, ExpOpConversionApprox will return failure and\n   // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.h", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -9,7 +9,9 @@ using namespace mlir::triton;\n void populateElementwiseOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, ModuleAxisInfoAnalysis &axisInfoAnalysis,\n-    ModuleAllocation &allocation, PatternBenefit benefit);\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    int computeCapability, PatternBenefit benefit);\n \n bool isLegalElementwiseOp(Operation *op);\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 288, "deletions": 3, "changes": 291, "file_content_changes": "@@ -3,6 +3,10 @@\n \n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"LoadStoreOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n #include <numeric>\n \n@@ -12,6 +16,7 @@ using namespace mlir::triton;\n using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::linearize;\n+using ::mlir::triton::gpu::getCTALayout;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n@@ -404,6 +409,18 @@ struct StoreAsyncOpConversion\n   LogicalResult\n   matchAndRewrite(triton::nvidia_gpu::StoreAsyncOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    auto srcTy = op.getSrc().getType().cast<RankedTensorType>();\n+    auto srcEncoding = srcTy.getEncoding();\n+    if (srcEncoding.isa<MmaEncodingAttr>()) {\n+      return lowerStoreAsyncWithSlice(op, adaptor, rewriter);\n+    } else {\n+      return lowerStoreAsync(op, adaptor, rewriter);\n+    }\n+  }\n+\n+  LogicalResult lowerStoreAsync(triton::nvidia_gpu::StoreAsyncOp op,\n+                                OpAdaptor adaptor,\n+                                ConversionPatternRewriter &rewriter) const {\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n \n@@ -413,6 +430,9 @@ struct StoreAsyncOpConversion\n     auto elemTy = srcTy.getElementType();\n \n     auto rank = srcTy.getRank();\n+    // The sotre async op only supports tensor with ranke <= 5.\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensor-dimension-size-and-format\n     assert(rank > 0 && rank <= 5);\n \n     auto moduleOp = op->getParentOfType<ModuleOp>();\n@@ -475,14 +495,14 @@ struct StoreAsyncOpConversion\n                            .cast<RankedTensorType>()\n                            .getShape();\n     auto shapePerCTA = getShapePerCTA(CTASplitNum, tensorShape);\n-    // magic 128 bytes\n+    const uint32_t bytesPerCacheline = 128;\n     uint32_t bytesPerElem = elemTy.getIntOrFloatBitWidth() / 8;\n     uint32_t numBox{1};\n     for (int i = 0; i < rank; ++i) {\n       auto dim = getDimOfOrder(dstOrder, i);\n       auto tNumElems = shapePerCTA[dim];\n-      if (i == 0 && tNumElems * bytesPerElem > 128) {\n-        tNumElems = 128 / bytesPerElem;\n+      if (i == 0 && tNumElems * bytesPerElem > bytesPerCacheline) {\n+        tNumElems = bytesPerCacheline / bytesPerElem;\n         numBox = (shapePerCTA[dim] + tNumElems - 1) / tNumElems;\n       }\n       boxDims.emplace_back(tNumElems);\n@@ -574,6 +594,268 @@ struct StoreAsyncOpConversion\n     return success();\n   }\n \n+  LogicalResult\n+  lowerStoreAsyncWithSlice(triton::nvidia_gpu::StoreAsyncOp op,\n+                           OpAdaptor adaptor,\n+                           ConversionPatternRewriter &rewriter) const {\n+    auto loc = op.getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    auto dst = op.getDst();\n+    auto src = op.getSrc();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto makeTensorPtr = tensorPtrMap->lookup(op.getOperation());\n+    auto dstTensorTy = makeTensorPtr.getResult()\n+                           .getType()\n+                           .cast<triton::PointerType>()\n+                           .getPointeeType()\n+                           .cast<RankedTensorType>();\n+    auto tensorShape = dstTensorTy.getShape();\n+    auto dstOrder = makeTensorPtr.getOrder();\n+    auto dstElemTy = dstTensorTy.getElementType();\n+\n+    auto rank = srcTy.getRank();\n+    // The sotre async op only supports tensor with ranke <= 5.\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensor-dimension-size-and-format\n+    assert(rank > 0 && rank <= 5);\n+\n+    auto moduleOp = op->getParentOfType<ModuleOp>();\n+    assert(moduleOp && \"Parent ModuleOp not found for StoreAsyncOp\");\n+\n+    auto llFuncOp = op->getParentOfType<LLVM::LLVMFuncOp>();\n+    assert(llFuncOp && \"LLVMFuncOp not found for StoreAsyncOp\");\n+\n+    int numTMADescs = getNumTMADescs(llFuncOp);\n+    assert(numTMADescs > 0);\n+\n+    auto ctaLayout = getCTALayout(dstTensorTy.getEncoding());\n+    // The order of smem should be consistent with gmem.\n+    SmallVector<unsigned> sharedOrder;\n+    for (auto o : makeTensorPtr.getOrder()) {\n+      sharedOrder.emplace_back(o);\n+    }\n+    auto sharedLayout = SharedEncodingAttr::get(ctx, tensorShape, sharedOrder,\n+                                                ctaLayout, dstElemTy);\n+\n+    mlir::triton::gpu::TMAInfo tmaInfo;\n+\n+    tmaInfo.tensorDataType = getCUtensorMapDataType(dstElemTy);\n+    tmaInfo.tensorRank = rank;\n+    assert(tmaMetadata);\n+\n+    unsigned TMADescIdx = tmaMetadata->size();\n+    unsigned numFuncArgs = llFuncOp.getBody().front().getNumArguments();\n+\n+    unsigned globalAddressArgIdx = getArgIdx(makeTensorPtr.getBase());\n+    tmaInfo.globalAddressArgIdx = globalAddressArgIdx;\n+    tmaInfo.TMADescArgIdx = numFuncArgs - numTMADescs + TMADescIdx;\n+\n+    auto getDimOfOrder = [](ArrayRef<int32_t> order, int32_t i) {\n+      auto it = std::find(order.begin(), order.end(), i);\n+      assert(it != order.end());\n+      return std::distance(order.begin(), it);\n+    };\n+\n+    std::vector<int32_t> globalDimsArgIdx;\n+    std::vector<int32_t> globalStridesArgIdx;\n+    // constant values are mapped to (-1 - value)\n+    for (int i = 0; i < rank; ++i) {\n+      int32_t argIdx = -1;\n+      auto dim = getDimOfOrder(dstOrder, i);\n+      argIdx = getArgIdx(makeTensorPtr.getShape()[dim]);\n+      globalDimsArgIdx.emplace_back(argIdx);\n+      // handle constant stride\n+      argIdx = getArgIdx(makeTensorPtr.getStrides()[dim]);\n+      globalStridesArgIdx.emplace_back(argIdx);\n+    }\n+\n+    tmaInfo.globalDimsArgIdx = globalDimsArgIdx;\n+    tmaInfo.globalStridesArgIdx = globalStridesArgIdx;\n+    std::vector<uint32_t> boxDims;\n+    auto CTAsPerCGA = sharedLayout.getCTALayout().getCTAsPerCGA();\n+    auto CTAOrder = sharedLayout.getCTALayout().getCTAOrder();\n+    auto CTASplitNum = sharedLayout.getCTALayout().getCTASplitNum();\n+    auto shapePerCTA = getShapePerCTA(CTASplitNum, tensorShape);\n+\n+    auto srcLayout = srcTy.getEncoding();\n+    auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n+\n+    unsigned numElems = triton::gpu::getTotalElemsPerThread(srcTy);\n+\n+    auto instrShape = mmaLayout.getInstrShape();\n+    auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+    uint32_t repM =\n+        ceil<unsigned>(shapePerCTA[0], instrShape[0] * warpsPerCTA[0]);\n+    uint32_t numElemsPerRep = numElems / repM;\n+\n+    const uint32_t bytesPerCacheline = 128;\n+    uint32_t bytesPerElem = dstElemTy.getIntOrFloatBitWidth() / 8;\n+    uint32_t numBox{1};\n+    for (int i = 0; i < rank; ++i) {\n+      auto dim = getDimOfOrder(dstOrder, i);\n+      auto tNumElems = shapePerCTA[dim];\n+      if (i == 0 && tNumElems * bytesPerElem > bytesPerCacheline) {\n+        tNumElems = bytesPerCacheline / bytesPerElem;\n+        numBox = (shapePerCTA[dim] + tNumElems - 1) / tNumElems;\n+      }\n+      if (i == 1) {\n+        tNumElems = tNumElems / repM / warpsPerCTA[0];\n+      }\n+      boxDims.emplace_back(tNumElems);\n+    }\n+    std::vector<uint32_t> elementStrides(rank, 1);\n+    tmaInfo.boxDims = boxDims;\n+    tmaInfo.elementStrides = elementStrides;\n+\n+    CUtensorMapSwizzle swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE;\n+    assert(((dstElemTy.getIntOrFloatBitWidth() == 16 &&\n+             sharedLayout.getVec() == 8) or\n+            (dstElemTy.getIntOrFloatBitWidth() == 32 &&\n+             sharedLayout.getVec() == 4)) &&\n+           \"Unexpected shared layout for StoreAsyncOp\");\n+    if (sharedLayout.getPerPhase() == 4 && sharedLayout.getMaxPhase() == 2)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_32B;\n+    else if (sharedLayout.getPerPhase() == 2 && sharedLayout.getMaxPhase() == 4)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_64B;\n+    else if (sharedLayout.getPerPhase() == 1 && sharedLayout.getMaxPhase() == 8)\n+      swizzle = CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_128B;\n+    else\n+      llvm::report_fatal_error(\"Unsupported shared layout for StoreAsyncOp\");\n+    tmaInfo.swizzle = swizzle;\n+    tmaInfo.interleave = CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE;\n+    tmaInfo.l2Promotion =\n+        CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_L2_128B;\n+    tmaInfo.oobFill =\n+        CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE;\n+\n+    tmaMetadata->emplace_back(tmaInfo);\n+\n+    Value llDst = adaptor.getDst();\n+    Value llSrc = adaptor.getSrc();\n+    auto srcShape = srcTy.getShape();\n+    auto dstElemPtrTy = ptr_ty(getTypeConverter()->convertType(dstElemTy), 3);\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+    smemBase = bitcast(smemBase, dstElemPtrTy);\n+\n+    SmallVector<Value> offsetVals;\n+    for (auto i = 0; i < srcShape.size(); ++i) {\n+      offsetVals.emplace_back(i32_val(0));\n+    }\n+\n+    Value tmaDesc =\n+        llFuncOp.getBody().front().getArgument(tmaInfo.TMADescArgIdx);\n+    auto ptrI8SharedTy = LLVM::LLVMPointerType::get(\n+        typeConverter->convertType(rewriter.getI8Type()), 3);\n+\n+    auto threadId = getThreadId(rewriter, loc);\n+    Value pred = icmp_eq(urem(threadId, i32_val(32)), i32_val(0));\n+\n+    auto llCoord = getTypeConverter()->unpackLLElements(loc, llDst, rewriter,\n+                                                        dst.getType());\n+    uint32_t boxStride = std::accumulate(boxDims.begin(), boxDims.end(), 1,\n+                                         std::multiplies<uint32_t>());\n+    boxStride = boxStride * repM * warpsPerCTA[0];\n+\n+    Value clusterCTAId = getClusterCTAId(rewriter, loc);\n+    SmallVector<Value> multiDimClusterCTAId =\n+        delinearize(rewriter, loc, clusterCTAId, CTAsPerCGA, CTAOrder);\n+\n+    // rowStride in bytes\n+    uint32_t rowStrideInBytes = shapePerCTA[dstOrder[0]] * bytesPerElem;\n+    uint32_t swizzlingByteWidth =\n+        std::min<uint32_t>(rowStrideInBytes, bytesPerCacheline);\n+\n+    unsigned numElemsPerSwizzlingRow = swizzlingByteWidth / bytesPerElem;\n+    unsigned leadingDimOffset =\n+        numElemsPerSwizzlingRow * shapePerCTA[dstOrder[1]];\n+\n+    uint32_t rowsPerRep = getShapePerCTATile(mmaLayout)[0];\n+\n+    Value warpId = udiv(threadId, i32_val(32));\n+    Value warpId0 = urem(urem(warpId, i32_val(warpsPerCTA[0])),\n+                         i32_val(srcShape[0] / instrShape[0]));\n+    auto srcOrder = triton::gpu::getOrder(srcLayout);\n+    unsigned inVec =\n+        srcOrder == sharedLayout.getOrder()\n+            ? triton::gpu::getContigPerThread(srcLayout)[srcOrder[0]]\n+            : 1;\n+    unsigned outVec = sharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    assert(minVec == 2);\n+\n+    auto wordTy = vec_ty(dstElemTy, minVec);\n+\n+    auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getSrc(),\n+                                                       rewriter, srcTy);\n+    for (uint32_t b = 0; b < numBox; ++b) {\n+      for (int rep = 0; rep < repM; ++rep) {\n+        Value rowOfWarp = add(mul(warpId0, i32_val(instrShape[0])),\n+                              i32_val(rep * rowsPerRep));\n+        uint32_t elemIdxOffset = rep * numElemsPerRep;\n+\n+        for (unsigned idx = 0; idx < numElemsPerRep / numBox; idx += 8) {\n+          uint32_t elemIdx = elemIdxOffset + b * numElemsPerRep / numBox + idx;\n+\n+          Value offset = rewriter.create<triton::nvgpu::OffsetOfStmatrixV4Op>(\n+              loc, i32_ty, threadId, rowOfWarp,\n+              i32_val(b * numElemsPerRep / numBox + idx), leadingDimOffset,\n+              numElemsPerSwizzlingRow, true);\n+\n+          Value addr = gep(dstElemPtrTy, smemBase, offset);\n+          Value words[4];\n+          for (unsigned i = 0; i < 8; ++i) {\n+            if (i % minVec == 0)\n+              words[i / 2] = undef(wordTy);\n+            words[i / 2] = insert_element(\n+                wordTy, words[i / 2], inVals[elemIdx + i], i32_val(i % minVec));\n+          }\n+\n+          rewriter.create<triton::nvgpu::StoreMatrixOp>(\n+              loc, bitcast(addr, ptrI8SharedTy),\n+              ValueRange{bitcast(words[0], i32_ty), bitcast(words[1], i32_ty),\n+                         bitcast(words[2], i32_ty), bitcast(words[3], i32_ty)});\n+        }\n+        rewriter.create<triton::nvgpu::FenceAsyncSharedOp>(loc, 0);\n+\n+        SmallVector<Value> coord;\n+        // raw coord\n+        for (int i = 0; i < rank; ++i) {\n+          auto dim = getDimOfOrder(dstOrder, i);\n+          coord.push_back(llCoord[dim]);\n+        }\n+        // coord with box and cta offset\n+        for (int i = 0; i < rank; ++i) {\n+          auto dim = getDimOfOrder(dstOrder, i);\n+          if (i == 0) {\n+            coord[i] = add(coord[i], i32_val(b * boxDims[i]));\n+            auto CTAOffset =\n+                mul(multiDimClusterCTAId[dim], i32_val(numBox * boxDims[i]));\n+            coord[i] = add(coord[i], CTAOffset);\n+          } else {\n+            Value blockOffset = i32_val(rep * instrShape[0] * warpsPerCTA[0]);\n+            Value warpOffset = mul(warpId0, i32_val(instrShape[0]));\n+            coord[i] = add(add(coord[i], add(blockOffset, warpOffset)),\n+                           mul(multiDimClusterCTAId[dim],\n+                               i32_val(boxDims[i] * repM * warpsPerCTA[0])));\n+          }\n+        }\n+        Value srcOffset =\n+            add(i32_val(b * boxStride + rep * instrShape[0] * warpsPerCTA[0] *\n+                                            instrShape[1] * warpsPerCTA[1] /\n+                                            numBox),\n+                mul(warpId0, i32_val(instrShape[0] * numElemsPerSwizzlingRow)));\n+        auto srcPtrTy = ptr_ty(getTypeConverter()->convertType(dstElemTy), 3);\n+        Value srcPtrBase = gep(srcPtrTy, smemBase, srcOffset);\n+        auto addr = bitcast(srcPtrBase, ptrI8SharedTy);\n+        rewriter.create<triton::nvgpu::TMAStoreTiledOp>(loc, tmaDesc, addr,\n+                                                        pred, coord);\n+      }\n+    }\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+\n private:\n   CUtensorMapDataType getCUtensorMapDataType(Type ty) const {\n     if (ty.isF16()) {\n@@ -1136,6 +1418,9 @@ struct InsertSliceAsyncV2OpConversion\n     auto rank = resultTy.getRank() - 1;\n \n     // TODO: support any valid rank in (3, 4, 5)\n+    // The sotre async op only supports tensor with ranke <= 5.\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensor-dimension-size-and-format\n     assert(rank > 0 && rank <= 5);\n     SmallVector<unsigned> shape;\n     auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 17, "deletions": 4, "changes": 21, "file_content_changes": "@@ -307,8 +307,10 @@ struct ReduceOpConversion\n     Operation *yield = block->getTerminator();\n     Operation *reduceOp = yield->getOperand(0).getDefiningOp();\n     if (!reduceOp || reduceOp->getNumOperands() != 2 ||\n-        reduceOp->getNumResults() != 1 ||\n-        !reduceOp->getResultTypes()[0].isInteger(32))\n+        reduceOp->getNumResults() != 1)\n+      return std::nullopt;\n+    auto intType = reduceOp->getResultTypes()[0].dyn_cast<IntegerType>();\n+    if (!intType || intType.getWidth() > 32)\n       return std::nullopt;\n     if (reduceOp->getOperand(0) != block->getArgument(0) ||\n         reduceOp->getOperand(1) != block->getArgument(1))\n@@ -382,8 +384,19 @@ struct ReduceOpConversion\n           mask = shl(i32_val(bitmask),\n                      and_(laneId, i32_val(~(numLaneToReduce - 1))));\n         }\n-        acc[0] = rewriter.create<NVVM::ReduxOp>(loc, acc[0].getType(), acc[0],\n-                                                *kind, mask);\n+        for (unsigned i = 0; i < acc.size(); ++i) {\n+          unsigned bitwidth = acc[i].getType().cast<IntegerType>().getWidth();\n+          if (bitwidth < 32) {\n+            if (*kind == NVVM::ReduxKind::MIN || *kind == NVVM::ReduxKind::MAX)\n+              acc[i] = sext(i32_ty, acc[i]);\n+            else\n+              acc[i] = zext(i32_ty, acc[i]);\n+          }\n+          acc[i] = rewriter.create<NVVM::ReduxOp>(loc, acc[i].getType(), acc[0],\n+                                                  *kind, mask);\n+          if (bitwidth < 32)\n+            acc[i] = trunc(int_ty(bitwidth), acc[i]);\n+        }\n         return;\n       }\n     }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 27, "deletions": 18, "changes": 45, "file_content_changes": "@@ -63,14 +63,17 @@ static void addWSNamedAttrs(Operation *op,\n \n class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n public:\n-  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx, bool isROCM)\n+  explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx, Target target)\n       : ConversionTarget(ctx) {\n     addLegalDialect<index::IndexDialect>();\n     addLegalDialect<LLVM::LLVMDialect>();\n-    if (isROCM) {\n-      addLegalDialect<ROCDL::ROCDLDialect>();\n-    } else {\n+    switch (target) {\n+    case Target::NVVM:\n       addLegalDialect<NVVM::NVVMDialect>();\n+      break;\n+    case Target::ROCDL:\n+      addLegalDialect<ROCDL::ROCDLDialect>();\n+      break;\n     }\n     addLegalOp<mlir::UnrealizedConversionCastOp>();\n   }\n@@ -359,13 +362,16 @@ struct CallOpConversion : public ConvertOpToLLVMPattern<triton::CallOp> {\n \n class TritonLLVMConversionTarget : public ConversionTarget {\n public:\n-  explicit TritonLLVMConversionTarget(MLIRContext &ctx, bool isROCM)\n+  explicit TritonLLVMConversionTarget(MLIRContext &ctx, Target target)\n       : ConversionTarget(ctx) {\n     addLegalDialect<LLVM::LLVMDialect>();\n-    if (isROCM) {\n-      addLegalDialect<ROCDL::ROCDLDialect>();\n-    } else {\n+    switch (target) {\n+    case Target::NVVM:\n       addLegalDialect<NVVM::NVVMDialect>();\n+      break;\n+    case Target::ROCDL:\n+      addLegalDialect<ROCDL::ROCDLDialect>();\n+      break;\n     }\n     addLegalDialect<mlir::triton::nvgpu::NVGPUDialect>();\n     addIllegalDialect<triton::TritonDialect>();\n@@ -387,7 +393,7 @@ struct ConvertTritonGPUToLLVM\n     mlir::LowerToLLVMOptions option(context);\n     option.overrideIndexBitwidth(32);\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-    TritonLLVMConversionTarget target(*context, isROCM);\n+    TritonLLVMConversionTarget convTarget(*context, target);\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n     int numCTAs = triton::gpu::TritonGPUDialect::getNumCTAs(mod);\n     int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n@@ -441,7 +447,7 @@ struct ConvertTritonGPUToLLVM\n     {\n       mlir::LowerToLLVMOptions option(context);\n       TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-      TritonLLVMFunctionConversionTarget funcTarget(*context, isROCM);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context, target);\n       RewritePatternSet funcPatterns(context);\n       funcPatterns.add<FuncOpConversion>(typeConverter, numWarps, allocation,\n                                          /*benefit=*/1);\n@@ -461,7 +467,7 @@ struct ConvertTritonGPUToLLVM\n     {\n       mlir::LowerToLLVMOptions option(context);\n       TritonGPUToLLVMTypeConverter typeConverter(context, option);\n-      TritonLLVMFunctionConversionTarget funcTarget(*context, isROCM);\n+      TritonLLVMFunctionConversionTarget funcTarget(*context, target);\n       RewritePatternSet funcPatterns(context);\n       funcPatterns.add<CallOpConversion>(typeConverter, numWarps, allocation,\n                                          /*benefit=*/1);\n@@ -522,7 +528,7 @@ struct ConvertTritonGPUToLLVM\n     populatePatterns1(populateTritonGPUToLLVMPatterns);\n     populatePatterns1(populateConvertLayoutOpToLLVMPatterns);\n     populatePatterns2(populateDotOpToLLVMPatterns);\n-    populatePatterns2(populateElementwiseOpToLLVMPatterns);\n+    populatePatterns4(populateElementwiseOpToLLVMPatterns);\n     populatePatterns3(populateLoadStoreOpToLLVMPatterns);\n     populatePatterns4(populateReduceOpToLLVMPatterns);\n     populatePatterns1(populateScanOpToLLVMPatterns);\n@@ -539,16 +545,19 @@ struct ConvertTritonGPUToLLVM\n     mlir::populateMathToLLVMConversionPatterns(typeConverter, patterns);\n \n     // Native lowering patterns\n-    if (isROCM) {\n+    switch (target) {\n+    case Target::NVVM:\n+      mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n+      break;\n+    case Target::ROCDL:\n       mlir::populateGpuToROCDLConversionPatterns(typeConverter, patterns,\n                                                  mlir::gpu::amd::HIP);\n-    } else {\n-      mlir::populateGpuToNVVMConversionPatterns(typeConverter, patterns);\n+      break;\n     }\n \n     mlir::cf::populateControlFlowToLLVMConversionPatterns(typeConverter,\n                                                           patterns);\n-    if (failed(applyPartialConversion(mod, target, std::move(patterns))))\n+    if (failed(applyPartialConversion(mod, convTarget, std::move(patterns))))\n       return signalPassFailure();\n \n     // Fold CTAId when there is only 1 CTA.\n@@ -828,9 +837,9 @@ struct ConvertTritonGPUToLLVM\n                                       .dyn_cast<MmaEncodingAttr>();\n       if (mmaLayout) {\n         bool isNativeHopperFP8 =\n-            AElType.isFloat8E5M2() || AElType.isFloat8E4M3FN();\n+            AElType.isFloat8E5M2() || AElType.isFloat8E4M3FNUZ();\n         bool isFP8 = isNativeHopperFP8 || AElType.isFloat8E5M2FNUZ() ||\n-                     AElType.isFloat8E4M3FNUZ();\n+                     AElType.isFloat8E4M3FN();\n         if (!isFP8 || (isNativeHopperFP8 && mmaLayout.isHopper()))\n           return;\n         promoteType = builder.getF16Type();"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 18, "deletions": 38, "changes": 56, "file_content_changes": "@@ -498,24 +498,6 @@ struct TritonAtomicRMWPattern\n   }\n };\n \n-template <class T>\n-struct TritonExternElementwisePattern : public OpConversionPattern<T> {\n-  using OpConversionPattern<T>::OpConversionPattern;\n-  using OpConversionPattern<T>::typeConverter;\n-  typedef typename OpConversionPattern<T>::OpAdaptor OpAdaptor;\n-\n-  LogicalResult\n-  matchAndRewrite(T op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<T>(\n-                      op, typeConverter->convertType(op.getType()),\n-                      adaptor.getArgs(), adaptor.getLibname(),\n-                      adaptor.getLibpath(), adaptor.getSymbol()),\n-                  adaptor.getAttributes());\n-    return success();\n-  }\n-};\n-\n template <class Op>\n struct TritonGenericPattern : public OpConversionPattern<Op> {\n   using OpConversionPattern<Op>::OpConversionPattern;\n@@ -695,26 +677,24 @@ class TritonReturnOpPattern : public OpConversionPattern<ReturnOp> {\n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n                             RewritePatternSet &patterns, unsigned numCTAs) {\n   MLIRContext *context = patterns.getContext();\n-  patterns\n-      .insert< // TODO: view should have custom pattern that views the layout\n-          TritonGenericPattern<triton::AdvanceOp>,\n-          TritonGenericPattern<triton::MakeTensorPtrOp>,\n-          TritonGenericPattern<triton::ViewOp>,\n-          TritonGenericPattern<triton::BitcastOp>,\n-          TritonGenericPattern<triton::FpToFpOp>,\n-          TritonGenericPattern<triton::IntToPtrOp>,\n-          TritonGenericPattern<triton::PtrToIntOp>,\n-          TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n-          TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n-          TritonReducePattern, TritonReduceReturnPattern, TritonScanPattern,\n-          TritonScanReturnPattern, TritonTransPattern, TritonExpandDimsPattern,\n-          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n-          TritonStorePattern,\n-          TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n-          TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n-          TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,\n-          TritonFuncOpPattern, TritonReturnOpPattern, TritonCallOpPattern>(\n-          typeConverter, context);\n+  patterns.insert< // TODO: view should have custom pattern that views the\n+                   // layout\n+      TritonGenericPattern<triton::AdvanceOp>,\n+      TritonGenericPattern<triton::MakeTensorPtrOp>,\n+      TritonGenericPattern<triton::ViewOp>,\n+      TritonGenericPattern<triton::BitcastOp>,\n+      TritonGenericPattern<triton::FpToFpOp>,\n+      TritonGenericPattern<triton::IntToPtrOp>,\n+      TritonGenericPattern<triton::PtrToIntOp>,\n+      TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n+      TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n+      TritonGenericPattern<triton::ElementwiseInlineAsmOp>, TritonReducePattern,\n+      TritonReduceReturnPattern, TritonScanPattern, TritonScanReturnPattern,\n+      TritonTransPattern, TritonExpandDimsPattern, TritonMakeRangePattern,\n+      TritonDotPattern, TritonLoadPattern, TritonStorePattern,\n+      TritonGenericPattern<triton::ExternElementwiseOp>, TritonPrintPattern,\n+      TritonAssertPattern, TritonAtomicRMWPattern, TritonFuncOpPattern,\n+      TritonReturnOpPattern, TritonCallOpPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 48, "deletions": 2, "changes": 50, "file_content_changes": "@@ -80,6 +80,16 @@ void LoadOp::print(OpAsmPrinter &printer) {\n   printer.printStrippedAttrOrType(getResult().getType());\n }\n \n+void LoadOp::getEffects(\n+    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>\n+        &effects) {\n+  effects.emplace_back(MemoryEffects::Read::get(), getPtr(),\n+                       SideEffects::DefaultResource::get());\n+  if (getIsVolatile())\n+    effects.emplace_back(MemoryEffects::Write::get(),\n+                         SideEffects::DefaultResource::get());\n+}\n+\n ParseResult StoreOp::parse(OpAsmParser &parser, OperationState &result) {\n   // Parse operands\n   SmallVector<OpAsmParser::UnresolvedOperand, 4> allOperands;\n@@ -401,8 +411,10 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n LogicalResult mlir::triton::DotOp::verify() {\n   auto aTy = getOperand(0).getType().cast<RankedTensorType>();\n   auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n-  if (aTy.getElementType() != bTy.getElementType())\n-    return emitError(\"element types of operands A and B must match\");\n+  if (aTy.getElementType().getIntOrFloatBitWidth() !=\n+      bTy.getElementType().getIntOrFloatBitWidth())\n+    return emitError(\n+        \"element types of operands A and B must have same bit width\");\n   auto aEncoding = aTy.getEncoding();\n   auto bEncoding = bTy.getEncoding();\n   if (!aEncoding && !bEncoding)\n@@ -416,6 +428,16 @@ LogicalResult mlir::triton::DotOp::verify() {\n                                                      bEncoding);\n }\n \n+//-- MakeRangeOp --\n+OpFoldResult MakeRangeOp::fold(FoldAdaptor adaptor) {\n+  // make_range(start, start + 1) -> constant(start)\n+  if (adaptor.getStart() + 1 == adaptor.getEnd()) {\n+    auto shapedType = getType().cast<ShapedType>();\n+    return SplatElementsAttr::get(shapedType, adaptor.getStartAttr());\n+  }\n+  return {};\n+}\n+\n //-- ReduceOp --\n static mlir::LogicalResult\n inferReduceReturnShape(const RankedTensorType &argTy, const Type &retEltTy,\n@@ -870,5 +892,29 @@ LogicalResult triton::ReturnOp::verify() {\n   return success();\n }\n \n+// -- ElementwiseInlineAsmOp --\n+void ElementwiseInlineAsmOp::getEffects(\n+    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>\n+        &effects) {\n+  if (getPure())\n+    return;\n+  effects.emplace_back(MemoryEffects::Write::get(),\n+                       SideEffects::DefaultResource::get());\n+  effects.emplace_back(MemoryEffects::Read::get(),\n+                       SideEffects::DefaultResource::get());\n+}\n+\n+// -- ExternElementwiseOp --\n+void ExternElementwiseOp::getEffects(\n+    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>\n+        &effects) {\n+  if (getPure())\n+    return;\n+  effects.emplace_back(MemoryEffects::Write::get(),\n+                       SideEffects::DefaultResource::get());\n+  effects.emplace_back(MemoryEffects::Read::get(),\n+                       SideEffects::DefaultResource::get());\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1205,6 +1205,10 @@ void SharedEncodingAttr::print(AsmPrinter &printer) const {\n \n bool MmaEncodingAttr::isVolta() const { return getVersionMajor() == 1; }\n \n+bool MmaEncodingAttr::isTuring() const {\n+  return getVersionMajor() == 2 && getVersionMinor() == 1;\n+}\n+\n bool MmaEncodingAttr::isAmpere() const { return getVersionMajor() == 2; }\n \n bool MmaEncodingAttr::isHopper() const { return getVersionMajor() == 3; }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 38, "deletions": 16, "changes": 54, "file_content_changes": "@@ -24,7 +24,7 @@ using ttg::SliceEncodingAttr;\n // supported\n static int getMMAVersionSafe(int computeCapability, tt::DotOp op) {\n   int baseVersion = 0;\n-  if (computeCapability < 80) {\n+  if (computeCapability < 75) {\n     baseVersion = 1;\n   } else if (computeCapability < 90) {\n     baseVersion = 2;\n@@ -43,17 +43,6 @@ static int getMMAVersionSafe(int computeCapability, tt::DotOp op) {\n   return 0;\n }\n \n-SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n-  if (version == 1)\n-    return {16, 16};\n-  else if (version == 2)\n-    return {16, 8};\n-  else {\n-    assert(false && \"version not supported\");\n-    return {0, 0};\n-  }\n-}\n-\n SmallVector<unsigned, 2>\n warpsPerTileV2(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps) {\n   auto filter = [&dotOp](Operation *op) {\n@@ -66,13 +55,11 @@ warpsPerTileV2(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps) {\n \n   SmallVector<unsigned, 2> ret = {1, 1};\n   SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n-  bool changed = false;\n   // TODO (@daadaada): double-check.\n   // original logic in\n   // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n   // seems buggy for shape = [32, 16] ?\n   do {\n-    changed = false;\n     if (ret[0] * ret[1] >= numWarps)\n       break;\n     if (shape[0] / shapePerWarp[0] / ret[0] >=\n@@ -115,6 +102,8 @@ warpsPerTileV3(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps,\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n   mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n+  mutable llvm::SmallVector<llvm::SetVector<Operation *>> dotOpSetVector;\n+  mutable llvm::SmallVector<unsigned> mmaV3InstrNs;\n \n   static bool bwdFilter(Operation *op) {\n     return op->getNumOperands() == 1 &&\n@@ -157,6 +146,36 @@ class BlockedToMMA : public mlir::RewritePattern {\n     }\n   }\n \n+  unsigned getMmaV3InstrN(tt::DotOp dotOp, unsigned currN) const {\n+    auto type = dotOp.getResult().getType().cast<RankedTensorType>();\n+    if (type.getEncoding().isa<MmaEncodingAttr>())\n+      return currN;\n+    for (size_t i = 0; i < dotOpSetVector.size(); ++i) {\n+      if (dotOpSetVector[i].count(dotOp.getOperation()) > 0)\n+        return mmaV3InstrNs[i];\n+    }\n+\n+    SetVector<Operation *> slices;\n+    mlir::getForwardSlice(dotOp.getResult(), &slices);\n+    mlir::getBackwardSlice(dotOp.getOperation(), &slices);\n+    unsigned N = currN;\n+    llvm::SetVector<Operation *> dotOpSet;\n+    for (Operation *iter : slices) {\n+      if (auto nextDotOp = dyn_cast<tt::DotOp>(iter)) {\n+        auto type = nextDotOp.getResult().getType().cast<RankedTensorType>();\n+        auto AType = nextDotOp.getOperand(0).getType().cast<RankedTensorType>();\n+        auto shapePerCTA = ttg::getShapePerCTA(type);\n+        auto instrShape = mmaVersionToInstrShape(3, shapePerCTA, AType);\n+        dotOpSet.insert(iter);\n+        if (instrShape[1] < N)\n+          N = instrShape[1];\n+      }\n+    }\n+    mmaV3InstrNs.push_back(N);\n+    dotOpSetVector.push_back(dotOpSet);\n+    return N;\n+  }\n+\n   static Value getMMAv3Operand(Value v, mlir::PatternRewriter &rewriter,\n                                int opIdx) {\n     auto cvtOp = dyn_cast_or_null<ttg::ConvertLayoutOp>(v.getDefiningOp());\n@@ -214,6 +233,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n     auto instrShape =\n         mmaVersionToInstrShape(versionMajor, retShapePerCTA, AType);\n+    if (versionMajor == 3)\n+      instrShape[1] = getMmaV3InstrN(dotOp, instrShape[1]);\n \n     // operands\n     Value a = dotOp.getA();\n@@ -255,11 +276,12 @@ class BlockedToMMA : public mlir::RewritePattern {\n           instrShape, oldAType.getShape(), oldBType.getShape(), retShapePerCTA,\n           isARow, isBRow, mmaV1Counter++);\n     } else if (versionMajor == 2 || versionMajor == 3) {\n+      int versionMinor = computeCapability == 75 ? 1 : 0;\n       auto warpsPerTile = getWarpsPerTile(dotOp, retShapePerCTA, versionMajor,\n                                           numWarps, instrShape);\n       mmaEnc = ttg::MmaEncodingAttr::get(oldRetType.getContext(), versionMajor,\n-                                         0 /*versionMinor*/, warpsPerTile,\n-                                         CTALayout, instrShape);\n+                                         versionMinor, warpsPerTile, CTALayout,\n+                                         instrShape);\n     }\n     auto newRetType = RankedTensorType::get(\n         oldRetType.getShape(), oldRetType.getElementType(), mmaEnc);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -121,6 +121,9 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n         cvtArgOp->getDialect()->getTypeID() !=\n             mlir::TypeID::get<arith::ArithDialect>())\n       return mlir::failure();\n+    // not handled in elementwise lowering.\n+    if (isa<arith::TruncIOp, arith::TruncFOp>(cvtArgOp))\n+      return mlir::failure();\n     // only considers conversions to dot operand\n     if (!cvtTy.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n       return mlir::failure();\n@@ -248,8 +251,6 @@ class TritonGPUOptimizeDotOperandsPass\n     patterns.add<FuseTransHopper>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();\n-    if (fixupLoops(m).failed())\n-      signalPassFailure();\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeEpilogue.cpp", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -127,9 +127,6 @@ class TritonGPUOptimizeEpiloguePass\n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n-    if (fixupLoops(m).failed()) {\n-      signalPassFailure();\n-    }\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 743, "deletions": 502, "changes": 1245, "file_content_changes": "@@ -12,11 +12,11 @@\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"mlir/Transforms/RegionUtils.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n-\n #include <memory>\n \n using namespace mlir;\n@@ -82,556 +82,774 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n   }\n };\n \n-// It's beneficial to move the conversion\n-// to after the reduce if necessary since it will be\n-// done on a rank-reduced tensor hence cheaper\n-class SimplifyReduceCvt : public mlir::RewritePattern {\n+//\n+class ConvertDotConvert : public mlir::RewritePattern {\n public:\n-  explicit SimplifyReduceCvt(mlir::MLIRContext *context)\n+  ConvertDotConvert(mlir::MLIRContext *context)\n       : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             2, context) {}\n+                             1, context) {}\n \n-  mlir::LogicalResult\n+  LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n+    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto dotOp = dstOp.getSrc().getDefiningOp<triton::DotOp>();\n+    if (!dotOp)\n       return mlir::failure();\n-    auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n-    triton::ReduceOp reduce;\n-    for (auto &use : convert.getResult().getUses()) {\n-      auto owner = llvm::dyn_cast<triton::ReduceOp>(use.getOwner());\n-      if (!owner) {\n-        continue;\n-      }\n-\n-      // TODO: This only moves conversions from the first argument which is\n-      // fine for argmin/argmax but may not be optimal generally\n-      if (convert.getResult() != owner.getOperands()[0]) {\n-        continue;\n-      }\n-      reduce = owner;\n-      break;\n-    }\n-    if (!reduce)\n+    if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n+        std::distance(dotOp->user_begin(), dotOp->user_end()) != 1)\n       return mlir::failure();\n-\n-    SmallVector<Value> newOperands = reduce.getOperands();\n-\n-    newOperands[0] = convert.getOperand();\n-    auto newEncoding =\n-        newOperands[0].getType().cast<RankedTensorType>().getEncoding();\n-\n-    // this may generate unsupported conversions in the LLVM codegen\n-    if (newEncoding.isa<triton::gpu::MmaEncodingAttr>()) {\n-      return failure();\n-    }\n-\n-    // ReduceOp does not support SharedLayout as its src layout, therefore\n-    // ConvertLayoutOp and ReduceOp should not be swapped when the conversion is\n-    // from SharedLayout to DistributedLayout\n-    if (newEncoding.isa<triton::gpu::SharedEncodingAttr>()) {\n+    auto cvtOp =\n+        dotOp.getOperand(2).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n+    if (!cvtOp)\n+      return mlir::failure();\n+    if (!cvtOp.getSrc().getDefiningOp<triton::LoadOp>())\n       return failure();\n-    }\n-\n-    for (unsigned i = 1; i < newOperands.size(); ++i) {\n-      auto oldTy = newOperands[i].getType().cast<RankedTensorType>();\n-      RankedTensorType newTy =\n-          RankedTensorType::Builder(oldTy).setEncoding(newEncoding);\n-\n-      newOperands[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), newTy, newOperands[i]);\n-    }\n-\n-    rewriter.setInsertionPoint(reduce);\n-    auto newReduce = rewriter.create<triton::ReduceOp>(\n-        op->getLoc(), newOperands, reduce.getAxis());\n-    auto &newCombineOp = newReduce.getCombineOp();\n-    rewriter.cloneRegionBefore(reduce.getCombineOp(), newCombineOp,\n-                               newCombineOp.end());\n-\n-    SmallVector<Value> newRet = newReduce.getResult();\n-    auto oldTypes = reduce.getResult().getType();\n-    for (unsigned i = 0; i < reduce.getNumOperands(); ++i) {\n-      // it's still beneficial to move the conversion\n-      // to after the reduce if necessary since it will be\n-      // done on a rank-reduced tensor hence cheaper\n-      if (newRet[i].getType() != oldTypes[i])\n-        newRet[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            op->getLoc(), oldTypes[i], newRet[i]);\n-    }\n-    rewriter.replaceAllUsesWith(reduce.getResult(), newRet);\n+    auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n+    auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+    if (dstTy != srcTy)\n+      return mlir::failure();\n \n-    return success();\n+    auto _0f = rewriter.create<arith::ConstantOp>(\n+        op->getLoc(), dstTy.getElementType(),\n+        rewriter.getZeroAttr(dstTy.getElementType()));\n+    auto _0 = rewriter.create<triton::SplatOp>(\n+        op->getLoc(), dotOp.getResult().getType(), _0f);\n+    auto newDot = rewriter.create<triton::DotOp>(\n+        op->getLoc(), dotOp.getResult().getType(), dotOp.getOperand(0),\n+        dotOp.getOperand(1), _0, dotOp.getAllowTF32());\n+    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), dstTy, newDot.getResult());\n+    rewriter.replaceOpWithNewOp<arith::AddFOp>(op, newCvt, cvtOp.getOperand());\n+    return mlir::success();\n   }\n };\n \n-// Layout conversions can't deduce their return type automatically.\n-// IIUC they are therefore not handled by DRR right now\n-class SimplifyConversion : public mlir::RewritePattern {\n+// Class to propagate layout globally within a function.\n+// The current algorithm works by analysis the IR and doing a one shot rewrite\n+// based on the analysis. The algorithm is as follows:\n+// 1. Find all the anchor ops. These are ops that have a layout we want to\n+// preserve.\n+//\n+// 2. Propagate the layout to every op reachable which is a transitive child of\n+// an anchor op until we reach a fix point.\n+// An op can have multiple transitive anchor parents therefore at this stage\n+// it may have multiple layout associated to it.\n+//\n+// 3. Resolve conflicts by deciding which of the multiple layouts the op should\n+// keep. If one of the parents has a different layout than what is picked a\n+// convert operation will be inserted. After this stage each value should have\n+// only one layout associated.\n+//\n+// 4. Rewrite the IR by walking the function following dominance order. Since we\n+// assume the IR is structured we just need to process the regions in the\n+// correct order. For each op rewrite it using the layout decided by the\n+// analysis phase.\n+class LayoutPropagation {\n public:\n-  explicit SimplifyConversion(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             4, context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n-      return mlir::failure();\n-    auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n-    return ConvertLayoutOp::canonicalize(convert, rewriter);\n-  }\n+  // Structure to keep track of the layout associated to a value.\n+  struct LayoutInfo {\n+    LayoutInfo(Attribute encoding) { encodings.insert(encoding); }\n+    LayoutInfo() {}\n+    llvm::SmallSetVector<Attribute, 8> encodings;\n+  };\n+  LayoutPropagation(triton::FuncOp F) : funcOp(F) {}\n+  // Find the anchor ops and set their layout in the data structure.\n+  void initAnchorLayout();\n+  // Recursively Propagate the layout to all the users of the anchor ops until\n+  // we reach a fix point.\n+  void propagateLayout();\n+  // Add layouts given in `Info` to the uses of `value`.\n+  SmallVector<Value> propagateToUsers(Value value, LayoutInfo &info);\n+  // Set the encoding to all the values and fill out the values with new layout\n+  // in `changed`.\n+  void setEncoding(ValueRange values, LayoutInfo &info,\n+                   SmallVector<Value> &changed, Operation *op);\n+  // Resolve cases where a value has multiple layouts associated to it.\n+  void resolveConflicts();\n+  // Rewrite the IR for the full module.\n+  void rewrite();\n+  // Rewrite the IR for a region.\n+  void rewriteRegion(Region &R);\n+  // Rewrite an op based on the layout picked by the analysis.\n+  Operation *rewriteOp(Operation *op);\n+  // Rewrite a for op based on the layout picked by the analysis.\n+  Operation *rewriteForOp(scf::ForOp forOp);\n+  Operation *rewriteIfOp(scf::IfOp ifOp);\n+  Operation *rewriteYieldOp(scf::YieldOp yieldOp);\n+  Operation *cloneElementwise(OpBuilder &rewriter, Operation *op,\n+                              Attribute encoding);\n+  // Map the original value to the rewritten one.\n+  void map(Value old, Value newV);\n+  // Return the mapped value in the given encoding. This will insert a convert\n+  // if the encoding is different than the encoding decided at resolve time.\n+  Value getValueAs(Value value, Attribute encoding);\n+  // Dump the current stage of layout information.\n+  void dump();\n+\n+private:\n+  // map from value to layout information.\n+  llvm::MapVector<Value, LayoutInfo> layouts;\n+  // map of the values rewrite based on their encoding.\n+  DenseMap<std::pair<Value, Attribute>, Value> rewriteMapping;\n+  std::vector<Operation *> opToDelete;\n+  triton::FuncOp funcOp;\n };\n \n-// -----------------------------------------------------------------------------\n-//\n-// -----------------------------------------------------------------------------\n+} // namespace\n \n-// op(cvt(arg_0), arg_1, ..., arg_n)\n-// -> cvt(op(arg_0, cvt(arg_1), ..., cvt(arg_n)))\n-void pushConversionForward(triton::gpu::ConvertLayoutOp cvt,\n-                           SetVector<Operation *> &cvtSlices,\n-                           mlir::PatternRewriter &rewriter) {\n-  auto srcEncoding =\n-      cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n-  auto dstEncoding =\n-      cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n-  IRMapping mapping;\n-  auto op = cvtSlices.front();\n-  for (Value arg : op->getOperands()) {\n-    if (arg.getDefiningOp() == cvt)\n-      mapping.map(arg, cvt.getOperand());\n-    else {\n-      auto oldType = arg.getType().dyn_cast<RankedTensorType>();\n-      // TODO: we may be creating block pointer load/store with mismatching\n-      // pointer type.\n-      if (!oldType)\n+// Look ahead to at the transitive uses and see if there is a convert to mma\n+// operations.\n+static bool hasConvertToMMATransisitiveUse(Operation *op, Attribute encoding) {\n+  SmallVector<Value> queue = {op->getResult(0)};\n+  SetVector<Operation *> forwardSlice;\n+  llvm::SmallDenseSet<Value> seen;\n+  while (!queue.empty()) {\n+    Value currentValue = queue.back();\n+    queue.pop_back();\n+    getForwardSlice(currentValue, &forwardSlice);\n+    for (Operation *op : forwardSlice) {\n+      if (auto convertOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+        if (convertOp.getResult()\n+                .getType()\n+                .cast<RankedTensorType>()\n+                .getEncoding() == encoding)\n+          return true;\n+      }\n+      auto yield = dyn_cast<scf::YieldOp>(op);\n+      if (!yield)\n         continue;\n-      auto newType = RankedTensorType::get(\n-          oldType.getShape(), oldType.getElementType(), srcEncoding);\n-      auto cvtI = rewriter.create<triton::gpu::ConvertLayoutOp>(arg.getLoc(),\n-                                                                newType, arg);\n-      if (Operation *argOp = arg.getDefiningOp())\n-        cvtI->moveAfter(argOp);\n-      mapping.map(arg, cvtI);\n+      auto forOp = dyn_cast<scf::ForOp>(yield.getOperation()->getParentOp());\n+      if (!forOp)\n+        continue;\n+      for (OpOperand &operand : yield->getOpOperands()) {\n+        Operation *def = operand.get().getDefiningOp();\n+        if (def && forwardSlice.count(def) &&\n+            (seen.insert(operand.get()).second == true))\n+          queue.push_back(forOp.getRegionIterArg(operand.getOperandNumber()));\n+      }\n     }\n   }\n-  rewriter.setInsertionPoint(op);\n-  if (op->getNumResults() == 0) {\n-    Operation *newOp = cloneWithInferType(rewriter, op, mapping);\n-    rewriter.eraseOp(op);\n-    return;\n-  }\n-  auto *newOp = cloneWithInferType(rewriter, op, mapping);\n-  auto newType = newOp->getResult(0).getType().cast<RankedTensorType>();\n-  auto newCvtType = RankedTensorType::get(\n-      newType.getShape(), newType.getElementType(), dstEncoding);\n-  auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-      newOp->getLoc(), newCvtType, newOp->getResult(0));\n-  rewriter.replaceOp(op, newCvt->getResults());\n+  return false;\n }\n \n-//\n-class MoveConvertOutOfIf : public mlir::RewritePattern {\n-public:\n-  explicit MoveConvertOutOfIf(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::IfOp::getOperationName(), 2, context) {}\n+// Return true if the op is an op with a layout we don't want to change. We will\n+// propagate the layout starting from anchor ops.\n+static bool isLayoutAnchor(Operation *op) {\n+  if (isa<triton::LoadOp, triton::StoreOp>(op))\n+    return isExpensiveLoadOrStore(op);\n+  if (isa<triton::DotOp, triton::AtomicRMWOp, triton::AtomicCASOp>(op))\n+    return true;\n+  return false;\n+}\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ifOp = cast<scf::IfOp>(*op);\n-    // If \u201cscf.if\u201d defines no values, \u201cscf.yield\u201d will be inserted implicitly.\n-    // However, \"scf.else\" is not required to be present, so we need to check\n-    // if it exists.\n-    auto thenYield = ifOp.thenYield();\n-    int numOps = thenYield.getNumOperands();\n-    SmallVector<Value> newThenYieldOps = thenYield.getOperands();\n-    SetVector<Operation *> thenCvts;\n-    SmallVector<Type> newRetTypes;\n-\n-    bool hasElse = !ifOp.getElseRegion().empty();\n-\n-    scf::YieldOp elseYield;\n-    SmallVector<Value> newElseYieldOps;\n-    SetVector<Operation *> elseCvts;\n-    if (hasElse) {\n-      elseYield = ifOp.elseYield();\n-      newElseYieldOps = elseYield.getOperands();\n+void LayoutPropagation::initAnchorLayout() {\n+  funcOp.walk([&](Operation *op) {\n+    if (isLayoutAnchor(op)) {\n+      for (auto result : op->getResults()) {\n+        if (auto tensorType = result.getType().dyn_cast<RankedTensorType>()) {\n+          // Workaround, don't popagate MMA layout unless there is a convert\n+          // back to mma further down to avoid generating reduction with MMA\n+          // layout that may have lower performance.\n+          // This can be improved with more aggressive backward propagation.\n+          if (tensorType.getEncoding().isa<triton::gpu::MmaEncodingAttr>() &&\n+              !hasConvertToMMATransisitiveUse(op, tensorType.getEncoding()))\n+            continue;\n+          layouts.insert({result, tensorType.getEncoding()});\n+        }\n+      }\n     }\n+  });\n+}\n \n-    IRMapping mapping;\n-    for (size_t i = 0; i < numOps; i++) {\n-      auto thenCvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-          thenYield.getOperand(i).getDefiningOp());\n-      if (hasElse) {\n-        auto elseYield = ifOp.elseYield();\n-        auto elseCvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-            elseYield.getOperand(i).getDefiningOp());\n-        if (thenCvt && elseCvt &&\n-            std::distance(elseCvt->user_begin(), elseCvt->user_end()) == 1 &&\n-            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1 &&\n-            thenCvt.getOperand().getType() == elseCvt.getOperand().getType()) {\n-          // If thenCvt and elseCvt's type are the same, it means a single\n-          // conversion is enough to replace both of them. We can move the\n-          // conversion out of scf.if and replace both thenCvt and elseCvt with\n-          // the new conversion.\n-          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n-          thenCvts.insert((Operation *)thenCvt);\n-          newRetTypes.push_back(thenCvt.getOperand().getType());\n-          mapping.map(elseCvt.getResult(), elseCvt.getOperand());\n-          elseCvts.insert((Operation *)elseCvt);\n-        } else\n-          // Cannot move out of scf.if because thenCvt != elseCvt\n-          // Moving it out of scf.if will introduce a new conversion\n-          newRetTypes.push_back(thenYield.getOperand(i).getType());\n-      } else {\n-        if (thenCvt &&\n-            std::distance(thenCvt->user_begin(), thenCvt->user_end()) == 1) {\n-          // If there's only a single use of the conversion then we can move it\n-          mapping.map(thenCvt.getResult(), thenCvt.getOperand());\n-          thenCvts.insert((Operation *)thenCvt);\n-          newRetTypes.push_back(thenCvt.getOperand().getType());\n-        } else\n-          // Cannot move out of scf.if because either there's another use of\n-          // the conversion or there's no conversion at all\n-          newRetTypes.push_back(thenYield.getOperand(i).getType());\n-      }\n+void LayoutPropagation::setEncoding(ValueRange values, LayoutInfo &info,\n+                                    SmallVector<Value> &changed,\n+                                    Operation *op) {\n+  for (Value value : values) {\n+    if (!value.getType().isa<RankedTensorType>())\n+      continue;\n+    bool hasChanged = false;\n+    for (auto encoding : info.encodings) {\n+      auto dstEncoding = inferDstEncoding(op, encoding);\n+      if (dstEncoding)\n+        hasChanged |= layouts[value].encodings.insert(*dstEncoding);\n     }\n-    if (mapping.getValueMap().empty())\n-      return mlir::failure();\n+    if (hasChanged)\n+      changed.push_back(value);\n+  }\n+}\n \n-    auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newRetTypes,\n-                                              ifOp.getCondition(), hasElse);\n-    auto rematerialize = [&](Block *block, SetVector<Operation *> &cvts) {\n-      for (Operation &op : block->getOperations()) {\n-        if (cvts.contains(&op)) {\n-          if (mapping.contains(op.getOperand(0)))\n-            mapping.map(op.getResult(0), mapping.lookup(op.getOperand(0)));\n-          continue;\n-        }\n-        cloneWithInferType(rewriter, &op, mapping);\n-      }\n-    };\n-    rewriter.setInsertionPointToEnd(newIfOp.thenBlock());\n-    rematerialize(ifOp.thenBlock(), thenCvts);\n-    if (hasElse) {\n-      rewriter.setInsertionPointToEnd(newIfOp.elseBlock());\n-      rematerialize(ifOp.elseBlock(), elseCvts);\n+SmallVector<Value> LayoutPropagation::propagateToUsers(Value value,\n+                                                       LayoutInfo &info) {\n+  SmallVector<Value> changed;\n+  for (OpOperand &use : value.getUses()) {\n+    Operation *user = use.getOwner();\n+    if (auto forOp = dyn_cast<scf::ForOp>(user)) {\n+      Value arg = forOp.getRegionIterArgForOpOperand(use);\n+      Value result = forOp.getResultForOpOperand(use);\n+      setEncoding({arg, result}, info, changed, user);\n+      continue;\n     }\n+    if (auto yieldOp = dyn_cast<scf::YieldOp>(user)) {\n+      auto parent = yieldOp->getParentOp();\n+      SmallVector<Value> valuesToPropagate = {\n+          parent->getResult(use.getOperandNumber())};\n+      if (auto forOp = dyn_cast<scf::ForOp>(parent))\n+        valuesToPropagate.push_back(\n+            forOp.getRegionIterArg(use.getOperandNumber()));\n+      if (isa<scf::ForOp, scf::IfOp>(parent))\n+        setEncoding({valuesToPropagate}, info, changed, user);\n+      // TODO: handle while.\n+      continue;\n+    }\n+    // Workaround: don't propagate through truncI\n+    if (isa<arith::TruncIOp>(user))\n+      continue;\n+    if (user->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() ||\n+        user->hasTrait<mlir::OpTrait::Elementwise>() ||\n+        isa<triton::ReduceOp, triton::ExpandDimsOp,\n+            triton::gpu::ConvertLayoutOp>(user)) {\n+      setEncoding(user->getResults(), info, changed, user);\n+      continue;\n+    }\n+  }\n+  return changed;\n+}\n+\n+void LayoutPropagation::propagateLayout() {\n+  SmallVector<Value> queue;\n+  for (auto it : layouts) {\n+    queue.push_back(it.first);\n+  }\n+  while (!queue.empty()) {\n+    Value currentValue = queue.back();\n+    LayoutInfo info = layouts[currentValue];\n+    queue.pop_back();\n+    SmallVector<Value> changed = propagateToUsers(currentValue, info);\n+    queue.insert(queue.end(), changed.begin(), changed.end());\n+  }\n+}\n \n-    rewriter.setInsertionPointAfter(newIfOp);\n-    SmallVector<Value> newRetValues = newIfOp.getResults();\n-    for (size_t i = 0; i < numOps; i++) {\n-      if (newIfOp.getResult(i).getType() != ifOp.getResult(i).getType()) {\n-        newRetValues[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            newIfOp.getLoc(), ifOp.getResult(i).getType(),\n-            newIfOp.getResult(i));\n+void LayoutPropagation::resolveConflicts() {\n+  for (auto &it : layouts) {\n+    LayoutInfo &info = it.second;\n+    if (info.encodings.size() <= 1)\n+      continue;\n+    // Hacky resolve, prefer block encoding.\n+    // TODO: add a proper heuristic.\n+    Attribute encoding = *info.encodings.begin();\n+    for (Attribute e : info.encodings) {\n+      if (e.isa<triton::gpu::BlockedEncodingAttr>()) {\n+        encoding = e;\n+        break;\n       }\n     }\n-\n-    rewriter.replaceOp(op, newRetValues);\n-    return mlir::success();\n+    info.encodings.clear();\n+    info.encodings.insert(encoding);\n   }\n-};\n+}\n \n-//\n-class RematerializeForward : public mlir::RewritePattern {\n-public:\n-  explicit RematerializeForward(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n+void LayoutPropagation::dump() {\n+  for (auto it : layouts) {\n+    llvm::errs() << \"Value: \";\n+    OpPrintingFlags flags;\n+    flags.skipRegions();\n+    it.first.print(llvm::errs(), flags);\n+    llvm::errs() << \" \\n encoding:\\n\";\n+    for (auto encoding : it.second.encodings) {\n+      encoding.print(llvm::errs());\n+      llvm::errs() << \"\\n\";\n+    }\n+    llvm::errs() << \"--\\n\";\n+  }\n+}\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *cvtOp,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(*cvtOp);\n-    auto srcEncoding =\n-        cvt.getOperand().getType().cast<RankedTensorType>().getEncoding();\n-    auto dstEncoding =\n-        cvt.getResult().getType().cast<RankedTensorType>().getEncoding();\n-    if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>() ||\n-        dstEncoding.isa<triton::gpu::SharedEncodingAttr>())\n-      return failure();\n-    // heuristics for flash attention\n-    if (srcEncoding.isa<triton::gpu::SliceEncodingAttr>())\n-      return failure();\n-    // For cases like:\n-    // %0 = convert_layout %arg0\n-    // We should try to move %0 out of scf.for first, if it couldn't be moved\n-    // out additional conversions will be added to the loop body.\n-    if (!cvt.getOperand().getDefiningOp() &&\n-        isa<scf::ForOp>(cvt->getParentOp()))\n-      return failure();\n+void LayoutPropagation::rewrite() { rewriteRegion(funcOp->getRegion(0)); }\n \n-    SetVector<Operation *> cvtSlices;\n-    auto filter = [&](Operation *op) {\n-      return op->getBlock() == cvt->getBlock() &&\n-             !isa<triton::gpu::ConvertLayoutOp, scf::YieldOp>(op) &&\n-             !(isa<triton::ReduceOp>(op) &&\n-               !op->getResult(0).getType().isa<RankedTensorType>());\n-    };\n-    mlir::getForwardSlice(cvt.getResult(), &cvtSlices, {filter});\n-    if (cvtSlices.empty())\n-      return failure();\n+static bool allowChangingSrcEncoding(Operation *op) {\n+  // For reductions returning a scalar we can change the src encoding without\n+  // affecting the output.\n+  if (isa<triton::ReduceOp>(op) &&\n+      !op->getResultTypes()[0].isa<RankedTensorType>() &&\n+      op->getNumOperands() == 1)\n+    return true;\n+  return false;\n+}\n \n-    for (Operation *op : cvtSlices) {\n-      // don't rematerialize anything expensive\n-      if (isExpensiveToRemat(op, srcEncoding))\n-        return failure();\n-      // don't rematerialize non-element-wise\n-      if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n-          !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp, triton::AssertOp, triton::PrintOp,\n-               triton::ReduceOp>(op))\n-        return failure();\n-      // don't rematerialize if it adds an extra conversion that can't\n-      // be removed\n-      for (Value arg : op->getOperands()) {\n-        Operation *argOp = arg.getDefiningOp();\n-        SetVector<Operation *> processed;\n-        SetVector<Attribute> layout;\n-        llvm::MapVector<Value, Attribute> toConvert;\n-        int numAddedConvs = simulateBackwardRematerialization(\n-            argOp, processed, layout, toConvert, srcEncoding);\n-        if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n-            cvtSlices.count(argOp) == 0 && numAddedConvs > 0)\n-          return failure();\n+void LayoutPropagation::rewriteRegion(Region &region) {\n+  SmallVector<Region *> queue = {&region};\n+  while (!queue.empty()) {\n+    Region *currentRegion = queue.back();\n+    queue.pop_back();\n+    for (Operation &op : currentRegion->getOps()) {\n+      bool needRewrite = false;\n+      SmallVector<Value> results = op.getResults();\n+      for (Value result : results) {\n+        auto it = layouts.find(result);\n+        // If we haven't mapped this value skip.\n+        if (it == layouts.end())\n+          continue;\n+        LayoutInfo &info = it->second;\n+        assert(info.encodings.size() == 1 &&\n+               \"we should have resolved to a single encoding\");\n+        auto encoding = result.getType().cast<RankedTensorType>().getEncoding();\n+        // If the encoding is already what we want skip.\n+        if (encoding == *info.encodings.begin())\n+          continue;\n+        needRewrite = true;\n       }\n+      if (needRewrite) {\n+        Operation *newOp = rewriteOp(&op);\n+        for (Region &R : newOp->getRegions())\n+          queue.push_back(&R);\n+      } else if (auto yieldOp = dyn_cast<scf::YieldOp>(&op)) {\n+        rewriteYieldOp(yieldOp);\n+      } else {\n+        bool canChangeSrcEncoding = allowChangingSrcEncoding(&op);\n+        // If we don't need to rewrite the op we still need to remap the\n+        // operands.\n+        for (OpOperand &operand : op.getOpOperands()) {\n+          auto it = layouts.find(operand.get());\n+          if (it == layouts.end())\n+            continue;\n+          Attribute encoding =\n+              operand.get().getType().cast<RankedTensorType>().getEncoding();\n+          if (canChangeSrcEncoding)\n+            encoding = it->second.encodings[0];\n+          Value newOperand = getValueAs(operand.get(), encoding);\n+          op.setOperand(operand.getOperandNumber(), newOperand);\n+        }\n+        for (Region &R : op.getRegions())\n+          queue.push_back(&R);\n+      }\n+    }\n+  }\n+  for (Operation *op : llvm::reverse(opToDelete))\n+    op->erase();\n+}\n+\n+void LayoutPropagation::map(Value old, Value newV) {\n+  rewriteMapping[{old, newV.getType().cast<RankedTensorType>().getEncoding()}] =\n+      newV;\n+}\n+\n+Value LayoutPropagation::getValueAs(Value value, Attribute encoding) {\n+  if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n+    Value rewrittenValue;\n+    auto layoutIt = layouts.find(value);\n+    if (layoutIt == layouts.end()) {\n+      rewrittenValue = value;\n+    } else {\n+      assert(layoutIt->second.encodings.size() == 1 &&\n+             \"we should have resolved to a single encoding\");\n+      Attribute encodingPicked = *(layoutIt->second.encodings.begin());\n+      if (encodingPicked == tensorType.getEncoding())\n+        rewrittenValue = value;\n+      else\n+        rewrittenValue = rewriteMapping[{value, encodingPicked}];\n     }\n+    assert(rewrittenValue);\n+    if (rewrittenValue.getType().cast<RankedTensorType>().getEncoding() ==\n+        encoding)\n+      return rewrittenValue;\n+    OpBuilder rewriter(value.getContext());\n+    rewriter.setInsertionPointAfterValue(rewrittenValue);\n+    auto tmpType = RankedTensorType::get(tensorType.getShape(),\n+                                         tensorType.getElementType(), encoding);\n+    Value converted = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        value.getLoc(), tmpType, rewrittenValue);\n+    // TODO: we could cache the conversion.\n+    return converted;\n+  }\n+  return value;\n+}\n \n-    // Call SimplifyReduceCvt instead of the general push conversion forward\n-    if (isa<triton::ReduceOp>(cvtSlices.front()))\n-      return failure();\n+Operation *LayoutPropagation::cloneElementwise(OpBuilder &rewriter,\n+                                               Operation *op,\n+                                               Attribute encoding) {\n+  Operation *newOp = rewriter.clone(*op);\n+  for (OpOperand &operand : op->getOpOperands())\n+    newOp->setOperand(\n+        operand.getOperandNumber(),\n+        getValueAs(operand.get(), *inferSrcEncoding(op, encoding)));\n+  for (unsigned i = 0, e = op->getNumResults(); i < e; ++i) {\n+    auto origType = op->getResult(i).getType().dyn_cast<RankedTensorType>();\n+    if (!origType)\n+      continue;\n+    auto newType = RankedTensorType::get(origType.getShape(),\n+                                         origType.getElementType(), encoding);\n+    newOp->getResult(i).setType(newType);\n+  }\n+  return newOp;\n+}\n \n-    pushConversionForward(cvt, cvtSlices, rewriter);\n-    return success();\n+Operation *LayoutPropagation::rewriteForOp(scf::ForOp forOp) {\n+  SmallVector<Value> operands;\n+  OpBuilder rewriter(forOp);\n+  for (auto [operand, result] :\n+       llvm::zip(forOp.getInitArgs(), forOp.getResults())) {\n+    Value convertedOperand = operand;\n+    if (layouts.count(result))\n+      convertedOperand =\n+          getValueAs(operand, *layouts[result].encodings.begin());\n+    operands.push_back(convertedOperand);\n+  }\n+  auto newForOp = rewriter.create<scf::ForOp>(\n+      forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n+      forOp.getStep(), operands);\n+\n+  newForOp.getBody()->getOperations().splice(\n+      newForOp.getBody()->getOperations().begin(),\n+      forOp.getBody()->getOperations());\n+\n+  for (auto [oldResult, newResult] :\n+       llvm::zip(forOp.getResults(), newForOp.getResults())) {\n+    if (oldResult.getType() == newResult.getType()) {\n+      oldResult.replaceAllUsesWith(newResult);\n+      continue;\n+    }\n+    map(oldResult, newResult);\n   }\n-};\n \n-// Layout conversions are expensive. They require going through\n-// shared memory, which is orders of magnitude slower than\n-// other non-i/o operations in the dialect.\n-// It therefore makes sense to remove them whenever possible,\n-// even if it means rematerializing all values whose definitions\n-// are reachable from it without passing through any memory operation.\n-class RematerializeBackward : public mlir::RewritePattern {\n-public:\n-  explicit RematerializeBackward(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             3, context) {}\n+  for (auto [oldArg, newArg] : llvm::zip(forOp.getBody()->getArguments(),\n+                                         newForOp.getBody()->getArguments())) {\n+    if (oldArg.getType() == newArg.getType()) {\n+      oldArg.replaceAllUsesWith(newArg);\n+      continue;\n+    }\n+    map(oldArg, newArg);\n+  }\n+  return newForOp.getOperation();\n+}\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *cvt,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    if (!llvm::isa<triton::gpu::ConvertLayoutOp>(cvt))\n-      return mlir::failure();\n-    // we don't touch block arguments\n-    Operation *op = cvt->getOperand(0).getDefiningOp();\n-    if (!op)\n-      return mlir::failure();\n-    // we don't want to rematerialize any conversion to/from shared\n-    if (triton::gpu::isSharedEncoding(cvt->getResults()[0]) ||\n-        triton::gpu::isSharedEncoding(cvt->getOperand(0)))\n-      return mlir::failure();\n-    // we don't handle conversions to DotOperandEncodingAttr\n-    // this is a heuristics to accommodate fused attention\n-    auto targetType = cvt->getResultTypes()[0].cast<RankedTensorType>();\n-    if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n-      return mlir::failure();\n-    // DFS\n-    SetVector<Operation *> processed;\n-    SetVector<Attribute> layout;\n-    llvm::MapVector<Value, Attribute> toConvert;\n-    if (simulateBackwardRematerialization(cvt, processed, layout, toConvert,\n-                                          targetType.getEncoding()) > 0)\n-      return mlir::failure();\n+Operation *LayoutPropagation::rewriteIfOp(scf::IfOp ifOp) {\n+  SmallVector<Value> operands;\n+  OpBuilder rewriter(ifOp);\n+  SmallVector<Type> newResultTypes(ifOp->getResultTypes());\n+  for (unsigned i = 0, e = ifOp->getNumResults(); i < e; ++i) {\n+    auto it = layouts.find(ifOp->getResult(i));\n+    if (it == layouts.end())\n+      continue;\n+    auto origType = ifOp->getResult(i).getType().cast<RankedTensorType>();\n+    Attribute encoding = *(it->second.encodings.begin());\n+    newResultTypes[i] = RankedTensorType::get(\n+        origType.getShape(), origType.getElementType(), encoding);\n+  }\n+  auto newIfOp = rewriter.create<scf::IfOp>(ifOp.getLoc(), newResultTypes,\n+                                            ifOp.getCondition(), true, true);\n+  newIfOp.getThenRegion().takeBody(ifOp.getThenRegion());\n+  newIfOp.getElseRegion().takeBody(ifOp.getElseRegion());\n+  for (auto [oldResult, newResult] :\n+       llvm::zip(ifOp.getResults(), newIfOp.getResults())) {\n+    if (oldResult.getType() == newResult.getType()) {\n+      oldResult.replaceAllUsesWith(newResult);\n+      continue;\n+    }\n+    map(oldResult, newResult);\n+  }\n+  return newIfOp.getOperation();\n+}\n \n-    IRMapping mapping;\n-    rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n-    rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n+Operation *LayoutPropagation::rewriteYieldOp(scf::YieldOp yieldOp) {\n+  OpBuilder rewriter(yieldOp);\n+  Operation *newYield = rewriter.clone(*yieldOp.getOperation());\n+  Operation *parentOp = yieldOp->getParentOp();\n+  for (OpOperand &operand : yieldOp->getOpOperands()) {\n+    Type yieldType = operand.get().getType();\n+    if (isa<scf::ForOp, scf::IfOp>(parentOp))\n+      yieldType = parentOp->getResult(operand.getOperandNumber()).getType();\n+    auto tensorType = yieldType.dyn_cast<RankedTensorType>();\n+    if (!tensorType)\n+      continue;\n+    Value newOperand = getValueAs(operand.get(), tensorType.getEncoding());\n+    newYield->setOperand(operand.getOperandNumber(), newOperand);\n+  }\n+  opToDelete.push_back(yieldOp.getOperation());\n+  return newYield;\n+}\n \n-    return mlir::success();\n+Operation *LayoutPropagation::rewriteOp(Operation *op) {\n+  opToDelete.push_back(op);\n+  if (auto forOp = dyn_cast<scf::ForOp>(op))\n+    return rewriteForOp(forOp);\n+  if (auto ifOp = dyn_cast<scf::IfOp>(op))\n+    return rewriteIfOp(ifOp);\n+  OpBuilder rewriter(op);\n+  Attribute encoding = *layouts[op->getResult(0)].encodings.begin();\n+  if (auto convertOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+    Attribute srcEncoding =\n+        convertOp.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+    auto it = layouts.find(convertOp.getOperand());\n+    if (it != layouts.end())\n+      srcEncoding = *(it->second.encodings.begin());\n+    Value src = getValueAs(convertOp.getOperand(), srcEncoding);\n+    auto tensorType = op->getResult(0).getType().cast<RankedTensorType>();\n+    auto newType = RankedTensorType::get(tensorType.getShape(),\n+                                         tensorType.getElementType(), encoding);\n+    auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(op->getLoc(),\n+                                                             newType, src);\n+    map(op->getResult(0), cvt.getResult());\n+    return cvt.getOperation();\n   }\n-};\n+  if (canFoldIntoConversion(op, encoding)) {\n+    Operation *newOp = rewriter.clone(*op);\n+    auto tensorType = op->getResult(0).getType().cast<RankedTensorType>();\n+    auto newType = RankedTensorType::get(tensorType.getShape(),\n+                                         tensorType.getElementType(), encoding);\n+    auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), newType, newOp->getResult(0));\n+    map(op->getResult(0), cvt.getResult());\n+    return cvt.getOperation();\n+  }\n+  if (op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() ||\n+      op->hasTrait<mlir::OpTrait::Elementwise>() ||\n+      isa<triton::ReduceOp, triton::ExpandDimsOp, triton::gpu::ConvertLayoutOp>(\n+          op)) {\n+    Operation *newOp = cloneElementwise(rewriter, op, encoding);\n+    for (auto [oldResult, newResult] :\n+         llvm::zip(op->getResults(), newOp->getResults()))\n+      map(oldResult, newResult);\n+    return newOp;\n+  }\n+  assert(0 && \"unexpected op in rewrite\");\n+  return nullptr;\n+}\n \n-// -----------------------------------------------------------------------------\n-//\n-// -----------------------------------------------------------------------------\n+static bool canBeRemat(Operation *op) {\n+  if (isa<triton::LoadOp, triton::StoreOp>(op))\n+    return !isExpensiveLoadOrStore(op);\n+  if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n+          triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n+          triton::AtomicCASOp, triton::DotOp>(op))\n+    return false;\n+  if (isa<scf::IfOp, scf::WhileOp, scf::ConditionOp>(op))\n+    return false;\n+\n+  return true;\n+}\n \n-class MoveConvertOutOfLoop : public mlir::RewritePattern {\n-public:\n-  explicit MoveConvertOutOfLoop(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::ForOp::getOperationName(), 1, context) {}\n-\n-  SmallVector<Value, 4>\n-  rematerializeForLoop(mlir::PatternRewriter &rewriter, scf::ForOp &forOp,\n-                       size_t i, RankedTensorType newType,\n-                       triton::gpu::ConvertLayoutOp origConversion) const {\n-    // Rewrite init argument\n-    auto origType = forOp.getInitArgs()[i].getType().cast<RankedTensorType>();\n-    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n-    newInitArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newInitArgs[i].getLoc(), newType, newInitArgs[i]);\n-    // Clone for loop\n-    auto newForOp = rewriter.create<scf::ForOp>(\n-        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n-        forOp.getStep(), newInitArgs);\n-    newForOp->moveBefore(forOp);\n-    rewriter.setInsertionPointToStart(newForOp.getBody());\n-    IRMapping mapping;\n-    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n-      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n-    mapping.map(origConversion.getResult(), newForOp.getRegionIterArgs()[i]);\n-\n-    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n-    for (Operation &op : forOp.getBody()->without_terminator()) {\n-      if (dyn_cast<triton::gpu::ConvertLayoutOp>(op) == origConversion)\n-        continue;\n+// Replace ForOp with a new ForOp with extra operands. The YieldOp is not\n+// updated and needs to be updated separatly for the loop to be correct.\n+static scf::ForOp replaceForOpWithNewSignature(OpBuilder &rewriter,\n+                                               scf::ForOp loop,\n+                                               ValueRange newIterOperands) {\n+  OpBuilder::InsertionGuard g(rewriter);\n+  rewriter.setInsertionPoint(loop);\n+\n+  // Create a new loop before the existing one, with the extra operands.\n+  rewriter.setInsertionPoint(loop);\n+  auto operands = llvm::to_vector<4>(loop.getIterOperands());\n+  operands.append(newIterOperands.begin(), newIterOperands.end());\n+  scf::ForOp newLoop = rewriter.create<scf::ForOp>(\n+      loop.getLoc(), loop.getLowerBound(), loop.getUpperBound(), loop.getStep(),\n+      operands);\n+  newLoop.getBody()->erase();\n+\n+  newLoop.getLoopBody().getBlocks().splice(\n+      newLoop.getLoopBody().getBlocks().begin(),\n+      loop.getLoopBody().getBlocks());\n+  for (Value operand : newIterOperands)\n+    newLoop.getBody()->addArgument(operand.getType(), operand.getLoc());\n+\n+  for (auto it : llvm::zip(loop.getResults(), newLoop.getResults().take_front(\n+                                                  loop.getNumResults())))\n+    std::get<0>(it).replaceAllUsesWith(std::get<1>(it));\n+  return newLoop;\n+}\n \n-      bool convert = llvm::any_of(op.getOperands(), [&](auto operand) {\n-        return operand == origConversion.getOperand();\n-      });\n-      auto convertLayout = [&](Value operand, Value value, Attribute encoding) {\n-        auto tensorType = value.getType().cast<RankedTensorType>();\n-        auto cvtType = RankedTensorType::get(\n-            tensorType.getShape(), tensorType.getElementType(), encoding);\n-        auto cvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-            op.getLoc(), cvtType, value);\n-        mapping.map(operand, cvt);\n-      };\n-      DenseMap<Value, Value> cvtValues;\n-      if (convert) {\n-        for (auto operand : op.getOperands()) {\n-          if (operand == origConversion.getOperand() ||\n-              !isa<RankedTensorType>(operand.getType()))\n-            continue;\n-          auto value = mapping.lookupOrDefault(operand);\n-          // Convert to the new type\n-          convertLayout(operand, value, newType.getEncoding());\n-          // Other ops don't use the converted value and we need to restore\n-          cvtValues[operand] = value;\n+static void rewriteSlice(SetVector<Value> &slice,\n+                         DenseMap<Value, Attribute> &layout,\n+                         ConvertLayoutOp convertOp, IRMapping &mapping) {\n+  SetVector<Operation *> opsToRewrite;\n+  for (Value v : slice) {\n+    if (v.getDefiningOp()) {\n+      opsToRewrite.insert(v.getDefiningOp());\n+    } else {\n+      opsToRewrite.insert(v.cast<BlockArgument>().getOwner()->getParentOp());\n+      // We also need to rewrite the yield op.\n+      opsToRewrite.insert(v.cast<BlockArgument>().getOwner()->getTerminator());\n+    }\n+  }\n+  opsToRewrite = multiRootTopologicalSort(opsToRewrite);\n+\n+  SmallVector<Operation *> deadLoops;\n+  OpBuilder builder(slice.begin()->getContext());\n+  for (Operation *op : opsToRewrite) {\n+    if (auto forOp = dyn_cast<scf::ForOp>(op)) {\n+      // Keep a mapping of the operands index to the new operands index.\n+      SmallVector<std::pair<size_t, size_t>> argMapping;\n+      SmallVector<Value> newOperands;\n+      for (auto arg : forOp.getRegionIterArgs()) {\n+        if (slice.count(arg)) {\n+          OpOperand &initVal = forOp.getOpOperandForRegionIterArg(arg);\n+          argMapping.push_back(\n+              std::make_pair(*forOp.getIterArgNumberForOpOperand(initVal),\n+                             forOp.getNumIterOperands() + newOperands.size()));\n+          newOperands.push_back(mapping.lookup(initVal.get()));\n         }\n       }\n-      auto *newOp = cloneWithInferType(rewriter, &op, mapping);\n-      if (convert) {\n-        for (auto result : op.getResults()) {\n-          if (!isa<RankedTensorType>(result.getType()))\n-            continue;\n-          auto value = mapping.lookupOrDefault(result);\n-          auto tensorType = result.getType().cast<RankedTensorType>();\n-          // Convert to the original type\n-          convertLayout(result, value, tensorType.getEncoding());\n-        }\n-        // Restore original values\n-        for (auto [operand, value] : cvtValues)\n-          mapping.map(operand, value);\n+      // Create a new for loop with the new operands.\n+      scf::ForOp newForOp =\n+          replaceForOpWithNewSignature(builder, forOp, newOperands);\n+      deadLoops.push_back(forOp.getOperation());\n+      Block &loopBody = *newForOp.getBody();\n+      for (auto m : argMapping) {\n+        mapping.map(newForOp.getResult(m.first), newForOp.getResult(m.second));\n+        int numIndVars = newForOp.getNumInductionVars();\n+        mapping.map(loopBody.getArgument(m.first + numIndVars),\n+                    loopBody.getArgument(m.second + numIndVars));\n+      }\n+      continue;\n+    }\n+    builder.setInsertionPoint(op);\n+    if (auto yieldOp = dyn_cast<scf::YieldOp>(op)) {\n+      auto yieldOperands = llvm::to_vector(yieldOp.getOperands());\n+      for (Value operand : yieldOp.getOperands()) {\n+        if (slice.count(operand) == 0)\n+          continue;\n+        yieldOperands.push_back(mapping.lookup(operand));\n       }\n+      builder.create<scf::YieldOp>(op->getLoc(), yieldOperands);\n+      op->erase();\n+      continue;\n+    }\n+    if (isa<arith::ConstantOp>(op)) {\n+      Operation *newOp = builder.clone(*op);\n+      auto tensorType = op->getResult(0).getType().cast<RankedTensorType>();\n+      auto newType = RankedTensorType::get(tensorType.getShape(),\n+                                           tensorType.getElementType(),\n+                                           layout[op->getResult(0)]);\n+      auto cvt = builder.create<triton::gpu::ConvertLayoutOp>(\n+          op->getLoc(), newType, newOp->getResult(0));\n+      mapping.map(op->getResult(0), cvt.getResult());\n+      continue;\n+    }\n+    Operation *newOp = builder.clone(*op, mapping);\n+    for (auto [old, newV] : llvm::zip(op->getResults(), newOp->getResults())) {\n+      auto it = layout.find(old);\n+      if (it == layout.end())\n+        continue;\n+      auto newType = RankedTensorType::get(\n+          old.getType().cast<RankedTensorType>().getShape(),\n+          old.getType().cast<RankedTensorType>().getElementType(), it->second);\n+      newV.setType(newType);\n     }\n-    // create yield, inserting conversions if necessary\n-    auto yieldOp = forOp.getBody()->getTerminator();\n-    SmallVector<Value, 4> newYieldArgs;\n-    // We use the new type for the result of the conversion\n-    for (Value arg : yieldOp->getOperands())\n-      newYieldArgs.push_back(mapping.lookup(arg));\n-    if (newYieldArgs[i].getType() != newType)\n-      newYieldArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          yieldOp->getLoc(), newType, newYieldArgs[i]);\n-    rewriter.create<scf::YieldOp>(forOp.getLoc(), newYieldArgs);\n-\n-    // replace\n-    SmallVector<Value, 4> newResults = newForOp->getResults();\n-    newResults[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newForOp.getLoc(), origType, newForOp->getResult(i));\n-    newResults[i].getDefiningOp()->moveAfter(newForOp);\n-\n-    return newResults;\n   }\n+  convertOp.replaceAllUsesWith(mapping.lookup(convertOp.getOperand()));\n+  convertOp.erase();\n+  for (Operation *op : deadLoops)\n+    op->erase();\n+}\n \n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto forOp = cast<scf::ForOp>(op);\n-    auto iterArgs = forOp.getRegionIterArgs();\n-    for (const auto &iterArg : llvm::enumerate(iterArgs)) {\n-      // skip non-tensor types\n-      if (!iterArg.value().getType().isa<RankedTensorType>())\n-        continue;\n-      SmallVector<Operation *> cvts;\n-      if (canMoveOutOfLoop(iterArg.value(), cvts).failed())\n-        continue;\n-      // check\n-      for (auto *op : cvts) {\n-        auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(op);\n-        auto targetType = op->getResultTypes()[0].cast<RankedTensorType>();\n-        auto newFor = rematerializeForLoop(rewriter, forOp, iterArg.index(),\n-                                           targetType, cvt);\n-        rewriter.replaceOp(forOp, newFor);\n-        return success();\n-      }\n+static void rewriteSlice(SetVector<Value> &slice,\n+                         DenseMap<Value, Attribute> &layout,\n+                         ConvertLayoutOp convertOp) {\n+  IRMapping mapping;\n+  rewriteSlice(slice, layout, convertOp, mapping);\n+}\n+\n+static void backwardRematerialization(ConvertLayoutOp convertOp) {\n+  // we don't want to rematerialize any conversion to/from shared\n+  if (triton::gpu::isSharedEncoding(convertOp.getResult()) ||\n+      triton::gpu::isSharedEncoding(convertOp.getOperand()))\n+    return;\n+  // we don't handle conversions to DotOperandEncodingAttr\n+  // this is a heuristics to accommodate fused attention\n+  auto targetType = convertOp->getResultTypes()[0].cast<RankedTensorType>();\n+  if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n+    return;\n+\n+  // 1. Take a backward slice of all the tensor dependencies.\n+  SetVector<Value> slice;\n+  DenseMap<Value, Attribute> layout;\n+  LogicalResult result = getConvertBackwardSlice(\n+      convertOp.getOperand(), slice, targetType.getEncoding(), layout);\n+  if (result.failed() || slice.empty())\n+    return;\n+\n+  // 2. Check if all the operations in the slice can be rematerialized.\n+  for (Value v : slice) {\n+    if (Operation *op = v.getDefiningOp()) {\n+      if (!canBeRemat(op))\n+        return;\n     }\n-    return failure();\n   }\n-};\n+  // 3. Rewrite the slice.\n+  rewriteSlice(slice, layout, convertOp);\n+}\n \n-//\n-class ConvertDotConvert : public mlir::RewritePattern {\n-public:\n-  ConvertDotConvert(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n+// For convert left we try to hoist them above type extension to reduce the cost\n+// of the convert.\n+static void hoistConvertOnTopOfExt(ConvertLayoutOp convertOp) {\n+  // we don't want to rematerialize any conversion to/from shared\n+  if (triton::gpu::isSharedEncoding(convertOp.getResult()) ||\n+      triton::gpu::isSharedEncoding(convertOp.getOperand()))\n+    return;\n+  // we don't handle conversions to DotOperandEncodingAttr\n+  // this is a heuristics to accommodate fused attention\n+  auto targetType = convertOp->getResultTypes()[0].cast<RankedTensorType>();\n+  if (targetType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n+    return;\n \n-  LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto dstOp = cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto dotOp =\n-        dyn_cast_or_null<triton::DotOp>(dstOp.getSrc().getDefiningOp());\n-    if (!dotOp)\n-      return mlir::failure();\n-    if (std::distance(dstOp->user_begin(), dstOp->user_end()) != 1 ||\n-        std::distance(dotOp->user_begin(), dotOp->user_end()) != 1)\n-      return mlir::failure();\n-    auto cvtOp = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(\n-        dotOp.getOperand(2).getDefiningOp());\n-    if (!cvtOp)\n-      return mlir::failure();\n-    auto loadOp =\n-        dyn_cast_or_null<triton::LoadOp>(cvtOp.getSrc().getDefiningOp());\n-    if (!loadOp)\n-      return mlir::failure();\n-    auto dstTy = dstOp.getResult().getType().cast<RankedTensorType>();\n-    auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n-    if (dstTy != srcTy)\n-      return mlir::failure();\n+  // 1. Take a backward slice of all the tensor dependencies.\n+  SetVector<Value> slice;\n+  DenseMap<Value, Attribute> layout;\n+  auto isExtOp = [](Operation *op) {\n+    return isa<arith::ExtSIOp, arith::ExtUIOp, arith::ExtFOp>(op);\n+  };\n+  // Get a backward slice but don't go past ext ops\n+  LogicalResult result = getConvertBackwardSlice(\n+      convertOp.getOperand(), slice, targetType.getEncoding(), layout, isExtOp);\n+  if (result.failed() || slice.empty())\n+    return;\n+  Operation *extOp = nullptr;\n+  // 2. Check if all the operations in the slice can be rematerialized.\n+  for (Value v : slice) {\n+    if (Operation *op = v.getDefiningOp()) {\n+      if (!canBeRemat(op))\n+        return;\n+      if (isExtOp(op)) {\n+        // Only apply it if there is a single ext op otherwise we would have to\n+        // duplicate the convert.\n+        if (extOp != nullptr)\n+          return;\n+        extOp = op;\n+      }\n+    }\n+  }\n+  if (extOp == nullptr)\n+    return;\n+  // Move the convert before the ext op and rewrite the slice.\n+  OpBuilder builder(extOp);\n+  auto tensorType = extOp->getOperand(0).getType().cast<RankedTensorType>();\n+  auto newType =\n+      RankedTensorType::get(tensorType.getShape(), tensorType.getElementType(),\n+                            layout[extOp->getResult(0)]);\n+  auto newConvertOp = builder.create<ConvertLayoutOp>(\n+      convertOp.getLoc(), newType, extOp->getOperand(0));\n+  IRMapping mapping;\n+  mapping.map(extOp->getOperand(0), newConvertOp.getResult());\n+  // 3. Rewrite the slice.\n+  rewriteSlice(slice, layout, convertOp, mapping);\n+}\n \n-    // TODO: int tensor cores\n-    auto out_dtype = dstTy.getElementType().cast<FloatType>();\n-    APFloat value(0.0f);\n-    if (out_dtype.isBF16())\n-      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n-    else if (out_dtype.isF16())\n-      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n-    else if (out_dtype.isF32())\n-      value = APFloat(0.0f);\n-    else\n-      llvm_unreachable(\"unsupported data type\");\n-\n-    auto _0f =\n-        rewriter.create<arith::ConstantFloatOp>(op->getLoc(), value, out_dtype);\n-    auto _0 = rewriter.create<triton::SplatOp>(\n-        op->getLoc(), dotOp.getResult().getType(), _0f);\n-    auto newDot = rewriter.create<triton::DotOp>(\n-        op->getLoc(), dotOp.getResult().getType(), dotOp.getOperand(0),\n-        dotOp.getOperand(1), _0, dotOp.getAllowTF32());\n-    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        op->getLoc(), dstTy, newDot.getResult());\n-    rewriter.replaceOpWithNewOp<arith::AddFOp>(op, newCvt, cvtOp.getOperand());\n-    return mlir::success();\n+static void backwardRematerialization(ModuleOp module) {\n+  SmallVector<ConvertLayoutOp> convertOps;\n+  module.walk(\n+      [&](ConvertLayoutOp convertOp) { convertOps.push_back(convertOp); });\n+  for (ConvertLayoutOp convertOp : convertOps) {\n+    backwardRematerialization(convertOp);\n   }\n-};\n+}\n \n-} // namespace\n+static void hoistConvert(ModuleOp module) {\n+  SmallVector<ConvertLayoutOp> convertOps;\n+  module.walk(\n+      [&](ConvertLayoutOp convertOp) { convertOps.push_back(convertOp); });\n+  for (ConvertLayoutOp convertOp : convertOps) {\n+    hoistConvertOnTopOfExt(convertOp);\n+  }\n+}\n \n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n@@ -646,22 +864,45 @@ class TritonGPURemoveLayoutConversionsPass\n     MLIRContext *context = &getContext();\n     ModuleOp m = getOperation();\n \n-    mlir::RewritePatternSet patterns(context);\n-\n-    patterns.add<SimplifyConversion>(context);\n-    patterns.add<SimplifyReduceCvt>(context);\n-    patterns.add<RematerializeBackward>(context);\n-    patterns.add<RematerializeForward>(context);\n-    patterns.add<MoveConvertOutOfLoop>(context);\n-    patterns.add<MoveConvertOutOfIf>(context);\n-    patterns.add<DecomposeDotOperand>(context);\n-    patterns.add<ConvertDotConvert>(context);\n+    // 1. Propagate layout forward starting from \"anchor\" ops.\n+    m.walk([](triton::FuncOp funcOp) {\n+      LayoutPropagation layoutPropagation(funcOp);\n+      layoutPropagation.initAnchorLayout();\n+      layoutPropagation.propagateLayout();\n+      layoutPropagation.resolveConflicts();\n+      layoutPropagation.rewrite();\n+    });\n+\n+    mlir::RewritePatternSet cleanUpPatterns(context);\n+    ConvertLayoutOp::getCanonicalizationPatterns(cleanUpPatterns, context);\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(cleanUpPatterns))\n+            .failed()) {\n+      signalPassFailure();\n+    }\n \n-    if (mlir::applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n+    // 2. For convert ops left try to rematerialize the slice of producer\n+    // operation to avoid having to convert.\n+    backwardRematerialization(m);\n+    // 3. For converts left try to hoist them above cast generating larger size\n+    // types in order to reduce the cost of the convert op.\n+    hoistConvert(m);\n+\n+    mlir::RewritePatternSet decomposePatterns(context);\n+    decomposePatterns.add<DecomposeDotOperand>(context);\n+    decomposePatterns.add<ConvertDotConvert>(context);\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(decomposePatterns))\n+            .failed()) {\n       signalPassFailure();\n     }\n \n-    if (fixupLoops(m).failed()) {\n+    // 4. Apply clean up patterns to remove remove dead convert and dead code\n+    // generated by the previous transformations.\n+    mlir::RewritePatternSet cleanUpPatterns2(context);\n+    populateForOpDeadArgumentElimination(cleanUpPatterns2);\n+    scf::ForOp::getCanonicalizationPatterns(cleanUpPatterns2, context);\n+    ConvertLayoutOp::getCanonicalizationPatterns(cleanUpPatterns2, context);\n+    if (mlir::applyPatternsAndFoldGreedily(m, std::move(cleanUpPatterns2))\n+            .failed()) {\n       signalPassFailure();\n     }\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 217, "deletions": 343, "changes": 560, "file_content_changes": "@@ -9,61 +9,6 @@\n \n namespace mlir {\n \n-namespace {\n-\n-class FixupLoop : public mlir::RewritePattern {\n-\n-public:\n-  explicit FixupLoop(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto forOp = cast<scf::ForOp>(op);\n-\n-    // Rewrite init argument\n-    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n-    bool shouldRematerialize = false;\n-    for (size_t i = 0; i < newInitArgs.size(); i++) {\n-      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n-          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n-        shouldRematerialize = true;\n-        break;\n-      }\n-    }\n-    if (!shouldRematerialize)\n-      return failure();\n-\n-    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n-        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n-        forOp.getStep(), newInitArgs);\n-    newForOp->moveBefore(forOp);\n-    rewriter.setInsertionPointToStart(newForOp.getBody());\n-    IRMapping mapping;\n-    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n-      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n-    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n-\n-    for (Operation &op : forOp.getBody()->getOperations()) {\n-      rewriter.clone(op, mapping);\n-    }\n-    rewriter.replaceOp(forOp, newForOp.getResults());\n-    return success();\n-  }\n-};\n-\n-} // namespace\n-\n-LogicalResult fixupLoops(ModuleOp mod) {\n-  auto *ctx = mod.getContext();\n-  mlir::RewritePatternSet patterns(ctx);\n-  patterns.add<FixupLoop>(ctx);\n-  if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())\n-    return failure();\n-  return success();\n-}\n-\n SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n                                                 const ArrayRef<int64_t> &shape,\n                                                 RankedTensorType type) {\n@@ -81,8 +26,8 @@ SmallVector<unsigned, 3> mmaVersionToInstrShape(int version,\n     SmallVector<unsigned> validN;\n \n     // MMAv3 with larger instruction shape is preferred.\n-    if (eltType.isFloat8E5M2() || eltType.isFloat8E4M3FN() || eltType.isF16() ||\n-        eltType.isBF16() || eltType.isF32()) {\n+    if (eltType.isFloat8E5M2() || eltType.isFloat8E4M3FNUZ() ||\n+        eltType.isF16() || eltType.isBF16() || eltType.isF32()) {\n       validN.assign({256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176,\n                      168, 160, 152, 144, 136, 128, 120, 112, 104, 96,  88,\n                      80,  72,  64,  56,  48,  40,  32,  24,  16,  8});\n@@ -295,30 +240,59 @@ std::string GraphLayoutMarker::getColor(const Type &type) const {\n }\n // -------------------------------------------------------------------------- //\n \n-// TODO: Interface\n-LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n-                             Attribute &ret) {\n-  ret = targetEncoding;\n-  if (auto expand_dims = dyn_cast<triton::ExpandDimsOp>(op)) {\n-    ret = triton::gpu::SliceEncodingAttr::get(\n-        op->getContext(), expand_dims.getAxis(), targetEncoding);\n-  }\n-  if (auto reduce = dyn_cast<triton::ReduceOp>(op)) {\n-    auto sliceEncoding =\n-        targetEncoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n-    if (!sliceEncoding)\n-      return failure();\n-    if (sliceEncoding.getDim() != reduce.getAxis())\n-      return failure();\n-    ret = sliceEncoding.getParent();\n-  }\n-  if (isa<triton::ViewOp, triton::CatOp>(op)) {\n-    return failure();\n-  }\n-  return success();\n+static std::optional<Attribute> inferDstEncoding(triton::ReduceOp op,\n+                                                 Attribute encoding) {\n+  return triton::gpu::SliceEncodingAttr::get(op->getContext(), op.getAxis(),\n+                                             encoding);\n+}\n+\n+static std::optional<Attribute> inferDstEncoding(triton::ExpandDimsOp op,\n+                                                 Attribute encoding) {\n+  auto sliceEncoding = encoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n+  if (!sliceEncoding)\n+    return std::nullopt;\n+  if (op.getAxis() != sliceEncoding.getDim())\n+    return std::nullopt;\n+  return sliceEncoding.getParent();\n+}\n+\n+static std::optional<Attribute> inferSrcEncoding(triton::ReduceOp op,\n+                                                 Attribute encoding) {\n+  auto sliceEncoding = encoding.dyn_cast<triton::gpu::SliceEncodingAttr>();\n+  if (!sliceEncoding)\n+    return std::nullopt;\n+  if (op.getAxis() != sliceEncoding.getDim())\n+    return std::nullopt;\n+  return sliceEncoding.getParent();\n+}\n+\n+static std::optional<Attribute> inferSrcEncoding(triton::ExpandDimsOp op,\n+                                                 Attribute encoding) {\n+  return triton::gpu::SliceEncodingAttr::get(op->getContext(), op.getAxis(),\n+                                             encoding);\n }\n \n-bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n+std::optional<Attribute> inferSrcEncoding(Operation *op, Attribute encoding) {\n+  if (auto reduceOp = dyn_cast<triton::ReduceOp>(op))\n+    return inferSrcEncoding(reduceOp, encoding);\n+  if (auto expand = dyn_cast<triton::ExpandDimsOp>(op))\n+    return inferSrcEncoding(expand, encoding);\n+  if (isa<triton::ViewOp, triton::CatOp>(op))\n+    return std::nullopt;\n+  return encoding;\n+}\n+\n+std::optional<Attribute> inferDstEncoding(Operation *op, Attribute encoding) {\n+  if (auto reduceOp = dyn_cast<triton::ReduceOp>(op))\n+    return inferDstEncoding(reduceOp, encoding);\n+  if (auto expand = dyn_cast<triton::ExpandDimsOp>(op))\n+    return inferDstEncoding(expand, encoding);\n+  if (isa<triton::ViewOp, triton::CatOp>(op))\n+    return std::nullopt;\n+  return encoding;\n+}\n+\n+bool isExpensiveLoadOrStore(Operation *op) {\n   // Case 1: Pointer of tensor is always expensive\n   auto operandType = op->getOperand(0).getType();\n   if (triton::isTensorPointerType(operandType))\n@@ -342,7 +316,7 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n-    return isExpensiveLoadOrStore(op, targetEncoding);\n+    return isExpensiveLoadOrStore(op);\n   if (isa<triton::CatOp>(op))\n     return triton::gpu::isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n@@ -355,75 +329,21 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   return false;\n }\n \n-bool canFoldConversion(Operation *op, Attribute targetEncoding) {\n+bool canFoldIntoConversion(Operation *op, Attribute targetEncoding) {\n   if (isa<triton::CatOp>(op))\n     return !triton::gpu::isExpensiveCat(cast<triton::CatOp>(op),\n                                         targetEncoding);\n-  return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-             triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n-}\n-\n-int simulateBackwardRematerialization(\n-    Operation *initOp, SetVector<Operation *> &processed,\n-    SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n-    Attribute targetEncoding) {\n-  // DFS\n-  std::vector<std::pair<Operation *, Attribute>> queue;\n-  queue.emplace_back(initOp, targetEncoding);\n-  // We want to see the effect of converting `initOp` to a new layout\n-  // so we initialize `numCvts = 1`.\n-  int numCvts = 1;\n-  while (!queue.empty()) {\n-    Operation *currOp;\n-    Attribute currLayout;\n-    std::tie(currOp, currLayout) = queue.back();\n-    queue.pop_back();\n-    // If the current operation is expensive to rematerialize,\n-    // we stop everything\n-    if (isExpensiveToRemat(currOp, currLayout))\n-      break;\n-    // A conversion will be removed here (i.e. transferred to operands)\n-    numCvts -= 1;\n-    // Done processing\n-    processed.insert(currOp);\n-    layout.insert(currLayout);\n-    // Add all operands to the queue\n-    for (Value argI : currOp->getOperands()) {\n-      Attribute newEncoding;\n-      // Cannot invert the current encoding for this operand\n-      // we stop everything\n-      if (failed(invertEncoding(currLayout, currOp, newEncoding)))\n-        return INT_MAX;\n-      if (toConvert.count(argI) && toConvert[argI] != newEncoding)\n-        return INT_MAX;\n-      if (auto ptrTy = argI.getType().dyn_cast<triton::PointerType>()) {\n-        if (ptrTy.getPointeeType().isa<RankedTensorType>()) {\n-          return INT_MAX;\n-        }\n-      }\n-\n-      Operation *opArgI = argI.getDefiningOp();\n-      toConvert.insert({argI, newEncoding});\n-      // 1. Only convert RankedTensorType\n-      // 2. Skip if there's no defining op\n-      // 3. Skip if the defining op has already been processed\n-      // 4. Skip or the defining op is in a different block\n-      if (!argI.getType().isa<RankedTensorType>() || !opArgI ||\n-          processed.contains(opArgI) ||\n-          opArgI->getBlock() != currOp->getBlock())\n-        continue;\n-      // If the conversion can be folded into opArgI then\n-      // we don't count this conversion as expensive\n-      if (canFoldConversion(opArgI, newEncoding))\n-        continue;\n-\n-      // We add one expensive conversion for the current operand\n-      numCvts += 1;\n-      queue.emplace_back(opArgI, newEncoding);\n+  if (auto convert = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+    if (targetEncoding.isa<triton::gpu::MmaEncodingAttr>()) {\n+      auto srcEncoding =\n+          convert.getOperand().getType().cast<RankedTensorType>().getEncoding();\n+      if (targetEncoding != srcEncoding)\n+        return false;\n     }\n+    return true;\n   }\n-  // return net number of conversions\n-  return numCvts;\n+  return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n+             triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }\n \n //\n@@ -464,213 +384,54 @@ Operation *cloneWithInferType(mlir::OpBuilder &rewriter, Operation *op,\n   return newOp;\n }\n \n-namespace {\n-\n-struct OpUseInfo {\n-  Value value;\n-  Operation *op;\n-  unsigned index;\n-};\n-\n-void getForwardSliceOpUseInfo(Operation *op,\n-                              SetVector<Operation *> *forwardSliceOps,\n-                              SmallVector<OpUseInfo> *forwardOpUseInfo) {\n-  if (!op)\n-    return;\n-\n-  for (Region &region : op->getRegions())\n-    for (Block &block : region)\n-      for (Operation &blockOp : block)\n-        if (forwardSliceOps->count(&blockOp) == 0)\n-          getForwardSliceOpUseInfo(&blockOp, forwardSliceOps, forwardOpUseInfo);\n-  for (Value result : op->getResults()) {\n-    for (OpOperand &operand : result.getUses()) {\n-      auto *blockOp = operand.getOwner();\n-      forwardOpUseInfo->push_back(\n-          {operand.get(), blockOp, operand.getOperandNumber()});\n-      if (forwardSliceOps->count(blockOp) == 0)\n-        getForwardSliceOpUseInfo(blockOp, forwardSliceOps, forwardOpUseInfo);\n-    }\n-  }\n-\n-  forwardSliceOps->insert(op);\n-}\n-} // namespace\n-\n-LogicalResult simulateForwardRematerializationInLoop(Operation *startOp,\n-                                                     BlockArgument arg,\n-                                                     Attribute targetEncoding) {\n-  // heuristics for flash attention\n-  if (targetEncoding.isa<triton::gpu::SharedEncodingAttr>())\n-    return failure();\n-  SetVector<Operation *> cvtSliceOps;\n-  SmallVector<OpUseInfo> cvtSliceOpUseInfo;\n-  getForwardSliceOpUseInfo(startOp, &cvtSliceOps, &cvtSliceOpUseInfo);\n-\n-  // Check if any additional conversion is needed along the way\n-  for (Operation *op : cvtSliceOps) {\n-    if (isa<scf::YieldOp>(op))\n+LogicalResult\n+getConvertBackwardSlice(Value root, SetVector<Value> &slice,\n+                        Attribute rootEncoding,\n+                        DenseMap<Value, Attribute> &layout,\n+                        std::function<bool(Operation *)> stopPropagation) {\n+  SmallVector<std::pair<Value, Attribute>> queue = {{root, rootEncoding}};\n+  while (!queue.empty()) {\n+    auto [currentValue, encoding] = queue.back();\n+    queue.pop_back();\n+    if (!currentValue.getType().isa<RankedTensorType>())\n       continue;\n-    // The first op doesn't push forward any conversion\n-    if (op != startOp) {\n-      if (isa<triton::ReduceOp>(op) &&\n-          !op->getResult(0).getType().isa<RankedTensorType>())\n-        return failure();\n-      // don't rematerialize anything expensive\n-      if (isExpensiveToRemat(op, targetEncoding))\n-        return failure();\n-      // don't rematerialize non-element-wise\n-      if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n-          !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp, triton::AssertOp, triton::PrintOp,\n-               triton::ReduceOp>(op))\n-        return failure();\n-    }\n-    // don't rematerialize if it adds an extra conversion that can't\n-    // be removed\n-    for (Value value : op->getOperands()) {\n-      Operation *argOp = arg.getDefiningOp();\n-      SetVector<Operation *> processed;\n-      SetVector<Attribute> layout;\n-      llvm::MapVector<Value, Attribute> toConvert;\n-      int numAddedConvs = simulateBackwardRematerialization(\n-          argOp, processed, layout, toConvert, targetEncoding);\n-      if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n-          cvtSliceOps.count(argOp) == 0 && numAddedConvs > 0)\n+    // Skip propagating through for op results for now.\n+    // TODO: enable this based on needs.\n+    if (currentValue.getDefiningOp<scf::ForOp>())\n+      return failure();\n+    slice.insert(currentValue);\n+    layout[currentValue] = encoding;\n+    if (auto *definingOp = currentValue.getDefiningOp()) {\n+      if (canFoldIntoConversion(definingOp, encoding))\n+        continue;\n+      if (stopPropagation && stopPropagation(definingOp))\n+        continue;\n+      if (isa<triton::CatOp>(definingOp))\n         return failure();\n-    }\n-  }\n-\n-  // We apply conservative analysis. Only when the final operand's index\n-  // matches the argument's index or their encoding match, we can rematerialize.\n-  for (auto &opUseInfo : cvtSliceOpUseInfo) {\n-    Operation *op = opUseInfo.op;\n-    if (isa<scf::YieldOp>(op)) {\n-      auto yieldIdx = opUseInfo.index;\n-      // 0 is the induction variable\n-      auto argIdx = arg.getArgNumber() - 1;\n-      if (yieldIdx != argIdx) {\n-        auto argType = arg.getType().cast<RankedTensorType>();\n-        auto yieldType =\n-            op->getOperand(yieldIdx).getType().dyn_cast<RankedTensorType>();\n-        if (!yieldType || argType.getEncoding() != yieldType.getEncoding())\n+      for (Value operand : definingOp->getOperands()) {\n+        auto srcEncoding = inferSrcEncoding(definingOp, encoding);\n+        if (!srcEncoding)\n           return failure();\n+        if (slice.count(operand) == 0)\n+          queue.push_back({operand, *srcEncoding});\n       }\n+      continue;\n     }\n-  }\n-  return success();\n-}\n-\n-void rematerializeConversionChain(\n-    const llvm::MapVector<Value, Attribute> &toConvert,\n-    mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n-    IRMapping &mapping) {\n-  SmallVector<Value, 4> sortedValues;\n-  SetVector<Operation *> tmp;\n-  for (auto &item : toConvert) {\n-    Value v = item.first;\n-    if (v.getDefiningOp())\n-      tmp.insert(v.getDefiningOp());\n-    else\n-      sortedValues.push_back(v);\n-  }\n-  tmp = mlir::multiRootTopologicalSort(tmp);\n-  for (Operation *op : tmp)\n-    sortedValues.push_back(op->getResult(0));\n-\n-  for (Value currOperand : sortedValues) {\n-    Value origOperand = currOperand;\n-    // unpack information\n-    Attribute targetLayout = toConvert.lookup(currOperand);\n-    // rematerialize the operand if necessary\n-    Operation *currOperation = currOperand.getDefiningOp();\n-    if (processed.contains(currOperation)) {\n-      Operation *newOperation =\n-          cloneWithInferType(rewriter, currOperation, mapping);\n-      newOperation->moveAfter(currOperation);\n-      currOperation = newOperation;\n-      currOperand = currOperation->getResult(0);\n-    }\n-    // compute target type for the layout cast\n-    auto currType = currOperand.getType().cast<RankedTensorType>();\n-    auto newType = RankedTensorType::get(\n-        currType.getShape(), currType.getElementType(), targetLayout);\n-    auto newOperand = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        currOperand.getLoc(), newType, currOperand);\n-    if (currOperation)\n-      newOperand->moveAfter(currOperation);\n-    else {\n-      Block *block = currOperand.cast<BlockArgument>().getOwner();\n-      newOperand->moveBefore(block, block->begin());\n+    auto blockArg = cast<BlockArgument>(currentValue);\n+    Block *block = blockArg.getOwner();\n+    Operation *parentOp = block->getParentOp();\n+    if (auto forOp = dyn_cast<scf::ForOp>(parentOp)) {\n+      OpOperand &initOperand = forOp.getOpOperandForRegionIterArg(blockArg);\n+      Value yieldOperand = forOp.getBody()->getTerminator()->getOperand(\n+          blockArg.getArgNumber() - forOp.getNumInductionVars());\n+      queue.push_back({initOperand.get(), encoding});\n+      queue.push_back({yieldOperand, encoding});\n+      continue;\n     }\n-    mapping.map(origOperand, newOperand);\n-  }\n-}\n-\n-LogicalResult canMoveOutOfLoop(BlockArgument arg,\n-                               SmallVector<Operation *> &cvts) {\n-  auto parentOp = arg.getOwner()->getParentOp();\n-  // Don't move if arg is defined in a while loop\n-  if (isa<scf::WhileOp>(parentOp))\n+    // TODO: add support for WhileOp and other region types.\n     return failure();\n-  // Skip if arg is not defined in scf.for\n-  if (!isa<scf::ForOp>(parentOp))\n-    return success();\n-  auto forOp = cast<scf::ForOp>(parentOp);\n-  // We only move `iterArg` out of the loop if\n-  // 1. There is no conversion\n-  // 2. There is only a single conversion\n-  // 3. Moving this conversion out of the loop will not generate any extra\n-  // non-removable conversion\n-  SetVector<RankedTensorType> cvtTypes;\n-  SetVector<Operation *> others;\n-  auto oldType = arg.getType().cast<RankedTensorType>();\n-  for (auto user : arg.getUsers()) {\n-    if (isa<triton::gpu::ConvertLayoutOp>(user)) {\n-      // Don't move if the conversion target is a dot operand or shared memory\n-      auto newType = user->getResults()[0].getType().cast<RankedTensorType>();\n-      if (oldType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n-          newType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n-        continue;\n-      }\n-      if (newType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n-        if (newType.getEncoding()\n-                .cast<triton::gpu::SharedEncodingAttr>()\n-                .getVec() == 1)\n-          continue;\n-      }\n-      cvts.emplace_back(user);\n-      cvtTypes.insert(newType);\n-    } else\n-      others.insert(user);\n   }\n-  // First condition\n-  if (cvts.empty())\n-    return success();\n-  if (cvtTypes.size() == 1) {\n-    // Third condition - part 1:\n-    // If the other or the cvt is in the different block, we cannot push the\n-    // conversion forward or backward\n-    for (auto *cvt : cvts) {\n-      if (cvt->getBlock() != forOp.getBody())\n-        return failure();\n-    }\n-    auto targetEncoding = cvtTypes.front().getEncoding();\n-    for (auto *other : others) {\n-      // Third condition - part 2:\n-      // If the other non-cvt op is in the different block, we cannot push the\n-      // conversion forward or backward\n-      if (other->getBlock() != forOp.getBody())\n-        return failure();\n-      // Third condition - part 3:\n-      // Check if we can directly use arg without conversion\n-      if (simulateForwardRematerializationInLoop(other, arg, targetEncoding)\n-              .failed())\n-        return failure();\n-    }\n-    return success();\n-  }\n-  return failure();\n+  return success();\n }\n \n // TODO(thomas): this is duplicated with what is in GPUToLLVM\n@@ -755,4 +516,117 @@ void setRoleId(Operation *op, int roleId) {\n   op->setAttr(\"agent.mutex_role\", attr);\n }\n \n+namespace {\n+\n+/// Detect dead arguments in scf.for op by assuming all the values are dead and\n+/// propagate liveness property.\n+struct ForOpDeadArgElimination : public OpRewritePattern<scf::ForOp> {\n+  using OpRewritePattern<scf::ForOp>::OpRewritePattern;\n+\n+  LogicalResult matchAndRewrite(scf::ForOp forOp,\n+                                PatternRewriter &rewriter) const final {\n+    Block &block = *forOp.getBody();\n+    auto yieldOp = cast<scf::YieldOp>(block.getTerminator());\n+    // Assume that nothing is live at the beginning and mark values as live\n+    // based on uses.\n+    DenseSet<Value> aliveValues;\n+    SmallVector<Value> queue;\n+    // Helper to mark values as live and add them to the queue of value to\n+    // propagate if it is the first time we detect the value as live.\n+    auto markLive = [&](Value val) {\n+      if (!forOp->isAncestor(val.getParentRegion()->getParentOp()))\n+        return;\n+      if (aliveValues.insert(val).second)\n+        queue.push_back(val);\n+    };\n+    // Mark all yield operands as live if the associated forOp result has any\n+    // use.\n+    for (auto result : llvm::enumerate(forOp.getResults())) {\n+      if (!result.value().use_empty())\n+        markLive(yieldOp.getOperand(result.index()));\n+    }\n+    if (aliveValues.size() == forOp.getNumResults())\n+      return failure();\n+    // Operations with side-effects are always live. Mark all theirs operands as\n+    // live.\n+    block.walk([&](Operation *op) {\n+      if (!isa<scf::YieldOp, scf::ForOp>(op) && !wouldOpBeTriviallyDead(op)) {\n+        for (Value operand : op->getOperands())\n+          markLive(operand);\n+      }\n+    });\n+    // Propagate live property until reaching a fixed point.\n+    while (!queue.empty()) {\n+      Value value = queue.pop_back_val();\n+      if (auto nestedFor = value.getDefiningOp<scf::ForOp>()) {\n+        auto result = value.cast<OpResult>();\n+        OpOperand &forOperand = nestedFor.getOpOperandForResult(result);\n+        markLive(forOperand.get());\n+        auto nestedYieldOp =\n+            cast<scf::YieldOp>(nestedFor.getBody()->getTerminator());\n+        Value nestedYieldOperand =\n+            nestedYieldOp.getOperand(result.getResultNumber());\n+        markLive(nestedYieldOperand);\n+        continue;\n+      }\n+      if (auto nestedIf = value.getDefiningOp<scf::IfOp>()) {\n+        auto result = value.cast<OpResult>();\n+        for (scf::YieldOp nestedYieldOp :\n+             {nestedIf.thenYield(), nestedIf.elseYield()}) {\n+          Value nestedYieldOperand =\n+              nestedYieldOp.getOperand(result.getResultNumber());\n+          markLive(nestedYieldOperand);\n+        }\n+        continue;\n+      }\n+      if (Operation *def = value.getDefiningOp()) {\n+        // TODO: support while ops.\n+        if (isa<scf::WhileOp>(def))\n+          return failure();\n+        for (Value operand : def->getOperands())\n+          markLive(operand);\n+        continue;\n+      }\n+      // If an argument block is live then the associated yield operand and\n+      // forOp operand are live.\n+      auto arg = value.cast<BlockArgument>();\n+      if (auto forOwner = dyn_cast<scf::ForOp>(arg.getOwner()->getParentOp())) {\n+        if (arg.getArgNumber() < forOwner.getNumInductionVars())\n+          continue;\n+        unsigned iterIdx = arg.getArgNumber() - forOwner.getNumInductionVars();\n+        Value yieldOperand =\n+            forOwner.getBody()->getTerminator()->getOperand(iterIdx);\n+        markLive(yieldOperand);\n+        markLive(forOwner.getIterOperands()[iterIdx]);\n+      }\n+    }\n+    SmallVector<unsigned> deadArg;\n+    for (auto yieldOperand : llvm::enumerate(yieldOp->getOperands())) {\n+      if (aliveValues.contains(yieldOperand.value()))\n+        continue;\n+      if (yieldOperand.value() == block.getArgument(yieldOperand.index() + 1))\n+        continue;\n+      deadArg.push_back(yieldOperand.index());\n+    }\n+    if (deadArg.empty())\n+      return failure();\n+    rewriter.updateRootInPlace(forOp, [&]() {\n+      // For simplicity we just change the dead yield operand to use the\n+      // associated argument and leave the operations and argument removal to\n+      // dead code elimination.\n+      for (unsigned deadArgIdx : deadArg) {\n+        BlockArgument arg = block.getArgument(deadArgIdx + 1);\n+        yieldOp.setOperand(deadArgIdx, arg);\n+      }\n+    });\n+    return success();\n+  }\n+};\n+\n+} // namespace\n+\n+void populateForOpDeadArgumentElimination(RewritePatternSet &patterns) {\n+  patterns.add<ForOpDeadArgElimination>(patterns.getContext());\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/MaterializeLoadStore.cpp", "status": "modified", "additions": 26, "deletions": 1, "changes": 27, "file_content_changes": "@@ -150,9 +150,34 @@ void MaterializeLoadStorePass::materializeStoreTilePtr(\n     return;\n   auto loc = store.getLoc();\n   OpBuilder builder(store);\n-  auto *ctx = store.getContext();\n   auto value = store.getValue();\n   auto dst = store.getPtr();\n+\n+  auto cvtOp = llvm::dyn_cast_or_null<mlir::triton::gpu::ConvertLayoutOp>(\n+      value.getDefiningOp());\n+  if (cvtOp) {\n+    auto srcTy = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+    auto dstTy = cvtOp.getResult().getType().cast<RankedTensorType>();\n+    auto elemTy = srcTy.getElementType();\n+    auto srcMmaLayout = srcTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    auto dstBlockedLayout = dstTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    auto truncFOP = llvm::dyn_cast_or_null<arith::TruncFOp>(\n+        cvtOp.getOperand().getDefiningOp());\n+    unsigned numElems = ttg::getTotalElemsPerThread(srcTy);\n+    auto inOrd = ttg::getOrder(srcTy.getEncoding());\n+    auto outOrd = ttg::getOrder(dstTy.getEncoding());\n+    if (srcMmaLayout && srcMmaLayout.isHopper() && dstBlockedLayout &&\n+        truncFOP && elemTy.getIntOrFloatBitWidth() == 16 && numElems >= 16 &&\n+        inOrd == outOrd) {\n+      builder.create<ttng::StoreAsyncOp>(loc, dst, cvtOp.getOperand());\n+      builder.create<ttg::AsyncBulkCommitGroupOp>(loc);\n+      builder.create<ttg::AsyncBulkWaitOp>(loc, 0);\n+      store->erase();\n+      return;\n+    }\n+  }\n+\n+  auto *ctx = store.getContext();\n   auto storeTy = value.getType().dyn_cast<RankedTensorType>();\n   assert(storeTy);\n   auto storeElemTy = storeTy.getElementType();"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/PlanCTA.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -644,9 +644,10 @@ bool CTAPlanner::isElementwiseOp(Operation *op) const {\n                 math::RsqrtOp, math::SqrtOp, math::TanhOp>(op))\n     return true;\n   if (llvm::isa<triton::IntToPtrOp, triton::PtrToIntOp, triton::BitcastOp,\n-                triton::FpToFpOp, triton::AddPtrOp,\n-                triton::PureExternElementwiseOp>(op))\n+                triton::FpToFpOp, triton::AddPtrOp>(op))\n     return true;\n+  if (auto externElementwiseOp = dyn_cast<triton::ExternElementwiseOp>(op))\n+    return externElementwiseOp.getPure();\n   if (llvm::isa<ttg::CmpIOp, ttg::CmpFOp, ttg::SelectOp>(op))\n     return true;\n   return false;"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSPipeline.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -173,6 +173,10 @@ scf::ForOp appendPipelineIdxToLoopArgs(scf::ForOp forOp, int numStages,\n     initValue = parentForOp.getBody()->getArguments().back();\n     Value numSteps = builder.createWithAgentIds<arith::SubIOp>(\n         loc, forOp.getUpperBound(), forOp.getLowerBound());\n+    auto one = builder.createWithAgentIds<arith::ConstantIntOp>(loc, 1, 32);\n+    numSteps = builder.createWithAgentIds<arith::AddIOp>(loc, numSteps,\n+                                                         forOp.getStep());\n+    numSteps = builder.createWithAgentIds<arith::SubIOp>(loc, numSteps, one);\n     numSteps = builder.createWithAgentIds<arith::DivUIOp>(loc, numSteps,\n                                                           forOp.getStep());\n     initValue ="}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 16, "deletions": 14, "changes": 30, "file_content_changes": "@@ -55,7 +55,7 @@ struct NVVMMetadata {\n \n // Add the nvvm related metadata to LLVM IR.\n static void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata,\n-                          bool isROCM) {\n+                          Target target) {\n   auto *module = func->getParent();\n   auto &ctx = func->getContext();\n \n@@ -85,16 +85,19 @@ static void amendLLVMFunc(llvm::Function *func, const NVVMMetadata &metadata,\n   }\n \n   if (metadata.isKernel) {\n-    if (isROCM) {\n-      func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);\n-      func->addFnAttr(\"amdgpu-flat-work-group-size\", \"1, 1024\");\n-    } else {\n+    switch (target) {\n+    case Target::NVVM: {\n       llvm::Metadata *mdArgs[] = {\n           llvm::ValueAsMetadata::get(func), llvm::MDString::get(ctx, \"kernel\"),\n           llvm::ValueAsMetadata::get(\n               llvm::ConstantInt::get(llvm::Type::getInt32Ty(ctx), 1))};\n       module->getOrInsertNamedMetadata(\"nvvm.annotations\")\n           ->addOperand(llvm::MDNode::get(ctx, mdArgs));\n+    } break;\n+    case Target::ROCDL: {\n+      func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);\n+      func->addFnAttr(\"amdgpu-flat-work-group-size\", \"1, 1024\");\n+    } break;\n     }\n   }\n }\n@@ -240,7 +243,7 @@ static void linkLibdevice(llvm::Module &module) {\n }\n \n bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n-                   llvm::StringRef path, bool isROCM) {\n+                   llvm::StringRef path, Target target) {\n   llvm::SMDiagnostic err;\n   auto &ctx = module.getContext();\n \n@@ -259,8 +262,7 @@ bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n     return true;\n   }\n \n-  // check if ROCM\n-  if (!isROCM) {\n+  if (target == Target::NVVM) {\n     if (name == \"libdevice\") {\n       linkLibdevice(module);\n     }\n@@ -274,7 +276,7 @@ bool linkExternLib(llvm::Module &module, llvm::StringRef name,\n \n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n-                      bool isROCM) {\n+                      Target target) {\n   DialectRegistry registry;\n   mlir::registerBuiltinDialectTranslation(registry);\n   mlir::registerLLVMDialectTranslation(registry);\n@@ -302,7 +304,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n   // dead code.\n   auto externLibs = getExternLibs(module);\n   for (auto &lib : externLibs) {\n-    if (linkExternLib(*llvmModule, lib.first, lib.second, isROCM))\n+    if (linkExternLib(*llvmModule, lib.first, lib.second, target))\n       return nullptr;\n   }\n \n@@ -318,7 +320,7 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n   for (auto &func : llvmModule->functions()) {\n     auto it = nvvmMetadata.find(func.getName());\n     if (it != nvvmMetadata.end())\n-      amendLLVMFunc(&func, it->second, isROCM);\n+      amendLLVMFunc(&func, it->second, target);\n   }\n \n   return llvmModule;\n@@ -328,7 +330,7 @@ std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module, int computeCapability,\n                            mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n-                           bool isROCM) {\n+                           Target target) {\n   mlir::PassManager pm(module->getContext());\n   mlir::registerPassManagerCLOptions();\n   if (failed(applyPassManagerCLOptions(pm))) {\n@@ -351,7 +353,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   pm.addPass(mlir::createConvertSCFToCFPass());\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(\n-      createConvertTritonGPUToLLVMPass({computeCapability, &tmaInfos, isROCM}));\n+      createConvertTritonGPUToLLVMPass({computeCapability, &tmaInfos, target}));\n   pm.addPass(createConvertNVGPUToLLVMPass());\n   pm.addPass(mlir::createArithToLLVMConversionPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n@@ -366,7 +368,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n-  auto llvmIR = translateLLVMToLLVMIR(llvmContext, module, isROCM);\n+  auto llvmIR = translateLLVMToLLVMIR(llvmContext, module, target);\n   if (!llvmIR) {\n     llvm::errs() << \"Translate to LLVM IR failed\";\n     return nullptr;"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 0, "deletions": 18, "changes": 18, "file_content_changes": "@@ -43,25 +43,7 @@ static bool findAndReplace(std::string &str, const std::string &begin,\n   return true;\n }\n \n-static void linkExternal(llvm::Module &module) {\n-  namespace fs = std::filesystem;\n-\n-  // TODO: enable generating bc file from clang.\n-  static const auto this_file_path = std::filesystem::path(__FILE__);\n-  static const auto path =\n-      this_file_path.parent_path().parent_path().parent_path().parent_path() /\n-      \"python\" / \"triton\" / \"hopper_lib\" / \"libhopper_helpers.bc\";\n-\n-  // static const std::filesystem::path path =\n-  //     std::filesystem::path(__BUILD_DIR__) / \"lib\" / \"Hopper\" /\n-  //     \"libhopper_helpers.bc\";\n-  if (mlir::triton::linkExternLib(module, \"libhopper_helpers\", path.string(),\n-                                  /*isROCM*/ false))\n-    llvm::errs() << \"Link failed for: libhopper_helpers.bc\";\n-}\n-\n std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n-  linkExternal(module);\n   // LLVM version in use may not officially support target hardware.\n   // Supported versions for LLVM 14 are here:\n   // https://github.com/llvm/llvm-project/blob/f28c006a5895fc0e329fe15fead81e37457cb1d1/clang/include/clang/Basic/BuiltinsNVPTX.def"}, {"filename": "python/src/extra/cuda.ll", "status": "removed", "additions": 0, "deletions": 17, "changes": 17, "file_content_changes": "@@ -1,17 +0,0 @@\n-; ~/.triton/llvm/llvm+mlir-17.0.0-x86_64-linux-gnu-ubuntu-18.04-release/bin/llvm-as ./src/extra/cuda.ll -o ./triton/language/extra/cuda.bc\n-\n-target datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\n-target triple = \"nvptx64-nvidia-cuda\"\n-\n-\n-define i64 @globaltimer() #0 {\n-  %1 = call i64 asm sideeffect \"mov.u64 $0, %globaltimer;\", \"=l\"() nounwind\n-  ret i64 %1\n-}\n-\n-define i32 @smid() #0 {\n-  %1 = call i32 asm \"mov.u32 $0, %smid;\", \"=r\"() nounwind\n-  ret i32 %1\n-}\n-\n-attributes #0 = { alwaysinline nounwind }"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 55, "deletions": 9, "changes": 64, "file_content_changes": "@@ -53,6 +53,7 @@\n #include <fstream>\n #include <optional>\n #include <pybind11/buffer_info.h>\n+#include <pybind11/embed.h>\n #include <pybind11/functional.h>\n #include <pybind11/pybind11.h>\n #include <pybind11/stl.h>\n@@ -80,6 +81,11 @@ void init_triton_runtime(py::module &&m) {\n       .value(\"CUDA\", CUDA)\n       .value(\"ROCM\", ROCM)\n       .export_values();\n+\n+  py::enum_<mlir::triton::Target>(m, \"TARGET\")\n+      .value(\"NVVM\", mlir::triton::NVVM)\n+      .value(\"ROCDL\", mlir::triton::ROCDL)\n+      .export_values();\n }\n \n // A custom op builder that keeps track of the last location\n@@ -166,6 +172,30 @@ class TritonOpBuilder {\n   bool lineInfoEnabled = !triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\");\n };\n \n+static std::string locationToString(mlir::Location loc) {\n+  std::string str;\n+  llvm::raw_string_ostream os(str);\n+  loc.print(os);\n+  os.flush(); // Make sure all the content is dumped into the 'str' string\n+  return str;\n+}\n+\n+static void outputWarning(mlir::Location loc, const std::string &msg) {\n+  std::string locStr = locationToString(loc);\n+\n+  py::exec(\n+      R\"(\n+import warnings\n+\n+def custom_showwarning(message, category, filename, lineno, file=None, line=None):\n+    print(f\"UserWarning: {message}\")\n+\n+warnings.showwarning = custom_showwarning\n+warnings.warn(f\"{loc}: {msg}\")\n+)\",\n+      py::globals(), py::dict(py::arg(\"loc\") = locStr, py::arg(\"msg\") = msg));\n+}\n+\n /*****************************************************************************/\n /* Python bindings for triton::ir                                            */\n /*****************************************************************************/\n@@ -591,6 +621,17 @@ void init_triton_ir(py::module &&m) {\n                  newBlock->erase();\n                }\n              });\n+             // 2. Check if the result of tl.advance is used\n+             self.walk([&](mlir::Operation *op) {\n+               if (mlir::isa<mlir::triton::AdvanceOp>(op) &&\n+                   op->getResult(0).use_empty())\n+                 outputWarning(op->getLoc(), \"The result of tl.advance is not \"\n+                                             \"being used. Note that tl.advance \"\n+                                             \"does not have any side effects. \"\n+                                             \"To move the block pointer, you \"\n+                                             \"need to assign the result of \"\n+                                             \"tl.advance to a variable.\");\n+             });\n            })\n       .def_property_readonly(\"type\", &mlir::triton::FuncOp::getFunctionType)\n       .def(\"reset_type\", &mlir::triton::FuncOp::setType);\n@@ -755,7 +796,7 @@ void init_triton_ir(py::module &&m) {\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getI64Type();\n            })\n-      .def(\"get_fp8e4_ty\",\n+      .def(\"get_fp8e4nv_ty\",\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getType<mlir::Float8E4M3FNUZType>();\n            })\n@@ -1419,12 +1460,8 @@ void init_triton_ir(py::module &&m) {\n               const std::string &libPath, const std::string &symbol,\n               std::vector<mlir::Value> &argList, mlir::Type retType,\n               bool isPure) -> mlir::Value {\n-             if (isPure)\n-               return self.create<mlir::triton::PureExternElementwiseOp>(\n-                   retType, argList, libName, libPath, symbol);\n-             else\n-               return self.create<mlir::triton::ImpureExternElementwiseOp>(\n-                   retType, argList, libName, libPath, symbol);\n+             return self.create<mlir::triton::ExternElementwiseOp>(\n+                 retType, argList, libName, libPath, symbol, isPure);\n            })\n       // Built-in instruction\n       .def(\"create_get_program_id\",\n@@ -1519,6 +1556,14 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::arith::SelectOp>(condition, trueValue,\n                                                        falseValue);\n            })\n+      .def(\"create_inline_asm\",\n+           [](TritonOpBuilder &self, const std::string &inlineAsm,\n+              const std::string &constraints,\n+              const std::vector<mlir::Value> &values, mlir::Type &type,\n+              bool isPure, int pack) -> mlir::Value {\n+             return self.create<mlir::triton::ElementwiseInlineAsmOp>(\n+                 type, inlineAsm, constraints, isPure, pack, values);\n+           })\n       .def(\"create_print\",\n            [](TritonOpBuilder &self, const std::string &prefix,\n               const std::vector<mlir::Value> &values) -> void {\n@@ -1804,11 +1849,12 @@ void init_triton_translation(py::module &m) {\n   m.def(\n       \"translate_triton_gpu_to_llvmir\",\n       [](mlir::ModuleOp op, int computeCapability,\n-         mlir::triton::gpu::TMAMetadataTy &tmaInfos, bool isROCM) {\n+         mlir::triton::gpu::TMAMetadataTy &tmaInfos,\n+         mlir::triton::Target target) {\n         py::gil_scoped_release allow_threads;\n         llvm::LLVMContext llvmContext;\n         auto llvmModule = ::mlir::triton::translateTritonGPUToLLVMIR(\n-            &llvmContext, op, computeCapability, tmaInfos, isROCM);\n+            &llvmContext, op, computeCapability, tmaInfos, target);\n         if (!llvmModule)\n           llvm::report_fatal_error(\"Failed to translate TritonGPU to LLVM IR.\");\n "}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 56, "deletions": 0, "changes": 56, "file_content_changes": "@@ -225,3 +225,59 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str):\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str)]\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)\n+\n+\n+#######################\n+# Reduction\n+#######################\n+\n+\n+@triton.jit\n+def _sum(x_ptr, y_ptr, output_ptr, n_elements,\n+         BLOCK_SIZE: tl.constexpr):\n+    pid = tl.program_id(axis=0)\n+    block_start = pid * BLOCK_SIZE\n+    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+    mask = offsets < n_elements\n+    x = tl.load(x_ptr + offsets, mask=mask)\n+    y = tl.load(y_ptr + offsets, mask=mask)\n+    # run in a loop to only to make it compute bound.\n+    for i in range(100):\n+        x = tl.sum(x, axis=0) + y\n+\n+    tl.store(output_ptr + offsets, x, mask=mask)\n+\n+\n+reduction_data = {\n+    'a100': {\n+        1024 * 16384: {'float16': 0.016, 'float32': 0.031, 'int16': 0.015, 'int32': 0.031},\n+        1024 * 65536: {'float16': 0.016, 'float32': 0.032, 'int16': 0.015, 'int32': 0.032},\n+    }\n+}\n+\n+\n+@pytest.mark.parametrize('N', reduction_data[DEVICE_NAME].keys())\n+@pytest.mark.parametrize(\"dtype_str\", ['float16', 'float32', 'int16', 'int32'])\n+def test_reductions(N, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n+    torch.manual_seed(0)\n+    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int16': torch.int16, 'int32': torch.int32}[dtype_str]\n+    ref_gpu_util = reduction_data[DEVICE_NAME][N][dtype_str]\n+    cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n+    max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n+    z = torch.empty((N, ), dtype=dtype, device='cuda')\n+    if dtype == torch.float16 or dtype == torch.float32:\n+        x = torch.randn_like(z)\n+        y = torch.randn_like(z)\n+    else:\n+        info = torch.iinfo(dtype)\n+        x = torch.randint(info.min, info.max, (N,), dtype=dtype, device='cuda')\n+        y = torch.randint(info.min, info.max, (N,), dtype=dtype, device='cuda')\n+    grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n+    fn = lambda: _sum[grid](x, y, z, N, BLOCK_SIZE=1024)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n+    cur_gpu_perf = 100. * 2. * N / ms * 1e-9\n+    cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)"}, {"filename": "python/test/unit/hopper/test_gemm.py", "status": "modified", "additions": 87, "deletions": 76, "changes": 163, "file_content_changes": "@@ -211,102 +211,113 @@ def matmul_kernel(\n \n \n @pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_OUTPUT,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n-                         [(128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n-                          for shape_w_c in [\n-                             # badcase from cublas-important-layers\n-                             [4096, 1, 1024, False, False, True],\n-                             [2048, 204, 1000, True, False, True],\n-                             [4096, 1, 1024, False, False, False],\n-                             [2048, 204, 1000, True, False, False],\n-                         ]\n+                         [\n+                             # corner shapes\n+                             (128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n+                             for shape_w_c in [\n+                                 [4096, 1, 1024, False, False, True],\n+                                 [2048, 204, 1000, True, False, True],\n+                                 [4096, 1, 1024, False, False, False],\n+                                 [2048, 204, 1000, True, False, False],\n+                             ]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              # softmax works for one CTA\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 64, 64, 64],\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [16, 16, 64, 4, 1, 16, 16, 64],\n-                             [64, 64, 32, 8, 1, 64, 64, 64],\n-                             [128, 128, 64, 4, 1, 128, 128, 128],\n-                         ]\n+                         ] + [\n+                             # softmax epilogue\n+                             (*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 64, 64, 64],\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [16, 16, 64, 4, 1, 16, 16, 64],\n+                                 [64, 64, 32, 8, 1, 64, 64, 64],\n+                                 [128, 128, 64, 4, 1, 128, 128, 128],\n+                             ]\n                              for epilogue in ['softmax']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n-                             for trans_output in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n+                             for trans_output in [False,]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 128, 128, 64],\n-                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n-                             # for chain-dot\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [64, 64, 16, 4, 1, None, None, None],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 16, 64, 4, 1, 128, 128, 64],\n-                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4, 8] for num_ctas in [1, 2]],\n-                             # repeat\n-                             [64, 64, 32, 8, 1, 128, 256, 64],\n-                             [64, 64, 16, 8, 2, 128, 128, 64],\n-                             # irregular shape\n-                             [128, 128, 64, 4, 1, 500, 200, 128],\n-                             [128, 128, 64, 4, 2, 513, 193, 192],\n-                         ]\n+                         ] + [\n+                             # loop over epilogues besides of softmax\n+                             (*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 128, 128, 64],\n+                                 *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n+                                 # for chain-dot\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [64, 64, 16, 4, 1, None, None, None],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 16, 64, 4, 1, 128, 128, 64],\n+                                 *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4, 8] for num_ctas in [1, 2]],\n+                                 # repeat\n+                                 [64, 64, 32, 8, 1, 128, 256, 64],\n+                                 [64, 64, 16, 8, 2, 128, 128, 64],\n+                                 # irregular shape\n+                                 [128, 128, 64, 4, 1, 500, 200, 128],\n+                                 [128, 128, 64, 4, 2, 513, 193, 192],\n+                             ]\n                              for epilogue in ['none', 'add-matrix', 'add-rows', 'add-cols', 'chain-dot']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n-                             for trans_output in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n+                             for trans_output in [False,]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n                              if not (epilogue == 'chain-dot' and (shape_w_c[6] is not None or shape_w_c[1] != shape_w_c[6]))\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 32, 4, 1, 128, 256, 64],\n-                             [128, 128, 16, 4, 4, 512, 256, 64],\n-                             [128, 256, 32, 4, 8, 256, 256, 192],\n-                             [512, 256, 32, 4, 8, 1024, 256, 192],\n-                             # BLOCK_K >= 128\n-                             [64, 128, 128, 4, 1, 512, 256, 256],\n-                             [128, 128, 128, 4, 1, 256, 256, 192],\n-                             [128, 128, 128, 4, 2, 256, 256, 192],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 32, 32, 4, 1, 128, 256, 64],\n-                             [32, 32, 16, 4, 1, 256, 256, 192],\n-                             [16, 32, 64, 4, 4, 512, 256, 64],\n-                         ]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                         ] + [\n+                             # loop over tile shapes and transpose combinations\n+                             (*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 32, 4, 1, 128, 256, 64],\n+                                 [128, 128, 16, 4, 4, 512, 256, 64],\n+                                 [128, 256, 32, 4, 8, 256, 256, 192],\n+                                 [512, 256, 32, 4, 8, 1024, 256, 192],\n+                                 # BLOCK_K >= 128\n+                                 [64, 128, 128, 4, 1, 512, 256, 256],\n+                                 [128, 128, 128, 4, 1, 256, 256, 192],\n+                                 [128, 128, 128, 4, 2, 256, 256, 192],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 32, 32, 4, 1, 128, 256, 64],\n+                                 [32, 32, 16, 4, 1, 256, 256, 192],\n+                                 [16, 32, 64, 4, 4, 512, 256, 64],\n+                             ]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n                              for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                              # loop over instr shapes\n-                              for n in [16, 32, 64, 128, 256]\n-                              for trans_output in [False, True]\n-                              for out_dtype in ['float16', 'float32']\n-                              for use_tma_store in [False, True]\n-                              for num_stages in [2, 4, 5, 7]\n-                              for enable_ws in [False, True]\n-                              ] + [(*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                                   # irregular shapes\n-                                   for shape_w_c in [\n-                                       [128, 128, 64, 4, 1],\n-                                       [256, 128, 64, 4, 2],\n-                                       [128, 128, 128, 4, 2],\n-                              ]\n-                             for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for trans_output in [False, True]\n-                             for out_dtype in ['float16', 'float32']\n+                         ] + [\n+                             # loop over instr shapes & pipeline stages\n+                             (64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for n in [16, 32, 64, 128, 256]\n+                             for trans_output in [False,]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n+                             for num_stages in [2, 4, 5, 7]\n+                             for enable_ws in [False, True]\n+                         ] + [\n+                             # irregular shapes\n+                             (*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [128, 128, 64, 4, 1],\n+                                 [256, 128, 64, 4, 2],\n+                                 [128, 128, 128, 4, 2],\n+                             ]\n+                             for shape in [\n+                                 [512, 360, 1024],\n+                                 [360, 4096, 512],\n+                             ]\n+                             for trans_output in [False,]\n+                             for out_dtype in ['float32',]\n                              for use_tma_store in [False, True]\n-                             for num_stages in [2, 3, 4]\n+                             for num_stages in [3, 4]\n                              for enable_ws in [False, True]\n                          ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 40, "deletions": 32, "changes": 72, "file_content_changes": "@@ -696,16 +696,18 @@ def full_static_persistent_matmul_kernel(\n \n @pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n                          [\n+                             # corner shapes\n                              (128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n                              for shape_w_c in [\n-                                 # bad from cublas-important-layers\n                                  [4096, 1, 1024, False, False],\n                                  [2048, 204, 1000, True, False],\n+                                 [16, 524288, 32, False, True],\n                              ]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for enable_ws in [True]\n                          ] + [\n+                             # softmax epilogue\n                              (*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                              # softmax works for one CTA\n                              for shape_w_c in [\n@@ -719,11 +721,12 @@ def full_static_persistent_matmul_kernel(\n                              for epilogue in ['softmax']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n                          ] + [\n+                             # loop over tile shapes and transpose combinations\n                              (*shape_w_c, trans_a, trans_b, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              for shape_w_c in [\n                                  [64, 64, 32, 4, 1, 128, 256, 64],\n@@ -739,58 +742,63 @@ def full_static_persistent_matmul_kernel(\n                                  [32, 32, 16, 4, 1, 256, 256, 192],\n                                  [16, 32, 64, 4, 4, 512, 256, 64],\n                              ]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 128, 128, 64],\n-                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1, 2, 4]],\n-                             # for chain-dot\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [64, 64, 16, 4, 1, None, None, None],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 16, 64, 4, 1, 128, 128, 64],\n-                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1, 2]],\n-                             #  # TODO: enable when num_warps != 4 is supported.\n-                             #  # repeat\n-                             #  # [64, 64, 32, 8, 1, 128, 256, 64],\n-                             #  # [64, 64, 16, 8, 2, 128, 128, 64],\n-                             # irregular shape\n-                             [128, 128, 64, 4, 1, 500, 200, 128],\n-                             [128, 128, 64, 4, 1, 513, 193, 192],\n-                         ]\n+                         ] + [\n+                             # loop over epilogues besides of softmax\n+                             (*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 128, 128, 64],\n+                                 *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1, 2, 4]],\n+                                 # for chain-dot\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [64, 64, 16, 4, 1, None, None, None],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 16, 64, 4, 1, 128, 128, 64],\n+                                 *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1, 2]],\n+                                 #  # TODO: enable when num_warps != 4 is supported.\n+                                 #  # repeat\n+                                 #  # [64, 64, 32, 8, 1, 128, 256, 64],\n+                                 #  # [64, 64, 16, 8, 2, 128, 128, 64],\n+                                 # irregular shape\n+                                 [128, 128, 64, 4, 1, 500, 200, 128],\n+                                 [128, 128, 64, 4, 1, 513, 193, 192],\n+                             ]\n                              for epilogue in ['none', 'add-matrix', 'add-rows', 'add-cols', 'chain-dot']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n                              if not (epilogue == 'chain-dot' and (shape_w_c[5] is not None or shape_w_c[0] != shape_w_c[1]))\n                          ] + [\n+                             # loop over instr shapes & pipeline stages\n                              (64, n, 16, 4, 1, 512, 256, 256, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                             # loop over instr shapes\n                              for n in [16, 32, 64, 128, 256]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                             for out_dtype in ['float32']\n+                             for use_tma_store in [False,]\n                              for num_stages in [2, 4, 5, 7]\n                              for enable_ws in [True]\n                          ] + [\n-                             (*shape_w_c, *shape, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              # irregular shapes\n+                             (*shape_w_c, *shape, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              for shape_w_c in [\n                                  [128, 128, 64, 4, 1],\n                                  [256, 128, 64, 4, 2],\n                                  [128, 128, 128, 4, 2]\n                              ]\n-                             for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for out_dtype in ['float16', 'float32']\n+                             for shape in [\n+                                 [512, 360, 1024],\n+                                 [360, 4096, 512],\n+                             ]\n+                             for out_dtype in ['float32']\n                              for use_tma_store in [False, True]\n-                             for num_stages in [2, 3, 4]\n+                             for num_stages in [3, 4]\n                              for enable_ws in [True]\n                          ]\n                          )"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 117, "deletions": 41, "changes": 158, "file_content_changes": "@@ -131,6 +131,8 @@ def check_type_supported(dtype, device):\n         cc = torch.cuda.get_device_capability()\n         if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n             pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+        if cc[0] < 9 and (dtype is tl.float8e4nv or dtype == \"float8e4\"):\n+            pytest.skip(\"float8e4 is only supported on NVGPU with cc >= 90\")\n \n \n class MmaLayout:\n@@ -855,7 +857,7 @@ def test_abs(dtype_x, device):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4nv, tl.float8e5])\n def test_abs_fp8(in_dtype, device):\n     if is_hip():\n         pytest.skip('test_abs_fp8 not supported on HIP.')\n@@ -1403,8 +1405,8 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     extended_exp = ((1 << (tl.float32.primitive_bitwidth - tl.float32.fp_mantissa_width - 1)) - 1) << tl.float32.fp_mantissa_width\n     # special cases, exp is 0b11..1\n-    if dtype in [tl.float8e4, tl.float8e4b15]:\n-        # float8e4m3 does not have infinities\n+    if dtype in [tl.float8e4nv, tl.float8e4b15]:\n+        # float8e4m3nv does not have infinities\n         output[fp == 0b01111111] = torch.nan\n         output[fp == 0b11111111] = torch.nan\n     else:\n@@ -1463,7 +1465,7 @@ def deserialize_fp8(np_data, in_dtype):\n         return np_data\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4nv, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"\n@@ -1472,9 +1474,10 @@ def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n         - conversion tri_fp8 = convert(input=tri_fp16, out=out_dtype) matches the original\n     this is only possible if both conversions are correct\n     \"\"\"\n+    check_type_supported(in_dtype, device)\n+    check_type_supported(out_dtype, device)\n     if is_hip():\n         pytest.skip('test_abs_fp8 not supported on HIP.')\n-    check_type_supported(out_dtype, device)\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -1511,8 +1514,6 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n def get_reduced_dtype(dtype_str, op):\n     if op in ('argmin', 'argmax'):\n         return 'int32'\n-    if dtype_str in ['int8', 'uint8', 'int16', 'uint16']:\n-        return 'int32'\n     if dtype_str == 'bfloat16':\n         return 'float32'\n     return dtype_str\n@@ -2208,7 +2209,7 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o\n     if capability[0] < 8:\n         if in_dtype == 'int8':\n             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n-        elif in_dtype == 'float32' and allow_tf32:\n+        elif allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n     if capability[0] == 7:\n         if (M, N, K, num_warps) == (128, 256, 32, 8):\n@@ -2237,12 +2238,12 @@ def kernel(X, stride_xm, stride_xk,\n                Y, stride_yk, stride_yn,\n                W, stride_wn, stride_wl,\n                Z, stride_zm, stride_zn,\n-               out_dtype: tl.constexpr,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n                ALLOW_TF32: tl.constexpr,\n                DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n-               COL_A: tl.constexpr, COL_B: tl.constexpr):\n+               COL_A: tl.constexpr, COL_B: tl.constexpr,\n+               out_dtype: tl.constexpr = tl.float32):\n         off_m = tl.arange(0, BLOCK_M)\n         off_n = tl.arange(0, BLOCK_N)\n         off_l = tl.arange(0, BLOCK_N)\n@@ -2270,7 +2271,7 @@ def kernel(X, stride_xm, stride_xk,\n             z = num / den[:, None]\n         if CHAIN_DOT:\n             w = tl.load(Ws)\n-            z = tl.dot(z.to(w.dtype), w, out_dtype=out_dtype)\n+            z = tl.dot(z.to(w.dtype), w, allow_tf32=ALLOW_TF32, out_dtype=out_dtype)\n         tl.store(Zs, z)\n     # input\n     rs = RandomState(17)\n@@ -2317,7 +2318,6 @@ def kernel(X, stride_xm, stride_xk,\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n                          z_tri, z_tri.stride(0), z_tri.stride(1),\n-                         out_dtype,\n                          COL_A=col_a, COL_B=col_b,\n                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n@@ -2326,21 +2326,26 @@ def kernel(X, stride_xm, stride_xk,\n                          DO_SOFTMAX=epilogue == 'softmax',\n                          CHAIN_DOT=epilogue == 'chain-dot',\n                          ALLOW_TF32=allow_tf32,\n-                         num_warps=num_warps, num_ctas=num_ctas)\n-    if not is_hip() and epilogue == 'softmax' and (in_dtype != 'float32' or allow_tf32):\n-        ptx = pgm.asm[\"ptx\"]\n-        start = ptx.find(\"shfl.sync\")\n-        end = ptx.find(\"cvt.rn.f16.f32\")\n-        red_code = ptx[start:end]\n-        assert len(red_code) > 0\n-        import os\n-        enable_mmav3 = os.environ.get('ENABLE_MMA_V3', 'not found').lower()\n-        enable_tma = os.environ.get('ENABLE_TMA', 'not found').lower()\n-        # skip this check on hopper because there are some functions whose name contain \"shared\" in ptx.\n-        # TODO: we should eliminate these unused functions in ptx code.\n-        if not (enable_mmav3 in [\"on\", \"true\", \"1\"] and enable_tma in [\"on\", \"true\", \"1\"]):\n-            assert \"shared\" not in red_code\n-        assert \"bar.sync\" not in red_code\n+                         num_warps=num_warps, num_ctas=num_ctas,\n+                         out_dtype=out_dtype)\n+    \n+    if epilogue == 'softmax' and (in_dtype != 'float32' or allow_tf32):\n+        if is_hip():\n+            pass\n+        else:\n+            ptx = pgm.asm[\"ptx\"]\n+            start = ptx.find(\"shfl.sync\")\n+            end = ptx.find(\"cvt.rn.f16.f32\")\n+            red_code = ptx[start:end]\n+            assert len(red_code) > 0\n+            import os\n+            enable_mmav3 = os.environ.get('ENABLE_MMA_V3', 'not found').lower()\n+            enable_tma = os.environ.get('ENABLE_TMA', 'not found').lower()\n+            # skip this check on hopper because there are some functions whose name contain \"shared\" in ptx.\n+            # TODO: we should eliminate these unused functions in ptx code.\n+            if not (enable_mmav3 in [\"on\", \"true\", \"1\"] and enable_tma in [\"on\", \"true\", \"1\"]):\n+                assert \"shared\" not in red_code\n+            assert \"bar.sync\" not in red_code\n     # torch result\n     if in_dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32),\n@@ -2381,16 +2386,26 @@ def kernel(X, stride_xm, stride_xk,\n     if in_dtype == 'float32' and allow_tf32:\n         assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.tf32.tf32', ptx)\n     elif in_dtype == 'float16' and out_dtype == tl.float32:\n-        assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f32.f16.f16', ptx)\n+        if capability[0] == 7 and capability[1] == 5:  # Turing\n+            assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.f16.f16', ptx)\n+        else:\n+            assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f32.f16.f16', ptx)\n     elif in_dtype == 'float16' and out_dtype == tl.float16:\n-        assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f16.f16.f16', ptx)\n+        if capability[0] == 7 and capability[1] == 5:  # Turing\n+            assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f16.f16.f16', ptx)\n+        else:\n+            assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f16.f16.f16', ptx)\n     elif in_dtype == 'int8':\n         assert 'wgmma.mma_async.sync.aligned' in ptx or\\\n             'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n @pytest.mark.parametrize('in_dtype', ['float32'])\n def test_dot_mulbroadcastred(in_dtype, device):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 8:\n+        pytest.skip(\"Requires sm >= 80 to run\")\n+\n     @triton.jit\n     def kernel(Z, X, Y,\n                M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n@@ -2491,22 +2506,25 @@ def kernel(out_ptr):\n \n @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n def test_dot_without_load(dtype_str, device):\n+    capability = torch.cuda.get_device_capability()\n+    allow_tf32 = capability[0] > 7\n+\n     if is_hip() and dtype_str == \"float16\":\n         pytest.skip(\"test_dot_without_load[float16] not supported in HIP\")\n \n     @triton.jit\n-    def _kernel(out):\n+    def _kernel(out, ALLOW_TF32: tl.constexpr):\n         a = GENERATE_TEST_HERE\n         b = GENERATE_TEST_HERE\n-        c = tl.dot(a, b)\n+        c = tl.dot(a, b, allow_tf32=ALLOW_TF32)\n         out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n         tl.store(out_ptr, c)\n     kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n     out_ref = torch.matmul(a, b)\n     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=device)\n-    kernel[(1,)](out)\n+    kernel[(1,)](out, ALLOW_TF32=allow_tf32)\n     assert torch.all(out == out_ref)\n \n # ---------------\n@@ -2981,8 +2999,10 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # test if\n # -------------\n \n+# TODO(Keren): if_exp_dynamic\n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n+\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_and_dynamic\", \"if_exp_static\", \"if_and_static\"])\n def test_if(if_type, device):\n \n     @triton.jit\n@@ -2994,8 +3014,10 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n                 tl.store(Ret, tl.load(XTrue))\n             else:\n                 tl.store(Ret, tl.load(XFalse))\n-        elif IfType == \"if_exp\":\n-            tl.store(Ret, tl.load(XTrue)) if pid % 2 else tl.store(Ret, tl.load(XFalse))\n+        elif IfType == \"if_exp_dynamic\":\n+            tl.store(Ret, tl.load(XTrue)) if pid % 2 == 0 else tl.store(Ret, tl.load(XFalse))\n+        elif IfType == \"if_exp_static\":\n+            tl.store(Ret, tl.load(XTrue)) if BoolVar else tl.store(Ret, tl.load(XFalse))\n         elif IfType == \"if_and_dynamic\":\n             if BoolVar and pid % 2 == 0:\n                 tl.store(Ret, tl.load(XTrue))\n@@ -3010,7 +3032,7 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n     cond = torch.ones(1, dtype=torch.int32, device=device)\n     x_true = torch.tensor([3.14], dtype=torch.float32, device=device)\n     x_false = torch.tensor([1.51], dtype=torch.float32, device=device)\n-    ret = torch.empty(1, dtype=torch.float32, device=device)\n+    ret = torch.zeros(1, dtype=torch.float32, device=device)\n \n     kernel[(1,)](cond, x_true, x_false, ret, if_type, True, 1)\n     assert torch.equal(ret, x_true)\n@@ -3133,6 +3155,60 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n \n+\n+# -----------------------\n+# test inline asm\n+# -----------------------\n+\n+@pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n+def test_inline_asm(num_ctas, device):\n+    check_cuda_only(device)\n+\n+    @triton.jit\n+    def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        y = tl.load(Y + tl.arange(0, BLOCK))\n+        s = tl.full([BLOCK], n, tl.int32)\n+        z = tl.inline_asm_elementwise(\"shf.l.wrap.b32 $0, $1, $2, $3;\", \"=r,r, r, r\", [x, y, s], dtype=tl.int32, is_pure=True, pack=1)\n+        tl.store(Z + tl.arange(0, BLOCK), z)\n+\n+    shape = (128, )\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str='uint32', rs=rs)\n+    y = numpy_random(shape, dtype_str='uint32', rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    n = 17\n+    z_tri = to_triton(numpy_random(shape, dtype_str='uint32', rs=rs), device=device)\n+    kernel[(1,)](x_tri, y_tri, z_tri, n, BLOCK=shape[0], num_ctas=num_ctas)\n+    y_ref = (y << n) | (x >> (32 - n))\n+    # compare\n+    np.testing.assert_equal(y_ref, to_numpy(z_tri))\n+\n+\n+@pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n+def test_inline_asm_packed(num_ctas, device):\n+    check_cuda_only(device)\n+\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        # shift 4x8bits values together.\n+        y = tl.inline_asm_elementwise(\"and.b32 $0, $1, 0x1F1F1F1F; \\\n+                                       shl.b32 $0, $0, 3;\",\n+                                      \"=r,r\", [x,], dtype=tl.int8, is_pure=True, pack=4)\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+    shape = (512, )\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str='uint8', rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(numpy_random(shape, dtype_str='uint8', rs=rs), device=device)\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], num_ctas=num_ctas)\n+    y_ref = x << 3\n+    # compare\n+    np.testing.assert_equal(y_ref, to_numpy(y_tri))\n+\n # -----------------------\n # test control flow\n # -----------------------\n@@ -3240,8 +3316,9 @@ def add_fn_static_cond(x, cond: tl.constexpr):\n         return x + 1\n \n \n+# TODO(Keren): if_exp\n @pytest.mark.parametrize(\"call_type\", [\"attribute\", \"attribute_jit\",\n-                                       \"jit\", \"jit_if\", \"jit_ifexp\", \"jit_expr\",\n+                                       \"jit\", \"jit_if\", \"jit_expr\",\n                                        \"jit_static_cond\", \"jit_noinline\", \"jit_extern\"])\n def test_if_call(call_type, device):\n     @triton.jit\n@@ -3272,7 +3349,7 @@ def kernel(Out, call_type: tl.constexpr):\n                 a = o\n                 a = add_fn_return(a, pid)\n                 o = a\n-        elif call_type == \"jit_ifexp\":\n+        elif call_type == \"jit_if_exp\":\n             # ifexp expression\n             if pid == 0:\n                 a = o\n@@ -3433,8 +3510,7 @@ def kernel(Out1, Out2):\n     out2 = to_triton(np.zeros((1,), dtype=np.int64), device=device)\n     h = kernel[(1,)](out1, out2)\n     assert out2[0] > 0\n-    # 2 inlined globaltimers + one extra in the wrapper extern function\n-    assert h.asm[\"ptx\"].count(\"%globaltimer\") == 3\n+    assert h.asm[\"ptx\"].count(\"%globaltimer\") == 2\n \n \n def test_smid(device):\n@@ -3449,7 +3525,7 @@ def kernel(Out):\n     out = to_triton(np.zeros((1024,), dtype=np.int32), device=device)\n     h = kernel[(out.shape[0],)](out)\n     assert out.sort()[0].unique().shape[0] > 0\n-    assert h.asm[\"ptx\"].count(\"%smid\") == 2\n+    assert h.asm[\"ptx\"].count(\"%smid\") == 1\n \n # -----------------------\n # test layout conversions"}, {"filename": "python/test/unit/language/test_line_info.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -116,5 +116,3 @@ def test_line_info(func: str):\n         assert (check_file_lines(file_lines, \"standard.py\", 33))\n         assert (check_file_lines(file_lines, \"standard.py\", 34))\n         assert (check_file_lines(file_lines, \"standard.py\", 36))\n-        # core.py is changed frequently, so we only check if it exists\n-        assert (check_file_lines(file_lines, \"core.py\", -1))"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 62, "deletions": 44, "changes": 106, "file_content_changes": "@@ -26,63 +26,79 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\n-    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE\",\n+    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE, ALLOW_TF32\",\n     itertools.chain(\n         *[\n             [\n                 # 1 warp\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 2 warp\n-                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 4 warp\n-                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 8 warp\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # variable input\n-                (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE),\n-                (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE, True),\n+                (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE, True),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n         *[\n             [\n-                (16, 16, 16, 1, 1, STAGES, 32, 32, 80, AT, BT, DTYPE, DTYPE),\n-                (64, 32, 64, 1, 2, STAGES, 128, 64, 128, AT, BT, DTYPE, DTYPE),\n-                (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 16, 1, 1, STAGES, 32, 32, 80, AT, BT, DTYPE, DTYPE, True),\n+                (64, 32, 64, 1, 2, STAGES, 128, 64, 128, AT, BT, DTYPE, DTYPE, True),\n+                (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE, True),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [4]\n         ],\n         # mixed-precision\n         *[\n             [\n-                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8e4\", \"float8e5\"),\n-                                     (\"float8e4\", \"float16\"),\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, True),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, True),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE, True),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float8e5\"),\n+                                     (\"float8e4nv\", \"float8e4nv\"),\n+                                     (\"float8e5\", \"float8e4nv\"),\n+                                     (\"float8e5\", \"float8e5\"),\n+                                     (\"float8e4nv\", \"float16\"),\n+                                     (\"float16\", \"float8e5\"),\n+                                     (\"float16\", \"float32\"),\n+                                     (\"float32\", \"float16\"),\n+                                     (\"bfloat16\", \"float32\"),\n+                                     (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n+        ],\n+        # mixed-precision block layout\n+        *[\n+            [\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, False),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, False),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE, False),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n                                      (\"float16\", \"float32\"),\n                                      (\"float32\", \"float16\"),\n@@ -92,22 +108,24 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n         *[\n             # float8e4b15 only supports row-col layout\n             [\n-                (128, 128, 32, 1, 4, 2, None, None, None, False, True, ADTYPE, BDTYPE),\n+                (128, 128, 32, 1, 4, 2, None, None, None, False, True, ADTYPE, BDTYPE, True),\n             ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n                                      (\"float8e4b15\", \"float16\"),\n                                      (\"float16\", \"float8e4b15\"),\n                                      (\"float8e5\", \"float8e5\"),\n-                                     (\"float8e4\", \"float8e4\"),\n+                                     (\"float8e4nv\", \"float8e4nv\"),\n                                      (\"int8\", \"int8\")]\n         ]\n     ),\n )\n-def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE):\n+def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE, ALLOW_TF32):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8 and (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\"):\n         pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n+    if capability[0] < 9 and (ADTYPE == \"float8e4nv\" or BDTYPE == \"float8e4nv\"):\n+        pytest.skip(\"Only test float8e4nv on devices with sm >= 90\")\n     if (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\") and SPLIT_K != 1:\n         pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n     torch.manual_seed(0)\n@@ -131,7 +149,7 @@ def maybe_upcast(x, dtype, is_float8):\n \n     def init_input(m, n, dtype):\n         if 'float8' in dtype:\n-            ewidth = {'float8e4b15': 4, 'float8e4': 4, 'float8e5': 5}[dtype]\n+            ewidth = {'float8e4b15': 4, 'float8e4nv': 4, 'float8e5': 5}[dtype]\n             sign = torch.randint(2, size=(m, n), device=\"cuda\", dtype=torch.int8) * 128\n             val = torch.randint(2**3 - 1, size=(m, n), device=\"cuda\", dtype=torch.int8) << 7 - ewidth\n             return sign | val\n@@ -168,7 +186,7 @@ def init_input(m, n, dtype):\n             a = triton.reinterpret(a, getattr(tl, ADTYPE))\n         if b_fp8:\n             b = triton.reinterpret(b, getattr(tl, BDTYPE))\n-        tt_c = triton.ops.matmul(a, b)\n+        tt_c = triton.ops.matmul(a, b, None, ALLOW_TF32)\n         torch.testing.assert_allclose(th_c, tt_c, atol=0, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/test/unit/tools/test_aot.py", "status": "modified", "additions": 138, "deletions": 37, "changes": 175, "file_content_changes": "@@ -23,26 +23,43 @@ def mul(x, y):\n import kernel_utils\n \n @triton.jit\n-def kernel(C, A, B,\n+def kernel(C, A, B, M, N, K,\n           stride_cm, stride_cn,\n           stride_am, stride_ak,\n           stride_bk, stride_bn,\n           BLOCK_M: tl.constexpr,\n           BLOCK_N: tl.constexpr,\n           BLOCK_K: tl.constexpr):\n-  ms = tl.arange(0, BLOCK_M)\n-  ns = tl.arange(0, BLOCK_N)\n-  ks = tl.arange(0, BLOCK_K)\n-  a = tl.load(A + ms[:, None] * stride_am + ks[None, :] * stride_ak)\n-  b = tl.load(B + ks[:, None] * stride_bk + ns[None, :] * stride_bn)\n-  c = tl.dot(a, b)\n-  c = kernel_utils.mul(c, c)\n-  tl.store(C + ms[:, None] * stride_cm + ns[None, :] * stride_cn, c)\n+  pid_m = tl.program_id(0)\n+  pid_n = tl.program_id(1)\n+\n+  offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n+  offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n+  offs_k = tl.arange(0, BLOCK_K)\n+  a_ptrs = A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+  b_ptrs = B + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+  accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n+  for k in range(0, tl.cdiv(K, BLOCK_K)):\n+      # Load the next block of A and B, generate a mask by checking the K dimension.\n+      # If it is out of bounds, set it to 0.\n+      a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_K, other=0.0)\n+      b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)\n+      # We accumulate along the K dimension.\n+      accumulator += tl.dot(a, b)\n+      # Advance the ptrs to the next K block.\n+      a_ptrs += BLOCK_K * stride_ak\n+      b_ptrs += BLOCK_K * stride_bk\n+\n+  c = kernel_utils.mul(accumulator, accumulator)\n+  # Write back the block of the output matrix C with masks.\n+  offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+  offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n+  c_ptrs = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+  tl.store(c_ptrs, c)\n \"\"\"\n \n-\n-def gen_test_bin(dir, M, N, K, BM, BN, BK):\n-    test_src = '''\n+test_utils_src = '''\n #include <cuda.h>\n #include <stdio.h>\n #include <stdint.h>\n@@ -78,10 +95,23 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n     fclose(file);\n }'''\n \n-    test_src += f'''\n+\n+def gen_kernel_library(dir, libname):\n+    c_files = glob.glob(os.path.join(dir, \"*.c\"))\n+    subprocess.run([\"gcc\"] + c_files + [\"-I\", cuda_include_dir(),\n+                                        \"-c\", \"-fPIC\"],\n+                   check=True, cwd=dir)\n+    o_files = glob.glob(os.path.join(dir, \"*.o\"))\n+    subprocess.run([\"gcc\"] + o_files + [\"-shared\",\n+                                        \"-o\", libname,\n+                                        \"-L\", libcuda_dirs()[0]],\n+                   check=True, cwd=dir)\n+\n+\n+def gen_test_bin(dir, M, N, K, exe=\"test\", algo_id=0):\n+    test_src = f'''\n int main(int argc, char **argv) {{\n   int M = {M}, N = {N}, K = {K};\n-  int BM = {M}, BN = {N}, BK = {K};\n \n   // initialize CUDA handles\n   CUdevice dev;\n@@ -96,7 +126,7 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n   cuMemAlloc(&B, K * N * 2);\n   cuMemAlloc(&C, M * N * 4);\n   cuStreamCreate(&stream, 0);\n-  load_matmul_fp16xfp16_16x16x16();\n+  load_matmul_fp16();\n \n   // initialize input data\n   int16_t hA[M*K];\n@@ -110,7 +140,13 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n \n   // launch kernel\n   cuStreamSynchronize(stream);\n-  CUresult ret = matmul_fp16xfp16_16x16x16(stream, M/BM, N/BN, 1, C, A, B, N, 1, K, 1, N, 1);\n+  CUresult ret;\n+  int algo_id = {algo_id};\n+  if (algo_id == 0) {{\n+    ret = matmul_fp16_default(stream, C, A, B, M, N, K, N, 1, K, 1, N, 1);\n+  }} else {{\n+    ret = matmul_fp16(stream, C, A, B, M, N, K, N, 1, K, 1, N, 1, {algo_id});\n+  }}\n   if (ret != 0) fprintf(stderr, \"kernel launch failed\\\\n\");\n   assert(ret == 0);\n \n@@ -123,41 +159,51 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n   write_buffer_to_csv(argv[3], hC, M*N);\n \n   // free cuda handles\n-  unload_matmul_fp16xfp16_16x16x16();\n+  unload_matmul_fp16();\n   cuMemFree(A);\n   cuMemFree(B);\n   cuMemFree(C);\n   cuCtxDestroy(ctx);\n }}\n '''\n-\n+    src = test_utils_src + test_src\n     with open(os.path.join(dir, \"test.c\"), \"w\") as file:\n-        file.write(test_src)\n-    c_files = glob.glob(os.path.join(dir, \"*.c\"))\n-    subprocess.run([\"gcc\"] + c_files + [\"-I\", cuda_include_dir(),\n-                                        \"-L\", libcuda_dirs()[0],\n-                                        \"-l\", \"cuda\",\n-                                        \"-o\", \"test\"], check=True, cwd=dir)\n+        file.write(src)\n+    subprocess.run([\"gcc\"] + [\"test.c\",\n+                              \"-I\", cuda_include_dir(),\n+                              \"-L\", libcuda_dirs()[0],\n+                              \"-l\", \"cuda\",\n+                              \"-L\", dir,\n+                              \"-l\", \"kernel\",\n+                              \"-o\", exe], check=True, cwd=dir)\n \n \n-def generate_matmul_launcher(dir, dtype, BM, BN, BK, ha_hb_hints):\n+def write_triton_kernels(dir, src, util_src):\n     kernel_path = os.path.join(dir, \"kernel.py\")\n     with open(kernel_path, \"w\") as file:\n-        file.write(kernel_src)\n+        file.write(src)\n \n     kernel_utils_path = os.path.join(dir, \"kernel_utils.py\")\n     with open(kernel_utils_path, \"w\") as file:\n-        file.write(kernel_utils_src)\n+        file.write(util_src)\n \n+    return kernel_path\n+\n+\n+def compile_aot_kernels(dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints):\n     compiler_path = os.path.join(triton.tools.__path__[0], \"compile.py\")\n-    linker_path = os.path.join(triton.tools.__path__[0], \"link.py\")\n \n     # compile all desired configs\n     for ha in ha_hb_hints:\n         for hb in ha_hb_hints:\n-            sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, i32:1, i32{hb}, i32:1, i32:16, i32:1, {BM}, {BN}, {BK}'\n-            name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n-            subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", kernel_path], check=True, cwd=dir)\n+            sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32, i32, i32, i32{ha}, i32:1, i32{hb}, i32:1, i32:16, i32:1, {BM}, {BN}, {BK}'\n+            name = f\"matmul_{dtype}\"\n+            grid = f'M/{BM}, N/{BN}, 1'\n+            subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", \"-g\", grid, kernel_path], check=True, cwd=dir)\n+\n+\n+def link_aot_kernels(dir):\n+    linker_path = os.path.join(triton.tools.__path__[0], \"link.py\")\n \n     # link all desired configs\n     h_files = glob.glob(os.path.join(dir, \"*.h\"))\n@@ -183,17 +229,22 @@ def test_compile_link_matmul():\n         dtype = \"fp16\"\n         BM, BN, BK = 16, 16, 16\n \n-        generate_matmul_launcher(tmp_dir, dtype, BM, BN, BK, ha_hb_hints=[\"\", \":16\"])\n+        kernel_path = write_triton_kernels(tmp_dir, kernel_src, kernel_utils_src)\n+        compile_aot_kernels(tmp_dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints=[\"\", \":16\"])\n+        link_aot_kernels(tmp_dir)\n \n         # compile test case\n         M, N, K = 16, 16, 16\n-        gen_test_bin(tmp_dir, M, N, K, BM, BN, BK)\n+        gen_kernel_library(tmp_dir, \"libkernel.so\")\n+        gen_test_bin(tmp_dir, M, N, K)\n \n         # initialize test data\n         a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n \n         # run test case\n-        subprocess.run([\"./test\", a_path, b_path, c_path], check=True, cwd=tmp_dir)\n+        env = os.environ.copy()\n+        env[\"LD_LIBRARY_PATH\"] = tmp_dir\n+        subprocess.run([\"./test\", a_path, b_path, c_path], env=env, check=True, cwd=tmp_dir)\n \n         # read data and compare against reference\n         c = np.genfromtxt(c_path, delimiter=\",\", dtype=np.int32)\n@@ -209,23 +260,73 @@ def test_launcher_has_no_available_kernel():\n         dtype = \"fp16\"\n         BM, BN, BK = 16, 16, 16\n \n-        generate_matmul_launcher(tmp_dir, dtype, BM, BN, BK, ha_hb_hints=[\":1\"])\n+        kernel_path = write_triton_kernels(tmp_dir, kernel_src, kernel_utils_src)\n+        compile_aot_kernels(tmp_dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints=[\":1\"])\n+        link_aot_kernels(tmp_dir)\n \n         # compile test case\n         M, N, K = 16, 16, 16\n-        gen_test_bin(tmp_dir, M, N, K, BM, BN, BK)\n+        gen_kernel_library(tmp_dir, \"libkernel.so\")\n+        gen_test_bin(tmp_dir, M, N, K)\n \n         # initialize test data\n         a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n \n         # run test case\n-        result = subprocess.run([\"./test\", a_path, b_path, c_path], cwd=tmp_dir, capture_output=True, text=True)\n+        env = os.environ.copy()\n+        env[\"LD_LIBRARY_PATH\"] = tmp_dir\n+        result = subprocess.run([\"./test\", a_path, b_path, c_path], env=env, cwd=tmp_dir, capture_output=True, text=True)\n \n         # It should fail since the launcher requires all the strides be 1 while they are not.\n         assert result.returncode == -6\n         assert \"kernel launch failed\" in result.stderr\n \n \n+def test_compile_link_autotune_matmul():\n+    np.random.seed(3)\n+\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+\n+        dtype = \"fp16\"\n+\n+        kernel_path = write_triton_kernels(tmp_dir, kernel_src, kernel_utils_src)\n+\n+        tile_sizes = [\n+            [16, 16, 16],\n+            [32, 32, 16],\n+            [32, 32, 32],\n+            [64, 64, 32],\n+        ]\n+\n+        for ts in tile_sizes:\n+            BM, BN, BK = ts[0], ts[1], ts[2]\n+            compile_aot_kernels(tmp_dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints=[\"\", \":16\"])\n+\n+        link_aot_kernels(tmp_dir)\n+\n+        gen_kernel_library(tmp_dir, \"libkernel.so\")\n+\n+        # compile test case\n+        M, N, K = 64, 64, 64\n+        # initialize test data\n+        a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n+        c_ref = np.matmul(a.astype(np.float32), b.astype(np.float32))\n+\n+        for algo_id in range(len(tile_sizes)):\n+            # generate and run test case\n+            test_name = f\"test_{algo_id}\"\n+            gen_test_bin(tmp_dir, M, N, K, exe=test_name, algo_id=algo_id)\n+\n+            env = os.environ.copy()\n+            env[\"LD_LIBRARY_PATH\"] = tmp_dir\n+            subprocess.run([f\"./{test_name}\", a_path, b_path, c_path], check=True, cwd=tmp_dir, env=env)\n+\n+            # read data and compare against reference\n+            c = np.genfromtxt(c_path, delimiter=\",\", dtype=np.int32)\n+            c_tri = c.reshape((M, N)).view(np.float32)\n+            np.testing.assert_allclose(c_tri, c_ref * c_ref, atol=1e-4, rtol=1e-4)\n+\n+\n def test_ttgir_to_ptx():\n     src = \"\"\"\n module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32, \"triton_gpu.num-ctas\" = 1 : i32} {"}, {"filename": "python/triton/common/build.py", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -18,7 +18,7 @@ def is_hip():\n \n @functools.lru_cache()\n def libcuda_dirs():\n-    libs = subprocess.check_output([\"ldconfig\", \"-p\"]).decode()\n+    libs = subprocess.check_output([\"/sbin/ldconfig\", \"-p\"]).decode()\n     # each line looks like the following:\n     # libcuda.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcuda.so.1\n     locs = [line.split()[-1] for line in libs.splitlines() if \"libcuda.so\" in line]\n@@ -27,6 +27,9 @@ def libcuda_dirs():\n     if locs:\n         msg += 'Possible files are located at %s.' % str(locs)\n         msg += 'Please create a symlink of libcuda.so to any of the file.'\n+    else:\n+        msg += 'Please make sure GPU is setup and then run \"/sbin/ldconfig\"'\n+        msg += ' (requires sudo) to refresh the linker cache.'\n     assert any(os.path.exists(os.path.join(path, 'libcuda.so')) for path in dirs), msg\n     return dirs\n "}, {"filename": "python/triton/compiler/__init__.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -1,4 +1,5 @@\n-from .compiler import CompiledKernel, compile, instance_descriptor\n+from .compiler import (CompiledKernel, compile, get_arch_default_num_stages,\n+                       get_arch_default_num_warps, instance_descriptor)\n from .errors import CompilationError\n \n-__all__ = [\"compile\", \"instance_descriptor\", \"CompiledKernel\", \"CompilationError\"]\n+__all__ = [\"compile\", \"instance_descriptor\", \"CompiledKernel\", \"CompilationError\", \"get_arch_default_num_warps\", \"get_arch_default_num_stages\"]"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 24, "deletions": 17, "changes": 41, "file_content_changes": "@@ -21,16 +21,8 @@ def mangle_ty(ty):\n         SIGNED = language.dtype.SIGNEDNESS.SIGNED\n         prefix = 'i' if ty.int_signedness == SIGNED else 'u'\n         return prefix + str(ty.int_bitwidth)\n-    if ty.is_fp8():\n-        return 'fp8'\n-    if ty.is_fp16():\n-        return 'fp16'\n-    if ty.is_bf16():\n-        return 'bf16'\n-    if ty.is_fp32():\n-        return 'fp32'\n-    if ty.is_fp64():\n-        return 'fp64'\n+    if ty.is_floating():\n+        return str(ty)\n     if ty.is_block():\n         elt = mangle_ty(ty.scalar)\n         shape = '_'.join(map(str, ty.shape))\n@@ -64,6 +56,10 @@ def _is_triton_scalar(o: Any) -> bool:\n     return _is_triton_tensor(o) and (not o.type.is_block() or o.type.numel == 1)\n \n \n+def _is_list_like(o: Any) -> bool:\n+    return isinstance(o, (list, tuple))\n+\n+\n def _unwrap_if_constexpr(o: Any):\n     return o.value if isinstance(o, constexpr) else o\n \n@@ -284,6 +280,9 @@ def _set_insertion_point_and_loc(self, ip, loc):\n     # AST visitor\n     #\n     def visit_compound_statement(self, stmts):\n+        # Ensure that stmts is iterable\n+        if not _is_list_like(stmts):\n+            stmts = [stmts]\n         for stmt in stmts:\n             ret_type = self.visit(stmt)\n             if ret_type is not None and isinstance(stmt, ast.Return):\n@@ -413,9 +412,9 @@ def visit_Assign(self, node):\n             raise UnsupportedLanguageConstruct(None, node, \"simultaneous multiple assignment is not supported.\")\n         names = _names[0]\n         values = self.visit(node.value)\n-        if not isinstance(names, tuple):\n+        if not _is_list_like(names):\n             names = [names]\n-        if not isinstance(values, tuple):\n+        if not _is_list_like(values):\n             values = [values]\n         native_nontensor_types = (language.dtype, )\n         for name, value in zip(names, values):\n@@ -619,11 +618,19 @@ def visit_If(self, node):\n     def visit_IfExp(self, node):\n         cond = self.visit(node.test)\n         if _is_triton_tensor(cond):\n-            cond = cond.to(language.int1, _builder=self.builder)\n-        if _unwrap_if_constexpr(cond):\n-            return self.visit(node.body)\n+            raise UnsupportedLanguageConstruct(\n+                None, node,\n+                \"Triton does not support `if` expressions (ternary operators) with dynamic conditions, use `if` statements instead\")\n         else:\n-            return self.visit(node.orelse)\n+            cond = _unwrap_if_constexpr(cond)\n+            if type(cond) not in _condition_types:  # not isinstance - we insist the real thing, no subclasses and no ducks\n+                raise UnsupportedLanguageConstruct(\n+                    None, node, \"`if` conditionals can only accept values of type {{{}}}, not objects of type {}\".format(\n+                        ', '.join(_.__name__ for _ in _condition_types), type(cond).__name__))\n+            if cond:\n+                return self.visit(node.body)\n+            else:\n+                return self.visit(node.orelse)\n \n     def visit_Pass(self, node):\n         pass\n@@ -1062,7 +1069,7 @@ def str_to_ty(name):\n         ty = str_to_ty(name[1:])\n         return language.pointer_type(ty)\n     tys = {\n-        \"fp8e4\": language.float8e4,\n+        \"fp8e4nv\": language.float8e4nv,\n         \"fp8e5\": language.float8e5,\n         \"fp8e4b15\": language.float8e4b15,\n         \"fp8e4b15x4\": language.float8e4b15x4,"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 40, "deletions": 6, "changes": 46, "file_content_changes": "@@ -12,7 +12,8 @@\n \n from .._C.libtriton.triton import (ClusterInfo, TMAInfos, add_external_libs,\n                                    compile_ptx_to_cubin, get_env_vars, get_num_warps,\n-                                   get_shared_memory_size, ir, translate_llvmir_to_ptx,\n+                                   get_shared_memory_size, ir, runtime,\n+                                   translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas\n from ..common.build import is_hip\n@@ -141,9 +142,9 @@ def ttgir_to_llir(mod, extern_libs, arch, tma_infos):\n         _add_external_libs(mod, extern_libs)\n     # TODO: separate tritongpu_to_llvmir for different backends\n     if _is_cuda(arch):\n-        return translate_triton_gpu_to_llvmir(mod, arch, tma_infos, False)\n+        return translate_triton_gpu_to_llvmir(mod, arch, tma_infos, runtime.TARGET.NVVM)\n     else:\n-        return translate_triton_gpu_to_llvmir(mod, 0, True)\n+        return translate_triton_gpu_to_llvmir(mod, 0, TMAInfos(), runtime.TARGET.ROCDL)\n \n \n # PTX translation\n@@ -296,6 +297,32 @@ def get_architecture_descriptor(capability):\n     return capability\n \n \n+def get_arch_default_num_warps(device_type):\n+    if device_type in [\"cuda\", \"hip\"]:\n+        num_warps = 4\n+    else:\n+        _device_backend = get_backend(device_type)\n+        assert _device_backend\n+        arch = _device_backend.get_architecture_descriptor()\n+        num_warps = arch[\"num_warps\"]\n+\n+    return num_warps\n+\n+\n+def get_arch_default_num_stages(device_type, capability=None):\n+    if device_type in [\"cuda\", \"hip\"]:\n+        arch = get_architecture_descriptor(capability)\n+        is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n+        num_stages = 3 if is_cuda and arch >= 75 else 2\n+    else:\n+        _device_backend = get_backend(device_type)\n+        assert _device_backend\n+        arch = _device_backend.get_architecture_descriptor()\n+        num_stages = arch[\"num_stages\"]\n+\n+    return num_stages\n+\n+\n def add_cuda_stages(arch, extern_libs, stages):\n \n     stages[\"ptx\"] = (lambda path: Path(path).read_text(),\n@@ -307,12 +334,14 @@ def add_cuda_stages(arch, extern_libs, stages):\n def compile(fn, **kwargs):\n     # Get device type to decide which backend should be used\n     device_type = kwargs.get(\"device_type\", \"cuda\")\n+    capability = kwargs.get(\"cc\", None)\n+\n     if is_hip():\n         device_type = \"hip\"\n \n     if device_type == \"cuda\":\n         _device_backend = get_backend(device_type)\n-        arch = get_architecture_descriptor(kwargs.get(\"cc\", None))\n+        arch = get_architecture_descriptor(capability)\n     else:\n         _device_backend = get_backend(device_type)\n         assert _device_backend\n@@ -323,9 +352,10 @@ def compile(fn, **kwargs):\n         is_cuda = False\n     context = ir.context()\n     constants = kwargs.get(\"constants\", dict())\n-    num_warps = kwargs.get(\"num_warps\", 4)\n+    num_warps = kwargs.get(\"num_warps\", get_arch_default_num_warps(device_type))\n+    assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n     num_ctas = kwargs.get(\"num_ctas\", 1)\n-    num_stages = kwargs.get(\"num_stages\", 3 if is_cuda and arch >= 75 else 2)\n+    num_stages = kwargs.get(\"num_stages\", get_arch_default_num_stages(device_type, capability=capability))\n     # TODO[shuhaoj]: Default should be to enable warp specialization once possible\n     enable_warp_specialization = kwargs.get(\"enable_warp_specialization\", False)\n     # TODO[shuhaoj]: persistent can be decoupled with warp specialization\n@@ -365,6 +395,10 @@ def compile(fn, **kwargs):\n                           lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n         _device_backend.add_stages(arch, extern_libs, stages)\n     else:\n+        # pass the user's configuration to the backend device.\n+        arch[\"num_warps\"] = num_warps\n+        arch[\"num_stages\"] = num_stages\n+        arch[\"num_ctas\"] = num_ctas\n         _device_backend.add_stages(arch, extern_libs, stages)\n \n     # find out the signature of the function"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -219,6 +219,7 @@ def format_of(ty):\n     return ptr_info;\n   }}\n   PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n+  ptr_info.valid = false;\n   return ptr_info;\n }}\n \n@@ -240,22 +241,21 @@ def format_of(ty):\n     return NULL;\n   }}\n \n-  if (launch_enter_hook != Py_None) {{\n-    PyObject_CallObject(launch_enter_hook, args);\n+  if (launch_enter_hook != Py_None && !PyObject_CallObject(launch_enter_hook, args)) {{\n+    return NULL;\n   }}\n \n \n   // raise exception asap\n   {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n+  Py_BEGIN_ALLOW_THREADS;\n   _launch(gridX, gridY, gridZ, num_warps, num_ctas, clusterDimX, clusterDimY, clusterDimZ, shared_memory, (CUstream)_stream, (CUfunction)_function{', ' + ', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items()) if len(signature) > 0 else ''});\n+  Py_END_ALLOW_THREADS;\n \n-  if (launch_exit_hook != Py_None) {{\n-    PyObject_CallObject(launch_exit_hook, args);\n-  }}\n-\n-  if(PyErr_Occurred()) {{\n+  if (launch_exit_hook != Py_None && !PyObject_CallObject(launch_exit_hook, args)) {{\n     return NULL;\n   }}\n+\n   // return None\n   Py_INCREF(Py_None);\n   return Py_None;"}, {"filename": "python/triton/hopper_lib/libhopper_helpers.bc", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 14, "deletions": 12, "changes": 26, "file_content_changes": "@@ -4,11 +4,21 @@\n from . import math\n from . import extra\n from .standard import (\n+    argmax,\n+    argmin,\n     cdiv,\n+    cumprod,\n+    cumsum,\n+    max,\n+    maximum,\n+    min,\n+    minimum,\n     sigmoid,\n     softmax,\n+    sum,\n     ravel,\n     swizzle2d,\n+    xor_sum,\n     zeros,\n     zeros_like,\n )\n@@ -17,8 +27,6 @@\n     abs,\n     advance,\n     arange,\n-    argmin,\n-    argmax,\n     associative_scan,\n     atomic_add,\n     atomic_and,\n@@ -35,8 +43,6 @@\n     cat,\n     constexpr,\n     cos,\n-    cumprod,\n-    cumsum,\n     debug_barrier,\n     device_assert,\n     device_print,\n@@ -51,9 +57,10 @@\n     float64,\n     float8e4b15,\n     float8e4b15x4,\n-    float8e4,\n+    float8e4nv,\n     float8e5,\n     function_type,\n+    inline_asm_elementwise,\n     int1,\n     int16,\n     int32,\n@@ -62,12 +69,8 @@\n     load,\n     log,\n     make_block_ptr,\n-    max,\n     max_constancy,\n     max_contiguous,\n-    maximum,\n-    min,\n-    minimum,\n     multiple_of,\n     num_programs,\n     pi32_t,\n@@ -80,7 +83,6 @@\n     static_assert,\n     static_print,\n     store,\n-    sum,\n     static_range,\n     tensor,\n     trans,\n@@ -93,7 +95,6 @@\n     view,\n     void,\n     where,\n-    xor_sum,\n )\n from .random import (\n     pair_uniform_to_normal,\n@@ -150,10 +151,11 @@\n     \"float64\",\n     \"float8e4b15\",\n     \"float8e4b15x4\",\n-    \"float8e4\",\n+    \"float8e4nv\",\n     \"float8e5\",\n     \"full\",\n     \"function_type\",\n+    \"inline_asm_elementwise\",\n     \"int1\",\n     \"int16\",\n     \"int32\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 56, "deletions": 203, "changes": 259, "file_content_changes": "@@ -6,8 +6,7 @@\n from typing import Callable, List, Sequence, TypeVar\n \n from .._C.libtriton.triton import ir\n-from ..runtime.jit import jit\n-from . import math, semantic\n+from . import semantic\n \n T = TypeVar('T')\n \n@@ -76,7 +75,7 @@ def _to_tensor(x, builder):\n class dtype:\n     SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n+    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4nv', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -104,7 +103,7 @@ def __init__(self, name):\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 15\n-            elif name == 'fp8e4':\n+            elif name == 'fp8e4nv':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 7\n@@ -136,15 +135,18 @@ def __init__(self, name):\n     def is_fp8(self):\n         return 'fp8' in self.name\n \n-    def is_fp8e4(self):\n-        return self.name == 'fp8e4'\n+    def is_fp8e4nv(self):\n+        return self.name == 'fp8e4nv'\n \n     def is_fp8e4b15(self):\n         return self.name == 'fp8e4b15'\n \n     def is_fp8e4b15x4(self):\n         return self.name == 'fp8e4b15x4'\n \n+    def is_fp8e5(self):\n+        return self.name == 'fp8e5'\n+\n     def is_fp16(self):\n         return self.name == 'fp16'\n \n@@ -202,6 +204,10 @@ def is_int(self):\n     def is_bool(self):\n         return self.is_int1()\n \n+    @staticmethod\n+    def is_dtype(type_str):\n+        return type_str in dtype.SINT_TYPES + dtype.UINT_TYPES + dtype.FP_TYPES + dtype.OTHER_TYPES\n+\n     @staticmethod\n     def is_void():\n         raise RuntimeError(\"Not implemented\")\n@@ -244,8 +250,8 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_int64_ty()\n         elif self.name == 'fp8e5':\n             return builder.get_fp8e5_ty()\n-        elif self.name == 'fp8e4':\n-            return builder.get_fp8e4_ty()\n+        elif self.name == 'fp8e4nv':\n+            return builder.get_fp8e4nv_ty()\n         elif self.name == 'fp8e4b15':\n             return builder.get_fp8e4b15_ty()\n         elif self.name == 'fp8e4b15x4':\n@@ -382,7 +388,7 @@ def to_ir(self, builder: ir.builder):\n uint32 = dtype('uint32')\n uint64 = dtype('uint64')\n float8e5 = dtype('fp8e5')\n-float8e4 = dtype('fp8e4')\n+float8e4nv = dtype('fp8e4nv')\n float8e4b15 = dtype('fp8e4b15')\n float8e4b15x4 = dtype('fp8e4b15x4')\n float16 = dtype('fp16')\n@@ -1351,11 +1357,6 @@ def make_combine_region(reduce_op):\n @builtin\n def _promote_reduction_input(t, _builder=None):\n     scalar_ty = t.type.scalar\n-    # input is extended to 32-bits if necessary\n-    # this increases numerical accuracy and can be done pretty much for free\n-    # on GPUs\n-    if scalar_ty.is_int() and scalar_ty.int_bitwidth < 32:\n-        return t.to(int32, _builder=_builder)\n \n     # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n     if scalar_ty is bfloat16:\n@@ -1382,170 +1383,6 @@ def _reduce_with_indices(input, axis, combine_fn, _builder=None, _generator=None\n     return rvalue, rindices\n \n \n-@jit\n-def minimum(x, y):\n-    \"\"\"\n-    Computes the element-wise minimum of :code:`x` and :code:`y`.\n-\n-    :param input: the first input tensor\n-    :type input: Block\n-    :param other: the second input tensor\n-    :type other: Block\n-    \"\"\"\n-    return math.min(x, y)\n-\n-\n-@jit\n-def maximum(x, y):\n-    \"\"\"\n-    Computes the element-wise maximum of :code:`x` and :code:`y`.\n-\n-    :param input: the first input tensor\n-    :type input: Block\n-    :param other: the second input tensor\n-    :type other: Block\n-    \"\"\"\n-    return math.max(x, y)\n-\n-# max and argmax\n-\n-\n-@jit\n-def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n-    if tie_break_left:\n-        tie = value1 == value2 and index1 < index2\n-    else:\n-        tie = False\n-    gt = value1 > value2 or tie\n-    v_ret = where(gt, value1, value2)\n-    i_ret = where(gt, index1, index2)\n-    return v_ret, i_ret\n-\n-\n-@jit\n-def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n-    return _argmax_combine(value1, index1, value2, index2, True)\n-\n-\n-@jit\n-def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n-    return _argmax_combine(value1, index1, value2, index2, False)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"maximum\",\n-                       return_indices_arg=\"return_indices\",\n-                       tie_break_arg=\"return_indices_tie_break_left\")\n-def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n-    input = _promote_reduction_input(input)\n-    if return_indices:\n-        if return_indices_tie_break_left:\n-            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n-        else:\n-            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n-    else:\n-        if constexpr(input.dtype.primitive_bitwidth) < 32:\n-            if constexpr(input.dtype.is_floating()):\n-                input = input.to(float32)\n-            else:\n-                assert input.dtype.is_integer_type()\n-                input = input.to(int32)\n-        return reduce(input, axis, maximum)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n-def argmax(input, axis, tie_break_left=True):\n-    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n-    return ret\n-\n-# min and argmin\n-\n-\n-@jit\n-def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n-    if tie_break_left:\n-        tie = value1 == value2 and index1 < index2\n-    else:\n-        tie = False\n-    lt = value1 < value2 or tie\n-    value_ret = where(lt, value1, value2)\n-    index_ret = where(lt, index1, index2)\n-    return value_ret, index_ret\n-\n-\n-@jit\n-def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n-    return _argmin_combine(value1, index1, value2, index2, True)\n-\n-\n-@jit\n-def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n-    return _argmin_combine(value1, index1, value2, index2, False)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"minimum\",\n-                       return_indices_arg=\"return_indices\",\n-                       tie_break_arg=\"return_indices_tie_break_left\")\n-def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n-    input = _promote_reduction_input(input)\n-    if return_indices:\n-        if return_indices_tie_break_left:\n-            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n-        else:\n-            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n-    else:\n-        if constexpr(input.dtype.primitive_bitwidth) < 32:\n-            if constexpr(input.dtype.is_floating()):\n-                input = input.to(float32)\n-            else:\n-                assert input.dtype.is_integer_type()\n-                input = input.to(int32)\n-        return reduce(input, axis, minimum)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"minimum index\",\n-                       tie_break_arg=\"tie_break_left\")\n-def argmin(input, axis, tie_break_left=True):\n-    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n-    return ret\n-\n-\n-@jit\n-def _sum_combine(a, b):\n-    return a + b\n-\n-# sum\n-\n-\n-@jit\n-@_add_reduction_docstr(\"sum\")\n-def sum(input, axis=None):\n-    input = _promote_reduction_input(input)\n-    return reduce(input, axis, _sum_combine)\n-\n-\n-@jit\n-def _xor_combine(a, b):\n-    return a ^ b\n-\n-\n-# xor sum\n-\n-@builtin\n-@_add_reduction_docstr(\"xor sum\")\n-def xor_sum(input, axis=None, _builder=None, _generator=None):\n-    scalar_ty = input.type.scalar\n-    if not scalar_ty.is_int():\n-        raise ValueError(\"xor_sum only supported for integers\")\n-\n-    input = _promote_reduction_input(input, _builder=_builder)\n-    return reduce(input, axis, _xor_combine,\n-                  _builder=_builder, _generator=_generator)\n-\n-\n # -----------------------\n # Scans\n # -----------------------\n@@ -1596,31 +1433,6 @@ def make_combine_region(scan_op):\n     axis = _constexpr_to_value(axis)\n     return semantic.associative_scan(input, axis, make_combine_region, _builder)\n \n-# cumsum\n-\n-\n-@jit\n-@_add_scan_docstr(\"cumsum\")\n-def cumsum(input, axis=0):\n-    # todo rename this to a generic function name\n-    input = _promote_reduction_input(input)\n-    return associative_scan(input, axis, _sum_combine)\n-\n-# cumprod\n-\n-\n-@jit\n-def _prod_combine(a, b):\n-    return a * b\n-\n-\n-@jit\n-@_add_scan_docstr(\"cumprod\")\n-def cumprod(input, axis=0):\n-    # todo rename this to a generic function name\n-    input = _promote_reduction_input(input)\n-    return associative_scan(input, axis, _prod_combine)\n-\n # -----------------------\n # Compiler Hint Ops\n # -----------------------\n@@ -1793,6 +1605,47 @@ def device_assert(cond, msg=\"\", _builder=None):\n     return semantic.device_assert(_to_tensor(cond, _builder), msg, file_name, func_name, lineno, _builder)\n \n \n+@builtin\n+def inline_asm_elementwise(asm: str, constraints: str, args: list, dtype, is_pure: bool, pack: int, _builder=None):\n+    '''\n+        Execute the inline assembly to a packed of elements of the tensor\n+        :param asm: assembly to be inlined, it has to match the target assembly format\n+        :param constraints: string representing the mapping of operands to register\n+        :param args: the arguments of the operation\n+        :param dtype: the element type of the returned variable\n+        :param is_pure: whether the operation is pure\n+        :param pack: the number of elements to be processed by one instance of inline assembly\n+        :param _builder: the builder\n+        :return: the return value of the function\n+    '''\n+    dispatch_args = args.copy()\n+    asm = _constexpr_to_value(asm)\n+    constraints = _constexpr_to_value(constraints)\n+    pack = _constexpr_to_value(pack)\n+    is_pure = _constexpr_to_value(is_pure)\n+    ret_shape = None\n+    arg_types = []\n+    res_ty = dtype\n+    for i in range(len(dispatch_args)):\n+        dispatch_args[i] = _to_tensor(dispatch_args[i], _builder)\n+        arg_types.append(dispatch_args[i].dtype)\n+    if len(arg_types) > 0:\n+        arg_types = tuple(arg_types)\n+        broadcast_arg = dispatch_args[0]\n+        # Get the broadcast shape over all the arguments\n+        for i, item in enumerate(dispatch_args):\n+            _, broadcast_arg = semantic.binary_op_type_checking_impl(\n+                item, broadcast_arg, _builder, arithmetic_check=False)\n+        # Change the shape of each argument based on the broadcast shape\n+        for i in range(len(dispatch_args)):\n+            dispatch_args[i], _ = semantic.binary_op_type_checking_impl(\n+                dispatch_args[i], broadcast_arg, _builder, arithmetic_check=False)\n+        ret_shape = broadcast_arg.shape\n+        res_ty = block_type(dtype, ret_shape)\n+    call = _builder.create_inline_asm(asm, constraints, [t.handle for t in args], res_ty.to_ir(_builder), is_pure, pack)\n+    return tensor(call, res_ty)\n+\n+\n # -----------------------\n # Iterators\n # -----------------------"}, {"filename": "python/triton/language/extra/cuda.bc", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/extra/cuda.py", "status": "modified", "additions": 6, "deletions": 10, "changes": 16, "file_content_changes": "@@ -1,19 +1,15 @@\n-import os\n-\n from .. import core\n \n-__path__ = os.path.dirname(os.path.abspath(__file__))\n-\n \n @core.extern\n def globaltimer(_builder=None):\n-    return core.extern_elementwise(\"cuda\", os.path.join(__path__, \"cuda.bc\"), [],\n-                                   {tuple(): (\"globaltimer\", core.dtype(\"int64\")),\n-                                    }, is_pure=False, _builder=_builder)\n+    return core.inline_asm_elementwise(\"mov.u64 $0, %globaltimer;\", \"=l\", [],\n+                                       dtype=core.int64, is_pure=False,\n+                                       pack=1, _builder=_builder)\n \n \n @core.extern\n def smid(_builder=None):\n-    return core.extern_elementwise(\"cuda\", os.path.join(__path__, \"cuda.bc\"), [],\n-                                   {tuple(): (\"smid\", core.dtype(\"int32\")),\n-                                    }, is_pure=True, _builder=_builder)\n+    return core.inline_asm_elementwise(\"mov.u32 $0, %smid;\", \"=r\", [],\n+                                       dtype=core.int32, is_pure=True,\n+                                       pack=1, _builder=_builder)"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -1,5 +1,6 @@\n from ..runtime.jit import jit\n from . import core as tl\n+from . import standard\n \n PHILOX_KEY_A: tl.constexpr = 0x9E3779B9\n PHILOX_KEY_B: tl.constexpr = 0xBB67AE85\n@@ -141,7 +142,7 @@ def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n @jit\n def pair_uniform_to_normal(u1, u2):\n     \"\"\"Box-Muller transform\"\"\"\n-    u1 = tl.maximum(1.0e-7, u1)\n+    u1 = standard.maximum(1.0e-7, u1)\n     th = 6.283185307179586 * u2\n     r = tl.sqrt(-2.0 * tl.log(u1))\n     return r * tl.cos(th), r * tl.sin(th)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 28, "deletions": 5, "changes": 33, "file_content_changes": "@@ -1,6 +1,5 @@\n from __future__ import annotations  # remove after python 3.11\n \n-import warnings\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n@@ -686,9 +685,8 @@ def cast(input: tl.tensor,\n     dst_sca_ty = dst_ty.scalar\n \n     if _is_cuda(builder.arch) and builder.arch < 89 and \\\n-       (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n-        warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n-                      \"Please use tl.float8e4b15.\", DeprecationWarning)\n+       (src_sca_ty.is_fp8e4nv() or dst_sca_ty.is_fp8e4nv()):\n+        assert False, \"fp8e4nv data type is not supported on CUDA arch < 89\"\n \n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n     if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n@@ -1260,8 +1258,33 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n+    def assert_dtypes_valid(lhs_dtype, rhs_dtype, arch):\n+        # Checks for non-cuda archs\n+        if not _is_cuda(builder.arch):\n+            assert lhs_dtype == rhs_dtype, f\"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!\"\n+            return\n+        # Checks for cuda arch\n+        if arch < 90:\n+            assert not lhs_dtype.is_fp8e4nv() and not rhs_dtype.is_fp8e4nv(), \"Dot op does not support fp8e4nv on CUDA arch < 90\"\n+            assert lhs_dtype == rhs_dtype, f\"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!\"\n+        else:\n+            assert not lhs_dtype.is_fp8e4b15() and not rhs_dtype.is_fp8e4b15(), \"Dot op does not support fp8e4b15 on CUDA arch >= 90\"\n+            assert not lhs_dtype.is_fp8e4b15x4() and not rhs_dtype.is_fp8e4b15x4(), \"Dot op does not support fp8e4b15x4 on CUDA arch >= 90\"\n+            if lhs_dtype.is_int() or rhs_dtype.is_int():\n+                assert lhs_dtype == rhs_dtype, f\"Both operands must be same type. First operand ({lhs_dtype}) and second operand ({rhs_dtype})\"\n+                assert lhs_dtype.is_int8() or lhs_dtype.is_uint8(), f\"Both operands must be either int8 or uint8. Operand type ({lhs_dtype})\"\n+            elif lhs_dtype.is_fp8() or rhs_dtype.is_fp8():\n+                assert lhs_dtype.is_fp8e4nv() or lhs_dtype.is_fp8e5(), f\"Only supports fp8e4nv or fp8e5. First operand ({lhs_dtype})\"\n+                assert rhs_dtype.is_fp8e4nv() or rhs_dtype.is_fp8e5(), f\"Only supports fp8e4nv or fp8e5. Second operand ({rhs_dtype})\"\n+            else:\n+                assert lhs_dtype.is_fp16() or lhs_dtype.is_bf16() or lhs_dtype.is_fp32() or lhs_dtype.is_int1(), f\"Unsupported dtype {lhs_dtype}\"\n+                assert rhs_dtype.is_fp16() or rhs_dtype.is_bf16() or rhs_dtype.is_fp32() or rhs_dtype.is_int1(), f\"Unsupported dtype {rhs_dtype}\"\n+                assert lhs_dtype == rhs_dtype, f\"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!\"\n+\n     assert lhs.type.is_block() and rhs.type.is_block()\n-    assert lhs.dtype == rhs.dtype, f\"First input ({lhs.dtype}) and second input ({rhs.dtype}) must have the same dtype!\"\n+\n+    assert_dtypes_valid(lhs.dtype, rhs.dtype, builder.arch)\n+\n     assert len(lhs.shape) == 2, f\"First input shape ({lhs.shape}) is not two dimensional!\"\n     assert len(rhs.shape) == 2, f\"Second input shape ({rhs.shape}) is not two dimensional!\"\n     assert lhs.shape[1].value == rhs.shape[0].value, f\"First input shape ({lhs.shape}) and second input shape {rhs.shape} are not compatible for matmul (second index of first shape ({lhs.shape[1].value}) must be equal to first index of second shape ({rhs.shape[0].value})\""}, {"filename": "python/triton/language/standard.py", "status": "modified", "additions": 194, "deletions": 5, "changes": 199, "file_content_changes": "@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from ..runtime.jit import jit\n-from . import core\n+from . import core, math\n \n # -----------------------\n # Standard library\n@@ -14,7 +14,7 @@ def cdiv(x, div):\n     Computes the ceiling division of :code:`x` by :code:`div`\n \n     :param x: the input number\n-    :type input: Block\n+    :type x: Block\n     :param div: the divisor\n     :param div: Block\n     \"\"\"\n@@ -30,9 +30,9 @@ def sigmoid(x):\n @jit\n @core._add_math_1arg_docstr(\"softmax\")\n def softmax(x, ieee_rounding=False):\n-    z = x - core.max(x, 0)\n+    z = x - max(x, 0)\n     num = core.exp(z)\n-    den = core.sum(num, 0)\n+    den = sum(num, 0)\n     return core.fdiv(num, den, ieee_rounding)\n \n \n@@ -73,7 +73,7 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n     # row-index of the first element of this group\n     off_i = group_id * size_g\n     # last group may have fewer rows\n-    size_g = core.minimum(size_i - off_i, size_g)\n+    size_g = minimum(size_i - off_i, size_g)\n     # new row and column indices\n     new_i = off_i + (ij % size_g)\n     new_j = (ij % size_gj) // size_g\n@@ -96,3 +96,192 @@ def zeros(shape, dtype):\n @jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)\n+\n+\n+@jit\n+def minimum(x, y):\n+    \"\"\"\n+    Computes the element-wise minimum of :code:`x` and :code:`y`.\n+\n+    :param input: the first input tensor\n+    :type input: Block\n+    :param other: the second input tensor\n+    :type other: Block\n+    \"\"\"\n+    return math.min(x, y)\n+\n+\n+@jit\n+def maximum(x, y):\n+    \"\"\"\n+    Computes the element-wise maximum of :code:`x` and :code:`y`.\n+\n+    :param input: the first input tensor\n+    :type input: Block\n+    :param other: the second input tensor\n+    :type other: Block\n+    \"\"\"\n+    return math.max(x, y)\n+\n+# max and argmax\n+\n+\n+@jit\n+def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    gt = value1 > value2 or tie\n+    v_ret = core.where(gt, value1, value2)\n+    i_ret = core.where(gt, index1, index2)\n+    return v_ret, i_ret\n+\n+\n+@jit\n+def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"maximum\",\n+                            return_indices_arg=\"return_indices\",\n+                            tie_break_arg=\"return_indices_tie_break_left\")\n+def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n+    input = core._promote_reduction_input(input)\n+    if return_indices:\n+        if return_indices_tie_break_left:\n+            return core._reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n+        else:\n+            return core._reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n+    else:\n+        if core.constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if core.constexpr(input.dtype.is_floating()):\n+                input = input.to(core.float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(core.int32)\n+        return core.reduce(input, axis, maximum)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n+def argmax(input, axis, tie_break_left=True):\n+    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n+    return ret\n+\n+# min and argmin\n+\n+\n+@jit\n+def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    lt = value1 < value2 or tie\n+    value_ret = core.where(lt, value1, value2)\n+    index_ret = core.where(lt, index1, index2)\n+    return value_ret, index_ret\n+\n+\n+@jit\n+def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"minimum\",\n+                            return_indices_arg=\"return_indices\",\n+                            tie_break_arg=\"return_indices_tie_break_left\")\n+def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n+    input = core._promote_reduction_input(input)\n+    if return_indices:\n+        if return_indices_tie_break_left:\n+            return core._reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n+        else:\n+            return core._reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n+    else:\n+        if core.constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if core.constexpr(input.dtype.is_floating()):\n+                input = input.to(core.float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(core.int32)\n+        return core.reduce(input, axis, minimum)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"minimum index\",\n+                            tie_break_arg=\"tie_break_left\")\n+def argmin(input, axis, tie_break_left=True):\n+    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n+    return ret\n+\n+\n+@jit\n+def _sum_combine(a, b):\n+    return a + b\n+\n+# sum\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"sum\")\n+def sum(input, axis=None):\n+    input = core._promote_reduction_input(input)\n+    return core.reduce(input, axis, _sum_combine)\n+\n+\n+@jit\n+def _xor_combine(a, b):\n+    return a ^ b\n+\n+# xor sum\n+\n+\n+@core.builtin\n+@core._add_reduction_docstr(\"xor sum\")\n+def xor_sum(input, axis=None, _builder=None, _generator=None):\n+    scalar_ty = input.type.scalar\n+    if not scalar_ty.is_int():\n+        raise ValueError(\"xor_sum only supported for integers\")\n+\n+    input = core._promote_reduction_input(input, _builder=_builder)\n+    return core.reduce(input, axis, _xor_combine,\n+                       _builder=_builder, _generator=_generator)\n+\n+# cumsum\n+\n+\n+@jit\n+@core._add_scan_docstr(\"cumsum\")\n+def cumsum(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = core._promote_reduction_input(input)\n+    return core.associative_scan(input, axis, _sum_combine)\n+\n+# cumprod\n+\n+\n+@jit\n+def _prod_combine(a, b):\n+    return a * b\n+\n+\n+@jit\n+@core._add_scan_docstr(\"cumprod\")\n+def cumprod(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = core._promote_reduction_input(input)\n+    return core.associative_scan(input, axis, _prod_combine)"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 14, "deletions": 9, "changes": 23, "file_content_changes": "@@ -81,8 +81,9 @@ def _kernel(A, B, C, M, N, K,\n             stride_bk, stride_bn,\n             stride_cm, stride_cn,\n             dot_out_dtype: tl.constexpr,\n+            allow_tf32: tl.constexpr,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n-            GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n+            GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr, AB_DTYPE: tl.constexpr\n             ):\n     # matrix multiplication\n     pid = tl.program_id(0)\n@@ -114,10 +115,10 @@ def _kernel(A, B, C, M, N, K,\n             _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n             a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n             b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n-        if a.dtype != b.dtype:\n+        if AB_DTYPE:\n             a = a.to(C.dtype.element_ty)\n             b = b.to(C.dtype.element_ty)\n-        acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n+        acc += tl.dot(a, b, out_dtype=dot_out_dtype, allow_tf32=allow_tf32)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n     acc = acc.to(C.dtype.element_ty)\n@@ -139,7 +140,7 @@ class _matmul(torch.autograd.Function):\n     _locks = {}\n \n     @staticmethod\n-    def _call(a, b, dot_out_dtype):\n+    def _call(a, b, dot_out_dtype, allow_tf32):\n         device = a.device\n         # handle non-contiguous inputs if necessary\n         if a.stride(0) > 1 and a.stride(1) > 1:\n@@ -151,8 +152,8 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        if a.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5] or\\\n-           b.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+        if a.dtype in [tl.float8e4nv, tl.float8e4b15, tl.float8e5] or\\\n+           b.dtype in [tl.float8e4nv, tl.float8e4b15, tl.float8e5]:\n             c_dtype = torch.float16\n         else:\n             c_dtype = get_higher_dtype(a.dtype, b.dtype)\n@@ -170,19 +171,23 @@ def _call(a, b, dot_out_dtype):\n                 dot_out_dtype = tl.float32\n             else:\n                 dot_out_dtype = tl.int32\n+        ab_dtype = True\n+        if a.dtype in [tl.float8e4nv, tl.float8e5] and b.dtype in [tl.float8e4nv, tl.float8e5]:\n+            ab_dtype = False\n         # launch kernel\n         grid = lambda META: (cdiv(M, META['BLOCK_M']) * cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n         _kernel[grid](a, b, c, M, N, K,\n                       a.stride(0), a.stride(1),\n                       b.stride(0), b.stride(1),\n                       c.stride(0), c.stride(1),\n                       dot_out_dtype=dot_out_dtype,\n-                      GROUP_M=8)\n+                      allow_tf32=allow_tf32,\n+                      GROUP_M=8, AB_DTYPE=ab_dtype)\n         return c\n \n     @staticmethod\n-    def forward(ctx, a, b, dot_out_dtype=None):\n-        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype)\n+    def forward(ctx, a, b, dot_out_dtype=None, allow_tf32=True):\n+        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype, allow_tf32=allow_tf32)\n \n \n matmul = _matmul.apply"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 12, "deletions": 3, "changes": 15, "file_content_changes": "@@ -17,9 +17,7 @@ static inline void gpuAssert(CUresult code, const char *file, int line) {\n \n #define CUDA_CHECK(ans)                                                        \\\n   {                                                                            \\\n-    gpuAssert((ans), __FILE__, __LINE__);                                      \\\n-    if (PyErr_Occurred())                                                      \\\n-      return NULL;                                                             \\\n+    { gpuAssert((ans), __FILE__, __LINE__); }                                  \\\n   }\n \n #define ADD_ENUM_ITEM(value)                                                   \\\n@@ -234,6 +232,8 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n   int32_t n_spills = 0;\n   // create driver handles\n   CUcontext pctx = 0;\n+\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuCtxGetCurrent(&pctx));\n   if (!pctx) {\n     CUDA_CHECK(cuDevicePrimaryCtxRetain(&pctx, device));\n@@ -264,6 +264,7 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n         cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n                            shared_optin - shared_static));\n   }\n+  Py_END_ALLOW_THREADS;\n \n   if (PyErr_Occurred()) {\n     return NULL;\n@@ -281,7 +282,9 @@ static PyObject *memAlloc(PyObject *self, PyObject *args) {\n     return NULL; // Error parsing arguments\n   }\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemAlloc(&dptr, bytesize));\n+  Py_END_ALLOW_THREADS;\n \n   return PyLong_FromUnsignedLongLong((unsigned long long)dptr);\n }\n@@ -300,7 +303,9 @@ static PyObject *memcpyHtoD(PyObject *self, PyObject *args) {\n   dstDevice = (CUdeviceptr)dstDevicePtr;\n   srcHost = (const void *)srcHostPtr;\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemcpyHtoD(dstDevice, srcHost, byteCount));\n+  Py_END_ALLOW_THREADS;\n \n   Py_RETURN_NONE;\n }\n@@ -312,7 +317,9 @@ static PyObject *memFree(PyObject *self, PyObject *args) {\n     return NULL; // Error parsing arguments\n   }\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemFree(dptr));\n+  Py_END_ALLOW_THREADS;\n \n   Py_RETURN_NONE;\n }\n@@ -400,10 +407,12 @@ static PyObject *tensorMapEncodeTiled(PyObject *self, PyObject *args) {\n     cuTensorMapEncodeTiledHandle = getCuTensorMapEncodeTiledHandle();\n   }\n   // Call the function\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuTensorMapEncodeTiledHandle(\n       tensorMap, tensorDataType, tensorRank, globalAddress, globalDim,\n       globalStrides, boxDim, elementStrides, interleave, swizzle, l2Promotion,\n       oobFill));\n+  Py_END_ALLOW_THREADS;\n \n   // Clean up\n   free(globalDim);"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 19, "deletions": 11, "changes": 30, "file_content_changes": "@@ -13,6 +13,7 @@\n \n from .._C.libtriton.triton import TMAInfos\n from ..common.backend import get_backend, path_to_ptxas\n+from ..language.core import dtype\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n TRITON_VERSION = \"2.1.0\"\n@@ -245,7 +246,7 @@ def _type_of(key):\n         dtype_str = str(key).split(\".\")[-1]\n         tys = {\n             \"bool\": \"i1\",\n-            \"float8e4\": \"fp8e4\",\n+            \"float8e4nv\": \"fp8e4nv\",\n             \"float8e5\": \"fp8e5\",\n             \"float8e4b15\": \"fp8e4b15\",\n             \"float8e4b15x4\": \"fp8e4b15x4\",\n@@ -358,19 +359,16 @@ def _make_launcher(self):\n \n         spec_keys = ', '.join(specializations)\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n-        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n+        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = triton.language.dtype(\\'{dflt}\\')' if dtype.is_dtype(f'{dflt}') else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n         args_signature = args_signature + ', ' if len(args_signature) > 0 else ''\n \n         src = f\"\"\"\n-def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_stages=3, enable_warp_specialization=False, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n-    from ..compiler import compile, CompiledKernel\n+import triton\n+def {self.fn.__name__}({args_signature}grid=None, num_warps=None, num_ctas=1, num_stages=None, enable_warp_specialization=False, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n+    from ..compiler import compile, CompiledKernel, get_arch_default_num_warps, get_arch_default_num_stages\n     sig_key = {f'{sig_keys},' if len(sig_keys) > 0 else ()}\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n-    key = (version_key, sig_key, constexpr_key, spec_key, num_warps, num_ctas, num_stages, enable_warp_specialization, self.debug)\n-    if not extern_libs is None:\n-      key = (key, tuple(extern_libs.items()))\n-    assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n     assert num_ctas > 0\n     assert grid is not None\n     if callable(grid):\n@@ -403,6 +401,15 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_s\n         else:\n             stream = device_backend.get_stream()\n \n+    if num_warps is None:\n+        num_warps = get_arch_default_num_warps(device_type)\n+    if num_stages is None:\n+        num_stages = get_arch_default_num_stages(device_type)\n+\n+    key = (version_key, sig_key, constexpr_key, spec_key, num_warps, num_ctas, num_stages, enable_warp_specialization, self.debug)\n+    if not extern_libs is None:\n+      key = (key, tuple(extern_libs.items()))\n+\n     bin = cache[device].get(key, None)\n     if bin is not None:\n       # build dict of constant values\n@@ -461,9 +468,6 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         self.arg_names = [v.name for v in signature.parameters.values()]\n         self.arg_defaults = [v.default for v in signature.parameters.values()]\n         self.has_defaults = any(v != inspect._empty for v in self.arg_defaults)\n-        # specialization hints\n-        self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n-        self.do_not_specialize = {self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # function source code (without decorators)\n         self.src = textwrap.dedent(inspect.getsource(fn))\n         self.src = self.src[self.src.find(\"def\"):]\n@@ -480,6 +484,10 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         self.__annotations__ = {name: _normalize_ty(ty) for name, ty in fn.__annotations__.items()}\n         # index of constexprs\n         self.constexprs = [self.arg_names.index(name) for name, ty in self.__annotations__.items() if 'constexpr' in ty]\n+        # specialization hints\n+        regular_args = [arg for i, arg in enumerate(self.arg_names) if i not in self.constexprs]\n+        self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n+        self.do_not_specialize = {regular_args.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # tma info\n         self.tensormaps_info = TMAInfos()\n         # launcher"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 43, "deletions": 35, "changes": 78, "file_content_changes": "@@ -3,6 +3,7 @@\n import subprocess\n import sys\n from contextlib import contextmanager\n+from typing import Any, Dict, List\n \n from . import language as tl\n from ._C.libtriton.triton import runtime\n@@ -201,37 +202,41 @@ class Benchmark:\n \n     def __init__(\n         self,\n-        x_names,\n-        x_vals,\n-        line_arg,\n-        line_vals,\n-        line_names,\n-        plot_name,\n-        args,\n-        xlabel='',\n-        ylabel='',\n-        x_log=False,\n-        y_log=False,\n+        x_names: List[str],\n+        x_vals: List[Any],\n+        line_arg: str,\n+        line_vals: List[Any],\n+        line_names: List[str],\n+        plot_name: str,\n+        args: Dict[str, Any],\n+        xlabel: str = '',\n+        ylabel: str = '',\n+        x_log: bool = False,\n+        y_log: bool = False,\n         color=None,\n         styles=None,\n     ):\n         \"\"\"\n-        Constructor\n+        Constructor.\n+        x_vals can be a list of scalars or a list of tuples/lists. If x_vals is a list\n+        of scalars and there are multiple x_names, all arguments will have the same value.\n+        If x_vals is a list of tuples/lists, each element should have the same length as\n+        x_names.\n \n-        :param x_names: Name of the arguments that should appear on the x axis of the plot. If the list contains more than one element, all the arguments are assumed to have the same value.\n+        :param x_names: Name of the arguments that should appear on the x axis of the plot.\n         :type x_names: List[str]\n         :param x_vals: List of values to use for the arguments in :code:`x_names`.\n         :type x_vals: List[Any]\n         :param line_arg: Argument name for which different values correspond to different lines in the plot.\n         :type line_arg: str\n         :param line_vals: List of values to use for the arguments in :code:`line_arg`.\n-        :type line_vals: List[str]\n+        :type line_vals: List[Any]\n         :param line_names: Label names for the different lines.\n         :type line_names: List[str]\n         :param plot_name: Name of the plot.\n         :type plot_name: str\n-        :param args: List of arguments to remain fixed throughout the benchmark.\n-        :type args: List[str]\n+        :param args: Dictionary of keyword arguments to remain fixed throughout the benchmark.\n+        :type args: Dict[str, Any]\n         :param xlabel: Label for the x axis of the plot.\n         :type xlabel: str, optional\n         :param ylabel: Label for the y axis of the plot.\n@@ -261,23 +266,25 @@ def __init__(self, fn, benchmarks):\n         self.fn = fn\n         self.benchmarks = benchmarks\n \n-    def _run(self, bench, save_path, show_plots, print_data):\n+    def _run(self, bench: Benchmark, save_path: str, show_plots: bool, print_data: bool):\n         import os\n \n         import matplotlib.pyplot as plt\n         import pandas as pd\n         y_mean = bench.line_names\n         y_min = [f'{x}-min' for x in bench.line_names]\n         y_max = [f'{x}-max' for x in bench.line_names]\n-        x_names_str = str(bench.x_names)\n-        df = pd.DataFrame(columns=[x_names_str] + y_mean + y_min + y_max)\n+        x_names = list(bench.x_names)\n+        df = pd.DataFrame(columns=x_names + y_mean + y_min + y_max)\n         for x in bench.x_vals:\n-            if not isinstance(x, list):\n-                x = [x]\n-            if len(x) == 1:\n-                x = x * len(bench.x_names)\n-            x_str = str(x)\n-            x_args = {x_name: x_in for x_name, x_in in zip(bench.x_names, x)}\n+            # x can be a single value or a sequence of values.\n+            if not isinstance(x, (list, tuple)):\n+                x = [x for _ in x_names]\n+\n+            if len(x) != len(x_names):\n+                raise ValueError(f\"Expected {len(x_names)} values, got {x}\")\n+            x_args = dict(zip(x_names, x))\n+\n             row_mean, row_min, row_max = [], [], []\n             for y in bench.line_vals:\n                 ret = self.fn(**x_args, **{bench.line_arg: y}, **bench.args)\n@@ -288,23 +295,24 @@ def _run(self, bench, save_path, show_plots, print_data):\n                 row_mean += [y_mean]\n                 row_min += [y_min]\n                 row_max += [y_max]\n-            df.loc[len(df)] = [x_str] + row_mean + row_min + row_max\n+            df.loc[len(df)] = list(x) + row_mean + row_min + row_max\n+\n         if bench.plot_name:\n             plt.figure()\n             ax = plt.subplot()\n-            x = x_names_str\n+            # Plot first x value on x axis if there are multiple.\n+            first_x = x_names[0]\n             for i, y in enumerate(bench.line_names):\n                 y_min, y_max = df[y + '-min'], df[y + '-max']\n                 col = bench.styles[i][0] if bench.styles else None\n                 sty = bench.styles[i][1] if bench.styles else None\n-                ax.plot(df[x], df[y], label=y, color=col, ls=sty)\n+                ax.plot(df[first_x], df[y], label=y, color=col, ls=sty)\n                 if not y_min.isnull().all() and not y_max.isnull().all():\n                     y_min = y_min.astype(float)\n                     y_max = y_max.astype(float)\n-                    ax.fill_between(df[x], y_min, y_max, alpha=0.15, color=col)\n+                    ax.fill_between(df[first_x], y_min, y_max, alpha=0.15, color=col)\n             ax.legend()\n-            xlabel = bench.xlabel if bench.xlabel else \" = \".join(bench.x_names)\n-            ax.set_xlabel(xlabel)\n+            ax.set_xlabel(bench.xlabel or first_x)\n             ax.set_ylabel(bench.ylabel)\n             # ax.set_title(bench.plot_name)\n             ax.set_xscale(\"log\" if bench.x_log else \"linear\")\n@@ -313,7 +321,7 @@ def _run(self, bench, save_path, show_plots, print_data):\n                 plt.show()\n             if save_path:\n                 plt.savefig(os.path.join(save_path, f\"{bench.plot_name}.png\"))\n-        df = df[[x_names_str] + bench.line_names]\n+        df = df[x_names + bench.line_names]\n         if print_data:\n             print(bench.plot_name + ':')\n             print(df)\n@@ -377,11 +385,11 @@ def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None)\n         assert dtype == torch.float16\n         ops_per_sub_core = 256  # 2 4x4x4 Tensor Cores\n     else:\n-        if dtype == torch.float32:\n+        if dtype in [torch.float32, torch.int32]:\n             ops_per_sub_core = 256\n-        elif dtype in [torch.float16, torch.bfloat16]:\n+        elif dtype in [torch.float16, torch.bfloat16, torch.int16]:\n             ops_per_sub_core = 512\n-        elif dtype in [torch.int8, tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+        elif dtype in [torch.int8, tl.float8e4nv, tl.float8e4b15, tl.float8e5]:\n             ops_per_sub_core = 1024\n         else:\n             raise RuntimeError(\"dtype not supported\")"}, {"filename": "python/triton/tools/compile.c", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -54,9 +54,12 @@ void load_{kernel_name}() {{\n /*\n {kernel_docstring}\n */\n-CUresult {kernel_name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {signature}) {{\n+CUresult {kernel_name}(CUstream stream, {signature}) {{\n     if ({kernel_name}_func == NULL)\n        load_{kernel_name}();\n+    unsigned int gX = {gridX};\n+    unsigned int gY = {gridY};\n+    unsigned int gZ = {gridZ};\n     void *args[{num_args}] = {{ {arg_pointers} }};\n     // TODO: shared memory\n     if(gX * gY * gZ > 0)"}, {"filename": "python/triton/tools/compile.h", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -10,7 +10,5 @@\n \n void unload_{kernel_name}(void);\n void load_{kernel_name}(void);\n-// tt-linker: {kernel_name}:{full_signature}\n-CUresult{_placeholder} {kernel_name}(CUstream stream, unsigned int gX,\n-                                     unsigned int gY, unsigned int gZ,\n-                                     {signature});\n+// tt-linker: {kernel_name}:{full_signature}:{algo_info}\n+CUresult{_placeholder} {kernel_name}(CUstream stream, {signature});"}, {"filename": "python/triton/tools/compile.py", "status": "modified", "additions": 14, "deletions": 3, "changes": 17, "file_content_changes": "@@ -43,10 +43,11 @@\n     parser.add_argument(\"path\", help=\"Path to Python source containing desired kernel in its scope. File will be executed.\")\n     parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\", help=\"Name of the kernel to compile\", required=True)\n     parser.add_argument(\"--num-warps\", \"-w\", type=int, default=1, help=\"Number of warps to launch the kernel\")\n-    parser.add_argument(\"--num-stages\", \"-ns\", type=int, default=3, help=\"Number of stages meta-parameter for the kernel\")\n+    parser.add_argument(\"--num-stages\", \"-ns\", type=int, default=3, help=\"Number of stages (meta-parameter of the kernel)\")\n     parser.add_argument(\"--out-name\", \"-on\", type=str, default=None, help=\"Out name for the compiled kernel\")\n     parser.add_argument(\"--out-path\", \"-o\", type=Path, default=None, help=\"Out filename\")\n     parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\", required=True)\n+    parser.add_argument(\"--grid\", \"-g\", type=str, help=\"Launch grid of the kernel\", required=True)\n     args = parser.parse_args()\n \n     out_name = args.out_name if args.out_name else args.kernel_name\n@@ -59,6 +60,8 @@\n     mod = importlib.util.module_from_spec(spec)\n     spec.loader.exec_module(mod)\n     kernel = getattr(mod, args.kernel_name)\n+    grid = args.grid.split(\",\")\n+    assert len(grid) == 3\n \n     # validate and parse signature\n     signature = list(map(lambda s: s.strip(\" \"), args.signature.split(\",\")))\n@@ -68,7 +71,8 @@ def hash_signature(signature: List[str]):\n         m.update(\" \".join(signature).encode())\n         return m.hexdigest()[:8]\n \n-    sig_hash = hash_signature(signature)\n+    meta_sig = f\"warps{args.num_warps}xstages{args.num_stages}\"\n+    sig_hash = hash_signature(signature + [meta_sig])\n \n     def constexpr(s):\n         try:\n@@ -88,6 +92,9 @@ def constexpr(s):\n     constexprs = {i: constexpr(s) for i, s in enumerate(signature)}\n     constexprs = {k: v for k, v in constexprs.items() if v is not None}\n     signature = {i: s.split(\":\")[0] for i, s in enumerate(signature) if i not in constexprs}\n+    const_sig = 'x'.join([str(v) for v in constexprs.values()])\n+    doc_string = [f\"{kernel.arg_names[i]}={constexprs[i]}\" for i in constexprs.keys()]\n+    doc_string += [f\"num_warps={args.num_warps}\", f\"num_stages={args.num_stages}\"]\n \n     # compile ast into cubin\n     for h in hints.values():\n@@ -119,9 +126,13 @@ def constexpr(s):\n         \"full_signature\": \", \".join([f\"{ty_to_cpp(signature[i])} {kernel.arg_names[i]}\" for i in signature.keys()]),\n         \"arg_pointers\": \", \".join([f\"&{arg}\" for arg in arg_names]),\n         \"num_args\": len(arg_names),\n-        \"kernel_docstring\": \"\",\n+        \"kernel_docstring\": doc_string,\n         \"shared\": ccinfo.shared,\n         \"num_warps\": args.num_warps,\n+        \"algo_info\": '_'.join([const_sig, meta_sig]),\n+        \"gridX\": grid[0],\n+        \"gridY\": grid[1],\n+        \"gridZ\": grid[2],\n         \"_placeholder\": \"\",\n     }\n     for ext in ['h', 'c']:"}, {"filename": "python/triton/tools/link.py", "status": "modified", "additions": 109, "deletions": 15, "changes": 124, "file_content_changes": "@@ -15,10 +15,12 @@ class LinkerError(Exception):\n \n @dataclass\n class KernelLinkerMeta:\n+    orig_kernel_name: str\n     arg_names: Sequence[str]\n     arg_ctypes: Sequence[str]\n     sizes: Sequence[Union[int, None]]\n     sig_hash: str\n+    triton_suffix: str\n     suffix: str\n     num_specs: int\n     \"\"\" number of specialized arguments \"\"\"\n@@ -29,8 +31,8 @@ def __init__(self) -> None:\n         import re\n \n         # [kernel_name, c signature]\n-        self.linker_directives = re.compile(\"//[\\\\s]*tt-linker:[\\\\s]*([\\\\w]+):(.+)\")\n-        # [name, suffix]\n+        self.linker_directives = re.compile(\"//[\\\\s]*tt-linker:[\\\\s]*([\\\\w]+):(.+):(.+)\")\n+        # [name, hash, suffix]\n         self.kernel_name = re.compile(\"^([\\\\w]+)_([\\\\w]+)_([\\\\w]+)$\")\n         # [(type, name)]\n         self.c_sig = re.compile(\"[\\\\s]*(\\\\w+)\\\\s(\\\\w+)[,]?\")\n@@ -45,17 +47,19 @@ def extract_linker_meta(self, header: str):\n             if ln.startswith(\"//\"):\n                 m = self.linker_directives.match(ln)\n                 if _exists(m):\n-                    ker_name, c_sig = m.group(1), m.group(2)\n+                    ker_name, c_sig, algo_info = m.group(1), m.group(2), m.group(3)\n                     name, sig_hash, suffix = self._match_name(ker_name)\n                     c_types, arg_names = self._match_c_sig(c_sig)\n                     num_specs, sizes = self._match_suffix(suffix, c_sig)\n                     self._add_kernel(\n-                        name,\n+                        \"_\".join([name, algo_info]),\n                         KernelLinkerMeta(\n+                            orig_kernel_name=name,\n                             arg_names=arg_names,\n                             arg_ctypes=c_types,\n                             sizes=sizes,\n                             sig_hash=sig_hash,\n+                            triton_suffix=suffix,\n                             suffix=suffix,\n                             num_specs=num_specs,\n                         ),\n@@ -126,44 +130,107 @@ def gen_signature(m):\n     return sig\n \n \n-def make_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n+# generate declarations of kernels with meta-parameter and constant values\n+def make_algo_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     return f\"\"\"\n-CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature_with_full_args(metas[-1])});\n+CUresult {name}(CUstream stream, {gen_signature_with_full_args(metas[-1])});\n void load_{name}();\n void unload_{name}();\n     \"\"\"\n \n \n-def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n+# generate declarations of kernels with meta-parameter and constant values\n+def make_global_decl(meta: KernelLinkerMeta) -> str:\n+    return f\"\"\"\n+CUresult {meta.orig_kernel_name}_default(CUstream stream, {gen_signature_with_full_args(meta)});\n+CUresult {meta.orig_kernel_name}(CUstream stream, {gen_signature_with_full_args(meta)}, int algo_id);\n+void load_{meta.orig_kernel_name}();\n+void unload_{meta.orig_kernel_name}();\n+    \"\"\"\n+\n+\n+# generate dispatcher function for kernels with different meta-parameter and constant values\n+def make_default_algo_kernel(meta: KernelLinkerMeta) -> str:\n+    src = f\"CUresult {meta.orig_kernel_name}_default(CUstream stream, {gen_signature_with_full_args(meta)}){{\\n\"\n+    src += f\"  return {meta.orig_kernel_name}(stream, {', '.join(meta.arg_names)}, 0);\\n\"\n+    src += \"}\\n\"\n+    return src\n+\n+\n+# generate dispatcher function for kernels with different integer value hints\n+def make_kernel_hints_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     src = f\"// launcher for: {name}\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n-        src += f\"CUresult {name}_{meta.sig_hash}_{meta.suffix}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(meta)});\\n\"\n+        src += f\"CUresult {meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}(CUstream stream, {gen_signature(meta)});\\n\"\n     src += \"\\n\"\n \n-    src += f\"CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature_with_full_args(metas[-1])}){{\"\n+    src += f\"CUresult {name}(CUstream stream, {gen_signature_with_full_args(metas[-1])}){{\"\n     src += \"\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n         cond_fn = lambda val, hint: f\"({val} % {hint} == 0)\" if hint == 16 else f\"({val} == {hint})\" if hint == 1 else None\n         conds = \" && \".join([cond_fn(val, hint) for val, hint in zip(meta.arg_names, meta.sizes) if hint is not None])\n         src += f\"  if ({conds})\\n\"\n         arg_names = [arg for arg, hint in zip(meta.arg_names, meta.sizes) if hint != 1]\n-        src += f\"    return {name}_{meta.sig_hash}_{meta.suffix}(stream, gX, gY, gZ, {', '.join(arg_names)});\\n\"\n+        src += f\"    return {meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}(stream, {', '.join(arg_names)});\\n\"\n     src += \"\\n\"\n     src += \"  return CUDA_ERROR_INVALID_VALUE;\\n\"\n     src += \"}\\n\"\n \n     for mode in [\"load\", \"unload\"]:\n         src += f\"\\n// {mode} for: {name}\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"void {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n+            src += f\"void {mode}_{meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += f\"void {mode}_{name}() {{\"\n         src += \"\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"  {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n+            src += f\"  {mode}_{meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += \"}\\n\"\n     return src\n \n \n+# generate dispatcher function for kernels with different meta-parameter and constant values\n+def make_kernel_meta_const_dispatcher(meta: KernelLinkerMeta) -> str:\n+    src = f\"CUresult {meta.orig_kernel_name}(CUstream stream, {gen_signature_with_full_args(meta)}, int algo_id){{\\n\"\n+    src += f\"  assert (algo_id < (int)sizeof({meta.orig_kernel_name}_kernels));\\n\"\n+    src += f\"  return {meta.orig_kernel_name}_kernels[algo_id](stream, {', '.join(meta.arg_names)});\\n\"\n+    src += \"}\\n\"\n+    return src\n+\n+\n+# generate definition of function pointers of kernel dispatchers based on meta-parameter and constant values\n+def make_func_pointers(names: str, meta: KernelLinkerMeta) -> str:\n+    # the table of hint dispatchers\n+    src = f\"typedef CUresult (*kernel_func_t)(CUstream stream, {gen_signature_with_full_args(meta)});\\n\"\n+    src += f\"kernel_func_t {meta.orig_kernel_name}_kernels[] = {{\\n\"\n+    for name in names:\n+        src += f\"  {name},\\n\"\n+    src += \"};\\n\"\n+    return src\n+\n+\n+# generate definition for load/unload functions for kernels with different meta-parameter and constant values\n+def make_kernel_load_def(names: str, meta: KernelLinkerMeta) -> str:\n+    src = \"\"\n+    for mode in [\"load\", \"unload\"]:\n+        src += f\"void {mode}_{meta.orig_kernel_name}(void){{\\n\"\n+        for name in names:\n+            src += f\"  {mode}_{name}();\\n\"\n+        src += \"}\\n\\n\"\n+    return src\n+\n+\n+def make_get_num_algos_decl(meta: KernelLinkerMeta) -> str:\n+    src = f\"int {meta.orig_kernel_name}_get_num_algos(void);\"\n+    return src\n+\n+\n+def make_get_num_algos_def(meta: KernelLinkerMeta) -> str:\n+    src = f\"int {meta.orig_kernel_name}_get_num_algos(void){{\\n\"\n+    src += f\"  return (int)sizeof({meta.orig_kernel_name}_kernels);\\n\"\n+    src += \"}\\n\"\n+    return src\n+\n+\n desc = \"\"\"\n Triton ahead-of-time linker:\n \n@@ -198,16 +265,43 @@ def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n         parser.extract_linker_meta(h_str)\n \n     # generate headers\n-    decls = [make_decls(name, meta) for name, meta in parser.kernels.items()]\n+    algo_decls = [make_algo_decls(name, meta) for name, meta in parser.kernels.items()]\n+    meta_lists = [meta for name, meta in parser.kernels.items()]\n+    meta = meta_lists[0][0]\n+    get_num_algos_decl = make_get_num_algos_decl(meta)\n+    global_decl = make_global_decl(meta)\n     with args.out.with_suffix(\".h\").open(\"w\") as fp:\n-        fp.write(\"#include <cuda.h>\\n\" + \"\\n\".join(decls))\n+        out = \"#include <cuda.h>\\n\"\n+        out += \"\\n\".join(algo_decls)\n+        out += \"\\n\"\n+        out += get_num_algos_decl\n+        out += \"\\n\"\n+        out += global_decl\n+        fp.write(out)\n \n     # generate source\n-    defs = [make_kernel_dispatcher(name, meta) for name, meta in parser.kernels.items()]\n+    defs = [make_kernel_hints_dispatcher(name, meta) for name, meta in parser.kernels.items()]\n+    names = [name for name in parser.kernels.keys()]\n+    func_pointers_def = make_func_pointers(names, meta)\n+    meta_const_def = make_kernel_meta_const_dispatcher(meta)\n+    load_unload_def = make_kernel_load_def(names, meta)\n+    get_num_algos_def = make_get_num_algos_def(meta)\n+    default_algo_kernel = make_default_algo_kernel(meta)\n     with args.out.with_suffix(\".c\").open(\"w\") as fp:\n         out = \"\"\n         out += \"#include <cuda.h>\\n\"\n         out += \"#include <stdint.h>\\n\"\n+        out += \"#include <assert.h>\\n\"\n         out += \"\\n\"\n         out += \"\\n\".join(defs)\n+        out += \"\\n\"\n+        out += func_pointers_def\n+        out += \"\\n\"\n+        out += get_num_algos_def\n+        out += \"\\n\"\n+        out += meta_const_def\n+        out += \"\\n\"\n+        out += load_unload_def\n+        out += \"\\n\"\n+        out += default_algo_kernel\n         fp.write(out)"}, {"filename": "python/tutorials/09-experimental-tma-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -32,6 +32,11 @@\n import triton\n import triton.language as tl\n \n+if torch.cuda.get_device_capability()[0] < 9:\n+    import sys\n+    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n+    sys.exit(0)\n+\n \n @triton.autotune(\n     configs=["}, {"filename": "python/tutorials/10-experimental-tma-store-matrix-multiplication.py", "status": "renamed", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -1,5 +1,5 @@\n \"\"\"\n-Matrix Multiplication with TMASTG (Experimental)\n+Matrix Multiplication with TMA Store (Experimental)\n ================================================\n In this tutorial, you will write a very short high-performance multiplication kernel that achieves\n performance on parallel with cuBLAS.\n@@ -32,6 +32,11 @@\n import triton\n import triton.language as tl\n \n+if torch.cuda.get_device_capability()[0] < 9:\n+    import sys\n+    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n+    sys.exit(0)\n+\n \n @triton.autotune(\n     configs=["}, {"filename": "test/Conversion/invalid.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   tt.func @convert_dot(%A: tensor<16x16xf32, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n-    // expected-error@+1 {{element types of operands A and B must match}}\n+    // expected-error@+1 {{element types of operands A and B must have same bit width}}\n     %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n         tensor<16x16xf32, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n     tt.return"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -201,5 +201,12 @@ tt.func @scan_op(%ptr: tensor<1x2x4x!tt.ptr<f32>>, %v : tensor<1x2x4xf32>) {\n   }) : (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n   tt.store %ptr, %a : tensor<1x2x4xf32>\n   tt.return\n+}\n \n+// CHECK-LABEL: inline_asm\n+// CHECK: tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\"\n+tt.func @inline_asm(%0: tensor<512xi8>) {\n+  %1 = tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\"\n+    {constraints = \"=r,r\", packed_element = 4 : i32, pure = true} %0 : tensor<512xi8> -> tensor<512xi8>\n+  tt.return\n }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 40, "deletions": 1, "changes": 41, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm=\"target=nvvm\" | FileCheck %s\n \n module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK: llvm.func @test_empty_kernel(%arg0: i64, %arg1: !llvm.ptr<f16, 1>)\n@@ -1396,3 +1396,42 @@ module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 2 :\n     tt.return\n   }\n }\n+\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 80 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: inline_asm\n+  tt.func public @inline_asm(%arg0: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked>\n+    %1 = tt.splat %arg0 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %2 = tt.addptr %1, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512xi8, #blocked>\n+// CHECK: %{{.*}} = llvm.inline_asm asm_dialect = att \"shl.b32 $0, $0, 3;\", \"=r,r\" %{{.*}} : (vector<4xi8>) -> vector<4xi8>\n+    %4 = tt.elementwise_inline_asm \"shl.b32 $0, $0, 3;\" {constraints = \"=r,r\", packed_element = 4 : i32, pure = true} %3 : tensor<512xi8, #blocked> -> tensor<512xi8, #blocked>\n+    %5 = tt.splat %arg1 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %6 = tt.addptr %5, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<512xi8, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 80 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: inline_asm_pack_16bit\n+  tt.func public @inline_asm_pack_16bit(%arg0: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i8, 1> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %0 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked>\n+    %1 = tt.splat %arg0 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %2 = tt.addptr %1, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<512xi8, #blocked>\n+// CHECK: %{{.*}} = llvm.inline_asm asm_dialect = att \"shl.b16 $0, $0, 3;\", \"=h,h\" %{{.*}} : (vector<2xi8>) -> vector<2xi8>\n+    %4 = tt.elementwise_inline_asm \"shl.b16 $0, $0, 3;\" {constraints = \"=h,h\", packed_element = 2 : i32, pure = true} %3 : tensor<512xi8, #blocked> -> tensor<512xi8, #blocked>\n+    %5 = tt.splat %arg1 : (!tt.ptr<i8, 1>) -> tensor<512x!tt.ptr<i8, 1>, #blocked>\n+    %6 = tt.addptr %5, %0 : tensor<512x!tt.ptr<i8, 1>, #blocked>, tensor<512xi32, #blocked>\n+    tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<512xi8, #blocked>\n+    tt.return\n+  }\n+}"}, {"filename": "test/Triton/canonicalize.mlir", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -0,0 +1,27 @@\n+// RUN: triton-opt %s -split-input-file -canonicalize | FileCheck %s\n+\n+// CHECK-LABEL: dead_load\n+tt.func @dead_load(%ptr: tensor<32x128x!tt.ptr<f16>>) {\n+  %mask = arith.constant dense<true> : tensor<32x128xi1>\n+  %other = arith.constant dense<0.00e+00> : tensor<32x128xf16>\n+  // CHECK-NOT: tt.load {{.*}} isVolatile = false\n+  //     CHECK: tt.load {{.*}} isVolatile = true\n+  %a = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16>\n+  %b = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : tensor<32x128xf16>\n+  tt.return\n+}\n+\n+\n+// CHECK-LABEL: make_range\n+tt.func @make_range() -> (tensor<128x1xi32>, tensor<1xi32>) {\n+  // CHECK-DAG: %[[c:.*]] = arith.constant dense<0> : tensor<128x1xi32>\n+  %a = tt.make_range {end = 1 : i32, start = 0 : i32} : tensor<1xi32>\n+  %b = tt.expand_dims %a {axis = 1 : i32} : (tensor<1xi32>) -> tensor<1x1xi32>\n+  %c = tt.broadcast %b : (tensor<1x1xi32>) -> tensor<128x1xi32>\n+\n+  // CHECK-DAG: %[[d:.*]] = arith.constant dense<1> : tensor<1xi32>\n+  %d = tt.make_range {end = 2 : i32, start = 1 : i32} : tensor<1xi32>\n+\n+  // CHECK-DAG: tt.return %[[c]], %[[d]] : tensor<128x1xi32>, tensor<1xi32>\n+  tt.return %c, %d : tensor<128x1xi32>, tensor<1xi32>\n+}"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 261, "deletions": 3, "changes": 264, "file_content_changes": "@@ -77,6 +77,20 @@ tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   tt.return\n }\n \n+// Hoist the convert on top of ext to make it cheaper.\n+// CHECK-LABEL: hoist_above_ext\n+tt.func @hoist_above_ext(%arg0: tensor<1024xf16, #layout0>, %arg1: f32) -> tensor<1024xf32, #layout1> {\n+// CHECK: %[[CVT:.+]] = triton_gpu.convert_layout\n+// CHECK: arith.extf %[[CVT]]\n+// CHECK-NOT: triton_gpu.convert_layout\n+// CHECK: tt.return\n+  %0 = arith.extf %arg0 : tensor<1024xf16, #layout0> to tensor<1024xf32, #layout0>\n+  %1 = tt.splat %arg1 : (f32) -> tensor<1024xf32, #layout1>\n+  %2 = triton_gpu.convert_layout %0 : (tensor<1024xf32, #layout0>) -> tensor<1024xf32, #layout1>\n+  %3 = arith.addf %1, %2 : tensor<1024xf32, #layout1>\n+  tt.return %3 : tensor<1024xf32, #layout1>\n+}\n+\n // CHECK-LABEL: if\n tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n@@ -229,8 +243,10 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n   // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32, 1>, [[$row_layout]]>, tensor<64x64xi32, [[$row_layout]]>\n   // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32, 1>, [[$row_layout]]>\n   // CHECK-NEXT: }\n-  // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n   // CHECK-NOT: triton_gpu.convert_layout\n+  //     CHECK: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[$row_layout]]>) -> tensor<64x64xf32, [[$col_layout_novec]]>\n+  // CHECK-NOT: triton_gpu.convert_layout\n+  //    CHECK:  tt.return\n   %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n   %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n   %c1 = arith.constant 1 : index\n@@ -276,6 +292,19 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n }\n \n // CHECK-LABEL: loop_if\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: scf.for\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK:   scf.if\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK:     scf.yield\n+//     CHECK:   else\n+//     CHECK:     scf.yield\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK:   scf.yield\n+//     CHECK: triton_gpu.convert_layout\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: tt.store\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n   %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n@@ -1125,14 +1154,14 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n-// Check if the SimplifyReduceCvt handles convert_layout lifted from the for loop.\n // CHECK-LABEL: reduce_cvt2\n // Match the reduction\n // CHECK: tt.reduce\n // CHECK-SAME: axis = 1\n // CHECK: (tensor<1x256xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n-// CHECK-NEXT: triton_gpu.convert_layout\n+// CHECK: triton_gpu.convert_layout\n // CHECK-NOT: triton_gpu.convert_layout\n+// CHECK: tt.return\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n@@ -1347,6 +1376,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-war\n // Check if MoveConvertOutOfLoop hangs because of adding additional conversions\n // CHECK-LABEL: loop_print\n // CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: tt.return\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -1502,3 +1532,231 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-war\n     tt.return\n   }\n }\n+\n+\n+// -----\n+\n+// Check that we don't have extra convert for flash attention IR.\n+#blocked = #triton_gpu.blocked<{sizePerThread = [4, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 1, 8], threadsPerWarp = [4, 1, 8], warpsPerCTA = [4, 1, 1], order = [1, 2, 0], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [1, 0, 2]}>\n+#blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 1, 8], threadsPerWarp = [1, 4, 8], warpsPerCTA = [1, 4, 1], order = [0, 2, 1], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [0, 1, 2]}>\n+#blocked6 = #triton_gpu.blocked<{sizePerThread = [8, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked7 = #triton_gpu.blocked<{sizePerThread = [8, 1, 1], threadsPerWarp = [8, 1, 4], warpsPerCTA = [1, 1, 4], order = [1, 0, 2], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [1, 0, 2]}>\n+#blocked8 = #triton_gpu.blocked<{sizePerThread = [1, 8, 1], threadsPerWarp = [1, 8, 4], warpsPerCTA = [1, 1, 4], order = [0, 1, 2], CTAsPerCGA = [1, 1, 1], CTASplitNum = [1, 1, 1], CTAOrder = [0, 1, 2]}>\n+#blocked9 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @attention_fw(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg3: f32, %arg4: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg7: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg8: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg9: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg10: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg11: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg12: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg13: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg14: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg15: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg16: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg17: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg18: i32, %arg19: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg20: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg21: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}) attributes {noinline = false} {\n+    %c0_i64 = arith.constant 0 : i64\n+    %c64_i64 = arith.constant 64 : i64\n+    %cst = arith.constant dense<0.000000e+00> : tensor<128x64xf16, #blocked>\n+    %cst_0 = arith.constant dense<0xFF800000> : tensor<128xf32, #blocked1>\n+    %cst_1 = arith.constant dense<0.000000e+00> : tensor<128xf32, #blocked1>\n+    %c64_i32 = arith.constant 64 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %cst_2 = arith.constant dense<0.000000e+00> : tensor<128x64xf32, #blocked2>\n+    %cst_3 = arith.constant 1.44269502 : f32\n+    %c128_i32 = arith.constant 128 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = tt.get_program_id y : i32\n+    %2 = arith.muli %1, %arg7 : i32\n+    %3 = arith.muli %1, %arg10 : i32\n+    %4 = tt.addptr %arg0, %2 : !tt.ptr<f16, 1>, i32\n+    %5 = arith.muli %0, %c128_i32 : i32\n+    %6 = arith.extsi %arg8 : i32 to i64\n+    %7 = arith.extsi %5 : i32 to i64\n+    %8 = tt.addptr %arg1, %3 : !tt.ptr<f16, 1>, i32\n+    %9 = arith.addi %arg20, %arg21 : i32\n+    %10 = arith.extsi %arg11 : i32 to i64\n+    %11 = tt.addptr %arg2, %3 : !tt.ptr<f16, 1>, i32\n+    %12 = arith.extsi %arg14 : i32 to i64\n+    %13 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked1>\n+    %14 = tt.splat %5 : (i32) -> tensor<128xi32, #blocked1>\n+    %15 = arith.addi %14, %13 : tensor<128xi32, #blocked1>\n+    %16 = arith.mulf %arg3, %cst_3 : f32\n+    %17 = tt.splat %4 : (!tt.ptr<f16, 1>) -> tensor<128x64x!tt.ptr<f16, 1>, #blocked3>\n+    %18 = tt.splat %7 : (i64) -> tensor<128xi64, #blocked3>\n+    %19 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked3>\n+    %20 = arith.extsi %19 : tensor<128xi32, #blocked3> to tensor<128xi64, #blocked3>\n+    %21 = arith.addi %18, %20 : tensor<128xi64, #blocked3>\n+    %22 = triton_gpu.convert_layout %21 : (tensor<128xi64, #blocked3>) -> tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>\n+    %23 = tt.expand_dims %22 {axis = 1 : i32} : (tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>) -> tensor<128x1xi64, #blocked4>\n+    %24 = tt.splat %6 : (i64) -> tensor<128x1xi64, #blocked4>\n+    %25 = arith.muli %23, %24 : tensor<128x1xi64, #blocked4>\n+    %26 = tt.broadcast %25 : (tensor<128x1xi64, #blocked4>) -> tensor<128x64xi64, #blocked4>\n+    %27 = triton_gpu.convert_layout %26 : (tensor<128x64xi64, #blocked4>) -> tensor<128x64xi64, #blocked3>\n+    %28 = tt.addptr %17, %27 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    %29 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+    %30 = arith.extsi %29 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+    %31 = triton_gpu.convert_layout %30 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>\n+    %32 = tt.expand_dims %31 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>) -> tensor<1x64xi64, #blocked5>\n+    %33 = tt.broadcast %32 : (tensor<1x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked5>\n+    %34 = triton_gpu.convert_layout %33 : (tensor<128x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked3>\n+    %35 = tt.addptr %28, %34 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    %36 = tt.load %35 {boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x64xf16, #blocked3>\n+    %37 = triton_gpu.convert_layout %36 : (tensor<128x64xf16, #blocked3>) -> tensor<128x64xf16, #blocked2>\n+    %38 = tt.splat %16 : (f32) -> tensor<128x64xf32, #blocked2>\n+    %39 = arith.extf %37 : tensor<128x64xf16, #blocked2> to tensor<128x64xf32, #blocked2>\n+    %40 = arith.mulf %39, %38 : tensor<128x64xf32, #blocked2>\n+    %41 = arith.truncf %40 : tensor<128x64xf32, #blocked2> to tensor<128x64xf16, #blocked2>\n+// CHECK-NOT: triton_gpu.convert_layout\n+//     CHECK: scf.for\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   tt.dot\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+//     CHECK:   triton_gpu.convert_layout %{{.*}} #triton_gpu.dot_op\n+// CHECK-NOT:   triton_gpu.convert_layout\n+//     CHECK:   tt.dot\n+//     CHECK:   scf.yield\n+    %42:5 = scf.for %arg22 = %c0_i32 to %9 step %c64_i32 iter_args(%arg23 = %cst_2, %arg24 = %cst_1, %arg25 = %cst_0, %arg26 = %c0_i64, %arg27 = %c0_i64) -> (tensor<128x64xf32, #blocked2>, tensor<128xf32, #blocked1>, tensor<128xf32, #blocked1>, i64, i64)  : i32 {\n+      %78 = tt.splat %8 : (!tt.ptr<f16, 1>) -> tensor<64x64x!tt.ptr<f16, 1>, #blocked6>\n+      %79 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked6>\n+      %80 = arith.extsi %79 : tensor<64xi32, #blocked6> to tensor<64xi64, #blocked6>\n+      %81 = triton_gpu.convert_layout %80 : (tensor<64xi64, #blocked6>) -> tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked7}>>\n+      %82 = tt.expand_dims %81 {axis = 1 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked7}>>) -> tensor<64x1xi64, #blocked7>\n+      %83 = tt.broadcast %82 : (tensor<64x1xi64, #blocked7>) -> tensor<64x64xi64, #blocked7>\n+      %84 = triton_gpu.convert_layout %83 : (tensor<64x64xi64, #blocked7>) -> tensor<64x64xi64, #blocked6>\n+      %85 = tt.addptr %78, %84 : tensor<64x64x!tt.ptr<f16, 1>, #blocked6>, tensor<64x64xi64, #blocked6>\n+      %86 = tt.splat %arg26 : (i64) -> tensor<64xi64, #blocked6>\n+      %87 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked6>\n+      %88 = arith.extsi %87 : tensor<64xi32, #blocked6> to tensor<64xi64, #blocked6>\n+      %89 = arith.addi %86, %88 : tensor<64xi64, #blocked6>\n+      %90 = triton_gpu.convert_layout %89 : (tensor<64xi64, #blocked6>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked8}>>\n+      %91 = tt.expand_dims %90 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked8}>>) -> tensor<1x64xi64, #blocked8>\n+      %92 = tt.splat %10 : (i64) -> tensor<1x64xi64, #blocked8>\n+      %93 = arith.muli %91, %92 : tensor<1x64xi64, #blocked8>\n+      %94 = tt.broadcast %93 : (tensor<1x64xi64, #blocked8>) -> tensor<64x64xi64, #blocked8>\n+      %95 = triton_gpu.convert_layout %94 : (tensor<64x64xi64, #blocked8>) -> tensor<64x64xi64, #blocked6>\n+      %96 = tt.addptr %85, %95 : tensor<64x64x!tt.ptr<f16, 1>, #blocked6>, tensor<64x64xi64, #blocked6>\n+      %97 = tt.load %96 {boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf16, #blocked6>\n+      %98 = tt.splat %11 : (!tt.ptr<f16, 1>) -> tensor<64x64x!tt.ptr<f16, 1>, #blocked3>\n+      %99 = tt.splat %arg27 : (i64) -> tensor<64xi64, #blocked3>\n+      %100 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+      %101 = arith.extsi %100 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+      %102 = arith.addi %99, %101 : tensor<64xi64, #blocked3>\n+      %103 = triton_gpu.convert_layout %102 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>\n+      %104 = tt.expand_dims %103 {axis = 1 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>) -> tensor<64x1xi64, #blocked4>\n+      %105 = tt.splat %12 : (i64) -> tensor<64x1xi64, #blocked4>\n+      %106 = arith.muli %104, %105 : tensor<64x1xi64, #blocked4>\n+      %107 = tt.broadcast %106 : (tensor<64x1xi64, #blocked4>) -> tensor<64x64xi64, #blocked4>\n+      %108 = triton_gpu.convert_layout %107 : (tensor<64x64xi64, #blocked4>) -> tensor<64x64xi64, #blocked3>\n+      %109 = tt.addptr %98, %108 : tensor<64x64x!tt.ptr<f16, 1>, #blocked3>, tensor<64x64xi64, #blocked3>\n+      %110 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+      %111 = arith.extsi %110 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+      %112 = triton_gpu.convert_layout %111 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>\n+      %113 = tt.expand_dims %112 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>) -> tensor<1x64xi64, #blocked5>\n+      %114 = tt.broadcast %113 : (tensor<1x64xi64, #blocked5>) -> tensor<64x64xi64, #blocked5>\n+      %115 = triton_gpu.convert_layout %114 : (tensor<64x64xi64, #blocked5>) -> tensor<64x64xi64, #blocked3>\n+      %116 = tt.addptr %109, %115 : tensor<64x64x!tt.ptr<f16, 1>, #blocked3>, tensor<64x64xi64, #blocked3>\n+      %117 = tt.load %116 {boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf16, #blocked3>\n+      %118 = triton_gpu.convert_layout %41 : (tensor<128x64xf16, #blocked2>) -> tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>\n+      %119 = triton_gpu.convert_layout %97 : (tensor<64x64xf16, #blocked6>) -> tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>\n+      %120 = tt.dot %118, %119, %cst {allowTF32 = true} : tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x64xf16, #blocked>\n+      %121 = triton_gpu.convert_layout %120 : (tensor<128x64xf16, #blocked>) -> tensor<128x64xf16, #blocked2>\n+      %122 = arith.extf %121 : tensor<128x64xf16, #blocked2> to tensor<128x64xf32, #blocked2>\n+      %123 = \"tt.reduce\"(%122) <{axis = 1 : i32}> ({\n+      ^bb0(%arg28: f32, %arg29: f32):\n+        %153 = arith.maxf %arg28, %arg29 : f32\n+        tt.reduce.return %153 : f32\n+      }) : (tensor<128x64xf32, #blocked2>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %124 = triton_gpu.convert_layout %123 : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<128xf32, #blocked1>\n+      %125 = arith.maxf %arg25, %124 : tensor<128xf32, #blocked1>\n+      %126 = arith.subf %arg25, %125 : tensor<128xf32, #blocked1>\n+      %127 = tt.extern_elementwise %126 {pure = true, libname = \"libdevice\", libpath = \"/root/.pyenv/versions/3.9.9/lib/python3.9/site-packages/triton/language/../third_party/cuda/lib/libdevice.10.bc\", symbol = \"__nv_exp2f\"} : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #blocked1>\n+      %128 = triton_gpu.convert_layout %125 : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>\n+      %129 = tt.expand_dims %128 {axis = 1 : i32} : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>) -> tensor<128x1xf32, #blocked9>\n+      %130 = triton_gpu.convert_layout %129 : (tensor<128x1xf32, #blocked9>) -> tensor<128x1xf32, #blocked2>\n+      %131 = tt.broadcast %130 : (tensor<128x1xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+      %132 = arith.subf %122, %131 : tensor<128x64xf32, #blocked2>\n+      %133 = tt.extern_elementwise %132 {pure = true, libname = \"libdevice\", libpath = \"/root/.pyenv/versions/3.9.9/lib/python3.9/site-packages/triton/language/../third_party/cuda/lib/libdevice.10.bc\", symbol = \"__nv_exp2f\"} : (tensor<128x64xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+      %134 = arith.mulf %arg24, %cst_1 : tensor<128xf32, #blocked1>\n+      %135 = arith.addf %134, %127 : tensor<128xf32, #blocked1>\n+      %136 = triton_gpu.convert_layout %135 : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>\n+      %137 = tt.expand_dims %136 {axis = 1 : i32} : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>) -> tensor<128x1xf32, #blocked9>\n+      %138 = triton_gpu.convert_layout %137 : (tensor<128x1xf32, #blocked9>) -> tensor<128x1xf32, #blocked2>\n+      %139 = tt.broadcast %138 : (tensor<128x1xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+      %140 = arith.mulf %arg23, %139 : tensor<128x64xf32, #blocked2>\n+      %141 = arith.truncf %133 : tensor<128x64xf32, #blocked2> to tensor<128x64xf16, #blocked2>\n+      %142 = triton_gpu.convert_layout %141 : (tensor<128x64xf16, #blocked2>) -> tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>>\n+      %143 = triton_gpu.convert_layout %117 : (tensor<64x64xf16, #blocked3>) -> tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>>\n+      %144 = triton_gpu.convert_layout %140 : (tensor<128x64xf32, #blocked2>) -> tensor<128x64xf32, #blocked>\n+      %145 = tt.dot %142, %143, %144 {allowTF32 = true} : tensor<128x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<128x64xf32, #blocked>\n+      %146 = triton_gpu.convert_layout %145 : (tensor<128x64xf32, #blocked>) -> tensor<128x64xf32, #blocked2>\n+      %147 = arith.mulf %arg24, %127 : tensor<128xf32, #blocked1>\n+      %148 = \"tt.reduce\"(%133) <{axis = 1 : i32}> ({\n+      ^bb0(%arg28: f32, %arg29: f32):\n+        %153 = arith.addf %arg28, %arg29 : f32\n+        tt.reduce.return %153 : f32\n+      }) : (tensor<128x64xf32, #blocked2>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %149 = triton_gpu.convert_layout %148 : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<128xf32, #blocked1>\n+      %150 = arith.addf %147, %149 : tensor<128xf32, #blocked1>\n+      %151 = arith.addi %arg26, %c64_i64 : i64\n+      %152 = arith.addi %arg27, %c64_i64 : i64\n+      scf.yield %146, %150, %125, %151, %152 : tensor<128x64xf32, #blocked2>, tensor<128xf32, #blocked1>, tensor<128xf32, #blocked1>, i64, i64\n+    }\n+    %43 = triton_gpu.convert_layout %42#1 : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>\n+    %44 = tt.expand_dims %43 {axis = 1 : i32} : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked9}>>) -> tensor<128x1xf32, #blocked9>\n+    %45 = triton_gpu.convert_layout %44 : (tensor<128x1xf32, #blocked9>) -> tensor<128x1xf32, #blocked2>\n+    %46 = tt.broadcast %45 : (tensor<128x1xf32, #blocked2>) -> tensor<128x64xf32, #blocked2>\n+    %47 = arith.divf %42#0, %46 : tensor<128x64xf32, #blocked2>\n+    %48 = arith.muli %1, %arg20 : i32\n+    %49 = tt.addptr %arg4, %48 : !tt.ptr<f32, 1>, i32\n+    %50 = tt.splat %49 : (!tt.ptr<f32, 1>) -> tensor<128x!tt.ptr<f32, 1>, #blocked1>\n+    %51 = tt.addptr %50, %15 : tensor<128x!tt.ptr<f32, 1>, #blocked1>, tensor<128xi32, #blocked1>\n+    %52 = tt.extern_elementwise %42#1 {pure = true, libname = \"libdevice\", libpath = \"/root/.pyenv/versions/3.9.9/lib/python3.9/site-packages/triton/language/../third_party/cuda/lib/libdevice.10.bc\", symbol = \"__nv_log2f\"} : (tensor<128xf32, #blocked1>) -> tensor<128xf32, #blocked1>\n+    %53 = arith.addf %42#2, %52 : tensor<128xf32, #blocked1>\n+    tt.store %51, %53 {cache = 1 : i32, evict = 1 : i32} : tensor<128xf32, #blocked1>\n+    %54 = tt.addptr %arg5, %2 : !tt.ptr<f16, 1>, i32\n+    %55 = arith.extsi %arg17 : i32 to i64\n+    %56 = arith.extsi %5 : i32 to i64\n+    %57 = arith.truncf %47 : tensor<128x64xf32, #blocked2> to tensor<128x64xf16, #blocked2>\n+    %58 = triton_gpu.convert_layout %57 : (tensor<128x64xf16, #blocked2>) -> tensor<128x64xf16, #blocked3>\n+    %59 = tt.splat %54 : (!tt.ptr<f16, 1>) -> tensor<128x64x!tt.ptr<f16, 1>, #blocked3>\n+    %60 = tt.splat %56 : (i64) -> tensor<128xi64, #blocked3>\n+    %61 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked3>\n+    %62 = arith.extsi %61 : tensor<128xi32, #blocked3> to tensor<128xi64, #blocked3>\n+    %63 = arith.addi %60, %62 : tensor<128xi64, #blocked3>\n+    %64 = triton_gpu.convert_layout %63 : (tensor<128xi64, #blocked3>) -> tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>\n+    %65 = tt.expand_dims %64 {axis = 1 : i32} : (tensor<128xi64, #triton_gpu.slice<{dim = 1, parent = #blocked4}>>) -> tensor<128x1xi64, #blocked4>\n+    %66 = tt.splat %55 : (i64) -> tensor<128x1xi64, #blocked4>\n+    %67 = arith.muli %65, %66 : tensor<128x1xi64, #blocked4>\n+    %68 = tt.broadcast %67 : (tensor<128x1xi64, #blocked4>) -> tensor<128x64xi64, #blocked4>\n+    %69 = triton_gpu.convert_layout %68 : (tensor<128x64xi64, #blocked4>) -> tensor<128x64xi64, #blocked3>\n+    %70 = tt.addptr %59, %69 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    %71 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked3>\n+    %72 = arith.extsi %71 : tensor<64xi32, #blocked3> to tensor<64xi64, #blocked3>\n+    %73 = triton_gpu.convert_layout %72 : (tensor<64xi64, #blocked3>) -> tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>\n+    %74 = tt.expand_dims %73 {axis = 0 : i32} : (tensor<64xi64, #triton_gpu.slice<{dim = 0, parent = #blocked5}>>) -> tensor<1x64xi64, #blocked5>\n+    %75 = tt.broadcast %74 : (tensor<1x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked5>\n+    %76 = triton_gpu.convert_layout %75 : (tensor<128x64xi64, #blocked5>) -> tensor<128x64xi64, #blocked3>\n+    %77 = tt.addptr %70, %76 : tensor<128x64x!tt.ptr<f16, 1>, #blocked3>, tensor<128x64xi64, #blocked3>\n+    tt.store %77, %58 {cache = 1 : i32, evict = 1 : i32} : tensor<128x64xf16, #blocked3>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+// CHECK-LABEL: axis_mismatch\n+tt.func @axis_mismatch(%arg0: f32) -> tensor<1xf32, #triton_gpu.slice<{dim = 0, parent = #blocked}>> {\n+// CHECK: %[[R:.+]] = \"tt.reduce\"(%0) <{axis = 1 : i32}>\n+// CHECK: %[[C:.+]] = triton_gpu.convert_layout %[[R]]\n+// CHECK: tt.return %[[C]]\n+  %0 = tt.splat %arg0 : (f32) -> tensor<1x16xf32, #blocked>\n+  %1 = \"tt.reduce\"(%0) <{axis = 1 : i32}> ({\n+    ^bb0(%arg9: f32, %arg10: f32):\n+    %60 = arith.addf %arg9, %arg10 : f32\n+    tt.reduce.return %60 : f32\n+  }) : (tensor<1x16xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %2 = triton_gpu.convert_layout %1 : (tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<1xf32, #blocked1>\n+  %3 = triton_gpu.convert_layout %2 : (tensor<1xf32, #blocked1>) -> tensor<1xf32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  tt.return %3: tensor<1xf32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+}"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -tritongpu-remove-layout-conversions -canonicalize | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -canonicalize | FileCheck %s\n \n #Cv2 = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n #Av2k1 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv2, kWidth=1}>"}, {"filename": "third_party/amd_hip_backend", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1 +1 @@\n-Subproject commit 7d9a43c9634590e8c35a29bc2a03753338ab8f7b\n+Subproject commit b362d80c5847a02bf1ba4bce04e5a7c85ca5d97c"}]