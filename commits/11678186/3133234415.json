[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -22,7 +22,11 @@ namespace gpu {\n \n unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape);\n \n-SmallVector<unsigned> getShapePerCTA(const BlockedEncodingAttr &layout);\n+SmallVector<unsigned> getSizePerThread(Attribute layout);\n+\n+SmallVector<unsigned> getShapePerCTA(const Attribute &layout);\n+\n+SmallVector<unsigned> getOrder(const Attribute &layout);\n \n } // namespace gpu\n } // namespace triton"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 37, "deletions": 32, "changes": 69, "file_content_changes": "@@ -11,6 +11,7 @@\n #include <numeric>\n \n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n@@ -32,39 +33,43 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> paddedRepShape(rank);\n-  // TODO: move to TritonGPUAttrDefs.h.inc\n-  auto getShapePerCTA = [&](const Attribute &layout, unsigned d) -> unsigned {\n-    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-      return blockedLayout.getSizePerThread()[d] *\n-             blockedLayout.getThreadsPerWarp()[d] *\n-             blockedLayout.getWarpsPerCTA()[d];\n-    } else {\n-      assert(0 && \"Unimplemented usage of getShapePerCTA\");\n-      return 0;\n-    }\n-  };\n-  if (srcLayout.isa<BlockedEncodingAttr>() &&\n-      dstLayout.isa<BlockedEncodingAttr>()) {\n-    auto srcBlockedLayout = srcLayout.cast<BlockedEncodingAttr>();\n-    auto dstBlockedLayout = dstLayout.cast<BlockedEncodingAttr>();\n-    auto inOrd = srcBlockedLayout.getOrder();\n-    auto outOrd = dstBlockedLayout.getOrder();\n-    // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n-    //       that we cannot do vectorization.\n-    inVec = outOrd[0] == 0  ? 1\n-            : inOrd[0] == 0 ? 1\n-                            : srcBlockedLayout.getSizePerThread()[inOrd[0]];\n-    outVec =\n-        outOrd[0] == 0 ? 1 : dstBlockedLayout.getSizePerThread()[outOrd[0]];\n-    unsigned pad = std::max(inVec, outVec);\n-    for (unsigned d = 0; d < rank; ++d) {\n-      paddedRepShape[d] = std::max(\n-          std::min<unsigned>(srcTy.getShape()[d], getShapePerCTA(srcLayout, d)),\n-          std::min<unsigned>(dstTy.getShape()[d],\n-                             getShapePerCTA(dstLayout, d)));\n-    }\n-    paddedRepShape[outOrd[0]] += pad;\n+  auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n+  auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n+  auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>();\n+  auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();\n+  assert((srcBlockedLayout || srcMmaLayout) &&\n+         \"Unexpected srcLayout in getScratchConfigForCvtLayout\");\n+  assert((dstBlockedLayout || dstMmaLayout) &&\n+         \"Unexpected dstLayout in getScratchConfigForCvtLayout\");\n+  assert(!(srcMmaLayout && dstMmaLayout) &&\n+         \"Unexpected mma -> mma layout conversion\");\n+  auto inOrd =\n+      srcMmaLayout ? dstBlockedLayout.getOrder() : srcBlockedLayout.getOrder();\n+  auto outOrd =\n+      dstMmaLayout ? srcBlockedLayout.getOrder() : dstBlockedLayout.getOrder();\n+  unsigned srcContigPerThread =\n+      srcBlockedLayout ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 2;\n+  unsigned dstContigPerThread =\n+      dstBlockedLayout ? dstBlockedLayout.getSizePerThread()[outOrd[0]] : 2;\n+  // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n+  //       that we cannot do vectorization.\n+  inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;\n+  outVec = outOrd[0] == 0 ? 1 : dstContigPerThread;\n+\n+  auto srcShapePerCTA = getShapePerCTA(srcLayout);\n+  auto dstShapePerCTA = getShapePerCTA(dstLayout);\n+\n+  unsigned pad = std::max(inVec, outVec);\n+  for (unsigned d = 0; d < rank; ++d) {\n+    paddedRepShape[d] =\n+        std::max(std::min<unsigned>(srcTy.getShape()[d], srcShapePerCTA[d]),\n+                 std::min<unsigned>(dstTy.getShape()[d], dstShapePerCTA[d]));\n+  }\n+  unsigned paddedDim = 1;\n+  if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n+    paddedDim = dstBlockedLayout.getOrder()[0];\n   }\n+  paddedRepShape[paddedDim] += pad;\n   return paddedRepShape;\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 101, "deletions": 71, "changes": 172, "file_content_changes": "@@ -29,7 +29,9 @@ using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n@@ -427,7 +429,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto order = blocked_layout.getOrder();\n     unsigned rank = shape.size();\n \n-    // step 1, delinearize threadId to get the base index\n+    // delinearize threadId to get the base index\n     SmallVector<Value> multiDimWarpId =\n         delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n     SmallVector<Value> multiDimThreadId =\n@@ -453,6 +455,13 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimBase;\n   }\n \n+  SmallVector<Value>\n+  emitBaseIndexForBlockedLayout(Location loc, ConversionPatternRewriter &b,\n+                                const MmaEncodingAttr &mmaLayout,\n+                                ArrayRef<int64_t> shape) const {\n+    // ongoing\n+  }\n+\n   SmallVector<SmallVector<Value>> emitIndices(Location loc,\n                                               ConversionPatternRewriter &b,\n                                               const Attribute &layout,\n@@ -1418,9 +1427,11 @@ struct ConvertLayoutOpConversion\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n-    if ((!srcLayout.isa<BlockedEncodingAttr>()) ||\n-        (!dstLayout.isa<BlockedEncodingAttr>())) {\n-      // TODO: not implemented\n+    if ((!srcLayout.isa<BlockedEncodingAttr>() &&\n+         !srcLayout.isa<MmaEncodingAttr>()) ||\n+        (!dstLayout.isa<BlockedEncodingAttr>() &&\n+         !dstLayout.isa<MmaEncodingAttr>())) {\n+      // TODO: to be implemented\n       llvm::errs() << \"Unsupported ConvertLayout found\";\n       return failure();\n     }\n@@ -1431,39 +1442,13 @@ struct ConvertLayoutOpConversion\n \n     auto shape = dstTy.getShape();\n     unsigned rank = dstTy.getRank();\n-    auto getContigPerThread = [&](const Attribute &layout,\n-                                  unsigned d) -> unsigned {\n-      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        return blockedLayout.getSizePerThread()[d];\n-      } else {\n-        assert(0 && \"Unimplemented usage of getContigPerThread\");\n-        return 0;\n-      }\n-    };\n-    auto getAccumElemsPerThread = [&](const Attribute &layout) -> unsigned {\n-      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        return product<unsigned>(blockedLayout.getSizePerThread());\n-      } else {\n-        assert(0 && \"Unimplemented usage of getAccumElemsPerThread\");\n-        return 0;\n-      }\n-    };\n-    auto getOrder = [&](const Attribute &layout) -> ArrayRef<unsigned> {\n-      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-        return blockedLayout.getOrder();\n-      } else {\n-        assert(0 && \"Unimplemented usage of getAccumElemsPerThread\");\n-        return {};\n-      }\n-    };\n-\n     SmallVector<unsigned> numReplicates(rank);\n     SmallVector<unsigned> inNumCTAsEachRep(rank);\n     SmallVector<unsigned> outNumCTAsEachRep(rank);\n     SmallVector<unsigned> inNumCTAs(rank);\n     SmallVector<unsigned> outNumCTAs(rank);\n-    auto srcShapePerCTA = getShapePerCTA(srcLayout.cast<BlockedEncodingAttr>());\n-    auto dstShapePerCTA = getShapePerCTA(dstLayout.cast<BlockedEncodingAttr>());\n+    auto srcShapePerCTA = getShapePerCTA(srcLayout);\n+    auto dstShapePerCTA = getShapePerCTA(dstLayout);\n     for (unsigned d = 0; d < rank; ++d) {\n       unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n       unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n@@ -1478,7 +1463,6 @@ struct ConvertLayoutOpConversion\n     }\n     // Potentially we need to store for multiple CTAs in this replication\n     unsigned accumNumReplicates = product<unsigned>(numReplicates);\n-    unsigned accumInSizePerThread = getAccumElemsPerThread(srcLayout);\n     unsigned elems = getElemsPerThread(srcLayout, srcTy.getShape());\n     auto vals = getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n     unsigned inVec = 0;\n@@ -1491,19 +1475,21 @@ struct ConvertLayoutOpConversion\n     for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n       auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n       rewriter.create<mlir::gpu::BarrierOp>(loc);\n-      if (auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>()) {\n-        processReplicaBlocked(loc, rewriter, /*stNotRd*/ true, srcTy,\n-                              inNumCTAsEachRep, multiDimRepId, inVec,\n-                              paddedRepShape, outOrd, vals, smemBase);\n+      if (srcLayout.isa<BlockedEncodingAttr>() ||\n+          srcLayout.isa<MmaEncodingAttr>()) {\n+        processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n+                       multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n+                       smemBase);\n       } else {\n         assert(0 && \"ConvertLayout with input layout not implemented\");\n         return failure();\n       }\n       rewriter.create<mlir::gpu::BarrierOp>(loc);\n-      if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n-        processReplicaBlocked(loc, rewriter, /*stNotRd*/ false, dstTy,\n-                              outNumCTAsEachRep, multiDimRepId, outVec,\n-                              paddedRepShape, outOrd, outVals, smemBase);\n+      if (dstLayout.isa<BlockedEncodingAttr>() ||\n+          dstLayout.isa<MmaEncodingAttr>()) {\n+        processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                       outNumCTAsEachRep, multiDimRepId, outVec, paddedRepShape,\n+                       outOrd, outVals, smemBase);\n       } else {\n         assert(0 && \"ConvertLayout with output layout not implemented\");\n         return failure();\n@@ -1518,30 +1504,67 @@ struct ConvertLayoutOpConversion\n   }\n \n private:\n-  void processReplicaBlocked(Location loc, ConversionPatternRewriter &rewriter,\n-                             bool stNotRd, RankedTensorType type,\n-                             ArrayRef<unsigned> numCTAsEachRep,\n-                             ArrayRef<unsigned> multiDimRepId, unsigned vec,\n-                             ArrayRef<unsigned> paddedRepShape,\n-                             ArrayRef<unsigned> outOrd,\n-                             SmallVector<Value> &vals, Value smemBase) const {\n+  template <typename T>\n+  SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) const {\n+    size_t rank = order.size();\n+    assert(input.size() == rank);\n+    SmallVector<T> result(rank);\n+    for (auto it : llvm::enumerate(order)) {\n+      result[rank - 1 - it.value()] = input[it.index()];\n+    }\n+    return result;\n+  };\n+\n+  // shared memory access for blocked or mma layout\n+  void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n+                      bool stNotRd, RankedTensorType type,\n+                      ArrayRef<unsigned> numCTAsEachRep,\n+                      ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                      ArrayRef<unsigned> paddedRepShape,\n+                      ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n+                      Value smemBase) const {\n     unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n-    auto layout = type.getEncoding().cast<BlockedEncodingAttr>();\n+    auto layout = type.getEncoding();\n+    auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n+    auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n     auto rank = type.getRank();\n-    auto sizePerThread = layout.getSizePerThread();\n+    auto sizePerThread = getSizePerThread(layout);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     SmallVector<unsigned> numCTAs(rank);\n-    SmallVector<unsigned> shapePerCTA(rank);\n+    auto shapePerCTA = getShapePerCTA(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n-      shapePerCTA[d] = layout.getSizePerThread()[d] *\n-                       layout.getThreadsPerWarp()[d] *\n-                       layout.getWarpsPerCTA()[d];\n       numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n     }\n     auto llvmElemTy = getTypeConverter()->convertType(type.getElementType());\n-    auto multiDimOffsetFirstElem =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, layout, type.getShape());\n+    SmallVector<Value> multiDimOffsetFirstElem;\n+    Value mmaGrpId;\n+    Value mmaGrpIdP8;\n+    Value mmaThreadIdInGrpM2;\n+    Value mmaThreadIdInGrpM2P1;\n+    if (blockedLayout) {\n+      multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n+          loc, rewriter, blockedLayout, type.getShape());\n+    } else if (mmaLayout) {\n+      // TODO: simplify these\n+      auto cast = rewriter.create<UnrealizedConversionCastOp>(\n+          loc, TypeRange{llvmIndexTy},\n+          ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(\n+              loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)});\n+      Value threadId = cast.getResult(0);\n+      Value warpSize = createIndexAttrConstant(\n+          rewriter, loc, this->getTypeConverter()->getIndexType(), 32);\n+      Value laneId = rewriter.create<LLVM::URemOp>(loc, threadId, warpSize);\n+      Value fourVal = idx_val(4);\n+      mmaGrpId = rewriter.create<LLVM::UDivOp>(loc, laneId, fourVal);\n+      mmaGrpIdP8 = rewriter.create<LLVM::AddOp>(loc, mmaGrpId, idx_val(8));\n+      Value mmaThreadIdInGrp =\n+          rewriter.create<LLVM::URemOp>(loc, laneId, fourVal);\n+      mmaThreadIdInGrpM2 =\n+          rewriter.create<LLVM::MulOp>(loc, mmaThreadIdInGrp, idx_val(2));\n+      mmaThreadIdInGrpM2P1 =\n+          rewriter.create<LLVM::AddOp>(loc, mmaThreadIdInGrpM2, idx_val(1));\n+    }\n     for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n       auto multiDimCTAInRepId =\n           getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n@@ -1555,18 +1578,27 @@ struct ConvertLayoutOpConversion\n       // TODO: This is actually redundant index calculation, we should\n       //       consider of caching the index calculation result in case\n       //       of performance issue observed.\n-      // for (unsigned elemId = linearCTAId * accumSizePerThread;\n-      //      elemId < (linearCTAId + 1) * accumSizePerThread; elemId += vec) {\n       for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n-        auto multiDimElemId =\n-            getMultiDimIndex<unsigned>(elemId, layout.getSizePerThread());\n         SmallVector<Value> multiDimOffset(rank);\n-        for (unsigned d = 0; d < rank; ++d) {\n-          multiDimOffset[d] = add(\n-              multiDimOffsetFirstElem[d],\n-              createIndexAttrConstant(rewriter, loc, llvmIndexTy,\n-                                      multiDimCTAInRepId[d] * shapePerCTA[d] +\n-                                          multiDimElemId[d]));\n+        if (blockedLayout) {\n+          SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n+              elemId, blockedLayout.getSizePerThread());\n+          for (unsigned d = 0; d < rank; ++d) {\n+            multiDimOffset[d] = rewriter.create<LLVM::AddOp>(\n+                loc, multiDimOffsetFirstElem[d],\n+                createIndexAttrConstant(rewriter, loc, llvmIndexTy,\n+                                        multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                                            multiDimElemId[d]));\n+          }\n+        } else if (mmaLayout) {\n+          assert(rank == 2);\n+          assert(mmaLayout.getVersion() == 2 &&\n+                 \"mmaLayout ver1 not implemented yet\");\n+          multiDimOffset[0] = elemId < 2 ? mmaGrpId : mmaGrpIdP8;\n+          multiDimOffset[1] =\n+              elemId % 2 == 0 ? mmaThreadIdInGrpM2 : mmaThreadIdInGrpM2P1;\n+        } else {\n+          assert(0 && \"unexpected layout in processReplica\");\n         }\n         Value offset =\n             linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n@@ -2582,16 +2614,14 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n \n   llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n     Attribute layout = type.getEncoding();\n-    if (layout && (layout.isa<BlockedEncodingAttr>() ||\n-                   layout.isa<SliceEncodingAttr>())) {\n+    if (layout &&\n+        (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n+         layout.isa<MmaEncodingAttr>())) {\n       unsigned numElementsPerThread =\n           getElemsPerThread(layout, type.getShape());\n       SmallVector<Type, 4> types(numElementsPerThread,\n                                  convertType(type.getElementType()));\n       return LLVM::LLVMStructType::getLiteral(&getContext(), types);\n-    } else if (auto mma_layout = layout.dyn_cast_or_null<MmaEncodingAttr>()) {\n-      // TODO: Not implemented\n-      return type;\n     } else if (auto shared_layout =\n                    layout.dyn_cast_or_null<SharedEncodingAttr>()) {\n       return LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 53, "deletions": 16, "changes": 69, "file_content_changes": "@@ -58,16 +58,52 @@ unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n   }\n }\n \n-SmallVector<unsigned> getShapePerCTA(const BlockedEncodingAttr &layout) {\n-  auto sizePerThread = layout.getSizePerThread();\n-  auto threadsPerWarp = layout.getThreadsPerWarp();\n-  auto warpsPerCTA = layout.getWarpsPerCTA();\n-  unsigned rank = sizePerThread.size();\n-  SmallVector<unsigned> shapePerCTA(rank);\n-  for (unsigned k = 0; k < rank; ++k) {\n-    shapePerCTA[k] = sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k];\n+SmallVector<unsigned> getSizePerThread(Attribute layout) {\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n+                                 blockedLayout.getSizePerThread().end());\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    assert(mmaLayout.getVersion() == 2 &&\n+           \"mmaLayout version = 1 is not implemented yet\");\n+    return SmallVector<unsigned>{2, 2};\n+  } else {\n+    assert(0 && \"getSizePerThread not implemented\");\n+    return {};\n+  }\n+}\n+\n+SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n+  SmallVector<unsigned> shape;\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    for (int d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n+      shape.push_back(blockedLayout.getSizePerThread()[d] *\n+                      blockedLayout.getThreadsPerWarp()[d] *\n+                      blockedLayout.getWarpsPerCTA()[d]);\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    assert(mmaLayout.getVersion() == 2 &&\n+           \"mmaLayout version = 1 is not implemented yet\");\n+    return {16 * mmaLayout.getWarpsPerCTA()[0],\n+            8 * mmaLayout.getWarpsPerCTA()[1]};\n+  } else {\n+    assert(0 && \"Unimplemented usage of getShapePerCTA\");\n+  }\n+\n+  return shape;\n+}\n+\n+SmallVector<unsigned> getOrder(const Attribute &layout) {\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return SmallVector<unsigned>(blockedLayout.getOrder().begin(),\n+                                 blockedLayout.getOrder().end());\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    return SmallVector<unsigned>{1, 0};\n+  } else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n+    return SmallVector<unsigned>(sharedLayout.getOrder().begin(),\n+                                 sharedLayout.getOrder().end());\n+  } else {\n+    assert(0 && \"Unimplemented usage of getOrder\");\n+    return {};\n   }\n-  return shapePerCTA;\n }\n \n } // namespace gpu\n@@ -142,14 +178,15 @@ SliceEncodingAttr BlockedEncodingAttr::squeeze(int axis) {\n \n unsigned BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n-  assert(rank == getSizePerThread().size() &&\n+  auto sizePerThread = getSizePerThread();\n+  auto warpsPerCTA = getWarpsPerCTA();\n+  auto threadsPerWarp = getThreadsPerWarp();\n+  assert(rank == sizePerThread.size() &&\n          \"unexpected rank in BlockedEncodingAttr::getElemsPerThread\");\n   SmallVector<unsigned> elemsPerThread(rank);\n-  for (size_t d = 0; d < rank; ++d) {\n-    unsigned elemsPerCTAThreads =\n-        getSizePerThread()[d] * getThreadsPerWarp()[d] * getWarpsPerCTA()[d];\n-    unsigned rep = ceil<unsigned>(shape[d], elemsPerCTAThreads);\n-    elemsPerThread[d] = rep * getSizePerThread()[d];\n+  for (size_t i = 0; i < rank; ++i) {\n+    unsigned t = sizePerThread[i] * threadsPerWarp[i] * warpsPerCTA[i];\n+    elemsPerThread[i] = ceil<unsigned>(shape[i], t) * sizePerThread[i];\n   }\n   return product<unsigned>(elemsPerThread);\n }\n@@ -433,7 +470,7 @@ mlir::LogicalResult ExtractSliceOp::inferReturnTypes(\n   if (axis < 0 || axis > srcShape.size())\n     return failure();\n   SmallVector<int64_t, 4> dstShape;\n-  for (int i = 0; i < srcShape.size(); ++i)\n+  for (int i = 0; i < srcShape.size(); i++)\n     if (i != axis)\n       dstShape.push_back(srcShape[i]);\n   auto returnType ="}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -1127,6 +1127,10 @@ def default_cache_dir():\n     return os.path.join(os.environ[\"HOME\"], \".triton\", \"cache\")\n \n \n+def default_cuda_dir():\n+    return os.path.join(\"/usr\", \"local\", \"cuda\")\n+\n+\n class CacheManager:\n \n     def __init__(self, key):\n@@ -1181,7 +1185,8 @@ def quiet():\n \n def _build(name, src, srcdir):\n     cuda_lib_dir = libcuda_dir()\n-    cu_include_dir = \"/usr/local/cuda/include\"\n+    cuda_path = os.environ.get('CUDA_PATH', default_cuda_dir())\n+    cu_include_dir = os.path.join(cuda_path, \"include\")\n     suffix = sysconfig.get_config_var('EXT_SUFFIX')\n     so = os.path.join(srcdir, '{name}{suffix}'.format(name=name, suffix=suffix))\n     # try to avoid setuptools if possible"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 21, "deletions": 1, "changes": 22, "file_content_changes": "@@ -486,7 +486,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   }\n }\n \n-\n // TODO: problems in MLIR's parser on slice layout\n // #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n // module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -495,3 +494,24 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n //     return\n //   }\n // }\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<2560 x i8>\n+  // CHECK-LABEL: convert_layout_mma_block\n+  func @convert_layout_mma_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x16xf32, #mma>) -> tensor<32x16xf32, #blocked0>\n+    return\n+  }\n+}"}]