[{"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"llvm/ADT/SetVector.h\"\n #include \"llvm/Support/raw_ostream.h\"\n \n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include <atomic>\n #include <limits>\n@@ -19,6 +20,8 @@ SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n                              unsigned &outVec);\n \n+SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op);\n+\n } // namespace triton\n \n /// Modified from llvm-15.0: llvm/ADT/AddressRanges.h"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "file_content_changes": "@@ -250,6 +250,12 @@ struct PTXIOInstr : public PTXInstrBase<PTXIOInstr> {\n     return *this;\n   }\n \n+  // Add \".shared\" suffix to instruction\n+  PTXIOInstr &shared(bool predicate = true) {\n+    o(\"shared\", predicate);\n+    return *this;\n+  }\n+\n   // Add \".v\" suffix to instruction\n   PTXIOInstr &v(int vecWidth, bool predicate = true) {\n     if (vecWidth > 1) {\n@@ -286,13 +292,11 @@ struct PTXCpAsyncWaitGroupInstr : public PTXCpAsyncInstrBase {\n \n struct PTXCpAsyncLoadInstr : public PTXCpAsyncInstrBase {\n   explicit PTXCpAsyncLoadInstr(PTXBuilder *builder,\n-                               triton::CacheModifier modifier,\n-                               triton::EvictionPolicy policy)\n+                               triton::CacheModifier modifier)\n       : PTXCpAsyncInstrBase(builder) {\n     o(triton::stringifyCacheModifier(modifier).str());\n     o(\"shared\");\n     o(\"global\");\n-    o(\"L2::\" + triton::stringifyEvictionPolicy(policy).str());\n   }\n };\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -324,7 +324,9 @@ def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n     \"Attribute\":$parent\n   );\n \n-  let extraClassDeclaration = extraBaseClassDeclaration;\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n+    SmallVector<int64_t> paddedShape(ArrayRef<int64_t> shape) const;\n+  }];\n }\n \n "}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -1,6 +1,7 @@\n #ifndef TRITON_TARGET_LLVMIRTRANSLATION_H\n #define TRITON_TARGET_LLVMIRTRANSLATION_H\n #include <memory>\n+#include <vector>\n \n namespace llvm {\n class Module;\n@@ -14,6 +15,11 @@ class ModuleOp;\n namespace mlir {\n namespace triton {\n \n+// add external dependent libs\n+void addExternalLibs(mlir::ModuleOp &module,\n+                     const std::vector<std::string> &names,\n+                     const std::vector<std::string> &paths);\n+\n // Translate TritonGPU dialect to LLVMIR, return null if failed.\n std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 40, "deletions": 3, "changes": 43, "file_content_changes": "@@ -14,6 +14,7 @@ using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n+using ::mlir::triton::gpu::SliceEncodingAttr;\n \n namespace mlir {\n \n@@ -33,6 +34,10 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> paddedRepShape(rank);\n+  if (auto srcSliceLayout = srcLayout.dyn_cast<SliceEncodingAttr>())\n+    srcLayout = srcSliceLayout.getParent();\n+  if (auto dstSliceLayout = dstLayout.dyn_cast<SliceEncodingAttr>())\n+    dstLayout = dstSliceLayout.getParent();\n   auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n   auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n   auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>();\n@@ -73,6 +78,31 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   return paddedRepShape;\n }\n \n+SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcShape = srcTy.getShape();\n+  auto rank = srcShape.size();\n+  auto axis = op.axis();\n+\n+  bool fast_reduce = axis == 1; // FIXME(Qingyi): The fastest-changing dimension\n+\n+  SmallVector<unsigned> smemShape;\n+  for (auto d : srcShape)\n+    smemShape.push_back(d);\n+\n+  if (fast_reduce) {\n+    unsigned sizeInterWarps = srcLayout.getWarpsPerCTA()[axis];\n+    smemShape[axis] = sizeInterWarps;\n+  } else {\n+    unsigned threadsPerCTAAxis =\n+        srcLayout.getThreadsPerWarp()[axis] * srcLayout.getWarpsPerCTA()[axis];\n+    smemShape[axis] = threadsPerCTAAxis;\n+  }\n+\n+  return smemShape;\n+}\n+\n class AllocationAnalysis {\n public:\n   AllocationAnalysis(Operation *operation, Allocation *allocation)\n@@ -127,9 +157,16 @@ class AllocationAnalysis {\n       // TODO(Keren): Reduce with index is not supported yet.\n       auto value = op->getOperand(0);\n       if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n-        auto bytes = tensorType.getNumElements() *\n-                     tensorType.getElementTypeBitWidth() / 8;\n-        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+        if (tensorType.getEncoding().isa<BlockedEncodingAttr>()) {\n+          auto smemShape = getScratchConfigForReduce(reduceOp);\n+          unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(),\n+                                           1, std::multiplies{});\n+          auto bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n+          allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+        } else {\n+          assert(0 && \"ReduceOp with input layout other than blocked layout is \"\n+                      \"not implemented yet\");\n+        }\n       }\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "file_content_changes": "@@ -42,18 +42,14 @@ void MembarAnalysis::dfsOperation(Operation *operation,\n \n void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n                               OpBuilder *builder) {\n-  if (op->getNumResults() < 1)\n-    return;\n-\n   if (isa<scf::ForOp>(op) || isa<scf::IfOp>(op) || isa<scf::YieldOp>(op) ||\n       isa<triton::gpu::ExtractSliceOp>(op) ||\n-      isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n       isa<triton::gpu::AllocTensorOp>(op)) {\n     // Do not insert barriers before control flow operations and\n     // alloc/extract/insert\n     // alloc is an allocation op without memory write.\n     // In contrast, arith.constant is an allocation op with memory write.\n-    // FIXME(Keren): extract and insert are always alias for now\n+    // FIXME(Keren): extract is always alias for now\n     return;\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 672, "deletions": 275, "changes": 947, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n #include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n #include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n #include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n #include \"mlir/Dialect/GPU/GPUDialect.h\"\n@@ -15,6 +16,7 @@\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Membar.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n@@ -77,7 +79,15 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n+#define fadd(...) rewriter.create<LLVM::FAddOp>(loc, __VA_ARGS__)\n #define mul(...) rewriter.create<LLVM::MulOp>(loc, __VA_ARGS__)\n+#define smax(...) rewriter.create<LLVM::SMaxOp>(loc, __VA_ARGS__)\n+#define umax(...) rewriter.create<LLVM::UMaxOp>(loc, __VA_ARGS__)\n+#define fmax(...) rewriter.create<LLVM::MaxNumOp>(loc, __VA_ARGS__)\n+#define smin(...) rewriter.create<LLVM::SMinOp>(loc, __VA_ARGS__)\n+#define umin(...) rewriter.create<LLVM::UMinOp>(loc, __VA_ARGS__)\n+#define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n+#define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n #define bitcast(...) rewriter.create<LLVM::BitcastOp>(loc, __VA_ARGS__)\n #define gep(...) rewriter.create<LLVM::GEPOp>(loc, __VA_ARGS__)\n@@ -90,13 +100,18 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n   rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)\n #define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)\n #define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n+#define icmp_eq(...)                                                           \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::eq, __VA_ARGS__)\n+#define icmp_slt(...)                                                          \\\n+  rewriter.create<LLVM::ICmpOp>(loc, LLVM::ICmpPredicate::slt, __VA_ARGS__)\n #define select(...) rewriter.create<LLVM::SelectOp>(loc, __VA_ARGS__)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n #define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define i32_ty rewriter.getIntegerType(32)\n+#define f32_ty rewriter.getF32Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n-#define void_ty LLVM::LLVMVoidType::get(ctx)\n+#define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n #define struct_ty(...) LLVM::LLVMStructType::getLiteral(__VA_ARGS__)\n \n // Creator for constant\n@@ -341,6 +356,20 @@ static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   return linearIndex;\n }\n \n+Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n+                  Value val, Value pred) {\n+  MLIRContext *ctx = rewriter.getContext();\n+  unsigned bits = val.getType().getIntOrFloatBitWidth();\n+  const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n+\n+  PTXBuilder builder;\n+  auto &st = builder.create<PTXIOInstr>(\"st\")->shared().b(bits);\n+  auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n+  auto *valOpr = builder.newOperand(val, c);\n+  st(ptrOpr, valOpr).predicate(pred, \"b\");\n+  return builder.launch(rewriter, loc, void_ty(ctx));\n+}\n+\n struct ConvertTritonGPUOpToLLVMPatternBase {\n   static SmallVector<Value>\n   getElementsFromStruct(Location loc, Value llvmStruct,\n@@ -513,17 +542,8 @@ class ConvertTritonGPUOpToLLVMPattern\n     unsigned dim = sliceLayout.getDim();\n     size_t rank = shape.size();\n     if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n-      SmallVector<int64_t> paddedShape(rank + 1);\n-      for (unsigned d = 0; d < rank + 1; ++d) {\n-        if (d < dim)\n-          paddedShape[d] = shape[d];\n-        else if (d == dim)\n-          paddedShape[d] = 1;\n-        else\n-          paddedShape[d] = shape[d - 1];\n-      }\n       auto paddedIndices = emitIndicesForBlockedLayout(\n-          loc, rewriter, blockedParent, paddedShape);\n+          loc, rewriter, blockedParent, sliceLayout.paddedShape(shape));\n       unsigned numIndices = paddedIndices.size();\n       SmallVector<SmallVector<Value>> resultIndices(numIndices);\n       for (unsigned i = 0; i < numIndices; ++i)\n@@ -545,31 +565,19 @@ class ConvertTritonGPUOpToLLVMPattern\n     }\n   }\n \n-  // Emit indices calculation within each ConversionPattern, and returns a\n-  // [elemsPerThread X rank] index matrix.\n-  // TODO: [goostavz] Double confirm the redundant indices calculations will\n-  //       be eliminated in the consequent MLIR/LLVM optimization. We might\n-  //       implement a indiceCache if necessary.\n-  SmallVector<SmallVector<Value>>\n-  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &rewriter,\n-                              const BlockedEncodingAttr &blockedLayout,\n-                              ArrayRef<int64_t> shape) const {\n-    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForBlockedLayout(const BlockedEncodingAttr &blockedLayout,\n+                             ArrayRef<int64_t> shape) const {\n     auto sizePerThread = blockedLayout.getSizePerThread();\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+\n     unsigned rank = shape.size();\n     SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n     SmallVector<unsigned> tilesPerDim(rank);\n     for (unsigned k = 0; k < rank; ++k)\n       tilesPerDim[k] = ceil<unsigned>(shape[k], shapePerCTA[k]);\n \n-    // step 1, delinearize threadId to get the base index\n-    auto multiDimBase =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n-\n-    // step 2, get offset of each element\n-    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n     SmallVector<SmallVector<unsigned>> offset(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n       // 1 block in minimum if shape[k] is less than shapePerCTA[k]\n@@ -586,12 +594,10 @@ class ConvertTritonGPUOpToLLVMPattern\n                                       threadsPerWarp[k] +\n                                   threadOffset * sizePerThread[k] + elemOffset);\n     }\n-    // step 3, add offset to base, and reorder the sequence of indices to\n-    // guarantee that elems in the same sizePerThread are adjacent in order\n-    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n-                                                SmallVector<Value>(rank));\n-    unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n \n+    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n+    unsigned totalSizePerThread = product<unsigned>(sizePerThread);\n+    SmallVector<SmallVector<unsigned>> reorderedOffset(elemsPerThread);\n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n       unsigned linearNanoTileId = n / totalSizePerThread;\n       unsigned linearNanoTileElemId = n % totalSizePerThread;\n@@ -604,10 +610,38 @@ class ConvertTritonGPUOpToLLVMPattern\n             multiDimNanoTileId[k] *\n                 (sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k]) +\n             multiDimNanoTileElemId[k];\n-        multiDimIdx[n][k] =\n-            add(multiDimBase[k], idx_val(offset[k][reorderedMultiDimId]));\n+        reorderedOffset[n].push_back(offset[k][reorderedMultiDimId]);\n       }\n     }\n+    return reorderedOffset;\n+  }\n+\n+  // Emit indices calculation within each ConversionPattern, and returns a\n+  // [elemsPerThread X rank] index matrix.\n+  // TODO: [goostavz] Double confirm the redundant indices calculations will\n+  //       be eliminated in the consequent MLIR/LLVM optimization. We might\n+  //       implement a indiceCache if necessary.\n+  SmallVector<SmallVector<Value>>\n+  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &rewriter,\n+                              const BlockedEncodingAttr &blockedLayout,\n+                              ArrayRef<int64_t> shape) const {\n+    // step 1, delinearize threadId to get the base index\n+    auto multiDimBase =\n+        emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+\n+    // step 2, get offset of each element\n+    SmallVector<SmallVector<unsigned>> offset =\n+        emitOffsetForBlockedLayout(blockedLayout, shape);\n+\n+    // step 3, add offset to base, and reorder the sequence of indices to\n+    // guarantee that elems in the same sizePerThread are adjacent in order\n+    unsigned rank = shape.size();\n+    unsigned elemsPerThread = offset.size();\n+    SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,\n+                                                SmallVector<Value>(rank));\n+    for (unsigned n = 0; n < elemsPerThread; ++n)\n+      for (unsigned k = 0; k < rank; ++k)\n+        multiDimIdx[n][k] = add(multiDimBase[k], idx_val(offset[n][k]));\n \n     return multiDimIdx;\n   }\n@@ -1103,7 +1137,7 @@ struct StoreOpConversion\n       llvm::SmallVector<Type> argTys({boolTy, ptr.getType()});\n       argTys.insert(argTys.end(), nWords, valArgTy);\n \n-      auto ASMReturnTy = LLVM::LLVMVoidType::get(ctx);\n+      auto ASMReturnTy = void_ty(ctx);\n \n       ptxBuilder.launch(rewriter, loc, ASMReturnTy);\n     }\n@@ -1216,6 +1250,360 @@ struct BroadcastOpConversion\n   }\n };\n \n+/// ====================== reduce codegen begin ==========================\n+\n+struct ReduceOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::ReduceOp> {\n+public:\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::ReduceOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override;\n+\n+private:\n+  void accumulate(ConversionPatternRewriter &rewriter, Location loc,\n+                  RedOp redOp, Value &acc, Value cur, bool isFirst) const;\n+\n+  Value shflSync(ConversionPatternRewriter &rewriter, Location loc, Value val,\n+                 int i) const;\n+\n+  // Use shared memory for reduction within warps and across warps\n+  LogicalResult matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n+                                     ConversionPatternRewriter &rewriter) const;\n+\n+  // Use warp shuffle for reduction within warps and shared memory for data\n+  // exchange across warps\n+  LogicalResult matchAndRewriteFast(triton::ReduceOp op, OpAdaptor adaptor,\n+                                    ConversionPatternRewriter &rewriter) const;\n+};\n+\n+LogicalResult\n+ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n+                                    ConversionPatternRewriter &rewriter) const {\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto rank = srcTy.getShape().size();\n+  if (op.axis() == 1) // FIXME(Qingyi): The fastest-changing dimension\n+    return matchAndRewriteFast(op, adaptor, rewriter);\n+  return matchAndRewriteBasic(op, adaptor, rewriter);\n+}\n+\n+void ReduceOpConversion::accumulate(ConversionPatternRewriter &rewriter,\n+                                    Location loc, RedOp redOp, Value &acc,\n+                                    Value cur, bool isFirst) const {\n+  if (isFirst) {\n+    acc = cur;\n+    return;\n+  }\n+  auto type = cur.getType();\n+  switch (redOp) {\n+  case RedOp::ADD:\n+    acc = add(acc, cur);\n+    break;\n+  case RedOp::MAX:\n+    if (type.isUnsignedInteger())\n+      acc = umax(acc, cur);\n+    else\n+      acc = smax(acc, cur);\n+    break;\n+  case RedOp::MIN:\n+    if (type.isUnsignedInteger())\n+      acc = umin(acc, cur);\n+    else\n+      acc = smin(acc, cur);\n+    break;\n+  case RedOp::FADD:\n+    acc = fadd(acc.getType(), acc, cur);\n+    break;\n+  case RedOp::FMAX:\n+    acc = fmax(acc, cur);\n+    break;\n+  case RedOp::FMIN:\n+    acc = fmin(acc, cur);\n+    break;\n+  case RedOp::XOR:\n+    acc = xor_(acc, cur);\n+    break;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported reduce op\");\n+  }\n+};\n+\n+Value ReduceOpConversion::shflSync(ConversionPatternRewriter &rewriter,\n+                                   Location loc, Value val, int i) const {\n+  MLIRContext *ctx = rewriter.getContext();\n+  unsigned bits = val.getType().getIntOrFloatBitWidth();\n+\n+  if (bits == 64) {\n+    Type vecTy = vec_ty(f32_ty, 2);\n+    Value vec = bitcast(vecTy, val);\n+    Value val0 = extract_element(f32_ty, vec, i32_val(0));\n+    Value val1 = extract_element(f32_ty, vec, i32_val(1));\n+    val0 = shflSync(rewriter, loc, val0, i);\n+    val1 = shflSync(rewriter, loc, val1, i);\n+    vec = undef(vecTy);\n+    vec = insert_element(vecTy, vec, val0, i32_val(0));\n+    vec = insert_element(vecTy, vec, val1, i32_val(1));\n+    return bitcast(val.getType(), vec);\n+  }\n+\n+  PTXBuilder builder;\n+  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n+  auto *dOpr = builder.newOperand(\"=r\");\n+  auto *aOpr = builder.newOperand(val, \"r\");\n+  auto *bOpr = builder.newConstantOperand(i);\n+  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n+  auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n+  shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n+  return builder.launch(rewriter, loc, val.getType(), false);\n+}\n+\n+LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n+    triton::ReduceOp op, OpAdaptor adaptor,\n+    ConversionPatternRewriter &rewriter) const {\n+  Location loc = op->getLoc();\n+  unsigned axis = op.axis();\n+\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcShape = srcTy.getShape();\n+\n+  auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+  auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+  Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+  smemBase = bitcast(elemPtrTy, smemBase);\n+\n+  auto smemShape = getScratchConfigForReduce(op);\n+\n+  unsigned srcElems = getElemsPerThread(srcTy);\n+  auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+  auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n+\n+  SmallVector<SmallVector<unsigned>> offset =\n+      emitOffsetForBlockedLayout(srcLayout, srcShape);\n+\n+  std::map<SmallVector<unsigned>, Value> accs;\n+  std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n+\n+  // reduce within threads\n+  for (unsigned i = 0; i < srcElems; ++i) {\n+    SmallVector<unsigned> key = offset[i];\n+    key[axis] = 0;\n+    bool isFirst = accs.find(key) == accs.end();\n+    accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+    if (isFirst)\n+      indices[key] = srcIndices[i];\n+  }\n+\n+  // cached int32 constants\n+  std::map<int, Value> ints;\n+  ints[0] = i32_val(0);\n+  for (int N = smemShape[axis] / 2; N > 0; N >>= 1)\n+    ints[N] = i32_val(N);\n+  Value sizePerThread = i32_val(srcLayout.getSizePerThread()[axis]);\n+\n+  // reduce across threads\n+  for (auto it : accs) {\n+    const SmallVector<unsigned> &key = it.first;\n+    Value acc = it.second;\n+    SmallVector<Value> writeIdx = indices[key];\n+\n+    writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n+    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+    Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    store(acc, writePtr);\n+\n+    SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n+    for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n+      readIdx[axis] = ints[N];\n+      Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n+      Value readOffset = select(\n+          readMask, linearize(rewriter, loc, readIdx, smemShape), ints[0]);\n+      Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n+      barrier();\n+      accumulate(rewriter, loc, op.redOp(), acc, load(readPtr), false);\n+      store(acc, writePtr);\n+    }\n+  }\n+\n+  // set output values\n+  if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n+    // nd-tensor where n >= 1\n+    auto resultLayout = resultTy.getEncoding();\n+    auto resultShape = resultTy.getShape();\n+\n+    unsigned resultElems = getElemsPerThread(resultTy);\n+    auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n+    assert(resultIndices.size() == resultElems);\n+\n+    barrier();\n+    SmallVector<Value> resultVals(resultElems);\n+    for (int i = 0; i < resultElems; i++) {\n+      SmallVector<Value> readIdx = resultIndices[i];\n+      readIdx.insert(readIdx.begin() + axis, ints[0]);\n+      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+      resultVals[i] = load(readPtr);\n+    }\n+\n+    SmallVector<Type> resultTypes(resultElems, llvmElemTy);\n+    Type structTy =\n+        LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n+    Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n+    rewriter.replaceOp(op, ret);\n+  } else {\n+    // 0d-tensor -> scalar\n+    barrier();\n+    Value resultVal = load(smemBase);\n+    rewriter.replaceOp(op, resultVal);\n+  }\n+\n+  return success();\n+}\n+\n+LogicalResult ReduceOpConversion::matchAndRewriteFast(\n+    triton::ReduceOp op, OpAdaptor adaptor,\n+    ConversionPatternRewriter &rewriter) const {\n+  Location loc = op->getLoc();\n+  unsigned axis = adaptor.axis();\n+\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcShape = srcTy.getShape();\n+  auto srcOrder = srcLayout.getOrder();\n+\n+  auto threadsPerWarp = srcLayout.getThreadsPerWarp();\n+  auto warpsPerCTA = srcLayout.getWarpsPerCTA();\n+\n+  auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+  auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+  Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+  smemBase = bitcast(elemPtrTy, smemBase);\n+\n+  auto order = srcLayout.getOrder();\n+  unsigned sizeIntraWarps = threadsPerWarp[axis];\n+  unsigned sizeInterWarps = warpsPerCTA[axis];\n+\n+  unsigned srcElems = getElemsPerThread(srcTy);\n+  auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n+  auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n+\n+  SmallVector<SmallVector<unsigned>> offset =\n+      emitOffsetForBlockedLayout(srcLayout, srcShape);\n+\n+  std::map<SmallVector<unsigned>, Value> accs;\n+  std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n+\n+  auto smemShape = getScratchConfigForReduce(op);\n+\n+  // reduce within threads\n+  for (unsigned i = 0; i < srcElems; ++i) {\n+    SmallVector<unsigned> key = offset[i];\n+    key[axis] = 0;\n+    bool isFirst = accs.find(key) == accs.end();\n+    accumulate(rewriter, loc, op.redOp(), accs[key], srcValues[i], isFirst);\n+    if (isFirst)\n+      indices[key] = srcIndices[i];\n+  }\n+\n+  Value threadId = getThreadId(rewriter, loc);\n+  Value warpSize = i32_val(32);\n+  Value warpId = udiv(threadId, warpSize);\n+  Value laneId = urem(threadId, warpSize);\n+\n+  SmallVector<Value> multiDimLaneId =\n+      delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+  SmallVector<Value> multiDimWarpId =\n+      delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+  Value laneIdAxis = multiDimLaneId[axis];\n+  Value warpIdAxis = multiDimWarpId[axis];\n+\n+  Value zero = i32_val(0);\n+  Value laneZero = icmp_eq(laneIdAxis, zero);\n+  Value warpZero = icmp_eq(warpIdAxis, zero);\n+\n+  for (auto it : accs) {\n+    const SmallVector<unsigned> &key = it.first;\n+    Value acc = it.second;\n+\n+    // reduce within warps\n+    for (unsigned N = sizeIntraWarps / 2; N > 0; N >>= 1) {\n+      Value shfl = shflSync(rewriter, loc, acc, N);\n+      accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+    }\n+\n+    if (sizeInterWarps == 1) {\n+      SmallVector<Value> writeIdx = indices[key];\n+      writeIdx[axis] = zero;\n+      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+      storeShared(rewriter, loc, writePtr, acc, laneZero);\n+    } else {\n+      SmallVector<Value> writeIdx = indices[key];\n+      writeIdx[axis] =\n+          warpIdAxis; // axis must be the fastest-changing dimension\n+      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+      storeShared(rewriter, loc, writePtr, acc, laneZero);\n+      barrier();\n+\n+      SmallVector<Value> readIdx = writeIdx;\n+      readIdx[axis] = urem(laneId, i32_val(sizeInterWarps));\n+      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+      acc = load(readPtr);\n+\n+      // reduce across warps\n+      for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n+        Value shfl = shflSync(rewriter, loc, acc, N);\n+        accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+      }\n+\n+      writeIdx[axis] = zero;\n+      writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+      writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+      storeShared(rewriter, loc, writePtr, acc, and_(laneZero, warpZero));\n+    }\n+  }\n+\n+  // set output values\n+  if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n+    // nd-tensor where n >= 1\n+    auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n+    auto resultShape = resultTy.getShape();\n+\n+    unsigned resultElems = getElemsPerThread(resultTy);\n+    auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n+    assert(resultIndices.size() == resultElems);\n+\n+    barrier();\n+    SmallVector<Value> resultVals(resultElems);\n+    for (int i = 0; i < resultElems; i++) {\n+      SmallVector<Value> readIdx = resultIndices[i];\n+      readIdx.insert(readIdx.begin() + axis, i32_val(0));\n+      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+      resultVals[i] = load(readPtr);\n+    }\n+\n+    SmallVector<Type> resultTypes(resultElems, llvmElemTy);\n+    Type structTy =\n+        LLVM::LLVMStructType::getLiteral(this->getContext(), resultTypes);\n+    Value ret = getStructFromElements(loc, resultVals, rewriter, structTy);\n+    rewriter.replaceOp(op, ret);\n+  } else {\n+    // 0d-tensor -> scalar\n+    barrier();\n+    Value resultVal = load(smemBase);\n+    rewriter.replaceOp(op, resultVal);\n+  }\n+\n+  return success();\n+}\n+\n+/// ====================== reduce codegen end ==========================\n+\n template <typename SourceOp>\n struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   using OpAdaptor = typename SourceOp::Adaptor;\n@@ -1391,213 +1779,76 @@ struct ExtractSliceOpConversion\n   }\n };\n \n-// TODO: rewrite Ternary/Binary/Unary as Elementwise\n-\n // A CRTP style of base class.\n template <typename SourceOp, typename DestOp, typename ConcreteT>\n-class BinaryOpConversionBase\n-    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n-public:\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-\n-  explicit BinaryOpConversionBase(LLVMTypeConverter &typeConverter,\n-                                  PatternBenefit benefit = 1)\n-      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n-\n-  LogicalResult\n-  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto resultTy = op.getType().template dyn_cast<RankedTensorType>();\n-    // ArithmeticToLLVM will handle the lowering of scalar ArithOps\n-    if (!resultTy)\n-      return failure();\n-\n-    Location loc = op->getLoc();\n-    auto resultLayout =\n-        resultTy.getEncoding().template dyn_cast<BlockedEncodingAttr>();\n-    auto resultShape = resultTy.getShape();\n-    assert(resultLayout && \"Unexpected resultLayout in BinaryOpConversion\");\n-    unsigned elems = getElemsPerThread(resultTy);\n-    Type elemTy =\n-        this->getTypeConverter()->convertType(resultTy.getElementType());\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-\n-    auto *concreteThis = static_cast<const ConcreteT *>(this);\n-    auto lhss = this->getElementsFromStruct(loc, concreteThis->getLhs(adaptor),\n-                                            rewriter);\n-    auto rhss = this->getElementsFromStruct(loc, concreteThis->getRhs(adaptor),\n-                                            rewriter);\n-    SmallVector<Value> resultVals(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = concreteThis->createDestOp(op, rewriter, elemTy, lhss[i],\n-                                                 rhss[i], loc);\n-    }\n-    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n-    rewriter.replaceOp(op, view);\n-    return success();\n-  }\n-};\n-\n-template <typename SourceOp, typename DestOp>\n-struct BinaryOpConversion\n-    : public BinaryOpConversionBase<SourceOp, DestOp,\n-                                    BinaryOpConversion<SourceOp, DestOp>> {\n-\n-  explicit BinaryOpConversion(LLVMTypeConverter &typeConverter,\n-                              PatternBenefit benefit = 1)\n-      : BinaryOpConversionBase<SourceOp, DestOp,\n-                               BinaryOpConversion<SourceOp, DestOp>>(\n-            typeConverter, benefit) {}\n-\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-  // An interface to support variant DestOp builder.\n-  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n-                      Type elemTy, Value lhs, Value rhs, Location loc) const {\n-    return rewriter.create<DestOp>(loc, elemTy, lhs, rhs);\n-  }\n-\n-  // Get the left operand of the op.\n-  Value getLhs(OpAdaptor adaptor) const { return adaptor.getLhs(); }\n-  // Get the right operand of the op.\n-  Value getRhs(OpAdaptor adaptor) const { return adaptor.getRhs(); }\n-};\n-\n-//\n-// Ternary\n-//\n-\n-template <typename SourceOp, typename DestOp, typename ConcreteT>\n-class TernaryOpConversionBase\n+class ElementwiseOpConversionBase\n     : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n public:\n   using OpAdaptor = typename SourceOp::Adaptor;\n \n-  explicit TernaryOpConversionBase(LLVMTypeConverter &typeConverter,\n-                                   PatternBenefit benefit = 1)\n+  explicit ElementwiseOpConversionBase(LLVMTypeConverter &typeConverter,\n+                                       PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n   LogicalResult\n   matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto resultTy = op.getType().template dyn_cast<RankedTensorType>();\n-    // ArithmeticToLLVM will handle the lowering of scalar ArithOps\n-    if (!resultTy)\n-      return failure();\n-\n+    auto resultTy = op.getType();\n     Location loc = op->getLoc();\n-    auto resultLayout =\n-        resultTy.getEncoding().template dyn_cast<BlockedEncodingAttr>();\n-    auto resultShape = resultTy.getShape();\n-    assert(resultLayout && \"Unexpected resultLayout in TernaryOpConversion\");\n     unsigned elems = getElemsPerThread(resultTy);\n-    Type elemTy =\n-        this->getTypeConverter()->convertType(resultTy.getElementType());\n+    auto resultElementTy = getElementTypeOrSelf(resultTy);\n+    Type elemTy = this->getTypeConverter()->convertType(resultElementTy);\n     SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+    Type structTy = this->getTypeConverter()->convertType(resultTy);\n \n     auto *concreteThis = static_cast<const ConcreteT *>(this);\n-    auto lhss =\n-        this->getElementsFromStruct(loc, adaptor.getOperands()[0], rewriter);\n-    auto rhss =\n-        this->getElementsFromStruct(loc, adaptor.getOperands()[1], rewriter);\n-    auto thss =\n-        this->getElementsFromStruct(loc, adaptor.getOperands()[2], rewriter);\n+    auto operands = getOperands(rewriter, adaptor, elems, loc);\n     SmallVector<Value> resultVals(elems);\n     for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = concreteThis->createDestOp(op, rewriter, elemTy, lhss[i],\n-                                                 rhss[i], thss[i], loc);\n+      resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n+                                                 operands[i], loc);\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n     return success();\n   }\n-};\n-\n-template <typename SourceOp, typename DestOp>\n-struct TernaryOpConversion\n-    : public TernaryOpConversionBase<SourceOp, DestOp,\n-                                     TernaryOpConversion<SourceOp, DestOp>> {\n-\n-  explicit TernaryOpConversion(LLVMTypeConverter &typeConverter,\n-                               PatternBenefit benefit = 1)\n-      : TernaryOpConversionBase<SourceOp, DestOp,\n-                                TernaryOpConversion<SourceOp, DestOp>>(\n-            typeConverter, benefit) {}\n-\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-  // An interface to support variant DestOp builder.\n-  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n-                      Type elemTy, Value lhs, Value rhs, Value th,\n-                      Location loc) const {\n-    return rewriter.create<DestOp>(loc, elemTy, lhs, rhs, th);\n-  }\n-};\n \n-//\n-// Unary\n-//\n-\n-template <typename SourceOp, typename DestOp, typename ConcreteT>\n-class UnaryOpConversionBase : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n-\n-public:\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-\n-  explicit UnaryOpConversionBase(LLVMTypeConverter &typeConverter,\n-                                 PatternBenefit benefit = 1)\n-      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n-\n-  LogicalResult\n-  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto resultTy = op.getType();\n-\n-    Location loc = op->getLoc();\n-    unsigned elems = getElemsPerThread(resultTy);\n-    Type elemTy =\n-        this->getTypeConverter()->convertType(getElementTypeOrSelf(resultTy));\n-    SmallVector<Type> types(elems, elemTy);\n-\n-    auto *concreteThis = static_cast<const ConcreteT *>(this);\n-    auto srcs = this->getElementsFromStruct(loc, concreteThis->getSrc(adaptor),\n-                                            rewriter);\n-    SmallVector<Value> resultVals(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] =\n-          concreteThis->createDestOp(op, rewriter, elemTy, srcs[i], loc);\n+protected:\n+  SmallVector<SmallVector<Value>>\n+  getOperands(ConversionPatternRewriter &rewriter, OpAdaptor adaptor,\n+              const unsigned elems, Location loc) const {\n+    SmallVector<SmallVector<Value>> operands(elems);\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n+      for (int i = 0; i < elems; ++i) {\n+        operands[i].push_back(sub_operands[i]);\n+      }\n     }\n-    Type structTy = this->getTypeConverter()->convertType(resultTy);\n-    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n-    rewriter.replaceOp(op, view);\n-    return success();\n+    return operands;\n   }\n };\n \n template <typename SourceOp, typename DestOp>\n-struct UnaryOpConversion\n-    : public UnaryOpConversionBase<SourceOp, DestOp,\n-                                   UnaryOpConversion<SourceOp, DestOp>> {\n-\n-  explicit UnaryOpConversion(LLVMTypeConverter &typeConverter,\n-                             PatternBenefit benefit = 1)\n-      : UnaryOpConversionBase<SourceOp, DestOp,\n-                              UnaryOpConversion<SourceOp, DestOp>>(\n+struct ElementwiseOpConversion\n+    : public ElementwiseOpConversionBase<\n+          SourceOp, DestOp, ElementwiseOpConversion<SourceOp, DestOp>> {\n+  using Base =\n+      ElementwiseOpConversionBase<SourceOp, DestOp,\n+                                  ElementwiseOpConversion<SourceOp, DestOp>>;\n+  using Base::Base;\n+  using OpAdaptor = typename Base::OpAdaptor;\n+\n+  explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n+                                   PatternBenefit benefit = 1)\n+      : ElementwiseOpConversionBase<SourceOp, DestOp, ElementwiseOpConversion>(\n             typeConverter, benefit) {}\n \n-  using OpAdaptor = typename SourceOp::Adaptor;\n   // An interface to support variant DestOp builder.\n-  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n-                      Type elemTy, Value src, Location loc) const {\n-    return rewriter.create<DestOp>(loc, elemTy, src);\n-  }\n-\n-  // Get the source operand of the op.\n-  Value getSrc(OpAdaptor adaptor) const {\n-    auto operands = adaptor.getOperands();\n-    if (operands.size() > 1)\n-      llvm::report_fatal_error(\"unary operator has more than one operand\");\n-    return operands.front();\n+  DestOp createDestOp(SourceOp op, OpAdaptor adaptor,\n+                      ConversionPatternRewriter &rewriter, Type elemTy,\n+                      ValueRange operands, Location loc) const {\n+    return rewriter.create<DestOp>(loc, elemTy, operands,\n+                                   adaptor.getAttributes().getValue());\n   }\n };\n \n@@ -1606,25 +1857,22 @@ struct UnaryOpConversion\n //\n \n struct CmpIOpConversion\n-    : public BinaryOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n-                                    CmpIOpConversion> {\n-  explicit CmpIOpConversion(LLVMTypeConverter &typeConverter,\n-                            PatternBenefit benefit = 1)\n-      : BinaryOpConversionBase(typeConverter, benefit) {}\n+    : public ElementwiseOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n+                                         CmpIOpConversion> {\n+  using Base = ElementwiseOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n+                                           CmpIOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n \n   // An interface to support variant DestOp builder.\n-  LLVM::ICmpOp createDestOp(triton::gpu::CmpIOp op,\n+  LLVM::ICmpOp createDestOp(triton::gpu::CmpIOp op, OpAdaptor adaptor,\n                             ConversionPatternRewriter &rewriter, Type elemTy,\n-                            Value lhs, Value rhs, Location loc) const {\n+                            ValueRange operands, Location loc) const {\n     return rewriter.create<LLVM::ICmpOp>(\n-        loc, elemTy, ArithCmpIPredicteToLLVM(op.predicate()), lhs, rhs);\n+        loc, elemTy, ArithCmpIPredicteToLLVM(op.predicate()), operands[0],\n+        operands[1]);\n   }\n \n-  // Get the left operand of the op.\n-  Value getLhs(OpAdaptor adaptor) const { return adaptor.lhs(); }\n-  // Get the right operand of the op.\n-  Value getRhs(OpAdaptor adaptor) const { return adaptor.rhs(); }\n-\n   static LLVM::ICmpPredicate\n   ArithCmpIPredicteToLLVM(arith::CmpIPredicate predicate) {\n     switch (predicate) {\n@@ -1650,25 +1898,22 @@ struct CmpIOpConversion\n };\n \n struct CmpFOpConversion\n-    : public BinaryOpConversionBase<triton::gpu::CmpFOp, LLVM::FCmpOp,\n-                                    CmpFOpConversion> {\n-  explicit CmpFOpConversion(LLVMTypeConverter &typeConverter,\n-                            PatternBenefit benefit = 1)\n-      : BinaryOpConversionBase(typeConverter, benefit) {}\n+    : public ElementwiseOpConversionBase<triton::gpu::CmpFOp, LLVM::FCmpOp,\n+                                         CmpFOpConversion> {\n+  using Base = ElementwiseOpConversionBase<triton::gpu::CmpFOp, LLVM::FCmpOp,\n+                                           CmpFOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n \n   // An interface to support variant DestOp builder.\n-  LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op,\n+  LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op, OpAdaptor adaptor,\n                             ConversionPatternRewriter &rewriter, Type elemTy,\n-                            Value lhs, Value rhs, Location loc) const {\n+                            ValueRange operands, Location loc) const {\n     return rewriter.create<LLVM::FCmpOp>(\n-        loc, elemTy, ArithCmpFPredicteToLLVM(op.predicate()), lhs, rhs);\n+        loc, elemTy, ArithCmpFPredicteToLLVM(op.predicate()), operands[0],\n+        operands[1]);\n   }\n \n-  // Get the left operand of the op.\n-  Value getLhs(OpAdaptor adaptor) const { return adaptor.lhs(); }\n-  // Get the right operand of the op.\n-  Value getRhs(OpAdaptor adaptor) const { return adaptor.rhs(); }\n-\n   static LLVM::FCmpPredicate\n   ArithCmpFPredicteToLLVM(arith::CmpFPredicate predicate) {\n     switch (predicate) {\n@@ -1721,15 +1966,16 @@ struct ConvertLayoutOpConversion\n         dstLayout.isa<DotOperandEncodingAttr>()) {\n       return lowerSharedToDotOperand(op, adaptor, rewriter);\n     }\n-    if ((!srcLayout.isa<BlockedEncodingAttr>() &&\n-         !srcLayout.isa<MmaEncodingAttr>()) ||\n-        (!dstLayout.isa<BlockedEncodingAttr>() &&\n-         !dstLayout.isa<MmaEncodingAttr>())) {\n-      // TODO: to be implemented\n-      return failure();\n+    if ((srcLayout.isa<BlockedEncodingAttr>() ||\n+         srcLayout.isa<MmaEncodingAttr>() ||\n+         srcLayout.isa<SliceEncodingAttr>()) &&\n+        (dstLayout.isa<BlockedEncodingAttr>() ||\n+         dstLayout.isa<MmaEncodingAttr>() ||\n+         dstLayout.isa<SliceEncodingAttr>())) {\n+      return lowerDistributedToDistributed(op, adaptor, rewriter);\n     }\n-\n-    return lowerDistributedToDistributed(op, adaptor, rewriter);\n+    // TODO: to be implemented\n+    return failure();\n   }\n \n private:\n@@ -1782,6 +2028,7 @@ void ConvertLayoutOpConversion::processReplica(\n   unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n   auto layout = type.getEncoding();\n   auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n+  auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n   auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n   auto rank = type.getRank();\n   auto sizePerThread = getSizePerThread(layout);\n@@ -1799,6 +2046,18 @@ void ConvertLayoutOpConversion::processReplica(\n   if (blockedLayout) {\n     multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n         loc, rewriter, blockedLayout, type.getShape());\n+  } else if (sliceLayout) {\n+    unsigned dim = sliceLayout.getDim();\n+    auto parent = sliceLayout.getParent();\n+    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      SmallVector<int64_t> paddedShape =\n+          sliceLayout.paddedShape(type.getShape());\n+      multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n+          loc, rewriter, blockedParent, paddedShape);\n+    } else {\n+      assert(0 && \"SliceEncodingAttr with parent other than \"\n+                  \"BlockedEncodingAttr not implemented\");\n+    }\n   } else if (mmaLayout) {\n     Value threadId = getThreadId(rewriter, loc);\n     Value warpSize = idx_val(32);\n@@ -1846,6 +2105,25 @@ void ConvertLayoutOpConversion::processReplica(\n                   idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n                           multiDimElemId[d]));\n         }\n+      } else if (sliceLayout) {\n+        unsigned dim = sliceLayout.getDim();\n+        auto parent = sliceLayout.getParent();\n+        if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+          SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n+              elemId, blockedParent.getSizePerThread());\n+          for (unsigned d = 0; d < rank + 1; ++d) {\n+            if (d == dim)\n+              continue;\n+            unsigned slicedD = d < dim ? d : (d - 1);\n+            multiDimOffset[slicedD] =\n+                add(multiDimOffsetFirstElem[d],\n+                    idx_val(multiDimCTAInRepId[slicedD] * shapePerCTA[slicedD] +\n+                            multiDimElemId[d]));\n+          }\n+        } else {\n+          assert(0 && \"SliceEncodingAttr with parent other than \"\n+                      \"BlockedEncodingAttr not implemented\");\n+        }\n       } else if (mmaLayout) {\n         assert(rank == 2);\n         assert(mmaLayout.getVersion() == 2 &&\n@@ -1935,6 +2213,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n     auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n     barrier();\n     if (srcLayout.isa<BlockedEncodingAttr>() ||\n+        srcLayout.isa<SliceEncodingAttr>() ||\n         srcLayout.isa<MmaEncodingAttr>()) {\n       processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n                      multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n@@ -3430,7 +3709,7 @@ struct AsyncWaitOpConversion\n \n     auto ctx = op.getContext();\n     auto loc = op.getLoc();\n-    auto voidTy = LLVM::LLVMVoidType::get(ctx);\n+    auto voidTy = void_ty(ctx);\n     auto ret = ptxBuilder.launch(rewriter, loc, voidTy);\n \n     // Safe to remove the op since it doesn't have any return value.\n@@ -3468,7 +3747,7 @@ struct InsertSliceAsyncOpConversion\n \n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto resTy = dst.getType().cast<RankedTensorType>();\n-    auto resElemTy = resTy.getElementType();\n+    auto resElemTy = getTypeConverter()->convertType(resTy.getElementType());\n     auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n     auto resSharedLayout = resTy.getEncoding().cast<SharedEncodingAttr>();\n     auto srcShape = srcTy.getShape();\n@@ -3489,7 +3768,7 @@ struct InsertSliceAsyncOpConversion\n     assert(axis == 0 && \"insert_slice_async: Only axis=0 is supported for now\");\n     auto dstBase = createIndexAttrConstant(rewriter, loc,\n                                            getTypeConverter()->getIndexType(),\n-                                           product<int64_t>(resTy.getShape()));\n+                                           product<int64_t>(srcTy.getShape()));\n     Value offset = mul(llIndex, dstBase);\n     auto dstPtrTy = LLVM::LLVMPointerType::get(\n         getTypeConverter()->convertType(resTy.getElementType()), 3);\n@@ -3599,44 +3878,130 @@ struct InsertSliceAsyncOpConversion\n       auto numWordElems = bitWidth / resElemTy.getIntOrFloatBitWidth();\n \n       // XXX(Keren): Tune CG and CA here.\n+      auto byteWidth = bitWidth / 8;\n       CacheModifier srcCacheModifier =\n-          bitWidth == 128 ? CacheModifier::CG : CacheModifier::CA;\n-      assert(bitWidth == 128 || bitWidth == 64 || bitWidth == 32);\n+          byteWidth == 16 ? CacheModifier::CG : CacheModifier::CA;\n+      assert(byteWidth == 16 || byteWidth == 8 || byteWidth == 4);\n+      auto resByteWidth = resElemTy.getIntOrFloatBitWidth() / 8;\n \n-      for (int wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n+      auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n+      for (unsigned wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n         PTXBuilder ptxBuilder;\n-        auto &copyAsyncOp = *ptxBuilder.create<PTXCpAsyncLoadInstr>(\n-            srcCacheModifier, op.evict());\n-\n-        auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n-        auto *dstOperand =\n-            ptxBuilder.newAddrOperand(tileOffset, \"r\", baseOffset);\n-        auto *srcOperand = ptxBuilder.newAddrOperand(srcElems[vecIdx], \"l\");\n-        auto *copySize = ptxBuilder.newConstantOperand(bitWidth);\n+        auto wordElemIdx = wordIdx * numWordElems;\n+        auto &copyAsyncOp =\n+            *ptxBuilder.create<PTXCpAsyncLoadInstr>(srcCacheModifier);\n+        auto *dstOperand = ptxBuilder.newAddrOperand(\n+            tileOffset, \"r\", (wordElemIdx + baseOffset) * resByteWidth);\n+        auto *srcOperand =\n+            ptxBuilder.newAddrOperand(srcElems[elemIdx + wordElemIdx], \"l\");\n+        auto *copySize = ptxBuilder.newConstantOperand(byteWidth);\n         auto *srcSize = copySize;\n         if (op.mask()) {\n           // We don't use predicate in this case, setting src-size to 0\n           // if there's any mask. cp.async will automatically fill the\n           // remaining slots with 0 if cp-size > src-size.\n           // XXX(Keren): Always assume other = 0 for now.\n-          auto selectOp = select(maskElems[vecIdx + wordIdx * numWordElems],\n-                                 i32_val(bitWidth), i32_val(0));\n+          auto selectOp = select(maskElems[elemIdx + wordElemIdx],\n+                                 i32_val(byteWidth), i32_val(0));\n           srcSize = ptxBuilder.newOperand(selectOp, \"r\");\n         }\n         copyAsyncOp(dstOperand, srcOperand, copySize, srcSize);\n-        ptxBuilder.launch(rewriter, loc, LLVM::LLVMVoidType::get(getContext()));\n+        ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n       }\n     }\n \n     PTXBuilder ptxBuilder;\n     ptxBuilder.create<PTXCpAsyncCommitGroupInstr>()->operator()();\n-    auto ret =\n-        ptxBuilder.launch(rewriter, loc, LLVM::LLVMVoidType::get(getContext()));\n-    rewriter.replaceOp(op, ret);\n+    ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n+    rewriter.replaceOp(op, llDst);\n     return success();\n   }\n };\n \n+struct ExtElemwiseOpConversion\n+    : public ElementwiseOpConversionBase<\n+          triton::ExtElemwiseOp, LLVM::LLVMFuncOp, ExtElemwiseOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<triton::ExtElemwiseOp, LLVM::LLVMFuncOp,\n+                                  ExtElemwiseOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    StringRef funcName = op.symbol();\n+    if (funcName.empty())\n+      llvm::errs() << \"ExtElemwiseOpConversion\";\n+\n+    Type funcType = getFunctionType(elemTy, operands);\n+    LLVM::LLVMFuncOp funcOp =\n+        appendOrGetFuncOp(rewriter, op, funcName, funcType);\n+    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult(0);\n+  }\n+\n+private:\n+  Type getFunctionType(Type resultType, ValueRange operands) const {\n+    SmallVector<Type> operandTypes(operands.getTypes());\n+    return LLVM::LLVMFunctionType::get(resultType, operandTypes);\n+  }\n+\n+  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter,\n+                                     triton::ExtElemwiseOp op,\n+                                     StringRef funcName, Type funcType) const {\n+    using LLVM::LLVMFuncOp;\n+\n+    auto funcAttr = StringAttr::get(op->getContext(), funcName);\n+    Operation *funcOp = SymbolTable::lookupNearestSymbolFrom(op, funcAttr);\n+    if (funcOp)\n+      return cast<LLVMFuncOp>(*funcOp);\n+\n+    mlir::OpBuilder b(op->getParentOfType<LLVMFuncOp>());\n+    auto ret = b.create<LLVMFuncOp>(op->getLoc(), funcName, funcType);\n+    ret.getOperation()->setAttr(\n+        \"libname\", StringAttr::get(op->getContext(), op.libname()));\n+    ret.getOperation()->setAttr(\n+        \"libpath\", StringAttr::get(op->getContext(), op.libpath()));\n+    return ret;\n+  }\n+};\n+\n+struct FDivOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::DivFOp, LLVM::InlineAsmOp,\n+                                  FDivOpConversion> {\n+  using Base = ElementwiseOpConversionBase<mlir::arith::DivFOp,\n+                                           LLVM::InlineAsmOp, FDivOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::DivFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+\n+    PTXBuilder ptxBuilder;\n+    auto &fdiv = *ptxBuilder.create<PTXInstr>(\"div\");\n+    unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n+    if (32 == bitwidth) {\n+      fdiv.o(\"full\").o(\"f32\");\n+      auto res = ptxBuilder.newOperand(\"=r\");\n+      auto lhs = ptxBuilder.newOperand(operands[0], \"r\");\n+      auto rhs = ptxBuilder.newOperand(operands[1], \"r\");\n+      fdiv(res, lhs, rhs);\n+    } else if (64 == bitwidth) {\n+      fdiv.o(\"rn\").o(\"f64\");\n+      auto res = ptxBuilder.newOperand(\"=l\");\n+      auto lhs = ptxBuilder.newOperand(operands[0], \"l\");\n+      auto rhs = ptxBuilder.newOperand(operands[1], \"l\");\n+      fdiv(res, lhs, rhs);\n+    } else {\n+      assert(0 && bitwidth && \"not supported\");\n+    }\n+\n+    Value ret = ptxBuilder.launch(rewriter, loc, elemTy, false);\n+    return ret;\n+  }\n+};\n+\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -3649,12 +4014,13 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n \n #define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n-  patterns.add<TernaryOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp);\n #undef POPULATE_TERNARY_OP\n \n #define POPULATE_BINARY_OP(SRC_OP, DST_OP)                                     \\\n-  patterns.add<BinaryOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+\n   POPULATE_BINARY_OP(arith::SubIOp, LLVM::SubOp) // -\n   POPULATE_BINARY_OP(arith::SubFOp, LLVM::FSubOp)\n   POPULATE_BINARY_OP(arith::AddIOp, LLVM::AddOp) // +\n@@ -3678,7 +4044,8 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n-  patterns.add<UnaryOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+\n   POPULATE_UNARY_OP(arith::TruncIOp, LLVM::TruncOp)\n   POPULATE_UNARY_OP(arith::TruncFOp, LLVM::FPTruncOp)\n   POPULATE_UNARY_OP(arith::ExtSIOp, LLVM::SExtOp)\n@@ -3688,12 +4055,22 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   POPULATE_UNARY_OP(arith::UIToFPOp, LLVM::UIToFPOp)\n   POPULATE_UNARY_OP(arith::SIToFPOp, LLVM::SIToFPOp)\n   POPULATE_UNARY_OP(arith::ExtFOp, LLVM::FPExtOp)\n+  POPULATE_UNARY_OP(math::LogOp, math::LogOp)\n+  POPULATE_UNARY_OP(math::CosOp, math::CosOp)\n+  POPULATE_UNARY_OP(math::SinOp, math::SinOp)\n+  POPULATE_UNARY_OP(math::SqrtOp, math::SqrtOp)\n+  POPULATE_UNARY_OP(math::ExpOp, math::ExpOp)\n   POPULATE_UNARY_OP(triton::BitcastOp, LLVM::BitcastOp)\n   POPULATE_UNARY_OP(triton::IntToPtrOp, LLVM::IntToPtrOp)\n   POPULATE_UNARY_OP(triton::PtrToIntOp, LLVM::PtrToIntOp)\n #undef POPULATE_UNARY_OP\n \n+  patterns.add<FDivOpConversion>(typeConverter, benefit);\n+\n+  patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n+\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n+  patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);\n \n@@ -3731,21 +4108,39 @@ class ConvertTritonGPUToLLVM\n \n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    // step 1: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // step 2: Allocate for shared memories\n-    // step 3: Convert the rest of ops via partial conversion\n-    // The reason for a seperation between 1/3 is that, step 2 is out of\n+    // step 1: Allocate shared memories and insert barriers\n+    // setp 2: Convert SCF to CFG\n+    // step 3: Convert FuncOp to LLVMFuncOp via partial conversion\n+    // step 4: Convert the rest of ops via partial conversion\n+    // The reason for putting step 1 before step 2 is that the membar analysis\n+    // currently only supports SCF but not CFG.\n+    // The reason for a seperation between 1/4 is that, step 3 is out of\n     // the scope of Dialect Conversion, thus we need to make sure the smem\n-    // is not revised during the conversion of step 3.\n+    // is not revised during the conversion of step 4.\n+    Allocation allocation(mod);\n+    MembarAnalysis membar(&allocation);\n+\n+    RewritePatternSet scf_patterns(context);\n+    mlir::populateLoopToStdConversionPatterns(scf_patterns);\n+    mlir::ConversionTarget scf_target(*context);\n+    scf_target.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp,\n+                            scf::WhileOp, scf::ExecuteRegionOp>();\n+    scf_target.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n+    if (failed(\n+            applyPartialConversion(mod, scf_target, std::move(scf_patterns))))\n+      return signalPassFailure();\n+\n     RewritePatternSet func_patterns(context);\n     func_patterns.add<FuncOpConversion>(typeConverter, numWarps, 1 /*benefit*/);\n     if (failed(\n             applyPartialConversion(mod, funcTarget, std::move(func_patterns))))\n       return signalPassFailure();\n \n-    Allocation allocation(mod);\n     auto axisAnalysis = runAxisAnalysis(mod);\n     initSharedMemory(allocation.getSharedMemorySize(), typeConverter);\n+    mod->setAttr(\"triton_gpu.shared\",\n+                 mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n+                                        allocation.getSharedMemorySize()));\n \n     // We set a higher benefit here to ensure triton's patterns runs before\n     // arith patterns for some encoding not supported by the community\n@@ -3788,9 +4183,11 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n   OpBuilder b(mod.getBodyRegion());\n   auto loc = mod.getLoc();\n   auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n-  auto arrayTy = LLVM::LLVMArrayType::get(elemTy, size);\n+  // Set array size 0 and external linkage indicates that we use dynamic shared\n+  // allocation to allow a larger shared memory size for each kernel.\n+  auto arrayTy = LLVM::LLVMArrayType::get(elemTy, 0);\n   auto global = b.create<LLVM::GlobalOp>(\n-      loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::Internal,\n+      loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,\n       \"global_smem\", /*value=*/Attribute(),\n       /*alignment=*/0, mlir::gpu::GPUDialect::getWorkgroupAddressSpace());\n   SmallVector<LLVM::LLVMFuncOp> funcs;"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 45, "deletions": 10, "changes": 55, "file_content_changes": "@@ -68,6 +68,19 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n                                  blockedLayout.getSizePerThread().end());\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    unsigned dim = sliceLayout.getDim();\n+    auto parent = sliceLayout.getParent();\n+    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      SmallVector<unsigned> sizePerThread(\n+          blockedParent.getSizePerThread().begin(),\n+          blockedParent.getSizePerThread().end());\n+      sizePerThread.erase(sizePerThread.begin() + dim);\n+      return sizePerThread;\n+    } else {\n+      assert(0 && \"SliceEncodingAttr with parent other than \"\n+                  \"BlockedEncodingAttr not implemented\");\n+    }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.getVersion() == 2 &&\n            \"mmaLayout version = 1 is not implemented yet\");\n@@ -100,6 +113,21 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n       shape.push_back(blockedLayout.getSizePerThread()[d] *\n                       blockedLayout.getThreadsPerWarp()[d] *\n                       blockedLayout.getWarpsPerCTA()[d]);\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    unsigned dim = sliceLayout.getDim();\n+    auto parent = sliceLayout.getParent();\n+    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      for (int d = 0, n = blockedParent.getOrder().size(); d < n; ++d) {\n+        if (d == dim)\n+          continue;\n+        shape.push_back(blockedParent.getSizePerThread()[d] *\n+                        blockedParent.getThreadsPerWarp()[d] *\n+                        blockedParent.getWarpsPerCTA()[d]);\n+      }\n+    } else {\n+      assert(0 && \"SliceEncodingAttr with parent other than \"\n+                  \"BlockedEncodingAttr not implemented\");\n+    }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     assert(mmaLayout.getVersion() == 2 &&\n            \"mmaLayout version = 1 is not implemented yet\");\n@@ -211,23 +239,30 @@ unsigned BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   return product<unsigned>(elemsPerThread);\n }\n \n+SmallVector<int64_t>\n+SliceEncodingAttr::paddedShape(ArrayRef<int64_t> shape) const {\n+  size_t rank = shape.size();\n+  unsigned dim = getDim();\n+  SmallVector<int64_t> retShape(rank + 1);\n+  for (unsigned d = 0; d < rank + 1; ++d) {\n+    if (d < dim)\n+      retShape[d] = shape[d];\n+    else if (d == dim)\n+      retShape[d] = 1;\n+    else\n+      retShape[d] = shape[d - 1];\n+  }\n+  return retShape;\n+}\n+\n unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n   auto parent = getParent();\n   unsigned dim = getDim();\n   if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n     assert(rank == blockedParent.getSizePerThread().size() - 1 &&\n            \"unexpected rank in SliceEncodingAttr::getElemsPerThread\");\n-    SmallVector<int64_t> paddedShape(rank + 1);\n-    for (unsigned d = 0; d < rank + 1; ++d) {\n-      if (d < dim)\n-        paddedShape[d] = shape[d];\n-      else if (d == dim)\n-        paddedShape[d] = 1;\n-      else\n-        paddedShape[d] = shape[d - 1];\n-    }\n-    return blockedParent.getElemsPerThread(paddedShape);\n+    return blockedParent.getElemsPerThread(paddedShape(shape));\n   } else {\n     assert(0 && \"getElemsPerThread not implemented\");\n     return 0;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "file_content_changes": "@@ -103,16 +103,21 @@ class SimplifyConversion : public mlir::RewritePattern {\n     if (!arg)\n       return mlir::failure();\n     // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n-    // cvt(insert_slice(x), type2) -> extract_slice(cvt(x, type2))\n     auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n     if (alloc_tensor) {\n       rewriter.replaceOpWithNewOp<triton::gpu::AllocTensorOp>(\n           op, op->getResult(0).getType());\n       return mlir::success();\n     }\n+    // cvt(insert_slice(x), type2) -> insert_slice(cvt(x, type2))\n     auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n     if (insert_slice) {\n       auto newType = op->getResult(0).getType();\n+      // Ensure that the new insert_slice op is placed in the same place as the\n+      // old insert_slice op. Otherwise, the new insert_slice op may be placed\n+      // after the async_wait op, which is not allowed.\n+      OpBuilder::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPoint(insert_slice);\n       auto new_arg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           op->getLoc(), newType, insert_slice.dst());\n       rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n@@ -126,6 +131,11 @@ class SimplifyConversion : public mlir::RewritePattern {\n     auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n     if (extract_slice) {\n       auto origType = extract_slice.src().getType().cast<RankedTensorType>();\n+      // Ensure that the new extract_slice op is placed in the same place as the\n+      // old extract_slice op. Otherwise, the new extract_slice op may be placed\n+      // after the async_wait op, which is not allowed.\n+      OpBuilder::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPoint(extract_slice);\n       auto newType = RankedTensorType::get(\n           origType.getShape(), origType.getElementType(),\n           op->getResult(0).getType().cast<RankedTensorType>().getEncoding());"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -78,6 +78,9 @@ class LoopPipeliner {\n   /// emit pipelined loads (before loop body)\n   void emitPrologue();\n \n+  /// emit pipelined loads (after loop body)\n+  void emitEpilogue();\n+\n   /// create the new ForOp (add new args & insert prefetched ops)\n   scf::ForOp createNewForOp();\n \n@@ -362,6 +365,23 @@ void LoopPipeliner::emitPrologue() {\n         loadStageBuffer[loadOp][numStages - 1], loopIterIdx, /*axis*/ 0);\n     loadsExtract[loadOp] = extractSlice;\n   }\n+  // bump up loopIterIdx, this is used for getting the correct slice for the\n+  // *next* iteration\n+  loopIterIdx = builder.create<arith::AddIOp>(\n+      loopIterIdx.getLoc(), loopIterIdx,\n+      builder.create<arith::ConstantIntOp>(loopIterIdx.getLoc(), 1, 32));\n+}\n+\n+void LoopPipeliner::emitEpilogue() {\n+  // If there's any outstanding async copies, we need to wait for them.\n+  // TODO(Keren): We may want to completely avoid the async copies in the last\n+  // few iterations by setting is_masked attribute to true. We don't want to use\n+  // the mask operand because it's a tensor but not a scalar.\n+  OpBuilder builder(forOp);\n+  OpBuilder::InsertionGuard g(builder);\n+  builder.setInsertionPointAfter(forOp);\n+  Operation *asyncWait =\n+      builder.create<triton::gpu::AsyncWaitOp>(forOp.getLoc(), 0);\n }\n \n scf::ForOp LoopPipeliner::createNewForOp() {\n@@ -581,6 +601,8 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n \n       scf::ForOp newForOp = pipeliner.createNewForOp();\n \n+      pipeliner.emitEpilogue();\n+\n       // replace the original loop\n       for (unsigned i = 0; i < forOp->getNumResults(); ++i)\n         forOp->getResult(i).replaceAllUsesWith(newForOp->getResult(i));"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 73, "deletions": 1, "changes": 74, "file_content_changes": "@@ -16,6 +16,9 @@\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n #include \"triton/tools/sys/getenv.hpp\"\n #include \"llvm/IR/Constants.h\"\n+#include \"llvm/IRReader/IRReader.h\"\n+#include \"llvm/Linker/Linker.h\"\n+#include \"llvm/Support/SourceMgr.h\"\n \n namespace mlir {\n namespace triton {\n@@ -136,23 +139,92 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnChange=*/true,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n-  pm.addPass(mlir::createLowerToCFGPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass());\n   // Conanicalize to eliminate the remaining UnrealizedConversionCastOp\n   pm.addPass(mlir::createCanonicalizerPass());\n+  pm.addPass(mlir::createCSEPass()); // Simplify the IR to improve readability.\n+  pm.addPass(mlir::createSymbolDCEPass());\n+  pm.addPass(mlir::createCanonicalizerPass());\n \n   if (failed(pm.run(module))) {\n     llvm::errs() << \"Pass execution failed\";\n     return nullptr;\n   }\n \n+  std::map<std::string, std::string> extern_libs;\n+  SmallVector<LLVM::LLVMFuncOp> funcs;\n+  module.walk([&](LLVM::LLVMFuncOp func) {\n+    if (func.isExternal())\n+      funcs.push_back(func);\n+  });\n+\n+  for (auto &func : funcs) {\n+    if (func.getOperation()->hasAttr(\"libname\")) {\n+      auto name =\n+          func.getOperation()->getAttr(\"libname\").dyn_cast<StringAttr>();\n+      auto path =\n+          func.getOperation()->getAttr(\"libpath\").dyn_cast<StringAttr>();\n+      if (name) {\n+        std::string lib_name = name.str();\n+        extern_libs[lib_name] = path.str();\n+      }\n+    }\n+  }\n+\n+  if (module.getOperation()->hasAttr(\"triton_gpu.externs\")) {\n+    auto dict = module.getOperation()\n+                    ->getAttr(\"triton_gpu.externs\")\n+                    .dyn_cast<DictionaryAttr>();\n+    for (auto &attr : dict) {\n+      extern_libs[attr.getName().strref().trim().str()] =\n+          attr.getValue().dyn_cast<StringAttr>().strref().trim().str();\n+    }\n+  }\n+\n   auto llvmir = translateLLVMToLLVMIR(llvmContext, module);\n   if (!llvmir) {\n     llvm::errs() << \"Translate to LLVM IR failed\";\n+    return nullptr;\n+  }\n+\n+  llvm::SMDiagnostic err;\n+  for (auto &lib : extern_libs) {\n+    auto ext_mod = llvm::parseIRFile(lib.second, err, *llvmContext);\n+    if (!ext_mod) {\n+      llvm::errs() << \"Failed to load extern lib \" << lib.first;\n+      return nullptr;\n+    }\n+    ext_mod->setTargetTriple(llvmir->getTargetTriple());\n+    ext_mod->setDataLayout(llvmir->getDataLayout());\n+\n+    if (llvm::Linker::linkModules(*llvmir, std::move(ext_mod))) {\n+      llvm::errs() << \"Failed to link extern lib \" << lib.first;\n+      return nullptr;\n+    }\n   }\n \n   return llvmir;\n }\n \n+void addExternalLibs(mlir::ModuleOp &module,\n+                     const std::vector<std::string> &names,\n+                     const std::vector<std::string> &paths) {\n+  if (names.empty() || names.size() != paths.size())\n+    return;\n+\n+  llvm::SmallVector<NamedAttribute, 2> attrs;\n+\n+  for (size_t i = 0; i < names.size(); ++i) {\n+    auto name = StringAttr::get(module->getContext(), names[i]);\n+    auto path = StringAttr::get(module->getContext(), paths[i]);\n+    NamedAttribute attr(name, path);\n+    attrs.push_back(attr);\n+  }\n+\n+  DictionaryAttr dict = DictionaryAttr::get(module->getContext(), attrs);\n+  module.getOperation()->setAttr(\"triton_gpu.externs\", dict);\n+  return;\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -1257,8 +1257,8 @@ void init_triton_translation(py::module &m) {\n   using ret = py::return_value_policy;\n \n   m.def(\"get_shared_memory_size\", [](mlir::ModuleOp module) {\n-    auto pass = std::make_unique<mlir::Allocation>(module);\n-    return pass->getSharedMemorySize();\n+    return module->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.shared\")\n+        .getInt();\n   });\n \n   m.def(\n@@ -1335,6 +1335,12 @@ void init_triton_translation(py::module &m) {\n           py::bytes bytes(cubin);\n           return bytes;\n         });\n+\n+  m.def(\"add_external_libs\",\n+        [](mlir::ModuleOp &op, const std::vector<std::string> &names,\n+           const std::vector<std::string> &paths) {\n+          ::mlir::triton::addExternalLibs(op, names, paths);\n+        });\n }\n \n void init_triton(py::module &m) {"}, {"filename": "python/tests/test_elementwise.py", "status": "added", "additions": 189, "deletions": 0, "changes": 189, "file_content_changes": "@@ -0,0 +1,189 @@\n+import tempfile\n+from inspect import Parameter, Signature\n+\n+import _testcapi\n+import pytest\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+torch_type = {\n+    \"bool\": torch.bool,\n+    \"int32\": torch.int32,\n+    \"float32\": torch.float32,\n+    \"float64\": torch.float64\n+}\n+\n+torch_ops = {\n+    \"log\": \"log\",\n+    \"cos\": \"cos\",\n+    \"sin\": \"sin\",\n+    \"sqrt\": \"sqrt\",\n+    \"abs\": \"abs\",\n+    \"exp\": \"exp\",\n+    \"sigmoid\": \"sigmoid\",\n+    \"umulhi\": None,\n+    \"cdiv\": None,\n+    \"fdiv\": \"div\",\n+    \"minimum\": \"minimum\",\n+    \"maximum\": \"maximum\",\n+    \"where\": \"where\",\n+}\n+\n+libdevice = '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'\n+\n+\n+def get_tensor(shape, data_type, b_positive=False):\n+    x = None\n+    if data_type.startswith('int'):\n+        x = torch.randint(2**31 - 1, shape, dtype=torch_type[data_type], device='cuda')\n+    elif data_type.startswith('bool'):\n+        x = torch.randint(1, shape, dtype=torch_type[data_type], device='cuda')\n+    else:\n+        x = torch.randn(shape, dtype=torch_type[data_type], device='cuda')\n+\n+    if b_positive:\n+        x = torch.abs(x)\n+\n+    return x\n+\n+\n+@pytest.mark.parametrize('expr, output_type, input0_type',\n+                         [('log', 'float32', 'float32'),\n+                          ('log', 'float64', 'float64'),\n+                             ('cos', 'float32', 'float32'),\n+                             ('cos', 'float64', 'float64'),\n+                             ('sin', 'float32', 'float32'),\n+                             ('sin', 'float64', 'float64'),\n+                             ('sqrt', 'float32', 'float32'),\n+                             ('sqrt', 'float64', 'float64'),\n+                             ('abs', 'float32', 'float32'),\n+                             ('exp', 'float32', 'float32'),\n+                             ('sigmoid', 'float32', 'float32'),\n+                          ])\n+def test_single_input(expr, output_type, input0_type):\n+    src = f\"\"\"\n+def kernel(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = tl.{expr}(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\"\"\"\n+    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n+    fp.write(src)\n+    fp.flush()\n+\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        pass\n+    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n+    parameters = []\n+    parameters.append(Parameter(\"X\", 1))\n+    parameters.append(Parameter(\"Y\", 1))\n+    parameters.append(Parameter(\"BLOCK\", 1))\n+    kernel.__signature__ = Signature(parameters=parameters)\n+    kernel = triton.jit(kernel)\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x = get_tensor(shape, input0_type, expr == 'log' or expr == 'sqrt')\n+    # triton result\n+    y = torch.zeros(shape, dtype=torch_type[output_type], device=\"cuda\")\n+    kernel[(1,)](x, y, BLOCK=shape[0], extern_libs={\"libdevice\": libdevice})\n+    # reference result\n+    y_ref = getattr(torch, torch_ops[expr])(x)\n+    # compare\n+    assert_close(y, y_ref)\n+\n+\n+@pytest.mark.parametrize('expr, output_type, input0_type, input1_type',\n+                         [('umulhi', 'int32', 'int32', 'int32'),\n+                          ('cdiv', 'int32', 'int32', 'int32'),\n+                             ('fdiv', 'float32', 'float32', 'float32'),\n+                             ('minimum', 'float32', 'float32', 'float32'),\n+                             ('maximum', 'float32', 'float32', 'float32'),\n+                          ])\n+def test_two_input(expr, output_type, input0_type, input1_type):\n+    src = f\"\"\"\n+def kernel(X0, X1, Y, BLOCK: tl.constexpr):\n+    x0 = tl.load(X0 + tl.arange(0, BLOCK))\n+    x1 = tl.load(X1 + tl.arange(0, BLOCK))\n+    y = tl.{expr}(x0, x1)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\"\"\"\n+    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n+    fp.write(src)\n+    fp.flush()\n+\n+    def kernel(X0, X1, Y, BLOCK: tl.constexpr):\n+        pass\n+    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n+    parameters = []\n+    parameters.append(Parameter(\"X0\", 1))\n+    parameters.append(Parameter(\"X1\", 1))\n+    parameters.append(Parameter(\"Y\", 1))\n+    parameters.append(Parameter(\"BLOCK\", 1))\n+    kernel.__signature__ = Signature(parameters=parameters)\n+    kernel = triton.jit(kernel)\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x0 = get_tensor(shape, input0_type)\n+    x1 = get_tensor(shape, input1_type)\n+\n+    # triton result\n+    y = torch.zeros(shape, dtype=torch_type[output_type], device=\"cuda\")\n+    kernel[(1,)](x0, x1, y, BLOCK=shape[0], extern_libs={\"libdevice\": libdevice})\n+    # reference result\n+\n+    if expr == \"cdiv\":\n+        y_ref = (x0 + x1 - 1) // x1\n+    elif expr == \"umulhi\":\n+        y_ref = ((x0.to(torch.int64) * x1) >> 32).to(torch.int32)\n+    else:\n+        y_ref = getattr(torch, torch_ops[expr])(x0, x1)\n+    # compare\n+    assert_close(y, y_ref)\n+\n+\n+@pytest.mark.parametrize('expr, output_type, input0_type, input1_type, input2_type',\n+                         [('where', \"int32\", \"bool\", \"int32\", \"int32\"), ])\n+def test_three_input(expr, output_type, input0_type, input1_type, input2_type):\n+    src = f\"\"\"\n+def kernel(X0, X1, X2, Y, BLOCK: tl.constexpr):\n+    x0 = tl.load(X0 + tl.arange(0, BLOCK))\n+    x1 = tl.load(X1 + tl.arange(0, BLOCK))\n+    x2 = tl.load(X2 + tl.arange(0, BLOCK))\n+    y = tl.{expr}(x0, x1, x2)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\"\"\"\n+    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n+    fp.write(src)\n+    fp.flush()\n+\n+    def kernel(X0, X1, X2, Y, BLOCK: tl.constexpr):\n+        pass\n+    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n+    parameters = []\n+    parameters.append(Parameter(\"X0\", 1))\n+    parameters.append(Parameter(\"X1\", 1))\n+    parameters.append(Parameter(\"X2\", 1))\n+    parameters.append(Parameter(\"Y\", 1))\n+    parameters.append(Parameter(\"BLOCK\", 1))\n+    kernel.__signature__ = Signature(parameters=parameters)\n+    kernel = triton.jit(kernel)\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x0 = get_tensor(shape, input0_type)\n+    x1 = get_tensor(shape, input1_type)\n+    x2 = get_tensor(shape, input1_type)\n+\n+    # triton result\n+    y = torch.zeros(shape, dtype=torch_type[output_type], device=\"cuda\")\n+    kernel[(1,)](x0, x1, x2, y, BLOCK=shape[0], extern_libs={\"libdevice\": libdevice})\n+    # reference result\n+\n+    y_ref = getattr(torch, torch_ops[expr])(x0, x1, x2)\n+    # compare\n+    assert_close(y, y_ref)"}, {"filename": "python/tests/test_ext_elemwise.py", "status": "added", "additions": 178, "deletions": 0, "changes": 178, "file_content_changes": "@@ -0,0 +1,178 @@\n+\n+import pytest\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n+    [4, 256, 1],\n+    [4, 1024, 256],\n+])\n+def test_sin_no_mask(num_warps, block_size, iter_size):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               block_size,\n+               iter_size: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+        for i in range(0, block_size, iter_size):\n+            offset = pid * block_size + tl.arange(0, iter_size)\n+            x_ptrs = x_ptr + offset\n+            x = tl.load(x_ptrs)\n+            y = tl.libdevice.sin(x)\n+            y_ptrs = y_ptr + offset\n+            tl.store(y_ptrs, y)\n+\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+\n+    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    y = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // (block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n+\n+    golden_y = torch.sin(x)\n+    assert_close(y, golden_y, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n+    [4, 256, 1],\n+    [4, 1024, 256],\n+])\n+def test_fmin_no_mask(num_warps, block_size, iter_size):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               block_size,\n+               iter_size: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+        for i in range(0, block_size, iter_size):\n+            offset = pid * block_size + tl.arange(0, iter_size)\n+            x_ptrs = x_ptr + offset\n+            y_ptrs = y_ptr + offset\n+\n+            x = tl.load(x_ptrs)\n+            y = tl.load(y_ptrs)\n+            z = tl.libdevice.min(x, y)\n+            z_ptrs = z_ptr + offset\n+            tl.store(z_ptrs, z)\n+\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+            z_ptr += iter_size\n+\n+    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // (block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n+\n+    golden_z = torch.minimum(x, y)\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n+    [4, 256, 1],\n+    [4, 1024, 256],\n+])\n+def test_fmad_rn_no_mask(num_warps, block_size, iter_size):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               w_ptr,\n+               block_size,\n+               iter_size: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+        for i in range(0, block_size, iter_size):\n+            offset = pid * block_size + tl.arange(0, iter_size)\n+            x_ptrs = x_ptr + offset\n+            y_ptrs = y_ptr + offset\n+            z_ptrs = z_ptr + offset\n+\n+            x = tl.load(x_ptrs)\n+            y = tl.load(y_ptrs)\n+            z = tl.load(z_ptrs)\n+\n+            w = tl.libdevice.fma_rn(x, y, z)\n+            w_ptrs = w_ptr + offset\n+            tl.store(w_ptrs, w)\n+\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+            z_ptr += iter_size\n+            w_ptr += iter_size\n+\n+    x = torch.randn((block_size,), device='cuda', dtype=torch.float64)\n+    y = torch.randn((block_size,), device='cuda', dtype=torch.float64)\n+    z = torch.randn((block_size,), device='cuda', dtype=torch.float64)\n+    w = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // (block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, w_ptr=w,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n+\n+    golden_w = x * y + z\n+    assert_close(w, golden_w, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+                         [('int32', 'libdevice.ffs', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n+                          ('int32', 'libdevice.ffs', '')])\n+def test_libdevice(dtype_str, expr, lib_path):\n+    src = f\"\"\"\n+def kernel(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = tl.{expr}(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\"\"\"\n+    import tempfile\n+    from inspect import Parameter, Signature\n+\n+    import _testcapi\n+\n+    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n+    fp.write(src)\n+    fp.flush()\n+\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        pass\n+    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n+    parameters = []\n+    parameters.append(Parameter(\"X\", 1))\n+    parameters.append(Parameter(\"Y\", 1))\n+    parameters.append(Parameter(\"BLOCK\", 1))\n+    kernel.__signature__ = Signature(parameters=parameters)\n+    kernel = triton.jit(kernel)\n+\n+    torch_type = {\n+        \"int32\": torch.int32,\n+        \"float32\": torch.float32,\n+        \"float64\": torch.float64\n+    }\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x = None\n+    if dtype_str == \"int32\":\n+        x = torch.randint(2**31 - 1, shape, dtype=torch_type[dtype_str], device=\"cuda\")\n+    else:\n+        x = torch.randn(shape, dtype=torch_type[dtype_str], device=\"cuda\")\n+    if expr == 'libdevice.ffs':\n+        y_ref = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+        for i in range(shape[0]):\n+            y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n+\n+    # triton result\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    kernel[(1,)](x, y, BLOCK=shape[0], extern_libs={\"libdevice\": lib_path})\n+    # compare\n+    assert_close(y, y_ref)"}, {"filename": "python/tests/test_reduce.py", "status": "added", "additions": 115, "deletions": 0, "changes": 115, "file_content_changes": "@@ -0,0 +1,115 @@\n+import pytest\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+dtype_mapping = {\n+    'float16': torch.float16,\n+    'float32': torch.float32,\n+    'float64': torch.float64,\n+}\n+\n+\n+def patch_kernel(template, to_replace):\n+    kernel = triton.JITFunction(template.fn)\n+    for key, value in to_replace.items():\n+        kernel.src = kernel.src.replace(key, value)\n+    return kernel\n+\n+\n+@triton.jit\n+def reduce1d_kernel(x_ptr, z_ptr, block: tl.constexpr):\n+    x = tl.load(x_ptr + tl.arange(0, block))\n+    tl.store(z_ptr, tl.OP(x, axis=0))\n+\n+\n+@triton.jit\n+def reduce2d_kernel(x_ptr, z_ptr, axis: tl.constexpr, block_m: tl.constexpr, block_n: tl.constexpr):\n+    range_m = tl.arange(0, block_m)\n+    range_n = tl.arange(0, block_n)\n+    x = tl.load(x_ptr + range_m[:, None] * block_n + range_n[None, :])\n+    z = tl.OP(x, axis=axis)\n+    if axis == 0:\n+        tl.store(z_ptr + range_n, z)\n+    else:\n+        tl.store(z_ptr + range_m, z)\n+\n+\n+reduce1d_configs = [\n+    (op, dtype, shape)\n+    for op in ['sum', 'min', 'max']\n+    for dtype in ['float16', 'float32', 'float64']\n+    for shape in [4, 8, 16, 32, 64, 128, 512, 1024]\n+]\n+\n+\n+@pytest.mark.parametrize('op, dtype, shape', reduce1d_configs)\n+def test_reduce1d(op, dtype, shape):\n+    dtype = dtype_mapping[dtype]\n+    x = torch.randn((shape,), device='cuda', dtype=dtype)\n+    z = torch.empty(\n+        tuple(),\n+        device=x.device,\n+        dtype=dtype,\n+    )\n+\n+    kernel = patch_kernel(reduce1d_kernel, {'OP': op})\n+    grid = (1,)\n+    kernel[grid](x_ptr=x, z_ptr=z, block=shape)\n+\n+    if op == 'sum':\n+        golden_z = torch.sum(x, dtype=dtype)\n+    elif op == 'min':\n+        golden_z = torch.min(x)\n+    else:\n+        golden_z = torch.max(x)\n+\n+    if op == 'sum':\n+        if shape >= 256:\n+            assert_close(z, golden_z, rtol=0.05, atol=0.1)\n+        elif shape >= 32:\n+            assert_close(z, golden_z, rtol=0.05, atol=0.02)\n+        else:\n+            assert_close(z, golden_z, rtol=0.01, atol=0.01)\n+    else:\n+        assert_close(z, golden_z, rtol=0.001, atol=0.001)\n+\n+\n+reduce2d_configs = [\n+    (op, dtype, shape, axis)\n+    for op in ['sum', 'min', 'max']\n+    for dtype in ['float16', 'float32', 'float64']\n+    for shape in [(1, 4), (1, 8), (1, 16), (1, 32), (2, 32), (4, 32), (4, 128), (32, 64)]\n+    for axis in [0, 1]\n+]\n+\n+\n+@pytest.mark.parametrize('op, dtype, shape, axis', reduce2d_configs)\n+def test_reduce2d(op, dtype, shape, axis):\n+    dtype = dtype_mapping[dtype]\n+    x = torch.randn(shape, device='cuda', dtype=dtype)\n+    reduced_shape = (shape[1 - axis],)\n+    z = torch.empty(reduced_shape, device=x.device, dtype=dtype)\n+\n+    kernel = patch_kernel(reduce2d_kernel, {'OP': op})\n+    grid = (1,)\n+    kernel[grid](x_ptr=x, z_ptr=z, axis=axis, block_m=shape[0], block_n=shape[1])\n+\n+    if op == 'sum':\n+        golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=dtype)\n+    elif op == 'min':\n+        golden_z = torch.min(x, dim=axis, keepdim=False)[0]\n+    else:\n+        golden_z = torch.max(x, dim=axis, keepdim=False)[0]\n+\n+    if op == 'sum':\n+        if shape[axis] >= 256:\n+            assert_close(z, golden_z, rtol=0.05, atol=0.1)\n+        elif shape[axis] >= 32:\n+            assert_close(z, golden_z, rtol=0.05, atol=0.02)\n+        else:\n+            assert_close(z, golden_z, rtol=0.01, atol=0.01)\n+    else:\n+        assert_close(z, golden_z, rtol=0.001, atol=0.001)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "file_content_changes": "@@ -36,6 +36,7 @@ def str_to_ty(name):\n         \"bf16\": triton.language.bfloat16,\n         \"fp32\": triton.language.float32,\n         \"fp64\": triton.language.float64,\n+        \"i1\": triton.language.int1,\n         \"i8\": triton.language.int8,\n         \"i16\": triton.language.int16,\n         \"i32\": triton.language.int32,\n@@ -45,7 +46,6 @@ def str_to_ty(name):\n         \"u32\": triton.language.uint32,\n         \"u64\": triton.language.uint64,\n         \"B\": triton.language.int1,\n-        \"i1\": triton.language.int1,\n     }\n     return tys[name]\n \n@@ -875,7 +875,7 @@ def optimize_tritongpu_ir(mod, num_stages):\n     pm.enable_debug()\n     # Get error in backend due to wrong conversion in expanding async-related instruction.\n     # TODO[Superjomn]: Open it when fixed.\n-    # pm.add_tritongpu_pipeline_pass(num_stages)\n+    pm.add_tritongpu_pipeline_pass(num_stages)\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n     pm.add_coalesce_pass()\n@@ -888,6 +888,13 @@ def optimize_tritongpu_ir(mod, num_stages):\n     return mod\n \n \n+def add_external_libs(mod, libs):\n+    for name, path in libs.items():\n+        if len(name) == 0 or len(path) == 0:\n+            return\n+    _triton.add_external_libs(mod, list(libs.keys()), list(libs.values()))\n+\n+\n def make_llvm_ir(mod):\n     return _triton.translate_triton_gpu_to_llvmir(mod)\n \n@@ -986,6 +993,8 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     module = optimize_tritongpu_ir(module, num_stages)\n     if output == \"ttgir\":\n         return module.str()\n+    if extern_libs:\n+        add_external_libs(module, extern_libs)\n \n     # llvm-ir\n     llvm_ir = make_llvm_ir(module)"}, {"filename": "python/triton/language/libdevice.10.bc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -226,7 +226,6 @@ def fdiv(input: tl.tensor,\n         raise ValueError(\"both operands of fdiv must have floating poscalar type\")\n     input, other = binary_op_type_checking_impl(input, other, builder, False, False, False, True)\n     ret = builder.create_fdiv(input.handle, other.handle)\n-    ret.set_fdiv_ieee_rounding(ieee_rounding)\n     return tl.tensor(ret, input.type)\n \n \n@@ -1074,7 +1073,8 @@ def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n \n def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     x, y = binary_op_type_checking_impl(x, y, builder)\n-    return tl.tensor(builder.create_umulhi(x.handle, y.handle), x.type)\n+    from . import libdevice\n+    return libdevice.mulhi(x, y, _builder=builder)\n \n \n def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "file_content_changes": "@@ -326,7 +326,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #shared0 = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem\n+  // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_alloc_tensor\n   func @basic_alloc_tensor() {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -343,7 +343,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #shared0 = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem\n+  // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_extract_slice\n   func @basic_extract_slice() {\n     // CHECK: %[[BASE0:.*]] = llvm.mlir.addressof @global_smem\n@@ -382,10 +382,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #slice2d1 = #triton_gpu.slice<{dim = 1, parent=#block2}>\n #slice3d0 = #triton_gpu.slice<{dim = 0, parent=#block3}>\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#A = #triton_gpu.shared<{vec = 4, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v4\n-  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -404,9 +404,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n-    // CHECK-SAME: cp.async.cg.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x80, 0x80\n+    // CHECK-SAME: cp.async.cg.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x10, 0x10\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n-    // CHECK-SAME: cp.async.cg.shared.global.L2::evict_normal [ ${{.*}} + 8 ], [ ${{.*}} + 0 ], 0x80, 0x80\n+    // CHECK-SAME: cp.async.cg.shared.global [ ${{.*}} + 16 ], [ ${{.*}} + 0 ], 0x10, 0x10\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f32>, #AL> -> tensor<2x16x64xf32, #A>\n@@ -445,13 +445,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x32x!tt.ptr<f32>, #AL> -> tensor<2x16x32xf32, #A>\n@@ -489,21 +489,21 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n@@ -545,7 +545,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [0, 1]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<1088 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked\n   func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -593,7 +593,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [16, 2], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<1280 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_vec\n   func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -617,7 +617,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<640 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_multi_rep\n   func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -682,7 +682,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n #mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<2560 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mma_block\n   func @convert_layout_mma_blocked(%arg0: tensor<32x16xf32, #mma>) {\n     // CHECK: nvvm.barrier0\n@@ -703,7 +703,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<16384 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_shared\n   func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: llvm.store"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -22,7 +22,7 @@\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n // CHECK: %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]], %[[CONSTANT_0]]\n // CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n-// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n@@ -78,7 +78,7 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n // CHECK:   %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]], %[[CONSTANT_0]]\n // CHECK:   %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n-// CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n+// CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:     tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n@@ -131,7 +131,7 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n // CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK: triton_gpu.async_wait {num = 1 : i32}\n // CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n-// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   tt.dot {{.*}}, %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]"}]