[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUDialect.td", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -23,14 +23,16 @@ def TritonGPU_Dialect : Dialect {\n   let extraClassDeclaration = [{\n     static std::string getNumWarpsAttrName() { return \"triton_gpu.num-warps\"; }\n     static int getNumWarps(ModuleOp mod) {\n-      if(!mod->hasAttr(\"triton_gpu.num-warps\"))\n+      Attribute numWarps = mod->getDiscardableAttr(\"triton_gpu.num-warps\");\n+      if(!numWarps)\n         llvm::report_fatal_error(\n             \"TritonGPU module should contain a triton_gpu.num-warps attribute\");\n-      return mod->getAttr(\"triton_gpu.num-warps\").cast<IntegerAttr>().getInt();\n+      return numWarps.cast<IntegerAttr>().getInt();\n     }\n   }];\n \n   let useDefaultAttributePrinterParser = 1;\n+  let usePropertiesForAttributes = 1;\n }\n \n #endif"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -111,15 +111,15 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n       }\n     }\n   } else if (Operation *op = value.getDefiningOp()) {\n-    if (Attribute attr = op->getAttr(\"tt.divisibility\")) {\n+    if (Attribute attr = op->getDiscardableAttr(\"tt.divisibility\")) {\n       auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n       knownDivisibility = DimVectorT(vals.begin(), vals.end());\n     }\n-    if (Attribute attr = op->getAttr(\"tt.contiguity\")) {\n+    if (Attribute attr = op->getDiscardableAttr(\"tt.contiguity\")) {\n       auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n       knownContiguity = DimVectorT(vals.begin(), vals.end());\n     }\n-    if (Attribute attr = op->getAttr(\"tt.constancy\")) {\n+    if (Attribute attr = op->getDiscardableAttr(\"tt.constancy\")) {\n       auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n       knownConstancy = DimVectorT(vals.begin(), vals.end());\n     }\n@@ -888,15 +888,15 @@ void AxisInfoAnalysis::visitOperation(\n   auto newContiguity = curr.getContiguity();\n   auto newDivisibility = curr.getDivisibility();\n   auto newConstancy = curr.getConstancy();\n-  if (Attribute attr = op->getAttr(\"tt.contiguity\")) {\n+  if (Attribute attr = op->getDiscardableAttr(\"tt.contiguity\")) {\n     auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n     newContiguity = AxisInfo::DimVectorT(vals.begin(), vals.end());\n   }\n-  if (Attribute attr = op->getAttr(\"tt.divisibility\")) {\n+  if (Attribute attr = op->getDiscardableAttr(\"tt.divisibility\")) {\n     auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n     newDivisibility = AxisInfo::DimVectorT(vals.begin(), vals.end());\n   }\n-  if (Attribute attr = op->getAttr(\"tt.constancy\")) {\n+  if (Attribute attr = op->getDiscardableAttr(\"tt.constancy\")) {\n     auto vals = attr.cast<DenseElementsAttr>().getValues<int>();\n     newConstancy = AxisInfo::DimVectorT(vals.begin(), vals.end());\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -118,9 +118,8 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<triton::FuncOp> {\n     // Create an LLVM function, use external linkage by default until MLIR\n     // functions have linkage.\n     LLVM::Linkage linkage = LLVM::Linkage::External;\n-    if (funcOp->hasAttr(\"llvm.linkage\")) {\n-      auto attr =\n-          funcOp->getAttr(\"llvm.linkage\").dyn_cast<mlir::LLVM::LinkageAttr>();\n+    if (auto linkageAttr = funcOp->getDiscardableAttr(\"llvm.linkage\")) {\n+      auto attr = linkageAttr.dyn_cast<mlir::LLVM::LinkageAttr>();\n       if (!attr) {\n         funcOp->emitError()\n             << \"Contains llvm.linkage attribute not of type LLVM::LinkageAttr\";"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 15, "deletions": 5, "changes": 20, "file_content_changes": "@@ -25,9 +25,19 @@ namespace ttg = triton::gpu;\n \n // pass named attrs (e.g., tt.contiguity) from Triton to Triton\n static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n-  for (const NamedAttribute attr : dictAttrs.getValue())\n-    if (!op->hasAttr(attr.getName()))\n-      op->setAttr(attr.getName(), attr.getValue());\n+  NamedAttrList attrs = op->getDiscardableAttrs();\n+  // Collect the attributes to propagate: the ones in dictAttrs and not yet on\n+  // the operation.\n+  SmallVector<NamedAttribute> toPropagate;\n+  for (const NamedAttribute attr : dictAttrs.getValue()) {\n+    if (!attrs.get(attr.getName()))\n+      toPropagate.push_back(attr);\n+  }\n+  // If we found any, let's set them here as a single step.\n+  if (toPropagate.size()) {\n+    attrs.append(toPropagate);\n+    op->setDiscardableAttrs(attrs);\n+  }\n }\n \n #define int_attr(num) builder.getI64IntegerAttr(num)\n@@ -428,7 +438,7 @@ void LoopPipeliner::emitPrologue() {\n               lookupOrDefault(loadOp.getOther(), stage),\n               loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n               loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n-          addNamedAttrs(newOp, op->getAttrDictionary());\n+          addNamedAttrs(newOp, op->getDiscardableAttrDictionary());\n         } else {\n           newOp = builder.clone(*op);\n         }\n@@ -652,7 +662,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n             nextMapping.lookupOrDefault(loadOp.getOther()),\n             loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n             loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n-        addNamedAttrs(nextOp, op->getAttrDictionary());\n+        addNamedAttrs(nextOp, op->getDiscardableAttrDictionary());\n         nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n       } else {\n         nextOp = builder.clone(*op, nextMapping);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 9, "deletions": 49, "changes": 58, "file_content_changes": "@@ -349,27 +349,23 @@ class RematerializeForward : public mlir::RewritePattern {\n     SetVector<Operation *> cvtSlices;\n     auto filter = [&](Operation *op) {\n       return op->getBlock() == cvt->getBlock() &&\n+             !isa<triton::gpu::ConvertLayoutOp, scf::YieldOp>(op) &&\n              !(isa<triton::ReduceOp>(op) &&\n-               !op->getResult(0).getType().isa<RankedTensorType>()) &&\n-             !isa<triton::gpu::ConvertLayoutOp>(op) && !isa<scf::YieldOp>(op);\n+               !op->getResult(0).getType().isa<RankedTensorType>());\n     };\n     mlir::getForwardSlice(cvt.getResult(), &cvtSlices, filter);\n-    if (cvtSlices.empty()) {\n+    if (cvtSlices.empty())\n       return failure();\n-    }\n \n-    llvm::MapVector<Value, Attribute> toConvert;\n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensiveToRemat(op, dstEncoding)) {\n+      if (expensiveToRemat(op, dstEncoding))\n         return failure();\n-      }\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n           !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp>(op) && !isa<triton::ReduceOp>(op)) {\n+          !isa<triton::StoreOp, triton::ReduceOp>(op))\n         return failure();\n-      }\n       // don't rematerialize if it adds an extra conversion that can't\n       // be removed\n       for (Value arg : op->getOperands()) {\n@@ -380,9 +376,8 @@ class RematerializeForward : public mlir::RewritePattern {\n         int numAddedConvs = simulateBackwardRematerialization(\n             argOp, processed, layout, toConvert, srcEncoding);\n         if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n-            cvtSlices.count(argOp) == 0 && numAddedConvs > 0) {\n+            cvtSlices.count(argOp) == 0 && numAddedConvs > 0)\n           return failure();\n-        }\n       }\n     }\n \n@@ -425,7 +420,6 @@ class RematerializeBackward : public mlir::RewritePattern {\n     SetVector<Operation *> processed;\n     SetVector<Attribute> layout;\n     llvm::MapVector<Value, Attribute> toConvert;\n-    std::vector<std::pair<Operation *, Attribute>> queue;\n     if (simulateBackwardRematerialization(cvt, processed, layout, toConvert,\n                                           targetType.getEncoding()) > 0)\n       return mlir::failure();\n@@ -507,49 +501,15 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n     auto forOp = cast<scf::ForOp>(op);\n     auto iterArgs = forOp.getRegionIterArgs();\n     for (const auto &iterArg : llvm::enumerate(iterArgs)) {\n-      // if (iterArg.index() != 1)\n-      //   continue;\n       // skip non-tensor types\n       if (!iterArg.value().getType().isa<RankedTensorType>())\n         continue;\n-      // we only move `iterArg` out of the loop if\n-      //   - there is only a single conversion use\n-      //   - moving this conversion out of the loop will not generate\n-      //     any extra non-removable conversion\n-      auto users = iterArg.value().getUsers();\n-      // check first condition\n-      SetVector<Type> cvtTargetTypes;\n-      for (auto user : users) {\n-        if (isa<triton::gpu::ConvertLayoutOp>(user)) {\n-          auto newType =\n-              user->getResults()[0].getType().cast<RankedTensorType>();\n-          auto oldType = user->getOperand(0).getType().cast<RankedTensorType>();\n-          if (oldType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n-              newType.getEncoding()\n-                  .isa<triton::gpu::DotOperandEncodingAttr>()) {\n-            continue;\n-          }\n-          if (newType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n-            if (newType.getEncoding()\n-                    .cast<triton::gpu::SharedEncodingAttr>()\n-                    .getVec() == 1)\n-              continue;\n-          }\n-          cvtTargetTypes.insert(newType);\n-        }\n-      }\n-      if (cvtTargetTypes.size() != 1)\n+      SmallVector<Operation *> cvts;\n+      if (canMoveOutOfLoop(iterArg.value(), cvts).failed())\n         continue;\n-      // TODO: check second condition\n-      for (auto user : users) {\n-        if (isa<triton::gpu::ConvertLayoutOp>(user))\n-          continue;\n-      }\n       // check\n-      for (auto op : iterArg.value().getUsers()) {\n+      for (auto *op : cvts) {\n         auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(op);\n-        if (!cvt)\n-          continue;\n         auto targetType = op->getResultTypes()[0].cast<RankedTensorType>();\n         auto newFor = rematerializeForLoop(rewriter, forOp, iterArg.index(),\n                                            targetType, cvt);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 60, "deletions": 0, "changes": 60, "file_content_changes": "@@ -269,4 +269,64 @@ void rematerializeConversionChain(\n   }\n }\n \n+LogicalResult canMoveOutOfLoop(BlockArgument arg,\n+                               SmallVector<Operation *> &cvts) {\n+  auto parentOp = arg.getOwner()->getParentOp();\n+  // Don't move if arg is defined in a while loop\n+  if (isa<scf::WhileOp>(parentOp))\n+    return failure();\n+  // Skip if arg is not defined in scf.for\n+  if (!isa<scf::ForOp>(parentOp))\n+    return success();\n+  auto forOp = cast<scf::ForOp>(parentOp);\n+  // We only move `iterArg` out of the loop if\n+  // 1. There is no conversion\n+  // 2. There is only a single conversion\n+  // 3. Moving this conversion out of the loop will not generate any extra\n+  // non-removable conversion\n+  DenseSet<Type> cvtTypes;\n+  SetVector<Operation *> others;\n+  auto oldType = arg.getType().cast<RankedTensorType>();\n+  for (auto user : arg.getUsers()) {\n+    if (isa<triton::gpu::ConvertLayoutOp>(user)) {\n+      // Don't move if the conversion target is a dot operand or shared memory\n+      auto newType = user->getResults()[0].getType().cast<RankedTensorType>();\n+      if (oldType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n+          newType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n+        continue;\n+      }\n+      if (newType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n+        if (newType.getEncoding()\n+                .cast<triton::gpu::SharedEncodingAttr>()\n+                .getVec() == 1)\n+          continue;\n+      }\n+      cvts.emplace_back(user);\n+      cvtTypes.insert(newType);\n+    } else\n+      others.insert(user);\n+  }\n+  // First condition\n+  if (cvts.empty())\n+    return success();\n+  if (cvtTypes.size() == 1) {\n+    // Second condition\n+    if (others.empty())\n+      return success();\n+    // Third condition: not complete\n+    // If the other or the cvt is in the different block, we cannot push the\n+    // conversion forward or backward\n+    for (auto *cvt : cvts) {\n+      if (cvt->getBlock() != forOp.getBody())\n+        return failure();\n+    }\n+    for (auto *other : others) {\n+      if (other->getBlock() != forOp.getBody())\n+        return failure();\n+    }\n+    return success();\n+  }\n+  return failure();\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -16,6 +16,8 @@ bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n \n bool expensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n+// skipInit is True when we only consider the operands of the initOp but\n+// not the initOp itself.\n int simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n@@ -28,6 +30,10 @@ void rematerializeConversionChain(\n     const llvm::MapVector<Value, Attribute> &toConvert,\n     mlir::PatternRewriter &rewriter, SetVector<Operation *> &processed,\n     IRMapping &mapping);\n+\n+LogicalResult canMoveOutOfLoop(BlockArgument arg,\n+                               SmallVector<Operation *> &cvts);\n+\n } // namespace mlir\n \n #endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 8, "deletions": 11, "changes": 19, "file_content_changes": "@@ -154,24 +154,21 @@ static std::map<std::string, std::string> getExternLibs(mlir::ModuleOp module) {\n       funcs.push_back(func);\n   });\n \n-  for (auto &func : funcs) {\n-    if (func.getOperation()->hasAttr(\"libname\")) {\n-      auto name =\n-          func.getOperation()->getAttr(\"libname\").dyn_cast<StringAttr>();\n-      auto path =\n-          func.getOperation()->getAttr(\"libpath\").dyn_cast<StringAttr>();\n+  for (LLVM::LLVMFuncOp func : funcs) {\n+    if (auto libnameAttr = func->getDiscardableAttr(\"libname\")) {\n+      auto name = libnameAttr.dyn_cast<StringAttr>();\n+      auto path = func.getOperation()\n+                      ->getDiscardableAttr(\"libpath\")\n+                      .dyn_cast<StringAttr>();\n       if (name) {\n         std::string libName = name.str();\n         externLibs[libName] = path.str();\n       }\n     }\n   }\n \n-  if (module.getOperation()->hasAttr(\"triton_gpu.externs\")) {\n-    auto dict = module.getOperation()\n-                    ->getAttr(\"triton_gpu.externs\")\n-                    .dyn_cast<DictionaryAttr>();\n-    for (auto &attr : dict) {\n+  if (auto externsAttr = module->getDiscardableAttr(\"triton_gpu.externs\")) {\n+    for (auto &attr : externsAttr.cast<DictionaryAttr>()) {\n       externLibs[attr.getName().strref().trim().str()] =\n           attr.getValue().dyn_cast<StringAttr>().strref().trim().str();\n     }"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 68, "deletions": 28, "changes": 96, "file_content_changes": "@@ -1969,6 +1969,23 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n     assert torch.all(out_dynamic == 2)\n \n \n+@pytest.mark.parametrize(\"literal, dtype_str\",\n+                         [(1e+50, \"f64\"), (1e+10, \"f32\"), (1.0, \"f32\"),\n+                          ('float(\"inf\")', \"f32\"), ('float(\"-inf\")', \"f32\"),\n+                          ('float(\"nan\")', \"f32\"), ('float(\"-nan\")', \"f32\"),\n+                          (0., \"f32\"),\n+                          (5, \"i32\"), (2**40, \"i64\"),])\n+def test_constexpr(literal, dtype_str):\n+    @triton.jit\n+    def kernel(out_ptr):\n+        val = GENERATE_TEST_HERE\n+        tl.store(out_ptr.to(tl.pointer_type(val.dtype)), val)\n+\n+    kernel_patched = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{literal}\"})\n+    out = torch.zeros((1,), dtype=torch.float32, device=\"cuda\")\n+    h = kernel_patched[(1,)](out)\n+    assert re.search(r\"arith.constant .* : \" + dtype_str, h.asm[\"ttir\"]) is not None\n+\n # TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n # @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n # def test_dot_without_load(dtype_str):\n@@ -2639,41 +2656,64 @@ def add_fn_static_cond(x, cond: tl.constexpr):\n         return x + 1\n \n \n-@pytest.mark.parametrize(\"call_type\", [\"attribute\", \"jit_function\", \"jit_function_return\",\n-                                       \"ifexp\", \"expr\", \"jit_function_static_cond\", \"jit_function_noinline\"])\n+@pytest.mark.parametrize(\"call_type\", [\"attribute\", \"attribute_jit\",\n+                                       \"jit\", \"jit_if\", \"jit_ifexp\", \"jit_expr\",\n+                                       \"jit_static_cond\", \"jit_noinline\", \"jit_extern\"])\n def test_if_call(call_type):\n     @triton.jit\n     def kernel(Out, call_type: tl.constexpr):\n         pid = tl.program_id(0)\n         o = tl.load(Out)\n-        if pid == 0:\n-            if call_type == \"attribute\":\n-                # call attribute\n-                a = o + 1\n-                a = a.to(tl.int32).to(tl.int32)\n+        if call_type == \"attribute\":\n+            # call attribute\n+            if pid == 0:\n+                a = o\n+                a = a.to(tl.int32).to(tl.int32) + 1\n                 o = a\n-            else:\n+        elif call_type == \"attribute_jit\":\n+            # call attribute and jit function\n+            if pid == 0:\n+                a = o\n+                a = tl.load(Out + add_fn(a) - 1).to(tl.int32) + 1\n+                o = a\n+        elif call_type == \"jit\":\n+            if pid == 0:\n+                # regular function call\n+                a = o\n+                a = add_fn(a)\n+                o = a\n+        elif call_type == \"jit_if\":\n+            # function without end_if block\n+            if pid == 0:\n+                a = o\n+                a = add_fn_return(a, pid)\n+                o = a\n+        elif call_type == \"jit_ifexp\":\n+            # ifexp expression\n+            if pid == 0:\n                 a = o\n-                if call_type == \"jit_function\":\n-                    # regular function call\n-                    a = add_fn(a)\n-                elif call_type == \"jit_function_return\":\n-                    # function without end_if block\n-                    a = add_fn_return(a, pid)\n-                elif call_type == \"ifexp\":\n-                    # ifexp expression\n-                    a = add_fn(a) if pid == 0 else add_fn_return(a, pid)\n-                elif call_type == \"expr\":\n-                    if pid == 1:\n-                        return\n-                    a = add_fn(a)\n-                    if pid == 0:\n-                        # call without return\n-                        add_fn_expr(Out, a)\n-                elif call_type == \"jit_function_static_cond\":\n-                    a = add_fn_static_cond(a, call_type)\n-                elif call_type == \"jit_function_noinline\":\n-                    a = add_fn_noinline(a)\n+                a = add_fn(a) if pid == 0 else add_fn_return(a, pid)\n+                o = a\n+        elif call_type == \"jit_expr\":\n+            # call without return\n+            if pid == 0:\n+                a = o + 1\n+                add_fn_expr(Out, a)\n+                o = a\n+        elif call_type == \"jit_static_cond\":\n+            if pid == 0:\n+                a = o + 1\n+                add_fn_static_cond(o, call_type)\n+                o = a\n+        elif call_type == \"jit_noinline\":\n+            if pid == 0:\n+                a = o + 1\n+                add_fn_noinline(a)\n+                o = a\n+        elif call_type == \"jit_extern\":\n+            if pid == 0:\n+                a = o + 1\n+                tl.cdiv(a, a)\n                 o = a\n \n         tl.store(Out, o)"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 92, "deletions": 58, "changes": 150, "file_content_changes": "@@ -97,6 +97,97 @@ def __exit__(self, *args, **kwargs):\n         self.generator.local_defs = self.prev_defs\n \n \n+# Check if the given syntax node has an \"early\" return\n+class ContainsReturnChecker(ast.NodeVisitor):\n+    def __init__(self, gscope):\n+        self.gscope = gscope\n+\n+    def _visit_stmts(self, body) -> bool:\n+        for s in body:\n+            if self.visit(s):\n+                return True\n+        return False\n+\n+    def _visit_function(self, fn) -> bool:\n+        # Currently we only support JITFunctions defined in the global scope\n+        if isinstance(fn, JITFunction) and not fn.noinline:\n+            fn_node = fn.parse()\n+            return ContainsReturnChecker(self.gscope).visit(fn_node)\n+        return False\n+\n+    def generic_visit(self, node) -> bool:\n+        ret = False\n+        for _, value in ast.iter_fields(node):\n+            if isinstance(value, list):\n+                for item in value:\n+                    if isinstance(item, ast.AST):\n+                        ret = ret or self.visit(item)\n+            elif isinstance(value, ast.AST):\n+                ret = ret or self.visit(value)\n+        return ret\n+\n+    def visit_Attribute(self, node: ast.Attribute) -> bool:\n+        # If the left part is a name, it's possible that\n+        # we call triton native function or a jit function from another module.\n+        # If the left part is not a name, it must return a tensor or a constexpr\n+        # whose methods do not contain return statements\n+        # e.g., (tl.load(x)).to(y)\n+        # So we only check if the expressions within value have return or not\n+        if isinstance(node.value, ast.Name):\n+            if node.value.id in self.gscope:\n+                value = self.gscope[node.value.id]\n+                fn = getattr(value, node.attr)\n+                return self._visit_function(fn)\n+            return False\n+        return self.visit(node.value)\n+\n+    def visit_Name(self, node: ast.Name) -> bool:\n+        if type(node.ctx) == ast.Store:\n+            return False\n+        if node.id in self.gscope:\n+            fn = self.gscope[node.id]\n+            return self._visit_function(fn)\n+        return False\n+\n+    def visit_Return(self, node: ast.Return) -> bool:\n+        return True\n+\n+    def visit_Assign(self, node: ast.Assign) -> bool:\n+        # There couldn't be an early return\n+        # x = ...\n+        return False\n+\n+    def visit_AugAssign(self, node: ast.AugAssign) -> bool:\n+        # There couldn't be an early return\n+        # x += ...\n+        return False\n+\n+    def visit_Module(self, node: ast.Module) -> bool:\n+        return self._visit_stmts(node.body)\n+\n+    def visit_FunctionDef(self, node: ast.FunctionDef) -> bool:\n+        return self._visit_stmts(node.body)\n+\n+    def visit_If(self, node: ast.If) -> bool:\n+        # TODO: optimize the following case in which we actually don't have\n+        # a return when static_cond is false:\n+        # if dynamic_cond\n+        #   if static_cond\n+        #     func_with_return\n+        #   else\n+        #     func_without_return\n+        ret = self._visit_stmts(node.body)\n+        if node.orelse:\n+            ret = ret or self._visit_stmts(node.orelse)\n+        return ret\n+\n+    def visit_IfExp(self, node: ast.IfExp) -> bool:\n+        return self.visit(node.body) or self.visit(node.orelse)\n+\n+    def visit_Call(self, node: ast.Call) -> bool:\n+        return self.visit(node.func)\n+\n+\n class CodeGenerator(ast.NodeVisitor):\n     def __init__(self, context, prototype, gscope, attributes, constants, function_name,\n                  module=None, is_kernel=False, function_types: Optional[Dict] = None,\n@@ -166,63 +257,6 @@ def visit_compound_statement(self, stmts):\n             if ret_type is not None and isinstance(stmt, ast.Return):\n                 self.last_ret_type = ret_type\n \n-    # TODO: should be its own AST visitor\n-    def contains_return_op(self, node):\n-        if isinstance(node, ast.Return):\n-            return True\n-        elif isinstance(node, ast.Assign):\n-            return self.contains_return_op(node.value)\n-        elif isinstance(node, ast.Module):\n-            pred = lambda s: self.contains_return_op(s)\n-            return any(pred(s) for s in node.body)\n-        elif isinstance(node, ast.FunctionDef):\n-            pred = lambda s: self.contains_return_op(s)\n-            return any(pred(s) for s in node.body)\n-        elif isinstance(node, ast.Call):\n-            def check_undefined_name(cur_node):\n-                # Check if name is an undefined local variable,\n-                # which can only be a tensor or a constexpr\n-                if isinstance(cur_node.func, ast.Attribute):\n-                    if isinstance(cur_node.func.value, ast.Name):\n-                        name = cur_node.func.value.id\n-                        if name not in self.lscope and name not in self.gscope:\n-                            return True\n-                        return False\n-                    # chain of calls\n-                    # e.g., tl.load(a).to(tl.float32)\n-                    return check_undefined_name(cur_node.func.value)\n-                return False\n-            if check_undefined_name(node):\n-                return False\n-            fn = self.visit(node.func)\n-            if isinstance(fn, JITFunction) and fn.noinline is not True:\n-                old_gscope = self.gscope\n-                self.gscope = sys.modules[fn.fn.__module__].__dict__\n-                ret = self.contains_return_op(fn.parse())\n-                self.gscope = old_gscope\n-                return ret\n-            return False\n-        elif isinstance(node, ast.If):\n-            pred = lambda s: self.contains_return_op(s)\n-            ret = any(pred(s) for s in node.body)\n-            if node.orelse:\n-                ret = ret or any(pred(s) for s in node.orelse)\n-            return ret\n-        elif isinstance(node, ast.IfExp):\n-            return self.contains_return_op(node.body) or self.contains_return_op(node.orelse)\n-        elif isinstance(node, ast.Expr):\n-            ret = False\n-            for _, value in ast.iter_fields(node):\n-                if isinstance(value, list):\n-                    for item in value:\n-                        if isinstance(item, ast.AST):\n-                            ret = ret or self.contains_return_op(item)\n-                elif isinstance(value, ast.AST):\n-                    ret = ret or self.contains_return_op(value)\n-            return ret\n-        else:\n-            return False\n-\n     def visit_Module(self, node):\n         ast.NodeVisitor.generic_visit(self, node)\n \n@@ -526,7 +560,7 @@ def visit_If(self, node):\n         cond = self.visit(node.test)\n         if _is_triton_tensor(cond):\n             cond = cond.to(language.int1, _builder=self.builder)\n-            if self.scf_stack or not self.contains_return_op(node):\n+            if self.scf_stack or not ContainsReturnChecker(self.gscope).visit(node):\n                 self.visit_if_scf(cond, node)\n             else:\n                 self.visit_if_top_level(cond, node)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "file_content_changes": "@@ -55,7 +55,17 @@ def _to_tensor(x, builder):\n         else:\n             raise RuntimeError(f'Nonrepresentable integer {x}.')\n     elif isinstance(x, float):\n-        return tensor(builder.get_fp32(x), float32)\n+        min_float32 = 2 ** -126\n+        max_float32 = (2 - 2**-23) * 2**127\n+        abs_x = __builtins__['abs'](x)\n+        if abs_x == float(\"inf\") or\\\n+           abs_x == 0.0 or \\\n+           x != x or \\\n+           min_float32 <= abs_x <= max_float32:\n+            return tensor(builder.get_fp32(x), float32)\n+        else:\n+            return tensor(builder.get_fp64(x), float64)\n+\n     elif isinstance(x, constexpr):\n         return _to_tensor(x.value, builder)\n     elif isinstance(x, tensor):"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "@@ -267,6 +267,58 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n   tt.return\n }\n \n+// CHECK-LABEL: loop_if\n+tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n+  %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n+  %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n+  %c1 = arith.constant 1 : index\n+  %c32 = arith.constant 32 : index\n+  %c0 = arith.constant 0 : index\n+  %i0 = arith.constant 0 : i32\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n+  %00 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice1dim1>\n+  %01 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #slice2dim0>\n+  %1 = tt.expand_dims %00 {axis = 1 : i32} : (tensor<64xi32, #slice1dim1>) -> tensor<64x1xi32, #blocked1>\n+  %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n+  %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n+  %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n+  %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n+  %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+  %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n+    %33 = \"triton_gpu.cmpi\"(%i0, %i0) {predicate = 4 : i64} : (i32, i32) -> i1\n+    %34 = scf.if %33 -> (tensor<64x64xf32, #blocked1>) {\n+      %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n+      %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n+      %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n+      %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n+      %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n+      scf.yield %27 : tensor<64x64xf32, #blocked1>\n+    } else {\n+      scf.yield %arg6 : tensor<64x64xf32, #blocked1>\n+    }\n+    %28 = arith.addf %arg6, %34 : tensor<64x64xf32, #blocked1>\n+    %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+    scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  }\n+  %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n+  %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n+  %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n+  %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+  %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n+  %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n+  %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n+  %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n+  tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n+  tt.return\n+}\n+\n // CHECK-LABEL: vecadd\n tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout"}]