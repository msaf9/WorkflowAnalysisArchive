[{"filename": ".github/workflows/documentation.yml", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -18,6 +18,11 @@ jobs:\n         with:\n           ref: 'gh-pages'\n \n+      - name: Clear docs\n+        run: |\n+          rm -r /tmp/triton-docs\n+        continue-on-error: true\n+\n       - name: Checkout branch\n         uses: actions/checkout@v1\n \n@@ -31,7 +36,6 @@ jobs:\n         run: |\n           git branch\n           # update docs\n-          rm -r /tmp/triton-docs; \n           mkdir /tmp/triton-docs;\n           mv docs/_build/html/* /tmp/triton-docs/\n           git checkout gh-pages"}, {"filename": "include/triton/codegen/selection/generator.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -4,6 +4,7 @@\n #define _TRITON_SELECTION_GENERATOR_H_\n \n #include \"triton/ir/visitor.h\"\n+#include \"triton/ir/instructions.h\"\n #include \"triton/codegen/analysis/layout.h\"\n #include <functional>\n \n@@ -261,6 +262,9 @@ class generator: public ir::visitor, public analysis::layout_visitor {\n \n   /// Record prefetch instrs that needs to be moved\n   std::map<ir::value*, std::vector<Value*>> prefetch_latch_to_bb_;\n+\n+  // Eviction policies\n+  std::map<ir::load_inst::EVICTION_POLICY, Value*> policies_;\n };\n \n }"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 23, "deletions": 3, "changes": 26, "file_content_changes": "@@ -91,6 +91,7 @@ Value* geper::operator()(Value *ptr, Value* off, const std::string& name){\n #define i8_ty                builder_->getInt8Ty()\n #define i16_ty               builder_->getInt16Ty()\n #define i32_ty               builder_->getInt32Ty()\n+#define i64_ty               builder_->getInt64Ty()\n #define vec_ty(type, num_el) VectorType::get(type, num_el, false)\n #define ptr_ty(...)          PointerType::get(__VA_ARGS__)\n // constants\n@@ -783,6 +784,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     int tot_width = nbits*vec;\n     int width = std::min(tot_width, max_word_width);\n     int n_words = std::max(1, tot_width / width);\n+    bool has_evict_policy = x->get_eviction_policy() != ir::load_inst::NORMAL;\n     // -----\n     // create inline asm string\n     // -----\n@@ -794,8 +796,9 @@ void generator::visit_load_inst(ir::load_inst* x){\n     asm_oss << \".global\";\n     if (x->get_cache_modifier() == ir::load_inst::CA) asm_oss << \".ca\";\n     if (x->get_cache_modifier() == ir::load_inst::CG) asm_oss << \".cg\";\n-    if (x->get_eviction_policy() == ir::load_inst::EVICT_LAST) asm_oss << \".L1::evict_last\";\n     if (x->get_eviction_policy() == ir::load_inst::EVICT_FIRST) asm_oss << \".L1::evict_first\";\n+    if (x->get_eviction_policy() == ir::load_inst::EVICT_LAST) asm_oss << \".L1::evict_last\";\n+    if (has_evict_policy) asm_oss << \".L2::cache_hint\";\n     if(n_words > 1)\n       asm_oss << \".v\" << n_words; // vector width\n     asm_oss << \".b\" << width; // word size\n@@ -806,7 +809,9 @@ void generator::visit_load_inst(ir::load_inst* x){\n     }\n     asm_oss << \"}\";\n     asm_oss << \", [ $\" << n_words + 1; // load\n-    asm_oss << \" + \" << in_off << \"];\"; // constant offset\n+    asm_oss << \" + \" << in_off << \"]\"; // constant offset\n+    if (has_evict_policy) asm_oss << \", $\" << n_words + 2;\n+    asm_oss << \";\";\n     bool has_other = other && (other != UndefValue::get(other->getType()));\n     std::vector<Value *> others;\n     // handle `other` values for indices where the mask\n@@ -827,7 +832,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n       if(ConstantInt* cst = dyn_cast<ConstantInt>(v))\n         asm_oss << \"0x\" << std::hex << cst->getSExtValue();\n       else{\n-        asm_oss << \"$\" << n_words + 2 + ii;\n+        asm_oss << \"$\" << n_words + has_evict_policy +  2 + ii;\n         others.push_back(v);\n       }\n       asm_oss.flags(flags);\n@@ -842,6 +847,8 @@ void generator::visit_load_inst(ir::load_inst* x){\n     std::vector<Type*> arg_tys = {pred->getType(), ptr->getType()};\n     for(Value *v: others)\n         arg_tys.push_back(v->getType());\n+    if (has_evict_policy) \n+      arg_tys.push_back(i64_ty);\n     FunctionType *asm_ty = FunctionType::get(ret_ty, arg_tys, false);\n     // ---\n     // create inline ASM constraints\n@@ -856,13 +863,19 @@ void generator::visit_load_inst(ir::load_inst* x){\n       asm_cstrt += \",\";\n       asm_cstrt += (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n     }\n+    if (has_evict_policy) \n+      asm_cstrt += \",l\";\n     // ---\n     // finally call inline ASM\n     // ---\n     InlineAsm *inlineAsm = InlineAsm::get(asm_ty, asm_oss.str(), asm_cstrt, true);\n     std::vector<Value*> args = {pred, ptr};\n     for(Value *v: others)\n         args.push_back(v);\n+    if (has_evict_policy)\n+      args.push_back(policies_.at(x->get_eviction_policy()));\n+  \n+    \n     Value *_ret = call(inlineAsm, args);\n     // ---\n     // extract and store return values\n@@ -2890,6 +2903,13 @@ void generator::visit_function(ir::function* fn) {\n     bbs_[block] = dst_block;\n   }\n   builder_->SetInsertPoint(bbs_[fn->blocks()[0]]);\n+  // create policies\n+  for(ir::load_inst::EVICTION_POLICY evict: {ir::load_inst::EVICT_FIRST, ir::load_inst::EVICT_LAST}){\n+    std::string policy = (evict == ir::load_inst::EVICT_FIRST) ? \"evict_first\" : \"evict_last\";\n+    std::string asm_str = \"createpolicy.fractional.L2::\" + policy + \".b64 $0;\";\n+    InlineAsm* iasm = InlineAsm::get(FunctionType::get(i64_ty, {}), asm_str, \"=l\", false);\n+    policies_[evict] = call(iasm);\n+  }\n   // initialize layouts\n   for(auto x: layouts_->get_all()){\n     visit_layout(x.second);"}, {"filename": "lib/ir/builder.cc", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,4 +1,3 @@\n-#include <bits/types/clock_t.h>\n #include <string>\n #include <algorithm>\n #include <iostream>"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 37, "deletions": 21, "changes": 58, "file_content_changes": "@@ -1032,28 +1032,44 @@ def kernel(VALUE, X):\n             kernel[(1, )](value, x)\n     else:\n         kernel[(1, )](value, x)\n-# -------------------------\n-# test dynamic parallelism\n-# -------------------------\n \n \n-@triton.jit\n-def mult(x, alpha):\n-    tl.store(x + tl.program_id(0), alpha)\n+# ----------------\n+# test constexpr\n+# ----------------\n \n+@pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>'])\n+@pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n+@pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n+def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n \n-@triton.jit\n-def stub(X, alpha, grid_0, grid_1, grid_2):\n-    tl.launch(mult, [X, alpha], [grid_0, grid_1, grid_2])\n-\n-\n-# def test_dyn_par(cond=True, device='cuda'):\n-#     n_pids = 10\n-#     # pids = torch.arange(n_pids, device=device)\n-#     # alpha = 2.0\n-#     # x_ref = pids * alpha\n-#     x_tri = torch.full((10,), fill_value=-1., device=device)\n-#     # cond = torch.tensor([cond], device=device)\n-#     stub[(1,)](x_tri, 3.14, n_pids, 1, 1)\n-#     print(x_tri)\n-#     # triton.testing.assert_almost_equal(x_ref, x_tri)\n+    @triton.jit\n+    def kernel(Z, X, Y):\n+        x = tl.load(X)\n+        y = tl.load(Y)\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z, z)\n+\n+    x_str = \"3.14\" if is_lhs_constexpr else \"x\"\n+    y_str = \"4.13\" if is_rhs_constexpr else \"y\"\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n+    x = numpy_random((1,), dtype_str=\"float32\")\n+    y = numpy_random((1,), dtype_str=\"float32\")\n+    z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n+    x_tri = to_triton(x)\n+    y_tri = to_triton(y)\n+    z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n+    kernel[(1,)](z_tri, x_tri, y_tri)\n+    np.testing.assert_allclose(z, to_numpy(z_tri))\n+\n+\n+def test_constexpr_shape():\n+\n+    @triton.jit\n+    def kernel(X):\n+        off = tl.arange(0, 128 + 128)\n+        tl.store(X + off, off)\n+\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    kernel[(1,)](x_tri)\n+    np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 47, "deletions": 27, "changes": 74, "file_content_changes": "@@ -23,6 +23,13 @@\n from .tools.disasm import extract\n \n \n+def current_cuda_stream(device_idx=0):\n+    # Torch's torch.cuda.current_stream() is slow. We provide this\n+    # function to give the user an opportunity to monkey-patch their\n+    # own faster current stream lookup.\n+    return torch.cuda.current_stream().cuda_stream\n+\n+\n def mangle_ty(ty):\n     if ty.is_ptr():\n         return 'P' + mangle_ty(ty.element_ty)\n@@ -54,6 +61,7 @@ def mangle_fn(name, arg_tys, constants):\n     mangled_constants = '_'.join([f'{i}c{key(constants[i])}' for i in sorted(constants)])\n     mangled_constants = mangled_constants.replace('.', '_d_')\n     mangled_constants = mangled_constants.replace(\"'\", '_sq_')\n+    mangled_constants = mangled_constants.replace(\"e-\", '_em_')\n     ret = f'{name}__{mangled_arg_names}__{mangled_constants}'\n     return ret\n \n@@ -388,12 +396,14 @@ def visit_Tuple(self, node):\n         return tuple(args)\n \n     def visit_BinOp(self, node):\n+        # visit operand\n         lhs = self.visit(node.left)\n         rhs = self.visit(node.right)\n-        if isinstance(lhs, triton.language.constexpr):\n-            lhs = lhs.value\n-        if isinstance(rhs, triton.language.constexpr):\n-            rhs = rhs.value\n+        is_lhs_constexpr = isinstance(lhs, triton.language.constexpr)\n+        is_rhs_constexpr = isinstance(rhs, triton.language.constexpr)\n+        lhs = lhs.value if is_lhs_constexpr else lhs\n+        rhs = rhs.value if is_rhs_constexpr else rhs\n+        # get function name\n         fn = {\n             ast.Add: '__add__',\n             ast.Sub: '__sub__',\n@@ -408,6 +418,10 @@ def visit_BinOp(self, node):\n             ast.BitOr: '__or__',\n             ast.BitXor: '__xor__',\n         }[type(node.op)]\n+        # return a new constexpr if both arg are constexprs\n+        if is_lhs_constexpr and is_rhs_constexpr:\n+            return triton.language.constexpr(getattr(lhs, fn)(rhs))\n+        # call operator\n         if is_triton_tensor(lhs):\n             return getattr(lhs, fn)(rhs, _builder=self.builder)\n         elif is_triton_tensor(rhs):\n@@ -466,14 +480,16 @@ def visit_Compare(self, node):\n         assert len(node.ops) == 1\n         lhs = self.visit(node.left)\n         rhs = self.visit(node.comparators[0])\n-        if isinstance(lhs, triton.language.constexpr):\n-            lhs = lhs.value\n-        if isinstance(rhs, triton.language.constexpr):\n-            rhs = rhs.value\n+        is_lhs_constexpr = isinstance(lhs, triton.language.constexpr)\n+        is_rhs_constexpr = isinstance(rhs, triton.language.constexpr)\n+        lhs = lhs.value if is_lhs_constexpr else lhs\n+        rhs = rhs.value if is_rhs_constexpr else rhs\n+        # handle `is`` and `is not``\n         if type(node.ops[0]) == ast.Is:\n             return triton.language.constexpr(lhs is rhs)\n         if type(node.ops[0]) == ast.IsNot:\n             return triton.language.constexpr(lhs is not rhs)\n+        # function name\n         fn = {\n             ast.Eq: '__eq__',\n             ast.NotEq: '__ne__',\n@@ -482,29 +498,32 @@ def visit_Compare(self, node):\n             ast.Gt: '__gt__',\n             ast.GtE: '__ge__',\n         }[type(node.ops[0])]\n+        # return a new constexpr if both arg are constexprs\n+        if is_lhs_constexpr and is_rhs_constexpr:\n+            return triton.language.constexpr(getattr(lhs, fn)(rhs))\n+        # call operator\n         if is_triton_tensor(lhs):\n             return getattr(lhs, fn)(rhs, _builder=self.builder)\n         elif is_triton_tensor(rhs):\n             fn = fn[:2] + 'r' + fn[2:]\n             return getattr(rhs, fn)(lhs, _builder=self.builder)\n         else:\n-            return getattr(lhs, fn)(rhs)\n+            assert False\n \n     def visit_UnaryOp(self, node):\n         op = self.visit(node.operand)\n         if type(node.op) == ast.Not:\n             assert isinstance(op, triton.language.constexpr), \"`not` only supported for constexpr at the moment\"\n             return triton.language.constexpr(not op)\n-        if isinstance(op, triton.language.constexpr):\n-            op = op.value\n         fn = {\n             ast.USub: '__neg__',\n             ast.UAdd: '__pos__',\n             ast.Invert: '__invert__',\n         }[type(node.op)]\n-        if is_triton_tensor(op):\n-            return getattr(op, fn)(_builder=self.builder)\n-        return getattr(op, fn)()\n+        if isinstance(op, triton.language.constexpr):\n+            return triton.language.constexpr(getattr(op.value, fn)())\n+        assert is_triton_tensor(op)\n+        return getattr(op, fn)(_builder=self.builder)\n \n     def visit_While(self, node):\n         current_bb = self.builder.get_insert_block()\n@@ -654,6 +673,10 @@ def visit_Call(self, node):\n             args = [arg.value if isinstance(arg, triton.language.constexpr) else arg\n                     for arg in args]\n             ret = fn(*args, **kws)\n+            if isinstance(ret, (bool, int, float)):\n+                ret = triton.language.core.constexpr(ret)\n+            else:\n+                ret = triton.language.core._to_tensor(ret, self.builder)\n         # special case: dynamic parallelism\n         # in this case the core primitive returns a proxy\n         # if isinstance(ret, triton.language.core.LaunchProxy):\n@@ -787,6 +810,7 @@ def __reduce__(self):\n \n \n class Kernel:\n+\n     @staticmethod\n     def _type_name(obj):\n         type_names = {\n@@ -915,28 +939,24 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n             raise TypeError(f\"Function takes {len(self.fn.arg_names)} positional arguments but {len(wargs)} were given\")\n         # handle annotations\n         for pos, _type in self.fn.annotations.items():\n+            assert _type == triton.language.constexpr, \"only constexpr annotations are supported for now\"\n             wargs[pos] = _type(wargs[pos])\n         # check that tensors are on GPU.\n         for arg in wargs:\n             if hasattr(arg, 'data_ptr'):\n                 assert arg.is_cuda, \"All tensors must be on GPU!\"\n-        # query device index and cuda stream\n+        # set device (i.e., make sure torch has the context initialized)\n         device = torch.cuda.current_device()\n         torch.cuda.set_device(device)\n+        # query compute capability\n         cc = torch.cuda.get_device_capability(device)\n         cc = str(cc[0]) + '-' + str(cc[1])\n-        # # query stream\n-        # # this is hacky but much faster than `torch.cuda.current_stream(device).cuda_stream`\n-        # # https://github.com/pytorch/pytorch/blob/master/c10/core/Stream.h#L154\n-        # # building a C wrapper to re-use the unpack function would add a build-time torch dependency\n-        # # and require different wheels for different torch versions -- undesirable!\n-        # bits = torch._C._cuda_getCurrentStream(device)\n-        # mask = 1 << 47\n-        # stream = ((bits & 0xFFFFFFFFFFFF) ^ mask) - mask\n-        stream = torch.cuda.current_stream(device).cuda_stream\n-        # make key for cache\n-        return _triton.runtime.launch(wargs, self.fn.do_not_specialize, self.fn.cache_key + cc, self.fn.arg_names, device, stream,\n-                                      self.fn.bin_cache, num_warps, num_stages, self.add_to_cache, grid)\n+        cache_key = self.fn.cache_key + cc\n+        # query current stream\n+        stream = current_cuda_stream(device)\n+        return _triton.runtime.launch(wargs, self.fn.do_not_specialize, cache_key, self.fn.arg_names,\n+                                      device, stream, self.fn.bin_cache, num_warps, num_stages, self.add_to_cache,\n+                                      grid)\n \n \n class Launcher:"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 9, "deletions": 62, "changes": 71, "file_content_changes": "@@ -337,68 +337,6 @@ def __init__(self, value):\n     def __repr__(self) -> str:\n         return f\"constexpr[{self.value}]\"\n \n-    def __add__(self, other):\n-        return self.value + other.value\n-\n-    def __radd__(self, other):\n-        return other.value + self.value\n-\n-    def __sub__(self, other):\n-        return self.value - other.value\n-\n-    def __rsub__(self, other):\n-        return other.value - self.value\n-\n-    def __mul__(self, other):\n-        return self.value * other.value\n-\n-    def __rmul__(self, other):\n-        return other.value * self.value\n-\n-    def __truediv__(self, other):\n-        return self.value / other.value\n-\n-    def __rtruediv__(self, other):\n-        return other.value / self.value\n-\n-    def __floordiv__(self, other):\n-        return self.value // other.value\n-\n-    def __rfloordiv__(self, other):\n-        return other.value // self.value\n-\n-    #\n-\n-    def __gt__(self, other):\n-        return self.value > other.value\n-\n-    def __rgt__(self, other):\n-        return other.value > self.value\n-\n-    def __ge__(self, other):\n-        return self.value >= other.value\n-\n-    def __rge__(self, other):\n-        return other.value >= self.value\n-\n-    def __lt__(self, other):\n-        return self.value < other.value\n-\n-    def __rlt__(self, other):\n-        return other.value < self.value\n-\n-    def __le__(self, other):\n-        return self.value <= other.value\n-\n-    def __rle__(self, other):\n-        return other.value <= self.value\n-\n-    def __eq__(self, other):\n-        return self.value == other.value\n-\n-    def __ne__(self, other):\n-        return self.value != other.value\n-\n     def __bool__(self):\n         return bool(self.value)\n \n@@ -442,6 +380,9 @@ def __init__(self, handle, type: dtype):\n         self.numel = 1\n         for s in self.shape:\n             self.numel *= s\n+        is_pow2 = (self.numel and (not(self.numel & (self.numel - 1))))\n+        if not is_pow2:\n+            raise ValueError(\"Triton tensors must have a power-of-two number of elements\")\n         self.numel = constexpr(self.numel)\n         self.type = type  # Tensor type (can be block_type)\n         # Following the practice in pytorch, dtype is scalar type\n@@ -496,6 +437,11 @@ def __mod__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.mod(self, other, _builder)\n \n+    @builtin\n+    def __rmod__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.mod(other, self, _builder)\n+\n     # unary operators\n     @builtin\n     def __neg__(self, _builder=None):\n@@ -564,6 +510,7 @@ def __lt__(self, other, _builder=None):\n \n     @builtin\n     def __rlt__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n         return semantic.less_than(other, self, _builder)\n \n     # <="}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 76, "deletions": 0, "changes": 76, "file_content_changes": "@@ -1,6 +1,8 @@\n+import functools\n import os\n import subprocess\n import sys\n+from contextlib import contextmanager\n \n import torch\n \n@@ -358,6 +360,80 @@ def get_max_tensorcore_tflops(dtype: torch.dtype, backend=None, device=None, clo\n     tflops = num_subcores * clock_rate * ops_per_sub_core * 1e-9\n     return tflops\n \n+# create decorator that wraps test function into\n+# a cuda-memcheck system call\n+\n+\n+def cuda_memcheck(**target_kwargs):\n+    def decorator(test_fn):\n+        @functools.wraps(test_fn)\n+        def wrapper(*args, **kwargs):\n+            import psutil\n+            ppid_name = psutil.Process(os.getppid()).name()\n+            run_cuda_memcheck = target_kwargs.items() <= kwargs.items()\n+            if run_cuda_memcheck and ppid_name != \"cuda-memcheck\":\n+                path = os.path.realpath(test_fn.__globals__[\"__file__\"])\n+                # get path of current file\n+                env = {\"PATH\": os.environ[\"PATH\"], \"PYTORCH_NO_CUDA_MEMORY_CACHING\": \"1\"}\n+                assert 'request' in kwargs, \"memcheck'ed test must have a (possibly unused) `request` fixture\"\n+                test_id = kwargs['request'].node.callspec.id\n+                cmd = f\"{path}::{test_fn.__name__}[{test_id}]\"\n+                out = subprocess.run([\"cuda-memcheck\", \"pytest\", \"-vs\", cmd], capture_output=True, env=env)\n+                assert out.returncode == 0, \"cuda-memcheck returned an error: bounds checkng failed\"\n+                assert \"ERROR SUMMARY: 0 errors\" in str(out.stdout)\n+            else:\n+                test_fn(*args, **kwargs)\n+        return wrapper\n+    return decorator\n+\n+\n+def nvsmi_attr(attrs):\n+    attrs = \",\".join(attrs)\n+    cmd = [\n+        \"nvidia-smi\",\n+        \"-i\",\n+        \"0\",\n+        \"--query-gpu=\" + attrs,\n+        \"--format=csv,noheader,nounits\",\n+    ]\n+    out = subprocess.check_output(cmd)\n+    ret = out.decode(sys.stdout.encoding).split(\",\")\n+    ret = [int(x) for x in ret]\n+    return ret\n+\n+\n+@contextmanager\n+def set_gpu_clock(ref_sm_clock=1350, ref_mem_clock=1215):\n+    try:\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-pm\", \"1\"])\n+        subprocess.check_output(\n+            [\n+                \"nvidia-smi\",\n+                \"-i\",\n+                \"0\",\n+                f\"--lock-gpu-clocks={ref_sm_clock},{ref_sm_clock}\",\n+            ]\n+        )\n+        subprocess.check_output(\n+            [\n+                \"nvidia-smi\",\n+                \"-i\",\n+                \"0\",\n+                f\"--lock-memory-clocks={ref_mem_clock},{ref_mem_clock}\",\n+            ]\n+        )\n+        cur_sm_clock = nvsmi_attr([\"clocks.current.sm\"])[0]\n+        cur_mem_clock = nvsmi_attr([\"clocks.current.memory\"])[0]\n+        assert abs(cur_sm_clock - ref_sm_clock) < 10, f\"GPU SMs must run at {ref_sm_clock} MHz\"\n+        assert abs(cur_mem_clock - ref_mem_clock) < 10, f\"GPU SMs must run at {ref_mem_clock} MHz\"\n+        tflops = 1e-6 * 2 * 108 * 4 * 256 * ref_sm_clock\n+        gbps = 640 * 2 * ref_mem_clock * 1e-3\n+        yield tflops, gbps\n+    finally:\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-pm\", \"0\"])\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-rgc\"])\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-rmc\"])\n+\n \n def get_max_simd_tflops(dtype: torch.dtype, backend=None, device=None):\n     if not backend:"}, {"filename": "python/tutorials/01-vector-add.py", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "file_content_changes": "@@ -24,11 +24,9 @@ def add_kernel(\n     y_ptr,  # *Pointer* to second input vector\n     output_ptr,  # *Pointer* to output vector\n     n_elements,  # Size of the vector\n-    time_start_ptr, time_end_ptr,\n     BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n                  # NOTE: `constexpr` so it can be used as a shape value\n ):\n-    tl.atomic_min(time_start_ptr, tl.clock())\n     # There are multiple 'program's processing different data. We identify which program\n     # we are here\n     pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0\n@@ -47,7 +45,6 @@ def add_kernel(\n     output = x + y\n     # Write x + y back to DRAM\n     tl.store(output_ptr + offsets, output, mask=mask)\n-    tl.atomic_max(time_end_ptr, tl.clock())\n \n \n # %%\n@@ -56,8 +53,6 @@ def add_kernel(\n \n \n def add(x: torch.Tensor, y: torch.Tensor):\n-    time_start = torch.zeros(1, dtype=torch.int64, device='cuda')\n-    time_end = torch.zeros(1, dtype=torch.int64, device='cuda')\n     # We need to preallocate the output\n     output = torch.empty_like(x)\n     assert x.is_cuda and y.is_cuda and output.is_cuda\n@@ -70,7 +65,7 @@ def add(x: torch.Tensor, y: torch.Tensor):\n     #  - each torch.tensor object is implicitly converted into a pointer to its first element.\n     #  - `triton.jit`'ed functions can be index with a launch grid to obtain a callable GPU kernel\n     #  - don't forget to pass meta-parameters as keywords arguments\n-    add_kernel[grid](x, y, output, n_elements, time_start, time_end, BLOCK_SIZE=1024)\n+    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n     # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n     # running asynchronously at this point.\n     return output"}]