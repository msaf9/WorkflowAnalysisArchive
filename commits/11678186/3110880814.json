[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 3, "deletions": 11, "changes": 14, "file_content_changes": "@@ -43,7 +43,6 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n       return 0;\n     }\n   };\n-  // blocked -> blocked\n   if (srcLayout.isa<BlockedEncodingAttr>() &&\n       dstLayout.isa<BlockedEncodingAttr>()) {\n     auto srcBlockedLayout = srcLayout.cast<BlockedEncodingAttr>();\n@@ -66,14 +65,6 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n     }\n     paddedRepShape[outOrd[0]] += pad;\n   }\n-  // blocked -> shared\n-  if (srcLayout.isa<BlockedEncodingAttr>() &&\n-      dstLayout.isa<SharedEncodingAttr>()) {\n-    auto sharedLayout = dstLayout.cast<SharedEncodingAttr>();\n-    for (int v : dstTy.getShape())\n-      paddedRepShape.push_back(v);\n-  }\n-\n   return paddedRepShape;\n }\n \n@@ -140,8 +131,9 @@ class AllocationAnalysis {\n       auto dstTy = cvtLayout.result().getType().cast<RankedTensorType>();\n       auto srcEncoding = srcTy.getEncoding();\n       auto dstEncoding = dstTy.getEncoding();\n-      if (srcEncoding.isa<SharedEncodingAttr>()) {\n-        // only block->block and block->shared is supported now\n+      if (srcEncoding.isa<SharedEncodingAttr>() ||\n+          dstEncoding.isa<SharedEncodingAttr>()) {\n+        // Only blocked -> blocked conversion requires for scratch allocation\n         return;\n       }\n       // ConvertLayoutOp with both input/output non-shared_layout"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 11, "changes": 17, "file_content_changes": "@@ -585,12 +585,13 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimIdx;\n   }\n \n+  template <typename T>\n   Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n                             Value smem, const Allocation *allocation,\n-                            Operation *op) const {\n+                            T value) const {\n     auto ptrTy = LLVM::LLVMPointerType::get(\n         this->getTypeConverter()->convertType(rewriter.getIntegerType(8)), 3);\n-    auto bufferId = allocation->getBufferId(op);\n+    auto bufferId = allocation->getBufferId(value);\n     assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n     size_t offset = allocation->getOffset(bufferId);\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n@@ -1399,8 +1400,6 @@ struct ConvertLayoutOpConversion\n     if ((!srcLayout.isa<BlockedEncodingAttr>()) ||\n         (!dstLayout.isa<BlockedEncodingAttr>())) {\n       // TODO: not implemented\n-      llvm::errs()\n-          << \"convert_layout except for blocked -> blocked is not implemented\";\n       return failure();\n     }\n     auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n@@ -1996,12 +1995,6 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     return failure();\n   }\n \n-  Value getSmemAddr(Value value, Location loc,\n-                    ConversionPatternRewriter &rewriter) const {\n-    return getSharedMemoryBase(loc, rewriter, smem, allocation,\n-                               value.getDefiningOp());\n-  }\n-\n   const Allocation *allocation;\n   Value smem;\n };\n@@ -2340,7 +2333,9 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     SmallVector<Value> ptrs(numPtrs);\n \n     Type smemPtrTy = helper.getShemPtrTy();\n-    auto smemBase = getSmemAddr(tensor, loc, rewriter);\n+    auto smemBase =\n+        getSharedMemoryBase(loc, rewriter, smem, allocation, tensor);\n+\n     for (int i = 0; i < numPtrs; i++) {\n       ptrs[i] = bit_cast(\n           smemPtrTy, gep(smemBase.getType(), smemBase, ValueRange({offs[i]})));"}]