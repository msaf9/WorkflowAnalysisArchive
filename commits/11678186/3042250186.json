[{"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -180,7 +180,7 @@ def triton_attention(\n ):\n     sparse_dot_sdd_nt = triton.ops.blocksparse.matmul(layout, block, \"sdd\", trans_a=False, trans_b=True, device=value.device)\n     sparse_dot_dsd_nn = triton.ops.blocksparse.matmul(layout, block, \"dsd\", trans_a=False, trans_b=False, device=value.device)\n-    sparse_softmax = triton.ops.blocksparse.softmax(layout, block, device=value.device, is_dense=True)\n+    sparse_softmax = triton.ops.blocksparse.softmax(layout, block, device=value.device)\n \n     w = sparse_dot_sdd_nt(query, key)\n     w = sparse_softmax(w, scale=scale, is_causal=True)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -820,7 +820,7 @@ def make_triton_ir(fn, signature, specialization, constants):\n     gscope = fn.__globals__.copy()\n     function_name = '_'.join([fn.__name__, kernel_suffix(signature, specialization)])\n     tys = signature.split(',')\n-    new_constants = {k: True if tys[k]==\"i1\" else 1 for k in specialization.equal_to_1}\n+    new_constants = {k: True if tys[k] == \"i1\" else 1 for k in specialization.equal_to_1}\n     new_attrs = {k: (\"multiple_of\", 16) for k in specialization.divisible_by_16}\n     all_constants = constants.copy()\n     all_constants.update(new_constants)"}]