[{"filename": "include/triton/Conversion/Passes.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -2,7 +2,6 @@\n #define TRITON_CONVERSION_PASSES_H\n \n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n "}, {"filename": "include/triton/Conversion/Passes.td", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -49,18 +49,4 @@ def ConvertTritonGPUToLLVM : Pass<\"convert-triton-gpu-to-llvm\", \"mlir::ModuleOp\"\n     ];\n }\n \n-def TritonConvertArithToIndex : Pass<\"triton-convert-arith-to-index\", \"mlir::ModuleOp\"> {\n-\n-    let summary = \"Convert arith to index\";\n-    \n-    let constructor = \"mlir::triton::createTritonConvertArithToIndexPass()\";\n-\n-    let description = [{\n-      Convert arith operation on index values to corresponding ops in the index dialect.\n-      We need this because SCFToCF conversion currently generates arith ops on indices.\n-    }];\n-\n-    let dependentDialects = [\"mlir::index::IndexDialect\"];\n-}\n-\n #endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h", "status": "removed", "additions": 0, "deletions": 20, "changes": 20, "file_content_changes": "@@ -1,20 +0,0 @@\n-#ifndef TRITON_CONVERSION_ARITH_TO_INDEX_H\n-#define TRITON_CONVERSION_ARITH_TO_INDEX_H\n-\n-#include \"mlir/Conversion/LLVMCommon/TypeConverter.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-#include <memory>\n-\n-namespace mlir {\n-\n-class ModuleOp;\n-template <typename T> class OperationPass;\n-\n-namespace triton {\n-\n-std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass();\n-\n-}\n-} // namespace mlir\n-\n-#endif\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ArithToIndexPass.cpp", "status": "removed", "additions": 0, "deletions": 90, "changes": 90, "file_content_changes": "@@ -1,90 +0,0 @@\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n-#include \"mlir/Analysis/DataFlowFramework.h\"\n-#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n-#include \"mlir/Conversion/ControlFlowToLLVM//ControlFlowToLLVM.h\"\n-#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n-#include \"mlir/Conversion/LLVMCommon/VectorPattern.h\"\n-#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n-#include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n-#include \"mlir/Dialect/Index/IR/IndexOps.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n-#include \"mlir/Pass/Pass.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-\n-using namespace mlir;\n-using namespace mlir::triton;\n-\n-#define GEN_PASS_CLASSES\n-#include \"triton/Conversion/Passes.h.inc\"\n-\n-namespace {\n-class TritonArithToIndexConversionTarget : public mlir::ConversionTarget {\n-public:\n-  static bool hasIndexResultOrOperand(Operation *op) {\n-    if (!op)\n-      return false;\n-    bool hasRetIndex = llvm::find_if(op->getResultTypes(), [](Type type) {\n-                         return type.isIndex();\n-                       }) != op->getResultTypes().end();\n-    bool hasArgIndex = llvm::find_if(op->getOperandTypes(), [](Type type) {\n-                         return type.isIndex();\n-                       }) != op->getOperandTypes().end();\n-    return !hasRetIndex && !hasArgIndex;\n-  }\n-\n-  explicit TritonArithToIndexConversionTarget(MLIRContext &ctx)\n-      : ConversionTarget(ctx) {\n-    addLegalDialect<index::IndexDialect>();\n-    addDynamicallyLegalDialect<arith::ArithDialect>(hasIndexResultOrOperand);\n-  }\n-};\n-\n-template <class SrcOp, class DstOp>\n-LogicalResult replaceArithWithIndex(SrcOp op, PatternRewriter &rewriter) {\n-  // if (!hasIndexResultOrOperand(&*op))\n-  //   return failure();\n-  rewriter.replaceOpWithNewOp<DstOp>(op, op->getResultTypes(),\n-                                     op->getOperands(), op->getAttrs());\n-  return success();\n-}\n-\n-LogicalResult replaceArithCmpWithIndexCmp(arith::CmpIOp op,\n-                                          PatternRewriter &rewriter) {\n-  // if (!hasIndexResultOrOperand(&*op))\n-  //   return failure();\n-  rewriter.replaceOpWithNewOp<index::CmpOp>(\n-      op, op.getResult().getType(), (index::IndexCmpPredicate)op.getPredicate(),\n-      op.getOperand(0), op.getOperand(1));\n-  return success();\n-}\n-\n-class ArithToIndex : public TritonConvertArithToIndexBase<ArithToIndex> {\n-public:\n-  void runOnOperation() override {\n-    MLIRContext *context = &getContext();\n-    ModuleOp mod = getOperation();\n-    TritonArithToIndexConversionTarget target(*context);\n-    RewritePatternSet patterns(context);\n-    patterns.add(replaceArithWithIndex<arith::IndexCastOp, index::CastSOp>);\n-    patterns.add(replaceArithWithIndex<arith::ConstantOp, index::ConstantOp>);\n-    patterns.add(replaceArithWithIndex<arith::AddIOp, index::AddOp>);\n-    patterns.add(replaceArithCmpWithIndexCmp);\n-    if (failed(applyPartialConversion(mod, target, std::move(patterns)))) {\n-      return signalPassFailure();\n-    }\n-  }\n-};\n-} // namespace\n-\n-namespace mlir {\n-namespace triton {\n-\n-std::unique_ptr<OperationPass<ModuleOp>> createTritonConvertArithToIndexPass() {\n-  return std::make_unique<::ArithToIndex>();\n-}\n-\n-} // namespace triton\n-} // namespace mlir\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,5 +1,4 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n-    ArithToIndexPass.cpp\n     ConvertLayoutOpToLLVM.cpp\n     DotOpToLLVM.cpp\n     ElementwiseOpToLLVM.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.cpp", "status": "modified", "additions": 82, "deletions": 12, "changes": 94, "file_content_changes": "@@ -443,6 +443,8 @@ Type DotOpMmaV2ConversionHelper::getShemPtrTy() const {\n     return ptr_ty(type::i16Ty(ctx), 3);\n   case TensorCoreType::FP32_TF32_TF32_FP32:\n     return ptr_ty(type::f32Ty(ctx), 3);\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return ptr_ty(type::f16Ty(ctx), 3);\n   case TensorCoreType::INT32_INT8_INT8_INT32:\n     return ptr_ty(type::i8Ty(ctx), 3);\n   default:\n@@ -471,6 +473,8 @@ Type DotOpMmaV2ConversionHelper::getMatType() const {\n   switch (mmaType) {\n   case TensorCoreType::FP32_FP16_FP16_FP32:\n     return fp16x2Pack4Ty;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return fp16x2Pack4Ty;\n   case TensorCoreType::FP32_BF16_BF16_FP32:\n     return bf16x2Pack4Ty;\n   case TensorCoreType::FP32_TF32_TF32_FP32:\n@@ -488,6 +492,8 @@ Type DotOpMmaV2ConversionHelper::getLoadElemTy() {\n   switch (mmaType) {\n   case TensorCoreType::FP32_FP16_FP16_FP32:\n     return vec_ty(type::f16Ty(ctx), 2);\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return vec_ty(type::f16Ty(ctx), 2);\n   case TensorCoreType::FP32_BF16_BF16_FP32:\n     return vec_ty(type::bf16Ty(ctx), 2);\n   case TensorCoreType::FP32_TF32_TF32_FP32:\n@@ -503,14 +509,19 @@ Type DotOpMmaV2ConversionHelper::getLoadElemTy() {\n \n Type DotOpMmaV2ConversionHelper::getMmaRetType() const {\n   Type fp32Ty = type::f32Ty(ctx);\n+  Type fp16Ty = type::f16Ty(ctx);\n   Type i32Ty = type::i32Ty(ctx);\n   Type fp32x4Ty =\n       LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n   Type i32x4Ty =\n       LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n+  Type fp16x2Pack2Ty = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(2, vec_ty(fp16Ty, 2)));\n   switch (mmaType) {\n   case TensorCoreType::FP32_FP16_FP16_FP32:\n     return fp32x4Ty;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return fp16x2Pack2Ty;\n   case TensorCoreType::FP32_BF16_BF16_FP32:\n     return fp32x4Ty;\n   case TensorCoreType::FP32_TF32_TF32_FP32:\n@@ -524,6 +535,25 @@ Type DotOpMmaV2ConversionHelper::getMmaRetType() const {\n   return Type{};\n }\n \n+int DotOpMmaV2ConversionHelper::getMmaRetSize() const {\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return 4;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return 2;\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return 4;\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return 4;\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return 4;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return 4;\n+}\n+\n DotOpMmaV2ConversionHelper::TensorCoreType\n DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(Type operandTy) {\n   auto tensorTy = operandTy.cast<RankedTensorType>();\n@@ -559,6 +589,9 @@ DotOpMmaV2ConversionHelper::getMmaType(triton::DotOp op) {\n   } else if (dTy.getElementType().isInteger(32)) {\n     if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n       return TensorCoreType::INT32_INT8_INT8_INT32;\n+  } else if (dTy.getElementType().isF16()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP16_FP16_FP16_FP16;\n   }\n \n   return TensorCoreType::NOT_APPLICABLE;\n@@ -1004,6 +1037,31 @@ Value MMA16816ConversionHelper::loadC(Value tensor, Value llTensor) const {\n   assert(structTy.getBody().size() == fcSize &&\n          \"DotOp's $c operand should pass the same number of values as $d in \"\n          \"mma layout.\");\n+\n+  auto numMmaRets = helper.getMmaRetSize();\n+  assert(numMmaRets == 4 || numMmaRets == 2);\n+  if (numMmaRets == 4) {\n+    return llTensor;\n+  } else if (numMmaRets == 2) {\n+    auto cPack = SmallVector<Value>();\n+    auto cElemTy = tensorTy.getElementType();\n+    int numCPackedElem = 4 / numMmaRets;\n+    Type cPackTy = vec_ty(cElemTy, numCPackedElem);\n+    for (int i = 0; i < fcSize; i += numCPackedElem) {\n+      Value pack = rewriter.create<LLVM::UndefOp>(loc, cPackTy);\n+      for (int j = 0; j < numCPackedElem; ++j) {\n+        pack = insert_element(\n+            cPackTy, pack, extract_val(cElemTy, llTensor, i + j), i32_val(j));\n+      }\n+      cPack.push_back(pack);\n+    }\n+\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(cPack.size(), cPackTy));\n+    Value result =\n+        typeConverter->packLLElements(loc, cPack, rewriter, structTy);\n+    return result;\n+  }\n   return llTensor;\n }\n LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n@@ -1032,14 +1090,18 @@ LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n   ValueTable hb = getValuesFromDotOperandLayoutStruct(\n       loadedB, std::max(numRepN / 2, 1), numRepK, bTensorTy);\n   auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n+  auto numMmaRets = helper.getMmaRetSize();\n+  int numCPackedElem = 4 / numMmaRets;\n \n   auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n     unsigned colsPerThread = numRepN * 2;\n     PTXBuilder builder;\n     auto &mma = *builder.create(helper.getMmaInstr().str());\n     // using =r for float32 works but leads to less readable ptx.\n     bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n-    auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n+    bool isAccF16 = dTensorTy.getElementType().isF16();\n+    auto retArgs =\n+        builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n     auto aArgs = builder.newListOperand({\n         {ha[{m, k}], \"r\"},\n         {ha[{m + 1, k}], \"r\"},\n@@ -1049,18 +1111,21 @@ LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n     auto bArgs =\n         builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n     auto cArgs = builder.newListOperand();\n-    for (int i = 0; i < 4; ++i) {\n-      cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n-                                           std::to_string(i)));\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      cArgs->listAppend(builder.newOperand(\n+          fc[(m * colsPerThread + 4 * n) / numCPackedElem + i],\n+          std::to_string(i)));\n       // reuse the output registers\n     }\n \n     mma(retArgs, aArgs, bArgs, cArgs);\n     Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n     Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-    for (int i = 0; i < 4; ++i)\n-      fc[m * colsPerThread + 4 * n + i] = extract_val(elemTy, mmaOut, i);\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      fc[(m * colsPerThread + 4 * n) / numCPackedElem + i] =\n+          extract_val(elemTy, mmaOut, i);\n+    }\n   };\n \n   for (int k = 0; k < numRepK; ++k)\n@@ -1070,14 +1135,19 @@ LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n \n   Type resElemTy = dTensorTy.getElementType();\n \n-  for (auto &elem : fc) {\n-    elem = bitcast(elem, resElemTy);\n-  }\n-\n   // replace with new packed result\n   Type structTy = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(fc.size(), resElemTy));\n-  Value res = typeConverter->packLLElements(loc, fc, rewriter, structTy);\n+      ctx, SmallVector<Type>(fc.size() * numCPackedElem, resElemTy));\n+  SmallVector<Value> results(fc.size() * numCPackedElem);\n+  for (int i = 0; i < fc.size(); ++i) {\n+    for (int j = 0; j < numCPackedElem; ++j) {\n+      results[i * numCPackedElem + j] =\n+          numCPackedElem > 1\n+              ? bitcast(extract_element(fc[i], i32_val(j)), resElemTy)\n+              : bitcast(fc[i], resElemTy);\n+    }\n+  }\n+  Value res = typeConverter->packLLElements(loc, results, rewriter, structTy);\n   rewriter.replaceOp(op, res);\n \n   return success();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "file_content_changes": "@@ -116,6 +116,7 @@ struct DotOpMmaV2ConversionHelper {\n     FP32_FP16_FP16_FP32 = 0, // default\n     FP32_BF16_BF16_FP32,\n     FP32_TF32_TF32_FP32,\n+    FP16_FP16_FP16_FP16,\n     // integer tensor core instr\n     INT32_INT1_INT1_INT32, // Not implemented\n     INT32_INT4_INT4_INT32, // Not implemented\n@@ -155,6 +156,8 @@ struct DotOpMmaV2ConversionHelper {\n \n   Type getMmaRetType() const;\n \n+  int getMmaRetSize() const;\n+\n   ArrayRef<int> getMmaInstrShape() const {\n     assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n            \"Unknown mma type found.\");\n@@ -174,6 +177,9 @@ struct DotOpMmaV2ConversionHelper {\n   }\n \n   // Deduce the TensorCoreType from either $a or $b's type.\n+  // TODO: both the input type ($a or $b) and output type ($c or $d) are\n+  // needed to differentiate TensorCoreType::FP32_FP16_FP16_FP32 from\n+  // TensorCoreType::FP16_FP16_FP16_FP16\n   static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy);\n \n   int getVec() const {\n@@ -202,6 +208,7 @@ struct DotOpMmaV2ConversionHelper {\n           {TensorCoreType::FP32_FP16_FP16_FP32, {16, 8, 16}},\n           {TensorCoreType::FP32_BF16_BF16_FP32, {16, 8, 16}},\n           {TensorCoreType::FP32_TF32_TF32_FP32, {16, 8, 8}},\n+          {TensorCoreType::FP16_FP16_FP16_FP16, {16, 8, 16}},\n \n           {TensorCoreType::INT32_INT1_INT1_INT32, {16, 8, 256}},\n           {TensorCoreType::INT32_INT4_INT4_INT32, {16, 8, 64}},\n@@ -217,6 +224,7 @@ struct DotOpMmaV2ConversionHelper {\n           {TensorCoreType::FP32_FP16_FP16_FP32, {8, 8, 8}},\n           {TensorCoreType::FP32_BF16_BF16_FP32, {8, 8, 8}},\n           {TensorCoreType::FP32_TF32_TF32_FP32, {8, 8, 4}},\n+          {TensorCoreType::FP16_FP16_FP16_FP16, {8, 8, 8}},\n \n           {TensorCoreType::INT32_INT1_INT1_INT32, {8, 8, 64}},\n           {TensorCoreType::INT32_INT4_INT4_INT32, {8, 8, 32}},\n@@ -234,6 +242,8 @@ struct DotOpMmaV2ConversionHelper {\n        \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n       {TensorCoreType::FP32_TF32_TF32_FP32,\n        \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n+      {TensorCoreType::FP16_FP16_FP16_FP16,\n+       \"mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\"},\n \n       {TensorCoreType::INT32_INT1_INT1_INT32,\n        \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n@@ -248,6 +258,7 @@ struct DotOpMmaV2ConversionHelper {\n       {TensorCoreType::FP32_FP16_FP16_FP32, 8},\n       {TensorCoreType::FP32_BF16_BF16_FP32, 8},\n       {TensorCoreType::FP32_TF32_TF32_FP32, 4},\n+      {TensorCoreType::FP16_FP16_FP16_FP16, 8},\n \n       {TensorCoreType::INT32_INT1_INT1_INT32, 128},\n       {TensorCoreType::INT32_INT4_INT4_INT32, 32},\n@@ -366,6 +377,20 @@ struct MMA16816ConversionHelper {\n     warp = udiv(thread, _32);\n   }\n \n+  MMA16816ConversionHelper(DotOp op, MmaEncodingAttr mmaLayout, Value thread,\n+                           ConversionPatternRewriter &rewriter,\n+                           TritonGPUToLLVMTypeConverter *typeConverter,\n+                           Location loc)\n+      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()), thread(thread),\n+        helper(mmaLayout), rewriter(rewriter), typeConverter(typeConverter),\n+        loc(loc), ctx(mmaLayout.getContext()) {\n+    helper.deduceMmaType(op);\n+\n+    Value _32 = i32_val(32);\n+    lane = urem(thread, _32);\n+    warp = udiv(thread, _32);\n+  }\n+\n   // Get a warpId for M axis.\n   Value getWarpM(int M) const {\n     auto matInstrShape = helper.getMmaInstrShape();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "file_content_changes": "@@ -33,8 +33,13 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                                     .getEncoding()\n                                     .dyn_cast<MmaEncodingAttr>();\n     if (!isOuter && mmaLayout && supportMMA(op, mmaLayout.getVersionMajor())) {\n-      if (mmaLayout.isVolta())\n+      if (mmaLayout.isVolta()) {\n+        if (D.getType().cast<RankedTensorType>().getElementType().isF16()) {\n+          llvm_unreachable(\n+              \"out_dtype=float16 for dot is not implemented on V100 yet\");\n+        }\n         return convertMMA884(op, adaptor, rewriter);\n+      }\n       if (mmaLayout.isAmpere())\n         return convertMMA16816(op, adaptor, rewriter);\n \n@@ -67,7 +72,7 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n     Value B = op.getB();\n     Value C = op.getC();\n \n-    MMA16816ConversionHelper mmaHelper(A.getType(), mmaLayout,\n+    MMA16816ConversionHelper mmaHelper(op, mmaLayout,\n                                        getThreadId(rewriter, loc), rewriter,\n                                        getTypeConverter(), loc);\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -946,6 +946,12 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n                                                 view.getResult());\n     return mlir::success();\n   }\n+  // cvt(cat) -> cat\n+  if (auto cat = dyn_cast<triton::CatOp>(arg)) {\n+    rewriter.replaceOpWithNewOp<triton::CatOp>(op, op->getResult(0).getType(),\n+                                               cat.getOperands());\n+    return mlir::success();\n+  }\n   // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n   auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n   if (alloc_tensor) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 126, "deletions": 43, "changes": 169, "file_content_changes": "@@ -25,9 +25,17 @@ namespace ttg = triton::gpu;\n static Type getI1SameShape(Value v) {\n   Type vType = v.getType();\n   auto i1Type = IntegerType::get(vType.getContext(), 1);\n-  auto tensorType = vType.cast<RankedTensorType>();\n-  return RankedTensorType::get(tensorType.getShape(), i1Type,\n-                               tensorType.getEncoding());\n+  if (auto tensorType = vType.dyn_cast<RankedTensorType>())\n+    return RankedTensorType::get(tensorType.getShape(), i1Type,\n+                                 tensorType.getEncoding());\n+  return i1Type;\n+}\n+\n+// pass named attrs (e.g., tt.contiguity) from Triton to Triton\n+static void addNamedAttrs(Operation *op, DictionaryAttr dictAttrs) {\n+  for (const NamedAttribute attr : dictAttrs.getValue())\n+    if (!op->hasAttr(attr.getName()))\n+      op->setAttr(attr.getName(), attr.getValue());\n }\n \n #define int_attr(num) builder.getI64IntegerAttr(num)\n@@ -69,6 +77,23 @@ class LoopPipeliner {\n   /// Block arguments that loads depend on\n   SetVector<BlockArgument> depArgs;\n \n+  /// If we have a load that immediately depends on a block argument in the\n+  /// current iteration, it is an immediate dependency. Otherwise, it is a\n+  /// non-immediate dependency, which means the load depends on a block argument\n+  /// in the previous iterations.\n+  /// For example:\n+  /// scf.for (%arg0, %arg1, %arg2) {\n+  ///   %0 = load %arg0  <--- immediate dep, this address is initialized at\n+  ///   numStages-2\n+  ///   %1 = load %arg1\n+  ///   %2 = add %1, %arg2\n+  ///   %3 = load %2  <--- non-immediate dep, %arg1 must be an update-to-date\n+  ///   value\n+  /// }\n+  SetVector<BlockArgument> immedidateDepArgs;\n+\n+  SetVector<BlockArgument> nonImmedidateDepArgs;\n+\n   /// Operations (inside the loop body) that loads depend on\n   SetVector<Operation *> depOps;\n \n@@ -79,6 +104,9 @@ class LoopPipeliner {\n \n   Value lookupOrDefault(Value origin, int stage);\n \n+  Value getLoadMask(triton::LoadOp loadOp, Value mappedMask, Value loopCond,\n+                    OpBuilder &builder);\n+\n   /// Returns a empty buffer of size <numStages, ...>\n   ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n \n@@ -170,7 +198,7 @@ LogicalResult LoopPipeliner::initialize() {\n   }\n \n   // can we use forOp.walk(...) here?\n-  SmallVector<triton::LoadOp, 2> allLoads;\n+  SmallVector<triton::LoadOp, 2> validLoads;\n   for (Operation &op : *loop)\n     if (auto loadOp = dyn_cast<triton::LoadOp>(&op)) {\n       auto ptr = loadOp.getPtr();\n@@ -184,30 +212,31 @@ LogicalResult LoopPipeliner::initialize() {\n                     .cast<triton::PointerType>()\n                     .getPointeeType();\n       unsigned width = vec * ty.getIntOrFloatBitWidth();\n+      // cp.async's cp-size can only be 4, 8 and 16.\n       if (width >= 32)\n-        allLoads.push_back(loadOp);\n+        validLoads.push_back(loadOp);\n     }\n \n   // Early stop: no need to continue if there is no load in the loop.\n-  if (allLoads.empty())\n+  if (validLoads.empty())\n     return failure();\n \n   // load => values that it depends on\n   DenseMap<Value, SetVector<Value>> loadDeps;\n-  for (triton::LoadOp loadOp : allLoads) {\n+  for (triton::LoadOp loadOp : validLoads) {\n     SetVector<Value> deps;\n     for (Value op : loadOp->getOperands())\n       collectDeps(op, numStages - 1, deps);\n     loadDeps[loadOp] = deps;\n   }\n \n-  // Don't pipeline loads that depend on other loads\n-  // (Because if a load depends on another load, this load needs to wait on the\n-  //  other load in the prologue, which is against the point of the pipeline\n-  //  pass)\n-  for (triton::LoadOp loadOp : allLoads) {\n+  // Don't pipeline valid loads that depend on other valid loads\n+  // (Because if a valid load depends on another valid load, this load needs to\n+  // wait on the other load in the prologue, which is against the point of the\n+  // pipeline pass)\n+  for (triton::LoadOp loadOp : validLoads) {\n     bool isCandidate = true;\n-    for (triton::LoadOp other : allLoads) {\n+    for (triton::LoadOp other : validLoads) {\n       if (loadDeps[loadOp].contains(other)) {\n         isCandidate = false;\n         break;\n@@ -264,20 +293,54 @@ LogicalResult LoopPipeliner::initialize() {\n   if (!loads.empty()) {\n     // Update depArgs & depOps\n     for (Value loadOp : loads) {\n-      for (Value dep : loadDeps[loadOp]) {\n-        // TODO: we should record the stage that the value is depended on\n-        if (auto arg = dep.dyn_cast<BlockArgument>())\n+      auto &deps = loadDeps[loadOp];\n+      for (auto &dep : deps) {\n+        if (auto arg = dep.dyn_cast<BlockArgument>()) {\n           depArgs.insert(arg);\n-        else\n+          if (deps.front().isa<BlockArgument>()) {\n+            immedidateDepArgs.insert(arg);\n+          } else {\n+            nonImmedidateDepArgs.insert(arg);\n+          }\n+        } else\n           depOps.insert(dep.getDefiningOp());\n       }\n     }\n     return success();\n   }\n \n+  // Check if immedidateDepArgs and nonImmedidateDepArgs are disjoint\n+  // If yes, we cannot pipeline the loop for now\n+  for (BlockArgument arg : immedidateDepArgs)\n+    if (nonImmedidateDepArgs.contains(arg)) {\n+      return failure();\n+    }\n+\n   return failure();\n }\n \n+Value LoopPipeliner::getLoadMask(triton::LoadOp loadOp, Value mappedMask,\n+                                 Value loopCond, OpBuilder &builder) {\n+  Type maskType = getI1SameShape(loadOp);\n+  Value mask = loadOp.getMask();\n+  Value newMask;\n+  if (mask) {\n+    Value cond = loopCond;\n+    if (isa<RankedTensorType>(maskType)) {\n+      cond = builder.create<triton::SplatOp>(mask.getLoc(), maskType, loopCond);\n+    }\n+    newMask = builder.create<arith::AndIOp>(mask.getLoc(), mappedMask, cond);\n+  } else {\n+    if (isa<RankedTensorType>(maskType)) {\n+      newMask = builder.create<triton::SplatOp>(loopCond.getLoc(), maskType,\n+                                                loopCond);\n+    } else {\n+      newMask = loopCond;\n+    }\n+  }\n+  return newMask;\n+}\n+\n void LoopPipeliner::emitPrologue() {\n   // llvm::errs() << \"loads to pipeline...:\\n\";\n   // for (Value load : loads)\n@@ -322,17 +385,9 @@ void LoopPipeliner::emitPrologue() {\n         }\n         // load => copy async\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n-          Value mask = lookupOrDefault(loadOp.getMask(), stage);\n-          Value newMask;\n-          if (mask) {\n-            Value splatCond = builder.create<triton::SplatOp>(\n-                mask.getLoc(), mask.getType(), loopCond);\n-            newMask =\n-                builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n-          } else {\n-            newMask = builder.create<triton::SplatOp>(\n-                loopCond.getLoc(), getI1SameShape(loadOp), loopCond);\n-          }\n+          Value newMask =\n+              getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n+                          loopCond, builder);\n           // TODO: check if the hardware supports async copy\n           newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n@@ -345,7 +400,19 @@ void LoopPipeliner::emitPrologue() {\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n       } else {\n-        newOp = builder.clone(*op);\n+        if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+          Value newMask =\n+              getLoadMask(loadOp, lookupOrDefault(loadOp.getMask(), stage),\n+                          loopCond, builder);\n+          newOp = builder.create<triton::LoadOp>(\n+              loadOp.getLoc(), loadOp.getResult().getType(),\n+              lookupOrDefault(loadOp.getPtr(), stage), newMask,\n+              lookupOrDefault(loadOp.getOther(), stage), loadOp.getCache(),\n+              loadOp.getEvict(), loadOp.getIsVolatile());\n+          addNamedAttrs(newOp, op->getAttrDictionary());\n+        } else {\n+          newOp = builder.clone(*op);\n+        }\n         // Update loop-carried uses\n         for (unsigned opIdx = 0; opIdx < op->getNumOperands(); ++opIdx) {\n           auto it = valueMapping.find(op->getOperand(opIdx));\n@@ -429,7 +496,10 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   //   (original args)\n   //   (insertSliceAsync buffer at stage numStages - 1) for each load\n   //   (extracted tensor) for each load\n-  //   (depArgs at stage numStages - 2)\n+  //   (depArgs at stage numStages - 1):\n+  //   for each dep arg that is not an immediate block argument\n+  //   (depArgs at stage numStages - 2):\n+  //   for each dep arg that is an immediate block argument\n   //   (iv at stage numStages - 2)\n   //   (pipeline iteration index)\n   //   (loop iteration index)\n@@ -450,7 +520,10 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   size_t depArgsBeginIdx = newLoopArgs.size();\n   for (BlockArgument depArg : depArgs) {\n     depArgsIdx[depArg] = newLoopArgs.size();\n-    newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n+    if (immedidateDepArgs.contains(depArg)) {\n+      newLoopArgs.push_back(valueMapping[depArg][numStages - 2]);\n+    } else\n+      newLoopArgs.push_back(valueMapping[depArg][numStages - 1]);\n   }\n \n   size_t nextIVIdx = newLoopArgs.size();\n@@ -549,7 +622,21 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   for (Operation *op : orderedDeps)\n     if (!loads.contains(op->getResult(0))) {\n-      Operation *nextOp = builder.clone(*op, nextMapping);\n+      Operation *nextOp;\n+      if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n+        auto newMask =\n+            getLoadMask(loadOp, nextMapping.lookupOrDefault(loadOp.getMask()),\n+                        nextLoopCond, builder);\n+        nextOp = builder.create<triton::LoadOp>(\n+            loadOp.getLoc(), loadOp.getResult().getType(),\n+            nextMapping.lookupOrDefault(loadOp.getPtr()), newMask,\n+            nextMapping.lookupOrDefault(loadOp.getOther()), loadOp.getCache(),\n+            loadOp.getEvict(), loadOp.getIsVolatile());\n+        addNamedAttrs(nextOp, op->getAttrDictionary());\n+        nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n+      } else {\n+        nextOp = builder.clone(*op, nextMapping);\n+      }\n \n       auto originYield = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n@@ -571,21 +658,17 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     // Update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n-      Value mask = loadOp.getMask();\n-      Value newMask;\n+      auto mask = loadOp.getMask();\n+      auto newMask =\n+          getLoadMask(loadOp, nextMapping.lookupOrDefault(loadOp.getMask()),\n+                      nextLoopCond, builder);\n       if (mask) {\n-        Value splatCond = builder.create<triton::SplatOp>(\n-            mask.getLoc(), mask.getType(), nextLoopCond);\n-        newMask = builder.create<arith::AndIOp>(\n-            mask.getLoc(), splatCond, nextMapping.lookupOrDefault(mask));\n         // If mask is defined outside the loop, don't update the map more than\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n-          nextMapping.map(mask, newMask);\n-        newMask = nextMapping.lookupOrDefault(loadOp.getMask());\n-      } else\n-        newMask = builder.create<triton::SplatOp>(\n-            loadOp.getLoc(), getI1SameShape(loadOp), nextLoopCond);\n+          nextMapping.map(loadOp.getMask(), newMask);\n+        newMask = nextMapping.lookupOrDefault(mask);\n+      }\n       Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n           nextMapping.lookupOrDefault(loadOp.getPtr()),"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "file_content_changes": "@@ -557,8 +557,19 @@ class ConvertDotConvert : public mlir::RewritePattern {\n       return mlir::failure();\n \n     // TODO: int tensor cores\n-    auto _0f = rewriter.create<arith::ConstantFloatOp>(\n-        op->getLoc(), APFloat(0.0f), dstTy.getElementType().cast<FloatType>());\n+    auto out_dtype = dstTy.getElementType().cast<FloatType>();\n+    APFloat value(0.0f);\n+    if (out_dtype.isBF16())\n+      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n+    else if (out_dtype.isF16())\n+      value = APFloat(APFloat::IEEEhalf(), APInt(16, 0));\n+    else if (out_dtype.isF32())\n+      value = APFloat(0.0f);\n+    else\n+      llvm_unreachable(\"unsupported data type\");\n+\n+    auto _0f =\n+        rewriter.create<arith::ConstantFloatOp>(op->getLoc(), value, out_dtype);\n     auto _0 = rewriter.create<triton::SplatOp>(\n         op->getLoc(), dotOp.getResult().getType(), _0f);\n     auto newDot = rewriter.create<triton::DotOp>("}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -179,6 +179,8 @@ int simulateBackwardRematerialization(\n       if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n               triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n         continue;\n+      if (isa<triton::ViewOp, triton::CatOp>(opArgI))\n+        continue;\n \n       // We add one expensive conversion for the current operand\n       numCvts += 1;"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -12,7 +12,6 @@\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/ArithToIndexPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"llvm/ADT/APInt.h\"\n@@ -296,7 +295,6 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n   pm.addPass(mlir::createConvertSCFToCFPass());\n-  pm.addPass(createTritonConvertArithToIndexPass());\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass(computeCapability));\n   pm.addPass(mlir::createArithToLLVMConversionPass());"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -1,6 +1,5 @@\n #include \"triton/Target/PTX/PTXTranslation.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include <optional>\n \n #include \"llvm/IR/IRBuilder.h\"\n #include \"llvm/IR/LegacyPassManager.h\"\n@@ -12,13 +11,19 @@\n #include \"llvm/Support/TargetSelect.h\"\n #include \"llvm/Target/TargetMachine.h\"\n \n+#include <mutex>\n+#include <optional>\n+\n namespace triton {\n \n static void initLLVM() {\n-  LLVMInitializeNVPTXTargetInfo();\n-  LLVMInitializeNVPTXTarget();\n-  LLVMInitializeNVPTXTargetMC();\n-  LLVMInitializeNVPTXAsmPrinter();\n+  static std::once_flag init_flag;\n+  std::call_once(init_flag, []() {\n+    LLVMInitializeNVPTXTargetInfo();\n+    LLVMInitializeNVPTXTarget();\n+    LLVMInitializeNVPTXTargetMC();\n+    LLVMInitializeNVPTXAsmPrinter();\n+  });\n }\n \n static bool findAndReplace(std::string &str, const std::string &begin,"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -69,7 +69,7 @@ def get_llvm_package_info():\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n     name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n-    version = \"llvm-17.0.0-8e5a41e8271f\"\n+    version = \"llvm-17.0.0-2538e550420f\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 51, "deletions": 26, "changes": 77, "file_content_changes": "@@ -1173,15 +1173,17 @@ def kernel(X, stride_xm, stride_xn,\n # ---------------\n \n \n-@pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype\",\n-                         [(*shape, 4, False, False, epilogue, allow_tf32, dtype)\n+@pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype\",\n+                         [(*shape, 4, False, False, epilogue, allow_tf32, in_dtype, out_dtype)\n                           for shape in [(64, 64, 64), (16, 16, 16)]\n                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n-                          for dtype in ['float16', 'float32']\n-                          if not (allow_tf32 and (dtype in ['float16']))] +\n+                          for in_dtype, out_dtype in [('float16', 'float16'),\n+                                                      ('float16', 'float32'),\n+                                                      ('float32', 'float32')]\n+                          if not (allow_tf32 and (in_dtype in ['float16']))] +\n \n-                         [(*shape_nw, col_a, col_b, 'none', allow_tf32, dtype)\n+                         [(*shape_nw, col_a, col_b, 'none', allow_tf32, in_dtype, out_dtype)\n                           for shape_nw in [[128, 256, 32, 8],\n                                            [128, 16, 32, 4],\n                                            [32, 128, 64, 4],\n@@ -1194,19 +1196,25 @@ def kernel(X, stride_xm, stride_xn,\n                           for allow_tf32 in [True]\n                           for col_a in [True, False]\n                           for col_b in [True, False]\n-                          for dtype in ['int8', 'float16', 'float32']])\n-def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, device='cuda'):\n+                          for in_dtype, out_dtype in [('int8', 'int8'),\n+                                                      ('float16', 'float16'),\n+                                                      ('float16', 'float32'),\n+                                                      ('float32', 'float32')]])\n+def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8:\n-        if dtype == 'int8':\n+        if in_dtype == 'int8':\n             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n-        elif dtype == 'float32' and allow_tf32:\n+        elif in_dtype == 'float32' and allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n     if capability[0] == 7:\n         if (M, N, K, num_warps) == (128, 256, 32, 8):\n             pytest.skip(\"shared memory out of resource\")\n+        if out_dtype == 'float16':\n+            # TODO: support out_dtype=float16 for tl.dot on V100\n+            pytest.skip(\"Only test out_dtype=float16 on devices with sm >=80\")\n \n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n@@ -1216,6 +1224,7 @@ def kernel(X, stride_xm, stride_xk,\n                Y, stride_yk, stride_yn,\n                W, stride_wn, stride_wl,\n                Z, stride_zm, stride_zn,\n+               out_dtype: tl.constexpr,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n                ALLOW_TF32: tl.constexpr,\n@@ -1231,7 +1240,7 @@ def kernel(X, stride_xm, stride_xk,\n         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n         x = tl.load(Xs)\n         y = tl.load(Ys)\n-        z = tl.dot(x, y, allow_tf32=ALLOW_TF32)\n+        z = tl.dot(x, y, allow_tf32=ALLOW_TF32, out_dtype=out_dtype)\n         if ADD_MATRIX:\n             z += tl.load(Zs)\n         if ADD_ROWS:\n@@ -1248,42 +1257,54 @@ def kernel(X, stride_xm, stride_xk,\n             z = num / den[:, None]\n         if CHAIN_DOT:\n             w = tl.load(Ws)\n-            z = tl.dot(z.to(w.dtype), w)\n+            z = tl.dot(z.to(w.dtype), w, out_dtype=out_dtype)\n         tl.store(Zs, z)\n     # input\n     rs = RandomState(17)\n     if col_a:\n-        x = numpy_random((K, M), dtype_str=dtype, rs=rs).T\n+        x = numpy_random((K, M), dtype_str=in_dtype, rs=rs).T\n     else:\n-        x = numpy_random((M, K), dtype_str=dtype, rs=rs)\n+        x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)\n     if col_b:\n-        y = numpy_random((N, K), dtype_str=dtype, rs=rs).T\n+        y = numpy_random((N, K), dtype_str=in_dtype, rs=rs).T\n     else:\n-        y = numpy_random((K, N), dtype_str=dtype, rs=rs)\n-    w = numpy_random((N, N), dtype_str=dtype, rs=rs)\n-    if 'int' not in dtype:\n+        y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)\n+    w = numpy_random((N, N), dtype_str=in_dtype, rs=rs)\n+    if 'int' not in in_dtype:\n         x *= .1\n         y *= .1\n-    if dtype == 'float32' and allow_tf32:\n+    if in_dtype == 'float32' and allow_tf32:\n         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n     x_tri = to_triton(x, device=device)\n     y_tri = to_triton(y, device=device)\n     w_tri = to_triton(w, device=device)\n     # triton result\n-    if dtype == 'int8':\n+    if out_dtype == 'int8':\n         z = 1 + numpy_random((M, N), dtype_str='int32', rs=rs)\n     else:\n-        z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n+        z = 1 + numpy_random((M, N), dtype_str=in_dtype, rs=rs) * .1\n \n     z_tri = to_triton(z, device=device)\n     if epilogue == 'trans':\n         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n+\n+    if out_dtype == 'int8':\n+        out_dtype = tl.int8\n+    elif out_dtype == 'float16' and epilogue != 'softmax':\n+        # TODO: for out_dtype == 'float16' and epilogue == 'softmax', it will\n+        # fail with the following error: 'llvm.fmul' op requires the same type\n+        # for all operands and results\n+        out_dtype = tl.float16\n+    else:\n+        out_dtype = tl.float32\n+\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n                          z_tri, z_tri.stride(0), z_tri.stride(1),\n+                         out_dtype,\n                          COL_A=col_a, COL_B=col_b,\n                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n@@ -1294,7 +1315,7 @@ def kernel(X, stride_xm, stride_xk,\n                          ALLOW_TF32=allow_tf32,\n                          num_warps=num_warps)\n     # torch result\n-    if dtype == 'int8':\n+    if in_dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32),\n                           y.astype(np.float32())).astype(np.int32)\n     else:\n@@ -1314,9 +1335,11 @@ def kernel(X, stride_xm, stride_xk,\n         z_ref = np.matmul(z_ref, w)\n     # compare\n     # print(z_ref[:,0], z_tri[:,0])\n-    if dtype == 'float32':\n+    if in_dtype == 'float32':\n         # XXX: Somehow there's a larger difference when we use float32\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n+    elif out_dtype == tl.float16:\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01, atol=1e-3)\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n@@ -1325,12 +1348,14 @@ def kernel(X, stride_xm, stride_xk,\n         # XXX: skip small sizes because they are not vectorized\n         assert 'ld.global.v4' in ptx\n         assert 'st.global.v4' in ptx\n-    if dtype == 'float32' and allow_tf32:\n+    if in_dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n-    elif dtype == 'float32' and allow_tf32:\n+    elif in_dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n-    elif dtype == 'int8':\n+    elif in_dtype == 'int8':\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n+    elif out_dtype == tl.float16:\n+        assert 'mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16' in ptx\n \n \n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n@@ -1467,7 +1492,7 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n         w = tl.load(in2_ptr + in2_offsets, mask=in2_offsets < in2_numel)\n \n         # Without a dot product the memory doesn't get promoted to shared.\n-        o = tl.dot(x, w)\n+        o = tl.dot(x, w, out_dtype=tl.float32)\n \n         # Store output\n         output_offsets = M_offsets[:, None] * out_stride + N_offsets[None, :]"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 6, "deletions": 8, "changes": 14, "file_content_changes": "@@ -701,14 +701,15 @@ def visit_For(self, node):\n         iv_type = triton.language.semantic.integer_promote_impl(lb.dtype, ub.dtype)\n         iv_type = triton.language.semantic.integer_promote_impl(iv_type, step.dtype)\n         iv_ir_type = iv_type.to_ir(self.builder)\n+        iv_is_signed = iv_type.int_signedness == triton.language.core.dtype.SIGNEDNESS.SIGNED\n         # lb/ub/step might be constexpr, we need to cast them to tensor\n         lb = lb.handle\n         ub = ub.handle\n         step = step.handle\n         # ForOp can only accept IndexType as lb/ub/step. Cast integer to Index\n-        lb = self.builder.create_to_index(lb)\n-        ub = self.builder.create_to_index(ub)\n-        step = self.builder.create_to_index(step)\n+        lb = self.builder.create_int_cast(lb, iv_ir_type, iv_is_signed)\n+        ub = self.builder.create_int_cast(ub, iv_ir_type, iv_is_signed)\n+        step = self.builder.create_int_cast(step, iv_ir_type, iv_is_signed)\n         # Create placeholder for the loop induction variable\n         iv = self.builder.create_undef(iv_ir_type)\n         self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n@@ -767,12 +768,9 @@ def visit_For(self, node):\n \n             # update induction variable with actual value, and replace all uses\n             self.builder.set_insertion_point_to_start(for_op.get_body(0))\n-            iv = self.builder.create_index_to_si(for_op.get_induction_var())\n-            iv = self.builder.create_int_cast(iv, iv_ir_type, True)\n+            iv = for_op.get_induction_var()\n             if negative_step:\n-                ub_si = self.builder.create_index_to_si(ub)\n-                ub_si = self.builder.create_int_cast(ub_si, iv_ir_type, True)\n-                iv = self.builder.create_sub(ub_si, iv)\n+                iv = self.builder.create_sub(ub, iv)\n             self.lscope[node.target.id].handle.replace_all_uses_with(iv)\n             self.set_value(node.target.id, triton.language.core.tensor(iv, iv_type))\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -862,7 +862,7 @@ def reshape(input, shape, _builder=None):\n \n \n @builtin\n-def dot(input, other, allow_tf32=True, _builder=None):\n+def dot(input, other, allow_tf32=True, out_dtype=float32, _builder=None):\n     \"\"\"\n     Returns the matrix product of two blocks.\n \n@@ -874,7 +874,8 @@ def dot(input, other, allow_tf32=True, _builder=None):\n     :type other: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n     \"\"\"\n     allow_tf32 = _constexpr_to_value(allow_tf32)\n-    return semantic.dot(input, other, allow_tf32, _builder)\n+    out_dtype = _constexpr_to_value(out_dtype)\n+    return semantic.dot(input, other, allow_tf32, out_dtype, _builder)\n \n \n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -1051,6 +1051,7 @@ def atomic_xchg(ptr: tl.tensor,\n def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,\n+        out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n     assert lhs.type.is_block() and rhs.type.is_block()\n     assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n@@ -1062,9 +1063,13 @@ def dot(lhs: tl.tensor,\n     if lhs.type.scalar.is_int():\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n-    else:\n+    elif lhs.type.scalar.is_fp32() or lhs.type.scalar.is_bf16():\n         _0 = builder.get_fp32(0)\n         ret_scalar_ty = tl.float32\n+    else:\n+        _0 = builder.get_fp16(0) if out_dtype.is_fp16() else builder.get_fp32(0)\n+        ret_scalar_ty = out_dtype\n+\n     M = lhs.type.shape[0]\n     N = rhs.type.shape[1]\n     _0 = builder.create_splat(_0, [M, N])"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -64,7 +64,7 @@ def _sdd_kernel(\n         else:\n             a = tl.load(a_ptrs, mask=offs_ak[None, :] < k, other=0.)\n             b = tl.load(b_ptrs, mask=offs_bk[:, None] < k, other=0.)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=tl.float32)\n         a_ptrs += TILE_K * stride_ak\n         b_ptrs += TILE_K * stride_bk\n     c = acc.to(C.dtype.element_ty)\n@@ -183,7 +183,7 @@ def _dsd_kernel(\n     for k in range(K, 0, -TILE_K):\n         a = tl.load(pa)\n         b = tl.load(pb)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=tl.float32)\n         pa += inc_a\n         pb += inc_b * stride_bk\n         pinc += 2"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 21, "deletions": 9, "changes": 30, "file_content_changes": "@@ -64,9 +64,9 @@ def _kernel(A, B, C, M, N, K,\n             stride_am, stride_ak,\n             stride_bk, stride_bn,\n             stride_cm, stride_cn,\n+            dot_out_dtype: tl.constexpr,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n             GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr,\n-            ACC_TYPE: tl.constexpr\n             ):\n     # matrix multiplication\n     pid = tl.program_id(0)\n@@ -88,7 +88,7 @@ def _kernel(A, B, C, M, N, K,\n     # pointers\n     A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n     B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n-    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n+    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=dot_out_dtype)\n     for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n         if EVEN_K:\n             a = tl.load(A)\n@@ -97,7 +97,7 @@ def _kernel(A, B, C, M, N, K,\n             k_remaining = K - k * (BLOCK_K * SPLIT_K)\n             a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n             b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n-        acc += tl.dot(a, b)\n+        acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n     acc = acc.to(C.dtype.element_ty)\n@@ -119,7 +119,7 @@ class _matmul(torch.autograd.Function):\n     _locks = {}\n \n     @staticmethod\n-    def _call(a, b):\n+    def _call(a, b, dot_out_dtype):\n         device = a.device\n         # handle non-contiguous inputs if necessary\n         if a.stride(0) > 1 and a.stride(1) > 1:\n@@ -132,20 +132,32 @@ def _call(a, b):\n         _, N = b.shape\n         # allocates output\n         c = torch.empty((M, N), device=device, dtype=a.dtype)\n-        # accumulator types\n-        ACC_TYPE = tl.float32 if a.dtype in [torch.float16, torch.bfloat16, torch.float32] else tl.int32\n+        if dot_out_dtype is None:\n+            if a.dtype in [torch.float16, torch.float32, torch.bfloat16]:\n+                dot_out_dtype = tl.float32\n+            else:\n+                dot_out_dtype = tl.int32\n+        else:\n+            assert isinstance(dot_out_dtype, torch.dtype), \"dot_out_dtype must be a torch.dtype\"\n+            if dot_out_dtype == torch.float16:\n+                dot_out_dtype = tl.float16\n+            elif dot_out_dtype in [torch.float32, torch.bfloat16]:\n+                dot_out_dtype = tl.float32\n+            else:\n+                dot_out_dtype = tl.int32\n         # launch kernel\n         grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']), META['SPLIT_K'])\n         _kernel[grid](a, b, c, M, N, K,\n                       a.stride(0), a.stride(1),\n                       b.stride(0), b.stride(1),\n                       c.stride(0), c.stride(1),\n-                      GROUP_M=8, ACC_TYPE=ACC_TYPE)\n+                      dot_out_dtype=dot_out_dtype,\n+                      GROUP_M=8)\n         return c\n \n     @staticmethod\n-    def forward(ctx, a, b):\n-        return _matmul._call(a, b)\n+    def forward(ctx, a, b, dot_out_dtype=None):\n+        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype)\n \n \n matmul = _matmul.apply"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 129, "deletions": 1, "changes": 130, "file_content_changes": "@@ -222,4 +222,132 @@ func.func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index,\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return %loop#1 : tensor<128x128xf32, #C>\n-}\n\\ No newline at end of file\n+}\n+\n+// CHECK: func.func @lut_bmm\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.insert_slice_async\n+// CHECK: triton_gpu.async_commit_group\n+// CHECK: %[[LUT_PTR:.*]] = tt.addptr\n+// CHECK: %arg27 = %[[LUT_PTR]]\n+// CHECK: %[[LUT_BUFFER_0:.*]] = tt.load %arg27, {{.*}}\n+// CHECK: %[[LUT_BUFFER_1:.*]] = arith.muli {{.*}}, %[[LUT_BUFFER_0]]\n+// CHECK: %[[LUT_BUFFER_2:.*]] = tt.splat %[[LUT_BUFFER_1]]\n+// CHECK: %[[NEXT_BUFFER_0:.*]] = tt.addptr {{.*}}, %[[LUT_BUFFER_2]]\n+// CHECK: %[[NEXT_BUFFER_1:.*]] = tt.addptr %arg26, {{.*}}\n+// CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_1]]\n+// CHECK: triton_gpu.insert_slice_async %[[NEXT_BUFFER_0]]\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}> \n+func.func @lut_bmm(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg7: i32 {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg9: i32 {tt.divisibility = 16 : i32}, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32 {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}, %arg17: i32 {tt.divisibility = 16 : i32}) { \n+  %cst = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma> \n+  %c4_i32 = arith.constant 4 : i32\n+  %c1 = arith.constant 1 : index\n+  %c0 = arith.constant 0 : index \n+  %c0_i64 = arith.constant 0 : i64\n+  %c1_i32 = arith.constant 1 : i32\n+  %0 = tt.get_program_id {axis = 2 : i32} : i32\n+  %1 = tt.get_program_id {axis = 0 : i32} : i32\n+  %2 = tt.get_program_id {axis = 1 : i32} : i32 \n+  %3 = tt.get_num_programs {axis = 0 : i32} : i32 \n+  %4 = tt.get_num_programs {axis = 1 : i32} : i32 \n+  %5 = arith.muli %1, %4 : i32\n+  %6 = arith.addi %5, %2 : i32\n+  %7 = arith.muli %4, %c4_i32 : i32\n+  %8 = arith.divsi %6, %7 : i32\n+  %9 = arith.muli %8, %c4_i32 : i32 \n+  %10 = arith.subi %3, %9 : i32 \n+  %11 = arith.cmpi slt, %10, %c4_i32 : i32 \n+  %12 = arith.select %11, %10, %c4_i32 : i32 \n+  %13 = arith.remsi %6, %12 : i32 \n+  %14 = arith.addi %9, %13 : i32 \n+  %15 = arith.remsi %6, %7 : i32 \n+  %16 = arith.divsi %15, %12 : i32 \n+  %17 = arith.muli %arg5, %0 : i32 \n+  %18 = tt.addptr %arg4, %17 : !tt.ptr<i64>, i32\n+  %19 = tt.addptr %18, %14 : !tt.ptr<i64>, i32\n+  %20 = tt.load %19 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64 \n+  %21 = tt.addptr %19, %c1_i32 : !tt.ptr<i64>, i32\n+  %22 = tt.load %21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64 \n+  %23 = arith.subi %22, %20 : i64 \n+  %24 = arith.cmpi eq, %23, %c0_i64 : i64 \n+  cf.cond_br %24, ^bb1, ^bb2\n+^bb1:  // pred: ^bb0\n+  return\n+^bb2:  // pred: ^bb0\n+  %25 = arith.muli %arg1, %0 : i32 \n+  %26 = tt.addptr %arg0, %25 : !tt.ptr<f16>, i32\n+  %27 = arith.extsi %arg2 : i32 to i64\n+  %28 = arith.muli %27, %20 : i64 \n+  %29 = tt.addptr %26, %28 : !tt.ptr<f16>, i64\n+  %30 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %31 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+  %32 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+  %33 = tt.expand_dims %30 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n+  %34 = tt.expand_dims %31 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xi32, #blocked1>\n+  %35 = tt.expand_dims %32 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<16x1xi32, #blocked>\n+  %36 = tt.splat %arg3 : (i32) -> tensor<16x1xi32, #blocked>\n+  %37 = arith.muli %36, %33 : tensor<16x1xi32, #blocked>\n+  %38 = tt.splat %29 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n+  %39 = tt.addptr %38, %37 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n+  %40 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  %41 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+  %42 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+  %43 = tt.expand_dims %40 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n+  %44 = tt.expand_dims %41 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x16xi32, #blocked1>\n+  %45 = tt.expand_dims %42 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n+  %46 = tt.broadcast %39 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n+  %47 = tt.broadcast %43 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n+  %48 = tt.broadcast %45 : (tensor<1x16xi32, #blocked>) -> tensor<16x16xi32, #blocked>\n+  %49 = tt.addptr %46, %47 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+  %50 = arith.muli %arg9, %0 : i32 \n+  %51 = tt.addptr %arg8, %50 : !tt.ptr<f16>, i32\n+  %52 = arith.muli %arg11, %16 : i32 \n+  %53 = tt.addptr %51, %52 : !tt.ptr<f16>, i32\n+  %54 = tt.splat %53 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked1>\n+  %55 = tt.addptr %54, %34 : tensor<16x1x!tt.ptr<f16>, #blocked1>, tensor<16x1xi32, #blocked1>\n+  %56 = tt.splat %arg12 : (i32) -> tensor<1x16xi32, #blocked1>\n+  %57 = arith.muli %56, %44 : tensor<1x16xi32, #blocked1>\n+  %58 = tt.broadcast %55 : (tensor<16x1x!tt.ptr<f16>, #blocked1>) -> tensor<16x16x!tt.ptr<f16>, #blocked1>\n+  %59 = tt.broadcast %57 : (tensor<1x16xi32, #blocked1>) -> tensor<16x16xi32, #blocked1>\n+  %60 = tt.addptr %58, %59 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi32, #blocked1>\n+  %61 = arith.muli %arg14, %0 : i32\n+  %62 = tt.addptr %arg13, %61 : !tt.ptr<f16>, i32\n+  %63 = arith.muli %arg15, %14 : i32\n+  %64 = tt.addptr %62, %63 : !tt.ptr<f16>, i32\n+  %65 = arith.muli %arg16, %16 : i32\n+  %66 = tt.addptr %64, %65 : !tt.ptr<f16>, i32\n+  %67 = tt.splat %arg17 : (i32) -> tensor<16x1xi32, #blocked>\n+  %68 = arith.muli %67, %35 : tensor<16x1xi32, #blocked>\n+  %69 = tt.splat %66 : (!tt.ptr<f16>) -> tensor<16x1x!tt.ptr<f16>, #blocked>\n+  %70 = tt.addptr %69, %68 : tensor<16x1x!tt.ptr<f16>, #blocked>, tensor<16x1xi32, #blocked>\n+  %71 = tt.broadcast %70 : (tensor<16x1x!tt.ptr<f16>, #blocked>) -> tensor<16x16x!tt.ptr<f16>, #blocked>\n+  %72 = tt.addptr %71, %48 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+  %73 = arith.muli %arg7, %0 : i32\n+  %74 = tt.addptr %arg6, %73 : !tt.ptr<i64>, i32\n+  %75 = tt.addptr %74, %20 : !tt.ptr<i64>, i64\n+  %76 = arith.index_cast %23 : i64 to index\n+  %77 = arith.extsi %arg10 : i32 to i64\n+  %78 = tt.splat %arg2 : (i32) -> tensor<16x16xi32, #blocked>\n+  %79:3 = scf.for %arg18 = %c0 to %76 step %c1 iter_args(%arg19 = %cst, %arg20 = %49, %arg21 = %75) -> (tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>) {\n+    %82 = tt.load %arg20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked>\n+    %83 = tt.load %arg21 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : i64\n+    %84 = arith.muli %77, %83 : i64\n+    %85 = tt.splat %84 : (i64) -> tensor<16x16xi64, #blocked1>\n+    %86 = tt.addptr %60, %85 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi64, #blocked1>\n+    %87 = tt.load %86 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked1>\n+    %88 = triton_gpu.convert_layout %82 : (tensor<16x16xf16, #blocked>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>>\n+    %89 = triton_gpu.convert_layout %87 : (tensor<16x16xf16, #blocked1>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>>\n+    %90 = tt.dot %88, %89, %arg19 {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma}>> -> tensor<16x16xf32, #mma>\n+    %91 = tt.addptr %arg20, %78 : tensor<16x16x!tt.ptr<f16>, #blocked>, tensor<16x16xi32, #blocked>\n+    %92 = tt.addptr %arg21, %c1_i32 : !tt.ptr<i64>, i32\n+    scf.yield %90, %91, %92 : tensor<16x16xf32, #mma>, tensor<16x16x!tt.ptr<f16>, #blocked>, !tt.ptr<i64>\n+  }\n+  %80 = arith.truncf %79#0 : tensor<16x16xf32, #mma> to tensor<16x16xf16, #mma>\n+  %81 = triton_gpu.convert_layout %80 : (tensor<16x16xf16, #mma>) -> tensor<16x16xf16, #blocked>\n+  tt.store %72, %81 {cache = 1 : i32, evict = 1 : i32} : tensor<16x16xf16, #blocked>\n+  return\n+}"}]