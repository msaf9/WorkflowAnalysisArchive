[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -31,6 +31,8 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n SmallVector<unsigned> getSizePerThread(Attribute layout);\n \n+SmallVector<unsigned> getContigPerThread(Attribute layout);\n+\n SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n \n SmallVector<unsigned> getShapePerCTA(const Attribute &layout);"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -13,6 +13,7 @@\n \n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n@@ -60,8 +61,8 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   assert(srcLayout && dstLayout &&\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n   auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n-  unsigned srcContigPerThread = getSizePerThread(srcLayout)[inOrd[0]];\n-  unsigned dstContigPerThread = getSizePerThread(dstLayout)[outOrd[0]];\n+  unsigned srcContigPerThread = getContigPerThread(srcLayout)[inOrd[0]];\n+  unsigned dstContigPerThread = getContigPerThread(dstLayout)[outOrd[0]];\n   // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n   //       that we cannot do vectorization.\n   inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 24, "deletions": 19, "changes": 43, "file_content_changes": "@@ -2901,12 +2901,12 @@ struct ConvertLayoutOpConversion\n         Value mmaThreadIdInGrp = urem(laneId, _4);\n         Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, _2);\n         Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, _1);\n-        Value colWarpOffset = mul(multiDimWarpId[0], _16);\n-        mmaColIdx[0] = add(mmaGrpId, colWarpOffset);\n-        mmaColIdx[1] = add(mmaGrpIdP8, colWarpOffset);\n-        Value rowWarpOffset = mul(multiDimWarpId[1], _8);\n-        mmaRowIdx[0] = add(mmaThreadIdInGrpM2, rowWarpOffset);\n-        mmaRowIdx[1] = add(mmaThreadIdInGrpM2P1, rowWarpOffset);\n+        Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n+        mmaRowIdx[0] = add(mmaGrpId, rowWarpOffset);\n+        mmaRowIdx[1] = add(mmaGrpIdP8, rowWarpOffset);\n+        Value colWarpOffset = mul(multiDimWarpId[1], _8);\n+        mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n+        mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n       } else if (mmaLayout.getVersion() == 1) {\n         multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n         multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 16));\n@@ -2920,7 +2920,7 @@ struct ConvertLayoutOpConversion\n         Value rowOffset = add(mul(multiDimWarpId[1], _16), partRowOffset);\n         mmaRowIdx[0] = add(urem(laneId, _2), rowOffset);\n         mmaRowIdx[1] = add(mmaRowIdx[0], _2);\n-        mmaColIdx[0] = add(udiv(urem(laneId, _4), _2), colOffset);\n+        mmaColIdx[0] = add(mul(udiv(urem(laneId, _4), _2), _2), colOffset);\n         mmaColIdx[1] = add(mmaColIdx[0], _1);\n         mmaColIdx[2] = add(mmaColIdx[0], _4);\n         mmaColIdx[3] = add(mmaColIdx[0], idx_val(5));\n@@ -2931,28 +2931,28 @@ struct ConvertLayoutOpConversion\n       assert(rank == 2);\n       SmallVector<Value> multiDimOffset(rank);\n       if (mmaLayout.getVersion() == 2) {\n-        multiDimOffset[0] = elemId < 2 ? mmaColIdx[0] : mmaColIdx[1];\n-        multiDimOffset[1] = elemId % 2 == 0 ? mmaRowIdx[0] : mmaRowIdx[1];\n+        multiDimOffset[0] = elemId < 2 ? mmaRowIdx[0] : mmaRowIdx[1];\n+        multiDimOffset[1] = elemId % 2 == 0 ? mmaColIdx[0] : mmaColIdx[1];\n         multiDimOffset[0] = add(\n             multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n         multiDimOffset[1] = add(\n             multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n       } else if (mmaLayout.getVersion() == 1) {\n         // the order of elements in a thread:\n-        //   c0, c1, c4, c5\n-        //   c2, c3, c6, c7\n+        //   c0, c1, ...  c4, c5\n+        //   c2, c3, ...  c6, c7\n         if (elemId < 2) {\n-          multiDimOffset[0] = mmaColIdx[elemId % 2];\n-          multiDimOffset[1] = mmaRowIdx[0];\n+          multiDimOffset[0] = mmaRowIdx[0];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2];\n         } else if (elemId >= 2 && elemId < 4) {\n-          multiDimOffset[0] = mmaColIdx[elemId % 2];\n-          multiDimOffset[1] = mmaRowIdx[1];\n+          multiDimOffset[0] = mmaRowIdx[1];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2];\n         } else if (elemId >= 4 && elemId < 6) {\n-          multiDimOffset[0] = mmaColIdx[elemId % 2 + 2];\n-          multiDimOffset[1] = mmaRowIdx[0];\n+          multiDimOffset[0] = mmaRowIdx[0];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n         } else if (elemId >= 6) {\n-          multiDimOffset[0] = mmaColIdx[elemId % 2 + 2];\n-          multiDimOffset[1] = mmaRowIdx[1];\n+          multiDimOffset[0] = mmaRowIdx[1];\n+          multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n         }\n         multiDimOffset[0] = add(\n             multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n@@ -3051,6 +3051,7 @@ void ConvertLayoutOpConversion::processReplica(\n                             multiDimCTAInRepId, shapePerCTA);\n       Value offset =\n           linearize(rewriter, loc, multiDimOffset, paddedRepShape, outOrd);\n+\n       auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n       Value ptr = gep(elemPtrTy, smemBase, offset);\n       auto vecTy = vec_ty(llvmElemTy, vec);\n@@ -3170,6 +3171,10 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n     ConversionPatternRewriter &rewriter) const {\n+  // TODO[Keren]: A temporary workaround for an issue from membar pass.\n+  // https://triton-lang.slack.com/archives/C042VBSQWNS/p1669796615860699?thread_ts=1669779203.526739&cid=C042VBSQWNS\n+  barrier();\n+\n   auto loc = op.getLoc();\n   Value src = op.src();\n   Value dst = op.result();"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -109,6 +109,8 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n     if (mmaLayout.getVersion() == 2) {\n       return {2, 2};\n     } else if (mmaLayout.getVersion() == 1) {\n+      // Note: here the definition of sizePerThread is obscure, which doesn't\n+      // mean vecSize=4 can be supported in the last dimension.\n       return {2, 4};\n     } else {\n       llvm_unreachable(\"Unexpected mma version\");\n@@ -140,6 +142,15 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   }\n }\n \n+SmallVector<unsigned> getContigPerThread(Attribute layout) {\n+  if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    assert(mmaLayout.getVersion() == 1 || mmaLayout.getVersion() == 2);\n+    return {1, 2};\n+  } else {\n+    return getSizePerThread(layout);\n+  }\n+}\n+\n SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n   SmallVector<unsigned> threads;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -735,9 +735,13 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_layout_mmav1_block\n   func @convert_layout_mmav1_blocked(%arg0: tensor<32x16xf32, #mma>) {\n     // CHECK: llvm.store\n-    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n-    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: nvvm.barrier0\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>"}]