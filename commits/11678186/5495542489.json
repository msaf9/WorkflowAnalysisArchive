[{"filename": "include/triton/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n add_subdirectory(Conversion)\n add_subdirectory(Dialect)\n+add_subdirectory(Target)"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -49,7 +49,8 @@ SmallVector<unsigned> getContigPerThread(Attribute layout);\n // for thread 0 would be [A_{0, 0}, A_{0, 0}, A_{0, 0}, A_{0, 0}], returns [1,\n // 1]. Whereas for a tensor shape [128, 128], the elements for thread 0 would be\n // [A_{0, 0}, A_{0, 1}, A_{0, 2}, A_{0, 3}], returns [1, 4].\n-SmallVector<unsigned> getUniqueContigPerThread(Type type);\n+SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n+                                               ArrayRef<int64_t> tensorShape);\n \n // Returns the number of threads per warp that have access to non-replicated\n // elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,"}, {"filename": "include/triton/Target/CMakeLists.txt", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+add_subdirectory(LLVMIR)"}, {"filename": "include/triton/Target/LLVMIR/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls -name LLVMIR)\n+add_public_tablegen_target(LLVMIRIncGen)"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n-#ifndef TRITON_TARGET_LLVMIRTRANSLATION_H\n-#define TRITON_TARGET_LLVMIRTRANSLATION_H\n+#ifndef TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n+#define TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n #include \"llvm/ADT/StringRef.h\"\n #include <memory>\n #include <string>"}, {"filename": "include/triton/Target/LLVMIR/Passes.h", "status": "added", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -0,0 +1,17 @@\n+#ifndef TRITON_TARGET_LLVM_IR_PASSES_H\n+#define TRITON_TARGET_LLVM_IR_PASSES_H\n+\n+#include \"mlir/Pass/Pass.h\"\n+\n+namespace mlir {\n+\n+/// Create a pass to add DIScope\n+std::unique_ptr<Pass> createLLVMDIScopePass();\n+\n+/// Generate the code for registering conversion passes.\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Target/LLVMIR/Passes.h.inc\"\n+\n+} // namespace mlir\n+\n+#endif // TRITON_TARGET_LLVM_IR_PASSES_H"}, {"filename": "include/triton/Target/LLVMIR/Passes.td", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+#ifndef TRITON_TARGET_LLVMIR_PASSES\n+#define TRITON_TARGET_LLVMIR_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+def LLVMDIScope: Pass<\"enable-line-info\", \"mlir::ModuleOp\"> {\n+  let summary = \"Materialize LLVM line info\";\n+  let description = [{\n+    This pass materializes line mapping information for LLVM IR dialect operations.\n+  }];\n+\n+  let constructor = \"mlir::createLLVMDIScopePass()\";\n+}\n+\n+#endif"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -918,7 +918,8 @@ unsigned ModuleAxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto order = triton::gpu::getOrder(layout);\n   unsigned align = getPtrAlignment(ptr);\n \n-  auto uniqueContigPerThread = triton::gpu::getUniqueContigPerThread(tensorTy);\n+  auto uniqueContigPerThread =\n+      triton::gpu::getUniqueContigPerThread(layout, tensorTy.getShape());\n   assert(order[0] < uniqueContigPerThread.size() &&\n          \"Unxpected uniqueContigPerThread size\");\n   unsigned contiguity = uniqueContigPerThread[order[0]];"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -60,7 +60,9 @@ unsigned ReduceOpHelper::getInterWarpSizeWithUniqueData() {\n \n unsigned ReduceOpHelper::getIntraWarpSizeWithUniqueData() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n-  return std::min(srcReduceDimSize,\n+  unsigned elementPerThreads = triton::gpu::getUniqueContigPerThread(\n+      getSrcLayout(), getSrcShape())[axis];\n+  return std::min(srcReduceDimSize / elementPerThreads,\n                   triton::gpu::getThreadsPerWarpWithUniqueData(\n                       getSrcLayout(), getSrcShape())[axis]);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -20,7 +20,7 @@ namespace SharedToDotOperandMMAv1 {\n using CoordTy = SmallVector<Value>;\n using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n \n-SmallVector<CoordTy> getMNCoords(Value thread,\n+SmallVector<CoordTy> getMNCoords(Value thread, Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  ArrayRef<unsigned int> wpt,\n                                  const MmaEncodingAttr &mmaLayout,\n@@ -187,8 +187,8 @@ struct ConvertLayoutOpConversion\n         auto [isARow, isBRow, isAVec4, isBVec4, _] =\n             mmaLayout.decodeVoltaLayoutStates();\n         auto coords = SharedToDotOperandMMAv1::getMNCoords(\n-            threadId, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout, shape,\n-            isARow, isBRow, isAVec4, isBVec4);\n+            threadId, loc, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout,\n+            shape, isARow, isBRow, isAVec4, isBVec4);\n         return coords[elemId];\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -339,7 +339,7 @@ namespace SharedToDotOperandMMAv1 {\n using CoordTy = SmallVector<Value>;\n using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n \n-SmallVector<CoordTy> getMNCoords(Value thread,\n+SmallVector<CoordTy> getMNCoords(Value thread, Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  ArrayRef<unsigned int> wpt,\n                                  const MmaEncodingAttr &mmaLayout,\n@@ -348,7 +348,6 @@ SmallVector<CoordTy> getMNCoords(Value thread,\n   static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n \n   auto *ctx = thread.getContext();\n-  auto loc = UnknownLoc::get(ctx);\n   Value _1 = i32_val(1);\n   Value _2 = i32_val(2);\n   Value _4 = i32_val(4);"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -36,7 +36,6 @@ bool isBroadcastConstantCombinable(Attribute value) {\n \n DenseElementsAttr getConstantValue(Builder &builder, Attribute value,\n                                    Value bcast_res) {\n-\n   auto resType = bcast_res.getType().cast<ShapedType>();\n   DenseElementsAttr res;\n   if (auto denseValue = value.dyn_cast<DenseElementsAttr>()) {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 6, "deletions": 11, "changes": 17, "file_content_changes": "@@ -221,28 +221,23 @@ SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   }\n }\n \n-SmallVector<unsigned> getUniqueContigPerThread(Type type) {\n-  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n-    return SmallVector<unsigned>(1, 1);\n-  auto tensorType = type.cast<RankedTensorType>();\n-  auto shape = tensorType.getShape();\n+SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n+                                               ArrayRef<int64_t> shape) {\n   // If slice layout, call recursively on parent layout, and drop\n   // sliced dim\n-  if (auto sliceLayout =\n-          tensorType.getEncoding().dyn_cast<SliceEncodingAttr>()) {\n+  if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n     auto parentShape = sliceLayout.paddedShape(shape);\n-    auto parentTy = RankedTensorType::get(\n-        parentShape, tensorType.getElementType(), parentLayout);\n-    auto parentUniqueContigPerThread = getUniqueContigPerThread(parentTy);\n+    auto parentUniqueContigPerThread =\n+        getUniqueContigPerThread(parentLayout, parentShape);\n     parentUniqueContigPerThread.erase(parentUniqueContigPerThread.begin() +\n                                       sliceLayout.getDim());\n     return parentUniqueContigPerThread;\n   }\n   // Base case\n   auto rank = shape.size();\n   SmallVector<unsigned> ret(rank);\n-  auto contigPerThread = getContigPerThread(tensorType.getEncoding());\n+  auto contigPerThread = getContigPerThread(layout);\n   assert(contigPerThread.size() == rank && \"Unexpected contigPerThread size\");\n   for (int d = 0; d < rank; ++d) {\n     ret[d] = std::min<unsigned>(shape[d], contigPerThread[d]);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -492,7 +492,7 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n     // replace\n     SmallVector<Value, 4> newResults = newForOp->getResults();\n     newResults[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        rewriter.getUnknownLoc(), origType, newForOp->getResult(i));\n+        newForOp.getLoc(), origType, newForOp->getResult(i));\n     newResults[i].getDefiningOp()->moveAfter(newForOp);\n     return newResults;\n   }"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1,9 +1,13 @@\n add_mlir_translation_library(TritonLLVMIR\n         LLVMIRTranslation.cpp\n+        LLVMDIScope.cpp\n \n         LINK_COMPONENTS\n         Core\n \n+        DEPENDS\n+        LLVMIRIncGen\n+\n         LINK_LIBS\n         ${CMAKE_DL_LIBS}\n         PUBLIC"}, {"filename": "lib/Target/LLVMIR/LLVMDIScope.cpp", "status": "added", "additions": 148, "deletions": 0, "changes": 148, "file_content_changes": "@@ -0,0 +1,148 @@\n+#include \"triton/Target/LLVMIR/Passes.h\"\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"llvm/BinaryFormat/Dwarf.h\"\n+#include \"llvm/Support/Debug.h\"\n+#include \"llvm/Support/Path.h\"\n+\n+//===----------------------------------------------------------------------===//\n+// This file implements a pass to add debug info scope to LLVM operations, and\n+// is inspired by the DIScopeForLLVMFuncOpPass in LLVM/MLIR. Different from the\n+// DIScopeForLLVMFuncOpPass, this pass also handles inlined functions.\n+//===----------------------------------------------------------------------===//\n+\n+using namespace mlir;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Target/LLVMIR/Passes.h.inc\"\n+\n+namespace {\n+\n+/// Attempt to extract a filename for the given loc.\n+FileLineColLoc extractFileLoc(Location loc) {\n+  if (auto fileLoc = dyn_cast<FileLineColLoc>(loc))\n+    return fileLoc;\n+  if (auto nameLoc = dyn_cast<NameLoc>(loc))\n+    return extractFileLoc(nameLoc.getChildLoc());\n+  if (auto opaqueLoc = dyn_cast<OpaqueLoc>(loc))\n+    return extractFileLoc(opaqueLoc.getFallbackLocation());\n+  if (auto fusedLoc = dyn_cast<FusedLoc>(loc))\n+    return extractFileLoc(fusedLoc.getLocations().front());\n+  if (auto callerLoc = dyn_cast<CallSiteLoc>(loc))\n+    return extractFileLoc(callerLoc.getCaller());\n+  StringAttr unknownFile = mlir::StringAttr::get(loc.getContext(), \"<unknown>\");\n+  return mlir::FileLineColLoc::get(unknownFile, 0, 0);\n+}\n+\n+/// Add a debug info scope to LLVMFuncOp that are missing it.\n+struct LLVMDIScopePass : public LLVMDIScopeBase<LLVMDIScopePass> {\n+  LLVMDIScopePass() = default;\n+\n+  void setSubprogramAttr(LLVM::LLVMFuncOp funcOp) {\n+    Location loc = funcOp.getLoc();\n+    if (loc->findInstanceOf<mlir::FusedLocWith<LLVM::DISubprogramAttr>>())\n+      return;\n+\n+    MLIRContext *context = &getContext();\n+\n+    // To find a DICompileUnitAttr attached to a parent (the module for\n+    // example), otherwise create a default one.\n+    LLVM::DICompileUnitAttr compileUnitAttr;\n+    if (ModuleOp module = funcOp->getParentOfType<ModuleOp>()) {\n+      auto fusedCompileUnitAttr =\n+          module->getLoc()\n+              ->findInstanceOf<mlir::FusedLocWith<LLVM::DICompileUnitAttr>>();\n+      if (fusedCompileUnitAttr)\n+        compileUnitAttr = fusedCompileUnitAttr.getMetadata();\n+    }\n+\n+    // Filename, line and colmun to associate to the function.\n+    LLVM::DIFileAttr fileAttr;\n+    int64_t line = 1, col = 1;\n+    FileLineColLoc fileLoc = extractFileLoc(loc);\n+    if (!fileLoc && compileUnitAttr) {\n+      fileAttr = compileUnitAttr.getFile();\n+    } else if (!fileLoc) {\n+      fileAttr = LLVM::DIFileAttr::get(context, \"<unknown>\", \"\");\n+    } else {\n+      line = fileLoc.getLine();\n+      col = fileLoc.getColumn();\n+      StringRef inputFilePath = fileLoc.getFilename().getValue();\n+      fileAttr = LLVM::DIFileAttr::get(\n+          context, llvm::sys::path::filename(inputFilePath),\n+          llvm::sys::path::parent_path(inputFilePath));\n+    }\n+    if (!compileUnitAttr) {\n+      compileUnitAttr = LLVM::DICompileUnitAttr::get(\n+          context, llvm::dwarf::DW_LANG_C, fileAttr,\n+          StringAttr::get(context, \"triton\"), /*isOptimized=*/true,\n+          LLVM::DIEmissionKind::LineTablesOnly);\n+    }\n+    auto subroutineTypeAttr =\n+        LLVM::DISubroutineTypeAttr::get(context, llvm::dwarf::DW_CC_normal, {});\n+\n+    StringAttr funcNameAttr = funcOp.getNameAttr();\n+    // Note that scopeline is set differently from LLVM's\n+    // DIScopeForLLVMFuncOpPass. I don't find reasons why scopeline should be\n+    // the column offset\n+    auto subprogramAttr =\n+        LLVM::DISubprogramAttr::get(context, compileUnitAttr, fileAttr,\n+                                    funcNameAttr, funcNameAttr, fileAttr,\n+                                    /*line=*/line,\n+                                    /*scopeline=*/line,\n+                                    LLVM::DISubprogramFlags::Definition |\n+                                        LLVM::DISubprogramFlags::Optimized,\n+                                    subroutineTypeAttr);\n+    funcOp->setLoc(FusedLoc::get(context, {loc}, subprogramAttr));\n+  }\n+\n+  // Get a nested loc for inlined functions\n+  Location getNestedLoc(Operation *op, LLVM::DIScopeAttr scopeAttr,\n+                        Location calleeLoc) {\n+    auto calleeFileName = extractFileLoc(calleeLoc).getFilename();\n+    auto context = op->getContext();\n+    LLVM::DIFileAttr calleeFileAttr = LLVM::DIFileAttr::get(\n+        context, llvm::sys::path::filename(calleeFileName),\n+        llvm::sys::path::parent_path(calleeFileName));\n+    auto lexicalBlockFileAttr = LLVM::DILexicalBlockFileAttr::get(\n+        context, scopeAttr, calleeFileAttr, /*discriminator=*/0);\n+    Location loc = op->getLoc();\n+    if (calleeLoc.isa<CallSiteLoc>()) {\n+      auto nestedLoc = calleeLoc.cast<CallSiteLoc>().getCallee();\n+      loc = getNestedLoc(op, lexicalBlockFileAttr, nestedLoc);\n+    }\n+    return FusedLoc::get(context, {loc}, lexicalBlockFileAttr);\n+  }\n+\n+  void setLexicalBlockFileAttr(Operation *op) {\n+    auto opLoc = op->getLoc();\n+    if (auto callSiteLoc = dyn_cast<CallSiteLoc>(opLoc)) {\n+      auto callerLoc = callSiteLoc.getCaller();\n+      auto calleeLoc = callSiteLoc.getCallee();\n+      LLVM::DIScopeAttr scopeAttr;\n+      // We assemble the full inline stack so the parent of this loc must be a\n+      // function\n+      auto funcOp = op->getParentOfType<LLVM::LLVMFuncOp>();\n+      auto funcOpLoc = funcOp.getLoc().cast<FusedLoc>();\n+      scopeAttr = funcOpLoc.getMetadata().cast<LLVM::DISubprogramAttr>();\n+      auto loc = getNestedLoc(op, scopeAttr, calleeLoc);\n+      op->setLoc(loc);\n+    }\n+  }\n+\n+  void runOnOperation() override {\n+    getOperation()->walk<WalkOrder::PreOrder>([&](Operation *op) -> void {\n+      if (isa<LLVM::LLVMFuncOp>(op))\n+        setSubprogramAttr(cast<LLVM::LLVMFuncOp>(op));\n+      else\n+        setLexicalBlockFileAttr(op);\n+    });\n+  }\n+};\n+\n+} // end anonymous namespace\n+\n+std::unique_ptr<Pass> mlir::createLLVMDIScopePass() {\n+  return std::make_unique<LLVMDIScopePass>();\n+}"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -2,6 +2,7 @@\n \n #include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/Transforms/Passes.h\"\n #include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n #include \"mlir/ExecutionEngine/OptUtils.h\"\n #include \"mlir/IR/Dialect.h\"\n@@ -15,6 +16,7 @@\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+#include \"triton/Target/LLVMIR/Passes.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"triton/Tools/Sys/GetPlatform.hpp\"\n #include \"llvm/ADT/APInt.h\"\n@@ -329,6 +331,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   }\n   auto printingFlags = mlir::OpPrintingFlags();\n   printingFlags.elideLargeElementsAttrs(16);\n+  printingFlags.enableDebugInfo();\n   pm.enableIRPrinting(\n       /*shouldPrintBeforePass=*/nullptr,\n       /*shouldPrintAfterPass=*/\n@@ -347,6 +350,8 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   // Simplify the IR\n   pm.addPass(mlir::createCSEPass());\n   pm.addPass(mlir::createSymbolDCEPass());\n+  if (!::triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\"))\n+    pm.addPass(mlir::createLLVMDIScopePass());\n \n   if (failed(pm.run(module))) {\n     llvm::errs() << \"Pass execution failed\";"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 499, "deletions": 511, "changes": 1010, "file_content_changes": "@@ -72,6 +72,90 @@ void init_triton_runtime(py::module &&m) {\n       .export_values();\n }\n \n+// A custom op builder that keeps track of the last location\n+class TritonOpBuilder {\n+public:\n+  TritonOpBuilder(mlir::MLIRContext *context) {\n+    builder = std::make_unique<mlir::OpBuilder>(context);\n+    lastLoc = std::make_unique<mlir::Location>(builder->getUnknownLoc());\n+  }\n+\n+  mlir::OpBuilder &getBuilder() { return *builder; }\n+\n+  bool isLineInfoEnabled() { return lineInfoEnabled; }\n+\n+  void setLastLoc(mlir::Location loc) {\n+    if (lineInfoEnabled)\n+      lastLoc = std::make_unique<mlir::Location>(loc);\n+  }\n+\n+  void setLastLoc(const std::string &fileName, int line, int column) {\n+    auto context = builder->getContext();\n+    setLastLoc(mlir::FileLineColLoc::get(context, fileName, line, column));\n+  }\n+\n+  mlir::Location getLastLoc() {\n+    assert(lastLoc);\n+    return *lastLoc;\n+  }\n+\n+  void setInsertionPointToStart(mlir::Block &block) {\n+    if (!block.empty())\n+      setLastLoc(block.begin()->getLoc());\n+    else\n+      setLastLoc(builder->getUnknownLoc());\n+    builder->setInsertionPointToStart(&block);\n+  }\n+\n+  void setInsertionPointToEnd(mlir::Block &block) {\n+    if (!block.empty())\n+      setLastLoc(block.back().getLoc());\n+    else\n+      setLastLoc(builder->getUnknownLoc());\n+    builder->setInsertionPointToEnd(&block);\n+  }\n+\n+  void setInsertionPointAfter(mlir::Operation &op) {\n+    setLastLoc(op.getLoc());\n+    builder->setInsertionPointAfter(&op);\n+  }\n+\n+  void restoreInsertionPoint(mlir::OpBuilder::InsertPoint pt) {\n+    if (pt.isSet() && pt.getPoint() != pt.getBlock()->end())\n+      setLastLoc(pt.getPoint()->getLoc());\n+    else\n+      setLastLoc(builder->getUnknownLoc());\n+    builder->restoreInsertionPoint(pt);\n+  }\n+\n+  template <typename OpTy, typename... Args> OpTy create(Args &&...args) {\n+    auto loc = getLastLoc();\n+    return builder->create<OpTy>(loc, std::forward<Args>(args)...);\n+  }\n+\n+  // Overload to create or fold a single result operation.\n+  template <typename OpTy, typename... Args>\n+  std::enable_if_t<OpTy::template hasTrait<mlir::OpTrait::OneResult>(),\n+                   mlir::Value>\n+  createOrFold(Args &&...args) {\n+    auto loc = getLastLoc();\n+    return builder->createOrFold<OpTy>(loc, std::forward<Args>(args)...);\n+  }\n+\n+  // Overload to create or fold a zero result operation.\n+  template <typename OpTy, typename... Args>\n+  std::enable_if_t<OpTy::template hasTrait<mlir::OpTrait::ZeroResults>(), OpTy>\n+  createOrFold(Args &&...args) {\n+    auto loc = getLastLoc();\n+    return builder->createOrFold<OpTy>(loc, std::forward<Args>(args)...);\n+  }\n+\n+private:\n+  std::unique_ptr<mlir::OpBuilder> builder;\n+  std::unique_ptr<mlir::Location> lastLoc;\n+  bool lineInfoEnabled = !triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\");\n+};\n+\n /*****************************************************************************/\n /* Python bindings for triton::ir                                            */\n /*****************************************************************************/\n@@ -191,6 +275,14 @@ void init_triton_ir(py::module &&m) {\n                                        self.getInputs().end());\n       });\n \n+  py::class_<mlir::Location>(m, \"location\")\n+      .def(\"__str__\", [](mlir::Location &self) {\n+        std::string str;\n+        llvm::raw_string_ostream os(str);\n+        self.print(os);\n+        return os.str();\n+      });\n+\n   py::class_<mlir::Value>(m, \"value\")\n       .def(\"set_attr\",\n            [](mlir::Value &self, std::string &name,\n@@ -481,367 +573,347 @@ void init_triton_ir(py::module &&m) {\n \n   py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\");\n \n-  py::class_<mlir::OpBuilder>(m, \"builder\", py::dynamic_attr())\n+  py::class_<TritonOpBuilder>(m, \"builder\", py::dynamic_attr())\n       .def(py::init<mlir::MLIRContext *>())\n-      // // getters\n-      .def_property_readonly(\"context\", &mlir::OpBuilder::getContext,\n-                             ret::reference)\n+      // getters\n       .def(\"create_module\",\n-           [](mlir::OpBuilder &self) -> mlir::ModuleOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::ModuleOp>(loc);\n-           })\n-      .def(\"ret\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Value> &vals) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::ReturnOp>(loc, vals);\n-           })\n-      .def(\"call\",\n-           [](mlir::OpBuilder &self, mlir::triton::FuncOp &func,\n-              std::vector<mlir::Value> &args) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n-             auto callOp = self.create<mlir::triton::CallOp>(loc, func, args);\n-             return callOp;\n+           [](TritonOpBuilder &self) -> mlir::ModuleOp {\n+             return self.create<mlir::ModuleOp>();\n            })\n       // insertion block/point\n       .def(\"set_insertion_point_to_start\",\n-           [](mlir::OpBuilder &self, mlir::Block &block) -> void {\n-             self.setInsertionPointToStart(&block);\n+           [](TritonOpBuilder &self, mlir::Block &block) -> void {\n+             self.setInsertionPointToStart(block);\n            })\n       .def(\"set_insertion_point_to_end\",\n-           [](mlir::OpBuilder &self, mlir::Block &block) {\n-             self.setInsertionPointToEnd(&block);\n+           [](TritonOpBuilder &self, mlir::Block &block) {\n+             self.setInsertionPointToEnd(block);\n            })\n       .def(\"set_insertion_point_after\",\n-           [](mlir::OpBuilder &self, mlir::Operation &op) {\n-             self.setInsertionPointAfter(&op);\n+           [](TritonOpBuilder &self, mlir::Operation &op) {\n+             self.setInsertionPointAfter(op);\n            })\n       .def(\n           \"get_insertion_block\",\n-          [](mlir::OpBuilder &self) -> mlir::Block * {\n-            return self.getInsertionBlock();\n+          [](TritonOpBuilder &self) -> mlir::Block * {\n+            return self.getBuilder().getInsertionBlock();\n           },\n           ret::reference)\n-      .def(\"get_insertion_point\", &mlir::OpBuilder::saveInsertionPoint)\n-      .def(\"restore_insertion_point\", &mlir::OpBuilder::restoreInsertionPoint)\n-      // .def(\"set_insert_point\", [](ir::builder *self,\n-      // std::pair<ir::basic_block*, ir::instruction*> pt) {\n-      //   ir::basic_block *bb = pt.first;\n-      //   ir::instruction *instr = pt.second;\n-      //   if (instr) {\n-      //     if (bb != instr->get_parent())\n-      //       throw std::runtime_error(\"invalid insertion point, instr not in\n-      //       bb\");\n-      //     self->set_insert_point(instr);\n-      //   } else {\n-      //     assert(bb);\n-      //     self->set_insert_point(bb);\n-      //   }\n-      // })\n+      .def(\"get_insertion_point\",\n+           [](TritonOpBuilder &self) {\n+             return self.getBuilder().saveInsertionPoint();\n+           })\n+      .def(\"restore_insertion_point\",\n+           [](TritonOpBuilder &self, mlir::OpBuilder::InsertPoint pt) {\n+             self.restoreInsertionPoint(pt);\n+           })\n       // Attr\n-      .def(\"get_bool_attr\", &mlir::OpBuilder::getBoolAttr)\n-      .def(\"get_int32_attr\", &mlir::OpBuilder::getI32IntegerAttr)\n+      .def(\"get_bool_attr\",\n+           [](TritonOpBuilder &self, bool value) {\n+             return self.getBuilder().getBoolAttr(value);\n+           })\n+      .def(\"get_int32_attr\",\n+           [](TritonOpBuilder &self, int32_t value) {\n+             return self.getBuilder().getI32IntegerAttr(value);\n+           })\n       // Use arith.ConstantOp to create constants\n       // Constants\n       .def(\"get_int1\",\n-           [](mlir::OpBuilder &self, bool v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, bool v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI1Type()));\n+                 v, self.getBuilder().getI1Type()));\n            })\n       .def(\"get_int8\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI8Type()));\n+                 v, self.getBuilder().getI8Type()));\n            })\n       .def(\"get_int16\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI16Type()));\n+                 v, self.getBuilder().getI16Type()));\n            })\n       .def(\"get_int32\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI32Type()));\n+                 v, self.getBuilder().getI32Type()));\n            })\n       .def(\"get_int64\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI64Type()));\n+                 v, self.getBuilder().getI64Type()));\n            })\n       .def(\"get_bf16\",\n-           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             auto type = self.getBF16Type();\n+           [](TritonOpBuilder &self, float v) -> mlir::Value {\n+             auto type = self.getBuilder().getBF16Type();\n              return self.create<mlir::arith::ConstantFloatOp>(\n-                 loc,\n                  mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n                  type);\n            })\n       .def(\"get_fp16\",\n-           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, float v) -> mlir::Value {\n              return self.create<mlir::arith::ConstantOp>(\n-                 loc, self.getF16FloatAttr(v));\n+                 self.getBuilder().getF16FloatAttr(v));\n            })\n       .def(\"get_fp32\",\n-           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, float v) -> mlir::Value {\n              return self.create<mlir::arith::ConstantOp>(\n-                 loc, self.getF32FloatAttr(v));\n+                 self.getBuilder().getF32FloatAttr(v));\n            })\n       .def(\"get_fp64\",\n-           [](mlir::OpBuilder &self, double v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, double v) -> mlir::Value {\n              return self.create<mlir::arith::ConstantOp>(\n-                 loc, self.getF64FloatAttr(v));\n+                 self.getBuilder().getF64FloatAttr(v));\n            })\n       .def(\"get_null_value\",\n-           [](mlir::OpBuilder &self, mlir::Type type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Type type) -> mlir::Value {\n              if (auto floatTy = type.dyn_cast<mlir::FloatType>())\n                return self.create<mlir::arith::ConstantFloatOp>(\n-                   loc, mlir::APFloat(floatTy.getFloatSemantics(), 0), floatTy);\n+                   mlir::APFloat(floatTy.getFloatSemantics(), 0), floatTy);\n              else if (auto intTy = type.dyn_cast<mlir::IntegerType>())\n-               return self.create<mlir::arith::ConstantIntOp>(loc, 0, intTy);\n+               return self.create<mlir::arith::ConstantIntOp>(0, intTy);\n              else\n                throw std::runtime_error(\"Not implemented\");\n            })\n       .def(\"get_all_ones_value\",\n-           [](mlir::OpBuilder &self, mlir::Type type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Type type) -> mlir::Value {\n              uint64_t val = 0xFFFFFFFFFFFFFFFF;\n              if (auto intTy = type.dyn_cast<mlir::IntegerType>())\n-               return self.create<mlir::arith::ConstantIntOp>(loc, val, intTy);\n+               return self.create<mlir::arith::ConstantIntOp>(val, intTy);\n              else\n                throw std::runtime_error(\"Not implemented\");\n            })\n \n       // Types\n       .def(\"get_void_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getNoneType();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getNoneType();\n            })\n       .def(\"get_int1_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getI1Type();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI1Type();\n            }) // or ret::copy?\n       .def(\"get_int8_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type { return self.getI8Type(); })\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI8Type();\n+           })\n       .def(\"get_int16_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::IntegerType>(16);\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getType<mlir::IntegerType>(16);\n+           })\n+      .def(\"get_int32_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI32Type();\n+           })\n+      .def(\"get_int64_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI64Type();\n            })\n-      .def(\n-          \"get_int32_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getI32Type(); })\n-      .def(\n-          \"get_int64_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getI64Type(); })\n       .def(\"get_fp8e4_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::Float8E4M3FNUZType>();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getType<mlir::Float8E4M3FNUZType>();\n            })\n       .def(\"get_fp8e4b15_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n+           [](TritonOpBuilder &self) -> mlir::Type {\n              // TODO: upstream FP8E4B15 into MLIR, or find a way to externally\n              // have a float-like type compatible with float only native ops\n-             return self.getType<mlir::Float8E4M3B11FNUZType>();\n+             return self.getBuilder().getType<mlir::Float8E4M3B11FNUZType>();\n            })\n       .def(\"get_fp8e5_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::Float8E5M2Type>();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getType<mlir::Float8E5M2Type>();\n+           })\n+      .def(\"get_half_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getF16Type();\n            })\n-      .def(\n-          \"get_half_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getF16Type(); })\n       .def(\"get_bf16_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getBF16Type();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getBF16Type();\n+           })\n+      .def(\"get_float_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getF32Type();\n+           })\n+      .def(\"get_double_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getF64Type();\n            })\n-      .def(\n-          \"get_float_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getF32Type(); })\n-      .def(\n-          \"get_double_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getF64Type(); })\n       .def(\"get_ptr_ty\",\n-           [](mlir::OpBuilder &self, mlir::Type &type,\n+           [](TritonOpBuilder &self, mlir::Type &type,\n               int addrSpace) -> mlir::Type {\n              return mlir::triton::PointerType::get(type, addrSpace);\n            })\n       .def(\"get_block_ty\",\n-           [](mlir::OpBuilder &self, mlir::Type &elementType,\n+           [](TritonOpBuilder &self, mlir::Type &elementType,\n               std::vector<int64_t> &shape) -> mlir::Type {\n              return mlir::RankedTensorType::get(shape, elementType);\n            })\n       .def(\"get_function_ty\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Type> inTypes,\n+           [](TritonOpBuilder &self, std::vector<mlir::Type> inTypes,\n               std::vector<mlir::Type> outTypes) -> mlir::Type {\n-             return self.getFunctionType(inTypes, outTypes);\n+             return self.getBuilder().getFunctionType(inTypes, outTypes);\n+           })\n+      // locs\n+      .def(\"set_loc\", [](TritonOpBuilder &self,\n+                         mlir::Location loc) { self.setLastLoc(loc); })\n+      .def(\"set_loc\",\n+           [](TritonOpBuilder &self, const std::string &fileName, int line,\n+              int column) { self.setLastLoc(fileName, line, column); })\n+      .def(\"get_loc\",\n+           [](TritonOpBuilder &self) -> mlir::Location {\n+             return self.getLastLoc();\n            })\n \n       // Ops\n       .def(\"get_or_insert_function\",\n-           [](mlir::OpBuilder &self, mlir::ModuleOp &module,\n+           [](TritonOpBuilder &self, mlir::ModuleOp &module,\n               std::string &funcName, mlir::Type &funcType,\n               std::string &visibility, bool noinline) -> mlir::triton::FuncOp {\n              if (mlir::Operation *funcOperation = module.lookupSymbol(funcName))\n                return llvm::dyn_cast<mlir::triton::FuncOp>(funcOperation);\n-             auto loc = self.getUnknownLoc();\n              if (auto funcTy = funcType.dyn_cast<mlir::FunctionType>()) {\n                llvm::SmallVector<mlir::NamedAttribute> attrs = {\n-                   mlir::NamedAttribute(self.getStringAttr(\"sym_visibility\"),\n-                                        self.getStringAttr(visibility)),\n-                   mlir::NamedAttribute(self.getStringAttr(\"noinline\"),\n-                                        self.getBoolAttr(noinline))};\n-               return self.create<mlir::triton::FuncOp>(loc, funcName, funcTy,\n+                   mlir::NamedAttribute(\n+                       self.getBuilder().getStringAttr(\"sym_visibility\"),\n+                       self.getBuilder().getStringAttr(visibility)),\n+                   mlir::NamedAttribute(\n+                       self.getBuilder().getStringAttr(\"noinline\"),\n+                       self.getBuilder().getBoolAttr(noinline))};\n+               return self.create<mlir::triton::FuncOp>(funcName, funcTy,\n                                                         attrs);\n              }\n              throw std::runtime_error(\"invalid function type\");\n            })\n       .def(\n           \"create_block\",\n-          [](mlir::OpBuilder &self) -> mlir::Block * {\n-            mlir::Region *parent = self.getBlock()->getParent();\n-            return self.createBlock(parent);\n+          [](TritonOpBuilder &self) -> mlir::Block * {\n+            mlir::Region *parent = self.getBuilder().getBlock()->getParent();\n+            return self.getBuilder().createBlock(parent);\n           },\n           ret::reference)\n       .def(\n           \"create_block_with_parent\",\n-          [](mlir::OpBuilder &self, mlir::Region &parent,\n+          [](TritonOpBuilder &self, mlir::Region &parent,\n              std::vector<mlir::Type> &argTypes) -> mlir::Block * {\n-            auto argLoc = self.getUnknownLoc();\n-            llvm::SmallVector<mlir::Location, 8> argLocs(argTypes.size(),\n-                                                         argLoc);\n-            return self.createBlock(&parent, {}, argTypes, argLocs);\n+            // TODO: update arg loc\n+            auto loc = self.getBuilder().getUnknownLoc();\n+            llvm::SmallVector<mlir::Location, 8> argLocs(argTypes.size(), loc);\n+            return self.getBuilder().createBlock(&parent, {}, argTypes,\n+                                                 argLocs);\n           },\n           ret::reference)\n       .def(\n           \"new_block\",\n-          [](mlir::OpBuilder &self) -> mlir::Block * {\n+          [](TritonOpBuilder &self) -> mlir::Block * {\n             return new mlir::Block();\n           },\n           ret::reference)\n+      // Function\n+      .def(\"ret\",\n+           [](TritonOpBuilder &self,\n+              std::vector<mlir::Value> &vals) -> mlir::OpState {\n+             return self.create<mlir::triton::ReturnOp>(vals);\n+           })\n+      .def(\"call\",\n+           [](TritonOpBuilder &self, mlir::triton::FuncOp &func,\n+              std::vector<mlir::Value> &args) -> mlir::OpState {\n+             return self.create<mlir::triton::CallOp>(func, args);\n+           })\n       // Unstructured control flow\n       .def(\"create_cond_branch\",\n-           [](mlir::OpBuilder &self, mlir::Value condition,\n-              mlir::Block *trueDest, mlir::Block *falseDest) {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::cf::CondBranchOp>(loc, condition, trueDest,\n-                                                 falseDest);\n-             return;\n+           [](TritonOpBuilder &self, mlir::Value condition,\n+              mlir::Block *trueDest, mlir::Block *falseDest) -> mlir::OpState {\n+             return self.create<mlir::cf::CondBranchOp>(condition, trueDest,\n+                                                        falseDest);\n            })\n       .def(\"create_branch\",\n-           [](mlir::OpBuilder &self, mlir::Block *dest,\n-              std::vector<mlir::Value> &args) {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::cf::BranchOp>(loc, dest, args);\n-             return;\n+           [](TritonOpBuilder &self, mlir::Block *dest,\n+              std::vector<mlir::Value> &args) -> mlir::OpState {\n+             return self.create<mlir::cf::BranchOp>(dest, args);\n            })\n       // Structured control flow\n       .def(\"create_for_op\",\n-           [](mlir::OpBuilder &self, mlir::Value &lb, mlir::Value &ub,\n+           [](TritonOpBuilder &self, mlir::Value &lb, mlir::Value &ub,\n               mlir::Value &step,\n               std::vector<mlir::Value> &initArgs) -> mlir::scf::ForOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::ForOp>(loc, lb, ub, step, initArgs);\n+             return self.create<mlir::scf::ForOp>(lb, ub, step, initArgs);\n            })\n       .def(\"create_if_op\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Type> &retTypes,\n+           [](TritonOpBuilder &self, std::vector<mlir::Type> &retTypes,\n               mlir::Value &condition, bool withElse) -> mlir::scf::IfOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::IfOp>(loc, retTypes, condition,\n-                                                 withElse);\n+             return self.create<mlir::scf::IfOp>(retTypes, condition, withElse);\n            })\n       .def(\"create_yield_op\",\n-           [](mlir::OpBuilder &self,\n+           [](TritonOpBuilder &self,\n               std::vector<mlir::Value> &yields) -> mlir::scf::YieldOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::YieldOp>(loc, yields);\n+             return self.create<mlir::scf::YieldOp>(yields);\n            })\n       .def(\"create_while_op\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Type> &retTypes,\n+           [](TritonOpBuilder &self, std::vector<mlir::Type> &retTypes,\n               std::vector<mlir::Value> &initArgs) -> mlir::scf::WhileOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::WhileOp>(loc, retTypes, initArgs);\n+             return self.create<mlir::scf::WhileOp>(retTypes, initArgs);\n            })\n       .def(\"create_condition_op\",\n-           [](mlir::OpBuilder &self, mlir::Value &cond,\n+           [](TritonOpBuilder &self, mlir::Value &cond,\n               std::vector<mlir::Value> &args) -> mlir::scf::ConditionOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::ConditionOp>(loc, cond, args);\n+             return self.create<mlir::scf::ConditionOp>(cond, args);\n            })\n \n       // miscellaneous\n       .def(\"create_make_range\",\n-           [](mlir::OpBuilder &self, int start, int end) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             auto retType =\n-                 mlir::RankedTensorType::get({end - start}, self.getI32Type());\n-             return self.create<mlir::triton::MakeRangeOp>(loc, retType, start,\n-                                                           end);\n+           [](TritonOpBuilder &self, int start, int end) -> mlir::Value {\n+             auto retType = mlir::RankedTensorType::get(\n+                 {end - start}, self.getBuilder().getI32Type());\n+             return self.create<mlir::triton::MakeRangeOp>(retType, start, end);\n            })\n \n       // Cast instructions\n       // Conversions for custom FP types (FP8)\n       .def(\"create_fp_to_fp\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::FpToFpOp>(loc, dstType, src);\n+             return self.create<mlir::triton::FpToFpOp>(dstType, src);\n            })\n       // Conversions for standard LLVM builtin types\n       .def(\"create_bitcast\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::BitcastOp>(loc, dstType, src);\n+             return self.create<mlir::triton::BitcastOp>(dstType, src);\n            })\n       .def(\"create_si_to_fp\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::SIToFPOp>(loc, dstType, src);\n+             return self.create<mlir::arith::SIToFPOp>(dstType, src);\n            })\n       .def(\"create_ui_to_fp\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::UIToFPOp>(loc, dstType, src);\n+             return self.create<mlir::arith::UIToFPOp>(dstType, src);\n            })\n       .def(\"create_fp_to_si\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::FPToSIOp>(loc, dstType, src);\n+             return self.create<mlir::arith::FPToSIOp>(dstType, src);\n            })\n       .def(\"create_fp_to_ui\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::FPToUIOp>(loc, dstType, src);\n+             return self.create<mlir::arith::FPToUIOp>(dstType, src);\n            })\n       .def(\"create_fp_ext\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::ExtFOp>(loc, dstType, src);\n+             return self.create<mlir::arith::ExtFOp>(dstType, src);\n            })\n       .def(\"create_fp_trunc\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::TruncFOp>(loc, dstType, src);\n+             return self.create<mlir::arith::TruncFOp>(dstType, src);\n            })\n       .def(\"create_int_cast\",\n-           [](mlir::OpBuilder &self, mlir::Value &src, mlir::Type &dstType,\n+           [](TritonOpBuilder &self, mlir::Value &src, mlir::Type &dstType,\n               bool isSigned) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              // get element type if necessary\n              mlir::Type srcType = src.getType();\n              auto srcTensorType = srcType.dyn_cast<mlir::RankedTensorType>();\n@@ -855,387 +927,330 @@ void init_triton_ir(py::module &&m) {\n              unsigned srcWidth = srcEltType.getIntOrFloatBitWidth();\n              unsigned dstWidth = dstEltType.getIntOrFloatBitWidth();\n              if (srcWidth == dstWidth)\n-               return self.create<mlir::arith::BitcastOp>(loc, dstType, src);\n+               return self.create<mlir::arith::BitcastOp>(dstType, src);\n              else if (srcWidth > dstWidth)\n-               return self.create<mlir::arith::TruncIOp>(loc, dstType, src);\n+               return self.create<mlir::arith::TruncIOp>(dstType, src);\n              else if (isSigned)\n-               return self.create<mlir::arith::ExtSIOp>(loc, dstType, src);\n+               return self.create<mlir::arith::ExtSIOp>(dstType, src);\n              else\n-               return self.create<mlir::arith::ExtUIOp>(loc, dstType, src);\n+               return self.create<mlir::arith::ExtUIOp>(dstType, src);\n            })\n       .def(\"create_to_index\",\n-           [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Value &input) -> mlir::Value {\n              return self.create<mlir::arith::IndexCastOp>(\n-                 loc, self.getIndexType(), input);\n+                 self.getBuilder().getIndexType(), input);\n            })\n       .def(\"create_index_to_si\",\n-           [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Value &input) -> mlir::Value {\n              return self.create<mlir::arith::IndexCastOp>(\n-                 loc, self.getI64Type(), input);\n+                 self.getBuilder().getI64Type(), input);\n            })\n       .def(\"create_fmul\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::MulFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::MulFOp>(lhs, rhs);\n            })\n       .def(\"create_fdiv\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::DivFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::DivFOp>(lhs, rhs);\n            })\n       .def(\"create_frem\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::RemFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::RemFOp>(lhs, rhs);\n            })\n       .def(\"create_fadd\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::AddFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::AddFOp>(lhs, rhs);\n            })\n       .def(\"create_fsub\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::SubFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::SubFOp>(lhs, rhs);\n            })\n       .def(\"create_mul\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::MulIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::MulIOp>(lhs, rhs);\n            })\n       .def(\"create_sdiv\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::DivSIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::DivSIOp>(lhs, rhs);\n            })\n       .def(\"create_udiv\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::DivUIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::DivUIOp>(lhs, rhs);\n            })\n       .def(\"create_srem\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::RemSIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::RemSIOp>(lhs, rhs);\n            })\n       .def(\"create_urem\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::RemUIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::RemUIOp>(lhs, rhs);\n            })\n       .def(\"create_add\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::AddIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::AddIOp>(lhs, rhs);\n            })\n       .def(\"create_sub\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::SubIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::SubIOp>(lhs, rhs));\n            })\n       .def(\"create_shl\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::ShLIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::ShLIOp>(lhs, rhs));\n            })\n       .def(\"create_lshr\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::ShRUIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::ShRUIOp>(lhs, rhs));\n            })\n       .def(\"create_ashr\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::ShRSIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::ShRSIOp>(lhs, rhs));\n            })\n       // AddPtr (similar to GEP)\n       .def(\"create_addptr\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+           [](TritonOpBuilder &self, mlir::Value &ptr,\n               mlir::Value &offset) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::AddPtrOp>(loc, ptr.getType(), ptr,\n+             return self.create<mlir::triton::AddPtrOp>(ptr.getType(), ptr,\n                                                         offset);\n            })\n       // Comparison (int)\n       .def(\"create_icmpSLE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::sle, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::sle, lhs, rhs);\n            })\n       .def(\"create_icmpSLT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::slt, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::slt, lhs, rhs);\n            })\n       .def(\"create_icmpSGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::sge, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::sge, lhs, rhs);\n            })\n       .def(\"create_icmpSGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::sgt, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::sgt, lhs, rhs);\n            })\n       .def(\"create_icmpULE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ule, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ule, lhs, rhs);\n            })\n       .def(\"create_icmpULT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ult, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ult, lhs, rhs);\n            })\n       .def(\"create_icmpUGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::uge, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::uge, lhs, rhs);\n            })\n       .def(\"create_icmpUGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ugt, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ugt, lhs, rhs);\n            })\n       .def(\"create_icmpEQ\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::eq, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::eq, lhs, rhs);\n            })\n       .def(\"create_icmpNE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ne, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ne, lhs, rhs);\n            })\n       // Comparison (float)\n       .def(\"create_fcmpOLT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OLT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OLT, lhs, rhs);\n            })\n       .def(\"create_fcmpOGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OGT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OGT, lhs, rhs);\n            })\n       .def(\"create_fcmpOLE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OLE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OLE, lhs, rhs);\n            })\n       .def(\"create_fcmpOGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OGE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OGE, lhs, rhs);\n            })\n       .def(\"create_fcmpOEQ\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OEQ, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OEQ, lhs, rhs);\n            })\n       .def(\"create_fcmpONE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::ONE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::ONE, lhs, rhs);\n            })\n       .def(\"create_fcmpULT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::ULT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::ULT, lhs, rhs);\n            })\n       .def(\"create_fcmpUGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UGT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UGT, lhs, rhs);\n            })\n       .def(\"create_fcmpULE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::ULE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::ULE, lhs, rhs);\n            })\n       .def(\"create_fcmpUGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UGE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UGE, lhs, rhs);\n            })\n       .def(\"create_fcmpUEQ\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UEQ, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UEQ, lhs, rhs);\n            })\n       .def(\"create_fcmpUNE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UNE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UNE, lhs, rhs);\n            })\n       // // Logical\n       .def(\"create_and\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::AndIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::AndIOp>(lhs, rhs);\n            })\n       .def(\"create_xor\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::XOrIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::XOrIOp>(lhs, rhs);\n            })\n       .def(\"create_or\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::OrIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::OrIOp>(lhs, rhs);\n            })\n       // Input/Output\n       .def(\"create_load\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy,\n               bool isVolatile) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::LoadOp>(\n-                 loc, ptrs, cacheModifier, evictionPolicy, isVolatile);\n+                 ptrs, cacheModifier, evictionPolicy, isVolatile);\n            })\n       .def(\"create_store\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &value,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs, mlir::Value &value,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptrs, value, cacheModifier,\n+             self.create<mlir::triton::StoreOp>(ptrs, value, cacheModifier,\n                                                 evictionPolicy);\n            })\n       .def(\"create_tensor_pointer_load\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+           [](TritonOpBuilder &self, mlir::Value &ptr,\n               std::vector<int32_t> &boundaryCheck,\n               std::optional<mlir::triton::PaddingOption> paddingOption,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy,\n               bool isVolatile) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::LoadOp>(\n-                 loc, ptr, boundaryCheck, paddingOption, cacheModifier,\n+                 ptr, boundaryCheck, paddingOption, cacheModifier,\n                  evictionPolicy, isVolatile);\n            })\n       .def(\"create_tensor_pointer_store\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &ptr, mlir::Value &val,\n               std::vector<int32_t> &boundaryCheck,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptr, val, boundaryCheck,\n+             self.create<mlir::triton::StoreOp>(ptr, val, boundaryCheck,\n                                                 cacheModifier, evictionPolicy);\n            })\n       .def(\"create_masked_load\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &mask,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs, mlir::Value &mask,\n               std::optional<mlir::Value> &other,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy,\n               bool isVolatile) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::LoadOp>(\n-                 loc, ptrs, mask, other.value_or(mlir::Value()), cacheModifier,\n+                 ptrs, mask, other.value_or(mlir::Value()), cacheModifier,\n                  evictionPolicy, isVolatile);\n            })\n       .def(\"create_masked_store\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs, mlir::Value &val,\n               mlir::Value &mask, mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptrs, val, mask,\n-                                                cacheModifier, evictionPolicy);\n+             self.create<mlir::triton::StoreOp>(ptrs, val, mask, cacheModifier,\n+                                                evictionPolicy);\n            })\n       .def(\"create_view\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg,\n+           [](TritonOpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              auto argType = arg.getType()\n                                 .dyn_cast<mlir::RankedTensorType>()\n                                 .getElementType();\n              return self.create<mlir::triton::ViewOp>(\n-                 loc, mlir::RankedTensorType::get(shape, argType), arg);\n+                 mlir::RankedTensorType::get(shape, argType), arg);\n            })\n       .def(\n           \"create_expand_dims\",\n-          [](mlir::OpBuilder &self, mlir::Value &arg, int axis) -> mlir::Value {\n-            auto loc = self.getUnknownLoc();\n+          [](TritonOpBuilder &self, mlir::Value &arg, int axis) -> mlir::Value {\n             auto argType = arg.getType().dyn_cast<mlir::RankedTensorType>();\n             auto argEltType = argType.getElementType();\n             std::vector<int64_t> retShape = argType.getShape();\n             retShape.insert(retShape.begin() + axis, 1);\n             return self.create<mlir::triton::ExpandDimsOp>(\n-                loc, mlir::RankedTensorType::get(retShape, argEltType), arg,\n-                axis);\n+                mlir::RankedTensorType::get(retShape, argEltType), arg, axis);\n           })\n       .def(\"create_cat\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              auto lhsType = lhs.getType().dyn_cast<mlir::RankedTensorType>();\n              auto rhsType = rhs.getType().dyn_cast<mlir::RankedTensorType>();\n              if (!(lhsType.getShape().size() == 1 &&\n@@ -1245,47 +1260,41 @@ void init_triton_ir(py::module &&m) {\n              std::vector<int64_t> shape{lhsType.getShape()[0] +\n                                         rhsType.getShape()[0]};\n              return self.create<mlir::triton::CatOp>(\n-                 loc,\n                  mlir::RankedTensorType::get(shape, lhsType.getElementType()),\n                  lhs, rhs);\n            })\n       .def(\"create_trans\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Value &arg) -> mlir::Value {\n              auto argType = arg.getType().dyn_cast<mlir::RankedTensorType>();\n              auto argEltType = argType.getElementType();\n              std::vector<int64_t> retShape = argType.getShape();\n              std::reverse(retShape.begin(), retShape.end());\n              return self.create<mlir::triton::TransOp>(\n-                 loc, mlir::RankedTensorType::get(retShape, argEltType), arg);\n+                 mlir::RankedTensorType::get(retShape, argEltType), arg);\n            })\n       .def(\"create_broadcast\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg,\n+           [](TritonOpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              if (auto argType =\n                      arg.getType().dyn_cast<mlir::RankedTensorType>())\n                return self.createOrFold<mlir::triton::BroadcastOp>(\n-                   loc,\n                    mlir::RankedTensorType::get(shape, argType.getElementType()),\n                    arg);\n              throw std::runtime_error(\n                  \"arg is not of RankedTensorType, use create_splat\");\n            })\n       .def(\"create_splat\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg,\n+           [](TritonOpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              auto argType = arg.getType();\n              auto ret = self.createOrFold<mlir::triton::SplatOp>(\n-                 loc, mlir::RankedTensorType::get(shape, argType), arg);\n+                 mlir::RankedTensorType::get(shape, argType), arg);\n              return ret;\n            })\n       // // atomic\n       .def(\"create_atomic_cas\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n+           [](TritonOpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n               mlir::Value &val, mlir::triton::MemSemantic sem) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n              if (auto srcTensorType =\n                      ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n@@ -1299,14 +1308,13 @@ void init_triton_ir(py::module &&m) {\n                                   .cast<mlir::triton::PointerType>();\n                dstType = ptrType.getPointeeType();\n              }\n-             return self.create<mlir::triton::AtomicCASOp>(loc, dstType, ptr,\n-                                                           cmp, val, sem);\n+             return self.create<mlir::triton::AtomicCASOp>(dstType, ptr, cmp,\n+                                                           val, sem);\n            })\n       .def(\"create_atomic_rmw\",\n-           [](mlir::OpBuilder &self, mlir::triton::RMWOp rmwOp,\n+           [](TritonOpBuilder &self, mlir::triton::RMWOp rmwOp,\n               mlir::Value &ptr, mlir::Value &val, mlir::Value &mask,\n               mlir::triton::MemSemantic sem) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n              if (auto srcTensorType =\n                      ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n@@ -1320,189 +1328,163 @@ void init_triton_ir(py::module &&m) {\n                                   .cast<mlir::triton::PointerType>();\n                dstType = ptrType.getPointeeType();\n              }\n-             return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n-                                                           ptr, val, mask, sem);\n+             return self.create<mlir::triton::AtomicRMWOp>(dstType, rmwOp, ptr,\n+                                                           val, mask, sem);\n            })\n       // External\n       .def(\"create_extern_elementwise\",\n-           [](mlir::OpBuilder &self, const std::string &libName,\n+           [](TritonOpBuilder &self, const std::string &libName,\n               const std::string &libPath, const std::string &symbol,\n               std::vector<mlir::Value> &argList, mlir::Type retType,\n               bool isPure) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              if (isPure)\n                return self.create<mlir::triton::PureExternElementwiseOp>(\n-                   loc, retType, argList, libName, libPath, symbol);\n+                   retType, argList, libName, libPath, symbol);\n              else\n                return self.create<mlir::triton::ImpureExternElementwiseOp>(\n-                   loc, retType, argList, libName, libPath, symbol);\n+                   retType, argList, libName, libPath, symbol);\n            })\n       // Built-in instruction\n       .def(\"create_get_program_id\",\n-           [](mlir::OpBuilder &self, int axis) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int axis) -> mlir::Value {\n              if (axis < 0 || axis > 3)\n                throw std::runtime_error(\"program_id must be in [0,3]\");\n              return self.create<mlir::triton::GetProgramIdOp>(\n-                 loc, self.getI32Type(),\n+                 self.getBuilder().getI32Type(),\n                  mlir::triton::ProgramIDDimAttr::get(\n-                     loc.getContext(), mlir::triton::ProgramIDDim(axis)));\n+                     self.getBuilder().getContext(),\n+                     mlir::triton::ProgramIDDim(axis)));\n            })\n       .def(\"create_get_num_programs\",\n-           [](mlir::OpBuilder &self, int axis) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int axis) -> mlir::Value {\n              return self.create<mlir::triton::GetNumProgramsOp>(\n-                 loc, self.getI32Type(), self.getI32IntegerAttr(axis));\n+                 self.getBuilder().getI32Type(),\n+                 self.getBuilder().getI32IntegerAttr(axis));\n            })\n       .def(\"create_dot\",\n-           [](mlir::OpBuilder &self, mlir::Value &a, mlir::Value &b,\n+           [](TritonOpBuilder &self, mlir::Value &a, mlir::Value &b,\n               mlir::Value &c, bool allowTF32) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::DotOp>(loc, c.getType(), a, b, c,\n+             return self.create<mlir::triton::DotOp>(c.getType(), a, b, c,\n                                                      allowTF32);\n            })\n       .def(\"create_exp\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::ExpOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::ExpOp>(val);\n            })\n       .def(\"create_cos\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::CosOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::CosOp>(val);\n            })\n       .def(\"create_sin\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::SinOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::SinOp>(val);\n            })\n       .def(\"create_log\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::LogOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::LogOp>(val);\n            })\n       .def(\"create_sqrt\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::SqrtOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::SqrtOp>(val);\n            })\n       .def(\"create_fabs\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::AbsFOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::AbsFOp>(val);\n            })\n       .def(\"create_iabs\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::AbsIOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::AbsIOp>(val);\n            })\n       .def(\"create_reduce\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Value> operands,\n+           [](TritonOpBuilder &self, std::vector<mlir::Value> operands,\n               int axis) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::ReduceOp>(loc, operands, axis);\n+             return self.create<mlir::triton::ReduceOp>(operands, axis);\n            })\n       .def(\"create_reduce_ret\",\n-           [](mlir::OpBuilder &self, py::args args) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, py::args args) -> mlir::OpState {\n              llvm::SmallVector<mlir::Value> return_values;\n              for (const auto &arg : args) {\n                return_values.push_back(py::cast<mlir::Value>(arg));\n              }\n-             return self.create<mlir::triton::ReduceReturnOp>(loc,\n-                                                              return_values);\n+             return self.create<mlir::triton::ReduceReturnOp>(return_values);\n            })\n       .def(\"create_scan\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Value> operands,\n+           [](TritonOpBuilder &self, std::vector<mlir::Value> operands,\n               int axis) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::ScanOp>(loc, operands, axis);\n+             return self.create<mlir::triton::ScanOp>(operands, axis);\n            })\n       .def(\"create_scan_ret\",\n-           [](mlir::OpBuilder &self, py::args args) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, py::args args) -> mlir::OpState {\n              llvm::SmallVector<mlir::Value> return_values;\n              for (const auto &arg : args) {\n                return_values.push_back(py::cast<mlir::Value>(arg));\n              }\n-             return self.create<mlir::triton::ScanReturnOp>(loc, return_values);\n+             return self.create<mlir::triton::ScanReturnOp>(return_values);\n            })\n       .def(\"create_ptr_to_int\",\n-           [](mlir::OpBuilder &self, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &val,\n               mlir::Type &type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::PtrToIntOp>(loc, type, val);\n+             return self.create<mlir::triton::PtrToIntOp>(type, val);\n            })\n       .def(\"create_int_to_ptr\",\n-           [](mlir::OpBuilder &self, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &val,\n               mlir::Type &type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::IntToPtrOp>(loc, type, val);\n+             return self.create<mlir::triton::IntToPtrOp>(type, val);\n            })\n       .def(\"create_select\",\n-           [](mlir::OpBuilder &self, mlir::Value &condition,\n+           [](TritonOpBuilder &self, mlir::Value &condition,\n               mlir::Value &trueValue, mlir::Value &falseValue) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::SelectOp>(loc, condition,\n-                                                       trueValue, falseValue);\n+             return self.create<mlir::arith::SelectOp>(condition, trueValue,\n+                                                       falseValue);\n            })\n       .def(\"create_print\",\n-           [](mlir::OpBuilder &self, const std::string &prefix,\n+           [](TritonOpBuilder &self, const std::string &prefix,\n               const std::vector<mlir::Value> &values) -> void {\n-             auto loc = self.getUnknownLoc();\n              self.create<mlir::triton::PrintOp>(\n-                 loc,\n-                 mlir::StringAttr::get(self.getContext(),\n+                 mlir::StringAttr::get(self.getBuilder().getContext(),\n                                        llvm::StringRef(prefix)),\n                  values);\n            })\n       .def(\"create_assert\",\n-           [](mlir::OpBuilder &self, mlir::Value &condition,\n+           [](TritonOpBuilder &self, mlir::Value &condition,\n               const std::string &message, const std::string &fileName,\n               const std::string &funcName, unsigned lineNo) -> void {\n-             auto loc = self.getUnknownLoc();\n-             auto messageAttr = mlir::StringAttr::get(self.getContext(),\n-                                                      llvm::StringRef(message));\n+             auto messageAttr = mlir::StringAttr::get(\n+                 self.getBuilder().getContext(), llvm::StringRef(message));\n              auto fileNameAttr = mlir::StringAttr::get(\n-                 self.getContext(), llvm::StringRef(fileName));\n+                 self.getBuilder().getContext(), llvm::StringRef(fileName));\n              auto funcNameAttr = mlir::StringAttr::get(\n-                 self.getContext(), llvm::StringRef(funcName));\n-             auto lineNoAttr = self.getI32IntegerAttr(lineNo);\n-             self.create<mlir::triton::AssertOp>(loc, condition, messageAttr,\n+                 self.getBuilder().getContext(), llvm::StringRef(funcName));\n+             auto lineNoAttr = self.getBuilder().getI32IntegerAttr(lineNo);\n+             self.create<mlir::triton::AssertOp>(condition, messageAttr,\n                                                  fileNameAttr, funcNameAttr,\n                                                  lineNoAttr);\n            })\n       // Undef\n       .def(\"create_undef\",\n-           [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<::mlir::LLVM::UndefOp>(loc, type);\n+           [](TritonOpBuilder &self, mlir::Type &type) -> mlir::Value {\n+             return self.create<::mlir::LLVM::UndefOp>(type);\n            })\n       // Force GPU barrier\n       .def(\"create_barrier\",\n-           [](mlir::OpBuilder &self) {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::gpu::BarrierOp>(loc);\n-           })\n+           [](TritonOpBuilder &self) { self.create<mlir::gpu::BarrierOp>(); })\n       // Make a block pointer (tensor pointer in Triton IR)\n       .def(\"create_make_block_ptr\",\n-           [](mlir::OpBuilder &self, mlir::Value &base,\n+           [](TritonOpBuilder &self, mlir::Value &base,\n               std::vector<mlir::Value> &shape,\n               std::vector<mlir::Value> &strides,\n               std::vector<mlir::Value> &offsets,\n               std::vector<int32_t> &tensorShape,\n               std::vector<int32_t> &order) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::MakeTensorPtrOp>(\n-                 loc, base, shape, strides, offsets, tensorShape, order);\n+                 base, shape, strides, offsets, tensorShape, order);\n            })\n       // Advance a block pointer\n       .def(\"create_advance\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+           [](TritonOpBuilder &self, mlir::Value &ptr,\n               std::vector<mlir::Value> &offsets) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::AdvanceOp>(loc, ptr.getType(),\n-                                                         ptr, offsets);\n+             return self.create<mlir::triton::AdvanceOp>(ptr.getType(), ptr,\n+                                                         offsets);\n            });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")\n@@ -1511,6 +1493,7 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              auto printingFlags = mlir::OpPrintingFlags();\n              printingFlags.elideLargeElementsAttrs(16);\n+             printingFlags.enableDebugInfo();\n              self.enableIRPrinting(\n                  /*shouldPrintBeforePass=*/nullptr,\n                  /*shouldPrintAfterPass=*/\n@@ -1650,71 +1633,76 @@ void init_triton_translation(py::module &m) {\n               \"failed to parse IR: \" + error.getMessage() +\n               \"lineno: \" + std::to_string(error.getLineNo()));\n         }\n-\n         // translate module to PTX\n         auto ptxCode =\n             triton::translateLLVMIRToPTX(*module, capability, version);\n         return ptxCode;\n       },\n       ret::take_ownership);\n \n-  m.def(\"compile_ptx_to_cubin\",\n-        [](const std::string &ptxCode, const std::string &ptxasPath,\n-           int capability) -> py::object {\n-          std::string cubin;\n-          {\n-            py::gil_scoped_release allow_threads;\n-\n-            // compile ptx with ptxas\n-            llvm::SmallString<64> fsrc;\n-            llvm::SmallString<64> flog;\n-            llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n-            llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n-            std::string fbin = std::string(fsrc) + \".o\";\n-            llvm::FileRemover logRemover(flog);\n-            llvm::FileRemover binRemover(fbin);\n-            const char *_fsrc = fsrc.c_str();\n-            const char *_flog = flog.c_str();\n-            const char *_fbin = fbin.c_str();\n-            std::ofstream ofs(_fsrc);\n-            ofs << ptxCode << std::endl;\n-            ofs.close();\n-            std::string cmd;\n-            int err;\n-            cmd = ptxasPath + \" -v --gpu-name=sm_\" +\n-                  std::to_string(capability) + (capability == 90 ? \"a \" : \" \") +\n-                  _fsrc + \" -o \" + _fsrc + \".o 2> \" + _flog;\n-\n-            err = system(cmd.c_str());\n-            if (err != 0) {\n-              err >>= 8;\n-              std::ifstream _log(_flog);\n-              std::string log(std::istreambuf_iterator<char>(_log), {});\n-              if (err == 255) {\n-                throw std::runtime_error(\n-                    \"Internal Triton PTX codegen error: \\n\" + log);\n-              } else if (err == 128 + SIGSEGV) {\n-                throw std::runtime_error(\"Please run `ptxas \" +\n-                                         fsrc.str().str() +\n-                                         \"` to confirm that this is a \"\n-                                         \"bug in `ptxas`\\n\" +\n-                                         log);\n-              } else {\n-                throw std::runtime_error(\"`ptxas` failed with error code \" +\n-                                         std::to_string(err) + \": \\n\" + log);\n-              }\n-              return {};\n+  m.def(\n+      \"compile_ptx_to_cubin\",\n+      [](const std::string &ptxCode, const std::string &ptxasPath,\n+         int capability) -> py::object {\n+        std::string cubin;\n+        {\n+          py::gil_scoped_release allow_threads;\n+\n+          // compile ptx with ptxas\n+          llvm::SmallString<64> fsrc;\n+          llvm::SmallString<64> flog;\n+          llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n+          llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n+          std::string fbin = std::string(fsrc) + \".o\";\n+          llvm::FileRemover logRemover(flog);\n+          llvm::FileRemover binRemover(fbin);\n+          const char *_fsrc = fsrc.c_str();\n+          const char *_flog = flog.c_str();\n+          const char *_fbin = fbin.c_str();\n+          std::ofstream ofs(_fsrc);\n+          ofs << ptxCode << std::endl;\n+          ofs.close();\n+\n+          auto lineInfoOption =\n+              triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\")\n+                  ? \"\"\n+                  : \" -lineinfo\";\n+          auto capabilitySuffix = (capability == 90) ? \"a \" : \" \";\n+          auto outputFileName = std::string(_fsrc) + \".o\";\n+          auto logRedirect = \" 2> \" + std::string(_flog);\n+          std::string cmd = ptxasPath + lineInfoOption + \" -v --gpu-name=sm_\" +\n+                            std::to_string(capability) + capabilitySuffix +\n+                            _fsrc + \" -o \" + outputFileName + logRedirect;\n+\n+          int err = system(cmd.c_str());\n+          if (err != 0) {\n+            err >>= 8;\n+            std::ifstream _log(_flog);\n+            std::string log(std::istreambuf_iterator<char>(_log), {});\n+            if (err == 255) {\n+              throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n+                                       log);\n+            } else if (err == 128 + SIGSEGV) {\n+              throw std::runtime_error(\"Please run `ptxas \" + fsrc.str().str() +\n+                                       \"` to confirm that this is a \"\n+                                       \"bug in `ptxas`\\n\" +\n+                                       log);\n             } else {\n-              llvm::FileRemover srcRemover(fsrc);\n-              std::ifstream _cubin(_fbin, std::ios::binary);\n-              cubin = std::string(std::istreambuf_iterator<char>(_cubin), {});\n-              _cubin.close();\n-              // Do not return here, exit the gil scope and return below\n+              throw std::runtime_error(\"`ptxas` failed with error code \" +\n+                                       std::to_string(err) + \": \\n\" + log);\n             }\n+            return {};\n+          } else {\n+            llvm::FileRemover srcRemover(fsrc);\n+            std::ifstream _cubin(_fbin, std::ios::binary);\n+            cubin = std::string(std::istreambuf_iterator<char>(_cubin), {});\n+            _cubin.close();\n+            // Do not return here, exit the gil scope and return below\n           }\n-          py::bytes bytes(cubin);\n-          return std::move(bytes);\n-        });\n+        }\n+        py::bytes bytes(cubin);\n+        return std::move(bytes);\n+      });\n \n   m.def(\"add_external_libs\",\n         [](mlir::ModuleOp &op, const std::vector<std::string> &names,"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 30, "deletions": 28, "changes": 58, "file_content_changes": "@@ -1612,14 +1612,15 @@ def test_scan_layouts(M, N, src_layout, axis, device):\n \n \n layouts = [\n-    # BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n-    # BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    BlockedLayout([4, 4], [2, 16], [4, 1], [1, 0]),\n     MmaLayout(version=(2, 0), warps_per_cta=[4, 1]),\n-    # MmaLayout(version=(2, 0), warps_per_cta=[2, 2])\n+    MmaLayout(version=(2, 0), warps_per_cta=[2, 2])\n ]\n \n \n-@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n+@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128], [32, 32]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_reduce_layouts(M, N, src_layout, axis, device):\n@@ -1630,31 +1631,30 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n     #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}}>\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n-    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n+    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n         %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n         %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n         %2 = tt.splat %arg1 : (i32) -> tensor<{M}x1xi32, #blocked>\n         %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #blocked>\n-        %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #blocked>\n-        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<f32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+        %4 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n+        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n         %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n         %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n-        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<f32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<f32>, #blocked>\n+        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n         %9 = tt.broadcast %7 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n-        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<f32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n-        %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>\n-        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n-        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xf32, #blocked>\n-        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xf32, #blocked>) -> tensor<{M}x{N}xf32, #src>\n+        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+        %11 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>\n+        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n+        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #blocked>\n+        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n         %15 = \"tt.reduce\"(%14) ({{\n-        ^bb0(%arg3: f32, %arg4: f32):\n-          %16 = \"triton_gpu.cmpf\"(%arg3, %arg4) {{predicate = 2 : i64}} : (f32, f32) -> i1\n-          %17 = arith.select %16, %arg3, %arg4 : f32\n-          tt.reduce.return %17 : f32\n-        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xf32, #src>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n-        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n-        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xf32, #blocked>\n-        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xf32, #blocked>\n+        ^bb0(%arg3: i32, %arg4: i32):\n+          %17 = arith.addi %arg3, %arg4 : i32\n+          tt.reduce.return %17 : i32\n+        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n+        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n+        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xi32, #blocked>\n         tt.return\n     }}\n     }}\n@@ -1667,19 +1667,21 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n         kernel = triton.compile(f.name)\n \n     rs = RandomState(17)\n-    x = rs.randint(0, 4, (M, N)).astype('float32')\n-    x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+    x = rs.randint(0, 20, (M, N)).astype('int32')\n \n     if axis == 0:\n-        z = np.zeros((1, N)).astype('float32')\n+        z = np.zeros((1, N)).astype('int32')\n     else:\n-        z = np.zeros((M, 1)).astype('float32')\n+        z = np.zeros((M, 1)).astype('int32')\n+\n     x_tri = torch.tensor(x, device=device)\n     z_tri = torch.tensor(z, device=device)\n \n-    kernel[(1, 1, 4)](x_tri, x_tri.stride(0), z_tri)\n-    z_ref = np.max(x, axis=axis, keepdims=True)\n-    np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+    pgm = kernel[(1, 1, 4)](x_tri, x_tri.stride(0), z_tri)\n+\n+    z_ref = np.sum(x, axis=axis, keepdims=True)\n+\n+    np.testing.assert_equal(z_ref, z_tri.cpu().numpy())\n \n \n layouts = ["}, {"filename": "python/test/unit/language/test_line_info.py", "status": "added", "additions": 120, "deletions": 0, "changes": 120, "file_content_changes": "@@ -0,0 +1,120 @@\n+import subprocess\n+import tempfile\n+\n+import pytest\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def kernel_single(X,\n+                  Y,\n+                  BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def device_inline(x):\n+    return x + x\n+\n+\n+@triton.jit\n+def kernel_call(X,\n+                Y,\n+                BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = device_inline(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+\n+@triton.jit(noinline=True)\n+def device_noinline(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = x + x\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+\n+@triton.jit\n+def kernel_call_noinline(X, Y, BLOCK: tl.constexpr):\n+    device_noinline(X, Y, BLOCK)\n+\n+\n+@triton.jit\n+def kernel_multi_files(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = tl.softmax(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+\n+def extract_file_lines(asm):\n+    fd, path = tempfile.mkstemp()\n+    with open(fd, 'wb') as cubin:\n+        cubin.write(asm)\n+    asm = subprocess.check_output([\"nvdisasm\", \"-g\", path]).decode(\"utf-8\")\n+    file_lines = []\n+    lines = asm.splitlines()\n+    for line in lines:\n+        if \"## File\" in line:\n+            entries = line[line.index(\"## File\"):].split(\",\")\n+            file_lines.append((entries[0].strip(), entries[1].strip()))\n+    return file_lines\n+\n+\n+def check_file_lines(file_lines, file_name, lineno):\n+    for file, line in file_lines:\n+        # -1 means do not check line number\n+        if lineno == -1:\n+            if file_name in file:\n+                return True\n+        if file_name in file and str(lineno) in line:\n+            return True\n+    return False\n+\n+\n+func_types = [\"single\", \"call\", \"call_noinline\", \"multi_files\"]\n+\n+\n+@pytest.mark.parametrize(\"func\", func_types)\n+def test_line_info(func: str):\n+    try:\n+        subprocess.check_output([\"nvdisasm\", \"-h\"])\n+    except BaseException:\n+        pytest.skip(\"nvdisasm is not available\")\n+\n+    shape = (128, )\n+    x = torch.arange(0, shape[0], dtype=torch.float32, device='cuda')\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    kernel_info = {}\n+    if func == \"single\":\n+        kernel_info = kernel_single[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"call\":\n+        kernel_info = kernel_call[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"call_noinline\":\n+        kernel_info = kernel_call_noinline[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"multi_files\":\n+        kernel_info = kernel_multi_files[(1,)](x, y, BLOCK=shape[0])\n+\n+    file_lines = extract_file_lines(kernel_info.asm[\"cubin\"])\n+    if func == \"single\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 15))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 16))\n+    elif func == \"call\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 28))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 21))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 30))\n+    elif func == \"call_noinline\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 42))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 35))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 36))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 37))\n+    elif func == \"multi_files\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 47))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 49))\n+        assert (check_file_lines(file_lines, \"standard.py\", 33))\n+        assert (check_file_lines(file_lines, \"standard.py\", 34))\n+        assert (check_file_lines(file_lines, \"standard.py\", 36))\n+        # core.py is changed frequently, so we only check if it exists\n+        assert (check_file_lines(file_lines, \"core.py\", -1))"}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -9,7 +9,7 @@\n assert_path = os.path.join(dir_path, \"assert_helper.py\")\n \n # TODO: bfloat16 after LLVM-15\n-func_types = [\"device_assert\", \"assert\", \"static_assert\", \"no_debug\"]\n+assert_types = [\"device_assert\", \"assert\", \"static_assert\", \"no_debug\"]\n nested_types = [(caller, callee) for caller in [\"true\", \"false\", \"none\"] for callee in [\"true\", \"false\", \"none\"]]\n torch_types = [\"int8\", \"uint8\", \"int16\", \"int32\", \"long\", \"float16\", \"float32\", \"float64\"]\n \n@@ -37,7 +37,7 @@ def test_print(func_type: str, data_type: str):\n         assert len(new_lines) == 1\n \n \n-@pytest.mark.parametrize(\"func_type\", func_types)\n+@pytest.mark.parametrize(\"func_type\", assert_types)\n def test_assert(func_type: str):\n     os.environ[\"TRITON_DEBUG\"] = \"1\"\n     proc = subprocess.Popen([sys.executable, assert_path, func_type], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -29,13 +29,12 @@ def test_op(M, N, dtype, mode):\n     # backward pass\n     elif mode == 'backward':\n         dy = torch.randn_like(tt_y)\n-        # run torch first to make sure cuda context is initialized\n-        # torch backward\n-        th_y.backward(dy)\n-        th_dx = x.grad.clone()\n         # triton backward\n-        x.grad.zero_()\n         tt_y.backward(dy)\n         tt_dx = x.grad.clone()\n+        # torch backward\n+        x.grad = None\n+        th_y.backward(dy)\n+        th_dx = x.grad.clone()\n \n         torch.testing.assert_allclose(th_dx, tt_dx)"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 53, "deletions": 11, "changes": 64, "file_content_changes": "@@ -75,6 +75,20 @@ def _check_fn_args(node, fn, args):\n                 raise UnsupportedLanguageConstruct(fn.src, node, f'Function {fn.__name__} is marked noinline, but was called with non-scalar argument {fn.arg_names[idx]}:{arg}')\n \n \n+def _get_fn_file_line(fn):\n+    base_fn = fn\n+    while not isinstance(base_fn, JITFunction):\n+        base_fn = base_fn.fn\n+    file_name = base_fn.fn.__code__.co_filename\n+    lines, begin_line = inspect.getsourcelines(base_fn.fn)\n+    for line in lines:\n+        if line.strip().startswith('@'):\n+            begin_line += 1\n+        else:\n+            break\n+    return file_name, begin_line\n+\n+\n _condition_types = {bool, int, type(None)}  # Python types accepted for conditionals inside kernels\n \n \n@@ -191,8 +205,13 @@ def visit_Call(self, node: ast.Call) -> bool:\n class CodeGenerator(ast.NodeVisitor):\n     def __init__(self, context, prototype, gscope, attributes, constants, function_name, arch,\n                  module=None, is_kernel=False, function_types: Optional[Dict] = None,\n-                 debug=False, noinline=False):\n+                 debug=False, noinline=False, file_name: Optional[str] = None, begin_line=0):\n+        self.context = context\n         self.builder = ir.builder(context)\n+        self.file_name = file_name\n+        # node.lineno starts from 1, so we need to subtract 1\n+        self.begin_line = begin_line - 1\n+        self.builder.set_loc(file_name, begin_line, 0)\n         self.builder.arch = arch\n         self.module = self.builder.create_module() if module is None else module\n         self.function_ret_types = {} if function_types is None else function_types\n@@ -249,6 +268,18 @@ def set_value(self, name: str,\n         self.lscope[name] = value\n         self.local_defs[name] = value\n \n+    def _get_insertion_point_and_loc(self):\n+        # XXX: this is a hack to get the location of the insertion point.\n+        # The insertion point's location could be invalid sometimes,\n+        # so we need to explicitly set the location\n+        loc = self.builder.get_loc()\n+        ip = self.builder.get_insertion_point()\n+        return ip, loc\n+\n+    def _set_insertion_point_and_loc(self, ip, loc):\n+        self.builder.restore_insertion_point(ip)\n+        self.builder.set_loc(loc)\n+\n     #\n     # AST visitor\n     #\n@@ -534,13 +565,13 @@ def visit_if_top_level(self, cond, node):\n     def visit_if_scf(self, cond, node):\n         with enter_sub_region(self) as sr:\n             liveins, _ = sr\n-            ip = self.builder.get_insertion_point()\n+            ip, last_loc = self._get_insertion_point_and_loc()\n             then_block = self.builder.create_block()\n             else_block = self.builder.create_block() if node.orelse else None\n             then_defs, else_defs, then_block, else_block, names, ret_types, _ = \\\n                 self.visit_then_else_blocks(node, liveins, then_block, else_block)\n             # create if op\n-            self.builder.restore_insertion_point(ip)\n+            self._set_insertion_point_and_loc(ip, last_loc)\n             if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, True)\n             then_block.merge_block_before(if_op.get_then_block())\n             self.builder.set_insertion_point_to_end(if_op.get_then_block())\n@@ -761,7 +792,7 @@ def visit_For(self, node):\n \n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n-            ip = self.builder.get_insertion_point()\n+            ip, last_loc = self._get_insertion_point_and_loc()\n \n             # create loop body block\n             block = self.builder.create_block()\n@@ -791,7 +822,7 @@ def visit_For(self, node):\n                     yields.append(language.core._to_tensor(self.local_defs[name], self.builder))\n \n             # create ForOp\n-            self.builder.restore_insertion_point(ip)\n+            self._set_insertion_point_and_loc(ip, last_loc)\n             for_op = self.builder.create_for_op(lb, ub, step, [arg.handle for arg in init_args])\n \n             self.scf_stack.append(node)\n@@ -868,9 +899,10 @@ def call_JitFunction(self, fn: JITFunction, args, kwargs):\n             gscope = sys.modules[fn.fn.__module__].__dict__\n             # If the callee is not set, we use the same debug setting as the caller\n             debug = self.debug if fn.debug is None else fn.debug\n-            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module,\n+            file_name, begin_line = _get_fn_file_line(fn)\n+            generator = CodeGenerator(self.context, prototype, gscope, attributes, constants, module=self.module,\n                                       function_name=fn_name, function_types=self.function_ret_types, debug=debug, noinline=fn.noinline,\n-                                      arch=self.builder.arch)\n+                                      file_name=file_name, begin_line=begin_line, arch=self.builder.arch)\n             generator.visit(fn.parse())\n             callee_ret_type = generator.last_ret_type\n             self.function_ret_types[fn_name] = callee_ret_type\n@@ -968,14 +1000,23 @@ def visit_JoinedStr(self, node):\n         return ''.join(values)\n \n     def visit(self, node):\n-        if node is not None:\n-            self.last_node = node\n+        if node is None:\n+            return\n         with warnings.catch_warnings():\n             # The ast library added visit_Constant and deprecated some other\n             # methods but we can't move to that without breaking Python 3.6 and 3.7.\n             warnings.simplefilter(\"ignore\", DeprecationWarning)  # python 3.9\n             warnings.simplefilter(\"ignore\", PendingDeprecationWarning)  # python 3.8\n-            return super().visit(node)\n+            self.last_node = node\n+            last_loc = self.builder.get_loc()\n+            if hasattr(node, 'lineno') and hasattr(node, 'col_offset'):\n+                self.builder.set_loc(self.file_name, self.begin_line + node.lineno, node.col_offset)\n+                last_loc = self.builder.get_loc()\n+            ret = super().visit(node)\n+            # Reset the location to the last one before the visit\n+            if last_loc:\n+                self.builder.set_loc(last_loc)\n+            return ret\n \n     def generic_visit(self, node):\n         raise UnsupportedLanguageConstruct(None, node, \"unsupported AST node type: {}\".format(type(node).__name__))\n@@ -1070,11 +1111,12 @@ def ast_to_ttir(fn, signature, specialization, constants, debug, arch):\n     all_constants = constants.copy()\n     all_constants.update(new_constants)\n     arg_types = [str_to_ty(v) for k, v in signature.items() if k not in constants]\n+    file_name, begin_line = _get_fn_file_line(fn)\n \n     prototype = language.function_type([], arg_types)\n     generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants,\n                               function_name=function_name, attributes=new_attrs,\n-                              is_kernel=True, debug=debug,\n+                              is_kernel=True, debug=debug, file_name=file_name, begin_line=begin_line,\n                               arch=arch)\n     try:\n         generator.visit(fn.parse())"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -111,7 +111,9 @@ def run(self, *args, **kwargs):\n         if config.pre_hook is not None:\n             full_nargs = {**self.nargs, **kwargs, **self.best_config.kwargs}\n             config.pre_hook(full_nargs)\n-        return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n+        ret = self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n+        self.nargs = None\n+        return ret\n \n     def prune_configs(self, kwargs):\n         pruned_configs = self.configs"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -69,6 +69,13 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n   int32_t n_regs = 0;\n   int32_t n_spills = 0;\n   // create driver handles\n+  CUcontext pctx = 0;\n+  CUDA_CHECK(cuCtxGetCurrent(&pctx));\n+  if (!pctx) {\n+    CUDA_CHECK(cuDevicePrimaryCtxRetain(&pctx, device));\n+    CUDA_CHECK(cuCtxSetCurrent(pctx));\n+  }\n+\n   CUDA_CHECK(cuModuleLoadData(&mod, data));\n   CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n   // get allocated registers and spilled registers from the function"}]