[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 0, "deletions": 10, "changes": 10, "file_content_changes": "@@ -95,16 +95,6 @@ jobs:\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n \n-      - name: Disable MMAV3 and TMA\n-        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'H100')}}\n-        run: |\n-          echo \"ENABLE_TMA=0\" >> \"${GITHUB_ENV}\"\n-          echo \"ENABLE_MMA_V3=0\" >> \"${GITHUB_ENV}\"\n-\n-      - name: Clear cache\n-        run: |\n-          rm -rf ~/.triton\n-\n       - name: Run python tests on CUDA with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n         run: |"}, {"filename": ".gitignore", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -24,3 +24,6 @@ venv.bak/\n # JetBrains project files\n .idea\n cmake-build-*\n+\n+# Third-party binaries\n+ptxas"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -146,6 +146,13 @@ Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n                 ArrayRef<unsigned> shape);\n \n+// Returns null if the op is not inside a agent region (warp specialization\n+// mode). Note that there should be at most one agent id attached to the\n+// operation.\n+std::optional<int> getWSAgentId(Operation *op);\n+std::optional<int> getWSRoleId(Operation *op);\n+void setRoleId(Operation *op, int roleId);\n+\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUOps.td", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -372,15 +372,15 @@ def TTNG_RegAllocOp : TTNG_Op<\"reg_alloc\", []> {\n \n   let arguments = (ins I32Attr: $regCount);\n \n-  let assemblyFormat = \"$regCount attr-dict `:` type(operands)\";\n+  let assemblyFormat = \"$regCount attr-dict\";\n }\n \n def TTNG_RegDeallocOp : TTNG_Op<\"reg_dealloc\", []> {\n   let summary = \"register deallocation\";\n \n   let arguments = (ins I32Attr: $regCount);\n \n-  let assemblyFormat = \"$regCount attr-dict `:` type(operands)\";\n+  let assemblyFormat = \"$regCount attr-dict\";\n }\n \n #endif"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -73,6 +73,8 @@ createTritonNvidiaGPUFenceInsertionPass(int computeCapability = 90);\n std::unique_ptr<Pass>\n createTritonGPURewriteTensorPointerPass(int computeCapability = 80);\n \n+std::unique_ptr<Pass> createTritonNvidiaGPUWSFixupMissingAttrs();\n+\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\""}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/Transforms/Passes.td", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -225,4 +225,22 @@ def TritonGPURewriteTensorPointer : Pass</*cli-arg*/\"tritongpu-rewrite-tensor-po\n   ];\n }\n \n+def TritonGPUWSFixupMissingAttrs : Pass<\"triton-nvidia-gpu-ws-fixup-missing-attrs\", \"mlir::ModuleOp\"> {\n+  let summary = \"Fixup missing WS related attributes\";\n+\n+  let description = [{\n+    WS related attributes are attached to some key operations and are used when lowering to llvm.\n+    However these attributes maybe be dropped in the following IR transform. This pass tries to\n+    fixup the missing attributes.\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUWSFixupMissingAttrs()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\",\n+                           \"mlir::scf::SCFDialect\",\n+                           \"mlir::arith::ArithDialect\"];\n+}\n+\n+\n #endif"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 22, "deletions": 8, "changes": 30, "file_content_changes": "@@ -5,6 +5,7 @@\n #include \"../lib/Conversion/TritonGPUToLLVM/Utility.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n #include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n@@ -117,15 +118,28 @@ void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n     return;\n   }\n \n-  if (isa<triton::gpu::AsyncWaitOp>(op) &&\n-      !isa<gpu::BarrierOp>(op->getNextNode())) {\n+  if (isa<triton::gpu::AsyncWaitOp, triton::gpu::AsyncBulkWaitOp>(op) &&\n+      !isa<gpu::BarrierOp>(op->getNextNode()) &&\n+      !(isa<LLVM::InlineAsmOp>(op->getNextNode()) &&\n+        (dyn_cast<LLVM::InlineAsmOp>(op->getNextNode())\n+             .getAsmString()\n+             .find(\"bar.sync\") != std::string::npos))) {\n     // If the current op is an async wait and the next op is not a barrier we\n     // insert a barrier op and sync\n     blockInfo->sync();\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPointAfter(op);\n-    builder->create<gpu::BarrierOp>(op->getLoc());\n-    blockInfo->sync();\n+    if (auto optionalAgentId = getWSAgentId(op)) {\n+      int agentId = *optionalAgentId, roleId = 0;\n+      if (auto optionalRoleId = getWSRoleId(op))\n+        roleId = *optionalRoleId;\n+      int barId = agentId + roleId + nameBarrierIdBegin;\n+      assert(barId < nameBarrierIdEnd);\n+      barSync(*builder, op, barId, 128);\n+    } else {\n+      builder->create<gpu::BarrierOp>(op->getLoc());\n+      blockInfo->sync();\n+    }\n     return;\n   }\n \n@@ -180,10 +194,10 @@ void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n     // TODO(Keren): Don't expose LLVM Dialect ops here\n     // TODO[shuhaoj]: Change hard code style of numThreads. Hide async_agent\n     // attr. Better way to determine barId (number of agents are limited).\n-    if (op->hasAttr(\"async_agent\")) {\n-      int agentId = getAgentIds(op).front(), roleId = 0;\n-      if (op->hasAttr(\"agent.mutex_role\"))\n-        roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+    if (auto optionalAgentId = getWSAgentId(op)) {\n+      int agentId = *optionalAgentId, roleId = 0;\n+      if (auto optionalRoleId = getWSRoleId(op))\n+        roleId = *optionalRoleId;\n       int barId = agentId + roleId + nameBarrierIdBegin;\n       assert(barId < nameBarrierIdEnd);\n       barSync(*builder, op, barId, 128);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 14, "deletions": 13, "changes": 27, "file_content_changes": "@@ -1,5 +1,7 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"Utility.h\"\n+\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n@@ -589,11 +591,10 @@ struct ConvertLayoutOpConversion\n       if (repId != 0) {\n         // TODO[shuhaoj]: change hard code style of numThreads. Hide async\n         // attr.  Better way to determine barId (number of agents are limited).\n-        if (op->hasAttr(\"async_agent\")) {\n-          int agentId = getAgentIds(op).front(), roleId = 0;\n-          if (op->hasAttr(\"agent.mutex_role\"))\n-            roleId =\n-                op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+        if (auto optionalAgentId = getWSAgentId(op)) {\n+          int agentId = *optionalAgentId, roleId = 0;\n+          if (auto optionalRoleId = getWSRoleId(op))\n+            roleId = *optionalRoleId;\n           int barId = agentId + roleId + nameBarrierIdBegin;\n           assert(barId < nameBarrierIdEnd);\n           auto bar = rewriter.create<LLVM::ConstantOp>(\n@@ -624,10 +625,10 @@ struct ConvertLayoutOpConversion\n \n       // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n       // attr.  Better way to determine barId (number of agents are limited).\n-      if (op->hasAttr(\"async_agent\")) {\n-        int agentId = getAgentIds(op).front(), roleId = 0;\n-        if (op->hasAttr(\"agent.mutex_role\"))\n-          roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+      if (auto optionalAgentId = getWSAgentId(op)) {\n+        int agentId = *optionalAgentId, roleId = 0;\n+        if (auto optionalRoleId = getWSRoleId(op))\n+          roleId = *optionalRoleId;\n         int barId = agentId + roleId + nameBarrierIdBegin;\n         assert(barId < nameBarrierIdEnd);\n         auto bar = rewriter.create<LLVM::ConstantOp>(\n@@ -793,10 +794,10 @@ struct ConvertLayoutOpConversion\n       }\n       // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n       // attr.  Better way to determine barId (number of agents are limited).\n-      if (op->hasAttr(\"async_agent\")) {\n-        int agentId = getAgentIds(op).front(), roleId = 0;\n-        if (op->hasAttr(\"agent.mutex_role\"))\n-          roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+      if (auto optionalAgentId = getWSAgentId(op)) {\n+        int agentId = *optionalAgentId, roleId = 0;\n+        if (auto optionalRoleId = getWSRoleId(op))\n+          roleId = *optionalRoleId;\n         int barId = agentId + roleId + nameBarrierIdBegin;\n         assert(barId < nameBarrierIdEnd);\n         auto bar = rewriter.create<LLVM::ConstantOp>("}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -1,6 +1,7 @@\n #include \"ReduceOpToLLVM.h\"\n #include \"Utility.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n using namespace mlir;\n@@ -289,7 +290,7 @@ struct ReduceOpConversion\n             triton::ReduceOp op) const {\n     // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n     // attr.\n-    if (op->hasAttr(\"async_agent\")) {\n+    if (getWSAgentId(op)) {\n       barSync(rewriter, op, getAgentIds(op).front(), 128);\n     } else {\n       barrier();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -575,7 +575,7 @@ struct ExtractSliceOpConversion\n     // newShape = rank_reduce(shape)\n     // Triton only supports static tensor sizes\n     SmallVector<Value, 4> strideVals;\n-    for (auto i = 0; i < op.static_sizes().size(); ++i) {\n+    for (auto i = 0; i < op.getStaticSizes().size(); ++i) {\n       if (op.getStaticSize(i) == 1) {\n         offsetVals.erase(offsetVals.begin() + i);\n       } else {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 30, "deletions": 30, "changes": 60, "file_content_changes": "@@ -11,6 +11,7 @@\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n #include <set>\n \n@@ -31,6 +32,7 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n using ::mlir::triton::gpu::TMAMetadataTy;\n+namespace ttng = ::mlir::triton::nvidia_gpu;\n \n typedef DenseMap<Operation *, triton::MakeTensorPtrOp> TensorPtrMapT;\n \n@@ -238,12 +240,26 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return llvmStruct;\n   }\n \n-  Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n-    auto tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n+  // Returns CTA level thread idx\n+  Value getThreadIdInCTA(ConversionPatternRewriter &rewriter,\n+                         Location loc) const {\n+    Value tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n         loc, ::mlir::gpu::Dimension::x);\n     return rewriter.create<arith::IndexCastOp>(loc, i32_ty, tid);\n   }\n \n+  // Returns CTA level thread idx for not ws mode.\n+  // Returns agent level thread idx for ws mode.\n+  Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n+    Value tid = getThreadIdInCTA(rewriter, loc);\n+    auto mod = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    if (ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod)) {\n+      Value _128 = rewriter.create<arith::ConstantIntOp>(loc, 128, 32);\n+      tid = rewriter.create<arith::RemSIOp>(loc, tid, _128);\n+    }\n+    return tid;\n+  }\n+\n   static Value getSRegValue(OpBuilder &b, Location loc,\n                             const std::string &sRegStr) {\n     PTXBuilder builder;\n@@ -984,32 +1000,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return ret;\n   }\n \n-  SmallVector<Value>\n-  emitBaseIndexForMmaLayoutV2(Location loc, ConversionPatternRewriter &rewriter,\n-                              const MmaEncodingAttr &mmaLayout,\n-                              RankedTensorType type) const {\n-    auto shape = type.getShape();\n-    auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n-    assert(warpsPerCTA.size() == 2);\n-    auto order = triton::gpu::getOrder(mmaLayout);\n-    Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = i32_val(32);\n-    Value laneId = urem(threadId, warpSize);\n-    Value warpId = udiv(threadId, warpSize);\n-\n-    SmallVector<Value> multiDimWarpId =\n-        delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n-    multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n-    multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n-    Value offWarp0 = mul(multiDimWarpId[0], i32_val(16));\n-    Value offWarp1 = mul(multiDimWarpId[1], i32_val(8));\n-\n-    SmallVector<Value> multiDimBase(2);\n-    multiDimBase[0] = add(udiv(laneId, i32_val(4)), offWarp0);\n-    multiDimBase[1] = add(mul(i32_val(2), urem(laneId, i32_val(4))), offWarp1);\n-    return multiDimBase;\n-  }\n-\n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForMmaLayoutV2(const MmaEncodingAttr &mmaLayout,\n                            RankedTensorType type) const {\n@@ -1062,8 +1052,18 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     else\n       warpsN = shape[1] / instrShape[1];\n \n-    SmallVector<Value> multiDimWarpId =\n-        delinearize(rewriter, loc, warpId, _warpsPerCTA, order);\n+    SmallVector<Value> multiDimWarpId(2);\n+    if (mmaLayout.isHopper()) {\n+      // TODO[goostavz]: the tiling order from CTA->warp level is different for\n+      // MMAv2/3. This is a workaround since we don't explicitly have warpGrp\n+      // level in the layout definition, and the tiling order of warpGrp->warp\n+      // must be fixed to meet the HW's needs. We may need to consider to\n+      // explicitly define warpGrpPerCTA for MMAv3 layout.\n+      multiDimWarpId[0] = urem(warpId, warpsPerCTA[0]);\n+      multiDimWarpId[1] = urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]);\n+    } else {\n+      multiDimWarpId = delinearize(rewriter, loc, warpId, _warpsPerCTA, order);\n+    }\n     Value warpId0 = urem(multiDimWarpId[0], i32_val(warpsM));\n     Value warpId1 = urem(multiDimWarpId[1], i32_val(warpsN));\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1606,10 +1606,10 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op->getLoc(), newType, extract_slice.getSource());\n     rewriter.replaceOpWithNewOp<triton::gpu::ExtractSliceOp>(\n-        op, resType, newArg.getResult(), extract_slice.offsets(),\n-        extract_slice.sizes(), extract_slice.strides(),\n-        extract_slice.static_offsets(), extract_slice.static_sizes(),\n-        extract_slice.static_strides());\n+        op, resType, newArg.getResult(), extract_slice.getOffsets(),\n+        extract_slice.getSizes(), extract_slice.getStrides(),\n+        extract_slice.getStaticOffsets(), extract_slice.getStaticSizes(),\n+        extract_slice.getStaticStrides());\n     return mlir::success();\n   }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 10, "deletions": 3, "changes": 13, "file_content_changes": "@@ -17,6 +17,7 @@\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n@@ -47,6 +48,12 @@ class TritonGPUReorderInstructionsPass\n     // Sink conversions into loops when they will increase\n     // register pressure\n     DenseMap<Operation *, Operation *> opToMove;\n+    auto moveAfter = [](Operation *lhs, Operation *rhs) {\n+      auto lhsId = getWSRoleId(lhs);\n+      auto rhsId = getWSRoleId(rhs);\n+      if (lhsId == rhsId)\n+        lhs->moveAfter(rhs);\n+    };\n     m.walk([&](triton::gpu::ConvertLayoutOp op) {\n       if (!willIncreaseRegisterPressure(op))\n         return;\n@@ -70,15 +77,15 @@ class TritonGPUReorderInstructionsPass\n       Operation *argOp = op.getOperand().getDefiningOp();\n       if (!argOp)\n         return;\n-      op->moveAfter(argOp);\n+      moveAfter(op, argOp);\n     });\n     // Move transpositions just after their definition\n     opToMove.clear();\n     m.walk([&](triton::TransOp op) {\n       Operation *argOp = op.getOperand().getDefiningOp();\n       if (!argOp)\n         return;\n-      op->moveAfter(argOp);\n+      moveAfter(op, argOp);\n     });\n     // Move `dot` operand so that conversions to opIdx=1 happens after\n     // conversions to opIdx=0\n@@ -104,7 +111,7 @@ class TritonGPUReorderInstructionsPass\n       // after the conversion to OpIdx=0.\n       if (!dom.dominates(op.getOperation(), AOp.getOperation()))\n         return;\n-      op->moveAfter(AOp);\n+      moveAfter(op, AOp);\n     });\n     return;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "@@ -731,4 +731,28 @@ Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n   return linear;\n }\n \n+std::optional<int> getWSAgentId(Operation *op) {\n+  int prevAgentId = -1;\n+  if (auto attr = op->getAttrOfType<DenseIntElementsAttr>(\"async_agent\")) {\n+    for (auto agentId : attr.getValues<int>()) {\n+      assert(prevAgentId == -1 && \"support at most one agent id\");\n+      prevAgentId = agentId;\n+    }\n+  }\n+  if (prevAgentId == -1)\n+    return std::nullopt;\n+  return prevAgentId;\n+}\n+\n+std::optional<int> getWSRoleId(Operation *op) {\n+  if (!op->hasAttr(\"agent.mutex_role\"))\n+    return std::nullopt;\n+  return op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+}\n+\n+void setRoleId(Operation *op, int roleId) {\n+  auto attr = IntegerAttr::get(IntegerType::get(op->getContext(), 32), roleId);\n+  op->setAttr(\"agent.mutex_role\", attr);\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -6,6 +6,7 @@ add_mlir_dialect_library(TritonNvidiaGPUTransforms\n   WSPipeline.cpp\n   WSMutex.cpp\n   WSMaterialization.cpp\n+  WSFixupMissingAttrs.cpp\n   FenceInsertion.cpp\n   RewriteTensorPointer.cpp\n   Utility.cpp"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSFixupMissingAttrs.cpp", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -0,0 +1,69 @@\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+namespace mlir {\n+\n+namespace ttng = triton::nvidia_gpu;\n+\n+namespace {\n+\n+class TritonGPUWSFixupMissingAttrsPass\n+    : public TritonGPUWSFixupMissingAttrsBase<\n+          TritonGPUWSFixupMissingAttrsPass> {\n+public:\n+  TritonGPUWSFixupMissingAttrsPass() = default;\n+\n+  void runOnOperation() override {\n+    ModuleOp mod = getOperation();\n+    if (!ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod))\n+      return;\n+    OpBuilder builder(mod);\n+    mod->walk([&](mlir::triton::FuncOp funcOp) {\n+      for (Operation &op : funcOp.getBody().front().getOperations()) {\n+        if (!isa<scf::IfOp>(&op))\n+          continue;\n+        auto agentIds = getAgentIds(&op);\n+        if (agentIds.size() != 1)\n+          continue;\n+        Block *roleIdBlock = nullptr;\n+        op.walk<WalkOrder::PreOrder>([&](Operation *subOp) {\n+          setAgentIds(subOp, agentIds);\n+          // Find the outter most common block that has roleId.\n+          // The below implementation assumes that:\n+          // - all lock/unlock ops are in the same block (denoted as B).\n+          // - there is always one scf.if op in the front of `B` which has\n+          //   role id attached.\n+          // The above assumptions are maintained by WSMutex pass currently.\n+          if (!roleIdBlock && isa<scf::IfOp>(subOp) && getWSRoleId(subOp))\n+            roleIdBlock = subOp->getBlock();\n+        });\n+        if (!roleIdBlock)\n+          continue;\n+        int roleId = 0;\n+        for (Operation &roleOp : roleIdBlock->getOperations()) {\n+          auto optionalRoleId = getWSRoleId(&roleOp);\n+          if (!optionalRoleId) {\n+            setRoleId(&roleOp, roleId);\n+          } else {\n+            roleId = *optionalRoleId;\n+          }\n+          roleOp.walk([&](Operation *subOp) { setRoleId(subOp, roleId); });\n+        }\n+      }\n+    });\n+  }\n+};\n+\n+} // namespace\n+\n+std::unique_ptr<Pass> createTritonNvidiaGPUWSFixupMissingAttrs() {\n+  return std::make_unique<TritonGPUWSFixupMissingAttrsPass>();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMaterialization.cpp", "status": "modified", "additions": 4, "deletions": 28, "changes": 32, "file_content_changes": "@@ -380,8 +380,10 @@ void materializeTokenOperations(Operation *parentOp, int numCTAs) {\n     Value bufferEmptyArray =\n         builder.create<ttng::AllocMBarrierOp>(tokenLoc, mBarriersTy, numCTAs);\n \n-    // Make sure that MBarriers are initialized in all CTAs\n-    if (numCTAs > 1) {\n+    if (numCTAs == 1) {\n+      builder.create<mlir::gpu::BarrierOp>(tokenLoc);\n+    } else {\n+      // Make sure that MBarriers are initialized in all CTAs\n       builder.create<triton::nvidia_gpu::ClusterArriveOp>(tokenLoc, false);\n       builder.create<triton::nvidia_gpu::ClusterWaitOp>(tokenLoc);\n     }\n@@ -706,32 +708,6 @@ struct WSMaterializationPass\n     materializeMutexOperations(mod);\n     tryRegisterRealloc(mod);\n \n-    mod->walk([](Operation *op) {\n-      bool hasTensor = 0;\n-      auto results = op->getResults();\n-      auto operands = op->getOperands();\n-      for (auto i : results) {\n-        if (isa<RankedTensorType>(i.getType())) {\n-          hasTensor = 1;\n-          break;\n-        }\n-      }\n-      if (!hasTensor) {\n-        for (auto i : operands) {\n-          if (isa<RankedTensorType>(i.getType())) {\n-            hasTensor = 1;\n-            break;\n-          }\n-        }\n-      }\n-\n-      if (!hasTensor && !isa<ttng::MBarrierWaitOp>(op) &&\n-          !isa<ttng::ExtractMBarrierOp>(op) &&\n-          !isa<ttng::MBarrierArriveOp>(op)) {\n-        op->removeAttr(\"async_agent\");\n-      }\n-    });\n-\n     // TODO: More flexible way to set num-warps\n     // One dma, one math warp group, set num-warps = 8\n     auto i32_ty = IntegerType::get(mod->getContext(), 32);"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMutex.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -264,8 +264,9 @@ void mutexSync(ModuleOp &mod, scf::IfOp &ifOp, scf::ForOp &persistentForOp,\n       });\n     for (int i = 0; i < numRoles; ++i) {\n       if (lockLocs[i] == op) {\n+        if (roleId != -1)\n+          op->setAttr(\"agent.mutex_role\", builder.getI32IntegerAttr(roleId));\n         roleId = i;\n-        op->setAttr(\"agent.mutex_role\", builder.getI32IntegerAttr(i));\n         break;\n       }\n     }"}, {"filename": "lib/Hopper/HopperHelpers.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -32,4 +32,4 @@\n #define __DEVICE__ __device__ inline\n #else\n #define __DEVICE__\n-#endif\n+#endif\n\\ No newline at end of file"}, {"filename": "python/setup.py", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -68,7 +68,9 @@ def get_pybind11_package_info():\n def get_llvm_package_info():\n     # added statement for Apple Silicon\n     system = platform.system()\n-    arch = 'x86_64'\n+    arch = platform.machine()\n+    if arch == 'aarch64':\n+        arch = 'arm64'\n     if system == \"Darwin\":\n         system_suffix = \"apple-darwin\"\n         arch = platform.machine()\n@@ -84,6 +86,9 @@ def get_llvm_package_info():\n     name = f'llvm+mlir-17.0.0-{arch}-{system_suffix}-{release_suffix}'\n     version = \"llvm-17.0.0-c5dede880d17\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n+    # FIXME: remove the following once github.com/ptillet/triton-llvm-releases has arm64 llvm releases\n+    if arch == 'arm64' and 'linux' in system_suffix:\n+        url = f\"https://github.com/acollins3/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n \n \n@@ -124,7 +129,10 @@ def download_and_copy_ptxas():\n     base_dir = os.path.dirname(__file__)\n     src_path = \"bin/ptxas\"\n     version = \"12.1.105\"\n-    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n+    arch = platform.machine()\n+    if arch == \"x86_64\":\n+        arch = \"64\"\n+    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-{arch}/cuda-nvcc-{version}-0.tar.bz2\"\n     dst_prefix = os.path.join(base_dir, \"triton\")\n     dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n     dst_path = os.path.join(dst_prefix, dst_suffix)"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -1629,6 +1629,10 @@ void init_triton_ir(py::module &&m) {\n              self.addPass(mlir::createTritonNvidiaGPUWSMaterializationPass(\n                  computeCapability));\n            })\n+      .def(\"add_tritongpu_ws_fixup_missing_attrs_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonNvidiaGPUWSFixupMissingAttrs());\n+           })\n       .def(\n           \"add_convert_triton_to_tritongpu_pass\",\n           [](mlir::PassManager &self, int numWarps, int threadsPerWarp,\n@@ -1757,6 +1761,7 @@ void init_triton_translation(py::module &m) {\n   });\n   m.def(\"get_num_warps\", [](mlir::ModuleOp mod) {\n     auto shared = mod->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.num-warps\");\n+    assert(shared);\n     return shared.getInt();\n   });\n "}, {"filename": "python/test/unit/hopper/test_gemm.py", "status": "modified", "additions": 30, "deletions": 60, "changes": 90, "file_content_changes": "@@ -29,7 +29,6 @@\n \n import triton\n import triton.language as tl\n-from .utils import get_proper_err, get_variant_golden\n \n \n @triton.jit\n@@ -94,11 +93,6 @@ def matmul_no_scf_kernel(\n                              ]))\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_gemm_no_scf(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, ENABLE_WS):\n-    if '-'.join(map(str, [USE_TMA_EPILOGUE, ENABLE_WS])) in [\n-        'True-True'\n-    ]:\n-        pytest.skip(\"error, skip\")\n-\n     if (TRANS_A):\n         a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -127,14 +121,12 @@ def test_gemm_no_scf(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE\n     a_f32 = a.to(torch.float32)\n     b_f32 = b.to(torch.float32)\n     golden = torch.matmul(a_f32, b_f32)\n-    golden_variant = get_variant_golden(a_f32, b_f32)\n-    golden_abs_err, golden_rel_err = get_proper_err(golden, golden_variant)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(\n         c,\n         golden,\n-        rtol=max(1e-2, 1.1 * golden_rel_err),\n-        atol=max(1e-3, 1.1 * golden_abs_err),\n+        rtol=1e-2,\n+        atol=1e-3,\n         check_dtype=False)\n \n \n@@ -152,6 +144,7 @@ def matmul_kernel(\n     DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n     A_ORDER_0: tl.constexpr, A_ORDER_1: tl.constexpr,\n     B_ORDER_0: tl.constexpr, B_ORDER_1: tl.constexpr,\n+    W_ORDER_0: tl.constexpr, W_ORDER_1: tl.constexpr,\n     Z_ORDER_0: tl.constexpr, Z_ORDER_1: tl.constexpr\n ):\n     pid = tl.program_id(axis=0)\n@@ -170,8 +163,9 @@ def matmul_kernel(\n                                    offsets=(block_offset_m, 0), block_shape=(BLOCK_M, BLOCK_K), order=(A_ORDER_0, A_ORDER_1))\n     b_tile_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n                                    offsets=(0, block_offset_n), block_shape=(BLOCK_K, BLOCK_N), order=(B_ORDER_0, B_ORDER_1))\n+    # for chain-dot, BLOCK_N must always be equal to N, and each program loads the whole W matrix\n     w_tile_ptr = tl.make_block_ptr(base=w_ptr, shape=(N, N), strides=(stride_wm, stride_wn),\n-                                   offsets=(0, block_offset_n), block_shape=(BLOCK_N, BLOCK_N), order=(Z_ORDER_1, Z_ORDER_0))\n+                                   offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_N), order=(W_ORDER_0, W_ORDER_1))\n     z = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n \n     offs_m = block_offset_m + tl.arange(0, BLOCK_M)\n@@ -216,7 +210,7 @@ def matmul_kernel(\n         tl.store(z_ptrs, z, mask=mask)\n \n \n-@pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_C,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n+@pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_OUTPUT,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n                          [(128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n                           for shape_w_c in [\n                              # badcase from cublas-important-layers\n@@ -228,7 +222,7 @@ def matmul_kernel(\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                               # softmax works for one CTA\n                               for shape_w_c in [\n                              [64, 64, 16, 4, 1, 64, 64, 64],\n@@ -242,10 +236,10 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                               for shape_w_c in [\n                              [64, 64, 16, 4, 1, 128, 128, 64],\n                              *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n@@ -267,11 +261,11 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                             if not (epilogue == 'chain-dot' and (shape_w_c[5] is not None or shape_w_c[0] != shape_w_c[1]))\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             if not (epilogue == 'chain-dot' and (shape_w_c[6] is not None or shape_w_c[1] != shape_w_c[6]))\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                               for shape_w_c in [\n                              [64, 64, 32, 4, 1, 128, 256, 64],\n                              [128, 128, 16, 4, 4, 512, 256, 64],\n@@ -290,34 +284,34 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                               # loop over instr shapes\n                               for n in [16, 32, 64, 128, 256]\n-                              for trans_c in [False, True]\n+                              for trans_output in [False, True]\n                               for out_dtype in ['float16', 'float32']\n                               for use_tma_store in [False, True]\n                               for num_stages in [2, 4, 5, 7]\n                               for enable_ws in [False, True]\n-                              ] + [(*shape_w_c, *shape, False, True, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                              ] + [(*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                                    # irregular shapes\n                                    for shape_w_c in [\n                                        [128, 128, 64, 4, 1],\n                                        [256, 128, 64, 4, 2],\n                                        [128, 128, 128, 4, 2],\n                               ]\n                              for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for num_stages in [2, 3, 4]\n                              for enable_ws in [False, True]\n                          ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()\n                     [0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B, TRANS_C, epilogue, out_dtype, USE_TMA_STORE, NUM_STAGES, ENABLE_WS):\n+def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B, TRANS_OUTPUT, epilogue, out_dtype, USE_TMA_STORE, NUM_STAGES, ENABLE_WS):\n     if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B])) in [\n         '16-32-64-4-4-512-256-64-True-False',\n         '16-32-64-4-4-512-256-64-True-True',\n@@ -334,22 +328,7 @@ def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A,\n         '16-32-64-8-2-256-256-256-False',\n         '16-32-64-8-2-256-256-256-True',\n     ]:\n-        pytest.skip('illegal memory access.')\n-\n-    # with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n-    if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K])) in [\n-        '64-64-32-8-1-128-256-64',\n-    ]:\n-        pytest.skip('Tensor-likes are not close!')\n-\n-    if NUM_CTAS > 1 and NUM_WARPS == 8:\n-        pytest.skip('Tensor-likes are not close!')\n-\n-    # with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n-    if ENABLE_WS:\n-        # example:\n-        # [128-128-64-4-1-None-None-None-False-False-False-chain-dot-float16-False-3-True]\n-        pytest.skip('hang!')\n+        pytest.skip('Known legacy issue, ldmatrix can only support x4')\n \n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n@@ -381,27 +360,23 @@ def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A,\n \n     # avoid out of memory\n     if epilogue in ['add-matrix', 'add-rows', 'add-cols']:\n-        if (TRANS_C):\n-            bias = torch.randn((M, N), device='cuda', dtype=torch_out_dtype)\n-        else:\n+        if (TRANS_OUTPUT):\n             bias = torch.randn((N, M), device='cuda', dtype=torch_out_dtype).T\n+        else:\n+            bias = torch.randn((M, N), device='cuda', dtype=torch_out_dtype)\n     else:\n         bias = torch.randn((1, 1), device='cuda', dtype=torch_out_dtype)\n \n-    if epilogue == 'chain-dot':\n-        if (TRANS_C):\n-            w = torch.randn((N, N), device='cuda', dtype=torch.float16).T\n-        else:\n-            w = torch.randn((M, M), device='cuda', dtype=torch.float16)\n-    else:\n-        w = torch.randn((1, 1), device='cuda', dtype=torch.float16).T\n+    # for chain-dot only\n+    w = torch.randn((N, N), device='cuda', dtype=torch.float16).T\n+    w_order = [0, 1]\n \n-    if (TRANS_C):\n-        z = torch.full((M, N), 1., device='cuda', dtype=torch_out_dtype)\n-        z_order = [1, 0]\n-    else:\n+    if (TRANS_OUTPUT):\n         z = torch.full((N, M), 1., device='cuda', dtype=torch_out_dtype).T\n         z_order = [0, 1]\n+    else:\n+        z = torch.full((M, N), 1., device='cuda', dtype=torch_out_dtype)\n+        z_order = [1, 0]\n \n     # torch result\n     a_f32 = a.to(torch.float32)\n@@ -445,17 +420,12 @@ def grid(META):\n                               CHAIN_DOT=epilogue == 'chain-dot',\n                               A_ORDER_0=a_order[0], A_ORDER_1=a_order[1],\n                               B_ORDER_0=b_order[0], B_ORDER_1=b_order[1],\n+                              W_ORDER_0=w_order[0], W_ORDER_1=w_order[1],\n                               Z_ORDER_0=z_order[0], Z_ORDER_1=z_order[1],\n                               num_warps=NUM_WARPS, num_ctas=NUM_CTAS, num_stages=NUM_STAGES,\n                               enable_warp_specialization=ENABLE_WS)\n \n     torch.set_printoptions(profile=\"full\")\n-    # print(\"abs_err: {}, rel_err: {}\".format(golden_abs_err, golden_rel_err))\n-    # print(\"golden: \")\n-    # print(golden)\n-    # print(\"result: \")\n-    # print(z)\n-    # print(\"max_gap: {}\".format(torch.max(torch.abs(z - golden))))\n     golden = torch.nn.functional.normalize(golden)\n     z = torch.nn.functional.normalize(z)\n     assert_close(z, golden,"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 7, "deletions": 16, "changes": 23, "file_content_changes": "@@ -26,15 +26,6 @@\n \n import triton\n import triton.language as tl\n-from .utils import get_proper_err, get_variant_golden\n-\n-\n-def isMMAV3OrTMAEnabled():\n-    import os\n-    for k in ('ENABLE_MMA_V3', 'ENABLE_TMA'):\n-        if os.environ.get(k, '0').lower() in ['1', 'on', 'true']:\n-            return True\n-    return False\n \n \n @triton.jit\n@@ -280,7 +271,11 @@ def tma_warp_specialized_matmul_kernel(\n ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B):\n-    pytest.skip('hang')\n+    if '-'.join(map(str, [M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B])) in [\n+        '4096-4096-256-128-256-16-1-False-True',\n+        '4096-4096-256-128-256-64-1-False-True'\n+    ]:\n+        pytest.skip('Insufficient register resources')\n \n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n@@ -607,8 +602,6 @@ def static_persistent_matmul_no_scf_kernel(\n                              ]))\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_static_persistent_matmul_no_scf_kernel(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, USE_TMA_LOAD):\n-    if isMMAV3OrTMAEnabled():\n-        pytest.skip(\"known failure\")\n     if (TRANS_A):\n         a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -641,14 +634,12 @@ def test_static_persistent_matmul_no_scf_kernel(M, N, K, NUM_CTAS, NUM_WARPS, TR\n     a_f32 = a.to(torch.float32)\n     b_f32 = b.to(torch.float32)\n     golden = torch.matmul(a_f32, b_f32)\n-    golden_variant = get_variant_golden(a_f32, b_f32)\n-    golden_abs_err, golden_rel_err = get_proper_err(golden, golden_variant)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(\n         c,\n         golden,\n-        rtol=max(1e-2, 1.1 * golden_rel_err),\n-        atol=max(1e-3, 1.1 * golden_abs_err),\n+        rtol=1e-2,\n+        atol=1e-3,\n         check_dtype=False)\n \n "}, {"filename": "python/test/unit/hopper/test_tma_store_gemm.py", "status": "modified", "additions": 1, "deletions": 32, "changes": 33, "file_content_changes": "@@ -28,36 +28,6 @@\n import triton.language as tl\n \n \n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)\n-\n-\n @triton.jit\n def matmul_tma_load_store(\n     a_ptr, b_ptr, c_ptr,\n@@ -118,6 +88,5 @@ def test_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_F\n                                   num_ctas=NUM_CTAS,\n                                   OUTPUT_F16=OUTPUT_F16)\n     golden = torch.matmul(a, b)\n-    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n     torch.set_printoptions(profile=\"full\")\n-    assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n+    assert_close(c, golden, rtol=1e-2, atol=1e-3, check_dtype=False)"}, {"filename": "python/test/unit/hopper/ttgir_tests/test_tma.py", "status": "modified", "additions": 2, "deletions": 7, "changes": 9, "file_content_changes": "@@ -23,7 +23,6 @@\n \n import pytest\n import torch\n-from test_util import get_proper_err\n from torch.testing import assert_close\n \n import triton\n@@ -63,13 +62,9 @@ def test_tma_wgmma_64_64_16_f16(TTGIR, TRANS_A, TRANS_B):\n \n     golden = torch.matmul(a, b)\n     torch.set_printoptions(profile=\"full\", sci_mode=False)\n-    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n     assert_close(\n         c,\n         golden,\n-        rtol=max(1e-4,\n-                 1.5 * golden_rel_err),\n-        atol=max(\n-            1e-4,\n-            1.5 * golden_abs_err),\n+        rtol=1e-2,\n+        atol=1e-3,\n         check_dtype=False)"}, {"filename": "python/test/unit/hopper/ttgir_tests/test_util.py", "status": "removed", "additions": 0, "deletions": 52, "changes": 52, "file_content_changes": "@@ -1,52 +0,0 @@\n-# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n-#\n-# Permission is hereby granted, free of charge, to any person obtaining\n-# a copy of this software and associated documentation files\n-# (the \"Software\"), to deal in the Software without restriction,\n-# including without limitation the rights to use, copy, modify, merge,\n-# publish, distribute, sublicense, and/or sell copies of the Software,\n-# and to permit persons to whom the Software is furnished to do so,\n-# subject to the following conditions:\n-#\n-# The above copyright notice and this permission notice shall be\n-# included in all copies or substantial portions of the Software.\n-#\n-# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n-# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n-# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n-# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n-# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n-# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n-# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n-\n-import torch\n-\n-\n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)"}, {"filename": "python/test/unit/hopper/utils.py", "status": "removed", "additions": 0, "deletions": 32, "changes": 32, "file_content_changes": "@@ -1,32 +0,0 @@\n-import torch\n-\n-\n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K), dtype=a.dtype).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K), dtype=a.dtype).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N), dtype=b.dtype).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N), dtype=b.dtype).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(golden, golden_variant):\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    # avoid problems when golden_rel_err is 'inf'\n-    abs_golden = torch.abs(golden) + torch.full_like(golden, torch.finfo(golden.dtype).smallest_normal)\n-    golden_rel_err = torch.max(torch.abs(golden_diff) / abs_golden).item()\n-    return (golden_abs_err, golden_rel_err)"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -120,11 +120,13 @@ def optimize_ttgir(mod, num_stages, num_warps, num_ctas, arch,\n     pm.add_tritongpu_optimize_dot_operands_pass()\n     pm.add_tritongpu_remove_layout_conversions_pass()\n     pm.add_tritongpu_decompose_conversions_pass()\n+    pm.add_tritongpu_ws_fixup_missing_attrs_pass()\n     pm.add_tritongpu_reorder_instructions_pass()\n     pm.add_cse_pass()\n     pm.add_symbol_dce_pass()\n     if arch // 10 >= 9:\n         pm.add_tritongpu_fence_insertion_pass()\n+    pm.add_tritongpu_ws_fixup_missing_attrs_pass()\n     pm.run(mod)\n     return mod\n \n@@ -556,15 +558,15 @@ def compile(fn, **kwargs):\n             asm[ir_name] = str(next_module)\n         if ir_name == \"llir\" and \"shared\" not in metadata:\n             metadata[\"shared\"] = get_shared_memory_size(module)\n-        if ir_name == \"ttgir\" and enable_warp_specialization:\n-            metadata[\"num_warps\"] = get_num_warps(module)\n+        if ir_name == \"ttgir\":\n+            metadata[\"enable_warp_specialization\"] = _triton.ir.is_ws_supported(next_module)\n+            if metadata[\"enable_warp_specialization\"]:\n+                metadata[\"num_warps\"] = get_num_warps(next_module)\n         if ir_name == \"ptx\":\n             metadata[\"name\"] = get_kernel_name(next_module, pattern='// .globl')\n         if ir_name == \"amdgcn\":\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n-        if ir_name == \"ttgir\":\n-            metadata[\"enable_warp_specialization\"] = _triton.ir.is_ws_supported(next_module)\n         if not is_cuda and not is_hip:\n             _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n@@ -689,7 +691,7 @@ def __getitem__(self, grid):\n         def runner(*args, stream=None):\n             args_expand = self.assemble_tensormap_to_arg(args, self.constants)\n             if stream is None:\n-                if self.device_type in [\"cuda\", \"rocm\"]:\n+                if self.device_type in [\"cuda\", \"hip\"]:\n                     stream = get_cuda_stream()\n                 else:\n                     stream = get_backend(self.device_type).get_stream(None)"}, {"filename": "python/triton/hopper_lib/libhopper_helpers.bc", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -979,8 +979,8 @@ def _store_block_pointer(ptr, val, mask, boundary_check, cache, eviction, builde\n     if not val.type.is_block():\n         val = broadcast_impl_shape(val, block_shape, builder)\n     assert val.type.is_block(), \"Value argument must be block type or a scalar\"\n-    assert block_shape == val.type.get_block_shapes(), \"Block shape and value shape mismatch\"\n-    assert ptr.type.element_ty.element_ty == val.type.element_ty, \"Block element type and value element type mismatch\"\n+    assert block_shape == val.type.get_block_shapes(), f\"Block shape({block_shape}) and value shape({val.type.get_block_shapes()}) mismatch\"\n+    assert ptr.type.element_ty.element_ty == val.type.element_ty, f\"Block element type({ptr.type.element_ty.element_ty}) and value element type({val.type.element_ty}) mismatch\"\n \n     elt_ty = ptr.type.element_ty.element_ty\n     assert elt_ty != tl.int1, \"`tl.int1` should be rewrited in `tl.make_block_ptr`\""}, {"filename": "python/triton/third_party/cuda/bin/ptxas", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/compile.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -43,6 +43,7 @@\n     parser.add_argument(\"path\", help=\"Path to Python source containing desired kernel in its scope. File will be executed.\")\n     parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\", help=\"Name of the kernel to compile\", required=True)\n     parser.add_argument(\"--num-warps\", \"-w\", type=int, default=1, help=\"Number of warps to launch the kernel\")\n+    parser.add_argument(\"--num-stages\", \"-ns\", type=int, default=3, help=\"Number of stages meta-parameter for the kernel\")\n     parser.add_argument(\"--out-name\", \"-on\", type=str, default=None, help=\"Out name for the compiled kernel\")\n     parser.add_argument(\"--out-path\", \"-o\", type=Path, default=None, help=\"Out filename\")\n     parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\", required=True)\n@@ -96,7 +97,7 @@ def constexpr(s):\n     config = triton.compiler.instance_descriptor(divisible_by_16=divisible_by_16, equal_to_1=equal_to_1)\n     for i in equal_to_1:\n         constexprs.update({i: 1})\n-    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=args.num_warps)\n+    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=args.num_warps, num_stages=args.num_stages)\n     arg_names = []\n     arg_types = []\n     for i in signature.keys():"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 36, "deletions": 34, "changes": 70, "file_content_changes": "@@ -32,33 +32,34 @@ def _fwd_kernel(\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n     stride_oz, stride_oh, stride_om, stride_on,\n-    Z, H, N_CTX,\n+    Z, H, N_CTX, P_SEQ,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n     IS_CAUSAL: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n-    qvk_offset = off_hz * stride_qh\n+    q_offset = off_hz * stride_qh\n+    kv_offset = off_hz * stride_kh\n     Q_block_ptr = tl.make_block_ptr(\n-        base=Q + qvk_offset,\n+        base=Q + q_offset,\n         shape=(N_CTX, BLOCK_DMODEL),\n         strides=(stride_qm, stride_qk),\n         offsets=(start_m * BLOCK_M, 0),\n         block_shape=(BLOCK_M, BLOCK_DMODEL),\n         order=(1, 0)\n     )\n     K_block_ptr = tl.make_block_ptr(\n-        base=K + qvk_offset,\n-        shape=(BLOCK_DMODEL, N_CTX),\n+        base=K + kv_offset,\n+        shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\n         strides=(stride_kk, stride_kn),\n         offsets=(0, 0),\n         block_shape=(BLOCK_DMODEL, BLOCK_N),\n         order=(0, 1)\n     )\n     V_block_ptr = tl.make_block_ptr(\n-        base=V + qvk_offset,\n-        shape=(N_CTX, BLOCK_DMODEL),\n+        base=V + kv_offset,\n+        shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\n         strides=(stride_vk, stride_vn),\n         offsets=(0, 0),\n         block_shape=(BLOCK_N, BLOCK_DMODEL),\n@@ -80,15 +81,15 @@ def _fwd_kernel(\n     q = (q * qk_scale).to(tl.float16)\n     # loop over k, v and update accumulator\n     lo = 0\n-    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n+    hi = P_SEQ + (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX + P_SEQ\n     for start_n in range(lo, hi, BLOCK_N):\n         # -- load k, v --\n         k = tl.load(K_block_ptr)\n         v = tl.load(V_block_ptr)\n         # -- compute qk ---\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         if IS_CAUSAL:\n-            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+            qk = tl.where(P_SEQ + offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n         qk += tl.dot(q, k)\n         # -- compute scaling constant ---\n         m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n@@ -110,7 +111,7 @@ def _fwd_kernel(\n     tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n     # write back O\n     O_block_ptr = tl.make_block_ptr(\n-        base=Out + qvk_offset,\n+        base=Out + q_offset,\n         shape=(N_CTX, BLOCK_DMODEL),\n         strides=(stride_om, stride_on),\n         offsets=(start_m * BLOCK_M, 0),\n@@ -146,8 +147,8 @@ def _bwd_kernel(\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n-    Z, H, N_CTX,\n-    num_block,\n+    Z, H, N_CTX, P_SEQ,\n+    num_block_q, num_block_kv,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n     CAUSAL: tl.constexpr,\n@@ -158,20 +159,20 @@ def _bwd_kernel(\n     qk_scale = sm_scale * 1.44269504\n     # offset pointers for batch/head\n     Q += off_z * stride_qz + off_h * stride_qh\n-    K += off_z * stride_qz + off_h * stride_qh\n-    V += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_kz + off_h * stride_kh\n+    V += off_z * stride_vz + off_h * stride_vh\n     DO += off_z * stride_qz + off_h * stride_qh\n     DQ += off_z * stride_qz + off_h * stride_qh\n-    DK += off_z * stride_qz + off_h * stride_qh\n-    DV += off_z * stride_qz + off_h * stride_qh\n-    for start_n in range(0, num_block):\n+    DK += off_z * stride_kz + off_h * stride_kh\n+    DV += off_z * stride_vz + off_h * stride_vh\n+    for start_n in range(0, num_block_kv):\n         if CAUSAL:\n-            lo = start_n * BLOCK_M\n+            lo = tl.math.max(start_n * BLOCK_M - P_SEQ, 0)\n         else:\n             lo = 0\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n-        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M) \n         offs_m = tl.arange(0, BLOCK_N)\n         offs_k = tl.arange(0, BLOCK_DMODEL)\n         # initialize pointers to value-like data\n@@ -183,20 +184,20 @@ def _bwd_kernel(\n         # pointer to row-wise quantities in value-like data\n         D_ptrs = D + off_hz * N_CTX\n         l_ptrs = L + off_hz * N_CTX\n-        # initialize dv amd dk\n-        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # initialize dk amd dv\n         dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n         # k and v stay in SRAM throughout\n         k = tl.load(k_ptrs)\n         v = tl.load(v_ptrs)\n         # loop over rows\n-        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+        for start_m in range(lo, num_block_q * BLOCK_M, BLOCK_M):\n             offs_m_curr = start_m + offs_m\n             # load q, k, v, do on-chip\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             if CAUSAL:\n-                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+                qk = tl.where(P_SEQ + offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n             else:\n                 qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n             qk += tl.dot(q, tl.trans(k))\n@@ -223,10 +224,10 @@ def _bwd_kernel(\n             q_ptrs += BLOCK_M * stride_qm\n             do_ptrs += BLOCK_M * stride_qm\n         # write-back\n-        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n         dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-        tl.store(dv_ptrs, dv)\n+        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n         tl.store(dk_ptrs, dk)\n+        tl.store(dv_ptrs, dv)\n \n \n empty = torch.empty(128, device=\"cuda\")\n@@ -245,7 +246,7 @@ def forward(ctx, q, k, v, causal, sm_scale):\n         BLOCK_N = 64\n         grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-\n+        P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n         num_warps = 4 if Lk <= 64 else 8\n         _fwd_kernel[grid](\n             q, k, v, sm_scale,\n@@ -255,7 +256,7 @@ def forward(ctx, q, k, v, causal, sm_scale):\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n+            q.shape[0], q.shape[1], q.shape[2], P_SEQ,\n             BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n             IS_CAUSAL=causal,\n             num_warps=num_warps,\n@@ -266,6 +267,7 @@ def forward(ctx, q, k, v, causal, sm_scale):\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n         ctx.causal = causal\n+        ctx.P_SEQ = P_SEQ\n         return o\n \n     @staticmethod\n@@ -290,8 +292,8 @@ def backward(ctx, do):\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            ctx.grid[0],\n+            q.shape[0], q.shape[1], q.shape[2], ctx.P_SEQ,\n+            ctx.grid[0], triton.cdiv(k.shape[2], BLOCK),\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n             CAUSAL=ctx.causal,\n@@ -303,17 +305,17 @@ def backward(ctx, do):\n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD, P_SEQ', [(6, 9, 1024, 64, 128)])\n @pytest.mark.parametrize('causal', [False, True])\n-def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n+def test_op(Z, H, N_CTX, D_HEAD, P_SEQ, causal, dtype=torch.float16):\n     torch.manual_seed(20)\n     q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n     sm_scale = 0.5\n     dout = torch.randn_like(q)\n     # reference implementation\n-    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+    M = torch.tril(torch.ones((N_CTX, N_CTX + P_SEQ), device=\"cuda\"), diagonal=P_SEQ)\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n     if causal:\n         p[:, :, M == 0] = float(\"-inf\")"}, {"filename": "python/tutorials/10-experimental-tmastg-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 29, "changes": 34, "file_content_changes": "@@ -84,31 +84,6 @@ def matmul_kernel(\n     tl.store(c_block_ptr, accumulator)\n \n \n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)\n-\n-\n def matmul(a, b):\n     # checks constraints\n     assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n@@ -134,15 +109,16 @@ def grid(META):\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n b = torch.randn((512, 512), device='cuda', dtype=torch.float16).T\n c = matmul(a, b)\n+c = torch.nn.functional.normalize(c)\n+\n+golden = torch.nn.functional.normalize(torch.matmul(a, b))\n \n-golden = torch.matmul(a, b)\n-golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n torch.set_printoptions(profile=\"full\")\n assert_close(\n     c,\n     golden,\n-    rtol=max(1e-4, 1.5 * golden_rel_err),\n-    atol=max(1e-4, 1.5 * golden_abs_err),\n+    rtol=1e-2,\n+    atol=1e-3,\n     check_dtype=False)\n \n "}]