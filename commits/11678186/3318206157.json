[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 24, "deletions": 11, "changes": 35, "file_content_changes": "@@ -657,7 +657,10 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n                  tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>()) {\n     return convertSplatLikeOpWithMmaLayout(\n         mmaLayout, resType, elemType, constVal, typeConverter, rewriter, loc);\n-  }\n+  } else\n+    assert(false && \"Unsupported layout found in ConvertSplatLikeOp\");\n+\n+  return Value{};\n }\n \n struct SplatOpConversion\n@@ -2474,16 +2477,27 @@ struct DotOpConversionHelper {\n \n   // Deduce the M and N from either $c or $d type.\n   static std::tuple<int, int> getMatShapeMN() {\n-    // According to DotOpConversionHelper::mmaInstrShape, all the matrix shape's\n+    // According to DotOpConversionHelper::mmaMatShape, all the matrix shape's\n     // M,N are {8,8}\n     return {8, 8};\n   }\n \n-  static std::tuple<int, int> getRepMN(const TensorType &tensorTy) {\n+  // Get the M and N of mma instruction shape.\n+  static std::tuple<int, int> getInstrShapeMN() {\n+    // According to DotOpConversionHelper::mmaInstrShape, all the M,N are {16,8}\n+    return {16, 8};\n+  }\n+\n+  static std::tuple<int, int> getRepMN(const RankedTensorType &tensorTy) {\n+    auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto wpt = mmaLayout.getWarpsPerCTA();\n+\n     int M = tensorTy.getShape()[0];\n     int N = tensorTy.getShape()[1];\n-    auto [matM, matN] = getMatShapeMN();\n-    return {M / matM, N / matN};\n+    auto [instrM, instrN] = getInstrShapeMN();\n+    int repM = std::max<int>(M / (wpt[0] * instrM), 1);\n+    int repN = std::max<int>(N / (wpt[1] * instrN), 1);\n+    return {repM, repN};\n   }\n \n   Type getShemPtrTy() const {\n@@ -2863,10 +2877,9 @@ struct MMA16816ConversionHelper {\n     // Else load a normal C tensor with mma layout, that should be a\n     // LLVM::struct with fcSize elements.\n     auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n-    printf(\"struct.size: %d, fcSize: %d\\n\", structTy.getBody().size(), fcSize);\n-    // assert(structTy.getBody().size() == fcSize &&\n-    //\"DotOp's $c operand should pass the same number of values as $d in \"\n-    //\"mma layout.\");\n+    assert(structTy.getBody().size() == fcSize &&\n+           \"DotOp's $c operand should pass the same number of values as $d in \"\n+           \"mma layout.\");\n     return llTensor;\n   }\n \n@@ -2900,7 +2913,7 @@ struct MMA16816ConversionHelper {\n     ValueTable hb = getValuesFromDotOperandLayoutStruct(\n         loadedB, std::max(numRepN / 2, 1), numRepK);\n     auto fc = ConvertTritonGPUOpToLLVMPatternBase::getElementsFromStruct(\n-        loc, adaptor.c(), rewriter);\n+        loc, loadedC, rewriter);\n     printf(\"fc.size: %d\\n\", fc.size());\n \n     auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n@@ -3088,7 +3101,7 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n     // operand $b\n     res = mmaHelper.loadB(src, adaptor.src());\n   } else if (dotOperandLayout.getOpIdx() == 2) {\n-    auto tensorTy = src.getType().cast<TensorType>();\n+    auto tensorTy = src.getType().cast<RankedTensorType>();\n     auto [repM, repN] = DotOpConversionHelper::getRepMN(tensorTy);\n     res = mmaHelper.loadC(src, adaptor.src());\n   }"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 33, "deletions": 21, "changes": 54, "file_content_changes": "@@ -78,24 +78,36 @@ def matmul_kernel(\n     tl.store(c_ptrs, accumulator)\n \n # TODO: DotConversion in TritonGPUToLLVM cannot support non-splat C for the moment\n-# @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n-#    [128, 256, 128, 4, 128, 256, 32],\n-#    # [256, 128, 64, 4, 256, 128, 16],\n-#    # [128, 16, 128, 4, 128, 16, 32],\n-#    # [32, 128, 256, 4, 32, 128, 64],\n-# ])\n-# def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n-#    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-#    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n-#    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n-#    grid = lambda META: (1, )\n-#    matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-#                        stride_am=a.stride(0), stride_ak=a.stride(1),\n-#                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n-#                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n-#                        M=a.shape[0], N=b.shape[1], K=a.shape[1],\n-#                        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n-#                        num_warps=NUM_WARPS)\n-#    golden = torch.matmul(a, b)\n-#    torch.set_printoptions(profile=\"full\")\n-#    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n+   [128, 256, 128, 4, 128, 256, 32],\n+   # [256, 128, 64, 4, 256, 128, 16],\n+   # [128, 16, 128, 4, 128, 16, 32],\n+   # [32, 128, 256, 4, 32, 128, 64],\n+])\n+def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n+   a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+   b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+   c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n+   grid = lambda META: (1, )\n+   matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                       stride_am=a.stride(0), stride_ak=a.stride(1),\n+                       stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                       stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                       M=a.shape[0], N=b.shape[1], K=a.shape[1],\n+                       BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n+                       num_warps=NUM_WARPS)\n+   golden = torch.matmul(a, b)\n+   torch.set_printoptions(profile=\"full\")\n+   assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n+\n+\n+if __name__ == \"__main__\":\n+    # works\n+    #test_gemm(SIZE_M=16, SIZE_N=8, SIZE_K=64, NUM_WARPS=1,\n+              #BLOCK_SIZE_M=16, BLOCK_SIZE_N=8, BLOCK_SIZE_K=16)\n+    test_gemm(SIZE_M=32, SIZE_N=8, SIZE_K=64, NUM_WARPS=1,\n+              BLOCK_SIZE_M=16, BLOCK_SIZE_N=8, BLOCK_SIZE_K=16)\n+              \n+    #test_gemm(SIZE_M=128, SIZE_N=256, SIZE_K=128, NUM_WARPS=4,\n+              #BLOCK_SIZE_M=128, BLOCK_SIZE_N=256, BLOCK_SIZE_K=32)\n+    #test_gemm_no_scf(128, 256, 32, 4)"}]