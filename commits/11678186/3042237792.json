[{"filename": "lib/codegen/pass.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -106,11 +106,11 @@ std::unique_ptr<llvm::Module> add_passes_to_emit_bin(\n   // run passes\n   inliner.run(ir);\n   dce.run(ir);\n-  // ir.print(std::cout);\n   peephole.run(ir);\n   dce.run(ir);\n   pipeline.run(ir);\n   dce.run(ir);\n+  // ir.print(std::cout);\n   disassociate.run(ir);\n   dce.run(ir);\n   align.run(ir);"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -180,7 +180,7 @@ def triton_attention(\n ):\n     sparse_dot_sdd_nt = triton.ops.blocksparse.matmul(layout, block, \"sdd\", trans_a=False, trans_b=True, device=value.device)\n     sparse_dot_dsd_nn = triton.ops.blocksparse.matmul(layout, block, \"dsd\", trans_a=False, trans_b=False, device=value.device)\n-    sparse_softmax = triton.ops.blocksparse.softmax(layout, block, device=value.device)\n+    sparse_softmax = triton.ops.blocksparse.softmax(layout, block, device=value.device, is_dense=True)\n \n     w = sparse_dot_sdd_nt(query, key)\n     w = sparse_softmax(w, scale=scale, is_causal=True)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 14, "deletions": 10, "changes": 24, "file_content_changes": "@@ -819,7 +819,8 @@ def make_triton_ir(fn, signature, specialization, constants):\n     # visit kernel AST\n     gscope = fn.__globals__.copy()\n     function_name = '_'.join([fn.__name__, kernel_suffix(signature, specialization)])\n-    new_constants = {k: 1 for k in specialization.equal_to_1}\n+    tys = signature.split(',')\n+    new_constants = {k: True if tys[k]==\"i1\" else 1 for k in specialization.equal_to_1}\n     new_attrs = {k: (\"multiple_of\", 16) for k in specialization.divisible_by_16}\n     all_constants = constants.copy()\n     all_constants.update(new_constants)\n@@ -897,7 +898,7 @@ def ty_to_cpp(ty):\n     if ty[0] == '*':\n         return \"CUdeviceptr\"\n     return {\n-        \"i1\": \"bool\",\n+        \"i1\": \"int32_t\",\n         \"i8\": \"int8_t\",\n         \"i16\": \"int16_t\",\n         \"i32\": \"int32_t\",\n@@ -967,7 +968,7 @@ def _build(name, src, path):\n     return so\n \n \n-def generate_torch_glue(kernel_name, config, signature, num_warps, binaries, tmpdir):\n+def generate_torch_glue(kernel_name, constants, signature, num_warps, binaries, tmpdir):\n     headers = dict()\n     tys = signature.split(',')\n     # write all cubins to header files\n@@ -989,9 +990,10 @@ def generate_torch_glue(kernel_name, config, signature, num_warps, binaries, tmp\n     def _extracted_type(ty):\n         if ty[0] == '*':\n             return \"PyObject*\"\n-        if ty[0] == 'i':\n-            return \"long\"\n         return {\n+            'i1': 'int32_t',\n+            'i32': 'int32_t',\n+            'i64': 'int64_t',\n             'u32': 'uint32_t',\n             'u64': 'uint64_t',\n             'fp32': 'float',\n@@ -1010,7 +1012,7 @@ def format_of(ty):\n             \"int64_t\": \"L\",\n         }[ty]\n \n-    format = \"iiil\" + ''.join([format_of(_extracted_type(ty)) for ty in tys])\n+    format = \"iiiK\" + ''.join([format_of(_extracted_type(ty)) for ty in tys])\n \n     # generate glue code\n     n_args = len(tys)\n@@ -1073,7 +1075,7 @@ def format_of(ty):\n   if(function == 0)\n     init_module(device);\n \n-  void *params[] = {{ {', '.join(f\"&arg{i}\" if tys[i][0]=='*' else f\"(void*)&arg{i}\" for i in range(n_args) if i not in config.equal_to_1)} }};\n+  void *params[] = {{ {', '.join(f\"&arg{i}\" for i in range(n_args) if i not in constants)} }};\n   CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*{num_warps}, 1, 1, {name}_shmem, stream, params, 0));\n }}\n \n@@ -1194,8 +1196,8 @@ def make_cache_key(fn, signature, configs, constants, num_warps, num_stages):\n     return key\n \n \n-def make_shared_object(fn, config, signature, num_warps, binaries, tmpdir):\n-    src = generate_torch_glue(fn.__name__, config, signature, num_warps, binaries, tmpdir)\n+def make_shared_object(fn, constants, signature, num_warps, binaries, tmpdir):\n+    src = generate_torch_glue(fn.__name__, constants, signature, num_warps, binaries, tmpdir)\n     src_path = os.path.join(tmpdir, \"main.c\")\n     with open(src_path, \"w\") as f:\n         f.write(src)\n@@ -1225,7 +1227,9 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n         binaries.append(_compile(fn, signature, device, constants, config, num_warps, num_stages, extern_libs, \"cubin\"))\n     # generate and compile glue code into shared object\n     with tempfile.TemporaryDirectory() as tmpdir:\n-        so = make_shared_object(fn, configs[0], signature, num_warps, binaries, tmpdir)\n+        all_constants = set(constants.keys())\n+        all_constants.update(configs[0].equal_to_1)\n+        so = make_shared_object(fn, all_constants, signature, num_warps, binaries, tmpdir)\n \n     # write shared object to cache\n     cache_manager.put(so)"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -237,7 +237,6 @@ def dsd_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, lut, width, out=N\n         TILE_M=block, TILE_N=TILE_N, TILE_K=min(block, 32), BLOCK=block, num_stages=4,\n         num_warps=4, GROUP_SIZE_M=4,\n     )\n-    print(pgm.asm[\"ptx\"])\n     # exit()\n     return c\n "}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -18,8 +18,8 @@ def num_warps(n):\n \n @triton.jit\n def _blocksparse_softmax_fwd(\n-    Out, A, stride_xz, LUT,\n-    R, extent, stride_zr, stride_hr,  # relative attention\n+    Out, A, LUT, R, stride_xz,\n+    extent, stride_zr, stride_hr,  # relative attention\n     scale, is_causal,\n     ROW_SIZE: tl.constexpr,\n     BLOCK_SIZE: tl.constexpr,\n@@ -164,8 +164,8 @@ def forward(\n         # enqueue kernel\n         out = torch.empty_like(a)\n         _blocksparse_softmax_fwd[grid](\n-            out, a, a.stride(0), lut,\n-            rel_logits, rel_shape[-1], rel_strides[0], rel_strides[1],  # relative attn\n+            out, a, lut, rel_logits, a.stride(0),\n+            rel_shape[-1], rel_strides[0], rel_strides[1],  # relative attn\n             scale,\n             is_causal,\n             BLOCK_SIZE=block,"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -159,6 +159,8 @@ class JITFunction:\n     def _key_of(arg):\n         if hasattr(arg, \"dtype\"):\n             return arg.dtype\n+        elif isinstance(arg, bool):\n+            return \"i1\"\n         elif isinstance(arg, int):\n             if -2**31 <= arg and arg <= 2**31 - 1:\n                 return \"i32\""}]