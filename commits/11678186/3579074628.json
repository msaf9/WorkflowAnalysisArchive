[{"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -132,6 +132,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n           AxisInfo::DimVectorT(ty.getShape().begin(), ty.getShape().end()));\n     }\n   }\n+  // TODO: refactor & complete binary ops\n   // Addition\n   if (llvm::isa<arith::AddIOp, triton::AddPtrOp>(op)) {\n     auto newContiguity = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n@@ -159,6 +160,20 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n                          newContiguity, newDivisibility, newConstancy);\n   }\n+  // Remainder\n+  if (llvm::isa<arith::RemSIOp, arith::RemUIOp>(op)) {\n+    auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) {\n+      return gcd(lhs.getContiguity(d), rhs.getDivisibility(d));\n+    };\n+    auto newDivisibility = [](AxisInfo lhs, AxisInfo rhs, int d) {\n+      return gcd(lhs.getDivisibility(d), rhs.getDivisibility(d));\n+    };\n+    auto newConstancy = [](AxisInfo lhs, AxisInfo rhs, int d) {\n+      return gcd(lhs.getConstancy(d), rhs.getConstancy(d));\n+    };\n+    curr = visitBinaryOp(op, operands[0]->getValue(), operands[1]->getValue(),\n+                         newContiguity, newDivisibility, newConstancy);\n+  }\n   // TODO: All other binary ops\n   if (llvm::isa<arith::AndIOp, arith::OrIOp>(op)) {\n     auto newContiguity = [](AxisInfo lhs, AxisInfo rhs, int d) { return 1; };"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -58,11 +58,6 @@ SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n   unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n   smemShapes[1].push_back(numWarps * 32);\n \n-  /// FIXME(Qingyi): This requirement is actually not necessary, because it is\n-  /// always smaller than smemShapes[0] shared memory block2\n-  smemShapes[2] = convertType<unsigned>(getSrcShape());\n-  smemShapes[2].erase(smemShapes[2].begin() + axis);\n-\n   return smemShapes;\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -973,8 +973,9 @@ struct MMA16816ConversionHelper {\n     if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n       Value warpM = getWarpM(shape[0]);\n       // load from smem\n+      int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / matShapeM);\n       loadFn = getLoadMatrixFn(\n-          tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n+          tensor, smemObj, mmaLayout, wpt /*wpt*/,\n           1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n           {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/,\n           true /*isA*/);\n@@ -1016,8 +1017,9 @@ struct MMA16816ConversionHelper {\n     int numRepN = getNumRepN(tensorTy, shape[1]);\n \n     Value warpN = getWarpN(shape[1]);\n+    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / matShapeN);\n     auto loadFn = getLoadMatrixFn(\n-        tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n+        tensor, smemObj, mmaLayout,  wpt /*wpt*/,\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/,\n         false /*isA*/);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 15, "deletions": 63, "changes": 78, "file_content_changes": "@@ -43,6 +43,8 @@ using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::LLVM::getStructFromElements;\n using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::LLVM::SharedMemoryObject;\n+using ::mlir::LLVM::shflSync;\n+using ::mlir::LLVM::storeShared;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getElemsPerThread;\n@@ -307,20 +309,6 @@ T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape,\n                                reorder(shape, order));\n }\n \n-Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n-                  Value val, Value pred) {\n-  MLIRContext *ctx = rewriter.getContext();\n-  unsigned bits = val.getType().getIntOrFloatBitWidth();\n-  const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n-\n-  PTXBuilder builder;\n-  auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n-  auto *valOpr = builder.newOperand(val, c);\n-  auto &st = builder.create<>(\"st\")->shared().b(bits);\n-  st(ptrOpr, valOpr).predicate(pred, \"b\");\n-  return builder.launch(rewriter, loc, void_ty(ctx));\n-}\n-\n struct ConvertTritonGPUOpToLLVMPatternBase {\n   static Value\n   getStructFromSharedMemoryObject(Location loc,\n@@ -1310,9 +1298,6 @@ struct ReduceOpConversion\n                            RedOp redOp, Value &acc, Value &accIndex, Value cur,\n                            Value curIndex, bool isFirst) const;\n \n-  Value shflSync(ConversionPatternRewriter &rewriter, Location loc, Value val,\n-                 int i) const;\n-\n   // Use shared memory for reduction within warps and across warps\n   LogicalResult matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n                                      ConversionPatternRewriter &rewriter) const;\n@@ -1440,34 +1425,6 @@ void ReduceOpConversion::accumulateWithIndex(\n   }\n }\n \n-Value ReduceOpConversion::shflSync(ConversionPatternRewriter &rewriter,\n-                                   Location loc, Value val, int i) const {\n-  unsigned bits = val.getType().getIntOrFloatBitWidth();\n-\n-  if (bits == 64) {\n-    Type vecTy = vec_ty(f32_ty, 2);\n-    Value vec = bitcast(val, vecTy);\n-    Value val0 = extract_element(f32_ty, vec, i32_val(0));\n-    Value val1 = extract_element(f32_ty, vec, i32_val(1));\n-    val0 = shflSync(rewriter, loc, val0, i);\n-    val1 = shflSync(rewriter, loc, val1, i);\n-    vec = undef(vecTy);\n-    vec = insert_element(vecTy, vec, val0, i32_val(0));\n-    vec = insert_element(vecTy, vec, val1, i32_val(1));\n-    return bitcast(vec, val.getType());\n-  }\n-\n-  PTXBuilder builder;\n-  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n-  auto *dOpr = builder.newOperand(\"=r\");\n-  auto *aOpr = builder.newOperand(val, \"r\");\n-  auto *bOpr = builder.newConstantOperand(i);\n-  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n-  auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n-  shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n-  return builder.launch(rewriter, loc, val.getType(), false);\n-}\n-\n LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     triton::ReduceOp op, OpAdaptor adaptor,\n     ConversionPatternRewriter &rewriter) const {\n@@ -1633,7 +1590,6 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto smemShapes = helper.getScratchConfigsFast();\n   unsigned elems = product<unsigned>(smemShapes[0]);\n   unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));\n-  maxElems = std::max(maxElems, product<unsigned>(smemShapes[2]));\n   Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(maxElems));\n   indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n \n@@ -1693,11 +1649,11 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n \n     // reduce within warps\n     for (unsigned N = sizeIntraWarps / 2; N > 0; N >>= 1) {\n-      Value shfl = shflSync(rewriter, loc, acc, N);\n+      Value shfl = shflSync(loc, rewriter, acc, N);\n       if (!withIndex) {\n         accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n       } else {\n-        Value shflIndex = shflSync(rewriter, loc, accIndex, N);\n+        Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n         accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n                             shflIndex, false);\n       }\n@@ -1718,8 +1674,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   barrier();\n \n   // the second round of shuffle reduction\n-  //   now the problem size: sizeInterWarps, s1, s2, .. , sn  =>\n-  //                                      1, s1, s2, .. , sn\n+  //   now the problem size: sizeInterWarps, s1, s2, .. , sn\n   //   where sizeInterWarps is 2^m\n   //\n   // each thread needs to process:\n@@ -1730,6 +1685,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   Value readOffset = threadId;\n   for (unsigned round = 0; round < elemsPerThread; ++round) {\n     Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+    // FIXME(Qingyi): need predicate icmp_slt(threadId, i32_val(sizeInerWarps))\n     Value acc = load(readPtr);\n     Value accIndex;\n     if (withIndex) {\n@@ -1738,17 +1694,18 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n     }\n \n     for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n-      Value shfl = shflSync(rewriter, loc, acc, N);\n+      Value shfl = shflSync(loc, rewriter, acc, N);\n       if (!withIndex) {\n         accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n       } else {\n-        Value shflIndex = shflSync(rewriter, loc, accIndex, N);\n+        Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n         accumulateWithIndex(rewriter, loc, op.redOp(), acc, accIndex, shfl,\n                             shflIndex, false);\n       }\n     }\n \n-    Value writeOffset = udiv(readOffset, i32_val(sizeInterWarps));\n+    // only the first thread in each sizeInterWarps is writing\n+    Value writeOffset = readOffset;\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     Value threadIsNeeded = icmp_slt(threadId, i32_val(elems));\n     Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));\n@@ -1775,22 +1732,17 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n     // nd-tensor where n >= 1\n     auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n-    SmallVector<unsigned> resultOrd;\n-    for (auto ord : order) {\n-      if (ord != 0)\n-        resultOrd.push_back(ord - 1);\n-    }\n-\n+    auto resultShape = resultTy.getShape();\n     unsigned resultElems = getElemsPerThread(resultTy);\n-    auto resultIndices =\n-        emitIndices(loc, rewriter, resultLayout, resultTy.getShape());\n+    auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n     assert(resultIndices.size() == resultElems);\n \n     SmallVector<Value> resultVals(resultElems);\n     for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n+      readIdx.insert(readIdx.begin() + axis, i32_val(0));\n       Value readOffset =\n-          linearize(rewriter, loc, readIdx, smemShapes[2], resultOrd);\n+          linearize(rewriter, loc, readIdx, smemShapes[0], order);\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       Value indexReadPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n       resultVals[i] = withIndex ? load(indexReadPtr) : load(readPtr);\n@@ -2090,7 +2042,7 @@ struct GetNumProgramsOpConversion\n     Location loc = op->getLoc();\n     assert(op.axis() < 3);\n \n-    Value blockId = rewriter.create<::mlir::gpu::BlockDimOp>(\n+    Value blockId = rewriter.create<::mlir::gpu::GridDimOp>(\n         loc, rewriter.getIndexType(), dims[op.axis()]);\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n     rewriter.replaceOpWithNewOp<UnrealizedConversionCastOp>("}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 50, "deletions": 13, "changes": 63, "file_content_changes": "@@ -113,10 +113,11 @@\n \n namespace mlir {\n namespace LLVM {\n+using namespace mlir::triton;\n \n-static Value getStructFromElements(Location loc, ValueRange resultVals,\n-                                   ConversionPatternRewriter &rewriter,\n-                                   Type structType) {\n+Value getStructFromElements(Location loc, ValueRange resultVals,\n+                            ConversionPatternRewriter &rewriter,\n+                            Type structType) {\n   if (!structType.isa<LLVM::LLVMStructType>()) {\n     return *resultVals.begin();\n   }\n@@ -130,9 +131,8 @@ static Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n-static SmallVector<Value>\n-getElementsFromStruct(Location loc, Value llvmStruct,\n-                      ConversionPatternRewriter &rewriter) {\n+SmallVector<Value> getElementsFromStruct(Location loc, Value llvmStruct,\n+                                         ConversionPatternRewriter &rewriter) {\n   if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n       llvmStruct.getType().isa<triton::PointerType>() ||\n       llvmStruct.getType().isa<LLVM::LLVMPointerType>())\n@@ -147,9 +147,6 @@ getElementsFromStruct(Location loc, Value llvmStruct,\n   return results;\n }\n \n-namespace {\n-using namespace mlir::triton;\n-\n // Create a 32-bit integer constant.\n Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n   auto i32ty = rewriter.getIntegerType(32);\n@@ -185,10 +182,8 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n                                           builder.getIntegerAttr(ty, value));\n }\n \n-} // namespace\n-\n /// Helper function to get strides from a given shape and its order\n-static SmallVector<Value>\n+SmallVector<Value>\n getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,\n                             Location loc, ConversionPatternRewriter &rewriter) {\n   auto rank = shape.size();\n@@ -264,7 +259,7 @@ struct SharedMemoryObject {\n   }\n };\n \n-static SharedMemoryObject\n+SharedMemoryObject\n getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n                                 ConversionPatternRewriter &rewriter) {\n   auto elems = getElementsFromStruct(loc, llvmStruct, rewriter);\n@@ -274,6 +269,48 @@ getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n           /*offsets=*/{elems.begin() + 1 + rank, elems.end()}};\n }\n \n+Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n+                  Value val, Value pred) {\n+  MLIRContext *ctx = rewriter.getContext();\n+  unsigned bits = val.getType().getIntOrFloatBitWidth();\n+  const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n+\n+  PTXBuilder builder;\n+  auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n+  auto *valOpr = builder.newOperand(val, c);\n+  auto &st = builder.create<>(\"st\")->shared().b(bits);\n+  st(ptrOpr, valOpr).predicate(pred, \"b\");\n+  return builder.launch(rewriter, loc, void_ty(ctx));\n+}\n+\n+Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+               int i) {\n+  unsigned bits = val.getType().getIntOrFloatBitWidth();\n+\n+  if (bits == 64) {\n+    Type vecTy = vec_ty(f32_ty, 2);\n+    Value vec = bitcast(val, vecTy);\n+    Value val0 = extract_element(f32_ty, vec, i32_val(0));\n+    Value val1 = extract_element(f32_ty, vec, i32_val(1));\n+    val0 = shflSync(loc, rewriter, val0, i);\n+    val1 = shflSync(loc, rewriter, val1, i);\n+    vec = undef(vecTy);\n+    vec = insert_element(vecTy, vec, val0, i32_val(0));\n+    vec = insert_element(vecTy, vec, val1, i32_val(1));\n+    return bitcast(vec, val.getType());\n+  }\n+\n+  PTXBuilder builder;\n+  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n+  auto *dOpr = builder.newOperand(\"=r\");\n+  auto *aOpr = builder.newOperand(val, \"r\");\n+  auto *bOpr = builder.newConstantOperand(i);\n+  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n+  auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n+  shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n+  return builder.launch(rewriter, loc, val.getType(), false);\n+}\n+\n } // namespace LLVM\n } // namespace mlir\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -187,6 +187,7 @@ void init_triton_ir(py::module &&m) {\n                /* issue a warning */\n              }\n            })\n+      .def(\"get_context\", &mlir::Value::getContext)\n       .def(\"replace_all_uses_with\",\n            [](mlir::Value &self, mlir::Value &newValue) {\n              self.replaceAllUsesWith(newValue);\n@@ -335,6 +336,16 @@ void init_triton_ir(py::module &&m) {\n         return funcs[0];\n       });\n \n+   m.def(\"make_attr\",\n+        [](const std::vector<int> &values, mlir::MLIRContext &context) {\n+          return mlir::DenseIntElementsAttr::get(\n+                     mlir::RankedTensorType::get(\n+                         {static_cast<int64_t>(values.size())},\n+                         mlir::IntegerType::get(&context, 32)),\n+                     values)\n+              .cast<mlir::Attribute>();\n+        });\n+\n   m.def(\n       \"parse_mlir_module\",\n       [](const std::string &inputFilename, mlir::MLIRContext &context) {"}, {"filename": "python/tests/test_blocksparse.py", "status": "added", "additions": 188, "deletions": 0, "changes": 188, "file_content_changes": "@@ -0,0 +1,188 @@\n+import pytest\n+import torch\n+\n+import triton\n+\n+# TODO: float32 fails\n+\n+@pytest.mark.parametrize(\"MODE\", [\"sdd\", \"dds\", \"dsd\"])\n+@pytest.mark.parametrize(\"TRANS_B\", [False, True])\n+@pytest.mark.parametrize(\"TRANS_A\", [False, True])\n+@pytest.mark.parametrize(\"BLOCK\", [16, 32, 64])\n+@pytest.mark.parametrize(\"DTYPE\", [torch.float16])\n+def test_matmul(MODE, TRANS_A, TRANS_B, BLOCK, DTYPE, Z=3, H=2, M=512, N=256, K=384):\n+    seed = 0\n+    torch.manual_seed(seed)\n+    is_sdd = MODE == \"sdd\"\n+    is_dsd = MODE == \"dsd\"\n+    is_dds = MODE == \"dds\"\n+    do_sparsify = lambda x: triton.testing.sparsify_tensor(x, layout, BLOCK)\n+    do_mask = lambda x: triton.testing.mask_tensor(x, layout, BLOCK)\n+    # create inputs\n+    # create op\n+    a_shape = (Z, H, K, M) if TRANS_A else (Z, H, M, K)\n+    b_shape = (Z, H, N, K) if TRANS_B else (Z, H, K, N)\n+    c_shape = (Z, H, M, N)\n+    shape = {\n+        \"sdd\": (M, N),\n+        \"dsd\": (a_shape[2], a_shape[3]),\n+        \"dds\": (b_shape[2], b_shape[3]),\n+    }[MODE]\n+    layout = torch.randint(2, (H, shape[0] // BLOCK, shape[1] // BLOCK))\n+    layout[1, 2, :] = 0\n+    layout[1, :, 1] = 0\n+    # create data\n+    a_ref, a_tri = triton.testing.make_pair(a_shape, alpha=.1, dtype=DTYPE)\n+    b_ref, b_tri = triton.testing.make_pair(b_shape, alpha=.1, dtype=DTYPE)\n+    dc_ref, dc_tri = triton.testing.make_pair(c_shape, dtype=DTYPE)\n+    # compute [torch]\n+    dc_ref = do_mask(dc_ref) if is_sdd else dc_ref\n+    a_ref = do_mask(a_ref) if is_dsd else a_ref\n+    b_ref = do_mask(b_ref) if is_dds else b_ref\n+    a_ref.requires_grad_().retain_grad()\n+    b_ref.requires_grad_().retain_grad()\n+    c_ref = torch.matmul(a_ref.transpose(2, 3) if TRANS_A else a_ref,\n+                         b_ref.transpose(2, 3) if TRANS_B else b_ref)\n+    c_ref.backward(dc_ref)\n+    c_ref = do_sparsify(c_ref) if is_sdd else c_ref\n+    da_ref = do_sparsify(a_ref.grad) if is_dsd else a_ref.grad\n+    db_ref = do_sparsify(b_ref.grad) if is_dds else b_ref.grad\n+    # triton result\n+    dc_tri = do_sparsify(dc_tri) if is_sdd else dc_tri\n+    a_tri = do_sparsify(a_tri) if is_dsd else a_tri\n+    b_tri = do_sparsify(b_tri) if is_dds else b_tri\n+    a_tri.requires_grad_().retain_grad()\n+    b_tri.requires_grad_().retain_grad()\n+    op = triton.ops.blocksparse.matmul(layout, BLOCK, MODE, trans_a=TRANS_A, trans_b=TRANS_B, device=\"cuda\")\n+    c_tri = triton.testing.catch_oor(lambda: op(a_tri, b_tri), pytest)\n+    triton.testing.catch_oor(lambda: c_tri.backward(dc_tri), pytest)\n+    da_tri = a_tri.grad\n+    db_tri = b_tri.grad\n+    # compare\n+    triton.testing.assert_almost_equal(c_ref, c_tri)\n+    triton.testing.assert_almost_equal(da_ref, da_tri)\n+    triton.testing.assert_almost_equal(db_ref, db_tri)\n+\n+\n+configs = [\n+    (16, 256),\n+    (32, 576),\n+    (64, 1871),\n+    (128, 2511),\n+]\n+\n+\n+@pytest.mark.parametrize(\"is_dense\", [False, True])\n+@pytest.mark.parametrize(\"BLOCK, WIDTH\", configs)\n+def test_softmax(BLOCK, WIDTH, is_dense, Z=2, H=2, is_causal=True, scale=0.4):\n+    # set seed\n+    torch.random.manual_seed(0)\n+    Z, H, M, N = 2, 3, WIDTH, WIDTH\n+    # initialize layout\n+    # make sure each row has at least one non-zero element\n+    layout = torch.randint(2, (H, M // BLOCK, N // BLOCK))\n+    if is_dense:\n+        layout[:] = 1\n+    else:\n+        layout[1, 2, :] = 0\n+        layout[1, :, 1] = 0\n+    # initialize data\n+    a_shape = (Z, H, M, N)\n+    a_ref, a_tri = triton.testing.make_pair(a_shape)\n+    dout_ref, dout_tri = triton.testing.make_pair(a_shape)\n+    # compute [torch]\n+    a_ref = triton.testing.mask_tensor(a_ref, layout, BLOCK, value=float(\"-inf\"))\n+    a_ref.retain_grad()\n+    at_mask = torch.ones((M, N), device=\"cuda\")\n+    if is_causal:\n+        at_mask = torch.tril(at_mask)\n+    M = at_mask[None, None, :, :] + torch.zeros_like(a_ref)\n+    a_ref[M == 0] = float(\"-inf\")\n+    out_ref = torch.softmax(a_ref * scale, -1)\n+    out_ref.backward(dout_ref)\n+    out_ref = triton.testing.sparsify_tensor(out_ref, layout, BLOCK)\n+    da_ref = triton.testing.sparsify_tensor(a_ref.grad, layout, BLOCK)\n+    # compute [triton]\n+    a_tri = triton.testing.sparsify_tensor(a_tri, layout, BLOCK)\n+    a_tri.retain_grad()\n+    dout_tri = triton.testing.sparsify_tensor(dout_tri, layout, BLOCK)\n+    op = triton.ops.blocksparse.softmax(layout, BLOCK, device=\"cuda\", is_dense=is_dense)\n+    out_tri = op(a_tri, scale=scale, is_causal=is_causal)\n+    out_tri.backward(dout_tri)\n+    da_tri = a_tri.grad\n+    # compare\n+    triton.testing.assert_almost_equal(out_tri, out_ref)\n+    triton.testing.assert_almost_equal(da_tri, da_ref)\n+\n+\n+@pytest.mark.parametrize(\"block\", [16, 32, 64])\n+@pytest.mark.parametrize(\"dtype\", [torch.float16])\n+def test_attention_fwd_bwd(\n+    block,\n+    dtype,\n+    input_scale=1.0,\n+    scale=1 / 8.0,\n+    n_ctx=256,\n+    batch_size=2,\n+    n_heads=2,\n+):\n+    # inputs\n+    qkv_shape = (batch_size, n_heads, n_ctx, 64)\n+    qkvs = [\n+        torch.nn.Parameter(input_scale * torch.randn(qkv_shape), requires_grad=True).to(dtype).cuda() for _ in range(3)\n+    ]\n+\n+    # Triton:\n+    n_blocks = n_ctx // block\n+    layout = torch.tril(torch.ones([n_heads, n_blocks, n_blocks], dtype=torch.long))\n+    query, key, value = [x.clone() for x in qkvs]\n+    query.retain_grad()\n+    key.retain_grad()\n+    value.retain_grad()\n+    attn_out = triton_attention(layout, block, query=query, key=key, value=value, scale=scale)\n+    # ad hoc loss\n+    loss = (attn_out ** 2).mean()\n+    loss.backward()\n+    grads = [query.grad, key.grad, value.grad]\n+\n+    # Torch version:\n+    torch_q, torch_k, torch_v = [x.clone() for x in qkvs]\n+    attn_mask = torch.ones([n_ctx, n_ctx], device=\"cuda\", dtype=dtype)\n+    attn_mask = torch.tril(attn_mask, diagonal=0)\n+    attn_mask = 1e6 * (-1 + (attn_mask.reshape((1, 1, n_ctx, n_ctx)).cuda()))\n+    torch_q.retain_grad()\n+    torch_k.retain_grad()\n+    torch_v.retain_grad()\n+    scores = scale * torch.einsum(\"bhsd,bhtd->bhst\", torch_q, torch_k)\n+    scores = scores + attn_mask\n+    probs = torch.softmax(scores, dim=-1)\n+    torch_attn_out = torch.einsum(\"bhst,bhtd->bhsd\", probs, torch_v)\n+    # ad hoc loss\n+    torch_loss = (torch_attn_out ** 2).mean()\n+    torch_loss.backward()\n+    torch_grads = [torch_q.grad, torch_k.grad, torch_v.grad]\n+\n+    # comparison\n+    # print(f\"Triton loss {loss} and torch loss {torch_loss}.  Also checking grads...\")\n+    triton.testing.assert_almost_equal(loss, torch_loss)\n+    for g1, g2 in zip(grads, torch_grads):\n+        triton.testing.assert_almost_equal(g1, g2)\n+\n+\n+@pytest.mark.parametrize(\"block\", [16, 32, 64])\n+def triton_attention(\n+    layout,\n+    block: int,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    scale: float,\n+):\n+    sparse_dot_sdd_nt = triton.ops.blocksparse.matmul(layout, block, \"sdd\", trans_a=False, trans_b=True, device=value.device)\n+    sparse_dot_dsd_nn = triton.ops.blocksparse.matmul(layout, block, \"dsd\", trans_a=False, trans_b=False, device=value.device)\n+    sparse_softmax = triton.ops.blocksparse.softmax(layout, block, device=value.device)\n+\n+    w = sparse_dot_sdd_nt(query, key)\n+    w = sparse_softmax(w, scale=scale, is_causal=True)\n+    a = sparse_dot_dsd_nn(w, value)\n+    return a"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -1515,7 +1515,7 @@ def _generate_src(self):\n            }\n         }\n \n-        #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n+        #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); if(PyErr_Occurred()) return NULL; }\n \n         static PyObject* loadBinary(PyObject* self, PyObject* args) {\n             const char* name;\n@@ -1530,7 +1530,6 @@ def _generate_src(self):\n             CUmodule mod;\n             int32_t n_regs = 0;\n             int32_t n_spills = 0;\n-            Py_BEGIN_ALLOW_THREADS;\n             // create driver handles\n             CUDA_CHECK(cuModuleLoadData(&mod, data));\n             CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n@@ -1548,7 +1547,6 @@ def _generate_src(self):\n               CUDA_CHECK(cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n               CUDA_CHECK(cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static));\n             }\n-            Py_END_ALLOW_THREADS;\n \n             if(PyErr_Occurred()) {\n               return NULL;"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -1117,16 +1117,16 @@ def sqrt(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n ##\n \n def multiple_of(x: tl.tensor, values: List[int]) -> tl.tensor:\n-    if len(x.shape) != len(values):\n-        raise ValueError(\"Shape of input to multiple_of does not match the length of values\")\n-    x.handle.multiple_of(values)\n-    return x\n-\n-\n+     if len(x.shape) != len(values):\n+         raise ValueError(\"Shape of input to multiple_of does not match the length of values\")\n+     x.handle.set_attr(\"tt.divisibility\", ir.make_attr(values, x.handle.get_context()))\n+     return x\n+ \n+ \n def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n     if len(x.shape) != len(values):\n         raise ValueError(\"Shape of input to max_contiguous does not match the length of values\")\n-    x.handle.max_contiguous(values)\n+    x.handle.set_attr(\"tt.contiguity\", ir.make_attr(values, x.handle.get_context()))\n     return x\n \n "}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -34,12 +34,12 @@ def sparsify_tensor(x, mask, block):\n     return ret\n \n \n-def make_pair(shape, device=\"cuda\", alpha=1e-2, beta=0., trans=False, data=None):\n+def make_pair(shape, device=\"cuda\", alpha=1e-2, beta=0., trans=False, data=None, dtype=torch.float32):\n     if data is None:\n-        data = torch.randn(shape, dtype=torch.float32, device=device)\n+        data = torch.randn(shape, dtype=torch.float32, requires_grad=True, device=device)\n     ref_ret = data\n     ref_ret = ref_ret * alpha + beta\n-    ref_ret = ref_ret.half().float()\n+    ref_ret = ref_ret.half().to(dtype)\n     if trans:\n         ref_ret = ref_ret.t().requires_grad_()\n     ref_ret = ref_ret.detach().requires_grad_()"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -956,9 +956,9 @@ func @test_get_program_id(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n func @test_get_num_program(%a: tensor<32x!tt.ptr<i32>, #blocked0>) {\n-  // CHECK: nvvm.read.ptx.sreg.ntid.x\n-  // CHECK: nvvm.read.ptx.sreg.ntid.y\n-  // CHECK: nvvm.read.ptx.sreg.ntid.z\n+  // CHECK: nvvm.read.ptx.sreg.nctaid.x\n+  // CHECK: nvvm.read.ptx.sreg.nctaid.y\n+  // CHECK: nvvm.read.ptx.sreg.nctaid.z\n   %blockdimx = tt.get_num_programs {axis=0:i32} : i32\n   %blockdimy = tt.get_num_programs {axis=1:i32} : i32\n   %blockdimz = tt.get_num_programs {axis=2:i32} : i32"}]