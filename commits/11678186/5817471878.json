[{"filename": "python/test/unit/hopper/test_persistent_warp_specialized_fused-attention.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -297,9 +297,6 @@ def backward(ctx, do):\n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n-    # with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n-    pytest.skip('unspecified launch failure')\n-\n     torch.manual_seed(20)\n     q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n     k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 77, "deletions": 112, "changes": 189, "file_content_changes": "@@ -114,17 +114,21 @@ def static_persistent_tma_matmul_kernel(\n         pre_pid_n = pid_n\n \n \n-@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,TRANS_A,TRANS_B', [\n-    [4096, 4096, 64, 64, 64, 16, 4, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 32, 4, 1, False, True],\n-    [4096, 4096, 64, 256, 64, 16, 4, 1, False, True],\n-    [4096, 4096, 64, 128, 128, 16, 4, 1, False, True],\n-    # TODO: fix issue for 8-warp persistent kernel\n-    # [4096, 4096, 64, 128, 128, 16, 8, 1, False, True],\n-    # [4096, 4096, 64, 128, 256, 16, 8, 1, False, True],\n-])\n+@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,TRANS_A,TRANS_B,KERN',\n+                         [(*shape, kern)\n+                          for shape in [\n+                             [4096, 4096, 64, 64, 64, 16, 4, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 32, 4, 1, False, True],\n+                             [4096, 4096, 64, 256, 64, 16, 4, 1, False, True],\n+                             [4096, 4096, 64, 128, 128, 16, 4, 1, False, True],\n+                             # TODO: fix issue for 8-warp persistent kernel\n+                             # [4096, 4096, 64, 128, 128, 16, 8, 1, False, True],\n+                             # [4096, 4096, 64, 128, 256, 16, 8, 1, False, True],\n+                         ]\n+                             for kern in ['vintage', 'stylish']\n+                         ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_user_defined_persistent_non_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, TRANS_A, TRANS_B):\n+def test_user_defined_persistent_non_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, TRANS_A, TRANS_B, KERN):\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -139,26 +143,13 @@ def test_user_defined_persistent_non_warp_specialized_gemm(M, N, K, BLOCK_M, BLO\n     num_SMs = torch.cuda.get_device_properties('cuda').multi_processor_count\n     grid = lambda META: (num_SMs,)\n \n-    def call_vintage():\n+    if KERN == 'vintage':\n         static_persistent_matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c, M=M, N=N, K=K, stride_am=a.stride(0), stride_ak=a.stride(1), stride_bk=b.stride(0), stride_bn=b.stride(1), stride_cm=c.stride(0), stride_cn=c.stride(1), BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K, NUM_SM=num_SMs, num_warps=NUM_WARPS, num_ctas=NUM_CTAS)\n-        return c\n-\n-    def call_stylish():\n+    elif KERN == 'stylish':\n         static_persistent_tma_matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c, M=M, N=N, K=K, stride_am=a.stride(0), stride_ak=a.stride(1), stride_bk=b.stride(0), stride_bn=b.stride(1), stride_cm=c.stride(0), stride_cn=c.stride(1), BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K, NUM_SM=num_SMs, num_warps=NUM_WARPS, num_ctas=NUM_CTAS)\n-        return c\n \n     th_c = torch.matmul(a, b)\n-\n-    # Test using old style of ptr calculation\n-    tt_c = call_vintage()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # Cealr c\n-    c = torch.randn((M, N), device=a.device, dtype=torch.float32)\n-\n-    # Test using make_block_ptr\n-    tt_c = call_stylish()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n+    torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n \n \n @triton.jit\n@@ -240,40 +231,37 @@ def tma_warp_specialized_matmul_kernel(\n     tl.store(c_ptrs, accumulator, mask=mask)\n \n \n-@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B', [\n-    [2048, 2048, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 16, 1, False, True],\n-    [128, 4096, 64, 64, 64, 16, 1, False, True],\n-    [4096, 128, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 32, 1, False, True],\n-    [4096, 4096, 256, 128, 128, 16, 1, False, True],\n-    [4096, 4096, 320, 128, 64, 64, 1, False, True],\n-    [4096, 4096, 320, 64, 128, 64, 1, False, True],\n-    [4096, 4096, 320, 128, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 64, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 64, 1, False, True],\n-    # numCTAs > 1\n-    [2048, 2048, 64, 128, 128, 64, 2, False, True],\n-    [2048, 2048, 64, 128, 128, 64, 2, False, True],\n-    [2048, 2048, 128, 256, 128, 64, 4, False, True],\n-    [4096, 4096, 128, 256, 128, 64, 4, False, True],\n-    [4096, 4096, 256, 128, 256, 64, 4, False, True],\n-    [4096, 4096, 256, 256, 256, 64, 4, False, True],\n-])\n+@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B,KERN',\n+                         [(*shape, kern)\n+                          for shape in [\n+                             [2048, 2048, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [128, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 128, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 32, 1, False, True],\n+                             [4096, 4096, 256, 128, 128, 16, 1, False, True],\n+                             [4096, 4096, 320, 128, 64, 64, 1, False, True],\n+                             [4096, 4096, 320, 64, 128, 64, 1, False, True],\n+                             [4096, 4096, 320, 128, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 64, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 1, False, True],\n+                             # numCTAs > 1\n+                             [2048, 2048, 64, 128, 128, 64, 2, False, True],\n+                             [2048, 2048, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 4, False, True],\n+                             [4096, 4096, 256, 256, 256, 64, 4, False, True],\n+                         ]\n+                             for kern in ['vintage', 'stylish']\n+                         ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B):\n-    if '-'.join(map(str, [M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B])) in [\n-        '4096-4096-256-128-256-16-1-False-True',\n-        '4096-4096-256-128-256-64-1-False-True'\n-    ]:\n-        pytest.skip('Insufficient register resources')\n-\n+def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B, KERN):\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -288,7 +276,7 @@ def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K\n \n     grid = lambda META: (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n \n-    def call_vintage():\n+    if KERN == 'vintage':\n         warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n@@ -299,9 +287,7 @@ def call_vintage():\n             num_warps=4,\n             num_ctas=NUM_CTAS,\n             enable_warp_specialization=True)\n-        return c\n-\n-    def call_stylish():\n+    elif KERN == 'stylish':\n         tma_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n@@ -312,20 +298,9 @@ def call_stylish():\n             num_warps=4,\n             num_ctas=NUM_CTAS,\n             enable_warp_specialization=True)\n-        return c\n \n     th_c = torch.matmul(a, b)\n-\n-    # Test using old style of ptr calculation\n-    tt_c = call_vintage()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # Cealr c\n-    c = torch.randn((M, N), device=a.device, dtype=torch.float32)\n-\n-    # # Test using make_block_ptr\n-    tt_c = call_stylish()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n+    torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n \n     # # #############################################Performance Evaluation#############################################\n     # fn = lambda: call_vintage()\n@@ -434,27 +409,31 @@ def static_persistent_tma_warp_specialized_matmul_kernel(\n         pre_pid_n = pid_n\n \n \n-@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B', [\n-    [2048, 2048, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 16, 1, False, True],\n-    [128, 4096, 64, 64, 64, 16, 1, False, True],\n-    [4096, 128, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 32, 1, False, True],\n-    [4096, 4096, 256, 128, 128, 16, 1, False, True],\n-    [4096, 4096, 320, 128, 64, 64, 1, False, True],\n-    [4096, 4096, 320, 64, 128, 64, 1, False, True],\n-    [4096, 4096, 320, 128, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 64, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 64, 1, False, True],\n-])\n+@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B,KERN',\n+                         [(*shape, kern)\n+                          for shape in [\n+                             [2048, 2048, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [128, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 128, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 32, 1, False, True],\n+                             [4096, 4096, 256, 128, 128, 16, 1, False, True],\n+                             [4096, 4096, 320, 128, 64, 64, 1, False, True],\n+                             [4096, 4096, 320, 64, 128, 64, 1, False, True],\n+                             [4096, 4096, 320, 128, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 64, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 1, False, True],\n+                         ]\n+                             for kern in ['vintage', 'stylish']\n+                         ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B):\n+def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B, KERN):\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -469,39 +448,25 @@ def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N\n     num_SMs = torch.cuda.get_device_properties('cuda').multi_processor_count\n     grid = lambda META: (num_SMs,)\n \n-    def call_vintage():\n+    if KERN == 'vintage':\n         static_persistent_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n             BLOCK_M, BLOCK_N, BLOCK_K, num_SMs)\n-        return c\n-\n-    def call_stylish():\n+    elif KERN == 'stylish':\n         static_persistent_tma_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n             BLOCK_M, BLOCK_N, BLOCK_K, num_SMs)\n-        return c\n \n     th_c = torch.matmul(a, b)\n-\n-    # Test using old style of ptr calculation\n-    tt_c = call_vintage()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # Cealr c\n-    c = torch.randn((M, N), device=a.device, dtype=torch.float32)\n-\n-    # Test using make_block_ptr\n-    tt_c = call_stylish()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n+    torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n     # #############################################Performance Evaluation#############################################\n     # fn = lambda: call_stylish()\n     # ms = triton.testing.do_bench(fn, warmup=25, rep=100)"}]