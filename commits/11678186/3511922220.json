[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -74,11 +74,9 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n   let builders = [\n     AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n                      \"ArrayRef<int64_t>\":$shape,\n+                     \"ArrayRef<unsigned>\":$order,\n                      \"Type\":$eltTy), [{\n         auto mmaEnc = dotOpEnc.getParent().dyn_cast<MmaEncodingAttr>();\n-        // Only support row major for now\n-        // TODO(Keren): check why column major code crashes\n-        SmallVector<unsigned> order = {1, 0};\n \n         if(!mmaEnc)\n           return $_get(context, 1, 1, 1, order);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 28, "deletions": 23, "changes": 51, "file_content_changes": "@@ -464,21 +464,6 @@ struct SharedMemoryObject {\n     }\n   }\n \n-  // XXX(Keren): a special allocator for 3d tensors. It's a workaround for\n-  // now since we don't have a correct way to encoding 3d tensors in the\n-  // pipeline pass.\n-  SharedMemoryObject(Value base, ArrayRef<int64_t> shape, Location loc,\n-                     ConversionPatternRewriter &rewriter)\n-      : base(base) {\n-    auto stride = 1;\n-    for (auto dim : llvm::reverse(shape)) {\n-      strides.emplace_back(i32_val(stride));\n-      offsets.emplace_back(i32_val(0));\n-      stride *= dim;\n-    }\n-    strides = llvm::to_vector<4>(llvm::reverse(strides));\n-  }\n-\n   SmallVector<Value> getElems() const {\n     SmallVector<Value> elems;\n     elems.push_back(base);\n@@ -1453,9 +1438,10 @@ struct BroadcastOpConversion\n     SmallVector<int64_t> resultLogicalShape(2 * rank);\n     SmallVector<unsigned> broadcastDims;\n     for (unsigned d = 0; d < rank; ++d) {\n-      unsigned resultShapePerCTA = triton::gpu::getSizePerThread(resultLayout)[d] *\n-                                   triton::gpu::getThreadsPerWarp(resultLayout)[d] *\n-                                   triton::gpu::getWarpsPerCTA(resultLayout)[d];\n+      unsigned resultShapePerCTA =\n+          triton::gpu::getSizePerThread(resultLayout)[d] *\n+          triton::gpu::getThreadsPerWarp(resultLayout)[d] *\n+          triton::gpu::getWarpsPerCTA(resultLayout)[d];\n       int64_t numCtas = ceil<unsigned>(resultShape[d], resultShapePerCTA);\n       if (srcShape[d] != resultShape[d]) {\n         assert(srcShape[d] == 1);\n@@ -1465,10 +1451,12 @@ struct BroadcastOpConversion\n             std::max<unsigned>(1, triton::gpu::getSizePerThread(srcLayout)[d]);\n       } else {\n         srcLogicalShape[d] = numCtas;\n-        srcLogicalShape[d + rank] = triton::gpu::getSizePerThread(resultLayout)[d];\n+        srcLogicalShape[d + rank] =\n+            triton::gpu::getSizePerThread(resultLayout)[d];\n       }\n       resultLogicalShape[d] = numCtas;\n-      resultLogicalShape[d + rank] = triton::gpu::getSizePerThread(resultLayout)[d];\n+      resultLogicalShape[d + rank] =\n+          triton::gpu::getSizePerThread(resultLayout)[d];\n \n       srcLogicalOrder[d] = order[d] + rank;\n       srcLogicalOrder[d + rank] = order[d];\n@@ -1983,6 +1971,7 @@ struct PrintfOpConversion\n       return \"%u\";\n     }\n     assert(false && \"not supported type\");\n+    return \"\";\n   }\n \n   // declare vprintf(i8*, i8*) as external function\n@@ -2251,8 +2240,18 @@ struct AllocTensorOpConversion\n         getTypeConverter()->convertType(resultTy.getElementType());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n     smemBase = bitcast(smemBase, elemPtrTy);\n+    auto order = resultTy.getEncoding().cast<SharedEncodingAttr>().getOrder();\n+    // workaround for 3D tensors\n+    // TODO: We need to modify the pipeline pass to give a proper shared encoding to 3D tensors\n+    SmallVector<unsigned> newOrder;\n+    if (resultTy.getShape().size() == 3) \n+      newOrder = {1 + order[0], 1 + order[1], 0};\n+    else\n+      newOrder = SmallVector<unsigned>(order.begin(), order.end());\n+\n+    \n     auto smemObj =\n-        SharedMemoryObject(smemBase, resultTy.getShape(), loc, rewriter);\n+        SharedMemoryObject(smemBase, resultTy.getShape(), newOrder, loc, rewriter);\n     auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n     rewriter.replaceOp(op, retVal);\n     return success();\n@@ -2302,6 +2301,10 @@ struct ExtractSliceOpConversion\n         strideVals.emplace_back(smemObj.strides[i]);\n       }\n     }\n+\n+    // llvm::outs() << \"extract slice\\n\";\n+    // llvm::outs() << strideVals[0] << \" \" << smemObj.strides[1] << \"\\n\";\n+    // llvm::outs() << strideVals[1] << \" \" << smemObj.strides[2] << \"\\n\";\n     auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n     auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n     auto resTy = op.getType().dyn_cast<RankedTensorType>();\n@@ -3262,8 +3265,8 @@ class MMA16816SmemLoader {\n     cMatShape = matShape[order[0]];\n     sMatShape = matShape[order[1]];\n \n-    cStride = smemStrides[1];\n-    sStride = smemStrides[0];\n+    cStride = smemStrides[order[0]];\n+    sStride = smemStrides[order[1]];\n \n     // rule: k must be the fast-changing axis.\n     needTrans = kOrder != order[0];\n@@ -5483,6 +5486,7 @@ Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n   }\n \n   assert(false && \"Unsupported mma layout found\");\n+  return {};\n }\n \n class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n@@ -6202,6 +6206,7 @@ class ConvertTritonGPUToLLVM\n             dstType.getShape(), dstType.getElementType(),\n             triton::gpu::SharedEncodingAttr::get(mod.getContext(), dstDotOp,\n                                                  srcType.getShape(),\n+                                                 getOrder(srcBlocked),\n                                                  srcType.getElementType()));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -201,7 +201,9 @@ LogicalResult LoopPipeliner::initialize() {\n                                              ty.getShape().end());\n             bufferShape.insert(bufferShape.begin(), numStages);\n             auto sharedEnc = ttg::SharedEncodingAttr::get(\n-                ty.getContext(), dotOpEnc, ty.getShape(), ty.getElementType());\n+                ty.getContext(), dotOpEnc, ty.getShape(),\n+                triton::gpu::getOrder(ty.getEncoding()),\n+                ty.getElementType());\n             loadsBufferType[loadOp] = RankedTensorType::get(\n                 bufferShape, ty.getElementType(), sharedEnc);\n           }"}, {"filename": "python/setup.py", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -6,7 +6,6 @@\n import subprocess\n import sys\n import tarfile\n-import tempfile\n import urllib.request\n from distutils.version import LooseVersion\n from typing import NamedTuple\n@@ -26,7 +25,9 @@ def get_build_type():\n     elif check_env_flag(\"REL_WITH_DEB_INFO\"):\n         return \"RelWithDebInfo\"\n     else:\n-        return \"Release\"\n+        return \"Debug\"\n+        # TODO(Keren): Restore this before we merge into master\n+        #return \"Release\"\n \n \n # --- third party packages -----\n@@ -124,19 +125,14 @@ def run(self):\n             self.build_extension(ext)\n \n     def build_extension(self, ext):\n-        self.debug = True\n         lit_dir = shutil.which('lit')\n         triton_cache_path = os.path.join(os.environ[\"HOME\"], \".triton\")\n         # lit is used by the test suite\n         thirdparty_cmake_args = get_thirdparty_packages(triton_cache_path)\n         extdir = os.path.abspath(os.path.dirname(self.get_ext_fullpath(ext.path)))\n         # create build directories\n-        build_suffix = 'debug' if self.debug else 'release'\n-        llvm_build_dir = os.path.join(tempfile.gettempdir(), \"llvm-\" + build_suffix)\n         if not os.path.exists(self.build_temp):\n             os.makedirs(self.build_temp)\n-        if not os.path.exists(llvm_build_dir):\n-            os.makedirs(llvm_build_dir)\n         # python directories\n         python_include_dir = distutils.sysconfig.get_python_inc()\n         cmake_args = [\n@@ -145,13 +141,13 @@ def build_extension(self, ext):\n             \"-DTRITON_BUILD_TUTORIALS=OFF\",\n             \"-DTRITON_BUILD_PYTHON_MODULE=ON\",\n             # '-DPYTHON_EXECUTABLE=' + sys.executable,\n-            # '-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON',\n+            '-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON',\n             \"-DPYTHON_INCLUDE_DIRS=\" + python_include_dir,\n             \"-DLLVM_EXTERNAL_LIT=\" + lit_dir\n         ] + thirdparty_cmake_args\n \n         # configuration\n-        cfg = \"Debug\" if self.debug else \"Release\"\n+        cfg = get_build_type()\n         build_args = [\"--config\", cfg]\n \n         if platform.system() == \"Windows\":\n@@ -183,7 +179,11 @@ def build_extension(self, ext):\n         \"torch\",\n         \"lit\",\n     ],\n-    package_data={\"triton/ops\": [\"*.c\"], \"triton/ops/blocksparse\": [\"*.c\"]},\n+    package_data={\n+        \"triton/ops\": [\"*.c\"],\n+        \"triton/ops/blocksparse\": [\"*.c\"],\n+        \"triton/language\": [\"*.bc\"]\n+    },\n     include_package_data=True,\n     ext_modules=[CMakeExtension(\"triton\", \"triton/_C/\")],\n     cmdclass={\"build_ext\": CMakeBuild},"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -551,7 +551,7 @@ void init_triton_ir(py::module &&m) {\n                return llvm::dyn_cast<mlir::FuncOp>(funcOperation);\n              auto loc = self.getUnknownLoc();\n              if (auto funcTy = funcType.dyn_cast<mlir::FunctionType>()) {\n-               mlir::ArrayRef<mlir::NamedAttribute> attrs = {\n+               llvm::SmallVector<mlir::NamedAttribute> attrs = {\n                    mlir::NamedAttribute(self.getStringAttr(\"sym_visibility\"),\n                                         self.getStringAttr(visibility))};\n                return self.create<mlir::FuncOp>(loc, funcName, funcTy, attrs);"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -186,9 +186,9 @@ def get_proper_err(a, b, golden):\n     [128, 256, 128, 4, 128, 256, 32, False, False],\n     [256, 128, 64, 4, 256, 128, 16, False, False],\n     [128, 64, 128, 4, 128, 64, 32, False, False],\n-    # TODO[goostavz]: fix these cases\n-    #[128, 64, 128, 4, 128, 64, 32, True, False],\n-    #[128, 64, 128, 4, 128, 64, 32, False, True],\n+    # trans\n+    [128, 64, 128, 4, 128, 64, 32, True, False],\n+    [128, 64, 128, 4, 128, 64, 32, False, True],\n ])\n def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n     if (TRANS_A):"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -882,14 +882,14 @@ def ttir_to_ttgir(mod, num_warps, num_stages):\n     pm.enable_debug()\n     # Convert blocked layout to mma layout for dot ops so that pipeline\n     # can get shared memory swizzled correctly.\n+    pm.add_coalesce_pass()\n     pm.add_triton_gpu_combine_pass()\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     # Prefetch must be done after pipeline pass because pipeline pass\n     # extracts slices from the original tensor.\n     pm.add_tritongpu_prefetch_pass()\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n-    pm.add_coalesce_pass()\n     pm.add_triton_gpu_combine_pass()\n     pm.add_licm_pass()\n     pm.add_triton_gpu_combine_pass()"}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -34,7 +34,7 @@ TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n \n   // create element type\n   Type eltType = IntegerType::get(&ctx, params.typeWidth);\n-  auto layout = SharedEncodingAttr::get(&ctx, encoding, params.shape, eltType);\n+  auto layout = SharedEncodingAttr::get(&ctx, encoding, params.shape, {1, 0}, eltType);\n \n   ASSERT_EQ(layout.getVec(), params.refSwizzle.vec);\n   ASSERT_EQ(layout.getPerPhase(), params.refSwizzle.perPhase);"}]