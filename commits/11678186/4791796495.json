[{"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -87,6 +87,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n+    if (computeCapability < 70)\n+      return failure();\n     auto dotOp = cast<triton::DotOp>(op);\n     // TODO: Check data-types and SM compatibility\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -3,6 +3,9 @@\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n+import torch\n+\n+import triton\n from . import core as tl\n from triton._C.libtriton.triton import ir\n \n@@ -1180,6 +1183,14 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n+    if torch.version.hip is None:\n+        device = triton.runtime.jit.get_current_device()\n+        capability = triton.runtime.jit.get_device_capability(device)\n+        capability = capability[0] * 10 + capability[1]\n+        if capability < 70:\n+            assert (\n+                not rhs.dtype.is_fp16() and not rhs.dtype.is_fp8()\n+            ), \"Float8 and Float16 types are not supported for compute capability < 70 (use Float32 or above)\"\n     assert lhs.type.is_block() and rhs.type.is_block()\n     assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n     assert len(lhs.shape) == 2 and len(rhs.shape) == 2"}]