[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -244,6 +244,7 @@ jobs:\n             mv cache reference\n           else\n             echo \"No artifact found with the name: $ARTIFACT_NAME\"\n+            exit 1\n           fi\n       - name: Download current job artifacts\n         uses: actions/download-artifact@v2\n@@ -294,6 +295,7 @@ jobs:\n         if: ${{ env.COMPARISON_RESULT == 'false' }}\n         uses: actions/github-script@v5\n         with:\n+          github-token: ${{ secrets.CI_ACCESS_TOKEN }}\n           script: |\n             const run_id = ${{ env.RUN_ID }};\n             const issue_number = context.payload.pull_request.number;"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -111,6 +111,16 @@ Reduction Ops\n     sum\n     xor_sum\n \n+Scan Ops\n+-------------\n+\n+.. autosummary::\n+    :toctree: generated\n+    :nosignatures:\n+\n+    associative_scan\n+    cumsum\n+    cumprod\n \n Atomic Ops\n ----------"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Attributes.h", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -0,0 +1,7 @@\n+#ifndef TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_\n+#define TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_\n+\n+#define GET_ATTRDEF_CLASSES\n+#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n+\n+#endif // TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -8,12 +8,10 @@\n \n // TritonGPU depends on Triton\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n \n-#define GET_ATTRDEF_CLASSES\n-#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n-\n #define GET_OP_CLASSES\n #include \"triton/Dialect/TritonGPU/IR/Ops.h.inc\"\n \n@@ -82,9 +80,10 @@ bool isaDistributedLayout(Attribute layout);\n \n bool isSharedEncoding(Value value);\n \n+bool isExpensiveCat(CatOp cat, Attribute &targetEncoding);\n+\n } // namespace gpu\n } // namespace triton\n-\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -18,8 +18,6 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n \n bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n \n-bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n-\n bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n // skipInit is True when we only consider the operands of the initOp but"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 21, "deletions": 1, "changes": 22, "file_content_changes": "@@ -10,11 +10,31 @@\n \n namespace mlir {\n \n+namespace {\n+\n+int getParentAxis(Attribute layout, int axis) {\n+  if (auto sliceEncoding = layout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    axis = axis < sliceEncoding.getDim() ? axis : axis + 1;\n+    return getParentAxis(sliceEncoding.getParent(), axis);\n+  }\n+  return axis;\n+}\n+\n+SmallVector<unsigned> getParentOrder(Attribute layout) {\n+  if (auto sliceEncoding = layout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    return getParentOrder(sliceEncoding.getParent());\n+  }\n+  return triton::gpu::getOrder(layout);\n+}\n+\n+} // namespace\n+\n bool ReduceOpHelper::isFastReduction() {\n   // Disable fast reduction only for debugging purpose\n   if (::triton::tools::getBoolEnv(\"DISABLE_FAST_REDUCTION\"))\n     return false;\n-  return axis == triton::gpu::getOrder(getSrcLayout())[0];\n+  return getParentAxis(getSrcLayout(), axis) ==\n+         getParentOrder(getSrcLayout())[0];\n }\n \n unsigned ReduceOpHelper::getInterWarpSize() {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 12, "deletions": 11, "changes": 23, "file_content_changes": "@@ -89,13 +89,14 @@ struct ReduceOpConversion\n   void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n                           Attribute layout, SmallVector<Value> &index,\n                           SmallVector<Value> &writeIdx,\n-                          std::map<int, Value> &ints, unsigned axis) const {\n+                          std::map<int, Value> &ints, unsigned originalAxis,\n+                          unsigned axis) const {\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-      auto dim = sliceLayout.getDim();\n-      assert(dim != axis && \"Reduction axis cannot be sliced\");\n+      // Recover the axis in the parent layout\n+      auto parentAxis = axis < sliceLayout.getDim() ? axis : axis + 1;\n       auto parentLayout = sliceLayout.getParent();\n       getWriteIndexBasic(rewriter, loc, parentLayout, index, writeIdx, ints,\n-                         axis);\n+                         originalAxis, parentAxis);\n       return;\n     }\n \n@@ -110,21 +111,21 @@ struct ReduceOpConversion\n       // we would have a single accumulation every `axisSizePerThread`\n       // contiguous values in the original tensor, so we would need\n       // to map every `axisSizePerThread` to 1 value in smem as:\n-      // writeIdx[axis] = index[axis] / axisSizePerThread\n-      writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+      // writeIdx[originalAxis] = index[originalAxis] / axisSizePerThread\n+      writeIdx[originalAxis] = udiv(index[originalAxis], axisSizePerThread);\n     } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n       if (!mmaLayout.isAmpere()) {\n         llvm::report_fatal_error(\"Unsupported layout\");\n       }\n-      if (axis == 0) {\n+      if (originalAxis == 0) {\n         // Because warpTileSize = [16, 8] and threadsPerWarp = [8, 4], each 8\n         // rows in smem would correspond to a warp. The mapping\n         // is: (warp_index) x 8 + (row index within warp)\n-        writeIdx[axis] =\n-            add(mul(udiv(index[axis], _16), _8), urem(index[axis], _8));\n+        writeIdx[originalAxis] = add(mul(udiv(index[originalAxis], _16), _8),\n+                                     urem(index[originalAxis], _8));\n       } else {\n         // Same as BlockedEncodingAttr case\n-        writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+        writeIdx[originalAxis] = udiv(index[originalAxis], axisSizePerThread);\n       }\n     } else {\n       llvm::report_fatal_error(\"Unsupported layout\");\n@@ -214,7 +215,7 @@ struct ReduceOpConversion\n       // get the writeIdx at which to write in smem\n       SmallVector<Value> writeIdx;\n       getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n-                         axis);\n+                         axis, axis);\n \n       // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 18, "deletions": 4, "changes": 22, "file_content_changes": "@@ -154,10 +154,24 @@ class StdSelectPattern : public OpConversionPattern<arith::SelectOp> {\n   matchAndRewrite(arith::SelectOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n-                      op, retType, adaptor.getCondition(),\n-                      adaptor.getTrueValue(), adaptor.getFalseValue()),\n-                  adaptor.getAttributes());\n+\n+    Value cond = adaptor.getCondition();\n+    if (llvm::isa<RankedTensorType>(retType) &&\n+        !llvm::isa<TensorType>(cond.getType())) {\n+      // triton_gpu.select doesn't support scalar condition values, so add a\n+      // splat\n+      auto retTypeTensor = llvm::cast<RankedTensorType>(retType);\n+      auto retShape = retTypeTensor.getShape();\n+      auto retEncoding = retTypeTensor.getEncoding();\n+      Type condTy =\n+          RankedTensorType::get(retShape, cond.getType(), retEncoding);\n+      cond = rewriter.create<triton::SplatOp>(op.getLoc(), condTy, cond);\n+    }\n+\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n+            op, retType, cond, adaptor.getTrueValue(), adaptor.getFalseValue()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -6,7 +6,7 @@\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n \n namespace mlir {\n namespace triton {"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 88, "deletions": 0, "changes": 88, "file_content_changes": "@@ -101,6 +101,93 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n   }\n };\n \n+// sum(x[:, :, None] * y[None, :, :], 1)\n+// -> dot(x, y)\n+class CombineBroadcastMulReducePattern : public mlir::RewritePattern {\n+private:\n+  static bool isAddF32(const Operation *op) {\n+    if (auto addf = dyn_cast_or_null<arith::AddFOp>(op))\n+      return addf.getType().getIntOrFloatBitWidth() <= 32;\n+    return false;\n+  }\n+\n+  static SmallVector<int> getEqualIndices(ArrayRef<int64_t> x,\n+                                          ArrayRef<int64_t> y) {\n+    SmallVector<int> res;\n+    for (int i = 0; i < x.size(); ++i)\n+      if (x[i] == y[i])\n+        res.push_back(i);\n+    return res;\n+  }\n+\n+public:\n+  CombineBroadcastMulReducePattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::ReduceOp::getOperationName(), 1, context) {\n+  }\n+\n+  mlir::LogicalResult matchAndRewrite(mlir::Operation *op,\n+                                      mlir::PatternRewriter &rewriter) const {\n+    auto reduceOp = llvm::dyn_cast<triton::ReduceOp>(op);\n+    if (!reduceOp)\n+      return mlir::failure();\n+    // only support reduce with simple addition\n+    Region &combineOp = reduceOp.getCombineOp();\n+    bool isReduceAdd = combineOp.hasOneBlock() &&\n+                       combineOp.front().getOperations().size() == 2 &&\n+                       isAddF32(&*combineOp.front().getOperations().begin());\n+    if (!isReduceAdd)\n+      return mlir::failure();\n+    // operand of reduce has to be mul\n+    auto mulOp = llvm::dyn_cast_or_null<arith::MulFOp>(\n+        reduceOp.getOperand(0).getDefiningOp());\n+    if (!mulOp)\n+      return mlir::failure();\n+    // mul operand has to be broadcast\n+    auto broadcastLhsOp = llvm::dyn_cast_or_null<triton::BroadcastOp>(\n+        mulOp.getOperand(0).getDefiningOp());\n+    if (!broadcastLhsOp)\n+      return mlir::failure();\n+    auto broadcastRhsOp = llvm::dyn_cast_or_null<triton::BroadcastOp>(\n+        mulOp.getOperand(1).getDefiningOp());\n+    if (!broadcastRhsOp)\n+      return mlir::failure();\n+    // broadcast operand is expand dims\n+    auto expandLhsOp = llvm::dyn_cast_or_null<triton::ExpandDimsOp>(\n+        broadcastLhsOp.getOperand().getDefiningOp());\n+    if (!expandLhsOp)\n+      return mlir::failure();\n+    auto expandRhsOp = llvm::dyn_cast_or_null<triton::ExpandDimsOp>(\n+        broadcastRhsOp.getOperand().getDefiningOp());\n+    if (!expandRhsOp)\n+      return mlir::failure();\n+    // get not-broadcast dimensions\n+    int expandLhsAxis = expandLhsOp.getAxis();\n+    int expandRhsAxis = expandRhsOp.getAxis();\n+    if (expandLhsAxis != 2 || expandRhsAxis != 0)\n+      return mlir::failure();\n+    auto broadcastLhsShape =\n+        broadcastLhsOp.getType().cast<ShapedType>().getShape();\n+    auto broadcastRhsShape =\n+        broadcastLhsOp.getType().cast<ShapedType>().getShape();\n+    if (broadcastLhsShape[2] < 16 || broadcastRhsShape[0] < 16)\n+      return mlir::failure();\n+    Type newAccType =\n+        RankedTensorType::get({broadcastLhsShape[0], broadcastRhsShape[2]},\n+                              broadcastLhsOp.getOperand()\n+                                  .getType()\n+                                  .cast<ShapedType>()\n+                                  .getElementType());\n+    rewriter.setInsertionPoint(op);\n+    auto newAcc = rewriter.create<triton::SplatOp>(\n+        op->getLoc(), newAccType,\n+        rewriter.create<arith::ConstantOp>(op->getLoc(),\n+                                           rewriter.getF32FloatAttr(0)));\n+    rewriter.replaceOpWithNewOp<triton::DotOp>(\n+        op, expandLhsOp.getOperand(), expandRhsOp.getOperand(), newAcc, true);\n+    return mlir::success();\n+  }\n+};\n+\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/Triton/Transforms/Passes.h.inc\"\n \n@@ -120,6 +207,7 @@ class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {\n     patterns.add<CombineSelectMaskedLoadPattern>(context);\n     // patterns.add<CombineAddPtrPattern>(context);\n     patterns.add<CombineBroadcastConstantPattern>(context);\n+    patterns.add<CombineBroadcastMulReducePattern>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "file_content_changes": "@@ -7,7 +7,6 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.cpp.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n \n using namespace mlir;\n@@ -368,9 +367,21 @@ bool isSharedEncoding(Value value) {\n   return false;\n }\n \n+bool isExpensiveCat(CatOp cat, Attribute &targetEncoding) {\n+  // If the new elements per thread is less than the old one, we will need to do\n+  // convert encoding that goes through shared memory anyway. So we consider it\n+  // as expensive.\n+  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n+  auto totalElemsPerThread = gpu::getTotalElemsPerThread(tensorTy);\n+  auto shape = tensorTy.getShape();\n+  auto elemTy = tensorTy.getElementType();\n+  auto newTotalElemsPerThread =\n+      gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n+  return newTotalElemsPerThread < totalElemsPerThread;\n+}\n+\n } // namespace gpu\n } // namespace triton\n-\n } // namespace mlir\n \n static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -80,30 +80,31 @@ class TritonGPUReorderInstructionsPass\n         return;\n       op->moveAfter(argOp);\n     });\n-    // Move `dot` operand so that conversions to opIdx=0 happens before\n-    // conversions to opIdx=1\n+    // Move `dot` operand so that conversions to opIdx=1 happens after\n+    // conversions to opIdx=0\n     m.walk([&](triton::gpu::ConvertLayoutOp op) {\n       auto dstType = op.getResult().getType().cast<RankedTensorType>();\n       auto dstEncoding =\n           dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n       if (!dstEncoding)\n         return;\n       int opIdx = dstEncoding.getOpIdx();\n-      if (opIdx != 0)\n+      if (opIdx != 1)\n         return;\n       if (op->getUsers().empty())\n         return;\n       auto dotUser = dyn_cast<triton::DotOp>(*op->user_begin());\n       if (!dotUser)\n         return;\n-      auto BOp = dotUser.getOperand(1).getDefiningOp();\n-      if (!BOp)\n+      auto AOp =\n+          dotUser.getOperand(0).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n+      if (!AOp)\n         return;\n-      // TODO: An alternative would be to move cvt of OpIdx=0 down instead of\n-      // movig cvt of OpIdx=1 up. This would allow re-ordering more cases.\n-      if (!dom.dominates(op.getOperand(), BOp))\n+      // Check that the conversion to OpIdx=1 happens before and can be moved\n+      // after the conversion to OpIdx=0.\n+      if (!dom.dominates(op.getOperation(), AOp.getOperation()))\n         return;\n-      op->moveBefore(BOp);\n+      op->moveAfter(AOp);\n     });\n     return;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 3, "deletions": 15, "changes": 18, "file_content_changes": "@@ -104,26 +104,13 @@ bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   return true;\n }\n \n-bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n-  // If the new elements per thread is less than the old one, we will need to do\n-  // convert encoding that goes through shared memory anyway. So we consider it\n-  // as expensive.\n-  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n-  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n-  auto shape = tensorTy.getShape();\n-  auto elemTy = tensorTy.getElementType();\n-  auto newTotalElemsPerThread =\n-      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n-  return newTotalElemsPerThread < totalElemsPerThread;\n-}\n-\n bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n     return isExpensiveLoadOrStore(op, targetEncoding);\n   if (isa<triton::CatOp>(op))\n-    return isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return triton::gpu::isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n@@ -136,7 +123,8 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n \n bool canFoldConversion(Operation *op, Attribute &targetEncoding) {\n   if (isa<triton::CatOp>(op))\n-    return !isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return !triton::gpu::isExpensiveCat(cast<triton::CatOp>(op),\n+                                        targetEncoding);\n   return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n              triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 58, "deletions": 10, "changes": 68, "file_content_changes": "@@ -1534,7 +1534,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     for type in ['int32', 'float32']\n     for axis in [1, 0]\n     for shape in scan2d_shapes\n-    for op in ['cumsum']\n+    for op in ['cumsum', 'cumprod']\n ]\n \n \n@@ -1557,7 +1557,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n     z = np.empty_like(x)\n     x_tri = to_triton(x, device=device)\n-    numpy_op = {'cumsum': np.cumsum}[op]\n+    numpy_op = {'cumsum': np.cumsum, 'cumprod': np.cumprod}[op]\n     z_dtype_str = dtype_str\n     z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n     # triton result\n@@ -1566,7 +1566,10 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     z_tri = to_numpy(z_tri)\n     # compare\n     if dtype_str == 'float32':\n-        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+        if op == 'cumprod':\n+            np.testing.assert_allclose(z_ref, z_tri, rtol=0.01, atol=1e-3)\n+        else:\n+            np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n     else:\n         np.testing.assert_equal(z_ref, z_tri)\n \n@@ -1830,7 +1833,8 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n @pytest.mark.parametrize(\"M, N\", [[128, 128], [256, 128], [256, 256], [128, 256]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"op\", [\"sum\", \"max\"])\n-def test_chain_reduce(M, N, src_layout, op, device):\n+@pytest.mark.parametrize(\"first_axis\", [0, 1])\n+def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n     op_str = \"\"\n     if op == \"sum\":\n         op_str = f\"\"\"\n@@ -1860,11 +1864,11 @@ def test_chain_reduce(M, N, src_layout, op, device):\n         %11 = \"tt.reduce\"(%10) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = 1 : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+        }}) {{axis = {first_axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>\n         %12 = \"tt.reduce\"(%11) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n         {op_str}\n-        }}) {{axis = 0 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> i32\n+        }}) {{axis = 0 : i32}} : (tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>) -> i32\n         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n         tt.return\n     }}\n@@ -2160,6 +2164,46 @@ def kernel(X, stride_xm, stride_xk,\n         assert 'mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16' in ptx\n \n \n+@pytest.mark.parametrize('in_dtype', ['float32'])\n+def test_dot_mulbroadcastred(in_dtype, device):\n+    @triton.jit\n+    def kernel(Z, X, Y,\n+               M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n+               BM: tl.constexpr, BN: tl.constexpr, BK: tl.constexpr):\n+        pidn = tl.program_id(1)\n+        pidm = tl.program_id(0)\n+        offm = tl.arange(0, BM)[:, None]\n+        offn = tl.arange(0, BN)[None, :]\n+        offak = tl.arange(0, BK)[None, :]\n+        offbk = tl.arange(0, BK)[:, None]\n+        acc = tl.full((BM, BN), 0.0, tl.float32)\n+        for ridx5 in range(0, K // BK):\n+            x = tl.load(X + ((pidm * K * BM) + (offm * K) + (ridx5 * BK) + offak))\n+            y = tl.load(Y + ((pidn * BN) + (offbk * N) + (ridx5 * N * BK) + offn))\n+            x = tl.expand_dims(x, axis=2)\n+            y = tl.expand_dims(y, axis=0)\n+            t = tl.sum(x * y, axis=1)\n+            acc = t + acc\n+        tl.store(Z + ((pidm * BM * N) + (pidn * BN) + (offm * N) + offn), acc)\n+    M, N, K = 256, 192, 160\n+    BM, BN, BK = 128, 32, 32\n+    rs = RandomState(17)\n+    x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)\n+    y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)\n+    x = x * 0.1\n+    y = y * 0.1\n+    z = numpy_random((M, N), dtype_str=in_dtype, rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    z_tri = to_triton(z, device=device)\n+    grid = M // BM, N // BN\n+    h = kernel[grid](z_tri, x_tri, y_tri, M, N, K, BM, BN, BK)\n+    z_ref = np.matmul(x, y)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), atol=0.01)\n+    assert \"tt.dot\" in h.asm['ttir']\n+    assert \"triton_gpu.async_wait {num = 2 : i32}\" in h.asm['ttgir']\n+\n+\n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n def test_full(dtype_str, device):\n     dtype = getattr(torch, dtype_str)\n@@ -3048,24 +3092,28 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n def test_while(device):\n \n     @triton.jit\n-    def kernel(InitI, Bound, CutOff, OutI, OutJ):\n+    def kernel(InitI, Bound, CutOff, OutI, OutInitI, OutJ):\n         init_i = tl.load(InitI)\n         curr_i = init_i\n         j = 0\n-        while curr_i == init_i and j < tl.load(Bound):\n+        # Check that init_i is not updated by the loop\n+        while j < tl.load(Bound):\n             curr_i = curr_i + (j == tl.load(CutOff))\n             j += 1\n+            tl.store(OutInitI, init_i)\n         tl.store(OutI, curr_i)\n         tl.store(OutJ, j)\n \n     out_i = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     out_j = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     init_i = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    out_init_i = to_triton(np.full((1,), 0, dtype=np.int32), device=device)\n     bound = to_triton(np.full((1,), 10, dtype=np.int32), device=device)\n     cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device=device)\n-    kernel[(1,)](init_i, bound, cut_off, out_i, out_j)\n+    kernel[(1,)](init_i, bound, cut_off, out_i, out_init_i, out_j)\n+    assert out_init_i[0] == init_i[0]\n     assert out_i[0] == init_i[0] + 1\n-    assert out_j[0] == cut_off[0] + 1\n+    assert out_j[0] == bound[0]\n \n # def test_for_if(device):\n "}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -563,8 +563,11 @@ def visit_If(self, node):\n             cond = cond.to(language.int1, _builder=self.builder)\n             contains_return = ContainsReturnChecker(self.gscope).visit(node)\n             if self.scf_stack and contains_return:\n-                raise UnsupportedLanguageConstruct(None, node,\n-                                                   \"Cannot have `return` statements inside `while` or `for` statements in triton\")\n+                raise UnsupportedLanguageConstruct(\n+                    None, node,\n+                    \"Cannot have `return` statements inside `while` or `for` statements in triton \"\n+                    \"(note that this also applies to `return` statements that are inside functions \"\n+                    \"transitively called from within `while`/`for` statements)\")\n             elif self.scf_stack or not contains_return:\n                 self.visit_if_scf(cond, node)\n             else:\n@@ -684,10 +687,6 @@ def visit_While(self, node):\n                     yields.append(loop_defs[name])\n             self.builder.create_yield_op([y.handle for y in yields])\n \n-        # update global uses in while_op\n-        for i, name in enumerate(names):\n-            after_block.replace_use_in_block_with(init_args[i].handle, after_block.arg(i))\n-\n         # WhileOp defines new values, update the symbol table (lscope, local_defs)\n         for i, name in enumerate(names):\n             new_def = language.core.tensor(while_op.get_result(i), ret_types[i])"}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -629,3 +629,9 @@ def cumsum(self, input, axis=None):\n         if axis is None:\n             return torch.cumsum(input)\n         return torch.cumsum(input, dim=axis)\n+\n+    @_tensor_operation\n+    def cumprod(self, input, axis=None):\n+        if axis is None:\n+            return torch.cumprod(input)\n+        return torch.cumprod(input, dim=axis)"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -13,6 +13,7 @@\n     zeros_like,\n )\n from .core import (\n+    TRITON_MAX_TENSOR_NUMEL,\n     abs,\n     advance,\n     arange,\n@@ -34,6 +35,7 @@\n     cat,\n     constexpr,\n     cos,\n+    cumprod,\n     cumsum,\n     debug_barrier,\n     device_assert,\n@@ -105,6 +107,7 @@\n \n \n __all__ = [\n+    \"TRITON_MAX_TENSOR_NUMEL\",\n     \"abs\",\n     \"advance\",\n     \"arange\",\n@@ -128,6 +131,7 @@\n     \"cdiv\",\n     \"constexpr\",\n     \"cos\",\n+    \"cumprod\",\n     \"cumsum\",\n     \"debug_barrier\",\n     \"device_assert\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 24, "deletions": 5, "changes": 29, "file_content_changes": "@@ -1562,6 +1562,20 @@ def cumsum(input, axis=0):\n     input = _promote_reduction_input(input)\n     return associative_scan(input, axis, _sum_combine)\n \n+# cumprod\n+\n+\n+@jit\n+def _prod_combine(a, b):\n+    return a * b\n+\n+\n+@jit\n+@_add_scan_docstr(\"cumprod\")\n+def cumprod(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = _promote_reduction_input(input)\n+    return associative_scan(input, axis, _prod_combine)\n \n # -----------------------\n # Compiler Hint Ops\n@@ -1615,15 +1629,15 @@ def max_contiguous(input, values, _builder=None):\n @builtin\n def static_print(*values, sep: str = \" \", end: str = \"\\n\", file=None, flush=False, _builder=None):\n     '''\n-    Print the values at compile time. The parameters are the same as the Python builtin :code:`print`.\n+    Print the values at compile time.  The parameters are the same as the builtin :code:`print`.\n \n-    Calling the Python builtin :code:`print` inside your kernel is the same as calling this.\n+    NOTE: Calling the Python builtin :code:`print` is not the same as calling this, it instead maps to :code:`device_print`,\n+    which has special requirements for the arguments.\n \n     .. highlight:: python\n     .. code-block:: python\n \n         tl.static_print(f\"{BLOCK_SIZE=}\")\n-        print(f\"{BLOCK_SIZE=}\")\n     '''\n     pass\n \n@@ -1645,13 +1659,18 @@ def static_assert(cond, msg=\"\", _builder=None):\n @builtin\n def device_print(prefix, *args, _builder=None):\n     '''\n-    Print the values at runtime from the device.  String formatting does not work, so you should\n-    provide the values you want to print as arguments.\n+    Print the values at runtime from the device.  String formatting does not work for runtime values, so you should\n+    provide the values you want to print as arguments.  The first value must be a string, all following values must\n+    be scalars or tensors.\n+\n+    Calling the Python builtin :code:`print` is the same as calling this function, and the requirements for the arguments will match\n+    this function (not the normal requirements for :code:`print`).\n \n     .. highlight:: python\n     .. code-block:: python\n \n         tl.device_print(\"pid\", pid)\n+        print(\"pid\", pid)\n \n     :param prefix: a prefix to print before the values. This is required to be a string literal.\n     :param args: the values to print. They can be any tensor or scalar."}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -67,3 +67,24 @@ tt.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n \n   tt.return\n }\n+\n+\n+// -----\n+\n+tt.func public @select_op(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i1) attributes {noinline = false} {\n+  // CHECK-LABEL: select_op\n+  %cst = arith.constant dense<0.000000e+00> : tensor<128xf32>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+  %2 = tt.addptr %1, %0 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xf32>\n+\n+  // CHECK: %[[splat:.*]] = tt.splat %arg2 : (i1) -> tensor<128xi1, #blocked>\n+  // CHECK-NEXT: %{{.*}} = \"triton_gpu.select\"(%[[splat]], %{{.*}}, %{{.*}}) : (tensor<128xi1, #blocked>, tensor<128xf32, #blocked>, tensor<128xf32, #blocked>) -> tensor<128xf32, #blocked>\n+  %4 = arith.select %arg2, %cst, %3 : tensor<128xf32>\n+\n+  %5 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %0 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n+  tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<128xf32>\n+  tt.return\n+}"}, {"filename": "test/TritonGPU/reorder-instructions.mlir", "status": "modified", "additions": 29, "deletions": 12, "changes": 41, "file_content_changes": "@@ -9,24 +9,41 @@\n #mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n #shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [0, 1]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n-  tt.func public @convert_cannot_hoist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+  tt.func public @convert_cannot_hoist(%arg0: tensor<32x32x!tt.ptr<f32>, #blocked>) attributes {noinline = false} {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     %cst_0 = arith.constant dense<1.230000e+02> : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n-    %0 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n-    %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<32x1xi32, #blocked>\n-    %2 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n-    %3 = tt.expand_dims %2 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x32xi32, #blocked>\n-    %4 = tt.broadcast %1 : (tensor<32x1xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n-    %5 = tt.broadcast %3 : (tensor<1x32xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n-    %6 = arith.addi %4, %5 : tensor<32x32xi32, #blocked>\n-    %7 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n-    %8 = tt.addptr %7, %6 : tensor<32x32x!tt.ptr<f32>, #blocked>, tensor<32x32xi32, #blocked>\n-    %9 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %9 = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n     %10 = triton_gpu.convert_layout %9 : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n     %11 = triton_gpu.convert_layout %10 : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n     %12 = tt.dot %11, %cst_0, %cst {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<32x32xf32, #mma>\n     %13 = triton_gpu.convert_layout %12 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n-    tt.store %8, %13 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf32, #blocked>\n+    tt.store %arg0, %13 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf32, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: sink_convert_idx_1\n+//       CHECK: triton_gpu.convert_layout %{{.*}} : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+//       CHECK: triton_gpu.convert_layout %{{.*}} : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+//       CHECK: tt.dot\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [0, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @sink_convert_idx_1(%arg0: tensor<32x32x!tt.ptr<f32>, #blocked>) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+    %B = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %BS = triton_gpu.convert_layout %B : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n+    %BD = triton_gpu.convert_layout %BS : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+    %cst_0 = arith.constant dense<1.230000e+02> : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+    %A = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %AS = triton_gpu.convert_layout %A : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n+    %AD = triton_gpu.convert_layout %AS : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+    %12 = tt.dot %AD, %BD, %cst {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<32x32xf32, #mma>\n+    %13 = triton_gpu.convert_layout %12 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    tt.store %arg0, %13 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf32, #blocked>\n     tt.return\n   }\n }"}]