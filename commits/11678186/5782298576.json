[{"filename": "python/test/unit/hopper/test_gemm.py", "status": "modified", "additions": 2, "deletions": 11, "changes": 13, "file_content_changes": "@@ -29,7 +29,6 @@\n \n import triton\n import triton.language as tl\n-from .utils import get_proper_err, get_variant_golden\n \n \n @triton.jit\n@@ -127,14 +126,12 @@ def test_gemm_no_scf(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE\n     a_f32 = a.to(torch.float32)\n     b_f32 = b.to(torch.float32)\n     golden = torch.matmul(a_f32, b_f32)\n-    golden_variant = get_variant_golden(a_f32, b_f32)\n-    golden_abs_err, golden_rel_err = get_proper_err(golden, golden_variant)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(\n         c,\n         golden,\n-        rtol=max(1e-2, 1.1 * golden_rel_err),\n-        atol=max(1e-3, 1.1 * golden_abs_err),\n+        rtol=1e-2,\n+        atol=1e-3,\n         check_dtype=False)\n \n \n@@ -450,12 +447,6 @@ def grid(META):\n                               enable_warp_specialization=ENABLE_WS)\n \n     torch.set_printoptions(profile=\"full\")\n-    # print(\"abs_err: {}, rel_err: {}\".format(golden_abs_err, golden_rel_err))\n-    # print(\"golden: \")\n-    # print(golden)\n-    # print(\"result: \")\n-    # print(z)\n-    # print(\"max_gap: {}\".format(torch.max(torch.abs(z - golden))))\n     golden = torch.nn.functional.normalize(golden)\n     z = torch.nn.functional.normalize(z)\n     assert_close(z, golden,"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 2, "deletions": 5, "changes": 7, "file_content_changes": "@@ -26,7 +26,6 @@\n \n import triton\n import triton.language as tl\n-from .utils import get_proper_err, get_variant_golden\n \n \n def isMMAV3OrTMAEnabled():\n@@ -641,14 +640,12 @@ def test_static_persistent_matmul_no_scf_kernel(M, N, K, NUM_CTAS, NUM_WARPS, TR\n     a_f32 = a.to(torch.float32)\n     b_f32 = b.to(torch.float32)\n     golden = torch.matmul(a_f32, b_f32)\n-    golden_variant = get_variant_golden(a_f32, b_f32)\n-    golden_abs_err, golden_rel_err = get_proper_err(golden, golden_variant)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(\n         c,\n         golden,\n-        rtol=max(1e-2, 1.1 * golden_rel_err),\n-        atol=max(1e-3, 1.1 * golden_abs_err),\n+        rtol=1e-2,\n+        atol=1e-3,\n         check_dtype=False)\n \n "}, {"filename": "python/test/unit/hopper/test_tma_store_gemm.py", "status": "modified", "additions": 1, "deletions": 32, "changes": 33, "file_content_changes": "@@ -28,36 +28,6 @@\n import triton.language as tl\n \n \n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)\n-\n-\n @triton.jit\n def matmul_tma_load_store(\n     a_ptr, b_ptr, c_ptr,\n@@ -118,6 +88,5 @@ def test_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_F\n                                   num_ctas=NUM_CTAS,\n                                   OUTPUT_F16=OUTPUT_F16)\n     golden = torch.matmul(a, b)\n-    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n     torch.set_printoptions(profile=\"full\")\n-    assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n+    assert_close(c, golden, rtol=1e-2, atol=1e-3, check_dtype=False)"}, {"filename": "python/test/unit/hopper/ttgir_tests/test_util.py", "status": "removed", "additions": 0, "deletions": 52, "changes": 52, "file_content_changes": "@@ -1,52 +0,0 @@\n-# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n-#\n-# Permission is hereby granted, free of charge, to any person obtaining\n-# a copy of this software and associated documentation files\n-# (the \"Software\"), to deal in the Software without restriction,\n-# including without limitation the rights to use, copy, modify, merge,\n-# publish, distribute, sublicense, and/or sell copies of the Software,\n-# and to permit persons to whom the Software is furnished to do so,\n-# subject to the following conditions:\n-#\n-# The above copyright notice and this permission notice shall be\n-# included in all copies or substantial portions of the Software.\n-#\n-# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n-# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n-# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n-# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n-# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n-# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n-# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n-\n-import torch\n-\n-\n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)"}, {"filename": "python/test/unit/hopper/utils.py", "status": "removed", "additions": 0, "deletions": 32, "changes": 32, "file_content_changes": "@@ -1,32 +0,0 @@\n-import torch\n-\n-\n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K), dtype=a.dtype).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K), dtype=a.dtype).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N), dtype=b.dtype).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N), dtype=b.dtype).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(golden, golden_variant):\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    # avoid problems when golden_rel_err is 'inf'\n-    abs_golden = torch.abs(golden) + torch.full_like(golden, torch.finfo(golden.dtype).smallest_normal)\n-    golden_rel_err = torch.max(torch.abs(golden_diff) / abs_golden).item()\n-    return (golden_abs_err, golden_rel_err)"}, {"filename": "python/tutorials/10-experimental-tmastg-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 29, "changes": 34, "file_content_changes": "@@ -84,31 +84,6 @@ def matmul_kernel(\n     tl.store(c_block_ptr, accumulator)\n \n \n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)\n-\n-\n def matmul(a, b):\n     # checks constraints\n     assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n@@ -134,15 +109,16 @@ def grid(META):\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n b = torch.randn((512, 512), device='cuda', dtype=torch.float16).T\n c = matmul(a, b)\n+c = torch.nn.functional.normalize(c)\n+\n+golden = torch.nn.functional.normalize(torch.matmul(a, b))\n \n-golden = torch.matmul(a, b)\n-golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n torch.set_printoptions(profile=\"full\")\n assert_close(\n     c,\n     golden,\n-    rtol=max(1e-4, 1.5 * golden_rel_err),\n-    atol=max(1e-4, 1.5 * golden_abs_err),\n+    rtol=1e-2,\n+    atol=1e-3,\n     check_dtype=False)\n \n "}]