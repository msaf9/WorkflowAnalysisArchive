[{"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "file_content_changes": "@@ -68,32 +68,42 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n   CUmodule mod;\n   int32_t n_regs = 0;\n   int32_t n_spills = 0;\n+  printf(\"Loading binary %s\\n\", name);\n   // create driver handles\n   CUDA_CHECK(cuModuleLoadData(&mod, data));\n+  printf(\"step 1\\n\");\n   CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n+  printf(\"step 2\\n\");\n   // get allocated registers and spilled registers from the function\n   CUDA_CHECK(cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS, fun));\n+  printf(\"step 3\\n\");\n   CUDA_CHECK(\n       cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun));\n+  printf(\"step 4\\n\");\n   n_spills /= 4;\n   // set dynamic shared memory if necessary\n   int shared_optin;\n   CUDA_CHECK(cuDeviceGetAttribute(\n       &shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN,\n       device));\n+  printf(\"step 5\\n\");\n   if (shared > 49152 && shared_optin > 49152) {\n     CUDA_CHECK(cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED));\n+    printf(\"step 6\\n\");\n     int shared_total, shared_static;\n     CUDA_CHECK(cuDeviceGetAttribute(\n         &shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR,\n         device));\n+    printf(\"step 7\\n\");\n     CUDA_CHECK(cuFuncGetAttribute(&shared_static,\n                                   CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n+    printf(\"step 8\\n\");\n     CUDA_CHECK(\n         cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n                            shared_optin - shared_static));\n+    printf(\"step 9\\n\");\n   }\n-\n+  printf(\"step end\\n\");\n   if (PyErr_Occurred()) {\n     return NULL;\n   }"}]