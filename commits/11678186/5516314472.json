[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -110,10 +110,11 @@ jobs:\n       - name: Regression tests\n         if: ${{ contains(matrix.runner, 'A100') }}\n         run: |\n+          python3 -m pip install pytest-rerunfailures\n           cd python/test/regression\n           sudo nvidia-smi -i 0 -pm 1\n-          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n-          python3 -m pytest -vs .\n+          sudo nvidia-smi -i 0 --lock-gpu-clocks=1280,1280\n+          python3 -m pytest -vs . --reruns 10\n           sudo nvidia-smi -i 0 -rgc\n \n   Integration-Tests-Third-Party:"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 49, "deletions": 88, "changes": 137, "file_content_changes": "@@ -38,56 +38,24 @@ def nvsmi(attrs):\n mem_clocks = {'v100': 877, 'a100': 1215}\n \n matmul_data = {\n-    'v100': {\n-        # square\n-        (512, 512, 512): {'float16': 0.158},\n-        (1024, 1024, 1024): {'float16': 0.466},\n-        (2048, 2048, 2048): {'float16': 0.695},\n-        (4096, 4096, 4096): {'float16': 0.831},\n-        (8192, 8192, 8192): {'float16': 0.849},\n-        # tall-skinny\n-        (16, 1024, 1024): {'float16': 0.0128},\n-        (16, 4096, 4096): {'float16': 0.0883},\n-        (16, 8192, 8192): {'float16': 0.101},\n-        (64, 1024, 1024): {'float16': 0.073},\n-        (64, 4096, 4096): {'float16': 0.270},\n-        (64, 8192, 8192): {'float16': 0.459},\n-        (1024, 64, 1024): {'float16': 0.0692},\n-        (4096, 64, 4096): {'float16': 0.264},\n-        (8192, 64, 8192): {'float16': 0.452},\n-        # Non pow 2 shapes\n-        (1000, 200, 100): {'float16': 0.084},\n-        (1000, 200, 700): {'float16': 0.084},\n-        (994, 136, 402): {'float16': 0.084},\n-        (995, 135, 409): {'float16': 0.084},\n-        (99, 1357, 409): {'float16': 0.084},\n-    },\n     # NOTE:\n-    # A100 in the CI server is slow-ish for some reason.\n-    # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n         # square\n-        (512, 512, 512): {'float16': 0.084, 'float32': 0.12, 'int8': 0.05},\n-        (1024, 1024, 1024): {'float16': 0.332, 'float32': 0.352, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.635, 'float32': 0.522, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.750, 'float32': 0.810, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.760, 'float32': 0.760, 'int8': 0.51},\n+        (512, 512, 512): {'float16': 0.061, 'float32': 0.097, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.283, 'float32': 0.313, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.618, 'float32': 0.532, 'int8': 0.34},\n+        (4096, 4096, 4096): {'float16': 0.751, 'float32': 0.726, 'int8': 0.46},\n+        (8192, 8192, 8192): {'float16': 0.786, 'float32': 0.754, 'int8': 0.51},\n         # tall-skinny\n-        (16, 1024, 1024): {'float16': 0.008, 'float32': 0.009, 'int8': 0.005},\n-        (16, 4096, 4096): {'float16': 0.036, 'float32': 0.038, 'int8': 0.026},\n-        (16, 8192, 8192): {'float16': 0.056, 'float32': 0.061, 'int8': 0.043},\n-        (64, 1024, 1024): {'float16': 0.020, 'float32': 0.030, 'int8': 0.017},\n-        (64, 4096, 4096): {'float16': 0.160, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.280, 'float32': 0.257, 'int8': 0.174},\n-        (1024, 64, 1024): {'float16': 0.040, 'float32': 0.050, 'int8': 0.017},\n-        (4096, 64, 4096): {'float16': 0.160, 'float32': 0.200, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.250, 'float32': 0.23, 'int8': 0.177},\n-        # Non pow 2 shapes\n-        (1000, 200, 100): {'float16': 0.011, 'float32': 0.017, 'int8': 0.05},\n-        (1000, 200, 700): {'float16': 0.027, 'float32': 0.047, 'int8': 0.05},\n-        (994, 136, 402): {'float16': 0.015, 'float32': 0.024, 'int8': 0.05},\n-        (995, 135, 409): {'float16': 0.015, 'float32': 0.025, 'int8': 0.05},\n-        (99, 1357, 409): {'float16': 0.011, 'float32': 0.036, 'int8': 0.05}\n+        (16, 1024, 1024): {'float16': 0.006, 'float32': 0.009, 'int8': 0.005},\n+        (16, 4096, 4096): {'float16': 0.057, 'float32': 0.051, 'int8': 0.026},\n+        (16, 8192, 8192): {'float16': 0.077, 'float32': 0.077, 'int8': 0.043},\n+        (64, 1024, 1024): {'float16': 0.018, 'float32': 0.023, 'int8': 0.017},\n+        (64, 4096, 4096): {'float16': 0.150, 'float32': 0.000, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.338, 'float32': 0.000, 'int8': 0.174},\n+        (1024, 64, 1024): {'float16': 0.029, 'float32': 0.046, 'int8': 0.017},\n+        (4096, 64, 4096): {'float16': 0.179, 'float32': 0.214, 'int8': 0.102},\n+        (8192, 64, 8192): {'float16': 0.278, 'float32': 0.000, 'int8': 0.177},\n     }\n }\n \n@@ -97,6 +65,8 @@ def nvsmi(attrs):\n                           for M, N, K in matmul_data[DEVICE_NAME].keys()\n                           for dtype_str in ['float16', 'float32']])\n def test_matmul(M, N, K, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n     if dtype_str in ['float32', 'int8'] and DEVICE_NAME != 'a100':\n         pytest.skip('Only test float32 & int8 on a100')\n     if (M, N, K) in [(64, 4096, 4096), (64, 8192, 8192), (8192, 64, 8192)] and dtype_str == 'float32':\n@@ -114,11 +84,11 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=300)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)\n \n \n #######################\n@@ -140,54 +110,43 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n \n \n elementwise_data = {\n-    'v100': {\n-        1024 * 16: {'float16': 0.0219, 'float32': 0.010},\n-        1024 * 64: {'float16': 0.0791, 'float32': 0.010},\n-        1024 * 256: {'float16': 0.243, 'float32': 0.010},\n-        1024 * 1024: {'float16': 0.530, 'float32': 0.010},\n-        1024 * 4096: {'float16': 0.796, 'float32': 0.010},\n-        1024 * 16384: {'float16': 0.905, 'float32': 0.010},\n-        1024 * 65536: {'float16': 0.939, 'float32': 0.010},\n-        # Non pow 2\n-        1020 * 100: {'float16': 0.010, 'float32': 0.010},\n-        995 * 125: {'float16': 0.010, 'float32': 0.010},\n-        10003 * 7007: {'float16': 0.010, 'float32': 0.010},\n-    },\n     'a100': {\n-        1024 * 16: {'float16': 0.010, 'bfloat16': 0.010, 'float32': 0.020},\n-        1024 * 64: {'float16': 0.040, 'bfloat16': 0.040, 'float32': 0.066},\n-        1024 * 256: {'float16': 0.132, 'bfloat16': 0.132, 'float32': 0.227},\n-        1024 * 1024: {'float16': 0.353, 'bfloat16': 0.353, 'float32': 0.488},\n-        1024 * 4096: {'float16': 0.605, 'bfloat16': 0.605, 'float32': 0.705},\n-        1024 * 16384: {'float16': 0.758, 'bfloat16': 0.750, 'float32': 0.819},\n-        1024 * 65536: {'float16': 0.850, 'bfloat16': 0.850, 'float32': 0.870},\n+        1024 * 16: {'float16': 0.003, 'float32': 0.007},\n+        1024 * 64: {'float16': 0.013, 'float32': 0.026},\n+        1024 * 256: {'float16': 0.053, 'float32': 0.105},\n+        1024 * 1024: {'float16': 0.212, 'float32': 0.420},\n+        1024 * 4096: {'float16': 0.791, 'float32': 0.668},\n+        1024 * 16384: {'float16': 0.762, 'float32': 0.812},\n+        1024 * 65536: {'float16': 0.846, 'float32': 0.869},\n         # Non pow 2\n-        1020 * 100: {'float16': 0.051, 'bfloat16': 0.051, 'float32': 0.103},\n-        995 * 125: {'float16': 0.063, 'bfloat16': 0.063, 'float32': 0.126},\n-        10003 * 7007: {'float16': 0.544, 'bfloat16': 0.541, 'float32': 0.861},\n+        1020 * 100: {'float16': 0.020, 'float32': 0.041},\n+        10003 * 7007: {'float16': 0.513, 'float32': 0.861},\n     }\n }\n \n \n @pytest.mark.parametrize('N', elementwise_data[DEVICE_NAME].keys())\n @pytest.mark.parametrize(\"dtype_str\", ['float16', 'bfloat16', 'float32'])\n def test_elementwise(N, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n     torch.manual_seed(0)\n     if dtype_str in ['bfloat16'] and DEVICE_NAME != 'a100':\n         pytest.skip('Only test bfloat16 on a100')\n     dtype = {'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float32': torch.float32}[dtype_str]\n-    ref_gpu_util = elementwise_data[DEVICE_NAME][N][dtype_str]\n+    ref_dtype_str = 'float16' if dtype_str == 'bfloat16' else dtype_str\n+    ref_gpu_util = elementwise_data[DEVICE_NAME][N][ref_dtype_str]\n     max_gpu_perf = get_dram_gbps()\n     z = torch.empty((N, ), dtype=dtype, device='cuda')\n     x = torch.randn_like(z)\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)\n \n #######################\n # Flash-Attention\n@@ -196,29 +155,29 @@ def test_elementwise(N, dtype_str):\n \n flash_attention_data = {\n     \"a100\": {\n-        (4, 48, 4096, 64, True, True, 'forward', 'float16'): 0.420,\n-        (4, 48, 4096, 64, True, True, 'backward', 'float16'): 0.202,\n-        (4, 48, 4096, 64, True, True, 'forward', 'bfloat16'): 0.355,\n-        (4, 48, 4096, 64, True, True, 'backward', 'bfloat16'): 0.201,\n-        (4, 48, 1024, 16, True, True, 'forward', 'float32'): 0.099,\n+        (4, 48, 4096, 64, True, True, 'forward', 'float16'): 0.424,\n+        (4, 48, 4096, 64, True, True, 'forward', 'bfloat16'): 0.379,\n+        (4, 48, 1024, 16, True, True, 'forward', 'float32'): 0.098,\n+        (4, 48, 4096, 64, True, True, 'backward', 'float16'): 0.201,\n+        (4, 48, 4096, 64, True, True, 'backward', 'bfloat16'): 0.199,\n         (4, 48, 1024, 16, True, True, 'backward', 'float32'): 0.087,\n-        (4, 48, 4096, 64, True, False, 'forward', 'float16'): 0.238,\n+        (4, 48, 4096, 64, True, False, 'forward', 'float16'): 0.240,\n+        (4, 48, 4096, 64, True, False, 'forward', 'bfloat16'): 0.210,\n+        (4, 48, 1024, 16, True, False, 'forward', 'float32'): 0.061,\n         (4, 48, 4096, 64, True, False, 'backward', 'float16'): 0.135,\n-        (4, 48, 4096, 64, True, False, 'forward', 'bfloat16'): 0.211,\n         (4, 48, 4096, 64, True, False, 'backward', 'bfloat16'): 0.135,\n-        (4, 48, 1024, 16, True, False, 'forward', 'float32'): 0.062,\n         (4, 48, 1024, 16, True, False, 'backward', 'float32'): 0.052,\n         (4, 48, 4096, 64, False, True, 'forward', 'float16'): 0.424,\n+        (4, 48, 4096, 64, False, True, 'forward', 'bfloat16'): 0.378,\n+        (4, 48, 1024, 16, False, True, 'forward', 'float32'): 0.099,\n         (4, 48, 4096, 64, False, True, 'backward', 'float16'): 0.262,\n-        (4, 48, 4096, 64, False, True, 'forward', 'bfloat16'): 0.370,\n         (4, 48, 4096, 64, False, True, 'backward', 'bfloat16'): 0.254,\n-        (4, 48, 1024, 16, False, True, 'forward', 'float32'): 0.099,\n         (4, 48, 1024, 16, False, True, 'backward', 'float32'): 0.125,\n         (4, 48, 4096, 64, False, False, 'forward', 'float16'): 0.238,\n-        (4, 48, 4096, 64, False, False, 'backward', 'float16'): 0.158,\n         (4, 48, 4096, 64, False, False, 'forward', 'bfloat16'): 0.211,\n-        (4, 48, 4096, 64, False, False, 'backward', 'bfloat16'): 0.134,\n         (4, 48, 1024, 16, False, False, 'forward', 'float32'): 0.062,\n+        (4, 48, 4096, 64, False, False, 'backward', 'float16'): 0.158,\n+        (4, 48, 4096, 64, False, False, 'backward', 'bfloat16'): 0.134,\n         (4, 48, 1024, 16, False, False, 'backward', 'float32'): 0.075,\n     }\n }\n@@ -230,6 +189,8 @@ def test_elementwise(N, dtype_str):\n @pytest.mark.parametrize(\"seq_par\", [True, False])\n @pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n def test_flash_attention(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n     is_backward = mode == 'backward'\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n@@ -250,7 +211,7 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str):\n         o = fn()\n         do = torch.randn_like(o)\n         fn = lambda: o.backward(do, retain_graph=True)\n-    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n     total_flops = 2 * flops_per_matmul\n@@ -263,4 +224,4 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str):\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str)]\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -401,6 +401,9 @@ def __init__(self, value):\n     def __repr__(self) -> str:\n         return f\"constexpr[{self.value}]\"\n \n+    def __index__(self):\n+        return self.value\n+\n     def __add__(self, other):\n         return constexpr(self.value + other.value)\n "}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 61, "deletions": 0, "changes": 61, "file_content_changes": "@@ -16,6 +16,67 @@ def nvsmi(attrs):\n     return ret\n \n \n+def do_bench_cudagraph(fn, rep=20, grad_to_none=None):\n+    import torch\n+    \"\"\"\n+    Benchmark the runtime of the provided function.\n+\n+    :param fn: Function to benchmark\n+    :type fn: Callable\n+    :param rep: Repetition time (in ms)\n+    :type rep: int\n+    :param grad_to_none: Reset the gradient of the provided tensor to None\n+    :type grad_to_none: torch.tensor, optional\n+    \"\"\"\n+    if torch.cuda.current_stream() == torch.cuda.default_stream():\n+        raise RuntimeError(\"Cannot capture graph in default stream. Please use side stream in benchmark code.\")\n+    # record CUDAGraph\n+    fn()\n+    if grad_to_none is not None:\n+        for x in grad_to_none:\n+            x.detach_()\n+            x.requires_grad_(True)\n+            x.grad = None\n+    g = torch.cuda.CUDAGraph()\n+    with torch.cuda.graph(g):\n+        fn()\n+    torch.cuda.synchronize()\n+    fn = lambda: g.replay()\n+    # Estimate the runtime of the function\n+    start_event = torch.cuda.Event(enable_timing=True)\n+    end_event = torch.cuda.Event(enable_timing=True)\n+    start_event.record()\n+    fn()\n+    end_event.record()\n+    torch.cuda.synchronize()\n+    estimate_ms = start_event.elapsed_time(end_event)\n+    # compute number of repetition to last `rep` ms\n+    n_repeat = max(1, int(rep / estimate_ms))\n+    # compute number of repetition to last `rep` ms\n+    start_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n+    end_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n+    ret = []\n+    n_retries = 50\n+    for _ in range(n_retries):\n+        # Benchmark\n+        torch.cuda.synchronize()\n+        for i in range(n_repeat):\n+            # we don't want `fn` to accumulate gradient values\n+            # if it contains a backward pass. So we clear the\n+            # provided gradients\n+            if grad_to_none is not None:\n+                for x in grad_to_none:\n+                    x.grad = None\n+            # record time of `fn`\n+            start_event[i].record()\n+            fn()\n+            end_event[i].record()\n+        torch.cuda.synchronize()\n+        times = torch.tensor([s.elapsed_time(e) for s, e in zip(start_event, end_event)])\n+        ret.append(torch.min(times))\n+    return torch.mean(torch.tensor(ret)).item()\n+\n+\n def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n              quantiles=None,\n              fast_flush=True,"}]