[{"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 0, "deletions": 43, "changes": 43, "file_content_changes": "@@ -55,49 +55,6 @@ func.func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n   // CHECK: return %6 : tensor<1024xi32, [[$target_layout]]>\n }\n \n-// CHECK-LABEL: remat_load_store\n-func.func @remat_load_store(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n-  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n-  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n-  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n-  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n-  // CHECK-NOT: triton_gpu.convert_layout\n-  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout1>\n-  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout1>\n-  tt.store %5, %4 : tensor<64xi32, #layout1>\n-  return\n-}\n-\n-// Don't rematerialize vectorized loads\n-// CHECK-LABEL: remat_expensive\n-func.func @remat_expensive(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n-  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout1>\n-  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout1>\n-  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout1>, tensor<64xi32, #layout1>\n-  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout1>\n-  // CHECK: triton_gpu.convert_layout\n-  // CHECK-NOT: triton_gpu.convert_layout\n-  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout1>) -> tensor<64xi32, #layout0>\n-  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout1>) -> tensor<64x!tt.ptr<i32>, #layout0>\n-  tt.store %5, %4 : tensor<64xi32, #layout0>\n-  return\n-}\n-\n-// Don't rematerialize loads when original and target layouts are different\n-// CHECK-LABEL: remat_multi_layout\n-func.func @remat_multi_layout(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n-  %0 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #layout0>\n-  %1 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<64x!tt.ptr<i32>, #layout0>\n-  %2 = tt.addptr %1, %0 : tensor<64x!tt.ptr<i32>, #layout0>, tensor<64xi32, #layout0>\n-  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xi32, #layout0>\n-  // CHECK: triton_gpu.convert_layout\n-  // CHECK-NOT: triton_gpu.convert_layout\n-  %4 = triton_gpu.convert_layout %3 : (tensor<64xi32, #layout0>) -> tensor<64xi32, #layout2>\n-  %5 = triton_gpu.convert_layout %2 : (tensor<64x!tt.ptr<i32>, #layout0>) -> tensor<64x!tt.ptr<i32>, #layout2>\n-  tt.store %5, %4 : tensor<64xi32, #layout2>\n-  return\n-}\n-\n // Always rematerialize single value loads\n // CHECK-LABEL: remat_single_value\n func.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {"}]