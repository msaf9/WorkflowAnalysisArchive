[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -164,6 +164,9 @@ class AllocationAnalysis {\n         auto smemShape = getScratchConfigForReduce(reduceOp);\n         unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n                                          std::multiplies{});\n+        auto mod = op->getParentOfType<ModuleOp>();\n+        unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+        elems = std::max<unsigned>(elems, numWarps * 32);\n         auto bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n         allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 42, "deletions": 0, "changes": 42, "file_content_changes": "@@ -64,6 +64,12 @@ Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n                                            rewriter.getF32FloatAttr(v));\n }\n \n+Value createConstantF64(Location loc, PatternRewriter &rewriter, float v) {\n+  auto type = type::f64Ty(rewriter.getContext());\n+  return rewriter.create<LLVM::ConstantOp>(loc, type,\n+                                           rewriter.getF64FloatAttr(v));\n+}\n+\n // Create an index type constant.\n static Value createIndexConstant(OpBuilder &builder, Location loc,\n                                  TypeConverter *converter, int64_t value) {\n@@ -132,6 +138,7 @@ void llPrintf(StringRef msg, ValueRange args,\n #define f64_ty rewriter.getF64Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n #define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n+#define f64_val(...) LLVM::createConstantF64(loc, rewriter, __VA_ARGS__)\n #define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n #define struct_ty(...) LLVM::LLVMStructType::getLiteral(ctx, __VA_ARGS__)\n \n@@ -2537,6 +2544,8 @@ class ElementwiseOpConversionBase\n     for (unsigned i = 0; i < elems; ++i) {\n       resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n                                                  operands[i], loc);\n+      if (!bool(resultVals[i]))\n+        return failure();\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n@@ -5552,6 +5561,32 @@ struct FDivOpConversion\n   }\n };\n \n+struct ExpOpConversionApprox\n+    : ElementwiseOpConversionBase<mlir::math::ExpOp, LLVM::InlineAsmOp,\n+                                  ExpOpConversionApprox> {\n+  using Base = ElementwiseOpConversionBase<mlir::math::ExpOp, LLVM::InlineAsmOp,\n+                                           ExpOpConversionApprox>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::math::ExpOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    // For FP64 input, call __nv_expf for higher-precision calculation\n+    if (elemTy.getIntOrFloatBitWidth() == 64)\n+      return {};\n+    const double log2e = 1.4426950408889634;\n+    Value prod =\n+        rewriter.create<LLVM::FMulOp>(loc, f32_ty, operands[0], f32_val(log2e));\n+    PTXBuilder ptxBuilder;\n+    auto &exp2 = ptxBuilder.create<PTXInstr>(\"ex2\")->o(\"approx\").o(\"f32\");\n+    auto output = ptxBuilder.newOperand(\"=f\");\n+    auto input = ptxBuilder.newOperand(prod, \"f\");\n+    exp2(output, input);\n+    return ptxBuilder.launch(rewriter, loc, f32_ty, false);\n+  }\n+};\n+\n /// ====================== atomic_rmw codegen begin ==========================\n struct AtomicRMWOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>,\n@@ -5712,6 +5747,13 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n \n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n+\n+  // ExpOpConversionApprox will try using ex2.approx if the input type is FP32.\n+  // For FP64 input type, ExpOpConversionApprox will return failure and\n+  // ElementwiseOpConversion<math::ExpOp, math::ExpOp> defined below will call\n+  // __nv_expf for higher-precision calculation\n+  patterns.add<ExpOpConversionApprox>(typeConverter, benefit);\n+\n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n   patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n "}, {"filename": "python/tests/test_elementwise.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -61,6 +61,7 @@ def get_tensor(shape, data_type, b_positive=False):\n                              ('sqrt', 'float64', 'float64'),\n                              ('abs', 'float32', 'float32'),\n                              ('exp', 'float32', 'float32'),\n+                             ('exp', 'float64', 'float64'),\n                              ('sigmoid', 'float32', 'float32'),\n                           ])\n def test_single_input(expr, output_type, input0_type):"}]