[{"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -809,11 +809,11 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n                                     nextIV, newForOp.getUpperBound());\n \n   pipelineIterIdx = newForOp.getRegionIterArgs()[ivIndex + 1];\n-  Value insertSliceIndex = builder.create<arith::RemUIOp>(\n+  Value insertSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n   loopIterIdx = newForOp.getRegionIterArgs()[ivIndex + 2];\n-  Value extractSliceIndex = builder.create<arith::RemUIOp>(\n+  Value extractSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -357,7 +357,7 @@ def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n     args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n-) for mode in ['bwd'] for causal in [False]]\n+) for mode in ['fwd'] for causal in [False]]\n \n \n @triton.testing.perf_report(configs)"}]