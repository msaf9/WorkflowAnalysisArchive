[{"filename": "lib/driver/llvm.cc", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -239,14 +239,12 @@ std::string ptx_to_cubin(const std::string& ptx, const std::string& ptxas, int c\n     unlink(_flog);\n     throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" + log);\n   }\n-  CUmodule ret;\n   std::ifstream _cubin(_fbin, std::ios::binary );\n   std::string cubin(std::istreambuf_iterator<char>(_cubin), {});\n   _cubin.close();\n   unlink(_fsrc);\n   unlink(_flog);\n   unlink(_fbin);\n-  dispatch::cuModuleLoadData(&ret, cubin.c_str());\n   return cubin;\n }\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -436,7 +436,7 @@ typedef std::map<std::string, py::object> asm_map_t;\n \n void init_triton_codegen(py::module &&m) {\n   m.def(\"compile_ttir\",\n-      [](backend_t backend, ir::module &ir, uint64_t device, int num_warps, int num_stages, py::dict& extern_libs) {\n+      [](backend_t backend, ir::module &ir, uint64_t device, int num_warps, int num_stages, py::dict& extern_libs, size_t cc) {\n           py::gil_scoped_release allow_threads;\n           std::string name = ir.get_function_list()[0]->get_name();\n           // record asm as we generate\n@@ -454,10 +454,12 @@ void init_triton_codegen(py::module &&m) {\n                 name, triton::codegen::create_extern_lib(name, path));\n           }\n           // device properties\n-          CUdevice dev = (CUdevice)device;\n-          size_t major = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR>(dev);\n-          size_t minor = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR>(dev);\n-          size_t cc = major*10 + minor;\n+          if (cc == 0) {\n+            CUdevice dev = (CUdevice)device;\n+            size_t major = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR>(dev);\n+            size_t minor = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR>(dev);\n+            cc = major*10 + minor;\n+          }\n           int version;\n           std::string ptxas_path = drv::path_to_ptxas(version);\n           // Triton-IR -> NVPTX LLVM-IR"}, {"filename": "python/test/unit/runtime/test_cache.py", "status": "modified", "additions": 32, "deletions": 0, "changes": 32, "file_content_changes": "@@ -1,6 +1,8 @@\n import os\n import re\n import shutil\n+import multiprocessing\n+from collections import namedtuple\n \n import pytest\n import torch\n@@ -172,3 +174,33 @@ def kernel_add(a, b, o, N: tl.constexpr):\n     assert len(kernel_add.cache) == 1\n     kernel_add.warmup(*args, grid=(1,))\n     assert len(kernel_add.cache) == 1\n+\n+\n+def test_compile_in_subproc() -> None:\n+    @triton.jit\n+    def kernel_sub(a, b, o, N: tl.constexpr):\n+        idx = tl.arange(0, N)\n+        tl.store(o + idx,\n+                 tl.load(a + idx) - tl.load(b + idx) * 777)\n+\n+    major, minor = torch.cuda.get_device_capability(0)\n+    cc = major * 10 + minor\n+    config = namedtuple(\"instance_descriptor\", [\n+        \"divisible_by_16\", \"equal_to_1\"])(\n+        tuple(range(4)),\n+        ())\n+\n+    proc = multiprocessing.Process(\n+        target=triton.compile,\n+        kwargs=dict(\n+            fn=kernel_sub,\n+            signature={0: \"*fp32\", 1: \"*fp32\", 2: \"*fp32\"},\n+            device=0,\n+            constants={3: 32},\n+            configs=[config],\n+            warm_cache_only=True,\n+            cc=cc,\n+        ))\n+    proc.start()\n+    proc.join()\n+    assert proc.exitcode == 0"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 14, "deletions": 7, "changes": 21, "file_content_changes": "@@ -880,7 +880,10 @@ def ptx_get_kernel_name(ptx: str) -> str:\n             return line.split()[-1]\n \n \n-def _compile(fn, signature: str, device: int = -1, constants=dict(), specialization=_triton.code_gen.instance_descriptor(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, output: str = \"ttgir\") -> Tuple[str, int, str]:\n+def _compile(fn, signature: str, device: int = -1, constants=dict(),\n+             specialization=_triton.code_gen.instance_descriptor(),\n+             num_warps: int = 4, num_stages: int = 3, extern_libs=None,\n+             output: str = \"ttgir\", cc=0) -> Tuple[str, int, str]:\n     valid_outputs = (\"ttir\", \"ttgir\", \"ptx\", \"cubin\")\n     assert output in valid_outputs, \"output should be one of [%s], but get \\\"%s\\\"\" % (','.join(valid_outputs), output)\n \n@@ -894,7 +897,7 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     backend = _triton.runtime.backend.CUDA\n     if extern_libs is None:\n         extern_libs = dict()\n-    name, asm, shared_mem = _triton.code_gen.compile_ttir(backend, module, device, num_warps, num_stages, extern_libs)\n+    name, asm, shared_mem = _triton.code_gen.compile_ttir(backend, module, device, num_warps, num_stages, extern_libs, cc)\n     return asm, shared_mem, name\n \n \n@@ -1179,7 +1182,8 @@ def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_sta\n     return key\n \n \n-def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n+def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4,\n+            num_stages: int = 3, extern_libs=None, configs=None, cc=0, warm_cache_only=False):\n     # we get the kernel, i.e. the first function generated in the module\n     assert len(configs) == 1\n     # cache manager\n@@ -1208,18 +1212,22 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n     if not fn_cache_manager.has_file(cubin_name) or \\\n        not fn_cache_manager.has_file(data_name) or \\\n        not fn_cache_manager.has_file(ptx_name):\n-        asm, shared, kernel_name = _compile(fn, signature, device, constants, configs[0], num_warps, num_stages, extern_libs, \"cubin\")\n+        asm, shared, kernel_name = _compile(fn, signature, device, constants, configs[0], num_warps, num_stages,\n+                                            extern_libs, \"cubin\", cc)\n         metadata = {\"name\": kernel_name, \"shared\": shared, \"num_warps\": num_warps, \"num_stages\": num_stages}\n         fn_cache_manager.put(asm[\"cubin\"], cubin_name)\n         fn_cache_manager.put(asm[\"ptx\"], ptx_name, binary=False)\n         fn_cache_manager.put(json.dumps(metadata), data_name, binary=False)\n \n-    return CompiledKernel(name, so_cache_manager._make_path(so_name), fn_cache_manager.cache_dir)\n+    if warm_cache_only:\n+        return  # load_binary() requires a valid cuda context\n+\n+    return CompiledKernel(name, so_cache_manager._make_path(so_name), fn_cache_manager.cache_dir, device)\n \n \n class CompiledKernel:\n \n-    def __init__(self, fn_name, so_path, cache_dir):\n+    def __init__(self, fn_name, so_path, cache_dir, device):\n         # initialize launcher\n         import importlib.util\n         spec = importlib.util.spec_from_file_location(\"launcher\", so_path)\n@@ -1239,7 +1247,6 @@ def __init__(self, fn_name, so_path, cache_dir):\n         with open(os.path.join(cache_dir, f\"{fn_name}.ptx\"), \"r\") as f:\n             self.asm[\"ptx\"] = f.read()\n \n-        device = torch.cuda.current_device()\n         mod, func, n_regs, n_spills = _triton.code_gen.load_binary(metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n         self.cu_module = mod\n         self.cu_function = func"}]