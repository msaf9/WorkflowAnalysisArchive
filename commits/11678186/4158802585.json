[{"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 18, "deletions": 17, "changes": 35, "file_content_changes": "@@ -16,12 +16,12 @@\n # of sequential models (e.g., Transformers) or neural networks with small batch size.\n # It takes a vector :math:`x` as input and produces a vector :math:`y` of the same shape as output.\n # The normalization is performed by subtracting the mean and dividing by the standard deviation of :math:`x`.\n-# After the normalization, a learnable linear transformation with weights :math:`w` and biases :math:`b` is applied. \n+# After the normalization, a learnable linear transformation with weights :math:`w` and biases :math:`b` is applied.\n # The forward pass can be expressed as follows:\n-# \n+#\n # .. math::\n #    y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\n-# \n+#\n # where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n # Let\u2019s first take a look at the foward pass implementation.\n \n@@ -51,7 +51,7 @@ def _layer_norm_fwd_fused(\n     N,  # number of columns in X\n     eps,  # epsilon to avoid division by zero\n     BLOCK_SIZE: tl.constexpr,\n-):  \n+):\n     # Map the program id to the row of X and Y it should compute.\n     row = tl.program_id(0)\n     Y += row * stride\n@@ -98,23 +98,23 @@ def _layer_norm_fwd_fused(\n #\n # .. math::\n #    \\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\n-# \n+#\n # where :math:`\\odot` denotes the element-wise multiplication, :math:`\\cdot` denotes the dot product, and :math:`\\sigma` is the standard deviation.\n # :math:`c_1` and :math:`c_2` are intermediate constants that improve the readability of the following implementation.\n-# \n+#\n # For the weights :math:`w` and biases :math:`b`, the VJPs :math:`\\nabla_{w}` and :math:`\\nabla_{b}` are more straightforward:\n-# \n+#\n # .. math::\n #    \\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\n #\n # Since the same weights :math:`w` and biases :math:`b` are used for all rows in the same batch, their gradients need to sum up.\n-# To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates \n+# To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n # partial :math:`\\nabla_{w}` and :math:`\\nabla_{b}` across certain rows into one of :math:`\\text{GROUP_SIZE_M}` independent buffers.\n # These buffers stay in the L2 cache and then are further reduced by another function to compute the actual :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n-# \n+#\n # Let the number of input rows :math:`M = 4` and :math:`\\text{GROUP_SIZE_M} = 2`,\n # here's a diagram of the parallel reduction strategy for :math:`\\nabla_{w}` (:math:`\\nabla_{b}` is omitted for brevity):\n-# \n+#\n #   .. image:: parallel_reduction.png\n #\n # In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n@@ -136,7 +136,7 @@ def _layer_norm_bwd_dx_fused(\n     stride,  # how much to increase the pointer when moving by 1 row\n     N,  # number of columns in X\n     eps,  # epsilon to avoid division by zero\n-    GROUP_SIZE_M: tl.constexpr, \n+    GROUP_SIZE_M: tl.constexpr,\n     BLOCK_SIZE_N: tl.constexpr\n ):\n     # Map the program id to the elements of X, DX, and DY it should compute.\n@@ -148,9 +148,9 @@ def _layer_norm_bwd_dx_fused(\n     DX += row * stride\n     # Offset locks and weights/biases gradient pointer for parallel reduction\n     lock_id = row % GROUP_SIZE_M\n-    Lock += lock_id               \n+    Lock += lock_id\n     Count = Lock + GROUP_SIZE_M\n-    DW = DW + lock_id * N + cols \n+    DW = DW + lock_id * N + cols\n     DB = DB + lock_id * N + cols\n     # Load data to SRAM\n     x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n@@ -185,15 +185,16 @@ def _layer_norm_bwd_dx_fused(\n     # Release the lock\n     tl.atomic_xchg(Lock, 0)\n \n+\n @triton.jit\n def _layer_norm_bwd_dwdb(\n-    DW,  # pointer to the partial sum of weights gradient \n+    DW,  # pointer to the partial sum of weights gradient\n     DB,  # pointer to the partial sum of biases gradient\n     FINAL_DW,  # pointer to the weights gradient\n     FINAL_DB,  # pointer to the biases gradient\n     M,  # GROUP_SIZE_M\n     N,  # number of columns\n-    BLOCK_SIZE_M: tl.constexpr, \n+    BLOCK_SIZE_M: tl.constexpr,\n     BLOCK_SIZE_N: tl.constexpr\n ):\n     # Map the program id to the elements of DW and DB it should compute.\n@@ -218,7 +219,7 @@ def _layer_norm_bwd_dwdb(\n # %%\n # Benchmark\n # ---------------------------------\n-# We can now compare the performance of our kernel against that of PyTorch. \n+# We can now compare the performance of our kernel against that of PyTorch.\n # Here we focus on inputs that have Less than 64KB per feature.\n # Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n \n@@ -363,4 +364,4 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n # References\n # --------------\n #\n-# .. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016\n\\ No newline at end of file\n+# .. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016"}]