[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -289,7 +289,7 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n }\n \n def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n-                               SameOperandsAndResultElementType]> {\n+                                 SameOperandsAndResultElementType]> {\n \n     let summary = \"transpose a tensor\";\n "}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -25,13 +25,14 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n   if (maybeSharedAllocationOp(op)) {\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n-    // FIXME(Keren): extract and insert are always alias for now\n+    // XXX(Keren): the following ops are always aliasing for now\n     if (isa<tensor::ExtractSliceOp, triton::TransOp>(op)) {\n       // extract_slice %src\n+      // trans %src\n       aliasInfo = AliasInfo(operands[0]->getValue());\n       pessimistic = false;\n-    } else if (isa<tensor::InsertSliceOp>(op) ||\n-               isa<triton::gpu::InsertSliceAsyncOp>(op)) {\n+    } else if (isa<tensor::InsertSliceOp, triton::gpu::InsertSliceAsyncOp>(\n+                   op)) {\n       // insert_slice_async %src, %dst, %index\n       // insert_slice %src into %dst[%offsets]\n       aliasInfo = AliasInfo(operands[1]->getValue());"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 17, "deletions": 3, "changes": 20, "file_content_changes": "@@ -298,10 +298,24 @@ class AllocationAnalysis {\n \n   /// Resolves liveness of all values involved under the root operation.\n   void resolveLiveness() {\n-    // In the SCF dialect, we always have a sequentially nested structure of\n-    // blocks\n+    // Assign an ID to each operation using post-order traversal.\n+    // To achieve the correct liveness range, the parent operation's ID\n+    // should be greater than each of its child operation's ID .\n+    // Example:\n+    //     ...\n+    //     %5 = triton.convert_layout %4\n+    //     %6 = scf.for ... iter_args(%arg0 = %0) -> (i32) {\n+    //       %2 = triton.convert_layout %5\n+    //       ...\n+    //       scf.yield %arg0\n+    //     }\n+    // For example, %5 is defined in the parent region and used in\n+    // the child region, and is not passed as a block argument.\n+    // %6 should should have an ID greater than its child operations,\n+    // otherwise %5 liveness range ends before the child operation's liveness\n+    // range ends.\n     DenseMap<Operation *, size_t> operationId;\n-    operation->walk<WalkOrder::PreOrder>(\n+    operation->walk<WalkOrder::PostOrder>(\n         [&](Operation *op) { operationId[op] = operationId.size(); });\n \n     // Analyze liveness of explicit buffers"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -52,6 +52,15 @@ func @convert(%A : !tt.ptr<f16>) {\n   return\n }\n \n+// CHECK-LABEL: trans\n+func @trans(%A : !tt.ptr<f16>) {\n+  // CHECK: %cst -> %cst\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+  // CHECK: %0 -> %cst\n+  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  return\n+}\n+\n // CHECK-LABEL: insert_slice_async\n func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -174,6 +174,14 @@ func @scratch() {\n   // CHECK-NEXT: size = 512\n }\n \n+// CHECK-LABEL: trans\n+func @trans(%A : !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 1024\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+  %b = tt.trans %tensor : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  return\n+}\n+\n // CHECK-LABEL: insert_slice_async\n func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n@@ -285,6 +293,25 @@ func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %\n   // CHECK-NEXT: size = 24576\n }\n \n+// c0 cannot be released in the loop\n+// CHECK-LABEL: for_use_ancestor\n+func @for_use_ancestor(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  // CHECK: offset = 0, size = 8192\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 8192, size = 8192\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 16384, size = 8192\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c0 = tt.trans %c_shared_init : (tensor<128x32xf16, #A_SHARED>) -> tensor<32x128xf16, #A_SHARED>\n+    // CHECK-NEXT: offset = 24576, size = 8192\n+    %c1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+    scf.yield %b_shared, %a_shared: tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  return\n+  // CHECK-NEXT: size = 32768\n+}\n+\n // a_shared_init, b_shared_init, and c_shared_init's liveness ranges are span over the entire function before cst2.\n // So they cannot be reused by cst0 and cst1, but can be reused by cst2.\n // CHECK-LABEL: for_if_for"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -111,6 +111,13 @@ func @extract_slice() {\n   return\n }\n \n+// CHECK-LABEL: trans\n+func @trans() {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #A_SHARED>\n+  %b = tt.trans %cst0 : (tensor<16x32xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+  return\n+}\n+\n // CHECK-LABEL: insert_slice_async\n func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>"}]