[{"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -40,7 +40,8 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n   if (TensorType ty = value.getType().dyn_cast<TensorType>())\n     rank = ty.getRank();\n   int divHint = 1;\n-  if (BlockArgument blockArg = value.dyn_cast<BlockArgument>()) {\n+  BlockArgument blockArg = value.dyn_cast<BlockArgument>();\n+  if (blockArg && blockArg.getOwner()->isEntryBlock()) {\n     Operation *op = blockArg.getOwner()->getParentOp();\n     if (FuncOp fun = dyn_cast<FuncOp>(op)) {\n       Attribute attr ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1867,8 +1867,8 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n     auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n         linearIdxInNanoTile, srcBlockedLayout.getSizePerThread());\n-    multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n     unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n+    multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n     unsigned wordVecIdx =\n         getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep);\n     wordVecs[wordVecIdx] ="}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 72, "deletions": 23, "changes": 95, "file_content_changes": "@@ -1,13 +1,13 @@\n-# import pytest\n-# import torch\n-# from torch.testing import assert_close\n+import pytest\n+import torch\n+from torch.testing import assert_close\n \n import triton\n import triton.language as tl\n \n \n @triton.jit\n-def matmul_kernel(\n+def matmul_no_scf_kernel(\n     a_ptr, b_ptr, c_ptr,\n     stride_am, stride_ak,\n     stride_bk, stride_bn,\n@@ -30,23 +30,72 @@ def matmul_kernel(\n # TODO: num_warps could only be 4 for now\n \n \n-# @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n-#     [128, 256, 32, 4],\n-#     [256, 128, 16, 4],\n-#     [128, 16, 32, 4],\n-#     [32, 128, 64, 4],\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n+    [128, 256, 32, 4],\n+    [256, 128, 16, 4],\n+    [128, 16, 32, 4],\n+    [32, 128, 64, 4],\n+])\n+def test_gemm_no_scf(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n+    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n+    grid = lambda META: (1, )\n+    matmul_no_scf_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                               stride_am=a.stride(0), stride_ak=a.stride(1),\n+                               stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                               stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                               M=SIZE_M, N=SIZE_N, K=SIZE_K,\n+                               num_warps=NUM_WARPS)\n+    golden = torch.matmul(a, b)\n+    torch.set_printoptions(profile=\"full\")\n+    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n+\n+\n+@triton.jit\n+def matmul_kernel(\n+    a_ptr, b_ptr, c_ptr,\n+    stride_am, stride_ak,\n+    stride_bk, stride_bn,\n+    stride_cm, stride_cn,\n+    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n+    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+):\n+    offs_m = tl.arange(0, BLOCK_SIZE_M)\n+    offs_n = tl.arange(0, BLOCK_SIZE_N)\n+    offs_k = tl.arange(0, BLOCK_SIZE_K)\n+    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n+    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n+    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    for k in range(0, K, BLOCK_SIZE_K):\n+        a = tl.load(a_ptrs)\n+        b = tl.load(b_ptrs)\n+        accumulator += tl.dot(a, b)\n+        a_ptrs += BLOCK_SIZE_K * stride_ak\n+        b_ptrs += BLOCK_SIZE_K * stride_bk\n+\n+    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n+    tl.store(c_ptrs, accumulator)\n+\n+# TODO: DotConversion in TritonGPUToLLVM cannot support non-splat C for the moment\n+# @pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K', [\n+#    [128, 256, 128, 4, 128, 256, 32],\n+#    # [256, 128, 64, 4, 256, 128, 16],\n+#    # [128, 16, 128, 4, 128, 16, 32],\n+#    # [32, 128, 256, 4, 32, 128, 64],\n # ])\n-# def test_gemm_impl(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n-#     a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n-#     b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n-#     c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n-#     grid = lambda META: (1, )\n-#     matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n-#                         stride_am=a.stride(0), stride_ak=a.stride(1),\n-#                         stride_bk=b.stride(0), stride_bn=b.stride(1),\n-#                         stride_cm=c.stride(0), stride_cn=c.stride(1),\n-#                         M=SIZE_M, N=SIZE_N, K=SIZE_K,\n-#                         num_warps=NUM_WARPS)\n-#     golden = torch.matmul(a, b)\n-#     torch.set_printoptions(profile=\"full\")\n-#     assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)\n+# def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K):\n+#    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+#    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+#    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n+#    grid = lambda META: (1, )\n+#    matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+#                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+#                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+#                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+#                        M=a.shape[0], N=b.shape[1], K=a.shape[1],\n+#                        BLOCK_SIZE_M=BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K,\n+#                        num_warps=NUM_WARPS)\n+#    golden = torch.matmul(a, b)\n+#    torch.set_printoptions(profile=\"full\")\n+#    assert_close(c, golden, rtol=1e-3, atol=1e-3, check_dtype=False)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -558,7 +558,10 @@ def visit_For(self, node):\n             raise RuntimeError('Only `range` iterator currently supported')\n         # static for loops: all iterator arguments are constexpr\n         iter_args = [self.visit(arg) for arg in node.iter.args]\n-        is_static = all([isinstance(x, triton.language.constexpr) for x in iter_args])\n+        static_unrolling = os.environ.get('TRITON_STATIC_LOOP_UNROLLING', False)\n+        is_static = False\n+        if static_unrolling:\n+            is_static = all([isinstance(x, triton.language.constexpr) for x in iter_args])\n         if is_static:\n             iter_args = [arg.value for arg in iter_args]\n             range = iterator(*iter_args)"}]