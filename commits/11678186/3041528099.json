[{"filename": "python/src/triton.cc", "status": "modified", "additions": 293, "deletions": 357, "changes": 650, "file_content_changes": "@@ -26,317 +26,280 @@ namespace py = pybind11;\n namespace ir = triton::ir;\n namespace drv = triton::driver;\n \n+\n /*****************************************************************************/\n /* Python bindings for triton::driver                                        */\n /*****************************************************************************/\n // information query\n-template <CUdevice_attribute attr>\n-int cuGetInfo(CUdevice device)\n-{\n+template<CUdevice_attribute attr>\n+int cuGetInfo(CUdevice device) {\n   int res;\n   drv::dispatch::cuDeviceGetAttribute(&res, attr, device);\n   return res;\n }\n \n-template <hipDeviceAttribute_t attr>\n-int hipGetInfo(hipDevice_t device)\n-{\n+template<hipDeviceAttribute_t attr>\n+int hipGetInfo(hipDevice_t device) {\n   int res;\n   drv::dispatch::hipDeviceGetAttribute(&res, attr, device);\n   return res;\n }\n \n-enum backend_t\n-{\n+enum backend_t {\n   HOST,\n   CUDA,\n   ROCM,\n };\n \n-void cu_enable_peer_access(uint64_t peer_ptr)\n-{\n+void cu_enable_peer_access(uint64_t peer_ptr){\n   CUcontext context;\n   drv::dispatch::cuPointerGetAttribute(&context, CU_POINTER_ATTRIBUTE_CONTEXT, peer_ptr);\n-  try\n-  {\n-    drv::dispatch::cuCtxEnablePeerAccess(context, 0);\n-  }\n-  catch (drv::exception::cuda::peer_access_already_enabled)\n-  {\n-  }\n+  try {\n+      drv::dispatch::cuCtxEnablePeerAccess(context, 0);\n+  } catch (drv::exception::cuda::peer_access_already_enabled) {}\n }\n \n void host_enqueue(uint64_t stream, uint64_t kernel,\n                   uint64_t grid_0, uint64_t grid_1, uint64_t grid_2,\n                   uint64_t block_0, uint64_t block_1, uint64_t block_2,\n-                  void *args_ptr, size_t args_size, int64_t shared_mem)\n-{\n+                  void* args_ptr, size_t args_size, int64_t shared_mem){\n   throw std::runtime_error(\"unsupported\");\n-  // auto hst = kernel->module()->hst();\n-  // hst_->futures->reserve(hst_->futures->size() + grid[0]*grid[1]*grid[2]);\n-  // char* params = new char[args_size];\n-  // std::memcpy((void*)params, (void*)args, args_size);\n-  // for(size_t i = 0; i < grid[0]; i++)\n-  //   for(size_t j = 0; j < grid[1]; j++)\n-  //     for(size_t k = 0; k < grid[2]; k++)\n-  //       hst_->futures->emplace_back(hst_->pool->enqueue(hst->fn, (char**)params, int32_t(i), int32_t(j), int32_t(k)));\n+// auto hst = kernel->module()->hst();\n+// hst_->futures->reserve(hst_->futures->size() + grid[0]*grid[1]*grid[2]);\n+// char* params = new char[args_size];\n+// std::memcpy((void*)params, (void*)args, args_size);\n+// for(size_t i = 0; i < grid[0]; i++)\n+//   for(size_t j = 0; j < grid[1]; j++)\n+//     for(size_t k = 0; k < grid[2]; k++)\n+//       hst_->futures->emplace_back(hst_->pool->enqueue(hst->fn, (char**)params, int32_t(i), int32_t(j), int32_t(k)));\n }\n \n void cu_enqueue(uint64_t stream, uint64_t kernel,\n                 uint64_t grid_0, uint64_t grid_1, uint64_t grid_2,\n                 uint64_t block_0, uint64_t block_1, uint64_t block_2,\n-                void *args_ptr, size_t args_size, int64_t shared_mem)\n-{\n+                void* args_ptr, size_t args_size, int64_t shared_mem){\n   void *config[] = {\n-      CU_LAUNCH_PARAM_BUFFER_POINTER, (void *)args_ptr,\n-      CU_LAUNCH_PARAM_BUFFER_SIZE, &args_size,\n-      CU_LAUNCH_PARAM_END};\n-  drv::dispatch::cuLaunchKernel((CUfunction)kernel, grid_0, grid_1, grid_2,\n-                                block_0, block_1, block_2,\n+      CU_LAUNCH_PARAM_BUFFER_POINTER, (void*)args_ptr,\n+      CU_LAUNCH_PARAM_BUFFER_SIZE,    &args_size,\n+      CU_LAUNCH_PARAM_END\n+  };\n+  drv::dispatch::cuLaunchKernel((CUfunction)kernel, grid_0, grid_1, grid_2, \n+                                block_0, block_1, block_2, \n                                 shared_mem, (CUstream)stream, nullptr, config);\n }\n \n void hip_enqueue(uint64_t stream, uint64_t kernel,\n-                 uint64_t grid_0, uint64_t grid_1, uint64_t grid_2,\n-                 uint64_t block_0, uint64_t block_1, uint64_t block_2,\n-                 void *args_ptr, size_t args_size, int64_t shared_mem)\n-{\n+                uint64_t grid_0, uint64_t grid_1, uint64_t grid_2,\n+                uint64_t block_0, uint64_t block_1, uint64_t block_2,\n+                void* args_ptr, size_t args_size, int64_t shared_mem) {\n   void *config[] = {\n-      HIP_LAUNCH_PARAM_BUFFER_POINTER, (void *)args_ptr,\n-      HIP_LAUNCH_PARAM_BUFFER_SIZE, &args_size,\n-      HIP_LAUNCH_PARAM_END};\n-  drv::dispatch::hipModuleLaunchKernel((hipFunction_t)kernel, grid_0, grid_1, grid_2,\n-                                       block_0, block_1, block_2,\n-                                       shared_mem, (hipStream_t)stream, nullptr, config);\n+      HIP_LAUNCH_PARAM_BUFFER_POINTER, (void*)args_ptr,\n+      HIP_LAUNCH_PARAM_BUFFER_SIZE,    &args_size,\n+      HIP_LAUNCH_PARAM_END\n+  };\n+  drv::dispatch::hipModuleLaunchKernel((hipFunction_t)kernel, grid_0, grid_1, grid_2, \n+                                block_0, block_1, block_2, \n+                                shared_mem, (hipStream_t)stream, nullptr, config);\n+\n }\n \n-long pow2_divisor(long N)\n-{\n-  if (N % 16 == 0)\n-    return 16;\n-  if (N % 8 == 0)\n-    return 8;\n-  if (N % 4 == 0)\n-    return 4;\n-  if (N % 2 == 0)\n-    return 2;\n-  return 1;\n+long pow2_divisor(long N){\n+    if(N % 16 == 0) return 16;\n+    if(N % 8 == 0) return 8;\n+    if(N % 4 == 0) return 4;\n+    if(N % 2 == 0) return 2;\n+    return 1;\n }\n \n // Returns something like \"int16\", whether dtype is a torch.dtype or\n // triton.language.dtype.\n-std::string dtype_cache_key_part(const py::object &dtype)\n-{\n-  if (py::hasattr(dtype, \"cache_key_part\"))\n-  {\n+std::string dtype_cache_key_part(const py::object& dtype) {\n+  if (py::hasattr(dtype, \"cache_key_part\")) {\n     // Presumed to be a triton.language.dtype.\n     return std::string(py::str(py::getattr(dtype, \"cache_key_part\")));\n-  }\n-  else\n-  {\n+  } else {\n     // Remove 'torch.' prefix from repr of torch.dtype.\n     py::object repr = py::repr(dtype);\n     size_t repr_len = PyUnicode_GET_LENGTH(repr.ptr());\n-    const char *repr_ptr = (const char *)PyUnicode_1BYTE_DATA(repr.ptr());\n-    if (repr_len <= 6 || strncmp(repr_ptr, \"torch.\", 6))\n-    {\n+    const char* repr_ptr = (const char*)PyUnicode_1BYTE_DATA(repr.ptr());\n+    if (repr_len <= 6 || strncmp(repr_ptr, \"torch.\", 6)) {\n       throw std::logic_error(\"invalid dtype: \" + std::string(repr_ptr, repr_len));\n     }\n     return std::string(repr_ptr + 6, repr_len - 6);\n   }\n }\n \n-size_t get_pointer_range_size(uint64_t addr)\n-{\n-  if (addr == 0)\n+size_t get_pointer_range_size(uint64_t addr){\n+  if(addr == 0)\n     return 0;\n   size_t size;\n   drv::dispatch::cuPointerGetAttribute(&size, CU_POINTER_ATTRIBUTE_RANGE_SIZE, (CUdeviceptr)addr);\n   return size;\n }\n \n // Launch\n-void parse_args(py::list &args, py::list do_not_specialize, const std::string &func_key, py::list &arg_names,\n-                std::string &cache_key, std::string &params, size_t &params_size, py::dict constants,\n-                int num_warps, int num_stages, py::dict &extern_libs)\n-{\n-  size_t len = PyList_Size(args.ptr());\n-  params.reserve(8 * len); // 8 max bytes by argument\n-  char *params_ptr = &params[0];\n-  cache_key = func_key;\n-  cache_key += \"-\" + std::to_string(num_warps);\n-  cache_key += \"-\" + std::to_string(num_stages);\n-  cache_key += \"-\";\n-  for (int i = 0; i < len; i++)\n-  {\n-    cache_key += \"_\";\n-    py::int_ py_i = py::int_(i);\n-    bool specialize = !do_not_specialize.contains(py_i);\n-    py::object arg = args[i];\n-    auto arg_ptr = arg.ptr();\n-\n-    // argument is `long`\n-    if (PyLong_Check(arg_ptr))\n-    {\n-      int overflow;\n-      long long value = PyLong_AsLongLongAndOverflow(arg_ptr, &overflow);\n-      // values equal to 1 are specialized\n-      if (specialize && (value == 1))\n-      {\n-        cache_key += \"1\";\n+void parse_args(py::list& args, py::list do_not_specialize, const std::string& func_key, py::list& arg_names,\n+                std::string& cache_key, std::string& params, size_t& params_size, py::dict constants,\n+                int num_warps, int num_stages, py::dict& extern_libs) {\n+    size_t len = PyList_Size(args.ptr());\n+    params.reserve(8*len); // 8 max bytes by argument\n+    char* params_ptr = &params[0];\n+    cache_key = func_key;\n+    cache_key += \"-\" + std::to_string(num_warps);\n+    cache_key += \"-\" + std::to_string(num_stages);\n+    cache_key += \"-\";\n+    for(int i = 0; i < len; i++){\n+      cache_key += \"_\";\n+      py::int_ py_i = py::int_(i);\n+      bool specialize = !do_not_specialize.contains(py_i);\n+      py::object arg = args[i];\n+      auto arg_ptr = arg.ptr();\n+\n+      // argument is `long`\n+      if(PyLong_Check(arg_ptr)){\n+        int overflow;\n+        long long value = PyLong_AsLongLongAndOverflow(arg_ptr, &overflow);\n+        // values equal to 1 are specialized\n+        if(specialize && (value == 1)){\n+          cache_key += \"1\";\n+          continue;\n+        }\n+        // int32, uint32, int64, and uint64 have different kernels\n+        if (!overflow && -0x8000'0000LL <= value && value <= 0x7FFF'FFFFLL) {\n+          cache_key += \"int32\";\n+          params_ptr = (char*)(((uintptr_t)params_ptr + 3) & (-4));\n+          std::memcpy(params_ptr, &value, 4);\n+          params_ptr += 4;\n+        } else if (!overflow && 0x8000'0000LL <= value && value <= 0xFFFF'FFFFLL) {\n+          cache_key += \"uint32\";\n+          params_ptr = (char*)(((uintptr_t)params_ptr + 3) & (-4));\n+          std::memcpy(params_ptr, &value, 4);\n+          params_ptr += 4;\n+        } else if (!overflow) {\n+          cache_key += \"int64\";\n+          params_ptr = (char*)(((uintptr_t)params_ptr + 7) & (-8));\n+          std::memcpy(params_ptr, &value, 8);\n+          params_ptr += 8;\n+        } else {\n+          if (PyErr_Occurred()) {\n+            throw std::logic_error(\"An error occurred?\");\n+          }\n+          unsigned long long unsigned_value = PyLong_AsUnsignedLongLong(arg_ptr);\n+          if (PyErr_Occurred()) {\n+            throw std::runtime_error(\"integer overflow in argument: \" + std::string(py::str(arg)));\n+          }\n+          cache_key += \"uint64\";\n+          params_ptr = (char*)(((uintptr_t)params_ptr + 7) & (-8));\n+          std::memcpy(params_ptr, &unsigned_value, 8);\n+          params_ptr += 8;\n+        }\n+        if(!specialize)\n+          continue;\n+        // values divisible by small powers of 2 are specialized\n+        cache_key += \"[multipleof(\";\n+        cache_key += std::to_string(pow2_divisor(value));\n+        cache_key += \")]\";\n         continue;\n       }\n-      // int32, uint32, int64, and uint64 have different kernels\n-      if (!overflow && -0x8000'0000LL <= value && value <= 0x7FFF'FFFFLL)\n-      {\n-        cache_key += \"int32\";\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 3) & (-4));\n+      // argument is `float`\n+      if(PyFloat_Check(arg_ptr)){\n+        cache_key += \"float32\";\n+        float value = PyFloat_AsDouble(arg_ptr);\n+        params_ptr = (char*)(((uintptr_t)params_ptr + 3) & (-4));\n         std::memcpy(params_ptr, &value, 4);\n         params_ptr += 4;\n+        continue;\n       }\n-      else if (!overflow && 0x8000'0000LL <= value && value <= 0xFFFF'FFFFLL)\n-      {\n-        cache_key += \"uint32\";\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 3) & (-4));\n-        std::memcpy(params_ptr, &value, 4);\n-        params_ptr += 4;\n+      // argument is `bool`\n+      if(PyBool_Check(arg_ptr)){\n+        cache_key += \"bool\";\n+        bool value =  arg_ptr == Py_True ? true : false;\n+        std::memcpy(params_ptr, &value, 1);\n+        params_ptr += 1;\n+        continue;\n       }\n-      else if (!overflow)\n-      {\n-        cache_key += \"int64\";\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 7) & (-8));\n+      // argument is tensor\n+      if(py::hasattr(arg, \"data_ptr\")){\n+        py::object data_ptr = arg.attr(\"data_ptr\")();\n+        long value = data_ptr.cast<long>();\n+        params_ptr = (char*)(((uintptr_t)params_ptr + 7) & (-8));\n+        // copy param\n         std::memcpy(params_ptr, &value, 8);\n         params_ptr += 8;\n+        // udpate cache key\n+        cache_key += dtype_cache_key_part(arg.attr(\"dtype\"));\n+        cache_key += \"*\";\n+        cache_key += \"[multipleof(\";\n+        size_t range_size = get_pointer_range_size(value);\n+        cache_key += std::to_string(std::min(pow2_divisor(value), pow2_divisor(range_size)));\n+        cache_key += \")]\";\n+        continue;\n       }\n-      else\n-      {\n-        if (PyErr_Occurred())\n-        {\n-          throw std::logic_error(\"An error occurred?\");\n-        }\n-        unsigned long long unsigned_value = PyLong_AsUnsignedLongLong(arg_ptr);\n-        if (PyErr_Occurred())\n-        {\n-          throw std::runtime_error(\"integer overflow in argument: \" + std::string(py::str(arg)));\n+      // argument is `constexpr`\n+      if (py::hasattr(arg, \"value\")) {\n+        py::object value = arg.attr(\"value\");\n+        // check if value is a callable object using PyCallable_Check\n+        if (PyCallable_Check(value.ptr())) {\n+          throw std::runtime_error(\n+              \"constant argument cannot be a callable object: \" +\n+              std::string(py::str(arg)));\n         }\n-        cache_key += \"uint64\";\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 7) & (-8));\n-        std::memcpy(params_ptr, &unsigned_value, 8);\n-        params_ptr += 8;\n+        py::object name = arg_names[i];\n+        constants[name] = value;\n+        py::object repr = py::repr(value);\n+        const char* start = (const char*)PyUnicode_1BYTE_DATA(repr.ptr());\n+        size_t len = PyUnicode_GET_LENGTH(repr.ptr());\n+        cache_key += std::string(start, len);\n+        continue;\n       }\n-      if (!specialize)\n+      std::string ty_str = arg.attr(\"__class__\").attr(\"__name__\").cast<std::string>();\n+      if(ty_str == \"NoneType\"){\n+        cache_key += \"None\";\n         continue;\n-      // values divisible by small powers of 2 are specialized\n-      cache_key += \"[multipleof(\";\n-      cache_key += std::to_string(pow2_divisor(value));\n-      cache_key += \")]\";\n-      continue;\n-    }\n-    // argument is `float`\n-    if (PyFloat_Check(arg_ptr))\n-    {\n-      cache_key += \"float32\";\n-      float value = PyFloat_AsDouble(arg_ptr);\n-      params_ptr = (char *)(((uintptr_t)params_ptr + 3) & (-4));\n-      std::memcpy(params_ptr, &value, 4);\n-      params_ptr += 4;\n-      continue;\n-    }\n-    // argument is `bool`\n-    if (PyBool_Check(arg_ptr))\n-    {\n-      cache_key += \"bool\";\n-      bool value = arg_ptr == Py_True ? true : false;\n-      std::memcpy(params_ptr, &value, 1);\n-      params_ptr += 1;\n-      continue;\n-    }\n-    // argument is tensor\n-    if (py::hasattr(arg, \"data_ptr\"))\n-    {\n-      py::object data_ptr = arg.attr(\"data_ptr\")();\n-      long value = data_ptr.cast<long>();\n-      params_ptr = (char *)(((uintptr_t)params_ptr + 7) & (-8));\n-      // copy param\n-      std::memcpy(params_ptr, &value, 8);\n-      params_ptr += 8;\n-      // udpate cache key\n-      cache_key += dtype_cache_key_part(arg.attr(\"dtype\"));\n-      cache_key += \"*\";\n-      cache_key += \"[multipleof(\";\n-      size_t range_size = get_pointer_range_size(value);\n-      cache_key += std::to_string(std::min(pow2_divisor(value), pow2_divisor(range_size)));\n-      cache_key += \")]\";\n-      continue;\n-    }\n-    // argument is `constexpr`\n-    if (py::hasattr(arg, \"value\"))\n-    {\n-      py::object value = arg.attr(\"value\");\n-      // check if value is a callable object using PyCallable_Check\n-      if (PyCallable_Check(value.ptr()))\n-      {\n-        throw std::runtime_error(\n-            \"constant argument cannot be a callable object: \" +\n-            std::string(py::str(arg)));\n       }\n-      py::object name = arg_names[i];\n-      constants[name] = value;\n-      py::object repr = py::repr(value);\n-      const char *start = (const char *)PyUnicode_1BYTE_DATA(repr.ptr());\n-      size_t len = PyUnicode_GET_LENGTH(repr.ptr());\n-      cache_key += std::string(start, len);\n-      continue;\n-    }\n-    std::string ty_str = arg.attr(\"__class__\").attr(\"__name__\").cast<std::string>();\n-    if (ty_str == \"NoneType\")\n-    {\n-      cache_key += \"None\";\n-      continue;\n+      std::string err_msg = \"Received type '\" + ty_str + \"' for argument \" + std::to_string(i) + \".\"\n+                            + \" Only int, float, bool, torch.Tensor, and triton.language.constexpr are supported.\";\n+      throw std::runtime_error(err_msg);\n     }\n-    std::string err_msg = \"Received type '\" + ty_str + \"' for argument \" + std::to_string(i) + \".\" + \" Only int, float, bool, torch.Tensor, and triton.language.constexpr are supported.\";\n-    throw std::runtime_error(err_msg);\n-  }\n   params_size = (std::ptrdiff_t)(params_ptr - &params[0]);\n \n-  for (auto item : extern_libs)\n-  {\n+  for (auto item : extern_libs) {\n     cache_key += \"-\" + item.first.cast<std::string>();\n     cache_key += \"_\" + item.second.cast<std::string>();\n   }\n }\n \n //\n \n-void init_triton_runtime(py::module &&m)\n-{\n+void init_triton_runtime(py::module &&m) {\n \n   // m.def(\"current_stream\", [](uint64_t device){\n   //   return (uint64_t)(c10::cuda::getCurrentCUDAStream(device).stream());\n   // });\n \n   // wrap backend_t\n   py::enum_<backend_t>(m, \"backend\")\n-      .value(\"HOST\", HOST)\n-      .value(\"CUDA\", CUDA)\n-      .value(\"ROCM\", ROCM)\n-      .export_values();\n+    .value(\"HOST\", HOST)\n+    .value(\"CUDA\", CUDA)\n+    .value(\"ROCM\", ROCM)\n+    .export_values();\n \n   // enable peer-to-peer\n-  m.def(\"enable_peer_access\", [](backend_t backend, uint64_t peer_ptr)\n-        {\n+  m.def(\"enable_peer_access\", [](backend_t backend, uint64_t peer_ptr) {\n       if (backend != CUDA)\n         throw std::runtime_error(\"P2P only supported on CUDA devices!\");\n-      cu_enable_peer_access(peer_ptr); });\n+      cu_enable_peer_access(peer_ptr);\n+    }\n+  );\n \n   // get range size for the given pointer\n   m.def(\"get_pointer_range_size\", &get_pointer_range_size);\n \n+\n   // cache key\n-  m.def(\"launch\", [](py::list args, py::list do_not_specialize, const std::string &func_key, py::list &arg_names,\n-                     py::object device, py::int_ stream, py::dict bin_cache, py::int_ num_warps, py::int_ num_stages,\n-                     py::dict extern_libs, py::function add_to_cache, py::object grid)\n-        {\n+  m.def(\"launch\", [](py::list args, py::list do_not_specialize, const std::string& func_key, py::list& arg_names, \n+                     py::object device, py::int_ stream, py::dict bin_cache, py::int_ num_warps, py::int_ num_stages, \n+                     py::dict extern_libs, py::function add_to_cache, py::object grid){\n     // parse arguments to compute cache key, compile-time constants and packed kernel arguments\n     long _num_warps = PyLong_AsLong(num_warps.ptr());\n     long _num_stages = PyLong_AsLong(num_stages.ptr());\n@@ -387,60 +350,60 @@ void init_triton_runtime(py::module &&m)\n                                     _num_warps*32, 1, 1, shared_mem, (CUstream)_stream, \n                                      nullptr, config);\n    }\n-    return bin; });\n+    return bin;\n+  });\n \n-  m.def(\"cc\", [](backend_t backend, uint64_t device) -> int\n-        {\n+  m.def(\"cc\", [](backend_t backend, uint64_t device) -> int {\n     if (backend == CUDA) {\n       CUdevice dev = (CUdevice)device;\n       int major = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR>(dev);\n       int minor = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR>(dev);\n       return major*10 + minor;\n     }\n-    return -1; });\n+    return -1;\n+  });\n \n   // query maximum shared memory\n-  m.def(\"max_shared_memory\", [](backend_t backend, uint64_t device)\n-        {\n+  m.def(\"max_shared_memory\", [](backend_t backend, uint64_t device) {\n       if (backend == HOST)\n         return 0;\n       if(backend == CUDA) \n         return cuGetInfo<CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN>(device);\n       if(backend == ROCM)\n         return hipGetInfo<hipDeviceAttributeMaxSharedMemoryPerBlock>(device);\n-      return -1; });\n+      return -1;\n+  });\n \n   // query DRAM & L2 cache\n-  m.def(\"memory_clock_rate\", [](backend_t backend, uint64_t device)\n-        {\n+  m.def(\"memory_clock_rate\", [](backend_t backend, uint64_t device) {\n     if (backend == CUDA) return cuGetInfo<CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE>(device);\n-    return -1; });\n-  m.def(\"global_memory_bus_width\", [](backend_t backend, uint64_t device)\n-        {\n+    return -1;\n+  });\n+  m.def(\"global_memory_bus_width\", [](backend_t backend, uint64_t device) {\n     if (backend == CUDA) return cuGetInfo<CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH>(device);\n-    return -1; });\n-  m.def(\"l2_cache_size\", [](backend_t backend, uint64_t device)\n-        {\n+    return -1;\n+  });\n+  m.def(\"l2_cache_size\", [](backend_t backend, uint64_t device) {\n     if (backend == CUDA) return cuGetInfo<CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE>(device);\n-    return -1; });\n+    return -1;\n+  });\n \n   // query clock rate (in kilohertz)\n-  m.def(\"clock_rate\", [](backend_t backend, uint64_t device)\n-        {\n+  m.def(\"clock_rate\", [](backend_t backend, uint64_t device) {\n     if (backend == CUDA) return cuGetInfo<CU_DEVICE_ATTRIBUTE_CLOCK_RATE>(device);\n-    return -1; });\n+    return -1;\n+  });\n \n-  m.def(\"num_sm\", [](backend_t backend, uint64_t device)\n-        {\n+  m.def(\"num_sm\", [](backend_t backend, uint64_t device) {\n     if (backend == CUDA) return cuGetInfo<CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT>(device);\n-    return -1; });\n+    return -1;\n+  });\n \n   // enqueue\n   m.def(\"enqueue\", [](backend_t backend, uint64_t stream, uint64_t kernel,\n                       uint64_t grid_0, uint64_t grid_1, uint64_t grid_2,\n                       uint64_t block_0, uint64_t block_1, uint64_t block_2,\n-                      const std::string &args, int64_t shared_mem)\n-        {\n+                      const std::string &args, int64_t shared_mem){\n     void* args_ptr = (void*)args.data();\n     size_t args_size = args.size();\n     // release the gil in case the enqueue blocks\n@@ -451,24 +414,26 @@ void init_triton_runtime(py::module &&m)\n     if(backend == CUDA)\n       cu_enqueue(stream, kernel, grid_0, grid_1, grid_2, block_0, block_1, block_2, args_ptr, args_size, shared_mem);\n     if(backend == ROCM)\n-      hip_enqueue(stream, kernel, grid_0, grid_1, grid_2, block_0, block_1, block_2, args_ptr, args_size, shared_mem); });\n+      hip_enqueue(stream, kernel, grid_0, grid_1, grid_2, block_0, block_1, block_2, args_ptr, args_size, shared_mem);\n+  });\n+\n+  \n }\n \n /*****************************************************************************/\n /* Python bindings for triton::codegen                                       */\n /*****************************************************************************/\n typedef std::map<std::string, py::object> asm_map_t;\n \n-// ---------------------------------------\n+// --------------------------------------- \n // Load provided assembly code into driver\n-// ---------------------------------------\n+// --------------------------------------- \n \n // CUDA\n-std::tuple<uint64_t, uint64_t, uint64_t, uint64_t> cu_load_binary(const std::string &name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev)\n-{\n+std::tuple<uint64_t, uint64_t, uint64_t, uint64_t> cu_load_binary(const std::string& name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev){\n   // load assembly\n   std::string assembly;\n-  if (asm_map.find(\"cubin\") != asm_map.end())\n+  if(asm_map.find(\"cubin\") != asm_map.end())\n     assembly = py::cast<std::string>(asm_map[\"cubin\"]);\n   else\n     assembly = py::cast<std::string>(asm_map[\"ptx\"]);\n@@ -486,8 +451,7 @@ std::tuple<uint64_t, uint64_t, uint64_t, uint64_t> cu_load_binary(const std::str\n   // set dynamic shared memory if necessary\n   int shared_optin;\n   drv::dispatch::cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, dev);\n-  if (n_shared_bytes > 49152 && shared_optin > 49152)\n-  {\n+  if(n_shared_bytes > 49152 && shared_optin > 49152){\n     drv::dispatch::cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED);\n     int shared_total, shared_static;\n     drv::dispatch::cuDeviceGetAttribute(&shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, dev);\n@@ -498,8 +462,7 @@ std::tuple<uint64_t, uint64_t, uint64_t, uint64_t> cu_load_binary(const std::str\n }\n \n // ROCM\n-std::tuple<uint64_t, uint64_t, uint64_t, uint64_t> hip_load_binary(const std::string &name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev)\n-{\n+std::tuple<uint64_t, uint64_t, uint64_t, uint64_t> hip_load_binary(const std::string& name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev){\n   py::bytes _assembly = asm_map[\"hsaco\"];\n   std::string assembly = py::cast<std::string>(_assembly);\n   // HSA-CO -> hipModule\n@@ -511,23 +474,22 @@ std::tuple<uint64_t, uint64_t, uint64_t, uint64_t> hip_load_binary(const std::st\n   return std::make_tuple((uint64_t)mod, (uint64_t)fun, 0, 0);\n }\n \n-// ---------------------------------------\n+// --------------------------------------- \n // Compile Triton-IR to assembly\n-// ---------------------------------------\n+// --------------------------------------- \n \n // CUDA\n std::tuple<std::string, asm_map_t, int> cu_compile_ttir(\n     const std::string &name, ir::module &ir, uint64_t device, int num_warps,\n     int num_stages, asm_map_t &asm_map,\n-    const triton::codegen::ExternLibMap &extern_lib_map)\n-{\n+    const triton::codegen::ExternLibMap &extern_lib_map) {\n   py::gil_scoped_release allow_threads;\n   llvm::LLVMContext ctx;\n   // device properties\n   CUdevice dev = (CUdevice)device;\n   size_t major = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR>(dev);\n   size_t minor = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR>(dev);\n-  size_t cc = major * 10 + minor;\n+  size_t cc = major*10 + minor;\n   int version;\n   std::string ptxas_path = drv::path_to_ptxas(version);\n   // Triton-IR -> NVPTX LLVM-IR\n@@ -545,8 +507,7 @@ std::tuple<std::string, asm_map_t, int> cu_compile_ttir(\n   asm_map[\"ptx\"] = py::cast(ptx);\n   // PTX -> Binary\n   std::string cubin = drv::ptx_to_cubin(ptx, ptxas_path, cc);\n-  if (!cubin.empty())\n-  {\n+  if(!cubin.empty()){\n     py::bytes bytes(cubin);\n     asm_map[\"cubin\"] = bytes;\n   }\n@@ -557,8 +518,7 @@ std::tuple<std::string, asm_map_t, int> cu_compile_ttir(\n std::tuple<std::string, asm_map_t, int> hip_compile_ttir(\n     const std::string &name, ir::module &ir, uint64_t device, int num_warps,\n     int num_stages, asm_map_t &asm_map,\n-    const triton::codegen::ExternLibMap &extern_lib_map)\n-{\n+    const triton::codegen::ExternLibMap &extern_lib_map) {\n   llvm::LLVMContext ctx;\n   // Triton-IR -> NVPTX LLVM-IR\n   triton::codegen::amd_cl_target target;\n@@ -576,13 +536,11 @@ std::tuple<std::string, asm_map_t, int> hip_compile_ttir(\n   return std::make_tuple(name, asm_map, n_shared_bytes);\n }\n \n-void init_triton_codegen(py::module &&m)\n-{\n+void init_triton_codegen(py::module &&m) {\n   m.def(\n       \"compile_ttir\",\n       [](backend_t backend, ir::module &ir, uint64_t device, int num_warps,\n-         int num_stages, py::dict &extern_libs)\n-      {\n+         int num_stages, py::dict& extern_libs) {\n         std::string name = ir.get_function_list()[0]->get_name();\n         // record asm as we generate\n         asm_map_t asm_map;\n@@ -592,28 +550,26 @@ void init_triton_codegen(py::module &&m)\n         llvm::LLVMContext ctx;\n         // construct extern lib map\n         triton::codegen::ExternLibMap extern_lib_map;\n-        for (auto item : extern_libs)\n-        {\n+        for (auto item : extern_libs) {\n           auto name = item.first.cast<std::string>();\n           auto path = item.second.cast<std::string>();\n           extern_lib_map.emplace(\n               name, triton::codegen::create_extern_lib(name, path));\n         }\n-        if (backend == CUDA)\n+        if(backend == CUDA)\n           return cu_compile_ttir(name, ir, device, num_warps, num_stages, asm_map, extern_lib_map);\n         assert(backend == ROCM);\n         return hip_compile_ttir(name, ir, device, num_warps, num_stages, asm_map, extern_lib_map);\n       },\n       py::return_value_policy::take_ownership);\n-  m.def(\n-      \"load_binary\", [](backend_t backend, const std::string &name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev)\n-      {\n+  m.def(\"load_binary\", [](backend_t backend, const std::string& name, asm_map_t &asm_map, size_t n_shared_bytes, uint64_t dev){\n \tpy::gil_scoped_release allow_threads;\n         if(backend == CUDA)\n           return cu_load_binary(name, asm_map, n_shared_bytes, dev);\n         assert(backend == ROCM);\n-        return hip_load_binary(name, asm_map, n_shared_bytes, dev); },\n-      py::return_value_policy::take_ownership);\n+        return hip_load_binary(name, asm_map, n_shared_bytes, dev);\n+      }, py::return_value_policy::take_ownership);\n+  \n \n   struct InstanceDescriptor\n   {\n@@ -628,12 +584,12 @@ void init_triton_codegen(py::module &&m)\n       .def_readonly(\"equal_to_1\", &InstanceDescriptor::equalTo1);\n }\n \n+\n /*****************************************************************************/\n /* Python bindings for triton::ir                                            */\n /*****************************************************************************/\n \n-void init_triton_ir(py::module &&m)\n-{\n+void init_triton_ir(py::module &&m) {\n   using ret = py::return_value_policy;\n   using namespace pybind11::literals;\n \n@@ -642,13 +598,13 @@ void init_triton_ir(py::module &&m)\n       .value(\"CA\", ir::load_inst::CA)\n       .value(\"CG\", ir::load_inst::CG)\n       .export_values();\n-\n+  \n   py::enum_<ir::load_inst::EVICTION_POLICY>(m, \"EVICTION_POLICY\")\n       .value(\"NORMAL\", ir::load_inst::NORMAL)\n       .value(\"EVICT_FIRST\", ir::load_inst::EVICT_FIRST)\n       .value(\"EVICT_LAST\", ir::load_inst::EVICT_LAST)\n       .export_values();\n-\n+  \n   py::enum_<ir::reduce_inst::op_t>(m, \"REDUCE_OP\")\n       .value(\"ADD\", ir::reduce_inst::ADD)\n       .value(\"FADD\", ir::reduce_inst::FADD)\n@@ -665,7 +621,7 @@ void init_triton_ir(py::module &&m)\n       .value(\"ARGFMIN\", ir::reduce_inst::ARGFMIN)\n       .value(\"ARGFMAX\", ir::reduce_inst::ARGFMAX)\n       .value(\"XOR\", ir::reduce_inst::XOR);\n-\n+  \n   py::enum_<ir::atomic_rmw_op_t>(m, \"ATOMIC_OP\")\n       .value(\"ADD\", ir::atomic_rmw_op_t::Add)\n       .value(\"FADD\", ir::atomic_rmw_op_t::FAdd)\n@@ -682,41 +638,41 @@ void init_triton_ir(py::module &&m)\n       .def(py::init<>());\n \n   py::class_<ir::value>(m, \"value\")\n-      .def(\"multiple_of\", [](ir::value *self, std::vector<unsigned> val)\n-           {\n+      .def(\"multiple_of\", [](ir::value *self, std::vector<unsigned> val) {\n         if (auto *instr = dynamic_cast<ir::instruction*>(self)) {\n           instr->set_metadata(ir::metadata::multiple_of, val);\n         } else\n-          throw std::runtime_error(\"multiple_of\"); })\n-      .def(\"max_contiguous\", [](ir::value *self, std::vector<unsigned> val)\n-           {\n+          throw std::runtime_error(\"multiple_of\");\n+      })\n+      .def(\"max_contiguous\", [](ir::value *self, std::vector<unsigned> val) {\n         if (auto *instr = dynamic_cast<ir::instruction*>(self)) {\n           instr->set_metadata(ir::metadata::max_contiguous, val);\n         } else\n-          throw std::runtime_error(\"max_contiguous\"); })\n-      .def(\"set_fdiv_ieee_rounding\", [](ir::value *self, bool val)\n-           {\n+          throw std::runtime_error(\"max_contiguous\");\n+      })\n+      .def(\"set_fdiv_ieee_rounding\", [](ir::value *self, bool val) {\n         if (auto *instr = dynamic_cast<ir::binary_operator*>(self))\n           instr->set_fdiv_ieee_rounding(val);\n         else\n-          throw std::runtime_error(\"set_fdiv_ieee_rounding\"); })\n-      .def(\"is_phi\", [](ir::value *self)\n-           {\n+          throw std::runtime_error(\"set_fdiv_ieee_rounding\");\n+      })\n+      .def(\"is_phi\", [](ir::value *self) {\n         if (auto *pn = dynamic_cast<ir::phi_node*>(self))\n           return true;\n-        return false; })\n-      .def(\"ops\", [](ir::value *self)\n-           {\n+        return false;\n+      })\n+      .def(\"ops\", [](ir::value *self) {\n         if (auto *instr = dynamic_cast<ir::instruction*>(self)) {\n           return instr->ops();\n         }\n-        throw std::runtime_error(\"cannot use ops()\"); })\n+        throw std::runtime_error(\"cannot use ops()\");\n+      })\n       .def(\"replace_all_uses_with\", &ir::value::replace_all_uses_with)\n-      .def(\"erase_from_parent\", [](ir::value *self)\n-           {\n+      .def(\"erase_from_parent\", [](ir::value *self) {\n         if (auto *instr = dynamic_cast<ir::instruction*>(self))\n           return instr->erase_from_parent();\n-        throw std::runtime_error(\"cannot use erase_from_parent\"); })\n+        throw std::runtime_error(\"cannot use erase_from_parent\");\n+      })\n       .def_property(\"name\", &ir::value::get_name, &ir::value::set_name)\n       .def_property_readonly(\"type\", &ir::value::get_type);\n \n@@ -731,23 +687,17 @@ void init_triton_ir(py::module &&m)\n \n   py::class_<ir::constant_int, ir::constant>(m, \"constant_int\")\n       .def_property_readonly(\"value\", &ir::constant_int::get_value)\n-      .def(\"__int__\", [](ir::constant_int *self)\n-           { return self->get_value(); })\n-      .def(\"__bool__\", [](ir::constant_int *self)\n-           { return self->get_value(); });\n+      .def(\"__int__\", [](ir::constant_int *self) { return self->get_value(); })\n+      .def(\"__bool__\", [](ir::constant_int *self) { return self->get_value(); });\n \n   py::class_<ir::constant_fp, ir::constant>(m, \"constant_float\")\n       .def_property_readonly(\"value\", &ir::constant_fp::get_value)\n-      .def(\n-          \"get\", [](ir::type *ty, double val)\n-          { return ir::constant_fp::get(ty, val); },\n-          ret::reference);\n+      .def(\"get\", [](ir::type* ty, double val) { return ir::constant_fp::get(ty, val); }, ret::reference);\n \n   py::class_<ir::instruction, ir::user>(m, \"instruction\")\n-      .def(\n-          \"get_parent\", [](ir::instruction *self)\n-          { return self->get_parent(); },\n-          ret::reference);\n+      .def(\"get_parent\", [](ir::instruction *self) {\n+        return self->get_parent();\n+      }, ret::reference);\n   py::class_<ir::phi_node, ir::instruction>(m, \"phi_node\")\n       .def(\"add_incoming\", &ir::phi_node::add_incoming);\n \n@@ -782,16 +732,11 @@ void init_triton_ir(py::module &&m)\n       .def(\"is_bf16\", &ir::type::is_bf16_ty)\n       .def(\"is_fp32\", &ir::type::is_fp32_ty)\n       .def(\"is_fp64\", &ir::type::is_fp64_ty)\n-      .def(\"is_int1\", [](ir::type *self)\n-           { return self->is_integer_ty(1); })\n-      .def(\"is_int8\", [](ir::type *self)\n-           { return self->is_integer_ty(8); })\n-      .def(\"is_int16\", [](ir::type *self)\n-           { return self->is_integer_ty(16); })\n-      .def(\"is_int32\", [](ir::type *self)\n-           { return self->is_integer_ty(32); })\n-      .def(\"is_int64\", [](ir::type *self)\n-           { return self->is_integer_ty(64); })\n+      .def(\"is_int1\", [](ir::type *self) { return self->is_integer_ty(1); })\n+      .def(\"is_int8\", [](ir::type *self) { return self->is_integer_ty(8); })\n+      .def(\"is_int16\", [](ir::type *self) { return self->is_integer_ty(16); })\n+      .def(\"is_int32\", [](ir::type *self) { return self->is_integer_ty(32); })\n+      .def(\"is_int64\", [](ir::type *self) { return self->is_integer_ty(64); })\n       .def(\"is_int_or_tileint\", &ir::type::is_int_or_tileint_ty)\n \n       .def(\"repr\", &ir::type::repr)\n@@ -807,36 +752,38 @@ void init_triton_ir(py::module &&m)\n \n   py::class_<ir::function_type, ir::type>(m, \"function_type\")\n       .def_property_readonly(\"ret_ty\", &ir::function_type::get_return_ty)\n-      .def_property_readonly(\"arg_tys\", [](ir::function_type *self)\n-                             { return std::vector<ir::type *>(self->params_begin(), self->params_end()); });\n+      .def_property_readonly(\"arg_tys\", [](ir::function_type* self){ \n+        return std::vector<ir::type*>(self->params_begin(), self->params_end());\n+      });\n \n   py::class_<ir::integer_type, ir::type>(m, \"integer_type\");\n \n   py::class_<ir::block_type, ir::type>(m, \"block_type\")\n       .def_property_readonly(\"shape\", &ir::block_type::get_shapes)\n       .def_property_readonly(\"numel\", &ir::type::get_tile_num_elements);\n-\n+  \n   py::class_<ir::struct_type, ir::type>(m, \"struct_type\")\n       .def(\"get\", &ir::struct_type::get, ret::reference)\n       .def_property_readonly(\"num_types\", &ir::struct_type::get_num_types);\n \n   py::class_<ir::module>(m, \"module\", py::dynamic_attr())\n       .def(py::init<std::string, ir::builder &>())\n       .def(\"has_function\", &ir::module::has_function)\n-      .def(\"get_functions\", &ir::module::get_function_list, ret::reference)\n       .def(\"get_function\", &ir::module::get_function, ret::reference)\n+      .def(\"get_functions\", &ir::module::get_function_list, ret::reference)\n       .def(\"get_or_insert_function\", &ir::module::get_or_insert_function, ret::reference)\n-      .def(\"print\", [](ir::module *self)\n-           { self->print(std::cout); })\n+      .def(\"print\", [](ir::module *self) {\n+          self->print(std::cout);\n+      })\n       .def(\"reset_ret_ty\", &ir::module::reset_ret_ty)\n-      .def(\"set_instr_metadata\", [](ir::module *self, const std::string &name, ir::value *value)\n-           {\n+      .def(\"set_instr_metadata\", [](ir::module *self, const std::string &name, ir::value *value) {\n           const auto metadatas = self->get_metadatas();\n           auto it = metadatas.find(name);\n           if (it != metadatas.end())\n             if (auto *instr = dynamic_cast<ir::instruction*>(value)) {\n               instr->set_metadata(it->second.first, it->second.second);\n-            } })\n+            }\n+    })\n       .def_property_readonly(\"builder\", &ir::module::get_builder, ret::reference);\n \n   using eattr = ir::attribute_kind_t;\n@@ -868,14 +815,12 @@ void init_triton_ir(py::module &&m)\n   py::class_<ir::basic_block, ir::value>(m, \"basic_block\")\n       .def(\"create\", &ir::basic_block::create, ret::reference, py::arg(), py::arg(), py::arg() = nullptr)\n       .def(\"get_predecessors\", &ir::basic_block::get_predecessors, ret::reference)\n-      .def(\n-          \"get_first_non_phi\", [](ir::basic_block *self) -> ir::instruction *\n-          {\n+      .def(\"get_first_non_phi\", [](ir::basic_block *self) -> ir::instruction* {\n         ir::basic_block::iterator it = self->get_first_non_phi();\n         if (it == self->end())\n           return nullptr;\n-        return *it; },\n-          ret::reference)\n+        return *it;\n+      }, ret::reference)\n       .def_property_readonly(\"parent\", &ir::basic_block::get_parent, ret::reference);\n \n   py::class_<ir::builder::iterator>(m, \"bb_iterator\");\n@@ -893,17 +838,14 @@ void init_triton_ir(py::module &&m)\n       .def(\"ret\", &ir::builder::create_ret, ret::reference)\n       // insertion block/point, insert points are represented as (*bb, *instr)\n       .def(\"get_insert_block\", &ir::builder::get_insert_block, ret::reference)\n-      .def(\"set_insert_block\", (void(ir::builder::*)(ir::basic_block *)) & ir::builder::set_insert_point)\n-      .def(\n-          \"get_insert_point\", [](ir::builder *self)\n-          {\n+      .def(\"set_insert_block\", (void (ir::builder::*)(ir::basic_block *)) & ir::builder::set_insert_point)\n+      .def(\"get_insert_point\", [](ir::builder *self) {\n         ir::basic_block *bb = self->get_insert_block();\n         ir::basic_block::iterator it = self->get_insert_point();\n         ir::instruction *instr = it == bb->end() ? nullptr : *it;\n-        return std::make_pair(bb, instr); },\n-          ret::reference)\n-      .def(\"set_insert_point\", [](ir::builder *self, std::pair<ir::basic_block *, ir::instruction *> pt)\n-           {\n+        return std::make_pair(bb, instr);\n+      }, ret::reference)\n+      .def(\"set_insert_point\", [](ir::builder *self, std::pair<ir::basic_block*, ir::instruction*> pt) {\n         ir::basic_block *bb = pt.first;\n         ir::instruction *instr = pt.second;\n         if (instr) {\n@@ -913,18 +855,13 @@ void init_triton_ir(py::module &&m)\n         } else {\n           assert(bb);\n           self->set_insert_point(bb);\n-        } })\n+        }\n+      })\n       // Constants\n       .def(\"get_int1\", &ir::builder::get_int1, ret::reference)\n-      .def(\n-          \"get_int32\", [](ir::builder *self, int32_t v)\n-          { return self->get_int32((uint32_t)v); },\n-          ret::reference)\n+      .def(\"get_int32\", [](ir::builder *self, int32_t v) { return self->get_int32((uint32_t)v); }, ret::reference)\n       .def(\"get_uint32\", &ir::builder::get_int32, ret::reference)\n-      .def(\n-          \"get_int64\", [](ir::builder *self, int64_t v)\n-          { return self->get_int64((uint64_t)v); },\n-          ret::reference)\n+      .def(\"get_int64\", [](ir::builder *self, int64_t v) { return self->get_int64((uint64_t)v); }, ret::reference)\n       .def(\"get_uint64\", &ir::builder::get_int64, ret::reference)\n       .def(\"get_float16\", &ir::builder::get_float16, ret::reference)\n       .def(\"get_float32\", &ir::builder::get_float32, ret::reference)\n@@ -968,28 +905,28 @@ void init_triton_ir(py::module &&m)\n       .def(\"create_frem\", &ir::builder::create_frem, ret::reference)\n       .def(\"create_fadd\", &ir::builder::create_fadd, ret::reference)\n       .def(\"create_fsub\", &ir::builder::create_fsub, ret::reference)\n-      .def(\"create_mul\", &ir::builder::create_mul, ret::reference,\n-           py::arg(\"lhs\"), py::arg(\"rhs\"),\n-           py::arg(\"has_nuw\") = false, py::arg(\"has_nsw\") = false)\n+      .def(\"create_mul\", &ir::builder::create_mul, ret::reference, \n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       .def(\"create_sdiv\", &ir::builder::create_sdiv, ret::reference)\n       .def(\"create_udiv\", &ir::builder::create_udiv, ret::reference)\n       .def(\"create_srem\", &ir::builder::create_srem, ret::reference)\n       .def(\"create_urem\", &ir::builder::create_urem, ret::reference)\n-      .def(\"create_add\", &ir::builder::create_add, ret::reference,\n-           py::arg(\"lhs\"), py::arg(\"rhs\"),\n-           py::arg(\"has_nuw\") = false, py::arg(\"has_nsw\") = false)\n+      .def(\"create_add\", &ir::builder::create_add, ret::reference, \n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       .def(\"create_sub\", &ir::builder::create_sub, ret::reference,\n-           py::arg(\"lhs\"), py::arg(\"rhs\"),\n-           py::arg(\"has_nuw\") = false, py::arg(\"has_nsw\") = false)\n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       .def(\"create_shl\", &ir::builder::create_shl, ret::reference,\n-           py::arg(\"lhs\"), py::arg(\"rhs\"),\n-           py::arg(\"has_nuw\") = false, py::arg(\"has_nsw\") = false)\n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       .def(\"create_lshr\", &ir::builder::create_lshr, ret::reference,\n-           py::arg(\"lhs\"), py::arg(\"rhs\"),\n-           py::arg(\"has_nuw\") = false, py::arg(\"has_nsw\") = false)\n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       .def(\"create_ashr\", &ir::builder::create_ashr, ret::reference,\n-           py::arg(\"lhs\"), py::arg(\"rhs\"),\n-           py::arg(\"has_nuw\") = false, py::arg(\"has_nsw\") = false)\n+                          py::arg(\"lhs\"), py::arg(\"rhs\"), \n+                          py::arg(\"has_nuw\")=false, py::arg(\"has_nsw\")=false)\n       // GEP\n       .def(\"create_gep\", &ir::builder::create_gep, ret::reference)\n       // Comparison (int)\n@@ -1066,8 +1003,7 @@ void init_triton_ir(py::module &&m)\n       .def(\"create_prefetch_s\", &ir::builder::create_prefetch_s, ret::reference);\n }\n \n-void init_triton(py::module &m)\n-{\n+void init_triton(py::module &m) {\n   py::module subm = m.def_submodule(\"triton\");\n   init_triton_codegen(std::move(subm.def_submodule(\"code_gen\")));\n   init_triton_runtime(std::move(subm.def_submodule(\"runtime\")));"}]