[{"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 113, "deletions": 22, "changes": 135, "file_content_changes": "@@ -12,6 +12,7 @@\n //===----------------------------------------------------------------------===//\n \n using namespace mlir;\n+namespace ttg = triton::gpu;\n \n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n@@ -36,6 +37,8 @@ class LoopPipeliner {\n   DenseMap<Value, Value> loadsMapping;\n   /// load => buffer\n   DenseMap<Value, Value> loadsBuffer;\n+  /// load => buffer type (with shared layout after swizzling)\n+  DenseMap<Value, RankedTensorType> loadsBufferType;\n   /// load => buffer at stage N\n   DenseMap<Value, SmallVector<Value>> loadStageBuffer;\n   /// load => after extract\n@@ -64,9 +67,11 @@ class LoopPipeliner {\n   bool isDirectUserOfAsyncLoad(Operation &op);\n \n   /// returns a empty buffer of size <numStages, ...>\n-  triton::gpu::AllocTensorOp allocateEmptyBuffer(Operation *op,\n-                                                 OpBuilder &builder);\n+  ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n \n+  /// compute type of shared buffers (with swizzled shared layouts)\n+  RankedTensorType getSwizzleType(ttg::DotOperandEncodingAttr dotOpEnc,\n+                                  RankedTensorType tensorType);\n public:\n   LoopPipeliner(scf::ForOp forOp, int numStages)\n       : forOp(forOp), numStages(numStages) {\n@@ -135,25 +140,71 @@ bool LoopPipeliner::isDirectUserOfAsyncLoad(Operation &op) {\n   return false;\n }\n \n-triton::gpu::AllocTensorOp\n+ttg::AllocTensorOp\n LoopPipeliner::allocateEmptyBuffer(Operation *op, OpBuilder &builder) {\n   // allocate a buffer for each pipelined tensor\n   // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n   Value convertLayout = loadsMapping[op->getResult(0)];\n   if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>()) {\n-    SmallVector<int64_t> shape(tensorType.getShape().begin(),\n-                               tensorType.getShape().end());\n-    shape.insert(shape.begin(), numStages);\n-    Type elementType = tensorType.getElementType();\n-    // The encoding of the buffer is similar to the original tensor\n-    Attribute encoding = tensorType.getEncoding();\n-    auto bufferType = RankedTensorType::get(shape, elementType, encoding);\n-    return builder.create<triton::gpu::AllocTensorOp>(convertLayout.getLoc(),\n-                                                      bufferType);\n+    return builder.create<ttg::AllocTensorOp>(convertLayout.getLoc(),\n+                                            loadsBufferType[op->getResult(0)]);\n   }\n   llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n }\n \n+// TODO: I copied the code from Swizzle.cpp. Should find a way to unify the\n+//       code path.\n+RankedTensorType\n+LoopPipeliner::getSwizzleType(ttg::DotOperandEncodingAttr dotOpEnc,\n+                              RankedTensorType ty) {\n+  int opIdx = dotOpEnc.getOpIdx();\n+  auto mmaEnc = dotOpEnc.getParent().cast<ttg::MmaEncodingAttr>();\n+  int version = mmaEnc.getVersion();\n+  auto tyEncoding = ty.getEncoding().cast<ttg::BlockedEncodingAttr>();\n+  auto order = tyEncoding.getOrder();\n+  // number of rows per phase\n+  int perPhase = 128 / (ty.getShape()[order[0]] *\n+                        (ty.getElementType().getIntOrFloatBitWidth() / 8));\n+  perPhase = std::max<int>(perPhase, 1);\n+  int vec = 1;\n+  int maxPhase = 1;\n+  // index of the inner dimension in `order`\n+  int inner = (opIdx == 0) ? 0 : 1;\n+  if (version == 1) {\n+    maxPhase = (order[inner] == 1 ? 8 : 4) / perPhase;\n+    // TODO: handle rep (see\n+    // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L209)\n+  } else if (version == 2) {\n+    auto eltTy = ty.getElementType();\n+    std::vector<size_t> mat_shape = {8, 8,\n+                                      2 * 64 / eltTy.getIntOrFloatBitWidth()};\n+    // for now, disable swizzle when using transposed int8 tensor cores\n+    if (ty.getElementType().isInteger(8) && order[0] == inner)\n+      perPhase = 1;\n+    else {\n+      if (opIdx == 0) {  // compute swizzling for A operand\n+        vec = order[0] == 1 ? mat_shape[2] : mat_shape[0]; // k : m\n+        int mmaStride = order[0] == 1 ? mat_shape[0] : mat_shape[2];\n+        maxPhase = mmaStride / perPhase;\n+      } else if (opIdx == 1) {  // compute swizzling for B operand\n+        vec = order[0] == 1 ? mat_shape[1] : mat_shape[2]; // n : k\n+        int mmaStride = order[0] == 1 ? mat_shape[2] : mat_shape[1];\n+        maxPhase = mmaStride / perPhase;\n+      } else\n+        llvm_unreachable(\"invalid operand index\");\n+    }\n+  } else  // version not in [1, 2]\n+    llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n+\n+  auto newEncoding = ttg::SharedEncodingAttr::get(\n+    ty.getContext(), vec, perPhase, maxPhase, order\n+  );\n+  SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n+                                   ty.getShape().end());\n+  bufferShape.insert(bufferShape.begin(), numStages);\n+  return RankedTensorType::get(bufferShape, ty.getElementType(), newEncoding);\n+}\n+\n /// A load instruction can be pipelined if:\n ///   - the load doesn't depend on any other loads (after loop peeling)\n ///   - (?) this load is not a loop-invariant value (we should run LICM before\n@@ -193,19 +244,22 @@ LogicalResult LoopPipeliner::initialize() {\n       }\n     }\n \n-    // For now, we only pipeline loads that have one covert_layout (to smem) use\n+    // We only pipeline loads that have one covert_layout (to dot_op) use\n     // TODO: lift this constraint in the future\n     if (isCandiate && loadOp.getResult().hasOneUse()) {\n       isCandiate = false;\n       Operation *use = *loadOp.getResult().getUsers().begin();\n       if (auto convertLayout =\n-              llvm::dyn_cast<triton::gpu::ConvertLayoutOp>(use)) {\n+              llvm::dyn_cast<ttg::ConvertLayoutOp>(use)) {\n         if (auto tensorType = convertLayout.getResult()\n                                   .getType()\n                                   .dyn_cast<RankedTensorType>()) {\n-          if (tensorType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n+          if (auto dotOpEnc = tensorType.getEncoding()\n+                                     .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n             isCandiate = true;\n             loadsMapping[loadOp] = convertLayout;\n+            loadsBufferType[loadOp] = getSwizzleType(dotOpEnc,\n+                                    loadOp.getType().cast<RankedTensorType>());\n           }\n         }\n       }\n@@ -282,8 +336,8 @@ void LoopPipeliner::emitPrologue() {\n         // load => copy async\n         // TODO: check if the hardware supports async copy\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n-          newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n-              op->getLoc(), loadsBuffer[loadOp].getType(),\n+          newOp = builder.create<ttg::InsertSliceAsyncOp>(\n+              op->getLoc(), loadsBufferType[loadOp],\n               lookupOrDefault(loadOp.ptr(), stage),\n               loadStageBuffer[loadOp][stage], pipelineIterIdx,\n               lookupOrDefault(loadOp.mask(), stage),\n@@ -310,7 +364,7 @@ void LoopPipeliner::emitPrologue() {\n             if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(newOp)) {\n               return loadOp.mask();\n             } else if (auto insertSliceAsyncOp =\n-                           llvm::dyn_cast<triton::gpu::InsertSliceAsyncOp>(\n+                           llvm::dyn_cast<ttg::InsertSliceAsyncOp>(\n                                newOp)) {\n               return insertSliceAsyncOp.mask();\n             } else {\n@@ -358,11 +412,14 @@ void LoopPipeliner::emitPrologue() {\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n   // async.wait & extract_slice\n-  Operation *asyncWait = builder.create<triton::gpu::AsyncWaitOp>(\n+  Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n       loads[0].getLoc(), loads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (Value loadOp : loads) {\n     auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n+    sliceType = RankedTensorType::get(sliceType.getShape(),\n+                                      sliceType.getElementType(),\n+                                      loadsBufferType[loadOp].getEncoding());\n     Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{intAttr(0), intAttr(0), intAttr(0)},\n@@ -504,15 +561,18 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n           nextMapping.map(mask, newMask);\n       }\n-      Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n-          op->getLoc(), loadsBuffer[loadOp].getType(),\n+      Value insertAsyncOp = builder.create<ttg::InsertSliceAsyncOp>(\n+          op->getLoc(), loadsBufferType[loadOp],\n           nextMapping.lookupOrDefault(loadOp.ptr()),\n           newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n           insertSliceIndex, nextMapping.lookupOrDefault(loadOp.mask()),\n           nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n       nextBuffers.push_back(insertAsyncOp);\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n+      sliceType = RankedTensorType::get(sliceType.getShape(),\n+                                        sliceType.getElementType(),\n+                                        loadsBufferType[loadOp].getEncoding());\n       nextOp = builder.create<tensor::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, intAttr(0), intAttr(0)},\n@@ -540,8 +600,39 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     }\n   }\n \n+  {\n+    OpBuilder::InsertionGuard guard(builder);\n+    for (Operation &op : *newForOp.getBody()) {\n+      if (auto dotOp = llvm::dyn_cast<triton::DotOp>(&op)) {\n+        builder.setInsertionPoint(&op);\n+        auto dotType = dotOp.getType().cast<RankedTensorType>();\n+        Value a = dotOp.a();\n+        Value b = dotOp.b();\n+        auto layoutCast = [&](Value dotOperand, int opIdx) -> Value {\n+          auto tensorType = dotOperand.getType().cast<RankedTensorType>();\n+          if (!tensorType.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n+            auto newEncoding = ttg::DotOperandEncodingAttr::get(\n+              tensorType.getContext(), opIdx, dotType.getEncoding()\n+            );\n+            auto newType = RankedTensorType::get(tensorType.getShape(),\n+                                                tensorType.getElementType(),\n+                                                newEncoding);\n+            return builder.create<ttg::ConvertLayoutOp>(\n+              dotOperand.getLoc(), newType, dotOperand\n+            );\n+          }\n+          return dotOperand;\n+        };\n+        a = layoutCast(a, 0);\n+        b = layoutCast(b, 1);\n+        dotOp->setOperand(0, a);\n+        dotOp->setOperand(1, b);\n+      }\n+    }\n+  }\n+\n   // async.wait & extract_slice\n-  Operation *asyncWait = builder.create<triton::gpu::AsyncWaitOp>(\n+  Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n       loads[0].getLoc(), loads.size() * (numStages - 2));\n   for (auto it = extractSlices.rbegin(); it != extractSlices.rend(); ++it) {\n     // move extract_slice after asyncWait"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -23,7 +23,9 @@\n // CHECK: %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n // CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n-// CHECK:   tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n+// CHECK:   %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n+// CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n+// CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n@@ -80,7 +82,9 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n // CHECK:   %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n // CHECK:   %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n-// CHECK:     tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n+// CHECK:     %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n+// CHECK:     %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n+// CHECK:     tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n@@ -134,7 +138,8 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n // CHECK: triton_gpu.async_wait {num = 1 : i32}\n // CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n-// CHECK:   tt.dot {{.*}}, %[[arg_b0]], {{.*}}\n+// CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n+// CHECK:   tt.dot {{.*}}, %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index"}]