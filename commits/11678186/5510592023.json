[{"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 38, "deletions": 18, "changes": 56, "file_content_changes": "@@ -8,20 +8,20 @@\n import triton.ops\n \n \n-def f8_to_f16(x):\n+def f8_to_f16(x, dtype):\n \n     @triton.jit\n     def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n         pid = tl.program_id(0)\n         offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n         mask = offs < N\n         x = tl.load(X + offs, mask=mask)\n-        y = x.to(tl.float8e5)\n-        tl.store(Y + offs, y, mask=mask)\n+        tl.store(Y + offs, x, mask=mask)\n \n     ret = torch.empty(x.shape, dtype=torch.float16, device=x.device)\n     grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n-    kernel[grid](ret, triton.reinterpret(x, tl.float8e5), ret.numel(), BLOCK_SIZE=1024)\n+    dtype = getattr(tl, dtype)\n+    kernel[grid](ret, triton.reinterpret(x, dtype), ret.numel(), BLOCK_SIZE=1024)\n     return ret\n \n \n@@ -85,13 +85,17 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n         # mixed-precision\n         *[\n             [\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (32, 64, 16, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n                 (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8\", \"float16\"), (\"float16\", \"float32\"), (\"float32\", \"float16\"),\n-                                     (\"bfloat16\", \"float32\"), (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n+            ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n+                                     (\"float8e4\", \"float16\"),\n+                                     (\"float16\", \"float8e5\"),\n+                                     (\"float16\", \"float32\"),\n+                                     (\"float32\", \"float16\"),\n+                                     (\"bfloat16\", \"float32\"),\n+                                     (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n         ]\n     ),\n )\n@@ -116,22 +120,38 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n+    a_fp8 = \"float8\" in ADTYPE\n+    b_fp8 = \"float8\" in BDTYPE\n \n-    def get_input(n, m, t, dtype):\n+    def maybe_upcast(x, dtype, is_float8):\n+        if is_float8:\n+            return f8_to_f16(x, dtype)\n+        return x\n+\n+    def init_input(n, m, t, dtype, is_float8):\n         if t:\n-            return get_input(m, n, False, dtype).t()\n-        if dtype == \"float8\":\n-            x = torch.randint(10, 50, (n, m), device=\"cuda\", dtype=torch.int8)\n-            return f8_to_f16(x)\n+            return init_input(m, n, False, dtype, is_float8).t()\n+        if is_float8:\n+            return torch.randint(20, 60, (n, m), device=\"cuda\", dtype=torch.int8)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n         return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n \n     # allocate/transpose inputs\n-    a = get_input(M, K, AT, ADTYPE)\n-    b = get_input(K, N, BT, BDTYPE)\n+    a = init_input(M, K, AT, ADTYPE, a_fp8)\n+    b = init_input(K, N, BT, BDTYPE, b_fp8)\n     # run test\n-    th_c = torch.matmul(a.to(torch.float32), b.to(torch.float32))\n+    th_a = maybe_upcast(a, ADTYPE, a_fp8).to(torch.float32)\n+    if AT and a_fp8:\n+        th_a = th_a.view(th_a.shape[::-1]).T\n+    th_b = maybe_upcast(b, BDTYPE, b_fp8).to(torch.float32)\n+    if BT and b_fp8:\n+        th_b = th_b.view(th_b.shape[::-1]).T\n+    th_c = torch.matmul(th_a, th_b)\n     try:\n+        if a_fp8:\n+            a = triton.reinterpret(a, getattr(tl, ADTYPE))\n+        if b_fp8:\n+            b = triton.reinterpret(b, getattr(tl, BDTYPE))\n         tt_c = triton.ops.matmul(a, b)\n         atol, rtol = 1e-2, 0\n         if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -111,8 +111,9 @@ def _kernel(A, B, C, M, N, K,\n             b = tl.load(B)\n         else:\n             k_remaining = K - k * (BLOCK_K * SPLIT_K)\n-            a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n-            b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n+            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n+            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n+            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n         a = a.to(C.dtype.element_ty)\n         b = b.to(C.dtype.element_ty)\n         acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n@@ -149,7 +150,11 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        c_dtype = get_higher_dtype(a.dtype, b.dtype)\n+        if a.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5] or\\\n+           b.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+            c_dtype = torch.float16\n+        else:\n+            c_dtype = get_higher_dtype(a.dtype, b.dtype)\n         c = torch.empty((M, N), device=device, dtype=c_dtype)\n         if dot_out_dtype is None:\n             if c_dtype in [torch.float16, torch.float32, torch.bfloat16]:"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -574,10 +574,14 @@ def __init__(self, base, dtype):\n         self.base = base\n         self.is_cuda = base.is_cuda\n         self.device = base.device\n+        self.shape = self.base.shape\n \n     def data_ptr(self):\n         return self.base.data_ptr()\n \n+    def stride(self, i):\n+        return self.base.stride(i)\n+\n     def __str__(self) -> str:\n         return f'TensorWrapper[{self.dtype}]({self.base})'\n "}]