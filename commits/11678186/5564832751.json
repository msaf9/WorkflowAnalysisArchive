[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 53, "deletions": 25, "changes": 78, "file_content_changes": "@@ -66,6 +66,7 @@ jobs:\n           python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n+          python3 -m pip install pytest-xdist\n \n       - name: Run lit tests\n         if: ${{ env.BACKEND == 'CUDA'}}\n@@ -82,7 +83,9 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest\n+          python3 -m pytest -n 8 --ignore=runtime\n+          # run runtime tests serially to avoid race condition with cache handling.\n+          python3 -m pytest runtime/\n \n       - name: Create artifacts archive\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n@@ -107,10 +110,11 @@ jobs:\n       - name: Regression tests\n         if: ${{ contains(matrix.runner, 'A100') }}\n         run: |\n+          python3 -m pip install pytest-rerunfailures\n           cd python/test/regression\n           sudo nvidia-smi -i 0 -pm 1\n-          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n-          python3 -m pytest -vs .\n+          sudo nvidia-smi -i 0 --lock-gpu-clocks=1280,1280\n+          python3 -m pytest -vs . --reruns 10\n           sudo nvidia-smi -i 0 -rgc\n \n   Integration-Tests-Third-Party:\n@@ -221,28 +225,52 @@ jobs:\n           GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         run: |\n           OWNER_REPO=\"${{ github.repository }}\"\n-          PR_NUMBER=$(gh api --method GET repos/$OWNER_REPO/pulls -f state=closed | jq \".[] | select(.merged_at != null) | .number\" | head -1)\n-          echo \"Last merged PR number: $PR_NUMBER\"\n-\n-          BRANCH_NAME=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.head.ref')\n-          echo \"BRANCH_NAME: $BRANCH_NAME\"\n-          WORKFLOW_RUN_ID=$(gh api --method GET repos/$OWNER_REPO/actions/runs | jq --arg branch_name \"$BRANCH_NAME\" '.workflow_runs[] | select(.head_branch == $branch_name)' | jq '.id' | head -1)\n-          echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n-          ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n-          echo \"ARTIFACT_URL: $ARTIFACT_URL\"\n-\n-          if [ -n \"$ARTIFACT_URL\" ]; then\n-            echo \"Downloading artifact: $ARTIFACT_URL\"\n-            curl --location --remote-header-name -H \"Authorization: token $GH_TOKEN\" -o reference.zip \"$ARTIFACT_URL\"\n-            # Print the size of the downloaded artifact\n-            echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" reference.zip)\"\n-            echo \"Artifact size (du): $(du -sh reference.zip)\"\n-            unzip reference.zip\n-            tar -xzf artifacts.tar.gz\n-            rm reference.zip\n-            rm artifacts.tar.gz\n-            mv cache reference\n-          else\n+          echo \"OWNER_REPO: $OWNER_REPO\"\n+          PR_NUMBERS=($(gh api --method GET repos/$OWNER_REPO/pulls -f state=closed | jq -r \".[] | select(.merged_at != null) | .number\"))\n+\n+          # Not all PRs go through integration tests\n+          success=0\n+          for PR_NUMBER in \"${PR_NUMBERS[@]}\"\n+          do\n+            echo \"Last merged PR number: $PR_NUMBER\"\n+            BRANCH_NAME=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.head.ref')\n+            echo \"BRANCH_NAME: $BRANCH_NAME\"\n+            USER_ID=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.user.id')\n+            echo \"USER_ID: $USER_ID\"\n+\n+            page=1\n+            while true; do\n+              run_id=$(gh api --method GET \"repos/$OWNER_REPO/actions/runs?page=$page&per_page=100\" | jq --arg branch_name \"$BRANCH_NAME\" --arg run_name \"Integration Tests\" --arg user_id \"$USER_ID\" '.workflow_runs[] | select(.head_branch == $branch_name and .name == $run_name and .actor.id == ($user_id | tonumber))' | jq '.id' | head -1)\n+              if [ \"$run_id\" != \"\" ]; then\n+                echo \"First run ID on branch $BRANCH_NAME is: $run_id\"\n+                WORKFLOW_RUN_ID=$run_id\n+                break\n+              fi\n+\n+              ((page++))\n+            done\n+\n+            echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n+            ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n+            echo \"ARTIFACT_URL: $ARTIFACT_URL\"\n+\n+            if [ -n \"$ARTIFACT_URL\" ]; then\n+              echo \"Downloading artifact: $ARTIFACT_URL\"\n+              curl --location --remote-header-name -H \"Authorization: token $GH_TOKEN\" -o reference.zip \"$ARTIFACT_URL\"\n+              # Print the size of the downloaded artifact\n+              echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" reference.zip)\"\n+              echo \"Artifact size (du): $(du -sh reference.zip)\"\n+              unzip reference.zip\n+              tar -xzf artifacts.tar.gz\n+              rm reference.zip\n+              rm artifacts.tar.gz\n+              mv cache reference\n+              success=1\n+              break\n+            fi\n+          done\n+\n+          if [ $success -eq 0 ]; then\n             echo \"No artifact found with the name: $ARTIFACT_NAME\"\n             exit 1\n           fi"}, {"filename": "CONTRIBUTING.md", "status": "modified", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -51,3 +51,72 @@ A well-structured RFC should include:\n ## New backends\n \n Due to limited resources, we need to prioritize the number of targets we support. We are committed to providing upstream support for Nvidia and AMD GPUs. However, if you wish to contribute support for other backends, please start your project in a fork. If your backend proves to be useful and meets our performance requirements, we will discuss the possibility of upstreaming it.\n+\n+\n+## Project Structure\n+```\n+triton\n+\u251c\u2500\u2500 lib : C++ code for python library\n+\u2502   \u251c\u2500\u2500Analysis\n+\u2502   \u2502\tMemory barrier analysis\n+\u2502   \u2502\tclass to extract axis information from MLIR ops\n+\u2502   \u2502\timplementation of the shared memory allocation analysis for Triton dialect\n+\u2502   \u2502\n+\u2502   \u251c\u2500\u2500Conversion\n+\u2502   \u2502\t\u251c\u2500\u2500TritonGPUToLLVM:  Transforms TritonGPU  to LLVM;\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500TritonToTritonGPU: Transforms ops to TritonGPU ops; loading, storing, arithmetic, casting, and tensor operations.\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u2502\n+\u2502   \u251c\u2500\u2500Dialect\n+\u2502   \u2502\t\u251c\u2500\u2500Triton\n+\u2502   \u2502\t\u2502\tDefines core IR for Triton compiler\n+\u2502   \u2502\t\u251c\u2500\u2500TritonGPU\n+\u2502   \u2502\t    Defines TritonGPU operation for IR\n+\u2502   \u2502\n+\u2502   \u251c\u2500\u2500Target: contains Triton targets for converting to PTX, LLVMIR and HSACO IR targets\n+\u2502   \u2502\n+\u251c\u2500\u2500 bin\n+\u251c\u2500\u2500 cmake\n+\u251c\u2500\u2500 docs \u251c\u2500\u2500 Documentation regarding using triton\n+\u251c\u2500\u2500 include\n+\u2502   CMakelists.txt\n+\u2502   \u251c\u2500\u2500triton\n+\u2502   \u2502   \u251c\u2500\u2500\n+\u251c\u2500\u2500 python\n+\u2502   \u251c\u2500\u2500\n+\u2502   \u251c\u2500\u2500 MANIFEST.in\n+\u2502   \u251c\u2500\u2500 README.md\n+\u2502   \u251c\u2500\u2500 build\n+\u2502   \u251c\u2500\u2500 examples\n+\u2502   \u251c\u2500\u2500 pyproject.toml\n+\u2502   \u251c\u2500\u2500 setup.py: pip install for python package\n+\u2502   \u251c\u2500\u2500 src\n+\u2502   \u251c\u2500\u2500 test\n+\u2502   \u251c\u2500\u2500 triton\n+\u2502   \u2502\t\u251c\u2500\u2500 _C: Includes header files and compiled .so file for C library\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500common: Has interface for CUDA hardware backend\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500compiler: contains code for compiling source code to IR and lauching GPU kernels\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500interpreter: memory-map for tensors, converting primitives to tensors, and arethmetic ops for tensors\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500language: core of triton language, load tensors to SRAM, language logic, etc.\n+\u2502   \u2502\t\u2502\n+\u2502   \u2502\t\u251c\u2500\u2500ops: contains functions for flash-attn, softmax, cross-entropy and other torch.nn.F functions\n+\u2502   \u2502\t\u251c\u2500\u2500runtime: contains impl jit compilation, autotuning, backend drivers,cahcing, error handles, etc.\n+\u2502   \u2502\t\u251c\u2500\u2500third_party\n+\u2502   \u2502\t\u251c\u2500\u2500tools\n+\u2502   \u251c\u2500\u2500 triton.egg-info\n+\u2502   \u251c\u2500\u2500 tutorials: contains tutorials for various use-cases\n+\u251c\u2500\u2500 test\n+\u2502   \u251c\u2500\u2500Analysis\n+\u2502   \u251c\u2500\u2500Conversion\n+\u2502   \u251c\u2500\u2500Dialect\n+\u2502   \u251c\u2500\u2500Target\n+\u251c\u2500\u2500 third_party\n+\u251c\u2500\u2500 unittest\n+\u2514\u2500\u2500 utils\n+```"}, {"filename": "README.md", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -42,6 +42,8 @@ pip install cmake; # build-time dependency\n pip install -e .\n ```\n \n+\n+\n # Changelog\n \n Version 2.0 is out! New features include:\n@@ -56,6 +58,9 @@ Community contributions are more than welcome, whether it be to fix bugs or to a\n \n If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n \n+\n+\n+\n # Compatibility\n \n Supported Platforms:"}, {"filename": "docs/meetups/07-18-2023.md", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+#### Agenda:\n+1. Alternative backend development approach (e.g. AMD, Intel)\n+2. State of the documentation, is there a planned effort? If yes, what do you think is the priority?\n+3. Mechanisms for smaller technical discussions: Slack channel per topic? Dedicated meetings for some topics?\n+4. Stability, testing, regressions: Improving CI and conformance/testing for validating new back-ends.\n+5. Language improvements/pain points\n+6. Windows Support\n+7. Discussion of known/anticipated design changes for H100\n+8. Some specific more tactical areas:\n+   - int8.\n+   - A low hanging fruit is to let tl.dot take int8 and leverage mma.\n+   - Sm75.\n+   - device functions. How hard is this to support while Triton frontend traverses AST?\n+   - remove torch dependencies from the frontend. (it sounds like there is already progress on this but could be worth discussing)"}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -111,6 +111,16 @@ Reduction Ops\n     sum\n     xor_sum\n \n+Scan Ops\n+-------------\n+\n+.. autosummary::\n+    :toctree: generated\n+    :nosignatures:\n+\n+    associative_scan\n+    cumsum\n+    cumprod\n \n Atomic Ops\n ----------"}, {"filename": "include/triton/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n add_subdirectory(Conversion)\n add_subdirectory(Dialect)\n+add_subdirectory(Target)"}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -45,6 +45,12 @@ class DialectInferLayoutInterface\n   inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n                      Attribute retEncoding,\n                      std::optional<Location> location) const = 0;\n+\n+  // Verify that the encoding are compatible to be used together in a dot\n+  // operation\n+  virtual LogicalResult\n+  verifyDotOpEncodingCompatibility(Operation *op, Attribute operandEncodingA,\n+                                   Attribute operandEncodingB) const = 0;\n };\n \n } // namespace triton"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 10, "deletions": 4, "changes": 14, "file_content_changes": "@@ -31,7 +31,8 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n //   fptoui, fptosi, uitofp, sitofp,\n //   extf, tructf,\n //   extui, extsi, tructi\n-def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n+def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [Elementwise,\n+                                         SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n                                          Pure,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -44,7 +45,8 @@ def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n     let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n }\n \n-def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n+def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [Elementwise,\n+                                         SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n                                          Pure,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -58,7 +60,8 @@ def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n }\n \n // arith.bitcast doesn't support pointers\n-def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n+def TT_BitcastOp : TT_Op<\"bitcast\", [Elementwise,\n+                                     SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n                                      Pure,\n                                      /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -73,6 +76,7 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n     // TODO: Add verifier\n }\n \n+// FIXME: Not elementwise because scalars are not supported\n def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n                                      Pure,\n@@ -99,6 +103,7 @@ def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n //\n def TT_AddPtrOp : TT_Op<\"addptr\",\n                      [Pure,\n+                      Elementwise,\n                       SameOperandsAndResultShape,\n                       SameOperandsAndResultEncoding,\n                       TypesMatchWith<\"result type matches ptr type\",\n@@ -458,7 +463,8 @@ def TT_ScanReturnOp: TT_Op<\"scan.return\",\n //\n class TT_ExternElementwiseOpBase<string mnemonic, list<Trait> traits = []> :\n     TT_Op<mnemonic,\n-         traits # [SameOperandsAndResultEncoding,\n+         traits # [Elementwise,\n+                   SameOperandsAndResultEncoding,\n                    SameVariadicOperandSize]> {\n \n     let description = [{"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -14,7 +14,7 @@ class TritonTypeDef<string name, string _mnemonic>\n }\n \n // Floating-point Type\n-def TT_Float : AnyTypeOf<[F8E4M3FN, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n+def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E4M3B11FNUZ, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n "}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -8,6 +8,8 @@ namespace triton {\n \n std::unique_ptr<Pass> createCombineOpsPass();\n \n+std::unique_ptr<Pass> createReorderBroadcastPass();\n+\n std::unique_ptr<Pass>\n createRewriteTensorPointerPass(int computeCapability = 80);\n "}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.td", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -19,6 +19,15 @@ def TritonCombineOps : Pass</*cli-arg*/\"triton-combine\", /*Op*/\"mlir::ModuleOp\">\n   let dependentDialects = [\"mlir::arith::ArithDialect\"];\n }\n \n+def TritonReorderBroadcast : Pass</*cli-arg*/\"triton-reorder-broadcast\", /*Op*/\"mlir::ModuleOp\"> {\n+  let summary = \"Moves broadcast and splat after elementwise operations\";\n+  let description = [{\n+    elementwise(splat(a), splat(b), ...) => splat(elementwise(a, b, ...))\n+  }];\n+  let constructor = \"mlir::triton::createReorderBroadcastPass()\";\n+  let dependentDialects = [\"mlir::triton::TritonDialect\"];\n+}\n+\n def TritonRewriteTensorPointer : Pass</*cli-arg*/\"triton-rewrite-tensor-pointer\", /*Op*/\"mlir::ModuleOp\"> {\n   let summary = \"Rewrite load/stores with tensor pointers into legacy load/stores\";\n   let description = [{"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Attributes.h", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -0,0 +1,7 @@\n+#ifndef TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_\n+#define TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_\n+\n+#define GET_ATTRDEF_CLASSES\n+#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n+\n+#endif // TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -8,12 +8,10 @@\n \n // TritonGPU depends on Triton\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n \n-#define GET_ATTRDEF_CLASSES\n-#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n-\n #define GET_OP_CLASSES\n #include \"triton/Dialect/TritonGPU/IR/Ops.h.inc\"\n \n@@ -51,7 +49,8 @@ SmallVector<unsigned> getContigPerThread(Attribute layout);\n // for thread 0 would be [A_{0, 0}, A_{0, 0}, A_{0, 0}, A_{0, 0}], returns [1,\n // 1]. Whereas for a tensor shape [128, 128], the elements for thread 0 would be\n // [A_{0, 0}, A_{0, 1}, A_{0, 2}, A_{0, 3}], returns [1, 4].\n-SmallVector<unsigned> getUniqueContigPerThread(Type type);\n+SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n+                                               ArrayRef<int64_t> tensorShape);\n \n // Returns the number of threads per warp that have access to non-replicated\n // elements of the tensor. E.g. for a blocked layout with sizePerThread = [1,\n@@ -82,9 +81,10 @@ bool isaDistributedLayout(Attribute layout);\n \n bool isSharedEncoding(Value value);\n \n+bool isExpensiveCat(CatOp cat, Attribute &targetEncoding);\n+\n } // namespace gpu\n } // namespace triton\n-\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -18,8 +18,6 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n \n bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n \n-bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n-\n bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n // skipInit is True when we only consider the operands of the initOp but"}, {"filename": "include/triton/Target/CMakeLists.txt", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+add_subdirectory(LLVMIR)"}, {"filename": "include/triton/Target/LLVMIR/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls -name LLVMIR)\n+add_public_tablegen_target(LLVMIRIncGen)"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n-#ifndef TRITON_TARGET_LLVMIRTRANSLATION_H\n-#define TRITON_TARGET_LLVMIRTRANSLATION_H\n+#ifndef TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n+#define TRITON_TARGET_LLVM_IR_LLVM_IR_TRANSLATION_H\n #include \"llvm/ADT/StringRef.h\"\n #include <memory>\n #include <string>"}, {"filename": "include/triton/Target/LLVMIR/Passes.h", "status": "added", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -0,0 +1,17 @@\n+#ifndef TRITON_TARGET_LLVM_IR_PASSES_H\n+#define TRITON_TARGET_LLVM_IR_PASSES_H\n+\n+#include \"mlir/Pass/Pass.h\"\n+\n+namespace mlir {\n+\n+/// Create a pass to add DIScope\n+std::unique_ptr<Pass> createLLVMDIScopePass();\n+\n+/// Generate the code for registering conversion passes.\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Target/LLVMIR/Passes.h.inc\"\n+\n+} // namespace mlir\n+\n+#endif // TRITON_TARGET_LLVM_IR_PASSES_H"}, {"filename": "include/triton/Target/LLVMIR/Passes.td", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+#ifndef TRITON_TARGET_LLVMIR_PASSES\n+#define TRITON_TARGET_LLVMIR_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+def LLVMDIScope: Pass<\"enable-line-info\", \"mlir::ModuleOp\"> {\n+  let summary = \"Materialize LLVM line info\";\n+  let description = [{\n+    This pass materializes line mapping information for LLVM IR dialect operations.\n+  }];\n+\n+  let constructor = \"mlir::createLLVMDIScopePass()\";\n+}\n+\n+#endif"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 30, "deletions": 10, "changes": 40, "file_content_changes": "@@ -162,16 +162,22 @@ class AllocationAnalysis {\n     }\n   }\n \n+  template <BufferT::BufferKind T>\n+  void maybeAddScratchBuffer(Operation *op, unsigned bytes) {\n+    if (bytes > 0)\n+      allocation->addBuffer<T>(op, bytes);\n+  }\n+\n   /// Initializes temporary shared memory for a given operation.\n   void getScratchValueSize(Operation *op) {\n     if (auto reduceOp = dyn_cast<triton::ReduceOp>(op)) {\n       ReduceOpHelper helper(reduceOp);\n       unsigned bytes = helper.getScratchSizeInBytes();\n-      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto scanOp = dyn_cast<triton::ScanOp>(op)) {\n       ScanLoweringHelper helper(scanOp);\n       unsigned bytes = helper.getScratchSizeInBytes();\n-      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n       auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();\n@@ -195,7 +201,7 @@ class AllocationAnalysis {\n           srcTy.getElementType().isa<triton::PointerType>()\n               ? elems * kPtrBitWidth / 8\n               : elems * std::max<int>(8, srcTy.getElementTypeBitWidth()) / 8;\n-      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto atomicRMWOp = dyn_cast<triton::AtomicRMWOp>(op)) {\n       auto value = op->getOperand(0);\n       // only scalar requires scratch memory\n@@ -212,7 +218,7 @@ class AllocationAnalysis {\n             elemTy.isa<triton::PointerType>()\n                 ? elems * kPtrBitWidth / 8\n                 : elems * std::max<int>(8, elemTy.getIntOrFloatBitWidth()) / 8;\n-        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+        maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }\n     } else if (auto atomicCASOp = dyn_cast<triton::AtomicCASOp>(op)) {\n       auto value = op->getOperand(0);\n@@ -224,13 +230,13 @@ class AllocationAnalysis {\n       auto bytes = elemTy.isa<triton::PointerType>()\n                        ? elems * kPtrBitWidth / 8\n                        : elems * elemTy.getIntOrFloatBitWidth() / 8;\n-      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto callOp = dyn_cast<CallOpInterface>(op)) {\n       auto callable = callOp.resolveCallable();\n       auto funcOp = dyn_cast<FunctionOpInterface>(callable);\n       auto *funcAlloc = &(*funcAllocMap)[funcOp];\n       auto bytes = funcAlloc->getSharedMemorySize();\n-      allocation->addBuffer<BufferT::BufferKind::Virtual>(op, bytes);\n+      maybeAddScratchBuffer<BufferT::BufferKind::Virtual>(op, bytes);\n     }\n   }\n \n@@ -382,10 +388,19 @@ class AllocationAnalysis {\n     DenseMap<BufferT *, size_t> bufferStart;\n     calculateStarts(buffers, bufferStart);\n \n+    // NOTE: The original paper doesn't consider interference between\n+    // the bumped ranges. Buffers that previously do not interfere with\n+    // could interfere after offset bumping if their liveness ranges overlap.\n+    // Therefore, we rerun the interference graph algorithm after bumping so\n+    // that we regroup the buffers and color them again. Since we always\n+    // increase the buffer offset and keep reducing conflicts, we will\n+    // eventually reach a fixed point.\n     GraphT interference;\n     buildInterferenceGraph(buffers, bufferStart, interference);\n-\n-    allocate(buffers, bufferStart, interference);\n+    do {\n+      allocate(buffers, interference, bufferStart);\n+      buildInterferenceGraph(buffers, bufferStart, interference);\n+    } while (!interference.empty());\n   }\n \n   /// Computes the initial shared memory offsets.\n@@ -451,6 +466,8 @@ class AllocationAnalysis {\n   void buildInterferenceGraph(const SmallVector<BufferT *> &buffers,\n                               const DenseMap<BufferT *, size_t> &bufferStart,\n                               GraphT &interference) {\n+    // Reset interference graph\n+    interference.clear();\n     for (auto x : buffers) {\n       for (auto y : buffers) {\n         if (x == y)\n@@ -473,8 +490,10 @@ class AllocationAnalysis {\n \n   /// Finalizes shared memory offsets considering interference.\n   void allocate(const SmallVector<BufferT *> &buffers,\n-                const DenseMap<BufferT *, size_t> &bufferStart,\n-                const GraphT &interference) {\n+                const GraphT &interference,\n+                DenseMap<BufferT *, size_t> &bufferStart) {\n+    // Reset shared memory size\n+    allocation->sharedMemorySize = 0;\n     // First-fit graph coloring\n     // Neighbors are nodes that interfere with each other.\n     // We color a node by finding the index of the first available\n@@ -508,6 +527,7 @@ class AllocationAnalysis {\n         adj = std::max(adj, bufferStart.lookup(y) + y->size);\n       }\n       x->offset = bufferStart.lookup(x) + colors.lookup(x) * adj;\n+      bufferStart[x] = x->offset;\n       allocation->sharedMemorySize =\n           std::max(allocation->sharedMemorySize, x->offset + x->size);\n     }"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -918,7 +918,8 @@ unsigned ModuleAxisInfoAnalysis::getPtrContiguity(Value ptr) {\n   auto order = triton::gpu::getOrder(layout);\n   unsigned align = getPtrAlignment(ptr);\n \n-  auto uniqueContigPerThread = triton::gpu::getUniqueContigPerThread(tensorTy);\n+  auto uniqueContigPerThread =\n+      triton::gpu::getUniqueContigPerThread(layout, tensorTy.getShape());\n   assert(order[0] < uniqueContigPerThread.size() &&\n          \"Unxpected uniqueContigPerThread size\");\n   unsigned contiguity = uniqueContigPerThread[order[0]];"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 28, "deletions": 5, "changes": 33, "file_content_changes": "@@ -10,11 +10,31 @@\n \n namespace mlir {\n \n+namespace {\n+\n+int getParentAxis(Attribute layout, int axis) {\n+  if (auto sliceEncoding = layout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    axis = axis < sliceEncoding.getDim() ? axis : axis + 1;\n+    return getParentAxis(sliceEncoding.getParent(), axis);\n+  }\n+  return axis;\n+}\n+\n+SmallVector<unsigned> getParentOrder(Attribute layout) {\n+  if (auto sliceEncoding = layout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    return getParentOrder(sliceEncoding.getParent());\n+  }\n+  return triton::gpu::getOrder(layout);\n+}\n+\n+} // namespace\n+\n bool ReduceOpHelper::isFastReduction() {\n   // Disable fast reduction only for debugging purpose\n   if (::triton::tools::getBoolEnv(\"DISABLE_FAST_REDUCTION\"))\n     return false;\n-  return axis == triton::gpu::getOrder(getSrcLayout())[0];\n+  return getParentAxis(getSrcLayout(), axis) ==\n+         getParentOrder(getSrcLayout())[0];\n }\n \n unsigned ReduceOpHelper::getInterWarpSize() {\n@@ -40,7 +60,9 @@ unsigned ReduceOpHelper::getInterWarpSizeWithUniqueData() {\n \n unsigned ReduceOpHelper::getIntraWarpSizeWithUniqueData() {\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n-  return std::min(srcReduceDimSize,\n+  unsigned elementPerThreads = triton::gpu::getUniqueContigPerThread(\n+      getSrcLayout(), getSrcShape())[axis];\n+  return std::min(srcReduceDimSize / elementPerThreads,\n                   triton::gpu::getThreadsPerWarpWithUniqueData(\n                       getSrcLayout(), getSrcShape())[axis]);\n }\n@@ -64,9 +86,10 @@ SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n \n   auto argLayout = getSrcLayout();\n   auto argLayoutMma = argLayout.dyn_cast<triton::gpu::MmaEncodingAttr>();\n-  // if (argLayoutMma && argLayoutMma.getVersionMajor() == 2 &&\n-  //     triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n-  //   return {{1, 1}, {1, 1}};\n+\n+  // that case doesn't need inter-warp communication\n+  if (isFastReduction() && triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n+    return {{0, 0}, {0, 0}};\n \n   /// shared memory block0\n   smemShapes[0] = convertType<unsigned>(getSrcShape());"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -20,7 +20,7 @@ namespace SharedToDotOperandMMAv1 {\n using CoordTy = SmallVector<Value>;\n using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n \n-SmallVector<CoordTy> getMNCoords(Value thread,\n+SmallVector<CoordTy> getMNCoords(Value thread, Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  ArrayRef<unsigned int> wpt,\n                                  const MmaEncodingAttr &mmaLayout,\n@@ -187,8 +187,8 @@ struct ConvertLayoutOpConversion\n         auto [isARow, isBRow, isAVec4, isBVec4, _] =\n             mmaLayout.decodeVoltaLayoutStates();\n         auto coords = SharedToDotOperandMMAv1::getMNCoords(\n-            threadId, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout, shape,\n-            isARow, isBRow, isAVec4, isBVec4);\n+            threadId, loc, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout,\n+            shape, isARow, isBRow, isAVec4, isBVec4);\n         return coords[elemId];\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -339,7 +339,7 @@ namespace SharedToDotOperandMMAv1 {\n using CoordTy = SmallVector<Value>;\n using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n \n-SmallVector<CoordTy> getMNCoords(Value thread,\n+SmallVector<CoordTy> getMNCoords(Value thread, Location loc,\n                                  ConversionPatternRewriter &rewriter,\n                                  ArrayRef<unsigned int> wpt,\n                                  const MmaEncodingAttr &mmaLayout,\n@@ -348,7 +348,6 @@ SmallVector<CoordTy> getMNCoords(Value thread,\n   static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n \n   auto *ctx = thread.getContext();\n-  auto loc = UnknownLoc::get(ctx);\n   Value _1 = i32_val(1);\n   Value _2 = i32_val(2);\n   Value _4 = i32_val(4);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 547, "deletions": 646, "changes": 1193, "file_content_changes": "@@ -4,6 +4,241 @@ using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n \n+/* ----- FP8E5M2 ------ */\n+// This data-type is the standard FP8E5M2 format\n+\n+const std::string Fp16_to_Fp8E5M2 =\n+    \"{                            \\n\"\n+    \".reg .b32 a<2>;              \\n\"\n+    \"and.b32 a0, $1, 0x7fff7fff;  \\n\"           // a0 &= 0x7fff7fff\n+    \"and.b32 a1, $2, 0x7fff7fff;  \\n\"           // (strip sign)\n+    \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n+    \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n+    \"lop3.b32 a0, $1, 0x80008000, a0, 0xea; \\n\" // a0 = a0|(0x80008000&in0)\n+    \"lop3.b32 a1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+    \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n+    \"}\";\n+\n+const std::string Fp8E5M2_to_Fp16 = \"{                           \\n\"\n+                                    \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n+                                    \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n+                                    \"}\";\n+\n+const std::string Fp8E5M2_to_Bf16 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+    \"prmt.b32 a0, 0, $2, 0x5140;            \\n\" // a0 = 0xf300f400\n+    \"prmt.b32 a1, 0, $2, 0x7362;            \\n\" // a1 = 0xf100f200\n+    \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+    \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+    \"shr.b32  b0, b0, 3;                    \\n\" // b0 >>= 3\n+    \"shr.b32  b1, b1, 3;                    \\n\" // shift into bf16 position\n+    \"add.u32  b0, b0, 0x38003800;           \\n\" // b0.exp += 2**7-2**4\n+                                                // exponent compensate = 112\n+    \"add.u32  b1, b1, 0x38003800;           \\n\" // b1 += 112<<7 | 112<<7<<16\n+    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+    \"}\";\n+\n+const std::string Bf16_to_Fp8E5M2 =\n+    \"{                                           \\n\" // bf16=fp8>>3 + 112<<7\n+    \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+    \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+    \"mov.u32 fp8_min, 0x38003800;                \\n\" // so bf16_min = 0x3800\n+    \"mov.u32 fp8_max, 0x57e057e0;                \\n\" // so bf16_max = 0x57e0\n+    \"mov.u32 rn_, 0x00100010;                    \\n\" // round to nearest\n+    \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+    \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+    \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+    \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+    \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+    // nosign = clamp(nosign, min, max)\n+    \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+    \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+    \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\"\n+    \"min.u32 nosign_0_0, nosign_0_0, 0x57e00000; \\n\"\n+    \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+    \"max.u32 nosign_0_1, nosign_0_1, 0x3800;     \\n\"\n+    \"min.u32 nosign_0_1, nosign_0_1, 0x57e0;     \\n\"\n+    \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+    \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+    \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\"\n+    \"min.u32 nosign_1_0, nosign_1_0, 0x57e00000; \\n\"\n+    \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+    \"max.u32 nosign_1_1, nosign_1_1, 0x3800;     \\n\"\n+    \"min.u32 nosign_1_1, nosign_1_1, 0x57e0;     \\n\"\n+    \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+    \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+    \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+    \"sub.u32 nosign0, nosign0, 0x38003800;       \\n\" // nosign0-=0x38003800\n+    \"sub.u32 nosign1, nosign1, 0x38003800;       \\n\" // (compensate offset)\n+    \"shl.b32 nosign0, nosign0, 3;                \\n\" // nosign0 <<= 3\n+    \"shl.b32 nosign1, nosign1, 3;                \\n\" // shift into to fp8e4\n+    \"prmt.b32 nosign, nosign0, nosign1, 0x7531;  \\n\" // nosign0 = 0xf100f200\n+                                                     // nosign1 = 0xf300f400\n+                                                     // nosign = 0xf3f4f1f2\n+    \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+    \"}\";\n+\n+/* ----- FP8E4M3B15 ------ */\n+// This data-type is a variant of the standard FP8E4M3 format.\n+// It was designed for fast software conversion to FP16 on\n+// nvidia GPUs that do not support it natively.\n+// Specifically, this data-type:\n+//    - has infinities\n+//    - has multiple nans (when all exponent bits are 1)\n+//    - has an exponent bias of 15 (vs. 7 for fp8e4m3)\n+\n+// Fp8E4M3B15 -> Fp16 (packed)\n+// fast conversion code provided by Scott Gray @ OpenAI\n+// $0 = (($2 << 1) & 0x80008000u) | (($2 << 7) & 0x3f803f80u);\n+// $1 = (($2 << 0) & 0x80008000u) | (($2 << 0) & 0x3f803f80u);\n+// WARN: subnormal (0bs0000xxx) are not handled\n+const std::string Fp8E4M3B15_to_Fp16 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>;                        \\n\"\n+    \"shl.b32 a0, $2, 1;                     \\n\"\n+    \"shl.b32 a1, $2, 7;                     \\n\"\n+    \"and.b32  $0, a0, 0x80008000;           \\n\"\n+    \"lop3.b32 $0, $0, a1, 0x3f803f80, 0xf8; \\n\"\n+    \"and.b32  $1, $2, 0x80008000;           \\n\"\n+    \"lop3.b32 $1, $1, $2, 0x3f803f80, 0xf8; \\n\"\n+    \"}\";\n+\n+// Fp16 -> Fp8E4M3B15 (packed)\n+// fast conversion code provided by Scott Gray @ OpenAI\n+// ret = ((e4.x >> 1) & (0x80008000u >> 1)) |\n+//       ((e4.x >> 7) & (0x3f803f80u >> 7)) |\n+//       ((e4.y >> 0) & (0x80008000u >> 0)) |\n+//       ((e4.y >> 0) & (0x3f803f80u >> 0)) ;\n+// WARN: subnormal (0bs0000xxx) are not handled\n+const std::string Fp16_to_Fp8E4M3B15 =\n+    \"{                                       \\n\"\n+    \".reg .b32 a<2>;                         \\n\"\n+    \"shr.b32  a0, $1, 1;                     \\n\"\n+    \"shr.b32  a1, $1, 7;                     \\n\"\n+    \"and.b32  $0,     a0, 0x40004000;        \\n\"\n+    \"lop3.b32 $0, $0, a1, 0x007f007f, 0xf8;  \\n\"\n+    \"lop3.b32 $0, $0, $2, 0x80008000, 0xf8;  \\n\"\n+    \"lop3.b32 $0, $0, $2, 0x3f803f80, 0xf8;  \\n\"\n+    \"}\";\n+\n+/* ----- FP8E4M3 ------ */\n+// Note: when handled by software, this format\n+// does not handle denormals and has\n+// more than a single NaN values.\n+\n+// Fp8E4M3 -> Fp16 (packed)\n+const std::string Fp8E4M3_to_Fp16 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+    \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+    \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+    \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n+    \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+    \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1\n+    \"shr.b32  b1, b1, 1;                    \\n\" // shift into fp16 position\n+    \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3\n+                                                // exponent compensate = 8\n+    \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8<<10 | 8<<10<<16\n+    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+    \"}\";\n+\n+// Fp16 -> Fp8E4M3 (packed)\n+const std::string Fp16_to_Fp8E4M3 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n+    \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000\n+                                                // (compensate offset)\n+    \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000\n+                                                // (8 << 10 | 8 << 10 << 16)\n+    \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1\n+    \"shl.b32 a1, a1, 1;                     \\n\" // shift into fp8e4 position\n+    \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff\n+    \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n+    \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080\n+    \"add.u32 a1, a1, 0x00800080;            \\n\" // (round to nearest)\n+    \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0|(0x80008000&in0)\n+    \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n+    \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n+    \"}\";\n+\n+// WARN: subnormal (0bs0000xxx) are not handled\n+const std::string Fp8E4M3_to_Bf16 =\n+    \"{                                      \\n\"\n+    \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+    \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+    \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+    \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n+    \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n+    \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n+    \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n+    \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n+                                                // exponent compensate = 120\n+    \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n+    \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+    \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+    \"}\";\n+\n+const std::string Bf16_to_Fp8E4M3 =\n+    \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n+    \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+    \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+    \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n+    \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n+    \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n+    \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+    \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+    \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+    \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+    \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+    // nosign = clamp(nosign, min, max)\n+    \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+    \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+    \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n+    \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n+    \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+    \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n+    \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n+    \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+    \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+    \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n+    \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n+    \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+    \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n+    \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n+    \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+    \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+    \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+    \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n+    \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n+    \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n+    \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n+    \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n+                                                     // nosign1 = 0x00f300f4\n+                                                     // nosign = 0xf3f4f1f2\n+    \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+    \"}\";\n+\n+/* ----- Packed integer to BF16 ------ */\n+const std::string S8_to_Bf16 =\n+    \"{                                           \\n\"\n+    \".reg .s8 s<4>;                              \\n\"\n+    \".reg .f32 f<4>;                             \\n\"\n+    \"mov.b32 {s0, s1, s2, s3}, $2;               \\n\" // unpack\n+    \"cvt.rn.f32.s8 f0, s0;                       \\n\" // no s8->bf16 pre-Hopper\n+    \"cvt.rn.f32.s8 f1, s1;                       \\n\" // fi[0:15] is always 0\n+    \"cvt.rn.f32.s8 f2, s2;                       \\n\" //\n+    \"cvt.rn.f32.s8 f3, s3;                       \\n\" //\n+    \"prmt.b32 $0, f0, f1, 0x7632;                \\n\" // f32->bf16 + pack\n+    \"prmt.b32 $1, f2, f3, 0x7632;                \\n\" //\n+    \"}\";\n+\n static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n                                         Type inType, Type ouType) {\n   auto inTensorTy = inType.dyn_cast<RankedTensorType>();\n@@ -129,449 +364,174 @@ inline SmallVector<Value> packI32(const SmallVector<Value> &inValues,\n   return outValues;\n }\n \n-struct FpToFpOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::FpToFpOp> {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  typedef std::function<SmallVector<Value>(\n-      Location, ConversionPatternRewriter &, const Value &, const Value &,\n-      const Value &, const Value &)>\n-      ConvertorT;\n-  /* ------------------ */\n-  // FP8 -> FP16\n-  /* ------------------ */\n-\n-  static SmallVector<Value>\n-  convertFp8x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const char *ptxAsm, const Value &v0, const Value &v1,\n-                       const Value &v2, const Value &v3) {\n-    auto ctx = rewriter.getContext();\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    Value fp8x4Vec = undef(fp8x4VecTy);\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n-    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n-\n-    PTXBuilder builder;\n-    auto &ptxOp = *builder.create(ptxAsm);\n-\n-    auto *o0 = builder.newOperand(\"=r\");\n-    auto *o1 = builder.newOperand(\"=r\");\n-    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-    ptxOp({o0, o1, i}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n-    auto fp16x2x2StructTy =\n-        struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n-    auto fp16x2x2Struct =\n-        builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n-    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct, 0);\n-    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct, 1);\n-    return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n-            extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n-            extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n-            extract_element(f16_ty, fp16x2Vec1, i32_val(1))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp8E4M3x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n-        \"{                                      \\n\"\n-        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n-        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n-        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n-        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-        \"shr.b32  b0, b0, 1;                    \\n\" // b0 >>= 1\n-        \"shr.b32  b1, b1, 1;                    \\n\" // shift into fp16 position\n-        \"add.u32  b0, b0, 0x20002000;           \\n\" // b0.exp += 2**4-2**3\n-                                                    // exponent compensate = 8\n-        \"add.u32  b1, b1, 0x20002000;           \\n\" // b1 += 8<<10 | 8<<10<<16\n-        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n-        \"}\";\n-    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  }\n+typedef std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n+                                         const Value &, const Value &,\n+                                         const Value &, const Value &)>\n+    ConverterT;\n \n-  static SmallVector<Value>\n-  convertFp8E5M2x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    // exponent bias of Fp8E5M2 and Fp16 are the same\n-    auto *ptxAsm = \"{                           \\n\"\n-                   \"prmt.b32 $0, 0, $2, 0x5140; \\n\\t\"\n-                   \"prmt.b32 $1, 0, $2, 0x7362; \\n\\t\"\n-                   \"}\";\n-    return convertFp8x4ToFp16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  }\n+static ConverterT makeConverterFromPtx(const std::string &ptxAsm, Type inType,\n+                                       Type outType) {\n \n-  /* ------------------ */\n-  // FP8 -> BF16\n-  /* ------------------ */\n-  static SmallVector<Value>\n-  convertFp8x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const char *ptxAsm, const Value &v0, const Value &v1,\n-                       const Value &v2, const Value &v3) {\n+  ConverterT converter = [ptxAsm, inType, outType](\n+                             Location loc, ConversionPatternRewriter &rewriter,\n+                             const Value &v0, const Value &v1, const Value &v2,\n+                             const Value &v3) -> SmallVector<Value> {\n+    SmallVector<Value> v = {v0, v1, v2, v3};\n     auto ctx = rewriter.getContext();\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    Value fp8x4Vec = undef(fp8x4VecTy);\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n-    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n-    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n-\n+    int inBitwidth = inType.getIntOrFloatBitWidth();\n+    int outBitwidth = outType.getIntOrFloatBitWidth();\n+    // first, we pack `v` into 32-bit ints\n+    int inVecWidth = 32 / inBitwidth;\n+    auto inVecTy = vec_ty(inType, inVecWidth);\n+    SmallVector<Value> inPacked(4 / inVecWidth, undef(inVecTy));\n+    for (size_t i = 0; i < 4; i++)\n+      inPacked[i / inVecWidth] = insert_element(\n+          inVecTy, inPacked[i / inVecWidth], v[i], i32_val(i % inVecWidth));\n+    for (size_t i = 0; i < inPacked.size(); i++)\n+      inPacked[i] = bitcast(inPacked[i], i32_ty);\n+\n+    // then, we run the provided inline PTX\n+    int outVecWidth = 32 / outBitwidth;\n+    int outNums = 4 / outVecWidth;\n     PTXBuilder builder;\n+    SmallVector<PTXBuilder::Operand *> operands;\n+    for (int i = 0; i < outNums; i++)\n+      operands.push_back(builder.newOperand(\"=r\"));\n+    for (Value inVal : inPacked)\n+      operands.push_back(builder.newOperand(inVal, \"r\"));\n     auto &ptxOp = *builder.create(ptxAsm);\n-\n-    auto *o0 = builder.newOperand(\"=r\");\n-    auto *o1 = builder.newOperand(\"=r\");\n-    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n-    ptxOp({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n-\n-    auto bf16x2VecTy = vec_ty(i16_ty, 2);\n-    auto bf16x2x2StructTy =\n-        struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n-    auto bf16x2x2Struct =\n-        builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n-    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct, 0);\n-    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct, 1);\n-    return {extract_element(i16_ty, bf16x2Vec0, i32_val(0)),\n-            extract_element(i16_ty, bf16x2Vec0, i32_val(1)),\n-            extract_element(i16_ty, bf16x2Vec1, i32_val(0)),\n-            extract_element(i16_ty, bf16x2Vec1, i32_val(1))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp8E4M3x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n-        \"{                                      \\n\"\n-        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n-        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n-        \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n-        \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n-        \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n-        \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n-        \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n-                                                    // exponent compensate = 120\n-        \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n-        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n-        \"}\";\n-    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  };\n-\n-  static SmallVector<Value>\n-  convertFp8E5M2x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // WARN: subnormal (0bs00000xx) are not handled\n-        \"{                                      \\n\"\n-        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n-        \"prmt.b32 a0, 0, $2, 0x5140;            \\n\" // a0 = 0xf300f400\n-        \"prmt.b32 a1, 0, $2, 0x7362;            \\n\" // a1 = 0xf100f200\n-        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // b0 = a0 & 0x7fff7fff\n-        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-        \"shr.b32  b0, b0, 3;                    \\n\" // b0 >>= 3\n-        \"shr.b32  b1, b1, 3;                    \\n\" // shift into bf16 position\n-        \"add.u32  b0, b0, 0x38003800;           \\n\" // b0.exp += 2**7-2**4\n-                                                    // exponent compensate = 112\n-        \"add.u32  b1, b1, 0x38003800;           \\n\" // b1 += 112<<7 | 112<<7<<16\n-        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n-        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n-        \"}\";\n-    return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+    ptxOp(operands, /*onlyAttachMLIRArgs=*/true);\n+    auto outVecTy = vec_ty(outType, outVecWidth);\n+    SmallVector<Value> outPacked;\n+    if (outNums == 1)\n+      outPacked.push_back(builder.launch(rewriter, loc, outVecTy, false));\n+    else {\n+      auto outStructTy = struct_ty(SmallVector<Type>(outNums, outVecTy));\n+      auto outStruct = builder.launch(rewriter, loc, outStructTy, false);\n+      for (int i = 0; i < outNums; i++)\n+        outPacked.push_back(extract_val(outVecTy, outStruct, i));\n+    }\n+    // unpack the output\n+    SmallVector<Value> ret;\n+    for (size_t i = 0; i < 4; i++)\n+      ret.push_back(extract_element(outType, outPacked[i / outVecWidth],\n+                                    i32_val(i % outVecWidth)));\n+    return ret;\n   };\n+  return converter;\n+}\n \n-  /* ------------------ */\n-  // FP16 -> FP8\n-  /* ------------------ */\n-\n-  static SmallVector<Value>\n-  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const char *ptxAsm, const Value &v0, const Value &v1,\n-                       const Value &v2, const Value &v3) {\n-    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n-    Value fp16x2Vec0 = undef(fp16x2VecTy);\n-    Value fp16x2Vec1 = undef(fp16x2VecTy);\n-    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n-    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n-    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n-    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n-    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n-    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n-\n-    PTXBuilder builder;\n-    auto &ptxOp = *builder.create(ptxAsm);\n-\n-    auto *o = builder.newOperand(\"=r\");\n-    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n-    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n-    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // WARN: subnormal Fp8s are not handled\n-        \"{                                      \\n\"\n-        \".reg .b32 a<2>, b<2>;                  \\n\" // see Fp8E4M3x4ToFp16x4\n-        \"sub.u32 a0, $1, 0x20002000;            \\n\" // a0 = input0 - 0x20002000\n-                                                    // (compensate offset)\n-        \"sub.u32 a1, $2, 0x20002000;            \\n\" // a1 = input1 - 0x20002000\n-                                                    // (8 << 10 | 8 << 10 << 16)\n-        \"shl.b32 a0, a0, 1;                     \\n\" // a0 <<= 1\n-        \"shl.b32 a1, a1, 1;                     \\n\" // shift into fp8e4 position\n-        \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\" // a0 &= 0x7fff7fff\n-        \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\" // (strip sign)\n-        \"add.u32 a0, a0, 0x00800080;            \\n\" // a0 += 0x00800080\n-        \"add.u32 a1, a1, 0x00800080;            \\n\" // (round to nearest)\n-        \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\" // b0 = a0|(0x80008000&in0)\n-        \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n-        \"prmt.b32 $0, b0, b1, 0x7531;           \\n\" // output = b1b0\n-        \"}\";\n-    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  }\n+class MultipleOperandsRange\n+    : public iterator_range<SmallVector<SmallVector<Value>>::iterator> {\n+  using ContainerT = SmallVector<SmallVector<Value>>;\n \n-  static SmallVector<Value>\n-  convertFp16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm =\n-        \"{                            \\n\"\n-        \".reg .b32 a<2>;              \\n\"\n-        \"and.b32 a0, $1, 0x7fff7fff;  \\n\"           // a0 &= 0x7fff7fff\n-        \"and.b32 a1, $2, 0x7fff7fff;  \\n\"           // (strip sign)\n-        \"add.u32 a0, a0, 0x00800080;  \\n\"           // a0 += 0x00800080\n-        \"add.u32 a1, a1, 0x00800080;  \\n\"           // (round to nearest)\n-        \"lop3.b32 a0, $1, 0x80008000, a0, 0xea; \\n\" // a0 = a0|(0x80008000&in0)\n-        \"lop3.b32 a1, $2, 0x80008000, a1, 0xea; \\n\" // (restore sign)\n-        \"prmt.b32 $0, a0, a1, 0x7531; \\n\\t\"         // output = a1a0\n-        \"}\";\n-    return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  }\n-\n-  /* ------------------ */\n-  // FP32 -> FP8\n-  /* ------------------ */\n-\n-  static SmallVector<Value>\n-  convertFp32x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n+public:\n+  using iterator_range<ContainerT::iterator>::iterator_range;\n+  ContainerT::reference operator[](ContainerT::size_type idx) {\n+    return begin()[idx];\n   }\n-\n-  static SmallVector<Value>\n-  convertFp32x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8E5M2x4(loc, rewriter, c0, c1, c2, c3);\n+  ContainerT::const_reference operator[](ContainerT::size_type idx) const {\n+    return begin()[idx];\n   }\n+  ContainerT::size_type size() const { return end() - begin(); }\n+};\n \n-  /* ------------------ */\n-  // BF16 -> FP8\n-  /* ------------------ */\n-\n-  static SmallVector<Value>\n-  convertBf16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const char *ptxAsm, const Value &v0, const Value &v1,\n-                       const Value &v2, const Value &v3) {\n-    auto bf16x2VecTy = vec_ty(i16_ty, 2);\n-    Value bf16x2Vec0 = undef(bf16x2VecTy);\n-    Value bf16x2Vec1 = undef(bf16x2VecTy);\n-    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v0, i32_val(0));\n-    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v1, i32_val(1));\n-    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v2, i32_val(0));\n-    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v3, i32_val(1));\n-    bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);\n-    bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n+// Base pattern for elementwise conversion using ConcreteT. Unpacks individual\n+// elements from a `!llvm.struct` via `llvm.extactvalue`, calls\n+// ConcreteT::createDestOps on each element, and packs them back into an\n+// `!llvm.struct` using `llvm.insertvalue`.\n+//\n+// Also supports processing the inputs in a vectorized form by consuming and\n+// producing multiple operand sets in ConcreteT::createDestOps.\n+template <typename SourceOp, typename ConcreteT>\n+class ElementwiseOpConversionBase\n+    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n+public:\n+  using OpAdaptor = typename SourceOp::Adaptor;\n \n-    PTXBuilder builder;\n-    auto &ptxOp = *builder.create(ptxAsm);\n+  explicit ElementwiseOpConversionBase(\n+      TritonGPUToLLVMTypeConverter &typeConverter, PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n-    auto *o = builder.newOperand(\"=r\");\n-    auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n-    auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n-    ptxOp({o, i0, i1}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n-  }\n+  LogicalResult\n+  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto resultTy = op.getType();\n+    Location loc = op->getLoc();\n+    // element type\n+    auto resultElementTy = getElementTypeOrSelf(resultTy);\n+    Type elemTy = this->getTypeConverter()->convertType(resultElementTy);\n+    SmallVector<SmallVector<Value>> allOperands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto argTy = op->getOperand(0).getType();\n+      auto subOperands = this->getTypeConverter()->unpackLLElements(\n+          loc, operand, rewriter, argTy);\n+      subOperands = unpackI32(subOperands, argTy, rewriter, loc,\n+                              this->getTypeConverter());\n+      allOperands.resize(subOperands.size());\n+      for (auto v : llvm::enumerate(subOperands))\n+        allOperands[v.index()].push_back(v.value());\n+    }\n+    if (allOperands.size() == 0)\n+      allOperands.push_back({});\n \n-  static SmallVector<Value>\n-  convertBf16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n-        \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n-        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n-        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n-        \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n-        \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n-        \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n-        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n-        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n-        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n-        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n-        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-\n-        // nosign = clamp(nosign, min, max)\n-        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n-        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n-        \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n-        \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n-        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n-        \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n-        \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n-        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n-        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n-        \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n-        \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n-        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n-        \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n-        \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n-        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n-\n-        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n-        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n-        \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n-        \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n-        \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n-        \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n-        \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n-                                                         // nosign1 = 0x00f300f4\n-                                                         // nosign = 0xf3f4f1f2\n-        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n-        \"}\";\n-    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n-  };\n+    SmallVector<Value> resultVals;\n+    for (auto it = allOperands.begin(), end = allOperands.end(); it != end;) {\n+      auto curr = static_cast<const ConcreteT *>(this)->createDestOps(\n+          op, adaptor, rewriter, elemTy, MultipleOperandsRange(it, end), loc);\n+      if (curr.size() == 0)\n+        return failure();\n+      for (auto v : curr) {\n+        if (!static_cast<bool>(v))\n+          return failure();\n+        resultVals.push_back(v);\n+      }\n+      it += curr.size();\n+    }\n+    if (op->getNumOperands() > 0) {\n+      auto argTy = op->getOperand(0).getType();\n+      resultVals = reorderValues(resultVals, argTy, resultTy);\n+    }\n+    resultVals =\n+        packI32(resultVals, resultTy, rewriter, loc, this->getTypeConverter());\n+    Value view = this->getTypeConverter()->packLLElements(loc, resultVals,\n+                                                          rewriter, resultTy);\n+    rewriter.replaceOp(op, view);\n \n-  static SmallVector<Value>\n-  convertBf16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n-        \"{                                           \\n\" // bf16=fp8>>3 + 112<<7\n-        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n-        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n-        \"mov.u32 fp8_min, 0x38003800;                \\n\" // so bf16_min = 0x3800\n-        \"mov.u32 fp8_max, 0x57e057e0;                \\n\" // so bf16_max = 0x57e0\n-        \"mov.u32 rn_, 0x00100010;                    \\n\" // round to nearest\n-        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n-        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n-        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n-        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n-        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n-\n-        // nosign = clamp(nosign, min, max)\n-        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n-        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n-        \"max.u32 nosign_0_0, nosign_0_0, 0x38000000; \\n\"\n-        \"min.u32 nosign_0_0, nosign_0_0, 0x57e00000; \\n\"\n-        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n-        \"max.u32 nosign_0_1, nosign_0_1, 0x3800;     \\n\"\n-        \"min.u32 nosign_0_1, nosign_0_1, 0x57e0;     \\n\"\n-        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n-        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n-        \"max.u32 nosign_1_0, nosign_1_0, 0x38000000; \\n\"\n-        \"min.u32 nosign_1_0, nosign_1_0, 0x57e00000; \\n\"\n-        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n-        \"max.u32 nosign_1_1, nosign_1_1, 0x3800;     \\n\"\n-        \"min.u32 nosign_1_1, nosign_1_1, 0x57e0;     \\n\"\n-        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n-\n-        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n-        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n-        \"sub.u32 nosign0, nosign0, 0x38003800;       \\n\" // nosign0-=0x38003800\n-        \"sub.u32 nosign1, nosign1, 0x38003800;       \\n\" // (compensate offset)\n-        \"shl.b32 nosign0, nosign0, 3;                \\n\" // nosign0 <<= 3\n-        \"shl.b32 nosign1, nosign1, 3;                \\n\" // shift into to fp8e4\n-        \"prmt.b32 nosign, nosign0, nosign1, 0x7531;  \\n\" // nosign0 = 0xf100f200\n-                                                         // nosign1 = 0xf300f400\n-                                                         // nosign = 0xf3f4f1f2\n-        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n-        \"}\";\n-    return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+    return success();\n   }\n+};\n \n-  /* ------------------ */\n-  // FP8 -> FP32\n-  /* ------------------ */\n-\n-  static SmallVector<Value>\n-  convertFp8E4M3x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-    return {convertFp16ToFp32(loc, rewriter, fp16Values[0]),\n-            convertFp16ToFp32(loc, rewriter, fp16Values[1]),\n-            convertFp16ToFp32(loc, rewriter, fp16Values[2]),\n-            convertFp16ToFp32(loc, rewriter, fp16Values[3])};\n-  }\n+template <typename SourceOp, typename DestOp>\n+struct ElementwiseOpConversion\n+    : public ElementwiseOpConversionBase<\n+          SourceOp, ElementwiseOpConversion<SourceOp, DestOp>> {\n+  using Base =\n+      ElementwiseOpConversionBase<SourceOp,\n+                                  ElementwiseOpConversion<SourceOp, DestOp>>;\n+  using Base::Base;\n+  using OpAdaptor = typename Base::OpAdaptor;\n \n-  static SmallVector<Value>\n-  convertFp8E5M2x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto fp16Values = convertFp8E5M2x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-    return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n-  }\n+  explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n+                                   PatternBenefit benefit = 1)\n+      : ElementwiseOpConversionBase<SourceOp, ElementwiseOpConversion>(\n+            typeConverter, benefit) {}\n \n-  //\n-\n-  static SmallVector<Value>\n-  convertFp8E4M3x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-    return {rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])};\n+  // An interface to support variant DestOp builder.\n+  SmallVector<DestOp> createDestOps(SourceOp op, OpAdaptor adaptor,\n+                                    ConversionPatternRewriter &rewriter,\n+                                    Type elemTy, MultipleOperandsRange operands,\n+                                    Location loc) const {\n+    return {rewriter.create<DestOp>(loc, elemTy, operands[0],\n+                                    adaptor.getAttributes().getValue())};\n   }\n+};\n \n-  static SmallVector<Value>\n-  convertFp64x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n-    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n-    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n-    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n-    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n-  }\n+// Attempts to use vectorized conversions via inline PTX when possible.\n+struct FpToFpOpConversion\n+    : public ElementwiseOpConversionBase<triton::FpToFpOp, FpToFpOpConversion> {\n+  using ElementwiseOpConversionBase<\n+      triton::FpToFpOp, FpToFpOpConversion>::ElementwiseOpConversionBase;\n \n   static Value convertBf16ToFp32(Location loc,\n                                  ConversionPatternRewriter &rewriter,\n@@ -619,156 +579,67 @@ struct FpToFpOpConversion\n     return builder.launch(rewriter, loc, f16_ty, false);\n   }\n \n-  ConvertorT getConversionFunc(Type srcTy, Type dstTy) const {\n-    auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNType>();\n+  ConverterT getConversionFunc(Type srcTy, Type dstTy) const {\n+    auto F8E4M3B15TyID = TypeID::get<mlir::Float8E4M3B11FNUZType>();\n+    auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNUZType>();\n     auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n     auto F16TyID = TypeID::get<mlir::Float16Type>();\n     auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n     auto F32TyID = TypeID::get<mlir::Float32Type>();\n     auto F64TyID = TypeID::get<mlir::Float64Type>();\n-    static DenseMap<std::pair<TypeID, TypeID>, ConvertorT> convertorMap = {\n+    static DenseMap<std::pair<TypeID, TypeID>, std::string> srcMap = {\n         // F8 -> F16\n-        {{F8E4M3TyID, F16TyID}, convertFp8E4M3x4ToFp16x4},\n-        {{F8E5M2TyID, F16TyID}, convertFp8E5M2x4ToFp16x4},\n+        {{F8E4M3B15TyID, F16TyID}, Fp8E4M3B15_to_Fp16},\n+        {{F8E4M3TyID, F16TyID}, Fp8E4M3_to_Fp16},\n+        {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n         // F16 -> F8\n-        {{F16TyID, F8E4M3TyID}, convertFp16x4ToFp8E4M3x4},\n-        {{F16TyID, F8E5M2TyID}, convertFp16x4ToFp8E5M2x4},\n+        {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n+        {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3},\n+        {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},\n         // F8 -> BF16\n-        {{F8E4M3TyID, BF16TyID}, convertFp8E4M3x4ToBf16x4},\n-        {{F8E5M2TyID, BF16TyID}, convertFp8E5M2x4ToBf16x4},\n+        {{F8E4M3TyID, BF16TyID}, Fp8E4M3_to_Bf16},\n+        {{F8E5M2TyID, BF16TyID}, Fp8E5M2_to_Bf16},\n         // BF16 -> F8\n-        {{BF16TyID, F8E4M3TyID}, convertBf16x4ToFp8E4M3x4},\n-        {{BF16TyID, F8E5M2TyID}, convertBf16x4ToFp8E5M2x4},\n-        // F8 -> F32\n-        {{F8E4M3TyID, F32TyID}, convertFp8E4M3x4ToFp32x4},\n-        {{F8E5M2TyID, F32TyID}, convertFp8E5M2x4ToFp32x4},\n-        // F32 -> F8\n-        {{F32TyID, F8E4M3TyID}, convertFp32x4ToFp8E4M3x4},\n-        {{F32TyID, F8E5M2TyID}, convertFp32x4ToFp8E5M2x4},\n+        {{BF16TyID, F8E4M3TyID}, Bf16_to_Fp8E4M3},\n+        {{BF16TyID, F8E5M2TyID}, Bf16_to_Fp8E5M2},\n     };\n \n     std::pair<TypeID, TypeID> key = {srcTy.getTypeID(), dstTy.getTypeID()};\n-    if (convertorMap.count(key) == 0) {\n+    if (srcMap.count(key) == 0) {\n       llvm::errs() << \"Unsupported conversion from \" << srcTy << \" to \" << dstTy\n                    << \"\\n\";\n       llvm_unreachable(\"\");\n     }\n-    return convertorMap.lookup(key);\n+    return makeConverterFromPtx(srcMap.lookup(key),\n+                                getTypeConverter()->convertType(srcTy),\n+                                getTypeConverter()->convertType(dstTy));\n   }\n \n-  LogicalResult\n-  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    // llvm::outs() << 0 << \"\\n\";\n-    auto srcTensorType = op.getFrom().getType().cast<mlir::RankedTensorType>();\n-    auto dstTensorType =\n-        op.getResult().getType().cast<mlir::RankedTensorType>();\n-    auto loc = op->getLoc();\n-    // check that the number of elements is divisible by 4\n-    // Get convertor\n-    auto cvtFunc = getConversionFunc(srcTensorType.getElementType(),\n-                                     dstTensorType.getElementType());\n-    // Unpack value\n-    auto inVals = getTypeConverter()->unpackLLElements(loc, adaptor.getFrom(),\n-                                                       rewriter, srcTensorType);\n-    inVals =\n-        unpackI32(inVals, srcTensorType, rewriter, loc, getTypeConverter());\n-    // Cast\n-    SmallVector<Value> outVals;\n-    auto elems = inVals.size();\n-    assert(elems % 4 == 0 &&\n+  SmallVector<Value> createDestOps(triton::FpToFpOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    assert(operands.size() % 4 == 0 &&\n            \"FP8 casting only support tensors with 4-aligned sizes\");\n-    for (size_t i = 0; i < elems; i += 4)\n-      outVals.append(cvtFunc(loc, rewriter, inVals[i], inVals[i + 1],\n-                             inVals[i + 2], inVals[i + 3]));\n+    auto srcElementType = getElementType(op.getFrom());\n+    auto dstElementType = getElementType(op.getResult());\n+    bool isSrcFP32 = srcElementType.isF32();\n+    bool isDstFP32 = dstElementType.isF32();\n+    auto cvtFunc = getConversionFunc(isSrcFP32 ? f16_ty : srcElementType,\n+                                     isDstFP32 ? f16_ty : dstElementType);\n+    SmallVector<Value> inVals = {operands[0][0], operands[1][0], operands[2][0],\n+                                 operands[3][0]};\n+    if (isSrcFP32)\n+      for (Value &v : inVals)\n+        v = convertFp32ToFp16(loc, rewriter, v);\n+    SmallVector<Value> outVals =\n+        cvtFunc(loc, rewriter, inVals[0], inVals[1], inVals[2], inVals[3]);\n+    assert(outVals.size() == inVals.size());\n+    if (isDstFP32)\n+      for (Value &v : outVals)\n+        v = convertFp16ToFp32(loc, rewriter, v);\n     // Pack values\n-    assert(outVals.size() == elems);\n-    outVals = reorderValues(outVals, srcTensorType, dstTensorType);\n-    outVals =\n-        packI32(outVals, dstTensorType, rewriter, loc, getTypeConverter());\n-    auto result = getTypeConverter()->packLLElements(loc, outVals, rewriter,\n-                                                     dstTensorType);\n-    rewriter.replaceOp(op, result);\n-    return success();\n-  }\n-};\n-\n-template <typename SourceOp, typename ConcreteT>\n-class ElementwiseOpConversionBase\n-    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n-public:\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-\n-  explicit ElementwiseOpConversionBase(\n-      TritonGPUToLLVMTypeConverter &typeConverter, PatternBenefit benefit = 1)\n-      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n-\n-  LogicalResult\n-  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto resultTy = op.getType();\n-    Location loc = op->getLoc();\n-    // element type\n-    auto resultElementTy = getElementTypeOrSelf(resultTy);\n-    Type elemTy = this->getTypeConverter()->convertType(resultElementTy);\n-    SmallVector<Value> resultVals;\n-    //\n-    SmallVector<SmallVector<Value>> allOperands;\n-    for (auto operand : adaptor.getOperands()) {\n-      auto argTy = op->getOperand(0).getType();\n-      auto sub_operands = this->getTypeConverter()->unpackLLElements(\n-          loc, operand, rewriter, argTy);\n-      sub_operands = unpackI32(sub_operands, argTy, rewriter, loc,\n-                               this->getTypeConverter());\n-      allOperands.resize(sub_operands.size());\n-      for (auto v : llvm::enumerate(sub_operands))\n-        allOperands[v.index()].push_back(v.value());\n-    }\n-    if (allOperands.size() == 0)\n-      allOperands.push_back({});\n-    for (const SmallVector<Value> &operands : allOperands) {\n-      Value curr =\n-          ((ConcreteT *)(this))\n-              ->createDestOp(op, adaptor, rewriter, elemTy, operands, loc);\n-      if (!bool(curr))\n-        return failure();\n-      resultVals.push_back(curr);\n-    }\n-    if (op->getNumOperands() > 0) {\n-      auto argTy = op->getOperand(0).getType();\n-      resultVals = reorderValues(resultVals, argTy, resultTy);\n-    }\n-    resultVals =\n-        packI32(resultVals, resultTy, rewriter, loc, this->getTypeConverter());\n-    Value view = this->getTypeConverter()->packLLElements(loc, resultVals,\n-                                                          rewriter, resultTy);\n-    rewriter.replaceOp(op, view);\n-\n-    return success();\n-  }\n-};\n-\n-template <typename SourceOp, typename DestOp>\n-struct ElementwiseOpConversion\n-    : public ElementwiseOpConversionBase<\n-          SourceOp, ElementwiseOpConversion<SourceOp, DestOp>> {\n-  using Base =\n-      ElementwiseOpConversionBase<SourceOp,\n-                                  ElementwiseOpConversion<SourceOp, DestOp>>;\n-  using Base::Base;\n-  using OpAdaptor = typename Base::OpAdaptor;\n-\n-  explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n-                                   PatternBenefit benefit = 1)\n-      : ElementwiseOpConversionBase<SourceOp, ElementwiseOpConversion>(\n-            typeConverter, benefit) {}\n-\n-  // An interface to support variant DestOp builder.\n-  DestOp createDestOp(SourceOp op, OpAdaptor adaptor,\n-                      ConversionPatternRewriter &rewriter, Type elemTy,\n-                      ValueRange operands, Location loc) const {\n-    return rewriter.create<DestOp>(loc, elemTy, operands,\n-                                   adaptor.getAttributes().getValue());\n+    return outVals;\n   }\n };\n \n@@ -781,12 +652,13 @@ struct CmpIOpConversion\n   using Adaptor = typename Base::OpAdaptor;\n \n   // An interface to support variant DestOp builder.\n-  LLVM::ICmpOp createDestOp(triton::gpu::CmpIOp op, OpAdaptor adaptor,\n-                            ConversionPatternRewriter &rewriter, Type elemTy,\n-                            ValueRange operands, Location loc) const {\n-    return rewriter.create<LLVM::ICmpOp>(\n-        loc, elemTy, ArithCmpIPredicateToLLVM(op.getPredicate()), operands[0],\n-        operands[1]);\n+  SmallVector<LLVM::ICmpOp>\n+  createDestOps(triton::gpu::CmpIOp op, OpAdaptor adaptor,\n+                ConversionPatternRewriter &rewriter, Type elemTy,\n+                MultipleOperandsRange operands, Location loc) const {\n+    return {rewriter.create<LLVM::ICmpOp>(\n+        loc, elemTy, ArithCmpIPredicateToLLVM(op.getPredicate()),\n+        operands[0][0], operands[0][1])};\n   }\n \n   static LLVM::ICmpPredicate\n@@ -822,13 +694,13 @@ struct CmpFOpConversion\n   using Adaptor = typename Base::OpAdaptor;\n \n   // An interface to support variant DestOp builder.\n-  static LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op, OpAdaptor adaptor,\n-                                   ConversionPatternRewriter &rewriter,\n-                                   Type elemTy, ValueRange operands,\n-                                   Location loc) {\n-    return rewriter.create<LLVM::FCmpOp>(\n-        loc, elemTy, ArithCmpFPredicateToLLVM(op.getPredicate()), operands[0],\n-        operands[1]);\n+  static SmallVector<LLVM::FCmpOp>\n+  createDestOps(triton::gpu::CmpFOp op, OpAdaptor adaptor,\n+                ConversionPatternRewriter &rewriter, Type elemTy,\n+                MultipleOperandsRange operands, Location loc) {\n+    return {rewriter.create<LLVM::FCmpOp>(\n+        loc, elemTy, ArithCmpFPredicateToLLVM(op.getPredicate()),\n+        operands[0][0], operands[0][1])};\n   }\n \n   static LLVM::FCmpPredicate\n@@ -869,17 +741,19 @@ struct ExternElementwiseOpConversion\n   using Adaptor = typename Base::OpAdaptor;\n   typedef typename Base::OpAdaptor OpAdaptor;\n \n-  Value createDestOp(T op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(T op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     StringRef funcName = op.getSymbol();\n     if (funcName.empty())\n       llvm::errs() << \"ExternElementwiseOpConversion\";\n \n-    Type funcType = getFunctionType(elemTy, operands);\n+    Type funcType = getFunctionType(elemTy, operands[0]);\n     LLVM::LLVMFuncOp funcOp =\n         appendOrGetFuncOp(rewriter, op, funcName, funcType);\n-    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult();\n+    return {\n+        rewriter.create<LLVM::CallOp>(loc, funcOp, operands[0]).getResult()};\n   }\n \n private:\n@@ -915,9 +789,10 @@ struct FDivOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::DivFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::DivFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     PTXBuilder ptxBuilder;\n     auto &fdiv = *ptxBuilder.create<PTXInstr>(\"div\");\n     unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n@@ -930,12 +805,14 @@ struct FDivOpConversion\n     }\n \n     auto res = ptxBuilder.newOperand(bitwidth == 32 ? \"=r\" : \"=l\");\n-    auto lhs = ptxBuilder.newOperand(operands[0], bitwidth == 32 ? \"r\" : \"l\");\n-    auto rhs = ptxBuilder.newOperand(operands[1], bitwidth == 32 ? \"r\" : \"l\");\n+    auto lhs =\n+        ptxBuilder.newOperand(operands[0][0], bitwidth == 32 ? \"r\" : \"l\");\n+    auto rhs =\n+        ptxBuilder.newOperand(operands[0][1], bitwidth == 32 ? \"r\" : \"l\");\n     fdiv(res, lhs, rhs);\n \n     Value ret = ptxBuilder.launch(rewriter, loc, elemTy, false);\n-    return ret;\n+    return {ret};\n   }\n };\n \n@@ -946,9 +823,10 @@ struct FMulOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::MulFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::MulFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto lhsElemTy = getElementType(op.getLhs());\n     auto rhsElemTy = getElementType(op.getRhs());\n     if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n@@ -958,13 +836,13 @@ struct FMulOpConversion\n                     \"    fma.rn.bf16 $0, $1, $2, c; } \\n\";\n       auto &fMul = *builder.create<PTXInstr>(ptxAsm);\n       auto res = builder.newOperand(\"=h\");\n-      auto lhs = builder.newOperand(operands[0], \"h\");\n-      auto rhs = builder.newOperand(operands[1], \"h\");\n+      auto lhs = builder.newOperand(operands[0][0], \"h\");\n+      auto rhs = builder.newOperand(operands[0][1], \"h\");\n       fMul({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n-      return builder.launch(rewriter, loc, i16_ty, false);\n+      return {builder.launch(rewriter, loc, i16_ty, false)};\n     } else {\n-      return rewriter.create<LLVM::FMulOp>(loc, elemTy, operands[0],\n-                                           operands[1]);\n+      return {rewriter.create<LLVM::FMulOp>(loc, elemTy, operands[0][0],\n+                                            operands[0][1])};\n     }\n   }\n };\n@@ -976,9 +854,10 @@ struct FAddOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::AddFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::AddFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto lhsElemTy = getElementType(op.getLhs());\n     auto rhsElemTy = getElementType(op.getRhs());\n     if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n@@ -988,13 +867,13 @@ struct FAddOpConversion\n                     \"   fma.rn.bf16 $0, $1, c, $2; } \\n\";\n       auto &fAdd = *builder.create<PTXInstr>(ptxAsm);\n       auto res = builder.newOperand(\"=h\");\n-      auto lhs = builder.newOperand(operands[0], \"h\");\n-      auto rhs = builder.newOperand(operands[1], \"h\");\n+      auto lhs = builder.newOperand(operands[0][0], \"h\");\n+      auto rhs = builder.newOperand(operands[0][1], \"h\");\n       fAdd({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n-      return builder.launch(rewriter, loc, i16_ty, false);\n+      return {builder.launch(rewriter, loc, i16_ty, false)};\n     } else {\n-      return rewriter.create<LLVM::FAddOp>(loc, elemTy, operands[0],\n-                                           operands[1]);\n+      return {rewriter.create<LLVM::FAddOp>(loc, elemTy, operands[0][0],\n+                                            operands[0][1])};\n     }\n   }\n };\n@@ -1006,9 +885,10 @@ struct FSubOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::SubFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::SubFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto lhsElemTy = getElementType(op.getLhs());\n     auto rhsElemTy = getElementType(op.getRhs());\n     if (lhsElemTy.isBF16() && rhsElemTy.isBF16()) {\n@@ -1018,33 +898,44 @@ struct FSubOpConversion\n                     \"    fma.rn.bf16 $0, $2, c, $1;} \\n\";\n       auto &fSub = *builder.create<PTXInstr>(ptxAsm);\n       auto res = builder.newOperand(\"=h\");\n-      auto lhs = builder.newOperand(operands[0], \"h\");\n-      auto rhs = builder.newOperand(operands[1], \"h\");\n+      auto lhs = builder.newOperand(operands[0][0], \"h\");\n+      auto rhs = builder.newOperand(operands[0][1], \"h\");\n       fSub({res, lhs, rhs}, /*onlyAttachMLIRArgs=*/true);\n-      return builder.launch(rewriter, loc, i16_ty, false);\n+      return {builder.launch(rewriter, loc, i16_ty, false)};\n     } else {\n-      return rewriter.create<LLVM::FSubOp>(loc, elemTy, operands[0],\n-                                           operands[1]);\n+      return {rewriter.create<LLVM::FSubOp>(loc, elemTy, operands[0][0],\n+                                            operands[0][1])};\n     }\n   }\n };\n \n+// Uses inline ptx to convert s8/u8 to bf16, since the\n struct SIToFPOpConversion\n     : ElementwiseOpConversionBase<mlir::arith::SIToFPOp, SIToFPOpConversion> {\n   using Base =\n       ElementwiseOpConversionBase<mlir::arith::SIToFPOp, SIToFPOpConversion>;\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::SIToFPOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n-    auto outElemTy = getElementType(op.getOut());\n-    if (outElemTy.isBF16()) {\n-      auto value = rewriter.create<LLVM::SIToFPOp>(loc, f32_ty, operands[0]);\n-      return FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, value);\n+  SmallVector<Value> createDestOps(mlir::arith::SIToFPOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n+    Type inElemTy = getElementType(op.getIn());\n+    Type outElemTy = getElementType(op.getOut());\n+    if (outElemTy.isBF16() && inElemTy.isInteger(8) && operands.size() >= 4) {\n+      auto cvtFunc = makeConverterFromPtx(\n+          S8_to_Bf16, getTypeConverter()->convertType(inElemTy),\n+          getTypeConverter()->convertType(outElemTy));\n+      auto outVals = cvtFunc(loc, rewriter, operands[0][0], operands[1][0],\n+                             operands[2][0], operands[3][0]);\n+      assert(outVals.size() == 4);\n+      return outVals;\n+    } else if (outElemTy.isBF16()) {\n+      auto value = rewriter.create<LLVM::SIToFPOp>(loc, f32_ty, operands[0][0]);\n+      return {FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, value)};\n     } else {\n-      return rewriter.create<LLVM::SIToFPOp>(loc, elemTy, operands[0]);\n+      return {rewriter.create<LLVM::SIToFPOp>(loc, elemTy, operands[0][0])};\n     }\n   }\n };\n@@ -1056,16 +947,17 @@ struct FPToSIOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::FPToSIOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::FPToSIOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto inElemTy = getElementType(op.getIn());\n     if (inElemTy.isBF16()) {\n       auto value =\n-          FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0]);\n-      return rewriter.create<LLVM::FPToSIOp>(loc, elemTy, value);\n+          FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0][0]);\n+      return {rewriter.create<LLVM::FPToSIOp>(loc, elemTy, value)};\n     } else {\n-      return rewriter.create<LLVM::FPToSIOp>(loc, elemTy, operands[0]);\n+      return {rewriter.create<LLVM::FPToSIOp>(loc, elemTy, operands[0][0])};\n     }\n   }\n };\n@@ -1077,16 +969,18 @@ struct ExtFOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::ExtFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::ExtFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto inElemTy = getElementType(op.getIn());\n     if (inElemTy.isBF16()) {\n       auto outElemTy = getElementType(op.getOut());\n       assert(outElemTy.isF32() && \"unsupported conversion\");\n-      return FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0]);\n+      return {\n+          FpToFpOpConversion::convertBf16ToFp32(loc, rewriter, operands[0][0])};\n     } else {\n-      return rewriter.create<LLVM::FPExtOp>(loc, elemTy, operands[0]);\n+      return {rewriter.create<LLVM::FPExtOp>(loc, elemTy, operands[0][0])};\n     }\n   }\n };\n@@ -1098,16 +992,18 @@ struct TruncFOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::arith::TruncFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::arith::TruncFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto outElemTy = getElementType(op.getOut());\n     if (outElemTy.isBF16()) {\n       auto inElemTy = getElementType(op.getIn());\n       assert(inElemTy.isF32() && \"unsupported conversion\");\n-      return FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, operands[0]);\n+      return {\n+          FpToFpOpConversion::convertFp32ToBf16(loc, rewriter, operands[0][0])};\n     } else {\n-      return rewriter.create<LLVM::FPTruncOp>(loc, elemTy, operands[0]);\n+      return {rewriter.create<LLVM::FPTruncOp>(loc, elemTy, operands[0][0])};\n     }\n   }\n };\n@@ -1119,22 +1015,23 @@ struct ExpOpConversionApprox\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::math::ExpOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::math::ExpOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     // For non-FP32 input, call __nv_expf for higher-precision calculation\n     if (elemTy.getIntOrFloatBitWidth() != 32)\n       return {};\n \n     const double log2e = 1.4426950408889634;\n-    Value prod = fmul(f32_ty, operands[0], f32_val(log2e));\n+    Value prod = fmul(f32_ty, operands[0][0], f32_val(log2e));\n \n     PTXBuilder ptxBuilder;\n     auto &exp2 = ptxBuilder.create<PTXInstr>(\"ex2\")->o(\"approx\").o(\"f32\");\n     auto output = ptxBuilder.newOperand(\"=f\");\n     auto input = ptxBuilder.newOperand(prod, \"f\");\n     exp2(output, input);\n-    return ptxBuilder.launch(rewriter, loc, f32_ty, false);\n+    return {ptxBuilder.launch(rewriter, loc, f32_ty, false)};\n   }\n };\n \n@@ -1145,13 +1042,14 @@ struct AbsIOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::math::AbsIOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::math::AbsIOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto boolFalse = rewriter.getBoolAttr(false);\n     auto constFalse = rewriter.create<LLVM::ConstantOp>(loc, boolFalse);\n-    return rewriter.create<LLVM::AbsOp>(loc, elemTy, operands[0],\n-                                        /*is_int_min_poison=*/constFalse);\n+    return {rewriter.create<LLVM::AbsOp>(loc, elemTy, operands[0][0],\n+                                         /*is_int_min_poison=*/constFalse)};\n   }\n };\n \n@@ -1162,9 +1060,10 @@ struct AbsFOpConversion\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(mlir::math::AbsFOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(mlir::math::AbsFOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     if (llvm::isa<IntegerType>(elemTy)) {\n       // Mask out the sign bit\n       auto num_bits =\n@@ -1173,10 +1072,10 @@ struct AbsFOpConversion\n       auto mask = (1u << (num_bits - 1u)) - 1u;\n       auto maskAttr = rewriter.getIntegerAttr(elemTy, mask);\n       auto maskConst = rewriter.create<LLVM::ConstantOp>(loc, maskAttr);\n-      return and_(operands[0], maskConst);\n+      return {and_(operands[0][0], maskConst)};\n     }\n \n-    return rewriter.create<LLVM::FAbsOp>(loc, elemTy, operands[0]);\n+    return {rewriter.create<LLVM::FAbsOp>(loc, elemTy, operands[0][0])};\n   }\n };\n \n@@ -1192,20 +1091,22 @@ struct IndexCastOpLowering\n   using Base::Base;\n   using Adaptor = typename Base::OpAdaptor;\n \n-  Value createDestOp(arith::IndexCastOp op, OpAdaptor adaptor,\n-                     ConversionPatternRewriter &rewriter, Type elemTy,\n-                     ValueRange operands, Location loc) const {\n+  SmallVector<Value> createDestOps(arith::IndexCastOp op, OpAdaptor adaptor,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Type elemTy, MultipleOperandsRange operands,\n+                                   Location loc) const {\n     auto inElemTy =\n         this->getTypeConverter()->convertType(getElementType(op.getIn()));\n     unsigned targetBits = elemTy.getIntOrFloatBitWidth();\n     unsigned sourceBits = inElemTy.getIntOrFloatBitWidth();\n \n     if (targetBits == sourceBits)\n-      return operands[0];\n+      return {operands[0][0]};\n     if (targetBits < sourceBits)\n-      return rewriter.replaceOpWithNewOp<LLVM::TruncOp>(op, elemTy,\n-                                                        operands[0]);\n-    return rewriter.replaceOpWithNewOp<LLVM::SExtOp>(op, elemTy, operands[0]);\n+      return {rewriter.replaceOpWithNewOp<LLVM::TruncOp>(op, elemTy,\n+                                                         operands[0][0])};\n+    return {\n+        rewriter.replaceOpWithNewOp<LLVM::SExtOp>(op, elemTy, operands[0][0])};\n   }\n };\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM/Fp8E4M3B15.cpp", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM/Fp8E5M2.cpp", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 42, "deletions": 13, "changes": 55, "file_content_changes": "@@ -331,18 +331,20 @@ struct ReduceOpConversion\n     unsigned elems = product<unsigned>(smemShapes[0]);\n     unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));\n \n-    SmallVector<Value> smemBases(op.getNumOperands());\n-    smemBases[0] = bitcast(\n-        getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys[0]);\n-    for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n-      smemBases[i] =\n-          bitcast(gep(elemPtrTys[i - 1], smemBases[i - 1], i32_val(maxElems)),\n-                  elemPtrTys[i]);\n-    }\n-\n     unsigned sizeIntraWarps = helper.getIntraWarpSizeWithUniqueData();\n     unsigned sizeInterWarps = helper.getInterWarpSizeWithUniqueData();\n \n+    SmallVector<Value> smemBases(op.getNumOperands());\n+    if (sizeInterWarps > 1) {\n+      smemBases[0] = bitcast(\n+          getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys[0]);\n+      for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n+        smemBases[i] =\n+            bitcast(gep(elemPtrTys[i - 1], smemBases[i - 1], i32_val(maxElems)),\n+                    elemPtrTys[i]);\n+      }\n+    }\n+\n     unsigned srcElems = getTotalElemsPerThread(srcTys[0]);\n     auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTys[0]);\n     auto srcValues = unpackInputs(loc, op, adaptor, rewriter);\n@@ -387,6 +389,7 @@ struct ReduceOpConversion\n     Value zero = i32_val(0);\n     Value laneZero = icmp_eq(laneIdAxis, zero);\n \n+    std::map<SmallVector<unsigned>, SmallVector<Value>> finalAccs;\n     for (auto it : accs) {\n       const SmallVector<unsigned> &key = it.first;\n       SmallVector<Value> acc = it.second;\n@@ -400,8 +403,13 @@ struct ReduceOpConversion\n         accumulate(rewriter, *combineOp, acc, shfl, false);\n       }\n \n+      if (sizeInterWarps == 1) {\n+        finalAccs[key] = acc;\n+        continue;\n+      }\n+\n       SmallVector<Value> writeIdx = indices[key];\n-      writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n+      writeIdx[axis] = warpIdAxis;\n       Value writeOffset =\n           linearize(rewriter, loc, writeIdx, smemShapes[0], order);\n       for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n@@ -410,6 +418,30 @@ struct ReduceOpConversion\n       }\n     }\n \n+    if (sizeInterWarps == 1) {\n+      SmallVector<Value> results(op.getNumOperands());\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        if (auto resultTy =\n+                op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n+          auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n+          unsigned resultElems = getTotalElemsPerThread(resultTy);\n+          SmallVector<SmallVector<unsigned>> resultOffset =\n+              emitOffsetForLayout(resultLayout, resultTy);\n+          SmallVector<Value> resultVals;\n+          for (int j = 0; j < resultElems; j++) {\n+            auto key = resultOffset[j];\n+            key.insert(key.begin() + axis, 0);\n+            resultVals.push_back(finalAccs[key][i]);\n+          }\n+          results[i] = getTypeConverter()->packLLElements(loc, resultVals,\n+                                                          rewriter, resultTy);\n+        } else\n+          results[i] = finalAccs.begin()->second[i];\n+      }\n+      rewriter.replaceOp(op, results);\n+      return success();\n+    }\n+\n     barrier();\n \n     // The second round of shuffle reduction\n@@ -463,9 +495,6 @@ struct ReduceOpConversion\n       }\n     }\n \n-    // We could avoid this barrier in some of the layouts, however this is not\n-    // the general case.\n-    // TODO: optimize the barrier in case the layouts are accepted.\n     barrier();\n \n     // set output values"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "file_content_changes": "@@ -212,17 +212,23 @@ struct CallOpConversion : public ConvertOpToLLVMPattern<triton::CallOp> {\n     // of shared memory and append it to the operands of the callOp.\n     auto loc = callOp.getLoc();\n     auto caller = callOp->getParentOfType<FunctionOpInterface>();\n-    auto base = allocation.getFunctionSharedMemoryBase(caller);\n-    auto *funcAllocation = allocation.getFuncData(caller);\n-    auto bufferId = funcAllocation->getBufferId(callOp);\n-    auto offset = funcAllocation->getOffset(bufferId);\n     auto ptrTy = LLVM::LLVMPointerType::get(\n         this->getTypeConverter()->convertType(rewriter.getI8Type()),\n         NVVM::kSharedMemorySpace);\n-    auto offsetValue = gep(ptrTy, base, i32_val(offset));\n     auto promotedOperands = this->getTypeConverter()->promoteOperands(\n         callOp.getLoc(), /*opOperands=*/callOp->getOperands(),\n         adaptor.getOperands(), rewriter);\n+    auto base = allocation.getFunctionSharedMemoryBase(caller);\n+    auto *funcAllocation = allocation.getFuncData(caller);\n+    auto bufferId = funcAllocation->getBufferId(callOp);\n+    // function doesn't have a shared mem buffer\n+    if (bufferId == (size_t)-1) {\n+      promotedOperands.push_back(base);\n+      return promotedOperands;\n+    }\n+    // function has a shared mem buffer\n+    auto offset = funcAllocation->getOffset(bufferId);\n+    auto offsetValue = gep(ptrTy, base, i32_val(offset));\n     promotedOperands.push_back(offsetValue);\n     return promotedOperands;\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -24,7 +24,10 @@ TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n     return convertTritonTensorType(type);\n   });\n   // Internally store float8 as int8\n-  addConversion([&](mlir::Float8E4M3FNType type) -> std::optional<Type> {\n+  addConversion([&](mlir::Float8E4M3B11FNUZType type) -> std::optional<Type> {\n+    return IntegerType::get(type.getContext(), 8);\n+  });\n+  addConversion([&](mlir::Float8E4M3FNUZType type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });\n   addConversion([&](mlir::Float8E5M2Type type) -> std::optional<Type> {"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 18, "deletions": 4, "changes": 22, "file_content_changes": "@@ -154,10 +154,24 @@ class StdSelectPattern : public OpConversionPattern<arith::SelectOp> {\n   matchAndRewrite(arith::SelectOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n-                      op, retType, adaptor.getCondition(),\n-                      adaptor.getTrueValue(), adaptor.getFalseValue()),\n-                  adaptor.getAttributes());\n+\n+    Value cond = adaptor.getCondition();\n+    if (llvm::isa<RankedTensorType>(retType) &&\n+        !llvm::isa<TensorType>(cond.getType())) {\n+      // triton_gpu.select doesn't support scalar condition values, so add a\n+      // splat\n+      auto retTypeTensor = llvm::cast<RankedTensorType>(retType);\n+      auto retShape = retTypeTensor.getShape();\n+      auto retEncoding = retTypeTensor.getEncoding();\n+      Type condTy =\n+          RankedTensorType::get(retShape, cond.getType(), retEncoding);\n+      cond = rewriter.create<triton::SplatOp>(op.getLoc(), condTy, cond);\n+    }\n+\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n+            op, retType, cond, adaptor.getTrueValue(), adaptor.getFalseValue()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 31, "deletions": 8, "changes": 39, "file_content_changes": "@@ -6,7 +6,6 @@\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n \n namespace mlir {\n namespace triton {\n@@ -404,18 +403,17 @@ LogicalResult mlir::triton::DotOp::verify() {\n   auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n   if (aTy.getElementType() != bTy.getElementType())\n     return emitError(\"element types of operands A and B must match\");\n-  auto aEncoding =\n-      aTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n-  auto bEncoding =\n-      bTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  auto aEncoding = aTy.getEncoding();\n+  auto bEncoding = bTy.getEncoding();\n   if (!aEncoding && !bEncoding)\n     return mlir::success();\n   // Verify that the encodings are valid.\n   if (!aEncoding || !bEncoding)\n     return emitError(\"mismatching encoding between A and B operands\");\n-  if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n-    return emitError(\"mismatching kWidth between A and B operands\");\n-  return mlir::success();\n+  Dialect &dialect = aEncoding.getDialect();\n+  auto interface = cast<DialectInferLayoutInterface>(&dialect);\n+  return interface->verifyDotOpEncodingCompatibility(getOperation(), aEncoding,\n+                                                     bEncoding);\n }\n \n //-- ReduceOp --\n@@ -641,6 +639,31 @@ LogicalResult ExpandDimsOp::canonicalize(ExpandDimsOp op,\n                                                  splat.getOperand());\n     return mlir::success();\n   }\n+  // expand_dims(broadcast) -> broadcast(expand_dims)\n+  //\n+  // On it's own this doesn't do much, but consider\n+  //    broadcast(expand_dims(broadcast))\n+  // -> broadcast(broadcast(expand_dims))\n+  // -> broadcast(expand_dims)\n+  if (auto broadcast = dyn_cast<triton::BroadcastOp>(definingOp)) {\n+    auto src = broadcast.getSrc();\n+    auto srcTy = src.getType().dyn_cast<RankedTensorType>();\n+    auto elemTy = srcTy.getElementType();\n+    auto srcShape = srcTy.getShape();\n+\n+    llvm::SmallVector<int64_t, 4> newExpandShape(srcShape.begin(),\n+                                                 srcShape.end());\n+    newExpandShape.insert(newExpandShape.begin() + op.getAxis(), 1);\n+    auto newExpandTy = RankedTensorType::get(newExpandShape, elemTy);\n+\n+    auto newExpand = rewriter.create<triton::ExpandDimsOp>(\n+        op.getLoc(), newExpandTy, src, op.getAxis());\n+    auto newBroadcast = rewriter.create<triton::BroadcastOp>(\n+        broadcast.getLoc(), op.getType(), newExpand.getResult());\n+    rewriter.replaceOp(op, {newBroadcast.getResult()});\n+    return mlir::success();\n+  }\n+\n   return mlir::failure();\n }\n "}, {"filename": "lib/Dialect/Triton/Transforms/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -4,6 +4,7 @@ add_public_tablegen_target(TritonCombineIncGen)\n \n add_mlir_dialect_library(TritonTransforms\n   Combine.cpp\n+  ReorderBroadcast.cpp\n   RewriteTensorPointer.cpp\n \n   DEPENDS"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -36,7 +36,6 @@ bool isBroadcastConstantCombinable(Attribute value) {\n \n DenseElementsAttr getConstantValue(Builder &builder, Attribute value,\n                                    Value bcast_res) {\n-\n   auto resType = bcast_res.getType().cast<ShapedType>();\n   DenseElementsAttr res;\n   if (auto denseValue = value.dyn_cast<DenseElementsAttr>()) {"}, {"filename": "lib/Dialect/Triton/Transforms/ReorderBroadcast.cpp", "status": "added", "additions": 243, "deletions": 0, "changes": 243, "file_content_changes": "@@ -0,0 +1,243 @@\n+#include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/Triton/Transforms/Passes.h\"\n+\n+#include <memory>\n+\n+namespace mlir {\n+#define GEN_PASS_DEF_TRITONREORDERBROADCAST\n+#include \"triton/Dialect/Triton/Transforms/Passes.h.inc\"\n+} // namespace mlir\n+\n+using namespace mlir;\n+\n+namespace {\n+\n+Operation *cloneWithNewArgsAndResultTypes(PatternRewriter &rewriter,\n+                                          Operation *op, ValueRange newOperands,\n+                                          TypeRange newTypes) {\n+  OperationState newElementwiseState(op->getLoc(), op->getName());\n+  newElementwiseState.addOperands(newOperands);\n+  newElementwiseState.addTypes(newTypes);\n+  newElementwiseState.addAttributes(op->getAttrs());\n+  return rewriter.create(newElementwiseState);\n+}\n+\n+bool isSplat(Operation *op) {\n+  if (auto splatOp = llvm::dyn_cast<triton::SplatOp>(op)) {\n+    return true;\n+  }\n+  DenseElementsAttr constAttr;\n+  return (matchPattern(op, m_Constant(&constAttr)) && constAttr.isSplat());\n+}\n+\n+// elementwise(splat(a), splat(b), ...) => splat(elementwise(a, b, ...))\n+struct MoveSplatAfterElementwisePattern\n+    : public mlir::OpTraitRewritePattern<mlir::OpTrait::Elementwise> {\n+\n+  MoveSplatAfterElementwisePattern(mlir::MLIRContext *context)\n+      : OpTraitRewritePattern(context) {}\n+\n+  mlir::LogicalResult match(Operation *op) const override {\n+    if (!isMemoryEffectFree(op)) {\n+      return mlir::failure();\n+    }\n+\n+    for (auto operand : op->getOperands()) {\n+      auto definingOp = operand.getDefiningOp();\n+      if (!definingOp)\n+        return mlir::failure();\n+\n+      if (!isSplat(definingOp)) {\n+        return mlir::failure();\n+      }\n+    }\n+    return mlir::success(op->getNumOperands() > 0);\n+  }\n+\n+  void rewrite(Operation *op, mlir::PatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+    auto operands = op->getOperands();\n+\n+    llvm::SmallVector<Value, 4> scalarOperands(operands.size());\n+    for (unsigned iOp = 0; iOp < operands.size(); ++iOp) {\n+      auto definingOp = operands[iOp].getDefiningOp();\n+\n+      DenseElementsAttr constAttr;\n+      if (auto splatOp = llvm::dyn_cast<triton::SplatOp>(definingOp)) {\n+        scalarOperands[iOp] = splatOp.getSrc();\n+      } else if (matchPattern(definingOp, m_Constant(&constAttr)) &&\n+                 constAttr.isSplat()) {\n+        auto value = constAttr.getSplatValue<Attribute>();\n+        scalarOperands[iOp] = arith::ConstantOp::materialize(\n+            rewriter, value, constAttr.getElementType(), loc);\n+      } else {\n+        llvm_unreachable(\"Expected a splat\");\n+      }\n+    }\n+\n+    auto resultTypes = op->getResultTypes();\n+    llvm::SmallVector<Type, 4> scalarResultTys;\n+    for (auto resultTy : resultTypes) {\n+      auto elemTy = resultTy.dyn_cast<TensorType>().getElementType();\n+      scalarResultTys.push_back(elemTy);\n+    }\n+\n+    auto newOp = cloneWithNewArgsAndResultTypes(rewriter, op, scalarOperands,\n+                                                scalarResultTys);\n+\n+    for (unsigned iRes = 0; iRes < resultTypes.size(); ++iRes) {\n+      auto newResult = rewriter.create<triton::SplatOp>(loc, resultTypes[iRes],\n+                                                        newOp->getResult(iRes));\n+      rewriter.replaceAllUsesWith(op->getResult(iRes), newResult);\n+    }\n+  }\n+};\n+\n+// elementwise(broadcast(a)) => broadcast(elementwise(a))\n+// This also generalizes to multiple arguments when the rest are splat-like\n+// Not handled: multiple broadcasted arguments\n+struct MoveBroadcastAfterElementwisePattern\n+    : public mlir::OpTraitRewritePattern<mlir::OpTrait::Elementwise> {\n+\n+  MoveBroadcastAfterElementwisePattern(mlir::MLIRContext *context)\n+      : OpTraitRewritePattern(context) {}\n+\n+  mlir::LogicalResult match(Operation *op) const override {\n+    if (!isMemoryEffectFree(op)) {\n+      return mlir::failure();\n+    }\n+\n+    auto operands = op->getOperands();\n+    bool seenBroadcast = false;\n+    for (auto operand : operands) {\n+      auto definingOp = operand.getDefiningOp();\n+      if (!definingOp) {\n+        return mlir::failure();\n+      }\n+\n+      if (auto broadcastOp = llvm::dyn_cast<triton::BroadcastOp>(definingOp)) {\n+        if (seenBroadcast) {\n+          // Only support one broadcasted argument for now\n+          return mlir::failure();\n+        }\n+        seenBroadcast = true;\n+      } else if (!isSplat(definingOp)) {\n+        // Not splat or broadcast\n+        return mlir::failure();\n+      }\n+    }\n+    return mlir::success(seenBroadcast);\n+  }\n+\n+  void rewrite(Operation *op, mlir::PatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+\n+    // Find broadcast op\n+    auto operands = op->getOperands();\n+    triton::BroadcastOp broadcastOp;\n+    for (auto operand : operands) {\n+      broadcastOp = operand.getDefiningOp<triton::BroadcastOp>();\n+      if (broadcastOp) {\n+        break;\n+      }\n+    }\n+\n+    auto src = broadcastOp.getSrc();\n+    auto srcTy = src.getType().dyn_cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    auto srcEncoding = srcTy.getEncoding();\n+\n+    // Reshape operands to match srcShape\n+    llvm::SmallVector<Value, 4> newOperands;\n+    for (auto operand : operands) {\n+      auto definingOp = operand.getDefiningOp();\n+      if (llvm::isa<triton::BroadcastOp>(definingOp)) {\n+        newOperands.push_back(src);\n+        continue;\n+      }\n+      auto elemTy =\n+          operand.getType().dyn_cast<RankedTensorType>().getElementType();\n+      auto newTy = RankedTensorType::get(srcShape, elemTy, srcEncoding);\n+      if (auto splatOp = llvm::dyn_cast<triton::SplatOp>(definingOp)) {\n+        auto newSplat =\n+            rewriter.create<triton::SplatOp>(loc, newTy, splatOp.getSrc());\n+        newOperands.push_back(newSplat);\n+        continue;\n+      }\n+      DenseElementsAttr constAttr;\n+      if (matchPattern(definingOp, m_Constant(&constAttr)) &&\n+          constAttr.isSplat()) {\n+        auto scalarValue = constAttr.getSplatValue<Attribute>();\n+        auto splatValue = SplatElementsAttr::get(newTy, scalarValue);\n+        auto newConstant =\n+            rewriter.create<arith::ConstantOp>(loc, newTy, splatValue);\n+        newOperands.push_back(newConstant);\n+        continue;\n+      }\n+      llvm_unreachable(\"Expected broadcast or splat\");\n+    }\n+\n+    // Reshape results to match srcShape\n+    llvm::SmallVector<Type, 4> newResultTypes;\n+    auto resultTypes = op->getResultTypes();\n+    for (auto resultTy : resultTypes) {\n+      auto elemTy = resultTy.dyn_cast<RankedTensorType>().getElementType();\n+      newResultTypes.push_back(\n+          RankedTensorType::get(srcShape, elemTy, srcEncoding));\n+    }\n+\n+    // Create new op and broadcast results\n+    auto newOp = cloneWithNewArgsAndResultTypes(rewriter, op, newOperands,\n+                                                newResultTypes);\n+    for (unsigned iRes = 0; iRes < newResultTypes.size(); ++iRes) {\n+      auto newResult = rewriter.create<triton::BroadcastOp>(\n+          loc, resultTypes[iRes], newOp->getResult(iRes));\n+      rewriter.replaceAllUsesWith(op->getResult(iRes), newResult);\n+    }\n+  }\n+};\n+\n+template <typename OpType>\n+class CanonicalizePattern : public mlir::OpRewritePattern<OpType> {\n+public:\n+  explicit CanonicalizePattern(mlir::MLIRContext *context)\n+      : mlir::OpRewritePattern<OpType>(context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(OpType op, mlir::PatternRewriter &rewriter) const override {\n+    return OpType::canonicalize(op, rewriter);\n+  }\n+};\n+\n+class ReorderBroadcastPass\n+    : public mlir::impl::TritonReorderBroadcastBase<ReorderBroadcastPass> {\n+public:\n+  void runOnOperation() override {\n+    mlir::MLIRContext *context = &getContext();\n+    mlir::RewritePatternSet patterns(context);\n+    mlir::ModuleOp m = getOperation();\n+\n+    patterns.add<CanonicalizePattern<triton::BroadcastOp>>(context);\n+    patterns.add<CanonicalizePattern<triton::ExpandDimsOp>>(context);\n+    // elementwise(broadcast(a)) => broadcast(elementwise(a))\n+    patterns.add<MoveBroadcastAfterElementwisePattern>(context);\n+    // elementwise(splat(a), splat(b), ...) => splat(elementwise(a, b, ...))\n+    patterns.add<MoveSplatAfterElementwisePattern>(context);\n+\n+    if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n+      signalPassFailure();\n+  }\n+};\n+\n+} // namespace\n+\n+std::unique_ptr<mlir::Pass> mlir::triton::createReorderBroadcastPass() {\n+  return std::make_unique<ReorderBroadcastPass>();\n+}"}, {"filename": "lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -470,8 +470,8 @@ class RewriteTensorPointerPass\n \n   void runOnOperation() override {\n     // Only rewrite if the hardware does not support\n-    if (computeCapability >= 90)\n-      return;\n+    // if (computeCapability >= 90)\n+    //   return;\n \n     // NOTES(Chenggang): we don't use `ConversionPatternRewriter`, because\n     // MLIR does not support one-multiple value mapping. For example, if we use"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 36, "deletions": 13, "changes": 49, "file_content_changes": "@@ -7,7 +7,6 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.cpp.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n \n using namespace mlir;\n@@ -222,28 +221,23 @@ SmallVector<unsigned> getContigPerThread(Attribute layout) {\n   }\n }\n \n-SmallVector<unsigned> getUniqueContigPerThread(Type type) {\n-  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n-    return SmallVector<unsigned>(1, 1);\n-  auto tensorType = type.cast<RankedTensorType>();\n-  auto shape = tensorType.getShape();\n+SmallVector<unsigned> getUniqueContigPerThread(Attribute layout,\n+                                               ArrayRef<int64_t> shape) {\n   // If slice layout, call recursively on parent layout, and drop\n   // sliced dim\n-  if (auto sliceLayout =\n-          tensorType.getEncoding().dyn_cast<SliceEncodingAttr>()) {\n+  if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parentLayout = sliceLayout.getParent();\n     auto parentShape = sliceLayout.paddedShape(shape);\n-    auto parentTy = RankedTensorType::get(\n-        parentShape, tensorType.getElementType(), parentLayout);\n-    auto parentUniqueContigPerThread = getUniqueContigPerThread(parentTy);\n+    auto parentUniqueContigPerThread =\n+        getUniqueContigPerThread(parentLayout, parentShape);\n     parentUniqueContigPerThread.erase(parentUniqueContigPerThread.begin() +\n                                       sliceLayout.getDim());\n     return parentUniqueContigPerThread;\n   }\n   // Base case\n   auto rank = shape.size();\n   SmallVector<unsigned> ret(rank);\n-  auto contigPerThread = getContigPerThread(tensorType.getEncoding());\n+  auto contigPerThread = getContigPerThread(layout);\n   assert(contigPerThread.size() == rank && \"Unexpected contigPerThread size\");\n   for (int d = 0; d < rank; ++d) {\n     ret[d] = std::min<unsigned>(shape[d], contigPerThread[d]);\n@@ -368,9 +362,21 @@ bool isSharedEncoding(Value value) {\n   return false;\n }\n \n+bool isExpensiveCat(CatOp cat, Attribute &targetEncoding) {\n+  // If the new elements per thread is less than the old one, we will need to do\n+  // convert encoding that goes through shared memory anyway. So we consider it\n+  // as expensive.\n+  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n+  auto totalElemsPerThread = gpu::getTotalElemsPerThread(tensorTy);\n+  auto shape = tensorTy.getShape();\n+  auto elemTy = tensorTy.getElementType();\n+  auto newTotalElemsPerThread =\n+      gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n+  return newTotalElemsPerThread < totalElemsPerThread;\n+}\n+\n } // namespace gpu\n } // namespace triton\n-\n } // namespace mlir\n \n static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,\n@@ -1100,6 +1106,23 @@ struct TritonGPUInferLayoutInterface\n           location, \"Dot's a/b's encoding should be of DotOperandEncodingAttr\");\n     return success();\n   }\n+\n+  LogicalResult\n+  verifyDotOpEncodingCompatibility(Operation *op, Attribute operandEncodingA,\n+                                   Attribute operandEncodingB) const override {\n+    auto aEncoding =\n+        operandEncodingA.dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    auto bEncoding =\n+        operandEncodingB.dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+    if (!aEncoding && !bEncoding)\n+      return mlir::success();\n+    // Verify that the encodings are valid.\n+    if (!aEncoding || !bEncoding)\n+      return op->emitError(\"mismatching encoding between A and B operands\");\n+    if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n+      return op->emitError(\"mismatching kWidth between A and B operands\");\n+    return success();\n+  }\n };\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 23, "deletions": 21, "changes": 44, "file_content_changes": "@@ -761,27 +761,29 @@ scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n     mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n   mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n-  // Clone the loop body, replace original args with args of the new ForOp\n-  // Insert async wait if necessary.\n+  // Clone the loop body, replace original args with args of the new ForOp.\n+  // We want to find cvt ops that match the following pattern:\n+  // %0 = load %ptr\n+  // %1 (dotOperand) = cvt %0\n   for (Operation &op : forOp.getBody()->without_terminator()) {\n-    // is modified\n-    auto it = std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n-    if (it == validLoads.end()) {\n-      Operation *newOp = cloneWithInferType(builder, &op, mapping);\n-      continue;\n-    }\n-\n-    // we replace the use new load use with a convert layout\n-    size_t i = std::distance(validLoads.begin(), it);\n-    auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n-    if (!cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n-      builder.clone(op, mapping);\n-      continue;\n+    if (auto cvtOp = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+      auto result = op.getResult(0);\n+      auto cvtDstTy = result.getType().cast<RankedTensorType>();\n+      if (cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n+        auto it =\n+            std::find(validLoads.begin(), validLoads.end(), op.getOperand(0));\n+        if (it != validLoads.end()) {\n+          // We replace the use new load use with a convert layout\n+          auto loadArgIdx = std::distance(validLoads.begin(), it);\n+          auto cvt = builder.create<ttg::ConvertLayoutOp>(\n+              result.getLoc(), cvtDstTy,\n+              newForOp.getRegionIterArgs()[loadIdx + loadArgIdx]);\n+          mapping.map(result, cvt.getResult());\n+          continue;\n+        }\n+      }\n     }\n-    auto cvt = builder.create<ttg::ConvertLayoutOp>(\n-        op.getResult(0).getLoc(), cvtDstTy,\n-        newForOp.getRegionIterArgs()[loadIdx + i]);\n-    mapping.map(op.getResult(0), cvt.getResult());\n+    cloneWithInferType(builder, &op, mapping);\n   }\n \n   return newForOp;\n@@ -807,11 +809,11 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n                                     nextIV, newForOp.getUpperBound());\n \n   pipelineIterIdx = newForOp.getRegionIterArgs()[ivIndex + 1];\n-  Value insertSliceIndex = builder.create<arith::RemSIOp>(\n+  Value insertSliceIndex = builder.create<arith::RemUIOp>(\n       nextIV.getLoc(), pipelineIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n   loopIterIdx = newForOp.getRegionIterArgs()[ivIndex + 2];\n-  Value extractSliceIndex = builder.create<arith::RemSIOp>(\n+  Value extractSliceIndex = builder.create<arith::RemUIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -492,7 +492,7 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n     // replace\n     SmallVector<Value, 4> newResults = newForOp->getResults();\n     newResults[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        rewriter.getUnknownLoc(), origType, newForOp->getResult(i));\n+        newForOp.getLoc(), origType, newForOp->getResult(i));\n     newResults[i].getDefiningOp()->moveAfter(newForOp);\n     return newResults;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -80,30 +80,31 @@ class TritonGPUReorderInstructionsPass\n         return;\n       op->moveAfter(argOp);\n     });\n-    // Move `dot` operand so that conversions to opIdx=0 happens before\n-    // conversions to opIdx=1\n+    // Move `dot` operand so that conversions to opIdx=1 happens after\n+    // conversions to opIdx=0\n     m.walk([&](triton::gpu::ConvertLayoutOp op) {\n       auto dstType = op.getResult().getType().cast<RankedTensorType>();\n       auto dstEncoding =\n           dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n       if (!dstEncoding)\n         return;\n       int opIdx = dstEncoding.getOpIdx();\n-      if (opIdx != 0)\n+      if (opIdx != 1)\n         return;\n       if (op->getUsers().empty())\n         return;\n       auto dotUser = dyn_cast<triton::DotOp>(*op->user_begin());\n       if (!dotUser)\n         return;\n-      auto BOp = dotUser.getOperand(1).getDefiningOp();\n-      if (!BOp)\n+      auto AOp =\n+          dotUser.getOperand(0).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n+      if (!AOp)\n         return;\n-      // TODO: An alternative would be to move cvt of OpIdx=0 down instead of\n-      // movig cvt of OpIdx=1 up. This would allow re-ordering more cases.\n-      if (!dom.dominates(op.getOperand(), BOp))\n+      // Check that the conversion to OpIdx=1 happens before and can be moved\n+      // after the conversion to OpIdx=0.\n+      if (!dom.dominates(op.getOperation(), AOp.getOperation()))\n         return;\n-      op->moveBefore(BOp);\n+      op->moveAfter(AOp);\n     });\n     return;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 3, "deletions": 15, "changes": 18, "file_content_changes": "@@ -104,26 +104,13 @@ bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   return true;\n }\n \n-bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n-  // If the new elements per thread is less than the old one, we will need to do\n-  // convert encoding that goes through shared memory anyway. So we consider it\n-  // as expensive.\n-  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n-  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n-  auto shape = tensorTy.getShape();\n-  auto elemTy = tensorTy.getElementType();\n-  auto newTotalElemsPerThread =\n-      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n-  return newTotalElemsPerThread < totalElemsPerThread;\n-}\n-\n bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n     return isExpensiveLoadOrStore(op, targetEncoding);\n   if (isa<triton::CatOp>(op))\n-    return isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return triton::gpu::isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n@@ -136,7 +123,8 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n \n bool canFoldConversion(Operation *op, Attribute &targetEncoding) {\n   if (isa<triton::CatOp>(op))\n-    return !isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return !triton::gpu::isExpensiveCat(cast<triton::CatOp>(op),\n+                                        targetEncoding);\n   return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n              triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1,9 +1,13 @@\n add_mlir_translation_library(TritonLLVMIR\n         LLVMIRTranslation.cpp\n+        LLVMDIScope.cpp\n \n         LINK_COMPONENTS\n         Core\n \n+        DEPENDS\n+        LLVMIRIncGen\n+\n         LINK_LIBS\n         ${CMAKE_DL_LIBS}\n         PUBLIC"}, {"filename": "lib/Target/LLVMIR/LLVMDIScope.cpp", "status": "added", "additions": 148, "deletions": 0, "changes": 148, "file_content_changes": "@@ -0,0 +1,148 @@\n+#include \"triton/Target/LLVMIR/Passes.h\"\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"llvm/BinaryFormat/Dwarf.h\"\n+#include \"llvm/Support/Debug.h\"\n+#include \"llvm/Support/Path.h\"\n+\n+//===----------------------------------------------------------------------===//\n+// This file implements a pass to add debug info scope to LLVM operations, and\n+// is inspired by the DIScopeForLLVMFuncOpPass in LLVM/MLIR. Different from the\n+// DIScopeForLLVMFuncOpPass, this pass also handles inlined functions.\n+//===----------------------------------------------------------------------===//\n+\n+using namespace mlir;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Target/LLVMIR/Passes.h.inc\"\n+\n+namespace {\n+\n+/// Attempt to extract a filename for the given loc.\n+FileLineColLoc extractFileLoc(Location loc) {\n+  if (auto fileLoc = dyn_cast<FileLineColLoc>(loc))\n+    return fileLoc;\n+  if (auto nameLoc = dyn_cast<NameLoc>(loc))\n+    return extractFileLoc(nameLoc.getChildLoc());\n+  if (auto opaqueLoc = dyn_cast<OpaqueLoc>(loc))\n+    return extractFileLoc(opaqueLoc.getFallbackLocation());\n+  if (auto fusedLoc = dyn_cast<FusedLoc>(loc))\n+    return extractFileLoc(fusedLoc.getLocations().front());\n+  if (auto callerLoc = dyn_cast<CallSiteLoc>(loc))\n+    return extractFileLoc(callerLoc.getCaller());\n+  StringAttr unknownFile = mlir::StringAttr::get(loc.getContext(), \"<unknown>\");\n+  return mlir::FileLineColLoc::get(unknownFile, 0, 0);\n+}\n+\n+/// Add a debug info scope to LLVMFuncOp that are missing it.\n+struct LLVMDIScopePass : public LLVMDIScopeBase<LLVMDIScopePass> {\n+  LLVMDIScopePass() = default;\n+\n+  void setSubprogramAttr(LLVM::LLVMFuncOp funcOp) {\n+    Location loc = funcOp.getLoc();\n+    if (loc->findInstanceOf<mlir::FusedLocWith<LLVM::DISubprogramAttr>>())\n+      return;\n+\n+    MLIRContext *context = &getContext();\n+\n+    // To find a DICompileUnitAttr attached to a parent (the module for\n+    // example), otherwise create a default one.\n+    LLVM::DICompileUnitAttr compileUnitAttr;\n+    if (ModuleOp module = funcOp->getParentOfType<ModuleOp>()) {\n+      auto fusedCompileUnitAttr =\n+          module->getLoc()\n+              ->findInstanceOf<mlir::FusedLocWith<LLVM::DICompileUnitAttr>>();\n+      if (fusedCompileUnitAttr)\n+        compileUnitAttr = fusedCompileUnitAttr.getMetadata();\n+    }\n+\n+    // Filename, line and colmun to associate to the function.\n+    LLVM::DIFileAttr fileAttr;\n+    int64_t line = 1, col = 1;\n+    FileLineColLoc fileLoc = extractFileLoc(loc);\n+    if (!fileLoc && compileUnitAttr) {\n+      fileAttr = compileUnitAttr.getFile();\n+    } else if (!fileLoc) {\n+      fileAttr = LLVM::DIFileAttr::get(context, \"<unknown>\", \"\");\n+    } else {\n+      line = fileLoc.getLine();\n+      col = fileLoc.getColumn();\n+      StringRef inputFilePath = fileLoc.getFilename().getValue();\n+      fileAttr = LLVM::DIFileAttr::get(\n+          context, llvm::sys::path::filename(inputFilePath),\n+          llvm::sys::path::parent_path(inputFilePath));\n+    }\n+    if (!compileUnitAttr) {\n+      compileUnitAttr = LLVM::DICompileUnitAttr::get(\n+          context, llvm::dwarf::DW_LANG_C, fileAttr,\n+          StringAttr::get(context, \"triton\"), /*isOptimized=*/true,\n+          LLVM::DIEmissionKind::LineTablesOnly);\n+    }\n+    auto subroutineTypeAttr =\n+        LLVM::DISubroutineTypeAttr::get(context, llvm::dwarf::DW_CC_normal, {});\n+\n+    StringAttr funcNameAttr = funcOp.getNameAttr();\n+    // Note that scopeline is set differently from LLVM's\n+    // DIScopeForLLVMFuncOpPass. I don't find reasons why scopeline should be\n+    // the column offset\n+    auto subprogramAttr =\n+        LLVM::DISubprogramAttr::get(context, compileUnitAttr, fileAttr,\n+                                    funcNameAttr, funcNameAttr, fileAttr,\n+                                    /*line=*/line,\n+                                    /*scopeline=*/line,\n+                                    LLVM::DISubprogramFlags::Definition |\n+                                        LLVM::DISubprogramFlags::Optimized,\n+                                    subroutineTypeAttr);\n+    funcOp->setLoc(FusedLoc::get(context, {loc}, subprogramAttr));\n+  }\n+\n+  // Get a nested loc for inlined functions\n+  Location getNestedLoc(Operation *op, LLVM::DIScopeAttr scopeAttr,\n+                        Location calleeLoc) {\n+    auto calleeFileName = extractFileLoc(calleeLoc).getFilename();\n+    auto context = op->getContext();\n+    LLVM::DIFileAttr calleeFileAttr = LLVM::DIFileAttr::get(\n+        context, llvm::sys::path::filename(calleeFileName),\n+        llvm::sys::path::parent_path(calleeFileName));\n+    auto lexicalBlockFileAttr = LLVM::DILexicalBlockFileAttr::get(\n+        context, scopeAttr, calleeFileAttr, /*discriminator=*/0);\n+    Location loc = op->getLoc();\n+    if (calleeLoc.isa<CallSiteLoc>()) {\n+      auto nestedLoc = calleeLoc.cast<CallSiteLoc>().getCallee();\n+      loc = getNestedLoc(op, lexicalBlockFileAttr, nestedLoc);\n+    }\n+    return FusedLoc::get(context, {loc}, lexicalBlockFileAttr);\n+  }\n+\n+  void setLexicalBlockFileAttr(Operation *op) {\n+    auto opLoc = op->getLoc();\n+    if (auto callSiteLoc = dyn_cast<CallSiteLoc>(opLoc)) {\n+      auto callerLoc = callSiteLoc.getCaller();\n+      auto calleeLoc = callSiteLoc.getCallee();\n+      LLVM::DIScopeAttr scopeAttr;\n+      // We assemble the full inline stack so the parent of this loc must be a\n+      // function\n+      auto funcOp = op->getParentOfType<LLVM::LLVMFuncOp>();\n+      auto funcOpLoc = funcOp.getLoc().cast<FusedLoc>();\n+      scopeAttr = funcOpLoc.getMetadata().cast<LLVM::DISubprogramAttr>();\n+      auto loc = getNestedLoc(op, scopeAttr, calleeLoc);\n+      op->setLoc(loc);\n+    }\n+  }\n+\n+  void runOnOperation() override {\n+    getOperation()->walk<WalkOrder::PreOrder>([&](Operation *op) -> void {\n+      if (isa<LLVM::LLVMFuncOp>(op))\n+        setSubprogramAttr(cast<LLVM::LLVMFuncOp>(op));\n+      else\n+        setLexicalBlockFileAttr(op);\n+    });\n+  }\n+};\n+\n+} // end anonymous namespace\n+\n+std::unique_ptr<Pass> mlir::createLLVMDIScopePass() {\n+  return std::make_unique<LLVMDIScopePass>();\n+}"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -2,6 +2,7 @@\n \n #include \"mlir/Conversion/Passes.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/Transforms/Passes.h\"\n #include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n #include \"mlir/ExecutionEngine/OptUtils.h\"\n #include \"mlir/IR/Dialect.h\"\n@@ -15,6 +16,7 @@\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n+#include \"triton/Target/LLVMIR/Passes.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"triton/Tools/Sys/GetPlatform.hpp\"\n #include \"llvm/ADT/APInt.h\"\n@@ -329,6 +331,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   }\n   auto printingFlags = mlir::OpPrintingFlags();\n   printingFlags.elideLargeElementsAttrs(16);\n+  printingFlags.enableDebugInfo();\n   pm.enableIRPrinting(\n       /*shouldPrintBeforePass=*/nullptr,\n       /*shouldPrintAfterPass=*/\n@@ -347,6 +350,8 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   // Simplify the IR\n   pm.addPass(mlir::createCSEPass());\n   pm.addPass(mlir::createSymbolDCEPass());\n+  if (!::triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\"))\n+    pm.addPass(mlir::createLLVMDIScopePass());\n \n   if (failed(pm.run(module))) {\n     llvm::errs() << \"Pass execution failed\";"}, {"filename": "python/setup.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -66,10 +66,14 @@ def get_pybind11_package_info():\n \n \n def get_llvm_package_info():\n-    # download if nothing is installed\n+    # added statement for Apple Silicon\n     system = platform.system()\n+    arch = 'x86_64'\n     if system == \"Darwin\":\n         system_suffix = \"apple-darwin\"\n+        cpu_type = os.popen('sysctl machdep.cpu.brand_string').read()\n+        if \"apple\" in cpu_type.lower():\n+            arch = 'arm64'\n     elif system == \"Linux\":\n         vglibc = tuple(map(int, platform.libc_ver()[1].split('.')))\n         vglibc = vglibc[0] * 100 + vglibc[1]\n@@ -79,7 +83,7 @@ def get_llvm_package_info():\n         return Package(\"llvm\", \"LLVM-C.lib\", \"\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n     use_assert_enabled_llvm = check_env_flag(\"TRITON_USE_ASSERT_ENABLED_LLVM\", \"False\")\n     release_suffix = \"assert\" if use_assert_enabled_llvm else \"release\"\n-    name = f'llvm+mlir-17.0.0-x86_64-{system_suffix}-{release_suffix}'\n+    name = f'llvm+mlir-17.0.0-{arch}-{system_suffix}-{release_suffix}'\n     version = \"llvm-17.0.0-c5dede880d17\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n@@ -118,10 +122,11 @@ def get_thirdparty_packages(triton_cache_path):\n \n \n def download_and_copy_ptxas():\n+\n     base_dir = os.path.dirname(__file__)\n     src_path = \"bin/ptxas\"\n-    version = \"12.1.105\"\n-    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n+    version = \"12.2.91\"\n+    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.2.0/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n     dst_prefix = os.path.join(base_dir, \"triton\")\n     dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n     dst_path = os.path.join(dst_prefix, dst_suffix)"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 566, "deletions": 535, "changes": 1101, "file_content_changes": "@@ -65,13 +65,97 @@ enum backend_t {\n \n void init_triton_runtime(py::module &&m) {\n   // wrap backend_t\n-  py::enum_<backend_t>(m, \"backend\")\n+  py::enum_<backend_t>(m, \"backend\", py::module_local())\n       .value(\"HOST\", HOST)\n       .value(\"CUDA\", CUDA)\n       .value(\"ROCM\", ROCM)\n       .export_values();\n }\n \n+// A custom op builder that keeps track of the last location\n+class TritonOpBuilder {\n+public:\n+  TritonOpBuilder(mlir::MLIRContext *context) {\n+    builder = std::make_unique<mlir::OpBuilder>(context);\n+    lastLoc = std::make_unique<mlir::Location>(builder->getUnknownLoc());\n+  }\n+\n+  mlir::OpBuilder &getBuilder() { return *builder; }\n+\n+  bool isLineInfoEnabled() { return lineInfoEnabled; }\n+\n+  void setLastLoc(mlir::Location loc) {\n+    if (lineInfoEnabled)\n+      lastLoc = std::make_unique<mlir::Location>(loc);\n+  }\n+\n+  void setLastLoc(const std::string &fileName, int line, int column) {\n+    auto context = builder->getContext();\n+    setLastLoc(mlir::FileLineColLoc::get(context, fileName, line, column));\n+  }\n+\n+  mlir::Location getLastLoc() {\n+    assert(lastLoc);\n+    return *lastLoc;\n+  }\n+\n+  void setInsertionPointToStart(mlir::Block &block) {\n+    if (!block.empty())\n+      setLastLoc(block.begin()->getLoc());\n+    else\n+      setLastLoc(builder->getUnknownLoc());\n+    builder->setInsertionPointToStart(&block);\n+  }\n+\n+  void setInsertionPointToEnd(mlir::Block &block) {\n+    if (!block.empty())\n+      setLastLoc(block.back().getLoc());\n+    else\n+      setLastLoc(builder->getUnknownLoc());\n+    builder->setInsertionPointToEnd(&block);\n+  }\n+\n+  void setInsertionPointAfter(mlir::Operation &op) {\n+    setLastLoc(op.getLoc());\n+    builder->setInsertionPointAfter(&op);\n+  }\n+\n+  void restoreInsertionPoint(mlir::OpBuilder::InsertPoint pt) {\n+    if (pt.isSet() && pt.getPoint() != pt.getBlock()->end())\n+      setLastLoc(pt.getPoint()->getLoc());\n+    else\n+      setLastLoc(builder->getUnknownLoc());\n+    builder->restoreInsertionPoint(pt);\n+  }\n+\n+  template <typename OpTy, typename... Args> OpTy create(Args &&...args) {\n+    auto loc = getLastLoc();\n+    return builder->create<OpTy>(loc, std::forward<Args>(args)...);\n+  }\n+\n+  // Overload to create or fold a single result operation.\n+  template <typename OpTy, typename... Args>\n+  std::enable_if_t<OpTy::template hasTrait<mlir::OpTrait::OneResult>(),\n+                   mlir::Value>\n+  createOrFold(Args &&...args) {\n+    auto loc = getLastLoc();\n+    return builder->createOrFold<OpTy>(loc, std::forward<Args>(args)...);\n+  }\n+\n+  // Overload to create or fold a zero result operation.\n+  template <typename OpTy, typename... Args>\n+  std::enable_if_t<OpTy::template hasTrait<mlir::OpTrait::ZeroResults>(), OpTy>\n+  createOrFold(Args &&...args) {\n+    auto loc = getLastLoc();\n+    return builder->createOrFold<OpTy>(loc, std::forward<Args>(args)...);\n+  }\n+\n+private:\n+  std::unique_ptr<mlir::OpBuilder> builder;\n+  std::unique_ptr<mlir::Location> lastLoc;\n+  bool lineInfoEnabled = !triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\");\n+};\n+\n /*****************************************************************************/\n /* Python bindings for triton::ir                                            */\n /*****************************************************************************/\n@@ -80,12 +164,14 @@ void init_triton_ir(py::module &&m) {\n   using ret = py::return_value_policy;\n   using namespace pybind11::literals;\n \n-  py::enum_<mlir::triton::PaddingOption>(m, \"PADDING_OPTION\")\n+  py::enum_<mlir::triton::PaddingOption>(m, \"PADDING_OPTION\",\n+                                         py::module_local())\n       .value(\"PAD_ZERO\", mlir::triton::PaddingOption::PAD_ZERO)\n       .value(\"PAD_NAN\", mlir::triton::PaddingOption::PAD_NAN)\n       .export_values();\n \n-  py::enum_<mlir::triton::CacheModifier>(m, \"CACHE_MODIFIER\")\n+  py::enum_<mlir::triton::CacheModifier>(m, \"CACHE_MODIFIER\",\n+                                         py::module_local())\n       .value(\"NONE\", mlir::triton::CacheModifier::NONE)\n       .value(\"CA\", mlir::triton::CacheModifier::CA)\n       .value(\"CG\", mlir::triton::CacheModifier::CG)\n@@ -94,20 +180,21 @@ void init_triton_ir(py::module &&m) {\n       .value(\"WT\", mlir::triton::CacheModifier::WT)\n       .export_values();\n \n-  py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\")\n+  py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\", py::module_local())\n       .value(\"ACQUIRE_RELEASE\", mlir::triton::MemSemantic::ACQUIRE_RELEASE)\n       .value(\"ACQUIRE\", mlir::triton::MemSemantic::ACQUIRE)\n       .value(\"RELEASE\", mlir::triton::MemSemantic::RELEASE)\n       .value(\"RELAXED\", mlir::triton::MemSemantic::RELAXED)\n       .export_values();\n \n-  py::enum_<mlir::triton::EvictionPolicy>(m, \"EVICTION_POLICY\")\n+  py::enum_<mlir::triton::EvictionPolicy>(m, \"EVICTION_POLICY\",\n+                                          py::module_local())\n       .value(\"NORMAL\", mlir::triton::EvictionPolicy::NORMAL)\n       .value(\"EVICT_FIRST\", mlir::triton::EvictionPolicy::EVICT_FIRST)\n       .value(\"EVICT_LAST\", mlir::triton::EvictionPolicy::EVICT_LAST)\n       .export_values();\n \n-  py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\")\n+  py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\", py::module_local())\n       .value(\"ADD\", mlir::triton::RMWOp::ADD)\n       .value(\"FADD\", mlir::triton::RMWOp::FADD)\n       .value(\"AND\", mlir::triton::RMWOp::AND)\n@@ -119,7 +206,7 @@ void init_triton_ir(py::module &&m) {\n       .value(\"UMIN\", mlir::triton::RMWOp::UMIN)\n       .value(\"UMAX\", mlir::triton::RMWOp::UMAX);\n \n-  py::class_<mlir::MLIRContext>(m, \"context\")\n+  py::class_<mlir::MLIRContext>(m, \"context\", py::module_local())\n       .def(py::init<>())\n       .def(\"load_triton\", [](mlir::MLIRContext &self) {\n         self.getOrLoadDialect<mlir::triton::TritonDialect>();\n@@ -175,7 +262,7 @@ void init_triton_ir(py::module &&m) {\n   // // py::class_<ir::undef_value, ir::constant>(m, \"undef\")\n   // //     .def(\"get\", &ir::undef_value::get, ret::reference);\n \n-  py::class_<mlir::Type>(m, \"type\")\n+  py::class_<mlir::Type>(m, \"type\", py::module_local())\n       .def(\"is_integer\", &mlir::Type::isInteger)\n       .def(\"is_fp16\", &mlir::Type::isF16)\n       .def(\"__str__\", [](mlir::Type &self) {\n@@ -185,13 +272,21 @@ void init_triton_ir(py::module &&m) {\n         return os.str();\n       });\n \n-  py::class_<mlir::FunctionType>(m, \"function_type\")\n+  py::class_<mlir::FunctionType>(m, \"function_type\", py::module_local())\n       .def(\"param_types\", [](mlir::FunctionType &self) {\n         return std::vector<mlir::Type>(self.getInputs().begin(),\n                                        self.getInputs().end());\n       });\n \n-  py::class_<mlir::Value>(m, \"value\")\n+  py::class_<mlir::Location>(m, \"location\", py::module_local())\n+      .def(\"__str__\", [](mlir::Location &self) {\n+        std::string str;\n+        llvm::raw_string_ostream os(str);\n+        self.print(os);\n+        return os.str();\n+      });\n+\n+  py::class_<mlir::Value>(m, \"value\", py::module_local())\n       .def(\"set_attr\",\n            [](mlir::Value &self, std::string &name,\n               mlir::Attribute &attr) -> void {\n@@ -215,14 +310,15 @@ void init_triton_ir(py::module &&m) {\n            })\n       .def(\"get_type\", &mlir::Value::getType);\n \n-  py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\");\n+  py::class_<mlir::BlockArgument, mlir::Value>(m, \"block_argument\",\n+                                               py::module_local());\n \n-  py::class_<mlir::Region>(m, \"region\")\n+  py::class_<mlir::Region>(m, \"region\", py::module_local())\n       .def(\"get_parent_region\", &mlir::Region::getParentRegion, ret::reference)\n       .def(\"size\", [](mlir::Region &self) { return self.getBlocks().size(); })\n       .def(\"empty\", &mlir::Region::empty);\n \n-  py::class_<mlir::Block>(m, \"block\")\n+  py::class_<mlir::Block>(m, \"block\", py::module_local())\n       .def(\"arg\",\n            [](mlir::Block &self, int index) -> mlir::BlockArgument {\n              return self.getArgument(index);\n@@ -291,12 +387,14 @@ void init_triton_ir(py::module &&m) {\n   //     .value(\"retune\", eattr::retune)\n   //     .value(\"not_implemented\", eattr::not_implemented);\n \n-  py::class_<mlir::Attribute>(m, \"attribute\");\n-  py::class_<mlir::IntegerAttr, mlir::Attribute>(m, \"integer_attr\");\n-  py::class_<mlir::BoolAttr, mlir::Attribute>(m, \"bool_attr\");\n+  py::class_<mlir::Attribute>(m, \"attribute\", py::module_local());\n+  py::class_<mlir::IntegerAttr, mlir::Attribute>(m, \"integer_attr\",\n+                                                 py::module_local());\n+  py::class_<mlir::BoolAttr, mlir::Attribute>(m, \"bool_attr\",\n+                                              py::module_local());\n \n   // Ops\n-  py::class_<mlir::OpState>(m, \"OpState\")\n+  py::class_<mlir::OpState>(m, \"OpState\", py::module_local())\n       .def(\"set_attr\",\n            [](mlir::OpState &self, std::string &name,\n               mlir::Attribute &attr) -> void { self->setAttr(name, attr); })\n@@ -335,23 +433,27 @@ void init_triton_ir(py::module &&m) {\n         return mlir::succeeded(mlir::verify(self.getOperation()));\n       });\n   // scf Ops\n-  py::class_<mlir::scf::ForOp, mlir::OpState>(m, \"ForOp\")\n+  py::class_<mlir::scf::ForOp, mlir::OpState>(m, \"ForOp\", py::module_local())\n       .def(\"get_induction_var\", &mlir::scf::ForOp::getInductionVar);\n \n-  py::class_<mlir::scf::IfOp, mlir::OpState>(m, \"IfOp\")\n+  py::class_<mlir::scf::IfOp, mlir::OpState>(m, \"IfOp\", py::module_local())\n       .def(\"get_then_block\", &mlir::scf::IfOp::thenBlock, ret::reference)\n       .def(\"get_else_block\", &mlir::scf::IfOp::elseBlock, ret::reference)\n       .def(\"get_then_yield\", &mlir::scf::IfOp::thenYield)\n       .def(\"get_else_yield\", &mlir::scf::IfOp::elseYield);\n-  py::class_<mlir::scf::YieldOp, mlir::OpState>(m, \"YieldOp\");\n-  py::class_<mlir::scf::WhileOp, mlir::OpState>(m, \"WhileOp\")\n+  py::class_<mlir::scf::YieldOp, mlir::OpState>(m, \"YieldOp\",\n+                                                py::module_local());\n+  py::class_<mlir::scf::WhileOp, mlir::OpState>(m, \"WhileOp\",\n+                                                py::module_local())\n       .def(\"get_before\", &mlir::scf::WhileOp::getBefore, ret::reference)\n       .def(\"get_after\", &mlir::scf::WhileOp::getAfter, ret::reference);\n-  py::class_<mlir::scf::ConditionOp, mlir::OpState>(m, \"ConditionOp\");\n+  py::class_<mlir::scf::ConditionOp, mlir::OpState>(m, \"ConditionOp\",\n+                                                    py::module_local());\n \n   // dynamic_attr is used to transfer ownership of the MLIR context to the\n   // module\n-  py::class_<mlir::ModuleOp, mlir::OpState>(m, \"module\", py::dynamic_attr())\n+  py::class_<mlir::ModuleOp, mlir::OpState>(m, \"module\", py::module_local(),\n+                                            py::dynamic_attr())\n       .def(\"dump\", &mlir::ModuleOp::dump)\n       .def(\"str\",\n            [](mlir::ModuleOp &self) -> std::string {\n@@ -431,7 +533,8 @@ void init_triton_ir(py::module &&m) {\n       },\n       ret::take_ownership);\n \n-  py::class_<mlir::triton::FuncOp, mlir::OpState>(m, \"function\")\n+  py::class_<mlir::triton::FuncOp, mlir::OpState>(m, \"function\",\n+                                                  py::module_local())\n       // .def_property_readonly(\"attrs\", &ir::function::attrs)\n       // .def(\"add_attr\", &ir::function::add_attr);\n       .def(\"args\",\n@@ -479,363 +582,371 @@ void init_triton_ir(py::module &&m) {\n       .def_property_readonly(\"type\", &mlir::triton::FuncOp::getFunctionType)\n       .def(\"reset_type\", &mlir::triton::FuncOp::setType);\n \n-  py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\");\n+  py::class_<mlir::OpBuilder::InsertPoint>(m, \"InsertPoint\",\n+                                           py::module_local());\n \n-  py::class_<mlir::OpBuilder>(m, \"builder\", py::dynamic_attr())\n+  py::class_<TritonOpBuilder>(m, \"builder\", py::module_local(),\n+                              py::dynamic_attr())\n       .def(py::init<mlir::MLIRContext *>())\n-      // // getters\n-      .def_property_readonly(\"context\", &mlir::OpBuilder::getContext,\n-                             ret::reference)\n+      // getters\n       .def(\"create_module\",\n-           [](mlir::OpBuilder &self) -> mlir::ModuleOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::ModuleOp>(loc);\n-           })\n-      .def(\"ret\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Value> &vals) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::ReturnOp>(loc, vals);\n-           })\n-      .def(\"call\",\n-           [](mlir::OpBuilder &self, mlir::triton::FuncOp &func,\n-              std::vector<mlir::Value> &args) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n-             auto callOp = self.create<mlir::triton::CallOp>(loc, func, args);\n-             return callOp;\n+           [](TritonOpBuilder &self) -> mlir::ModuleOp {\n+             return self.create<mlir::ModuleOp>();\n            })\n       // insertion block/point\n       .def(\"set_insertion_point_to_start\",\n-           [](mlir::OpBuilder &self, mlir::Block &block) -> void {\n-             self.setInsertionPointToStart(&block);\n+           [](TritonOpBuilder &self, mlir::Block &block) -> void {\n+             self.setInsertionPointToStart(block);\n            })\n       .def(\"set_insertion_point_to_end\",\n-           [](mlir::OpBuilder &self, mlir::Block &block) {\n-             self.setInsertionPointToEnd(&block);\n+           [](TritonOpBuilder &self, mlir::Block &block) {\n+             self.setInsertionPointToEnd(block);\n            })\n       .def(\"set_insertion_point_after\",\n-           [](mlir::OpBuilder &self, mlir::Operation &op) {\n-             self.setInsertionPointAfter(&op);\n+           [](TritonOpBuilder &self, mlir::Operation &op) {\n+             self.setInsertionPointAfter(op);\n            })\n       .def(\n           \"get_insertion_block\",\n-          [](mlir::OpBuilder &self) -> mlir::Block * {\n-            return self.getInsertionBlock();\n+          [](TritonOpBuilder &self) -> mlir::Block * {\n+            return self.getBuilder().getInsertionBlock();\n           },\n           ret::reference)\n-      .def(\"get_insertion_point\", &mlir::OpBuilder::saveInsertionPoint)\n-      .def(\"restore_insertion_point\", &mlir::OpBuilder::restoreInsertionPoint)\n-      // .def(\"set_insert_point\", [](ir::builder *self,\n-      // std::pair<ir::basic_block*, ir::instruction*> pt) {\n-      //   ir::basic_block *bb = pt.first;\n-      //   ir::instruction *instr = pt.second;\n-      //   if (instr) {\n-      //     if (bb != instr->get_parent())\n-      //       throw std::runtime_error(\"invalid insertion point, instr not in\n-      //       bb\");\n-      //     self->set_insert_point(instr);\n-      //   } else {\n-      //     assert(bb);\n-      //     self->set_insert_point(bb);\n-      //   }\n-      // })\n+      .def(\"get_insertion_point\",\n+           [](TritonOpBuilder &self) {\n+             return self.getBuilder().saveInsertionPoint();\n+           })\n+      .def(\"restore_insertion_point\",\n+           [](TritonOpBuilder &self, mlir::OpBuilder::InsertPoint pt) {\n+             self.restoreInsertionPoint(pt);\n+           })\n       // Attr\n-      .def(\"get_bool_attr\", &mlir::OpBuilder::getBoolAttr)\n-      .def(\"get_int32_attr\", &mlir::OpBuilder::getI32IntegerAttr)\n+      .def(\"get_bool_attr\",\n+           [](TritonOpBuilder &self, bool value) {\n+             return self.getBuilder().getBoolAttr(value);\n+           })\n+      .def(\"get_int32_attr\",\n+           [](TritonOpBuilder &self, int32_t value) {\n+             return self.getBuilder().getI32IntegerAttr(value);\n+           })\n       // Use arith.ConstantOp to create constants\n       // Constants\n       .def(\"get_int1\",\n-           [](mlir::OpBuilder &self, bool v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, bool v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI1Type()));\n+                 v, self.getBuilder().getI1Type()));\n            })\n       .def(\"get_int8\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI8Type()));\n+                 v, self.getBuilder().getI8Type()));\n            })\n       .def(\"get_int16\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI16Type()));\n+                 v, self.getBuilder().getI16Type()));\n            })\n       .def(\"get_int32\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI32Type()));\n+                 v, self.getBuilder().getI32Type()));\n            })\n       .def(\"get_int64\",\n-           [](mlir::OpBuilder &self, int64_t v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI64Type()));\n+           })\n+      .def(\"get_uint8\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI8Type()));\n+           })\n+      .def(\"get_uint16\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n-                 loc, v, self.getI64Type()));\n+                 v, self.getBuilder().getI16Type()));\n+           })\n+      .def(\"get_uint32\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI32Type()));\n+           })\n+      .def(\"get_uint64\",\n+           [](TritonOpBuilder &self, uint64_t v) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::ConstantIntOp>(\n+                 v, self.getBuilder().getI64Type()));\n            })\n       .def(\"get_bf16\",\n-           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             auto type = self.getBF16Type();\n+           [](TritonOpBuilder &self, float v) -> mlir::Value {\n+             auto type = self.getBuilder().getBF16Type();\n              return self.create<mlir::arith::ConstantFloatOp>(\n-                 loc,\n                  mlir::APFloat(type.getFloatSemantics(), std::to_string(v)),\n                  type);\n            })\n       .def(\"get_fp16\",\n-           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, float v) -> mlir::Value {\n              return self.create<mlir::arith::ConstantOp>(\n-                 loc, self.getF16FloatAttr(v));\n+                 self.getBuilder().getF16FloatAttr(v));\n            })\n       .def(\"get_fp32\",\n-           [](mlir::OpBuilder &self, float v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, float v) -> mlir::Value {\n              return self.create<mlir::arith::ConstantOp>(\n-                 loc, self.getF32FloatAttr(v));\n+                 self.getBuilder().getF32FloatAttr(v));\n            })\n       .def(\"get_fp64\",\n-           [](mlir::OpBuilder &self, double v) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, double v) -> mlir::Value {\n              return self.create<mlir::arith::ConstantOp>(\n-                 loc, self.getF64FloatAttr(v));\n+                 self.getBuilder().getF64FloatAttr(v));\n            })\n       .def(\"get_null_value\",\n-           [](mlir::OpBuilder &self, mlir::Type type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Type type) -> mlir::Value {\n              if (auto floatTy = type.dyn_cast<mlir::FloatType>())\n                return self.create<mlir::arith::ConstantFloatOp>(\n-                   loc, mlir::APFloat(floatTy.getFloatSemantics(), 0), floatTy);\n+                   mlir::APFloat(floatTy.getFloatSemantics(), 0), floatTy);\n              else if (auto intTy = type.dyn_cast<mlir::IntegerType>())\n-               return self.create<mlir::arith::ConstantIntOp>(loc, 0, intTy);\n+               return self.create<mlir::arith::ConstantIntOp>(0, intTy);\n              else\n                throw std::runtime_error(\"Not implemented\");\n            })\n       .def(\"get_all_ones_value\",\n-           [](mlir::OpBuilder &self, mlir::Type type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Type type) -> mlir::Value {\n              uint64_t val = 0xFFFFFFFFFFFFFFFF;\n              if (auto intTy = type.dyn_cast<mlir::IntegerType>())\n-               return self.create<mlir::arith::ConstantIntOp>(loc, val, intTy);\n+               return self.create<mlir::arith::ConstantIntOp>(val, intTy);\n              else\n                throw std::runtime_error(\"Not implemented\");\n            })\n \n       // Types\n       .def(\"get_void_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getNoneType();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getNoneType();\n            })\n       .def(\"get_int1_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getI1Type();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI1Type();\n            }) // or ret::copy?\n       .def(\"get_int8_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type { return self.getI8Type(); })\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI8Type();\n+           })\n       .def(\"get_int16_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::IntegerType>(16);\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getType<mlir::IntegerType>(16);\n+           })\n+      .def(\"get_int32_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI32Type();\n+           })\n+      .def(\"get_int64_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getI64Type();\n            })\n-      .def(\n-          \"get_int32_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getI32Type(); })\n-      .def(\n-          \"get_int64_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getI64Type(); })\n       .def(\"get_fp8e4_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::Float8E4M3FNType>();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getType<mlir::Float8E4M3FNUZType>();\n+           })\n+      .def(\"get_fp8e4b15_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             // TODO: upstream FP8E4B15 into MLIR, or find a way to externally\n+             // have a float-like type compatible with float only native ops\n+             return self.getBuilder().getType<mlir::Float8E4M3B11FNUZType>();\n            })\n       .def(\"get_fp8e5_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::Float8E5M2Type>();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getType<mlir::Float8E5M2Type>();\n+           })\n+      .def(\"get_half_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getF16Type();\n            })\n-      .def(\n-          \"get_half_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getF16Type(); })\n       .def(\"get_bf16_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getBF16Type();\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getBF16Type();\n+           })\n+      .def(\"get_float_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getF32Type();\n+           })\n+      .def(\"get_double_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             return self.getBuilder().getF64Type();\n            })\n-      .def(\n-          \"get_float_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getF32Type(); })\n-      .def(\n-          \"get_double_ty\",\n-          [](mlir::OpBuilder &self) -> mlir::Type { return self.getF64Type(); })\n       .def(\"get_ptr_ty\",\n-           [](mlir::OpBuilder &self, mlir::Type &type,\n+           [](TritonOpBuilder &self, mlir::Type &type,\n               int addrSpace) -> mlir::Type {\n              return mlir::triton::PointerType::get(type, addrSpace);\n            })\n       .def(\"get_block_ty\",\n-           [](mlir::OpBuilder &self, mlir::Type &elementType,\n+           [](TritonOpBuilder &self, mlir::Type &elementType,\n               std::vector<int64_t> &shape) -> mlir::Type {\n              return mlir::RankedTensorType::get(shape, elementType);\n            })\n       .def(\"get_function_ty\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Type> inTypes,\n+           [](TritonOpBuilder &self, std::vector<mlir::Type> inTypes,\n               std::vector<mlir::Type> outTypes) -> mlir::Type {\n-             return self.getFunctionType(inTypes, outTypes);\n+             return self.getBuilder().getFunctionType(inTypes, outTypes);\n+           })\n+      // locs\n+      .def(\"set_loc\", [](TritonOpBuilder &self,\n+                         mlir::Location loc) { self.setLastLoc(loc); })\n+      .def(\"set_loc\",\n+           [](TritonOpBuilder &self, const std::string &fileName, int line,\n+              int column) { self.setLastLoc(fileName, line, column); })\n+      .def(\"get_loc\",\n+           [](TritonOpBuilder &self) -> mlir::Location {\n+             return self.getLastLoc();\n            })\n \n       // Ops\n       .def(\"get_or_insert_function\",\n-           [](mlir::OpBuilder &self, mlir::ModuleOp &module,\n+           [](TritonOpBuilder &self, mlir::ModuleOp &module,\n               std::string &funcName, mlir::Type &funcType,\n               std::string &visibility, bool noinline) -> mlir::triton::FuncOp {\n              if (mlir::Operation *funcOperation = module.lookupSymbol(funcName))\n                return llvm::dyn_cast<mlir::triton::FuncOp>(funcOperation);\n-             auto loc = self.getUnknownLoc();\n              if (auto funcTy = funcType.dyn_cast<mlir::FunctionType>()) {\n                llvm::SmallVector<mlir::NamedAttribute> attrs = {\n-                   mlir::NamedAttribute(self.getStringAttr(\"sym_visibility\"),\n-                                        self.getStringAttr(visibility)),\n-                   mlir::NamedAttribute(self.getStringAttr(\"noinline\"),\n-                                        self.getBoolAttr(noinline))};\n-               return self.create<mlir::triton::FuncOp>(loc, funcName, funcTy,\n+                   mlir::NamedAttribute(\n+                       self.getBuilder().getStringAttr(\"sym_visibility\"),\n+                       self.getBuilder().getStringAttr(visibility)),\n+                   mlir::NamedAttribute(\n+                       self.getBuilder().getStringAttr(\"noinline\"),\n+                       self.getBuilder().getBoolAttr(noinline))};\n+               return self.create<mlir::triton::FuncOp>(funcName, funcTy,\n                                                         attrs);\n              }\n              throw std::runtime_error(\"invalid function type\");\n            })\n       .def(\n           \"create_block\",\n-          [](mlir::OpBuilder &self) -> mlir::Block * {\n-            mlir::Region *parent = self.getBlock()->getParent();\n-            return self.createBlock(parent);\n+          [](TritonOpBuilder &self) -> mlir::Block * {\n+            mlir::Region *parent = self.getBuilder().getBlock()->getParent();\n+            return self.getBuilder().createBlock(parent);\n           },\n           ret::reference)\n       .def(\n           \"create_block_with_parent\",\n-          [](mlir::OpBuilder &self, mlir::Region &parent,\n+          [](TritonOpBuilder &self, mlir::Region &parent,\n              std::vector<mlir::Type> &argTypes) -> mlir::Block * {\n-            auto argLoc = self.getUnknownLoc();\n-            llvm::SmallVector<mlir::Location, 8> argLocs(argTypes.size(),\n-                                                         argLoc);\n-            return self.createBlock(&parent, {}, argTypes, argLocs);\n+            // TODO: update arg loc\n+            auto loc = self.getBuilder().getUnknownLoc();\n+            llvm::SmallVector<mlir::Location, 8> argLocs(argTypes.size(), loc);\n+            return self.getBuilder().createBlock(&parent, {}, argTypes,\n+                                                 argLocs);\n           },\n           ret::reference)\n       .def(\n           \"new_block\",\n-          [](mlir::OpBuilder &self) -> mlir::Block * {\n+          [](TritonOpBuilder &self) -> mlir::Block * {\n             return new mlir::Block();\n           },\n           ret::reference)\n+      // Function\n+      .def(\"ret\",\n+           [](TritonOpBuilder &self,\n+              std::vector<mlir::Value> &vals) -> mlir::OpState {\n+             return self.create<mlir::triton::ReturnOp>(vals);\n+           })\n+      .def(\"call\",\n+           [](TritonOpBuilder &self, mlir::triton::FuncOp &func,\n+              std::vector<mlir::Value> &args) -> mlir::OpState {\n+             return self.create<mlir::triton::CallOp>(func, args);\n+           })\n       // Unstructured control flow\n       .def(\"create_cond_branch\",\n-           [](mlir::OpBuilder &self, mlir::Value condition,\n-              mlir::Block *trueDest, mlir::Block *falseDest) {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::cf::CondBranchOp>(loc, condition, trueDest,\n-                                                 falseDest);\n-             return;\n+           [](TritonOpBuilder &self, mlir::Value condition,\n+              mlir::Block *trueDest, mlir::Block *falseDest) -> mlir::OpState {\n+             return self.create<mlir::cf::CondBranchOp>(condition, trueDest,\n+                                                        falseDest);\n            })\n       .def(\"create_branch\",\n-           [](mlir::OpBuilder &self, mlir::Block *dest,\n-              std::vector<mlir::Value> &args) {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::cf::BranchOp>(loc, dest, args);\n-             return;\n+           [](TritonOpBuilder &self, mlir::Block *dest,\n+              std::vector<mlir::Value> &args) -> mlir::OpState {\n+             return self.create<mlir::cf::BranchOp>(dest, args);\n            })\n       // Structured control flow\n       .def(\"create_for_op\",\n-           [](mlir::OpBuilder &self, mlir::Value &lb, mlir::Value &ub,\n+           [](TritonOpBuilder &self, mlir::Value &lb, mlir::Value &ub,\n               mlir::Value &step,\n               std::vector<mlir::Value> &initArgs) -> mlir::scf::ForOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::ForOp>(loc, lb, ub, step, initArgs);\n+             return self.create<mlir::scf::ForOp>(lb, ub, step, initArgs);\n            })\n       .def(\"create_if_op\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Type> &retTypes,\n+           [](TritonOpBuilder &self, std::vector<mlir::Type> &retTypes,\n               mlir::Value &condition, bool withElse) -> mlir::scf::IfOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::IfOp>(loc, retTypes, condition,\n-                                                 withElse);\n+             return self.create<mlir::scf::IfOp>(retTypes, condition, withElse);\n            })\n       .def(\"create_yield_op\",\n-           [](mlir::OpBuilder &self,\n+           [](TritonOpBuilder &self,\n               std::vector<mlir::Value> &yields) -> mlir::scf::YieldOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::YieldOp>(loc, yields);\n+             return self.create<mlir::scf::YieldOp>(yields);\n            })\n       .def(\"create_while_op\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Type> &retTypes,\n+           [](TritonOpBuilder &self, std::vector<mlir::Type> &retTypes,\n               std::vector<mlir::Value> &initArgs) -> mlir::scf::WhileOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::WhileOp>(loc, retTypes, initArgs);\n+             return self.create<mlir::scf::WhileOp>(retTypes, initArgs);\n            })\n       .def(\"create_condition_op\",\n-           [](mlir::OpBuilder &self, mlir::Value &cond,\n+           [](TritonOpBuilder &self, mlir::Value &cond,\n               std::vector<mlir::Value> &args) -> mlir::scf::ConditionOp {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::scf::ConditionOp>(loc, cond, args);\n+             return self.create<mlir::scf::ConditionOp>(cond, args);\n            })\n \n       // miscellaneous\n       .def(\"create_make_range\",\n-           [](mlir::OpBuilder &self, int start, int end) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             auto retType =\n-                 mlir::RankedTensorType::get({end - start}, self.getI32Type());\n-             return self.create<mlir::triton::MakeRangeOp>(loc, retType, start,\n-                                                           end);\n+           [](TritonOpBuilder &self, int start, int end) -> mlir::Value {\n+             auto retType = mlir::RankedTensorType::get(\n+                 {end - start}, self.getBuilder().getI32Type());\n+             return self.create<mlir::triton::MakeRangeOp>(retType, start, end);\n            })\n \n       // Cast instructions\n       // Conversions for custom FP types (FP8)\n       .def(\"create_fp_to_fp\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::FpToFpOp>(loc, dstType, src);\n+             return self.create<mlir::triton::FpToFpOp>(dstType, src);\n            })\n       // Conversions for standard LLVM builtin types\n       .def(\"create_bitcast\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::BitcastOp>(loc, dstType, src);\n+             return self.create<mlir::triton::BitcastOp>(dstType, src);\n            })\n       .def(\"create_si_to_fp\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::SIToFPOp>(loc, dstType, src);\n+             return self.create<mlir::arith::SIToFPOp>(dstType, src);\n            })\n       .def(\"create_ui_to_fp\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::UIToFPOp>(loc, dstType, src);\n+             return self.create<mlir::arith::UIToFPOp>(dstType, src);\n            })\n       .def(\"create_fp_to_si\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::FPToSIOp>(loc, dstType, src);\n+             return self.create<mlir::arith::FPToSIOp>(dstType, src);\n            })\n       .def(\"create_fp_to_ui\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::FPToUIOp>(loc, dstType, src);\n+             return self.create<mlir::arith::FPToUIOp>(dstType, src);\n            })\n       .def(\"create_fp_ext\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::ExtFOp>(loc, dstType, src);\n+             return self.create<mlir::arith::ExtFOp>(dstType, src);\n            })\n       .def(\"create_fp_trunc\",\n-           [](mlir::OpBuilder &self, mlir::Value &src,\n+           [](TritonOpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::TruncFOp>(loc, dstType, src);\n+             return self.create<mlir::arith::TruncFOp>(dstType, src);\n            })\n       .def(\"create_int_cast\",\n-           [](mlir::OpBuilder &self, mlir::Value &src, mlir::Type &dstType,\n+           [](TritonOpBuilder &self, mlir::Value &src, mlir::Type &dstType,\n               bool isSigned) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              // get element type if necessary\n              mlir::Type srcType = src.getType();\n              auto srcTensorType = srcType.dyn_cast<mlir::RankedTensorType>();\n@@ -849,387 +960,330 @@ void init_triton_ir(py::module &&m) {\n              unsigned srcWidth = srcEltType.getIntOrFloatBitWidth();\n              unsigned dstWidth = dstEltType.getIntOrFloatBitWidth();\n              if (srcWidth == dstWidth)\n-               return self.create<mlir::arith::BitcastOp>(loc, dstType, src);\n+               return self.create<mlir::arith::BitcastOp>(dstType, src);\n              else if (srcWidth > dstWidth)\n-               return self.create<mlir::arith::TruncIOp>(loc, dstType, src);\n+               return self.create<mlir::arith::TruncIOp>(dstType, src);\n              else if (isSigned)\n-               return self.create<mlir::arith::ExtSIOp>(loc, dstType, src);\n+               return self.create<mlir::arith::ExtSIOp>(dstType, src);\n              else\n-               return self.create<mlir::arith::ExtUIOp>(loc, dstType, src);\n+               return self.create<mlir::arith::ExtUIOp>(dstType, src);\n            })\n       .def(\"create_to_index\",\n-           [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Value &input) -> mlir::Value {\n              return self.create<mlir::arith::IndexCastOp>(\n-                 loc, self.getIndexType(), input);\n+                 self.getBuilder().getIndexType(), input);\n            })\n       .def(\"create_index_to_si\",\n-           [](mlir::OpBuilder &self, mlir::Value &input) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Value &input) -> mlir::Value {\n              return self.create<mlir::arith::IndexCastOp>(\n-                 loc, self.getI64Type(), input);\n+                 self.getBuilder().getI64Type(), input);\n            })\n       .def(\"create_fmul\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::MulFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::MulFOp>(lhs, rhs);\n            })\n       .def(\"create_fdiv\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::DivFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::DivFOp>(lhs, rhs);\n            })\n       .def(\"create_frem\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::RemFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::RemFOp>(lhs, rhs);\n            })\n       .def(\"create_fadd\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::AddFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::AddFOp>(lhs, rhs);\n            })\n       .def(\"create_fsub\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::SubFOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::SubFOp>(lhs, rhs);\n            })\n       .def(\"create_mul\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::MulIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::MulIOp>(lhs, rhs);\n            })\n       .def(\"create_sdiv\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::DivSIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::DivSIOp>(lhs, rhs);\n            })\n       .def(\"create_udiv\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::DivUIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::DivUIOp>(lhs, rhs);\n            })\n       .def(\"create_srem\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::RemSIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::RemSIOp>(lhs, rhs);\n            })\n       .def(\"create_urem\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::RemUIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::RemUIOp>(lhs, rhs);\n            })\n       .def(\"create_add\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::AddIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::AddIOp>(lhs, rhs);\n            })\n       .def(\"create_sub\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::SubIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::SubIOp>(lhs, rhs));\n            })\n       .def(\"create_shl\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::ShLIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::ShLIOp>(lhs, rhs));\n            })\n       .def(\"create_lshr\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::ShRUIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::ShRUIOp>(lhs, rhs));\n            })\n       .def(\"create_ashr\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return mlir::Value(\n-                 self.create<mlir::arith::ShRSIOp>(loc, lhs, rhs));\n+             return mlir::Value(self.create<mlir::arith::ShRSIOp>(lhs, rhs));\n            })\n       // AddPtr (similar to GEP)\n       .def(\"create_addptr\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+           [](TritonOpBuilder &self, mlir::Value &ptr,\n               mlir::Value &offset) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::AddPtrOp>(loc, ptr.getType(), ptr,\n+             return self.create<mlir::triton::AddPtrOp>(ptr.getType(), ptr,\n                                                         offset);\n            })\n       // Comparison (int)\n       .def(\"create_icmpSLE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::sle, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::sle, lhs, rhs);\n            })\n       .def(\"create_icmpSLT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::slt, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::slt, lhs, rhs);\n            })\n       .def(\"create_icmpSGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::sge, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::sge, lhs, rhs);\n            })\n       .def(\"create_icmpSGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::sgt, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::sgt, lhs, rhs);\n            })\n       .def(\"create_icmpULE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ule, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ule, lhs, rhs);\n            })\n       .def(\"create_icmpULT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ult, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ult, lhs, rhs);\n            })\n       .def(\"create_icmpUGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::uge, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::uge, lhs, rhs);\n            })\n       .def(\"create_icmpUGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ugt, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ugt, lhs, rhs);\n            })\n       .def(\"create_icmpEQ\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::eq, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::eq, lhs, rhs);\n            })\n       .def(\"create_icmpNE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpIOp>(\n-                 loc, mlir::arith::CmpIPredicate::ne, lhs, rhs);\n+                 mlir::arith::CmpIPredicate::ne, lhs, rhs);\n            })\n       // Comparison (float)\n       .def(\"create_fcmpOLT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OLT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OLT, lhs, rhs);\n            })\n       .def(\"create_fcmpOGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OGT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OGT, lhs, rhs);\n            })\n       .def(\"create_fcmpOLE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OLE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OLE, lhs, rhs);\n            })\n       .def(\"create_fcmpOGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OGE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OGE, lhs, rhs);\n            })\n       .def(\"create_fcmpOEQ\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::OEQ, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::OEQ, lhs, rhs);\n            })\n       .def(\"create_fcmpONE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::ONE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::ONE, lhs, rhs);\n            })\n       .def(\"create_fcmpULT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::ULT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::ULT, lhs, rhs);\n            })\n       .def(\"create_fcmpUGT\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UGT, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UGT, lhs, rhs);\n            })\n       .def(\"create_fcmpULE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::ULE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::ULE, lhs, rhs);\n            })\n       .def(\"create_fcmpUGE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UGE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UGE, lhs, rhs);\n            })\n       .def(\"create_fcmpUEQ\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UEQ, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UEQ, lhs, rhs);\n            })\n       .def(\"create_fcmpUNE\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::arith::CmpFOp>(\n-                 loc, mlir::arith::CmpFPredicate::UNE, lhs, rhs);\n+                 mlir::arith::CmpFPredicate::UNE, lhs, rhs);\n            })\n       // // Logical\n       .def(\"create_and\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::AndIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::AndIOp>(lhs, rhs);\n            })\n       .def(\"create_xor\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::XOrIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::XOrIOp>(lhs, rhs);\n            })\n       .def(\"create_or\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::OrIOp>(loc, lhs, rhs);\n+             return self.create<mlir::arith::OrIOp>(lhs, rhs);\n            })\n       // Input/Output\n       .def(\"create_load\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy,\n               bool isVolatile) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::LoadOp>(\n-                 loc, ptrs, cacheModifier, evictionPolicy, isVolatile);\n+                 ptrs, cacheModifier, evictionPolicy, isVolatile);\n            })\n       .def(\"create_store\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &value,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs, mlir::Value &value,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptrs, value, cacheModifier,\n+             self.create<mlir::triton::StoreOp>(ptrs, value, cacheModifier,\n                                                 evictionPolicy);\n            })\n       .def(\"create_tensor_pointer_load\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+           [](TritonOpBuilder &self, mlir::Value &ptr,\n               std::vector<int32_t> &boundaryCheck,\n               std::optional<mlir::triton::PaddingOption> paddingOption,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy,\n               bool isVolatile) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::LoadOp>(\n-                 loc, ptr, boundaryCheck, paddingOption, cacheModifier,\n+                 ptr, boundaryCheck, paddingOption, cacheModifier,\n                  evictionPolicy, isVolatile);\n            })\n       .def(\"create_tensor_pointer_store\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &ptr, mlir::Value &val,\n               std::vector<int32_t> &boundaryCheck,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptr, val, boundaryCheck,\n+             self.create<mlir::triton::StoreOp>(ptr, val, boundaryCheck,\n                                                 cacheModifier, evictionPolicy);\n            })\n       .def(\"create_masked_load\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &mask,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs, mlir::Value &mask,\n               std::optional<mlir::Value> &other,\n               mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy,\n               bool isVolatile) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::LoadOp>(\n-                 loc, ptrs, mask, other.value_or(mlir::Value()), cacheModifier,\n+                 ptrs, mask, other.value_or(mlir::Value()), cacheModifier,\n                  evictionPolicy, isVolatile);\n            })\n       .def(\"create_masked_store\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptrs, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &ptrs, mlir::Value &val,\n               mlir::Value &mask, mlir::triton::CacheModifier cacheModifier,\n               mlir::triton::EvictionPolicy evictionPolicy) -> void {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::triton::StoreOp>(loc, ptrs, val, mask,\n-                                                cacheModifier, evictionPolicy);\n+             self.create<mlir::triton::StoreOp>(ptrs, val, mask, cacheModifier,\n+                                                evictionPolicy);\n            })\n       .def(\"create_view\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg,\n+           [](TritonOpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              auto argType = arg.getType()\n                                 .dyn_cast<mlir::RankedTensorType>()\n                                 .getElementType();\n              return self.create<mlir::triton::ViewOp>(\n-                 loc, mlir::RankedTensorType::get(shape, argType), arg);\n+                 mlir::RankedTensorType::get(shape, argType), arg);\n            })\n       .def(\n           \"create_expand_dims\",\n-          [](mlir::OpBuilder &self, mlir::Value &arg, int axis) -> mlir::Value {\n-            auto loc = self.getUnknownLoc();\n+          [](TritonOpBuilder &self, mlir::Value &arg, int axis) -> mlir::Value {\n             auto argType = arg.getType().dyn_cast<mlir::RankedTensorType>();\n             auto argEltType = argType.getElementType();\n             std::vector<int64_t> retShape = argType.getShape();\n             retShape.insert(retShape.begin() + axis, 1);\n             return self.create<mlir::triton::ExpandDimsOp>(\n-                loc, mlir::RankedTensorType::get(retShape, argEltType), arg,\n-                axis);\n+                mlir::RankedTensorType::get(retShape, argEltType), arg, axis);\n           })\n       .def(\"create_cat\",\n-           [](mlir::OpBuilder &self, mlir::Value &lhs,\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              auto lhsType = lhs.getType().dyn_cast<mlir::RankedTensorType>();\n              auto rhsType = rhs.getType().dyn_cast<mlir::RankedTensorType>();\n              if (!(lhsType.getShape().size() == 1 &&\n@@ -1239,47 +1293,41 @@ void init_triton_ir(py::module &&m) {\n              std::vector<int64_t> shape{lhsType.getShape()[0] +\n                                         rhsType.getShape()[0]};\n              return self.create<mlir::triton::CatOp>(\n-                 loc,\n                  mlir::RankedTensorType::get(shape, lhsType.getElementType()),\n                  lhs, rhs);\n            })\n       .def(\"create_trans\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, mlir::Value &arg) -> mlir::Value {\n              auto argType = arg.getType().dyn_cast<mlir::RankedTensorType>();\n              auto argEltType = argType.getElementType();\n              std::vector<int64_t> retShape = argType.getShape();\n              std::reverse(retShape.begin(), retShape.end());\n              return self.create<mlir::triton::TransOp>(\n-                 loc, mlir::RankedTensorType::get(retShape, argEltType), arg);\n+                 mlir::RankedTensorType::get(retShape, argEltType), arg);\n            })\n       .def(\"create_broadcast\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg,\n+           [](TritonOpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              if (auto argType =\n                      arg.getType().dyn_cast<mlir::RankedTensorType>())\n                return self.createOrFold<mlir::triton::BroadcastOp>(\n-                   loc,\n                    mlir::RankedTensorType::get(shape, argType.getElementType()),\n                    arg);\n              throw std::runtime_error(\n                  \"arg is not of RankedTensorType, use create_splat\");\n            })\n       .def(\"create_splat\",\n-           [](mlir::OpBuilder &self, mlir::Value &arg,\n+           [](TritonOpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              auto argType = arg.getType();\n              auto ret = self.createOrFold<mlir::triton::SplatOp>(\n-                 loc, mlir::RankedTensorType::get(shape, argType), arg);\n+                 mlir::RankedTensorType::get(shape, argType), arg);\n              return ret;\n            })\n       // // atomic\n       .def(\"create_atomic_cas\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n+           [](TritonOpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n               mlir::Value &val, mlir::triton::MemSemantic sem) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n              if (auto srcTensorType =\n                      ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n@@ -1293,14 +1341,13 @@ void init_triton_ir(py::module &&m) {\n                                   .cast<mlir::triton::PointerType>();\n                dstType = ptrType.getPointeeType();\n              }\n-             return self.create<mlir::triton::AtomicCASOp>(loc, dstType, ptr,\n-                                                           cmp, val, sem);\n+             return self.create<mlir::triton::AtomicCASOp>(dstType, ptr, cmp,\n+                                                           val, sem);\n            })\n       .def(\"create_atomic_rmw\",\n-           [](mlir::OpBuilder &self, mlir::triton::RMWOp rmwOp,\n+           [](TritonOpBuilder &self, mlir::triton::RMWOp rmwOp,\n               mlir::Value &ptr, mlir::Value &val, mlir::Value &mask,\n               mlir::triton::MemSemantic sem) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n              if (auto srcTensorType =\n                      ptr.getType().dyn_cast<mlir::RankedTensorType>()) {\n@@ -1314,192 +1361,166 @@ void init_triton_ir(py::module &&m) {\n                                   .cast<mlir::triton::PointerType>();\n                dstType = ptrType.getPointeeType();\n              }\n-             return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n-                                                           ptr, val, mask, sem);\n+             return self.create<mlir::triton::AtomicRMWOp>(dstType, rmwOp, ptr,\n+                                                           val, mask, sem);\n            })\n       // External\n       .def(\"create_extern_elementwise\",\n-           [](mlir::OpBuilder &self, const std::string &libName,\n+           [](TritonOpBuilder &self, const std::string &libName,\n               const std::string &libPath, const std::string &symbol,\n               std::vector<mlir::Value> &argList, mlir::Type retType,\n               bool isPure) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              if (isPure)\n                return self.create<mlir::triton::PureExternElementwiseOp>(\n-                   loc, retType, argList, libName, libPath, symbol);\n+                   retType, argList, libName, libPath, symbol);\n              else\n                return self.create<mlir::triton::ImpureExternElementwiseOp>(\n-                   loc, retType, argList, libName, libPath, symbol);\n+                   retType, argList, libName, libPath, symbol);\n            })\n       // Built-in instruction\n       .def(\"create_get_program_id\",\n-           [](mlir::OpBuilder &self, int axis) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int axis) -> mlir::Value {\n              if (axis < 0 || axis > 3)\n                throw std::runtime_error(\"program_id must be in [0,3]\");\n              return self.create<mlir::triton::GetProgramIdOp>(\n-                 loc, self.getI32Type(),\n+                 self.getBuilder().getI32Type(),\n                  mlir::triton::ProgramIDDimAttr::get(\n-                     loc.getContext(), mlir::triton::ProgramIDDim(axis)));\n+                     self.getBuilder().getContext(),\n+                     mlir::triton::ProgramIDDim(axis)));\n            })\n       .def(\"create_get_num_programs\",\n-           [](mlir::OpBuilder &self, int axis) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, int axis) -> mlir::Value {\n              return self.create<mlir::triton::GetNumProgramsOp>(\n-                 loc, self.getI32Type(), self.getI32IntegerAttr(axis));\n+                 self.getBuilder().getI32Type(),\n+                 self.getBuilder().getI32IntegerAttr(axis));\n            })\n       .def(\"create_dot\",\n-           [](mlir::OpBuilder &self, mlir::Value &a, mlir::Value &b,\n+           [](TritonOpBuilder &self, mlir::Value &a, mlir::Value &b,\n               mlir::Value &c, bool allowTF32) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::DotOp>(loc, c.getType(), a, b, c,\n+             return self.create<mlir::triton::DotOp>(c.getType(), a, b, c,\n                                                      allowTF32);\n            })\n       .def(\"create_exp\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::ExpOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::ExpOp>(val);\n            })\n       .def(\"create_cos\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::CosOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::CosOp>(val);\n            })\n       .def(\"create_sin\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::SinOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::SinOp>(val);\n            })\n       .def(\"create_log\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::LogOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::LogOp>(val);\n            })\n       .def(\"create_sqrt\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::SqrtOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::SqrtOp>(val);\n            })\n       .def(\"create_fabs\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::AbsFOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::AbsFOp>(val);\n            })\n       .def(\"create_iabs\",\n-           [](mlir::OpBuilder &self, mlir::Value &val) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::math::AbsIOp>(loc, val);\n+           [](TritonOpBuilder &self, mlir::Value &val) -> mlir::Value {\n+             return self.create<mlir::math::AbsIOp>(val);\n            })\n       .def(\"create_reduce\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Value> operands,\n+           [](TritonOpBuilder &self, std::vector<mlir::Value> operands,\n               int axis) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::ReduceOp>(loc, operands, axis);\n+             return self.create<mlir::triton::ReduceOp>(operands, axis);\n            })\n       .def(\"create_reduce_ret\",\n-           [](mlir::OpBuilder &self, py::args args) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, py::args args) -> mlir::OpState {\n              llvm::SmallVector<mlir::Value> return_values;\n              for (const auto &arg : args) {\n                return_values.push_back(py::cast<mlir::Value>(arg));\n              }\n-             return self.create<mlir::triton::ReduceReturnOp>(loc,\n-                                                              return_values);\n+             return self.create<mlir::triton::ReduceReturnOp>(return_values);\n            })\n       .def(\"create_scan\",\n-           [](mlir::OpBuilder &self, std::vector<mlir::Value> operands,\n+           [](TritonOpBuilder &self, std::vector<mlir::Value> operands,\n               int axis) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::ScanOp>(loc, operands, axis);\n+             return self.create<mlir::triton::ScanOp>(operands, axis);\n            })\n       .def(\"create_scan_ret\",\n-           [](mlir::OpBuilder &self, py::args args) -> mlir::OpState {\n-             auto loc = self.getUnknownLoc();\n+           [](TritonOpBuilder &self, py::args args) -> mlir::OpState {\n              llvm::SmallVector<mlir::Value> return_values;\n              for (const auto &arg : args) {\n                return_values.push_back(py::cast<mlir::Value>(arg));\n              }\n-             return self.create<mlir::triton::ScanReturnOp>(loc, return_values);\n+             return self.create<mlir::triton::ScanReturnOp>(return_values);\n            })\n       .def(\"create_ptr_to_int\",\n-           [](mlir::OpBuilder &self, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &val,\n               mlir::Type &type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::PtrToIntOp>(loc, type, val);\n+             return self.create<mlir::triton::PtrToIntOp>(type, val);\n            })\n       .def(\"create_int_to_ptr\",\n-           [](mlir::OpBuilder &self, mlir::Value &val,\n+           [](TritonOpBuilder &self, mlir::Value &val,\n               mlir::Type &type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::IntToPtrOp>(loc, type, val);\n+             return self.create<mlir::triton::IntToPtrOp>(type, val);\n            })\n       .def(\"create_select\",\n-           [](mlir::OpBuilder &self, mlir::Value &condition,\n+           [](TritonOpBuilder &self, mlir::Value &condition,\n               mlir::Value &trueValue, mlir::Value &falseValue) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::arith::SelectOp>(loc, condition,\n-                                                       trueValue, falseValue);\n+             return self.create<mlir::arith::SelectOp>(condition, trueValue,\n+                                                       falseValue);\n            })\n       .def(\"create_print\",\n-           [](mlir::OpBuilder &self, const std::string &prefix,\n+           [](TritonOpBuilder &self, const std::string &prefix,\n               const std::vector<mlir::Value> &values) -> void {\n-             auto loc = self.getUnknownLoc();\n              self.create<mlir::triton::PrintOp>(\n-                 loc,\n-                 mlir::StringAttr::get(self.getContext(),\n+                 mlir::StringAttr::get(self.getBuilder().getContext(),\n                                        llvm::StringRef(prefix)),\n                  values);\n            })\n       .def(\"create_assert\",\n-           [](mlir::OpBuilder &self, mlir::Value &condition,\n+           [](TritonOpBuilder &self, mlir::Value &condition,\n               const std::string &message, const std::string &fileName,\n               const std::string &funcName, unsigned lineNo) -> void {\n-             auto loc = self.getUnknownLoc();\n-             auto messageAttr = mlir::StringAttr::get(self.getContext(),\n-                                                      llvm::StringRef(message));\n+             auto messageAttr = mlir::StringAttr::get(\n+                 self.getBuilder().getContext(), llvm::StringRef(message));\n              auto fileNameAttr = mlir::StringAttr::get(\n-                 self.getContext(), llvm::StringRef(fileName));\n+                 self.getBuilder().getContext(), llvm::StringRef(fileName));\n              auto funcNameAttr = mlir::StringAttr::get(\n-                 self.getContext(), llvm::StringRef(funcName));\n-             auto lineNoAttr = self.getI32IntegerAttr(lineNo);\n-             self.create<mlir::triton::AssertOp>(loc, condition, messageAttr,\n+                 self.getBuilder().getContext(), llvm::StringRef(funcName));\n+             auto lineNoAttr = self.getBuilder().getI32IntegerAttr(lineNo);\n+             self.create<mlir::triton::AssertOp>(condition, messageAttr,\n                                                  fileNameAttr, funcNameAttr,\n                                                  lineNoAttr);\n            })\n       // Undef\n       .def(\"create_undef\",\n-           [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<::mlir::LLVM::UndefOp>(loc, type);\n+           [](TritonOpBuilder &self, mlir::Type &type) -> mlir::Value {\n+             return self.create<::mlir::LLVM::UndefOp>(type);\n            })\n       // Force GPU barrier\n       .def(\"create_barrier\",\n-           [](mlir::OpBuilder &self) {\n-             auto loc = self.getUnknownLoc();\n-             self.create<mlir::gpu::BarrierOp>(loc);\n-           })\n+           [](TritonOpBuilder &self) { self.create<mlir::gpu::BarrierOp>(); })\n       // Make a block pointer (tensor pointer in Triton IR)\n       .def(\"create_make_block_ptr\",\n-           [](mlir::OpBuilder &self, mlir::Value &base,\n+           [](TritonOpBuilder &self, mlir::Value &base,\n               std::vector<mlir::Value> &shape,\n               std::vector<mlir::Value> &strides,\n               std::vector<mlir::Value> &offsets,\n               std::vector<int32_t> &tensorShape,\n               std::vector<int32_t> &order) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::MakeTensorPtrOp>(\n-                 loc, base, shape, strides, offsets, tensorShape, order);\n+                 base, shape, strides, offsets, tensorShape, order);\n            })\n       // Advance a block pointer\n       .def(\"create_advance\",\n-           [](mlir::OpBuilder &self, mlir::Value &ptr,\n+           [](TritonOpBuilder &self, mlir::Value &ptr,\n               std::vector<mlir::Value> &offsets) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::AdvanceOp>(loc, ptr.getType(),\n-                                                         ptr, offsets);\n+             return self.create<mlir::triton::AdvanceOp>(ptr.getType(), ptr,\n+                                                         offsets);\n            });\n \n-  py::class_<mlir::PassManager>(m, \"pass_manager\")\n+  py::class_<mlir::PassManager>(m, \"pass_manager\", py::module_local())\n       .def(py::init<mlir::MLIRContext *>())\n       .def(\"enable_debug\",\n            [](mlir::PassManager &self) {\n@@ -1508,6 +1529,7 @@ void init_triton_ir(py::module &&m) {\n              self.getContext()->disableMultithreading();\n              auto printingFlags = mlir::OpPrintingFlags();\n              printingFlags.elideLargeElementsAttrs(16);\n+             printingFlags.enableDebugInfo();\n              auto print_always = [](mlir::Pass *, mlir::Operation *) {\n                return true;\n              };\n@@ -1554,6 +1576,10 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::triton::createCombineOpsPass());\n            })\n+      .def(\"add_reorder_broadcast_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::triton::createReorderBroadcastPass());\n+           })\n       .def(\"add_rewrite_tensor_pointer_pass\",\n            [](mlir::PassManager &self, int computeCapability) {\n              self.addPass(mlir::triton::createRewriteTensorPointerPass(\n@@ -1646,71 +1672,76 @@ void init_triton_translation(py::module &m) {\n               \"failed to parse IR: \" + error.getMessage() +\n               \"lineno: \" + std::to_string(error.getLineNo()));\n         }\n-\n         // translate module to PTX\n         auto ptxCode =\n             triton::translateLLVMIRToPTX(*module, capability, version);\n         return ptxCode;\n       },\n       ret::take_ownership);\n \n-  m.def(\"compile_ptx_to_cubin\",\n-        [](const std::string &ptxCode, const std::string &ptxasPath,\n-           int capability) -> py::object {\n-          std::string cubin;\n-          {\n-            py::gil_scoped_release allow_threads;\n-\n-            // compile ptx with ptxas\n-            llvm::SmallString<64> fsrc;\n-            llvm::SmallString<64> flog;\n-            llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n-            llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n-            std::string fbin = std::string(fsrc) + \".o\";\n-            llvm::FileRemover logRemover(flog);\n-            llvm::FileRemover binRemover(fbin);\n-            const char *_fsrc = fsrc.c_str();\n-            const char *_flog = flog.c_str();\n-            const char *_fbin = fbin.c_str();\n-            std::ofstream ofs(_fsrc);\n-            ofs << ptxCode << std::endl;\n-            ofs.close();\n-            std::string cmd;\n-            int err;\n-            cmd = ptxasPath + \" -v --gpu-name=sm_\" +\n-                  std::to_string(capability) + (capability == 90 ? \"a \" : \" \") +\n-                  _fsrc + \" -o \" + _fsrc + \".o 2> \" + _flog;\n-\n-            err = system(cmd.c_str());\n-            if (err != 0) {\n-              err >>= 8;\n-              std::ifstream _log(_flog);\n-              std::string log(std::istreambuf_iterator<char>(_log), {});\n-              if (err == 255) {\n-                throw std::runtime_error(\n-                    \"Internal Triton PTX codegen error: \\n\" + log);\n-              } else if (err == 128 + SIGSEGV) {\n-                throw std::runtime_error(\"Please run `ptxas \" +\n-                                         fsrc.str().str() +\n-                                         \"` to confirm that this is a \"\n-                                         \"bug in `ptxas`\\n\" +\n-                                         log);\n-              } else {\n-                throw std::runtime_error(\"`ptxas` failed with error code \" +\n-                                         std::to_string(err) + \": \\n\" + log);\n-              }\n-              return {};\n+  m.def(\n+      \"compile_ptx_to_cubin\",\n+      [](const std::string &ptxCode, const std::string &ptxasPath,\n+         int capability) -> py::object {\n+        std::string cubin;\n+        {\n+          py::gil_scoped_release allow_threads;\n+\n+          // compile ptx with ptxas\n+          llvm::SmallString<64> fsrc;\n+          llvm::SmallString<64> flog;\n+          llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n+          llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n+          std::string fbin = std::string(fsrc) + \".o\";\n+          llvm::FileRemover logRemover(flog);\n+          llvm::FileRemover binRemover(fbin);\n+          const char *_fsrc = fsrc.c_str();\n+          const char *_flog = flog.c_str();\n+          const char *_fbin = fbin.c_str();\n+          std::ofstream ofs(_fsrc);\n+          ofs << ptxCode << std::endl;\n+          ofs.close();\n+\n+          auto lineInfoOption =\n+              triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\")\n+                  ? \"\"\n+                  : \" -lineinfo\";\n+          auto capabilitySuffix = (capability == 90) ? \"a \" : \" \";\n+          auto outputFileName = std::string(_fsrc) + \".o\";\n+          auto logRedirect = \" 2> \" + std::string(_flog);\n+          std::string cmd = ptxasPath + lineInfoOption + \" -v --gpu-name=sm_\" +\n+                            std::to_string(capability) + capabilitySuffix +\n+                            _fsrc + \" -o \" + outputFileName + logRedirect;\n+\n+          int err = system(cmd.c_str());\n+          if (err != 0) {\n+            err >>= 8;\n+            std::ifstream _log(_flog);\n+            std::string log(std::istreambuf_iterator<char>(_log), {});\n+            if (err == 255) {\n+              throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n+                                       log);\n+            } else if (err == 128 + SIGSEGV) {\n+              throw std::runtime_error(\"Please run `ptxas \" + fsrc.str().str() +\n+                                       \"` to confirm that this is a \"\n+                                       \"bug in `ptxas`\\n\" +\n+                                       log);\n             } else {\n-              llvm::FileRemover srcRemover(fsrc);\n-              std::ifstream _cubin(_fbin, std::ios::binary);\n-              cubin = std::string(std::istreambuf_iterator<char>(_cubin), {});\n-              _cubin.close();\n-              // Do not return here, exit the gil scope and return below\n+              throw std::runtime_error(\"`ptxas` failed with error code \" +\n+                                       std::to_string(err) + \": \\n\" + log);\n             }\n+            return {};\n+          } else {\n+            llvm::FileRemover srcRemover(fsrc);\n+            std::ifstream _cubin(_fbin, std::ios::binary);\n+            cubin = std::string(std::istreambuf_iterator<char>(_cubin), {});\n+            _cubin.close();\n+            // Do not return here, exit the gil scope and return below\n           }\n-          py::bytes bytes(cubin);\n-          return std::move(bytes);\n-        });\n+        }\n+        py::bytes bytes(cubin);\n+        return std::move(bytes);\n+      });\n \n   m.def(\"add_external_libs\",\n         [](mlir::ModuleOp &op, const std::vector<std::string> &names,"}, {"filename": "python/test/kernel_comparison/kernels.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -23,7 +23,7 @@ name_and_extension:\n     extension: ptx\n   - name: _kernel_0d1d2d345d6d7c89c1011c\n     extension: ptx\n-  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15c16d17d18d19c20d21d22d23c2425d26d27\n+  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15d16c17d18d19d20c21d22d23d24c2526d27d\n     extension: ptx\n   - name: _fwd_kernel_0d1d2d34d5d6d7d8d9d10c11d12d13d14c15d16d17d18c19d20d21d22c2324d25d\n     extension: ptx"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 83, "deletions": 68, "changes": 151, "file_content_changes": "@@ -38,54 +38,40 @@ def nvsmi(attrs):\n mem_clocks = {'v100': 877, 'a100': 1215}\n \n matmul_data = {\n-    'v100': {\n-        # square\n-        (512, 512, 512): {'float16': 0.158},\n-        (1024, 1024, 1024): {'float16': 0.466},\n-        (2048, 2048, 2048): {'float16': 0.695},\n-        (4096, 4096, 4096): {'float16': 0.831},\n-        (8192, 8192, 8192): {'float16': 0.849},\n-        # tall-skinny\n-        (16, 1024, 1024): {'float16': 0.0128},\n-        (16, 4096, 4096): {'float16': 0.0883},\n-        (16, 8192, 8192): {'float16': 0.101},\n-        (64, 1024, 1024): {'float16': 0.073},\n-        (64, 4096, 4096): {'float16': 0.270},\n-        (64, 8192, 8192): {'float16': 0.459},\n-        (1024, 64, 1024): {'float16': 0.0692},\n-        (4096, 64, 4096): {'float16': 0.264},\n-        (8192, 64, 8192): {'float16': 0.452},\n-    },\n     # NOTE:\n-    # A100 in the CI server is slow-ish for some reason.\n-    # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n-        (512, 512, 512): {'float16': 0.084, 'float32': 0.13, 'int8': 0.05},\n-        (1024, 1024, 1024): {'float16': 0.332, 'float32': 0.35, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.641, 'float32': 0.57, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.785, 'float32': 0.75, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.805, 'float32': 0.85, 'int8': 0.51},\n+        # square\n+        (512, 512, 512): {'float16': 0.061, 'float32': 0.097, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.283, 'float32': 0.313, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.618, 'float32': 0.532, 'int8': 0.34},\n+        (8192, 8192, 8192): {'float16': 0.786, 'float32': 0.754, 'int8': 0.51},\n         # tall-skinny\n-        (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n-        (16, 4096, 4096): {'float16': 0.044, 'float32': 0.0457, 'int8': 0.0259},\n-        (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n-        (64, 1024, 1024): {'float16': 0.028, 'float32': 0.0509, 'int8': 0.0169},\n-        (64, 4096, 4096): {'float16': 0.163, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.285, 'float32': 0.257, 'int8': 0.174},\n-        (1024, 64, 1024): {'float16': 0.033, 'float32': 0.0458, 'int8': 0.017},\n-        (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.254, 'float32': 0.230, 'int8': 0.177},\n+        (16, 1024, 1024): {'float16': 0.006, 'float32': 0.009, 'int8': 0.005},\n+        (16, 4096, 4096): {'float16': 0.057, 'float32': 0.051, 'int8': 0.026},\n+        (16, 8192, 8192): {'float16': 0.077, 'float32': 0.077, 'int8': 0.043},\n+        (64, 1024, 1024): {'float16': 0.018, 'float32': 0.023, 'int8': 0.017},\n+        (64, 4096, 4096): {'float16': 0.150, 'float32': 0.000, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.338, 'float32': 0.000, 'int8': 0.174},\n+        (1024, 64, 1024): {'float16': 0.029, 'float32': 0.046, 'int8': 0.017},\n+        (4096, 64, 4096): {'float16': 0.179, 'float32': 0.214, 'int8': 0.102},\n+        (8192, 64, 8192): {'float16': 0.278, 'float32': 0.000, 'int8': 0.177},\n+        # test EVEN_K==False\n+        (8192, 8192, 8176): {'float16': 0.786, 'float32': 0.696, 'int8': 0.51},\n     }\n }\n \n \n @pytest.mark.parametrize('M, N, K, dtype_str',\n                          [(M, N, K, dtype_str)\n                           for M, N, K in matmul_data[DEVICE_NAME].keys()\n-                          for dtype_str in ['float16']])\n+                          for dtype_str in ['float16', 'float32']])\n def test_matmul(M, N, K, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n     if dtype_str in ['float32', 'int8'] and DEVICE_NAME != 'a100':\n         pytest.skip('Only test float32 & int8 on a100')\n+    if (M, N, K) in [(64, 4096, 4096), (64, 8192, 8192), (8192, 64, 8192)] and dtype_str == 'float32':\n+        pytest.skip('Out of shared memory in float32')\n     dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n     torch.manual_seed(0)\n     ref_gpu_util = matmul_data[DEVICE_NAME][(M, N, K)][dtype_str]\n@@ -99,11 +85,11 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=300)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)\n \n \n #######################\n@@ -125,42 +111,42 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n \n \n elementwise_data = {\n-    'v100': {\n-        1024 * 16: 0.0219,\n-        1024 * 64: 0.0791,\n-        1024 * 256: 0.243,\n-        1024 * 1024: 0.530,\n-        1024 * 4096: 0.796,\n-        1024 * 16384: 0.905,\n-        1024 * 65536: 0.939,\n-    },\n     'a100': {\n-        1024 * 16: 0.010,\n-        1024 * 64: 0.040,\n-        1024 * 256: 0.132,\n-        1024 * 1024: 0.353,\n-        1024 * 4096: 0.605,\n-        1024 * 16384: 0.758,\n-        1024 * 65536: 0.850,\n+        1024 * 16: {'float16': 0.003, 'float32': 0.007},\n+        1024 * 64: {'float16': 0.013, 'float32': 0.026},\n+        1024 * 256: {'float16': 0.053, 'float32': 0.105},\n+        1024 * 1024: {'float16': 0.212, 'float32': 0.420},\n+        1024 * 16384: {'float16': 0.762, 'float32': 0.812},\n+        1024 * 65536: {'float16': 0.846, 'float32': 0.869},\n+        # Non pow 2\n+        1020 * 100: {'float16': 0.020, 'float32': 0.041},\n+        10003 * 7007: {'float16': 0.513, 'float32': 0.861},\n     }\n }\n \n \n @pytest.mark.parametrize('N', elementwise_data[DEVICE_NAME].keys())\n-def test_elementwise(N):\n+@pytest.mark.parametrize(\"dtype_str\", ['float16', 'bfloat16', 'float32'])\n+def test_elementwise(N, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n     torch.manual_seed(0)\n-    ref_gpu_util = elementwise_data[DEVICE_NAME][N]\n+    if dtype_str in ['bfloat16'] and DEVICE_NAME != 'a100':\n+        pytest.skip('Only test bfloat16 on a100')\n+    dtype = {'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float32': torch.float32}[dtype_str]\n+    ref_dtype_str = 'float16' if dtype_str == 'bfloat16' else dtype_str\n+    ref_gpu_util = elementwise_data[DEVICE_NAME][N][ref_dtype_str]\n     max_gpu_perf = get_dram_gbps()\n-    z = torch.empty((N, ), dtype=torch.float16, device='cuda')\n+    z = torch.empty((N, ), dtype=dtype, device='cuda')\n     x = torch.randn_like(z)\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)\n \n #######################\n # Flash-Attention\n@@ -169,34 +155,63 @@ def test_elementwise(N):\n \n flash_attention_data = {\n     \"a100\": {\n-        (4, 48, 4096, 64, 'forward', 'float16'): 0.37,\n-        (4, 48, 4096, 64, 'backward', 'float16'): 0.25,\n+        (4, 48, 4096, 64, True, True, 'forward', 'float16'): 0.433,\n+        (4, 48, 4096, 64, True, True, 'forward', 'bfloat16'): 0.392,\n+        (4, 48, 1024, 16, True, True, 'forward', 'float32'): 0.106,\n+        (4, 48, 4096, 64, True, True, 'backward', 'float16'): 0.204,\n+        (4, 48, 4096, 64, True, True, 'backward', 'bfloat16'): 0.202,\n+        (4, 48, 1024, 16, True, True, 'backward', 'float32'): 0.089,\n+        (4, 48, 4096, 64, True, False, 'forward', 'float16'): 0.242,\n+        (4, 48, 4096, 64, True, False, 'forward', 'bfloat16'): 0.220,\n+        (4, 48, 1024, 16, True, False, 'forward', 'float32'): 0.069,\n+        (4, 48, 4096, 64, True, False, 'backward', 'float16'): 0.136,\n+        (4, 48, 4096, 64, True, False, 'backward', 'bfloat16'): 0.135,\n+        (4, 48, 1024, 16, True, False, 'backward', 'float32'): 0.052,\n+        (4, 48, 4096, 64, False, True, 'forward', 'float16'): 0.432,\n+        (4, 48, 4096, 64, False, True, 'forward', 'bfloat16'): 0.392,\n+        (4, 48, 1024, 16, False, True, 'forward', 'float32'): 0.107,\n+        (4, 48, 4096, 64, False, True, 'backward', 'float16'): 0.265,\n+        (4, 48, 4096, 64, False, True, 'backward', 'bfloat16'): 0.257,\n+        (4, 48, 1024, 16, False, True, 'backward', 'float32'): 0.128,\n+        (4, 48, 4096, 64, False, False, 'forward', 'float16'): 0.251,\n+        (4, 48, 4096, 64, False, False, 'forward', 'bfloat16'): 0.220,\n+        (4, 48, 1024, 16, False, False, 'forward', 'float32'): 0.069,\n+        (4, 48, 4096, 64, False, False, 'backward', 'float16'): 0.159,\n+        (4, 48, 4096, 64, False, False, 'backward', 'bfloat16'): 0.138,\n+        (4, 48, 1024, 16, False, False, 'backward', 'float32'): 0.076,\n     }\n }\n \n \n-@pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n+@pytest.mark.parametrize(\"dtype_str\", ['float16', 'bfloat16', 'float32'])\n @pytest.mark.parametrize(\"mode\", ['forward', 'backward'])\n-@pytest.mark.parametrize(\"dtype_str\", ['float16'])\n-def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n+@pytest.mark.parametrize(\"causal\", [True, False])\n+@pytest.mark.parametrize(\"seq_par\", [True, False])\n+@pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n+def test_flash_attention(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str):\n+    stream = torch.cuda.Stream()\n+    torch.cuda.set_stream(stream)\n     is_backward = mode == 'backward'\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         pytest.skip(\"Flash attention only supported for compute capability < 80\")\n     torch.manual_seed(20)\n-    dtype = {'float16': torch.float16, 'float32': torch.float32, 'int8': torch.int8}[dtype_str]\n+    dtype = {'float16': torch.float16, 'bfloat16': torch.bfloat16, 'float32': torch.float32}[dtype_str]\n     # init data\n+    if dtype_str == 'float32':\n+        N_CTX = 1024\n+        D_HEAD = 16\n     q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n     k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n     v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n     sm_scale = 0.2\n     # benchmark\n-    fn = lambda: triton.ops.attention(q, k, v, sm_scale)\n+    fn = lambda: triton.ops.attention(q, k, v, causal, sm_scale, seq_par)\n     if is_backward:\n         o = fn()\n         do = torch.randn_like(o)\n         fn = lambda: o.backward(do, retain_graph=True)\n-    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n+    ms = triton.testing.do_bench_cudagraph(fn)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n     total_flops = 2 * flops_per_matmul\n@@ -207,6 +222,6 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n-    ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, seq_par, causal, mode, dtype_str)]\n     print_perf(ms, cur_gpu_util, ref_gpu_util)\n-    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n+    triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.02, rtol=0.01)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 146, "deletions": 142, "changes": 288, "file_content_changes": "@@ -119,18 +119,18 @@ def check_type_supported(dtype, device):\n class MmaLayout:\n     def __init__(self, version, warps_per_cta):\n         self.version = version\n-        self.warps_per_cta = str(warps_per_cta)\n+        self.warps_per_cta = warps_per_cta\n \n     def __str__(self):\n         return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}}}>\"\n \n \n class BlockedLayout:\n     def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n-        self.sz_per_thread = str(size_per_thread)\n-        self.threads_per_warp = str(threads_per_warp)\n-        self.warps_per_cta = str(warps_per_cta)\n-        self.order = str(order)\n+        self.sz_per_thread = size_per_thread\n+        self.threads_per_warp = threads_per_warp\n+        self.warps_per_cta = warps_per_cta\n+        self.order = order\n \n     def __str__(self):\n         return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n@@ -706,8 +706,8 @@ def test_abs(dtype_x, device):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-def test_abs_f8(in_dtype, device):\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+def test_abs_fp8(in_dtype, device):\n \n     @triton.jit\n     def abs_kernel(X, Z, SIZE: tl.constexpr):\n@@ -723,7 +723,6 @@ def abs_kernel(X, Z, SIZE: tl.constexpr):\n     f8 = triton.reinterpret(f8_tensor, in_dtype)\n     n_elements = f8_tensor.numel()\n     out_f8 = torch.empty_like(f8_tensor)\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     abs_kernel[(1,)](f8, triton.reinterpret(out_f8, in_dtype), n_elements)\n \n     f32_tensor = convert_float_to_float32(f8_tensor, in_dtype)\n@@ -1102,6 +1101,7 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n     ('bfloat16', 'float32', False),\n     ('float32', 'int32', True),\n     ('float32', 'int1', False),\n+    ('int8', 'bfloat16', False),\n ] + [\n     (f'uint{x}', f'int{x}', True) for x in [8, 16, 32, 64]\n ] + [\n@@ -1112,43 +1112,46 @@ def test_cast(dtype_x, dtype_z, bitcast, device):\n     check_type_supported(dtype_x, device)\n     check_type_supported(dtype_z, device)\n \n+    size = 1024\n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n-    x0 = 43 if dtype_x in int_dtypes else 43.5\n-    if dtype_x in float_dtypes and dtype_z == 'int1':\n-        x0 = 0.5\n     if dtype_x.startswith('bfloat'):\n-        x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n+        x_tri = torch.randn(size, dtype=getattr(torch, dtype_x), device=device)\n     else:\n-        x = np.array([x0], dtype=getattr(np, dtype_x))\n+        x = numpy_random(size, dtype_str=dtype_x, low=-10, high=10) * 10\n+        # Triton clamps negative values to zero, while numpy wraps around\n+        # intmax, so avoid negatives for now.\n+        # TODO: figure out which one should actually be happening, and test it\n+        if dtype_z in uint_dtypes:\n+            x = np.absolute(x)\n         x_tri = to_triton(x, device=device)\n \n     # triton kernel\n     @triton.jit\n-    def kernel(X, Z, BITCAST: tl.constexpr):\n-        x_ptr = X + tl.arange(0, 1)\n-        z_ptr = Z + tl.arange(0, 1)\n+    def kernel(X, Z, BITCAST: tl.constexpr, SIZE: tl.constexpr):\n+        x_ptr = X + tl.arange(0, SIZE)\n+        z_ptr = Z + tl.arange(0, SIZE)\n         x = tl.load(x_ptr)\n         z = x.to(Z.dtype.element_ty, bitcast=BITCAST)\n         tl.store(z_ptr, z)\n \n     dtype_z_np = dtype_z if dtype_z != 'int1' else 'bool_'\n     # triton result\n     if dtype_z.startswith('bfloat'):\n-        z_tri = torch.empty((1,), dtype=getattr(torch, dtype_z), device=device)\n+        z_tri = torch.empty((size,), dtype=getattr(torch, dtype_z), device=device)\n     else:\n-        z_tri = to_triton(np.empty((1, ), dtype=getattr(np, dtype_z_np)), device=device)\n-    kernel[(1, )](x_tri, z_tri, BITCAST=bitcast)\n+        z_tri = to_triton(np.empty((size, ), dtype=getattr(np, dtype_z_np)), device=device)\n+    kernel[(1, )](x_tri, z_tri, BITCAST=bitcast, SIZE=size, num_warps=1)\n     # torch result\n     if dtype_z.startswith('bfloat') or dtype_x.startswith('bfloat'):\n         assert bitcast is False\n         z_ref = x_tri.to(z_tri.dtype)\n-        assert z_tri == z_ref\n+        torch.testing.assert_close(z_ref, z_tri, rtol=0, atol=0)\n     else:\n         if bitcast:\n             z_ref = x.view(getattr(np, dtype_z_np))\n         else:\n             z_ref = x.astype(getattr(np, dtype_z_np))\n-        assert to_numpy(z_tri) == z_ref\n+        np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0, atol=0)\n \n \n @pytest.mark.parametrize(\"dtype_str, num_warps\", [(dtype_str, num_warps) for dtype_str in int_dtypes + float_dtypes for num_warps in [4, 8]])\n@@ -1215,7 +1218,7 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     fp = fp.view(getattr(torch, f\"int{dtype.primitive_bitwidth}\"))\n     exp_width = dtype.primitive_bitwidth - dtype.fp_mantissa_width - 1\n-    exp_bias = 2 ** (exp_width - 1) - 1\n+    exp_bias = dtype.exponent_bias\n     sign = ((fp >> (dtype.primitive_bitwidth - 1)) & 0x01).int()\n     exp = ((fp >> dtype.fp_mantissa_width) & ((1 << exp_width) - 1)).int()\n     frac = (fp & ((1 << dtype.fp_mantissa_width) - 1)).int()\n@@ -1228,7 +1231,7 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n     extended_exp = ((1 << (tl.float32.primitive_bitwidth - tl.float32.fp_mantissa_width - 1)) - 1) << tl.float32.fp_mantissa_width\n     # special cases, exp is 0b11..1\n-    if dtype == tl.float8e4:\n+    if dtype in [tl.float8e4, tl.float8e4b15]:\n         # float8e4m3 does not have infinities\n         output[fp == 0b01111111] = torch.nan\n         output[fp == 0b11111111] = torch.nan\n@@ -1255,52 +1258,43 @@ def test_convert_float16_to_float32(in_dtype, device):\n     assert torch.all(f16_input[other] == f32_output[other])\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n-def test_f8_xf16_roundtrip(in_dtype, out_dtype, device):\n-    \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n-    check_type_supported(out_dtype, device)\n-\n-    @triton.jit\n-    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n-        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-        mask = offsets < n_elements\n-        input = tl.load(input_ptr + offsets, mask=mask)\n-        output = input\n-        tl.store(output_ptr + offsets, output, mask=mask)\n-\n-    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device=device)\n-    # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n-    all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n-    f8_tensor[all_exp_ones] = 0\n-    f8 = triton.reinterpret(f8_tensor, in_dtype)\n-    n_elements = f8_tensor.numel()\n-    xf16 = torch.empty_like(f8_tensor, dtype=out_dtype)\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n-\n-    # exponent_mask = 0b01111100 for float8e5\n-    # exponent_mask = 0b01111000 for float8e4\n-    exponent_mask = 0b01111111 ^ ((1 << in_dtype.fp_mantissa_width) - 1)\n-    normal = torch.logical_and((f8_tensor & exponent_mask) != 0, (f8_tensor & exponent_mask) != exponent_mask)\n-    ref16 = convert_float_to_float32(f8_tensor, in_dtype)\n-    # WARN: currently only normal float8s are handled\n-    assert torch.all(xf16[normal] == ref16[normal])\n-\n-    f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n-    f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n-    copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n+def serialize_fp8(np_data, in_dtype):\n+    if in_dtype == tl.float8e4b15:\n+        # triton's f8e4b15 format is optimized for software emulation\n+        # as a result, each pack of 4xfp8 values:\n+        # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n+        # is actually internally stored as\n+        # s0s2b0b2s1s3b1b3\n+        # we apply the conversion here\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n+        signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n+        bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n+        # tensor of triton fp8 data\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n \n-    assert torch.all(f8_tensor == f8_output_tensor)\n \n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n+def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n+    \"\"\"\n+    For all possible float8 values (ref_fp8 = range(0, 256)), test that:\n+        - conversion tri_fp16 = convert(input=ref_fp8, out=out_dtype) matches the reference\n+        - conversion tri_fp8 = convert(input=tri_fp16, out=out_dtype) matches the original\n+    this is only possible if both conversions are correct\n+    \"\"\"\n+    check_type_supported(out_dtype, device)\n+    from contextlib import nullcontext as does_not_raise\n+    expectation = does_not_raise()\n+    err_msg = None\n+    if (in_dtype == tl.float8e4b15 and out_dtype != torch.float16) or\\\n+       (in_dtype != torch.float16 and out_dtype == tl.float8e4b15):\n+        expectation = pytest.raises(triton.CompilationError)\n+        err_msg = \"fp8e4b15 can only be converted to/from fp16\"\n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16])\n-def test_f16_to_f8_rounding(in_dtype, out_dtype, device):\n-    \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n-    error is the minimum over all float8.\n-    Or the same explanation a bit mathier:\n-    for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n@@ -1309,50 +1303,28 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    i16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device=device)\n-    f16_input = i16_input.view(out_dtype)\n-    n_elements = f16_input.numel()\n-    f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n-    f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-    copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n-\n-    f16_output = torch.empty_like(f16_input, dtype=out_dtype)\n-    copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n-\n-    abs_error = torch.abs(f16_input - f16_output)\n-\n-    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device=device)\n-    all_f8_vals = triton.reinterpret(all_f8_vals_tensor, in_dtype)\n-    all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=out_dtype)\n-    copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n-\n-    all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n-        torch.isfinite(all_f8_vals_in_f16)\n-    ]\n-\n-    min_error = torch.min(\n-        torch.abs(\n-            f16_input.reshape((-1, 1))\n-            - all_finite_f8_vals_in_f16.reshape((1, -1))\n-        ),\n-        dim=1,\n-    )[0]\n-\n-    # WARN: only normalized numbers are handled\n-    f8_normal_min = 1 << in_dtype.fp_mantissa_width  # 0b00001000 for float8e4\n-    f8_normal_max = 0b01111110 if in_dtype == tl.float8e4 else 0b01111011\n-    f16_min, f16_max, f16_max_minus_1 = convert_float_to_float32(torch.tensor([f8_normal_min, f8_normal_max, f8_normal_max - 1], dtype=torch.int8), in_dtype)\n-    assert torch.all(torch.isfinite(f16_min))\n-    assert torch.all(torch.isfinite(f16_max))\n-    thres_error = f16_max - f16_max_minus_1\n-    mismatch = torch.logical_and(\n-        torch.logical_or(abs_error != min_error, abs_error > thres_error), torch.logical_and(torch.isfinite(f16_input), torch.logical_and(torch.abs(f16_input) <= f16_max, torch.abs(f16_input) >= f16_min))\n-    )\n-    assert torch.all(\n-        torch.logical_not(mismatch)\n-    ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n-\n+    # initialize array containing all possible f8 values except NaN\n+    ref_fp8 = np.array(range(-128, 128), dtype=np.int8)\n+    is_nan = (ref_fp8 & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n+    exp_mask = 0b01111111 ^ ((1 << in_dtype.fp_mantissa_width) - 1)\n+    is_subnormal = np.logical_or((ref_fp8 & exp_mask) == 0, (ref_fp8 & exp_mask) == exp_mask)\n+    ref_fp8[is_nan] = 0\n+    ref_fp8[is_subnormal] = 0\n+    tri_fp8 = torch.from_numpy(serialize_fp8(ref_fp8, in_dtype)).cuda()\n+    tri_fp16 = torch.empty(256, dtype=out_dtype, device=\"cuda\")\n+    with expectation as e:\n+        copy_kernel[(1,)](triton.reinterpret(tri_fp8, in_dtype), tri_fp16, tri_fp16.shape[0], BLOCK_SIZE=1024)\n+\n+        ref_fp8 = torch.from_numpy(ref_fp8).cuda()\n+        ref_fp16 = convert_float_to_float32(ref_fp8, in_dtype)\n+        assert torch.all(tri_fp16[~is_subnormal] == ref_fp16[~is_subnormal])\n+\n+        ref_fp8 = torch.empty_like(tri_fp16, dtype=torch.int8)\n+        copy_kernel[(1,)](tri_fp16, triton.reinterpret(ref_fp8, in_dtype), tri_fp16.shape[0], BLOCK_SIZE=1024)\n+        assert torch.all(tri_fp8 == ref_fp8)\n+\n+    if err_msg is not None:\n+        assert err_msg in str(e)\n \n # ---------------\n # test reduce\n@@ -1534,7 +1506,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     for type in ['int32', 'float32']\n     for axis in [1, 0]\n     for shape in scan2d_shapes\n-    for op in ['cumsum']\n+    for op in ['cumsum', 'cumprod']\n ]\n \n \n@@ -1557,7 +1529,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n     z = np.empty_like(x)\n     x_tri = to_triton(x, device=device)\n-    numpy_op = {'cumsum': np.cumsum}[op]\n+    numpy_op = {'cumsum': np.cumsum, 'cumprod': np.cumprod}[op]\n     z_dtype_str = dtype_str\n     z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n     # triton result\n@@ -1566,7 +1538,10 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     z_tri = to_numpy(z_tri)\n     # compare\n     if dtype_str == 'float32':\n-        np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+        if op == 'cumprod':\n+            np.testing.assert_allclose(z_ref, z_tri, rtol=0.01, atol=1e-3)\n+        else:\n+            np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n     else:\n         np.testing.assert_equal(z_ref, z_tri)\n \n@@ -1643,12 +1618,13 @@ def test_scan_layouts(M, N, src_layout, axis, device):\n layouts = [\n     BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n     BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    BlockedLayout([4, 4], [2, 16], [4, 1], [1, 0]),\n     MmaLayout(version=(2, 0), warps_per_cta=[4, 1]),\n     MmaLayout(version=(2, 0), warps_per_cta=[2, 2])\n ]\n \n \n-@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n+@pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128], [32, 32]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n def test_reduce_layouts(M, N, src_layout, axis, device):\n@@ -1659,31 +1635,30 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n     #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}}>\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n-    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n+    tt.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: i32 {{tt.divisibility = 16 : i32}}, %arg2: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n         %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n         %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n         %2 = tt.splat %arg1 : (i32) -> tensor<{M}x1xi32, #blocked>\n         %3 = arith.muli %1, %2 : tensor<{M}x1xi32, #blocked>\n-        %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #blocked>\n-        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<f32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+        %4 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n+        %5 = tt.addptr %4, %3 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n         %6 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n         %7 = tt.expand_dims %6 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n-        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<f32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<f32>, #blocked>\n+        %8 = tt.broadcast %5 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n         %9 = tt.broadcast %7 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n-        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<f32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n-        %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>\n-        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n-        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xf32, #blocked>\n-        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xf32, #blocked>) -> tensor<{M}x{N}xf32, #src>\n+        %10 = tt.addptr %8, %9 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+        %11 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>\n+        %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<i32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n+        %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #blocked>\n+        %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #src>\n         %15 = \"tt.reduce\"(%14) ({{\n-        ^bb0(%arg3: f32, %arg4: f32):\n-          %16 = \"triton_gpu.cmpf\"(%arg3, %arg4) {{predicate = 2 : i64}} : (f32, f32) -> i1\n-          %17 = arith.select %16, %arg3, %arg4 : f32\n-          tt.reduce.return %17 : f32\n-        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xf32, #src>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n-        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n-        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xf32, #blocked>\n-        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xf32, #blocked>\n+        ^bb0(%arg3: i32, %arg4: i32):\n+          %17 = arith.addi %arg3, %arg4 : i32\n+          tt.reduce.return %17 : i32\n+        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n+        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xi32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xi32, #blocked>\n+        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xi32, #blocked>\n         tt.return\n     }}\n     }}\n@@ -1696,22 +1671,21 @@ def test_reduce_layouts(M, N, src_layout, axis, device):\n         kernel = triton.compile(f.name)\n \n     rs = RandomState(17)\n-    x = rs.randint(0, 4, (M, N)).astype('float32')\n-    x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+    x = rs.randint(0, 20, (M, N)).astype('int32')\n \n     if axis == 0:\n-        z = np.zeros((1, N)).astype('float32')\n+        z = np.zeros((1, N)).astype('int32')\n     else:\n-        z = np.zeros((M, 1)).astype('float32')\n+        z = np.zeros((M, 1)).astype('int32')\n \n     x_tri = torch.tensor(x, device=device)\n     z_tri = torch.tensor(z, device=device)\n \n     pgm = kernel[(1, 1, 4)](x_tri, x_tri.stride(0), z_tri)\n \n-    z_ref = np.max(x, axis=axis, keepdims=True)\n+    z_ref = np.sum(x, axis=axis, keepdims=True)\n \n-    np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+    np.testing.assert_equal(z_ref, z_tri.cpu().numpy())\n \n \n layouts = [\n@@ -2102,7 +2076,6 @@ def kernel(X, stride_xm, stride_xk,\n         out_dtype = tl.float16\n     else:\n         out_dtype = tl.float32\n-\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n@@ -2117,6 +2090,14 @@ def kernel(X, stride_xm, stride_xk,\n                          CHAIN_DOT=epilogue == 'chain-dot',\n                          ALLOW_TF32=allow_tf32,\n                          num_warps=num_warps)\n+    if epilogue == 'softmax' and (in_dtype != 'float32' or allow_tf32):\n+        ptx = pgm.asm[\"ptx\"]\n+        start = ptx.find(\"shfl.sync\")\n+        end = ptx.find(\"cvt.rn.f16.f32\")\n+        red_code = ptx[start:end]\n+        assert len(red_code) > 0\n+        assert \"shared\" not in red_code\n+        assert \"bar.sync\" not in red_code\n     # torch result\n     if in_dtype == 'int8':\n         z_ref = np.matmul(x.astype(np.float32),\n@@ -2201,9 +2182,13 @@ def kernel(Z, X, Y,\n     assert \"triton_gpu.async_wait {num = 2 : i32}\" in h.asm['ttgir']\n \n \n-@pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n+@pytest.mark.parametrize(\"dtype_str\", int_dtypes + uint_dtypes + float_dtypes + ['bfloat16'])\n def test_full(dtype_str, device):\n-    dtype = getattr(torch, dtype_str)\n+    if dtype_str in uint_dtypes and not hasattr(torch, dtype_str):\n+        # PyTorch only has unsigned 8, but not 16, 32, or 64\n+        dtype = getattr(torch, dtype_str[1:])  # uintx -> intx\n+    else:\n+        dtype = getattr(torch, dtype_str)\n     check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     @triton.jit\n@@ -2508,14 +2493,18 @@ def test_default(device):\n     ret1 = torch.zeros(1, dtype=torch.int32, device=device)\n \n     @triton.jit\n-    def _kernel(ret0, ret1, value):\n+    def _kernel(ret0, ret1, value=3):\n         tl.store(ret0, _impl())\n         tl.store(ret1, _impl(value))\n \n     _kernel[(1,)](ret0, ret1, value)\n     assert ret0.item() == 10\n     assert ret1.item() == value\n \n+    _kernel[(1,)](ret0, ret1)\n+    assert ret0.item() == 10\n+    assert ret1.item() == 3\n+\n # ---------------\n # test noop\n # ----------------\n@@ -2706,7 +2695,7 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n def test_if(if_type, device):\n \n     @triton.jit\n@@ -3093,6 +3082,21 @@ def kernel(InitI, Bound, CutOff, OutI, OutInitI, OutJ):\n     assert out_i[0] == init_i[0] + 1\n     assert out_j[0] == bound[0]\n \n+def test_while(device):\n+    @triton.jit\n+    def nested_while(data, countPtr):\n+        for i in range(10):\n+            count = tl.load(countPtr)\n+            while count > 0:\n+                tl.store(data, tl.load(data) + 1.0)\n+                count = count - 2\n+\n+    counter = torch.tensor([8], dtype=torch.int32, device=device)\n+    data = torch.zeros((1,), device=device, dtype=torch.float32)\n+    nested_while[(1,)](data, counter)\n+    assert data[0] == 40\n+\n+\n # def test_for_if(device):\n \n #     @triton.jit"}, {"filename": "python/test/unit/language/test_line_info.py", "status": "added", "additions": 120, "deletions": 0, "changes": 120, "file_content_changes": "@@ -0,0 +1,120 @@\n+import subprocess\n+import tempfile\n+\n+import pytest\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def kernel_single(X,\n+                  Y,\n+                  BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+\n+@triton.jit\n+def device_inline(x):\n+    return x + x\n+\n+\n+@triton.jit\n+def kernel_call(X,\n+                Y,\n+                BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = device_inline(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+\n+@triton.jit(noinline=True)\n+def device_noinline(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = x + x\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+\n+@triton.jit\n+def kernel_call_noinline(X, Y, BLOCK: tl.constexpr):\n+    device_noinline(X, Y, BLOCK)\n+\n+\n+@triton.jit\n+def kernel_multi_files(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = tl.softmax(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\n+\n+def extract_file_lines(asm):\n+    fd, path = tempfile.mkstemp()\n+    with open(fd, 'wb') as cubin:\n+        cubin.write(asm)\n+    asm = subprocess.check_output([\"nvdisasm\", \"-g\", path]).decode(\"utf-8\")\n+    file_lines = []\n+    lines = asm.splitlines()\n+    for line in lines:\n+        if \"## File\" in line:\n+            entries = line[line.index(\"## File\"):].split(\",\")\n+            file_lines.append((entries[0].strip(), entries[1].strip()))\n+    return file_lines\n+\n+\n+def check_file_lines(file_lines, file_name, lineno):\n+    for file, line in file_lines:\n+        # -1 means do not check line number\n+        if lineno == -1:\n+            if file_name in file:\n+                return True\n+        if file_name in file and str(lineno) in line:\n+            return True\n+    return False\n+\n+\n+func_types = [\"single\", \"call\", \"call_noinline\", \"multi_files\"]\n+\n+\n+@pytest.mark.parametrize(\"func\", func_types)\n+def test_line_info(func: str):\n+    try:\n+        subprocess.check_output([\"nvdisasm\", \"-h\"])\n+    except BaseException:\n+        pytest.skip(\"nvdisasm is not available\")\n+\n+    shape = (128, )\n+    x = torch.arange(0, shape[0], dtype=torch.float32, device='cuda')\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    kernel_info = {}\n+    if func == \"single\":\n+        kernel_info = kernel_single[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"call\":\n+        kernel_info = kernel_call[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"call_noinline\":\n+        kernel_info = kernel_call_noinline[(1,)](x, y, BLOCK=shape[0])\n+    elif func == \"multi_files\":\n+        kernel_info = kernel_multi_files[(1,)](x, y, BLOCK=shape[0])\n+\n+    file_lines = extract_file_lines(kernel_info.asm[\"cubin\"])\n+    if func == \"single\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 15))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 16))\n+    elif func == \"call\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 28))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 21))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 30))\n+    elif func == \"call_noinline\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 42))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 35))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 36))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 37))\n+    elif func == \"multi_files\":\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 47))\n+        assert (check_file_lines(file_lines, \"test_line_info.py\", 49))\n+        assert (check_file_lines(file_lines, \"standard.py\", 33))\n+        assert (check_file_lines(file_lines, \"standard.py\", 34))\n+        assert (check_file_lines(file_lines, \"standard.py\", 36))\n+        # core.py is changed frequently, so we only check if it exists\n+        assert (check_file_lines(file_lines, \"core.py\", -1))"}, {"filename": "python/test/unit/language/test_subprocess.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -9,7 +9,7 @@\n assert_path = os.path.join(dir_path, \"assert_helper.py\")\n \n # TODO: bfloat16 after LLVM-15\n-func_types = [\"device_assert\", \"assert\", \"static_assert\", \"no_debug\"]\n+assert_types = [\"device_assert\", \"assert\", \"static_assert\", \"no_debug\"]\n nested_types = [(caller, callee) for caller in [\"true\", \"false\", \"none\"] for callee in [\"true\", \"false\", \"none\"]]\n torch_types = [\"int8\", \"uint8\", \"int16\", \"int32\", \"long\", \"float16\", \"float32\", \"float64\"]\n \n@@ -37,7 +37,7 @@ def test_print(func_type: str, data_type: str):\n         assert len(new_lines) == 1\n \n \n-@pytest.mark.parametrize(\"func_type\", func_types)\n+@pytest.mark.parametrize(\"func_type\", assert_types)\n def test_assert(func_type: str):\n     os.environ[\"TRITON_DEBUG\"] = \"1\"\n     proc = subprocess.Popen([sys.executable, assert_path, func_type], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -33,7 +33,8 @@ def test_op(M, N, dtype, mode):\n         tt_y.backward(dy)\n         tt_dx = x.grad.clone()\n         # torch backward\n-        x.grad.zero_()\n+        x.grad = None\n         th_y.backward(dy)\n         th_dx = x.grad.clone()\n+\n         torch.testing.assert_allclose(th_dx, tt_dx)"}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -10,22 +10,23 @@\n                                                  (4, 48, 1024, 64),\n                                                  (4, 48, 1024, 128)])\n @pytest.mark.parametrize('dtype', [torch.float16, torch.bfloat16])\n-def test_op(Z, H, N_CTX, D_HEAD, dtype):\n+@pytest.mark.parametrize('causal', [True, False])\n+@pytest.mark.parametrize('seq_par', [True, False])\n+def test_op(Z, H, N_CTX, D_HEAD, dtype, causal, seq_par):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         pytest.skip(\"Flash attention only supported for compute capability < 80\")\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n-    sm_scale = 0.2\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    sm_scale = 0.5\n     dout = torch.randn_like(q)\n     # reference implementation\n     M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n-    for z in range(Z):\n-        for h in range(H):\n-            p[:, :, M == 0] = float(\"-inf\")\n+    if causal:\n+        p[:, :, M == 0] = float(\"-inf\")\n     p = torch.softmax(p.float(), dim=-1).to(dtype)\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n@@ -34,7 +35,7 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype):\n     ref_dk, k.grad = k.grad.clone(), None\n     ref_dq, q.grad = q.grad.clone(), None\n     # # triton implementation\n-    tri_out = triton.ops.attention(q, k, v, sm_scale)\n+    tri_out = triton.ops.attention(q, k, v, causal, sm_scale, seq_par)\n     # print(ref_out)\n     # print(tri_out)\n     tri_out.backward(dout)"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 49, "deletions": 30, "changes": 79, "file_content_changes": "@@ -8,20 +8,20 @@\n import triton.ops\n \n \n-def f8_to_f16(x):\n+def f8_to_f16(x, dtype):\n \n     @triton.jit\n     def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n         pid = tl.program_id(0)\n         offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n         mask = offs < N\n         x = tl.load(X + offs, mask=mask)\n-        y = x.to(tl.float8e5)\n-        tl.store(Y + offs, y, mask=mask)\n+        tl.store(Y + offs, x, mask=mask)\n \n     ret = torch.empty(x.shape, dtype=torch.float16, device=x.device)\n     grid = lambda META: (triton.cdiv(x.numel(), META['BLOCK_SIZE']),)\n-    kernel[grid](ret, triton.reinterpret(x, tl.float8e5), ret.numel(), BLOCK_SIZE=1024)\n+    dtype = getattr(tl, dtype)\n+    kernel[grid](ret, triton.reinterpret(x, dtype), ret.numel(), BLOCK_SIZE=1024)\n     return ret\n \n \n@@ -63,36 +63,39 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # variable input\n-                (128, 128, 32, 1, 4, 2, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n-                (128, 256, 64, 1, 8, 3, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n+                (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n         *[\n             [\n-                (16, 16, 16, 1, 1, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (64, 32, 64, 1, 2, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (128, 64, 16, 1, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 32, 1, 8, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, STAGES, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 16, 1, 1, STAGES, 32, 32, 80, AT, BT, DTYPE, DTYPE),\n+                (64, 32, 64, 1, 2, STAGES, 128, 64, 128, AT, BT, DTYPE, DTYPE),\n+                (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE),\n+                (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE),\n                 # split-k\n-                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 8, 4, STAGES, 1024, 1024, 32, AT, BT, DTYPE, DTYPE),\n-            ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [2, 3, 4]\n+                (64, 64, 16, 8, 4, STAGES, 128, 128, 768, AT, BT, DTYPE, DTYPE),\n+                (64, 64, 16, 8, 4, STAGES, 128, 128, 32, AT, BT, DTYPE, DTYPE),\n+            ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [4]\n         ],\n         # mixed-precision\n         *[\n             [\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (32, 64, 16, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n-                (128, 128, 32, 8, 4, 2, 1024, 1024, 1024, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8\", \"float16\"), (\"float16\", \"float32\"), (\"float32\", \"float16\"),\n-                                     (\"bfloat16\", \"float32\"), (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n+                (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n+                                     (\"float8e4\", \"float16\"),\n+                                     (\"float16\", \"float8e5\"),\n+                                     (\"float16\", \"float32\"),\n+                                     (\"float32\", \"float16\"),\n+                                     (\"bfloat16\", \"float32\"),\n+                                     (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n         ]\n     ),\n )\n@@ -117,22 +120,38 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n+    a_fp8 = \"float8\" in ADTYPE\n+    b_fp8 = \"float8\" in BDTYPE\n \n-    def get_input(n, m, t, dtype):\n+    def maybe_upcast(x, dtype, is_float8):\n+        if is_float8:\n+            return f8_to_f16(x, dtype)\n+        return x\n+\n+    def init_input(n, m, t, dtype, is_float8):\n         if t:\n-            return get_input(m, n, False, dtype).t()\n-        if dtype == \"float8\":\n-            x = torch.randint(10, 50, (n, m), device=\"cuda\", dtype=torch.int8)\n-            return f8_to_f16(x)\n+            return init_input(m, n, False, dtype, is_float8).t()\n+        if is_float8:\n+            return torch.randint(20, 60, (n, m), device=\"cuda\", dtype=torch.int8)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n         return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n \n     # allocate/transpose inputs\n-    a = get_input(M, K, AT, ADTYPE)\n-    b = get_input(K, N, BT, BDTYPE)\n+    a = init_input(M, K, AT, ADTYPE, a_fp8)\n+    b = init_input(K, N, BT, BDTYPE, b_fp8)\n     # run test\n-    th_c = torch.matmul(a.to(torch.float32), b.to(torch.float32))\n+    th_a = maybe_upcast(a, ADTYPE, a_fp8).to(torch.float32)\n+    if AT and a_fp8:\n+        th_a = th_a.view(th_a.shape[::-1]).T\n+    th_b = maybe_upcast(b, BDTYPE, b_fp8).to(torch.float32)\n+    if BT and b_fp8:\n+        th_b = th_b.view(th_b.shape[::-1]).T\n+    th_c = torch.matmul(th_a, th_b)\n     try:\n+        if a_fp8:\n+            a = triton.reinterpret(a, getattr(tl, ADTYPE))\n+        if b_fp8:\n+            b = triton.reinterpret(b, getattr(tl, BDTYPE))\n         tt_c = triton.ops.matmul(a, b)\n         atol, rtol = 1e-2, 0\n         if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:"}, {"filename": "python/test/unit/tools/test_aot.py", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -92,7 +92,7 @@ def kernel(C, A, B,\n   cuMemAlloc(&B, K * N * 2);\n   cuMemAlloc(&C, M * N * 4);\n   cuStreamCreate(&stream, 0);\n-  load_kernel();\n+  load_matmul_fp16xfp16_16x16x16();\n \n   // initialize input data\n   int16_t hA[M*K];\n@@ -105,10 +105,9 @@ def kernel(C, A, B,\n   cuMemcpyHtoD(B, hB, K*N*2);\n \n   // launch kernel\n-  int numWarps = 1;\n   int gX = 1, gY = 1, gZ = 1;\n   cuStreamSynchronize(stream);\n-  kernel(stream, M/BM, N/BN, 1, numWarps, C, A, B, N, K, N);\n+  matmul_fp16xfp16_16x16x16(stream, M/BM, N/BN, 1, C, A, B, N, K, N);\n   cuStreamSynchronize(stream);\n \n   // read data\n@@ -119,7 +118,7 @@ def kernel(C, A, B,\n \n \n   // free cuda handles\n-  unload_kernel();\n+  unload_matmul_fp16xfp16_16x16x16();\n   cuMemFree(A);\n   cuMemFree(B);\n   cuMemFree(C);\n@@ -153,7 +152,7 @@ def test_compile_link_matmul():\n             for hb in hints:\n                 sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, 1, i32{hb}, 1, i32:16, 1, {BM}, {BN}, {BK}'\n                 name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n-                subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, kernel_path], check=True, cwd=tmp_dir)\n+                subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", kernel_path], check=True, cwd=tmp_dir)\n \n         # link all desired configs\n         h_files = glob.glob(os.path.join(tmp_dir, \"*.h\"))"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 63, "deletions": 14, "changes": 77, "file_content_changes": "@@ -75,6 +75,20 @@ def _check_fn_args(node, fn, args):\n                 raise UnsupportedLanguageConstruct(fn.src, node, f'Function {fn.__name__} is marked noinline, but was called with non-scalar argument {fn.arg_names[idx]}:{arg}')\n \n \n+def _get_fn_file_line(fn):\n+    base_fn = fn\n+    while not isinstance(base_fn, JITFunction):\n+        base_fn = base_fn.fn\n+    file_name = base_fn.fn.__code__.co_filename\n+    lines, begin_line = inspect.getsourcelines(base_fn.fn)\n+    for line in lines:\n+        if line.strip().startswith('@'):\n+            begin_line += 1\n+        else:\n+            break\n+    return file_name, begin_line\n+\n+\n _condition_types = {bool, int, type(None)}  # Python types accepted for conditionals inside kernels\n \n \n@@ -189,10 +203,16 @@ def visit_Call(self, node: ast.Call) -> bool:\n \n \n class CodeGenerator(ast.NodeVisitor):\n-    def __init__(self, context, prototype, gscope, attributes, constants, function_name,\n+    def __init__(self, context, prototype, gscope, attributes, constants, function_name, arch,\n                  module=None, is_kernel=False, function_types: Optional[Dict] = None,\n-                 debug=False, noinline=False):\n+                 debug=False, noinline=False, file_name: Optional[str] = None, begin_line=0):\n+        self.context = context\n         self.builder = ir.builder(context)\n+        self.file_name = file_name\n+        # node.lineno starts from 1, so we need to subtract 1\n+        self.begin_line = begin_line - 1\n+        self.builder.set_loc(file_name, begin_line, 0)\n+        self.builder.arch = arch\n         self.module = self.builder.create_module() if module is None else module\n         self.function_ret_types = {} if function_types is None else function_types\n         self.prototype = prototype\n@@ -248,6 +268,18 @@ def set_value(self, name: str,\n         self.lscope[name] = value\n         self.local_defs[name] = value\n \n+    def _get_insertion_point_and_loc(self):\n+        # XXX: this is a hack to get the location of the insertion point.\n+        # The insertion point's location could be invalid sometimes,\n+        # so we need to explicitly set the location\n+        loc = self.builder.get_loc()\n+        ip = self.builder.get_insertion_point()\n+        return ip, loc\n+\n+    def _set_insertion_point_and_loc(self, ip, loc):\n+        self.builder.restore_insertion_point(ip)\n+        self.builder.set_loc(loc)\n+\n     #\n     # AST visitor\n     #\n@@ -533,13 +565,13 @@ def visit_if_top_level(self, cond, node):\n     def visit_if_scf(self, cond, node):\n         with enter_sub_region(self) as sr:\n             liveins, _ = sr\n-            ip = self.builder.get_insertion_point()\n+            ip, last_loc = self._get_insertion_point_and_loc()\n             then_block = self.builder.create_block()\n             else_block = self.builder.create_block() if node.orelse else None\n             then_defs, else_defs, then_block, else_block, names, ret_types, _ = \\\n                 self.visit_then_else_blocks(node, liveins, then_block, else_block)\n             # create if op\n-            self.builder.restore_insertion_point(ip)\n+            self._set_insertion_point_and_loc(ip, last_loc)\n             if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, True)\n             then_block.merge_block_before(if_op.get_then_block())\n             self.builder.set_insertion_point_to_end(if_op.get_then_block())\n@@ -627,6 +659,7 @@ def visit_UnaryOp(self, node):\n     def visit_While(self, node):\n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n+            ip, last_loc = self._get_insertion_point_and_loc()\n \n             # loop body (the after region)\n             # loop_block = self.builder.create_block()\n@@ -636,6 +669,7 @@ def visit_While(self, node):\n             self.visit_compound_statement(node.body)\n             self.scf_stack.pop()\n             loop_defs = self.local_defs\n+            dummy.erase()\n \n             # collect loop-carried values\n             names = []\n@@ -652,7 +686,7 @@ def visit_While(self, node):\n                     ret_types.append(loop_defs[name].type)\n                     init_args.append(liveins[name])\n \n-            self.builder.set_insertion_point_to_end(insert_block)\n+            self._set_insertion_point_and_loc(ip, last_loc)\n             while_op = self.builder.create_while_op([ty.to_ir(self.builder) for ty in ret_types],\n                                                     [arg.handle for arg in init_args])\n             # merge the condition region\n@@ -760,7 +794,7 @@ def visit_For(self, node):\n \n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n-            ip = self.builder.get_insertion_point()\n+            ip, last_loc = self._get_insertion_point_and_loc()\n \n             # create loop body block\n             block = self.builder.create_block()\n@@ -790,7 +824,7 @@ def visit_For(self, node):\n                     yields.append(language.core._to_tensor(self.local_defs[name], self.builder))\n \n             # create ForOp\n-            self.builder.restore_insertion_point(ip)\n+            self._set_insertion_point_and_loc(ip, last_loc)\n             for_op = self.builder.create_for_op(lb, ub, step, [arg.handle for arg in init_args])\n \n             self.scf_stack.append(node)\n@@ -867,7 +901,10 @@ def call_JitFunction(self, fn: JITFunction, args, kwargs):\n             gscope = sys.modules[fn.fn.__module__].__dict__\n             # If the callee is not set, we use the same debug setting as the caller\n             debug = self.debug if fn.debug is None else fn.debug\n-            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=debug, noinline=fn.noinline)\n+            file_name, begin_line = _get_fn_file_line(fn)\n+            generator = CodeGenerator(self.context, prototype, gscope, attributes, constants, module=self.module,\n+                                      function_name=fn_name, function_types=self.function_ret_types, debug=debug, noinline=fn.noinline,\n+                                      file_name=file_name, begin_line=begin_line, arch=self.builder.arch)\n             generator.visit(fn.parse())\n             callee_ret_type = generator.last_ret_type\n             self.function_ret_types[fn_name] = callee_ret_type\n@@ -965,14 +1002,23 @@ def visit_JoinedStr(self, node):\n         return ''.join(values)\n \n     def visit(self, node):\n-        if node is not None:\n-            self.last_node = node\n+        if node is None:\n+            return\n         with warnings.catch_warnings():\n             # The ast library added visit_Constant and deprecated some other\n             # methods but we can't move to that without breaking Python 3.6 and 3.7.\n             warnings.simplefilter(\"ignore\", DeprecationWarning)  # python 3.9\n             warnings.simplefilter(\"ignore\", PendingDeprecationWarning)  # python 3.8\n-            return super().visit(node)\n+            self.last_node = node\n+            last_loc = self.builder.get_loc()\n+            if hasattr(node, 'lineno') and hasattr(node, 'col_offset'):\n+                self.builder.set_loc(self.file_name, self.begin_line + node.lineno, node.col_offset)\n+                last_loc = self.builder.get_loc()\n+            ret = super().visit(node)\n+            # Reset the location to the last one before the visit\n+            if last_loc:\n+                self.builder.set_loc(last_loc)\n+            return ret\n \n     def generic_visit(self, node):\n         raise UnsupportedLanguageConstruct(None, node, \"unsupported AST node type: {}\".format(type(node).__name__))\n@@ -1015,8 +1061,9 @@ def str_to_ty(name):\n         ty = str_to_ty(name[1:])\n         return language.pointer_type(ty)\n     tys = {\n-        \"fp8e5\": language.float8e5,\n         \"fp8e4\": language.float8e4,\n+        \"fp8e5\": language.float8e5,\n+        \"fp8e4b15\": language.float8e4b15,\n         \"fp16\": language.float16,\n         \"bf16\": language.bfloat16,\n         \"fp32\": language.float32,\n@@ -1048,7 +1095,7 @@ def kernel_suffix(signature, specialization):\n     return suffix\n \n \n-def ast_to_ttir(fn, signature, specialization, constants, debug):\n+def ast_to_ttir(fn, signature, specialization, constants, debug, arch):\n     # canonicalize signature\n     if isinstance(signature, str):\n         signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n@@ -1066,11 +1113,13 @@ def ast_to_ttir(fn, signature, specialization, constants, debug):\n     all_constants = constants.copy()\n     all_constants.update(new_constants)\n     arg_types = [str_to_ty(v) for k, v in signature.items() if k not in constants]\n+    file_name, begin_line = _get_fn_file_line(fn)\n \n     prototype = language.function_type([], arg_types)\n     generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants,\n                               function_name=function_name, attributes=new_attrs,\n-                              is_kernel=True, debug=debug)\n+                              is_kernel=True, debug=debug, file_name=file_name, begin_line=begin_line,\n+                              arch=arch)\n     try:\n         generator.visit(fn.parse())\n     except CompilationError as e:"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -55,6 +55,7 @@ def optimize_ttir(mod, arch):\n     pm.add_inliner_pass()\n     pm.add_triton_combine_pass()\n     pm.add_canonicalizer_pass()\n+    pm.add_reorder_broadcast_pass()\n     pm.add_cse_pass()\n     pm.add_licm_pass()\n     pm.add_symbol_dce_pass()\n@@ -396,7 +397,7 @@ def compile(fn, **kwargs):\n     stages = dict()\n     stages[\"ast\"] = (lambda path: fn, None)\n     stages[\"ttir\"] = (lambda path: parse_mlir_module(path, context),\n-                      lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug), arch))\n+                      lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug, arch=arch), arch))\n     stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n                        lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps), num_stages, arch))\n     stages[\"llir\"] = (lambda path: Path(path).read_text(),"}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -629,3 +629,9 @@ def cumsum(self, input, axis=None):\n         if axis is None:\n             return torch.cumsum(input)\n         return torch.cumsum(input, dim=axis)\n+\n+    @_tensor_operation\n+    def cumprod(self, input, axis=None):\n+        if axis is None:\n+            return torch.cumprod(input)\n+        return torch.cumprod(input, dim=axis)"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -35,6 +35,7 @@\n     cat,\n     constexpr,\n     cos,\n+    cumprod,\n     cumsum,\n     debug_barrier,\n     device_assert,\n@@ -48,6 +49,7 @@\n     float16,\n     float32,\n     float64,\n+    float8e4b15,\n     float8e4,\n     float8e5,\n     function_type,\n@@ -130,6 +132,7 @@\n     \"cdiv\",\n     \"constexpr\",\n     \"cos\",\n+    \"cumprod\",\n     \"cumsum\",\n     \"debug_barrier\",\n     \"device_assert\",\n@@ -143,6 +146,7 @@\n     \"float16\",\n     \"float32\",\n     \"float64\",\n+    \"float8e4b15\",\n     \"float8e4\",\n     \"float8e5\",\n     \"full\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 63, "deletions": 5, "changes": 68, "file_content_changes": "@@ -7,7 +7,7 @@\n \n from .._C.libtriton.triton import ir\n from ..runtime.jit import jit\n-from . import semantic\n+from . import math, semantic\n \n T = TypeVar('T')\n \n@@ -76,7 +76,7 @@ def _to_tensor(x, builder):\n class dtype:\n     SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n+    FP_TYPES = ['fp8e4b15', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -96,24 +96,34 @@ def __init__(self, name):\n             self.int_bitwidth = int(name.split('int')[-1])\n             self.primitive_bitwidth = self.int_bitwidth\n         elif name in dtype.FP_TYPES:\n-            if name == 'fp8e4':\n+            if name == 'fp8e4b15':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n+                self.exponent_bias = 15\n+            elif name == 'fp8e4':\n+                self.fp_mantissa_width = 3\n+                self.primitive_bitwidth = 8\n+                self.exponent_bias = 7\n             elif name == 'fp8e5':\n                 self.fp_mantissa_width = 2\n                 self.primitive_bitwidth = 8\n+                self.exponent_bias = 15\n             elif name == 'fp16':\n                 self.fp_mantissa_width = 10\n                 self.primitive_bitwidth = 16\n+                self.exponent_bias = 15\n             elif name == 'bf16':\n                 self.fp_mantissa_width = 7\n                 self.primitive_bitwidth = 16\n+                self.exponent_bias = 127\n             elif name == 'fp32':\n                 self.fp_mantissa_width = 23\n                 self.primitive_bitwidth = 32\n+                self.exponent_bias = 127\n             elif name == 'fp64':\n                 self.fp_mantissa_width = 53\n                 self.primitive_bitwidth = 64\n+                self.exponent_bias = 1023\n             else:\n                 raise RuntimeError(f'Unsupported floating-point type {name}')\n         elif name == 'void':\n@@ -122,6 +132,12 @@ def __init__(self, name):\n     def is_fp8(self):\n         return 'fp8' in self.name\n \n+    def is_fp8e4(self):\n+        return self.name == 'fp8e4'\n+\n+    def is_fp8e4b15(self):\n+        return self.name == 'fp8e4b15'\n+\n     def is_fp16(self):\n         return self.name == 'fp16'\n \n@@ -223,6 +239,8 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_fp8e5_ty()\n         elif self.name == 'fp8e4':\n             return builder.get_fp8e4_ty()\n+        elif self.name == 'fp8e4b15':\n+            return builder.get_fp8e4b15_ty()\n         elif self.name == 'fp16':\n             return builder.get_half_ty()\n         elif self.name == 'bf16':\n@@ -356,6 +374,7 @@ def to_ir(self, builder: ir.builder):\n uint64 = dtype('uint64')\n float8e5 = dtype('fp8e5')\n float8e4 = dtype('fp8e4')\n+float8e4b15 = dtype('fp8e4b15')\n float16 = dtype('fp16')\n bfloat16 = dtype('bf16')\n float32 = dtype('fp32')\n@@ -382,6 +401,9 @@ def __init__(self, value):\n     def __repr__(self) -> str:\n         return f\"constexpr[{self.value}]\"\n \n+    def __index__(self):\n+        return self.value\n+\n     def __add__(self, other):\n         return constexpr(self.value + other.value)\n \n@@ -1400,6 +1422,11 @@ def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n     return _argmax_combine(value1, index1, value2, index2, False)\n \n \n+@jit\n+def _fast_max(x, y):\n+    return math.max(x, y)\n+\n+\n @jit\n @_add_reduction_docstr(\"maximum\",\n                        return_indices_arg=\"return_indices\",\n@@ -1412,7 +1439,13 @@ def max(input, axis=None, return_indices=False, return_indices_tie_break_left=Tr\n         else:\n             return _reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, maximum)\n+        if constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if constexpr(input.dtype.is_floating()):\n+                input = input.to(float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(int32)\n+        return reduce(input, axis, _fast_max)\n \n \n @jit\n@@ -1446,6 +1479,11 @@ def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n     return _argmin_combine(value1, index1, value2, index2, False)\n \n \n+@jit\n+def _fast_min(x, y):\n+    return math.min(x, y)\n+\n+\n @jit\n @_add_reduction_docstr(\"minimum\",\n                        return_indices_arg=\"return_indices\",\n@@ -1458,7 +1496,13 @@ def min(input, axis=None, return_indices=False, return_indices_tie_break_left=Tr\n         else:\n             return _reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, minimum)\n+        if constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if constexpr(input.dtype.is_floating()):\n+                input = input.to(float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(int32)\n+        return reduce(input, axis, _fast_min)\n \n \n @jit\n@@ -1562,6 +1606,20 @@ def cumsum(input, axis=0):\n     input = _promote_reduction_input(input)\n     return associative_scan(input, axis, _sum_combine)\n \n+# cumprod\n+\n+\n+@jit\n+def _prod_combine(a, b):\n+    return a * b\n+\n+\n+@jit\n+@_add_scan_docstr(\"cumprod\")\n+def cumprod(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = _promote_reduction_input(input)\n+    return associative_scan(input, axis, _prod_combine)\n \n # -----------------------\n # Compiler Hint Ops"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -1,5 +1,6 @@\n from __future__ import annotations  # remove after python 3.11\n \n+import warnings\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n@@ -662,6 +663,11 @@ def bitcast(input: tl.tensor,\n                      dst_ty)\n \n \n+# TODO: architecture descriptor class\n+def _is_cuda(arch):\n+    return isinstance(arch, int)\n+\n+\n def cast(input: tl.tensor,\n          dst_ty: tl.dtype,\n          builder: ir.builder) -> tl.tensor:\n@@ -676,6 +682,16 @@ def cast(input: tl.tensor,\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n \n+    if _is_cuda(builder.arch) and builder.arch < 89 and \\\n+       (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n+        warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n+                      \"Please use tl.float8e4b15.\", DeprecationWarning)\n+\n+    # Unsupported conversion:\n+    if (src_sca_ty.is_fp8e4b15() and not dst_sca_ty.is_fp16()) or \\\n+       (dst_sca_ty.is_fp8e4b15() and not src_sca_ty.is_fp16()):\n+        raise ValueError('fp8e4b15 can only be converted to/from fp16')\n+\n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n     if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n        (src_sca_ty.is_floating() and dst_sca_ty.is_fp8()):"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 279, "deletions": 129, "changes": 408, "file_content_changes": "@@ -3,6 +3,9 @@\n ===============\n This is a Triton implementation of the Flash Attention algorithm\n (see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n+\n+Sequence Parallel implementation inspired by HazyResearch\n+(see https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attn_triton.py)\n \"\"\"\n \n import torch\n@@ -23,68 +26,119 @@ def _fwd_kernel(\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n+    qvk_offset = off_hz * stride_qh\n+    Q_block_ptr = tl.make_block_ptr(\n+        base=Q + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_qm, stride_qk),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    K_block_ptr = tl.make_block_ptr(\n+        base=K + qvk_offset,\n+        shape=(BLOCK_DMODEL, N_CTX),\n+        strides=(stride_kk, stride_kn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_DMODEL, BLOCK_N),\n+        order=(0, 1)\n+    )\n+    V_block_ptr = tl.make_block_ptr(\n+        base=V + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_vk, stride_vn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_N, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    O_block_ptr = tl.make_block_ptr(\n+        base=Out + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_om, stride_on),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n     # initialize offsets\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     offs_n = tl.arange(0, BLOCK_N)\n-    offs_d = tl.arange(0, BLOCK_DMODEL)\n-    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n-    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    # Initialize pointers to Q, K, V\n-    q_ptrs = Q + off_q\n-    k_ptrs = K + off_k\n-    v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # causal check on every loop iteration can be expensive\n+    # and peeling the last iteration of the loop does not work well with ptxas\n+    # so we have a mode to do the causal check in a separate kernel entirely\n+    if MODE == 0:  # entire non-causal attention\n+        lo, hi = 0, N_CTX\n+    if MODE == 1:  # entire causal attention\n+        lo, hi = 0, (start_m + 1) * BLOCK_M\n+    if MODE == 2:  # off band-diagonal\n+        lo, hi = 0, start_m * BLOCK_M\n+    if MODE == 3:  # on band-diagonal\n+        l_ptrs = L + off_hz * N_CTX + offs_m\n+        m_ptrs = M + off_hz * N_CTX + offs_m\n+        m_i = tl.load(m_ptrs)\n+        l_i = tl.load(l_ptrs)\n+        acc += tl.load(O_block_ptr).to(tl.float32)\n+        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n+    # credits to: Adam P. Goucher (https://github.com/apgoucher):\n+    # scale sm_scale by 1/log_2(e) and use\n+    # 2^x instead of exp in the loop because CSE and LICM\n+    # don't work as expected with `exp` in the loop\n+    qk_scale = sm_scale * 1.44269504\n     # load q: it will stay in SRAM throughout\n-    q = tl.load(q_ptrs)\n+    q = tl.load(Q_block_ptr)\n+    q = (q * qk_scale).to(K.dtype.element_ty)\n+    # advance block pointers to first iteration of the loop\n+    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n+    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n-    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+    for start_n in range(lo, hi, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(k_ptrs)\n+        k = tl.load(K_block_ptr)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, k)\n-        qk *= sm_scale\n-        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        # compute new m\n-        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n-        # correct old l\n-        l_prev *= tl.exp(m_prev - m_curr)\n-        # attention weights\n-        p = tl.exp(qk - m_curr[:, None])\n-        l_curr = tl.sum(p, 1) + l_prev\n-        # rescale operands of matmuls\n-        l_rcp = 1. / l_curr\n-        p *= l_rcp[:, None]\n-        acc *= (l_prev * l_rcp)[:, None]\n+        qk += tl.dot(q, k, allow_tf32=True)\n+        if MODE == 1 or MODE == 3:\n+            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        p = tl.math.exp2(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        alpha = tl.math.exp2(m_i - m_i_new)\n+        beta = tl.math.exp2(m_ij - m_i_new)\n+        l_i *= alpha\n+        l_i_new = l_i + beta * l_ij\n+        # scale p\n+        p_scale = beta / l_i_new\n+        p = p * p_scale[:, None]\n+        # scale acc\n+        acc_scale = l_i / l_i_new\n+        acc = acc * acc_scale[:, None]\n         # update acc\n-        p = p.to(Q.dtype.element_ty)\n-        v = tl.load(v_ptrs)\n-        acc += tl.dot(p, v)\n+        v = tl.load(V_block_ptr)\n+        p = p.to(V.dtype.element_ty)\n+        acc += tl.dot(p, v, allow_tf32=True)\n         # update m_i and l_i\n-        l_prev = l_curr\n-        m_prev = m_curr\n+        l_i = l_i_new\n+        m_i = m_i_new\n         # update pointers\n-        k_ptrs += BLOCK_N * stride_kn\n-        v_ptrs += BLOCK_N * stride_vk\n-    # rematerialize offsets to save registers\n-    start_m = tl.program_id(0)\n-    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, l_prev)\n-    tl.store(m_ptrs, m_prev)\n-    # initialize pointers to output\n-    offs_n = tl.arange(0, BLOCK_DMODEL)\n-    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n-    out_ptrs = Out + off_o\n-    tl.store(out_ptrs, acc)\n+    tl.store(l_ptrs, l_i)\n+    tl.store(m_ptrs, m_i)\n+    # write back O\n+    tl.store(O_block_ptr, acc.to(K.dtype.element_ty))\n \n \n @jit\n@@ -107,94 +161,168 @@ def _bwd_preprocess(\n     tl.store(Delta + off_m, delta)\n \n \n+@jit\n+def _bwd_kernel_one_col_block(\n+    Q, K, V, sm_scale, qk_scale,\n+    Out, DO,\n+    DQ, DK, DV,\n+    L, M,\n+    D,\n+    stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kn, stride_kk,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    Z, H, N_CTX,\n+    off_hz, start_n, num_block,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+    SEQUENCE_PARALLEL: tl.constexpr,\n+    MODE: tl.constexpr,\n+):\n+    if SEQUENCE_PARALLEL:\n+        DQ += stride_dqa.to(tl.int64) * start_n\n+    if MODE == 0:\n+        lo = 0\n+    else:\n+        lo = start_n * BLOCK_M\n+    # initialize row/col offsets\n+    offs_qm = lo + tl.arange(0, BLOCK_M)\n+    offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n+    offs_m = tl.arange(0, BLOCK_N)\n+    offs_k = tl.arange(0, BLOCK_DMODEL)\n+    # initialize pointers to value-like data\n+    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+    v_ptrs = V + (offs_n[:, None] * stride_vk + offs_k[None, :] * stride_vn)\n+    do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+    dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+    # pointer to row-wise quantities in value-like data\n+    D_ptrs = D + off_hz * N_CTX\n+    m_ptrs = M + off_hz * N_CTX\n+    # initialize dv amd dk\n+    dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # k and v stay in SRAM throughout\n+    k = tl.load(k_ptrs)\n+    v = tl.load(v_ptrs)\n+    # loop over rows\n+    for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+        offs_m_curr = start_m + offs_m\n+        # load q, k, v, do on-chip\n+        q = tl.load(q_ptrs)\n+        # recompute p = softmax(qk, dim=-1).T\n+        # NOTE: `do` is pre-divided by `l`; no normalization here\n+        if MODE == 1:\n+            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+        else:\n+            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk += tl.dot(q, tl.trans(k))\n+        qk *= qk_scale\n+        m = tl.load(m_ptrs + offs_m_curr)\n+        p = tl.math.exp2(qk - m[:, None])\n+        # compute dv\n+        do = tl.load(do_ptrs)\n+        dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do, allow_tf32=True)\n+        # compute dp = dot(v, do)\n+        Di = tl.load(D_ptrs + offs_m_curr)\n+        # dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n+        dp = tl.dot(do, tl.trans(v), allow_tf32=True)\n+        # compute ds = p * (dp - delta[:, None])\n+        ds = (p * (dp - Di[:, None]) * sm_scale).to(Q.dtype.element_ty)\n+        # compute dk = dot(ds.T, q)\n+        dk += tl.dot(tl.trans(ds), q, allow_tf32=True)\n+        # compute dq\n+        if not SEQUENCE_PARALLEL:\n+            dq = tl.load(dq_ptrs)\n+            dq += tl.dot(ds, k, allow_tf32=True)\n+            tl.store(dq_ptrs, dq)\n+        elif SEQUENCE_PARALLEL:\n+            # dq = tl.dot(ds, k, allow_tf32=True)\n+            dq = tl.trans(tl.dot(tl.trans(k), tl.trans(ds), allow_tf32=True))\n+            tl.store(dq_ptrs, dq)\n+\n+        # increment pointers\n+        dq_ptrs += BLOCK_M * stride_qm\n+        q_ptrs += BLOCK_M * stride_qm\n+        do_ptrs += BLOCK_M * stride_qm\n+    # write-back\n+    dv_ptrs = DV + (offs_n[:, None] * stride_vk + offs_k[None, :] * stride_vn)\n+    dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+    tl.store(dv_ptrs, dv)\n+    tl.store(dk_ptrs, dk)\n+\n+\n @jit\n def _bwd_kernel(\n-    Q, K, V, sm_scale, Out, DO,\n+    # fmt: off\n+    Q, K, V, sm_scale,\n+    Out, DO,\n     DQ, DK, DV,\n     L, M,\n     D,\n-    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n     Z, H, N_CTX,\n-    num_block,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    SEQUENCE_PARALLEL: tl.constexpr,\n+    MODE: tl.constexpr,\n+    # fmt: on\n ):\n+    qk_scale = sm_scale * 1.44269504\n     off_hz = tl.program_id(0)\n     off_z = off_hz // H\n     off_h = off_hz % H\n     # offset pointers for batch/head\n     Q += off_z * stride_qz + off_h * stride_qh\n-    K += off_z * stride_qz + off_h * stride_qh\n-    V += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_kz + off_h * stride_kh\n+    V += off_z * stride_vz + off_h * stride_vh\n     DO += off_z * stride_qz + off_h * stride_qh\n     DQ += off_z * stride_qz + off_h * stride_qh\n-    DK += off_z * stride_qz + off_h * stride_qh\n-    DV += off_z * stride_qz + off_h * stride_qh\n-    for start_n in range(0, num_block):\n-        lo = start_n * BLOCK_M\n-        # initialize row/col offsets\n-        offs_qm = lo + tl.arange(0, BLOCK_M)\n-        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n-        offs_m = tl.arange(0, BLOCK_N)\n-        offs_k = tl.arange(0, BLOCK_DMODEL)\n-        # initialize pointers to value-like data\n-        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        # pointer to row-wise quantities in value-like data\n-        D_ptrs = D + off_hz * N_CTX\n-        m_ptrs = M + off_hz * N_CTX\n-        # initialize dv amd dk\n-        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-        # k and v stay in SRAM throughout\n-        k = tl.load(k_ptrs)\n-        v = tl.load(v_ptrs)\n-        # loop over rows\n-        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n-            offs_m_curr = start_m + offs_m\n-            # load q, k, v, do on-chip\n-            q = tl.load(q_ptrs)\n-            # recompute p = softmax(qk, dim=-1).T\n-            # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.dot(q, tl.trans(k))\n-            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n-            m = tl.load(m_ptrs + offs_m_curr)\n-            p = tl.exp(qk * sm_scale - m[:, None])\n-            # compute dv\n-            do = tl.load(do_ptrs)\n-            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n-            # compute dp = dot(v, do)\n-            Di = tl.load(D_ptrs + offs_m_curr)\n-            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n-            dp += tl.dot(do, tl.trans(v))\n-            # compute ds = p * (dp - delta[:, None])\n-            ds = p * dp * sm_scale\n-            # compute dk = dot(ds.T, q)\n-            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)\n-            # compute dq\n-            dq = tl.load(dq_ptrs)\n-            dq += tl.dot(ds.to(Q.dtype.element_ty), k)\n-            tl.store(dq_ptrs, dq)\n-            # increment pointers\n-            dq_ptrs += BLOCK_M * stride_qm\n-            q_ptrs += BLOCK_M * stride_qm\n-            do_ptrs += BLOCK_M * stride_qm\n-        # write-back\n-        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n-        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-        tl.store(dv_ptrs, dv)\n-        tl.store(dk_ptrs, dk)\n+    DK += off_z * stride_kz + off_h * stride_kh\n+    DV += off_z * stride_vz + off_h * stride_vh\n+\n+    num_block_n = tl.cdiv(N_CTX, BLOCK_N)\n+    if not SEQUENCE_PARALLEL:\n+        for start_n in range(0, num_block_n):\n+            _bwd_kernel_one_col_block(\n+                Q, K, V, sm_scale, qk_scale, Out, DO,\n+                DQ, DK, DV,\n+                L, M,\n+                D,\n+                stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n+                stride_kz, stride_kh, stride_kn, stride_kk,\n+                stride_vz, stride_vh, stride_vk, stride_vn,\n+                Z, H, N_CTX,\n+                off_hz, start_n, num_block_n,\n+                BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL,\n+                BLOCK_N=BLOCK_N,\n+                SEQUENCE_PARALLEL=SEQUENCE_PARALLEL,\n+                MODE=MODE,\n+            )\n+    else:\n+        start_n = tl.program_id(1)\n+        _bwd_kernel_one_col_block(\n+            Q, K, V, sm_scale, qk_scale, Out, DO,\n+            DQ, DK, DV,\n+            L, M,\n+            D,\n+            stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n+            stride_kz, stride_kh, stride_kn, stride_kk,\n+            stride_vz, stride_vh, stride_vk, stride_vn,\n+            Z, H, N_CTX,\n+            off_hz, start_n, num_block_n,\n+            BLOCK_M=BLOCK_M, BLOCK_DMODEL=BLOCK_DMODEL,\n+            BLOCK_N=BLOCK_N,\n+            SEQUENCE_PARALLEL=SEQUENCE_PARALLEL,\n+            MODE=MODE,\n+        )\n \n \n class _attention(torch.autograd.Function):\n \n     @staticmethod\n-    def forward(ctx, q, k, v, sm_scale):\n+    def forward(ctx, q, k, v, causal, sm_scale, sequence_parallel=False):\n         # only support for Ampere now\n         capability = torch.cuda.get_device_capability()\n         if capability[0] < 8:\n@@ -209,58 +337,80 @@ def forward(ctx, q, k, v, sm_scale):\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         num_warps = 4 if Lk <= 64 else 8\n-\n-        _fwd_kernel[grid](\n-            q, k, v, sm_scale,\n-            L, m,\n-            o,\n-            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=Lk, num_warps=num_warps,\n-            num_stages=2,\n-        )\n+        if causal:\n+            modes = [1] if q.shape[2] <= 2048 else [2, 3]\n+        else:\n+            modes = [0]\n+        for mode in modes:\n+            _fwd_kernel[grid](\n+                q, k, v, sm_scale,\n+                L, m,\n+                o,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2],\n+                BLOCK_M=128, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\n+                MODE=mode,\n+                num_warps=num_warps,\n+                num_stages=2)\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n+        ctx.causal = causal\n+        ctx.sequence_parallel = sequence_parallel\n         return o\n \n     @staticmethod\n     def backward(ctx, do):\n         BLOCK = 128\n         q, k, v, o, l, m = ctx.saved_tensors\n+        sequence_parallel = ctx.sequence_parallel\n+        seq_len_kv = k.shape[2]\n         do = do.contiguous()\n-        dq = torch.zeros_like(q, dtype=torch.float32)\n+        if sequence_parallel:\n+            replicas = cdiv(seq_len_kv, BLOCK)\n+            new_dq_shape = (replicas,) + q.shape\n+            dq = torch.zeros(new_dq_shape, device=q.device, dtype=q.dtype)\n+        else:\n+            dq = torch.zeros_like(q, dtype=torch.float32)\n         dk = torch.empty_like(k)\n         dv = torch.empty_like(v)\n         do_scaled = torch.empty_like(do)\n         delta = torch.empty_like(l)\n+        if ctx.causal:\n+            mode = 1\n+        else:\n+            mode = 0\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n             BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n-        _bwd_kernel[(ctx.grid[1],)](\n+        _bwd_kernel[(ctx.grid[1], cdiv(seq_len_kv, BLOCK) if sequence_parallel else 1)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n             dq, dk, dv,\n             l, m,\n             delta,\n-            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            o.numel(), q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             q.shape[0], q.shape[1], q.shape[2],\n-            ctx.grid[0],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL,\n+            SEQUENCE_PARALLEL=sequence_parallel,\n+            MODE=mode,\n+            num_warps=8,\n             num_stages=1,\n         )\n-        return dq, dk, dv, None\n+\n+        if len(dq.shape) == 5:\n+            dq = dq.sum(dim=0)\n+        return dq, dk, dv, None, None, None\n \n \n attention = _attention.apply"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -111,8 +111,9 @@ def _kernel(A, B, C, M, N, K,\n             b = tl.load(B)\n         else:\n             k_remaining = K - k * (BLOCK_K * SPLIT_K)\n-            a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n-            b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n+            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n+            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n+            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n         a = a.to(C.dtype.element_ty)\n         b = b.to(C.dtype.element_ty)\n         acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n@@ -149,7 +150,11 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        c_dtype = get_higher_dtype(a.dtype, b.dtype)\n+        if a.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5] or\\\n+           b.dtype in [tl.float8e4, tl.float8e4b15, tl.float8e5]:\n+            c_dtype = torch.float16\n+        else:\n+            c_dtype = get_higher_dtype(a.dtype, b.dtype)\n         c = torch.empty((M, N), device=device, dtype=c_dtype)\n         if dot_out_dtype is None:\n             if c_dtype in [torch.float16, torch.float32, torch.bfloat16]:"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -111,7 +111,9 @@ def run(self, *args, **kwargs):\n         if config.pre_hook is not None:\n             full_nargs = {**self.nargs, **kwargs, **self.best_config.kwargs}\n             config.pre_hook(full_nargs)\n-        return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n+        ret = self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n+        self.nargs = None\n+        return ret\n \n     def prune_configs(self, kwargs):\n         pruned_configs = self.configs"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -69,6 +69,13 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n   int32_t n_regs = 0;\n   int32_t n_spills = 0;\n   // create driver handles\n+  CUcontext pctx = 0;\n+  CUDA_CHECK(cuCtxGetCurrent(&pctx));\n+  if (!pctx) {\n+    CUDA_CHECK(cuDevicePrimaryCtxRetain(&pctx, device));\n+    CUDA_CHECK(cuCtxSetCurrent(pctx));\n+  }\n+\n   CUDA_CHECK(cuModuleLoadData(&mod, data));\n   CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n   // get allocated registers and spilled registers from the function"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -208,8 +208,9 @@ def _type_of(key):\n         dtype_str = str(key).split(\".\")[-1]\n         tys = {\n             \"bool\": \"i1\",\n-            \"float8e5\": \"fp8e5\",\n             \"float8e4\": \"fp8e4\",\n+            \"float8e5\": \"fp8e5\",\n+            \"float8e4b15\": \"fp8e4b15\",\n             \"float16\": \"fp16\",\n             \"bfloat16\": \"bf16\",\n             \"float32\": \"fp32\",\n@@ -315,9 +316,10 @@ def _make_launcher(self):\n \n         spec_keys = ', '.join(specializations)\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n+        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n \n         src = f\"\"\"\n-def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n+def {self.fn.__name__}({args_signature}, grid=None, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n     from ..compiler import compile, CompiledKernel\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n@@ -326,6 +328,7 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     if not extern_libs is None:\n       key = (key, tuple(extern_libs.items()))\n     assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n+    assert grid is not None\n     if callable(grid):\n         grid = grid({{{grid_args}}})\n     grid_size = len(grid)\n@@ -406,7 +409,8 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         # function signature information\n         signature = inspect.signature(fn)\n         self.arg_names = [v.name for v in signature.parameters.values()]\n-        self.has_defaults = any(v.default != inspect._empty for v in signature.parameters.values())\n+        self.arg_defaults = [v.default for v in signature.parameters.values()]\n+        self.has_defaults = any(v != inspect._empty for v in self.arg_defaults)\n         # specialization hints\n         self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n         self.do_not_specialize = {self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n@@ -573,10 +577,14 @@ def __init__(self, base, dtype):\n         self.base = base\n         self.is_cuda = base.is_cuda\n         self.device = base.device\n+        self.shape = self.base.shape\n \n     def data_ptr(self):\n         return self.base.data_ptr()\n \n+    def stride(self, i):\n+        return self.base.stride(i)\n+\n     def __str__(self) -> str:\n         return f'TensorWrapper[{self.dtype}]({self.base})'\n "}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 61, "deletions": 0, "changes": 61, "file_content_changes": "@@ -16,6 +16,67 @@ def nvsmi(attrs):\n     return ret\n \n \n+def do_bench_cudagraph(fn, rep=20, grad_to_none=None):\n+    import torch\n+    \"\"\"\n+    Benchmark the runtime of the provided function.\n+\n+    :param fn: Function to benchmark\n+    :type fn: Callable\n+    :param rep: Repetition time (in ms)\n+    :type rep: int\n+    :param grad_to_none: Reset the gradient of the provided tensor to None\n+    :type grad_to_none: torch.tensor, optional\n+    \"\"\"\n+    if torch.cuda.current_stream() == torch.cuda.default_stream():\n+        raise RuntimeError(\"Cannot capture graph in default stream. Please use side stream in benchmark code.\")\n+    # record CUDAGraph\n+    fn()\n+    if grad_to_none is not None:\n+        for x in grad_to_none:\n+            x.detach_()\n+            x.requires_grad_(True)\n+            x.grad = None\n+    g = torch.cuda.CUDAGraph()\n+    with torch.cuda.graph(g):\n+        fn()\n+    torch.cuda.synchronize()\n+    fn = lambda: g.replay()\n+    # Estimate the runtime of the function\n+    start_event = torch.cuda.Event(enable_timing=True)\n+    end_event = torch.cuda.Event(enable_timing=True)\n+    start_event.record()\n+    fn()\n+    end_event.record()\n+    torch.cuda.synchronize()\n+    estimate_ms = start_event.elapsed_time(end_event)\n+    # compute number of repetition to last `rep` ms\n+    n_repeat = max(1, int(rep / estimate_ms))\n+    # compute number of repetition to last `rep` ms\n+    start_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n+    end_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n+    ret = []\n+    n_retries = 50\n+    for _ in range(n_retries):\n+        # Benchmark\n+        torch.cuda.synchronize()\n+        for i in range(n_repeat):\n+            # we don't want `fn` to accumulate gradient values\n+            # if it contains a backward pass. So we clear the\n+            # provided gradients\n+            if grad_to_none is not None:\n+                for x in grad_to_none:\n+                    x.grad = None\n+            # record time of `fn`\n+            start_event[i].record()\n+            fn()\n+            end_event[i].record()\n+        torch.cuda.synchronize()\n+        times = torch.tensor([s.elapsed_time(e) for s, e in zip(start_event, end_event)])\n+        ret.append(torch.min(times))\n+    return torch.mean(torch.tensor(ret)).item()\n+\n+\n def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n              quantiles=None,\n              fast_flush=True,"}, {"filename": "python/triton/third_party/cuda/bin/ptxas", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/compile.c", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -41,7 +41,7 @@ void load_{kernel_name}() {{\n     void *bin = (void *)&CUBIN_NAME;\n     int shared = {shared};\n     CUDA_CHECK(cuModuleLoadData(&{kernel_name}_mod, bin));\n-    CUDA_CHECK(cuModuleGetFunction(&{kernel_name}_func, {kernel_name}_mod, \"{kernel_name}\"));\n+    CUDA_CHECK(cuModuleGetFunction(&{kernel_name}_func, {kernel_name}_mod, \"{triton_kernel_name}\"));\n     // set dynamic shared memory if necessary\n     int shared_optin;\n     CUDA_CHECK(cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, dev));\n@@ -54,11 +54,11 @@ void load_{kernel_name}() {{\n /*\n {kernel_docstring}\n */\n-CUresult {kernel_name}(CUstream stream, unsigned int gX,unsigned int gY,unsigned int gZ,unsigned int numWarps, {signature}) {{\n+CUresult {kernel_name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {signature}) {{\n     if ({kernel_name}_func == NULL)\n        load_{kernel_name}();\n     void *args[{num_args}] = {{ {arg_pointers} }};\n     // TODO: shared memory\n     if(gX * gY * gZ > 0)\n-      return cuLaunchKernel({kernel_name}_func, gX, gY, gZ, numWarps * 32, 1, 1, {shared}, stream, args, NULL);\n+      return cuLaunchKernel({kernel_name}_func, gX, gY, gZ, {num_warps} * 32, 1, 1, {shared}, stream, args, NULL);\n }}"}, {"filename": "python/triton/tools/compile.h", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -11,5 +11,6 @@\n void unload_{kernel_name}(void);\n void load_{kernel_name}(void);\n // tt-linker: {kernel_name}:{signature}\n-CUresult{kernel_name}(CUstream stream, unsigned int gX, unsigned int gY,\n-                      unsigned int gZ, unsigned int numWarps, {signature});\n+CUresult{_placeholder} {kernel_name}(CUstream stream, unsigned int gX,\n+                                     unsigned int gY, unsigned int gZ,\n+                                     {signature});"}, {"filename": "python/triton/tools/compile.py", "status": "modified", "additions": 25, "deletions": 7, "changes": 32, "file_content_changes": "@@ -1,8 +1,10 @@\n import binascii\n+import hashlib\n import importlib.util\n import sys\n from argparse import ArgumentParser\n from pathlib import Path\n+from typing import List\n \n import triton\n from triton.compiler.code_generator import kernel_suffix\n@@ -26,7 +28,7 @@\n \n The resulting entry point will have signature\n \n-CUresult kernel_{specialization_suffix}(CUstream stream, unsigned gX, unsigned gY, unsigned gZ, unsigned numWarps, float* arg0, int32_t arg1, int32_t arg2)\n+CUresult kernel_{specialization_suffix}(CUstream stream, unsigned gX, unsigned gY, unsigned gZ, float* arg0, int32_t arg1, int32_t arg2)\n \n Different such specialized entry points can be combined using the `linker.py` script.\n \n@@ -39,12 +41,16 @@\n     # command-line arguments\n     parser = ArgumentParser(description=desc)\n     parser.add_argument(\"path\", help=\"Path to Python source containing desired kernel in its scope. File will be executed.\")\n-    parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\")\n-    parser.add_argument(\"--out-path\", \"-o\", type=Path, help=\"Out filename\")\n+    parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\", help=\"Name of the kernel to compile\", required=True)\n+    parser.add_argument(\"--num-warps\", \"-w\", type=int, default=1, help=\"Number of warps to launch the kernel\")\n     parser.add_argument(\"--out-name\", \"-on\", type=str, default=None, help=\"Out name for the compiled kernel\")\n-    parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\")\n+    parser.add_argument(\"--out-path\", \"-o\", type=Path, default=None, help=\"Out filename\")\n+    parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\", required=True)\n     args = parser.parse_args()\n \n+    out_name = args.out_name if args.out_name else args.kernel_name\n+    out_path = args.out_path if args.out_path else out_name\n+\n     # execute python sources and extract functions wrapped in JITFunction\n     arg_path = Path(args.path)\n     sys.path.insert(0, str(arg_path.parent))\n@@ -56,6 +62,13 @@\n     # validate and parse signature\n     signature = list(map(lambda s: s.strip(\" \"), args.signature.split(\",\")))\n \n+    def hash_signature(signature: List[str]):\n+        m = hashlib.sha256()\n+        m.update(\" \".join(signature).encode())\n+        return m.hexdigest()[:8]\n+\n+    sig_hash = hash_signature(signature)\n+\n     def constexpr(s):\n         try:\n             ret = int(s)\n@@ -68,6 +81,7 @@ def constexpr(s):\n         except ValueError:\n             pass\n         return None\n+\n     hints = {i: constexpr(s.split(\":\")[1]) for i, s in enumerate(signature) if \":\" in s}\n     hints = {k: v for k, v in hints.items() if v is not None}\n     constexprs = {i: constexpr(s) for i, s in enumerate(signature)}\n@@ -80,24 +94,28 @@ def constexpr(s):\n     divisible_by_16 = [i for i, h in hints.items() if h == 16]\n     equal_to_1 = [i for i, h in hints.items() if h == 1]\n     config = triton.compiler.instance_descriptor(divisible_by_16=divisible_by_16, equal_to_1=equal_to_1)\n-    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=1)\n+    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=args.num_warps)\n     arg_names = [kernel.arg_names[i] for i in signature.keys()]\n \n     # dump C stub code\n     suffix = kernel_suffix(signature.values(), config)\n-    func_name = '_'.join([kernel.__name__, suffix])\n+    func_name = '_'.join([out_name, sig_hash, suffix])\n+    triton_kernel_name = '_'.join([args.kernel_name, suffix])\n     hex_ = str(binascii.hexlify(ccinfo.asm[\"cubin\"]))[2:-1]\n     params = {\n         \"kernel_name\": func_name,\n+        \"triton_kernel_name\": triton_kernel_name,\n         \"bin_size\": len(hex_),\n         \"bin_data\": \", \".join([f\"0x{x}{y}\" for x, y in zip(hex_[::2], hex_[1::2])]),\n         \"signature\": \", \".join([f\"{ty_to_cpp(ty)} {name}\" for name, ty in zip(arg_names, signature.values())]),\n         \"arg_pointers\": \", \".join([f\"&{arg}\" for arg in arg_names]),\n         \"num_args\": len(arg_names),\n         \"kernel_docstring\": \"\",\n         \"shared\": ccinfo.shared,\n+        \"num_warps\": args.num_warps,\n+        \"_placeholder\": \"\",\n     }\n     for ext in ['h', 'c']:\n         template_path = Path(__file__).parent / f\"compile.{ext}\"\n-        with args.out_path.with_suffix(f\".{suffix}.{ext}\").open(\"w\") as fp:\n+        with out_path.with_suffix(f\".{sig_hash}_{suffix}.{ext}\").open(\"w\") as fp:\n             fp.write(Path(template_path).read_text().format(**params))"}, {"filename": "python/triton/tools/link.py", "status": "modified", "additions": 12, "deletions": 10, "changes": 22, "file_content_changes": "@@ -18,6 +18,7 @@ class KernelLinkerMeta:\n     arg_names: Sequence[str]\n     arg_ctypes: Sequence[str]\n     sizes: Sequence[Union[int, None]]\n+    sig_hash: str\n     suffix: str\n     num_specs: int\n     \"\"\" number of specialized arguments \"\"\"\n@@ -30,7 +31,7 @@ def __init__(self) -> None:\n         # [kernel_name, c signature]\n         self.linker_directives = re.compile(\"//[\\\\s]*tt-linker:[\\\\s]*([\\\\w]+):(.+)\")\n         # [name, suffix]\n-        self.kernel_name = re.compile(\"([\\\\w]+)_([\\\\w]+)\")\n+        self.kernel_name = re.compile(\"^([\\\\w]+)_([\\\\w]+)_([\\\\w]+)$\")\n         # [(argnum, d|c)]\n         self.kernel_suffix = re.compile(\"([0-9]+)([c,d])\")\n         # [(type, name)]\n@@ -45,7 +46,7 @@ def extract_linker_meta(self, header: str):\n                 m = self.linker_directives.match(ln)\n                 if _exists(m):\n                     ker_name, c_sig = m.group(1), m.group(2)\n-                    name, suffix = self._match_name(ker_name)\n+                    name, sig_hash, suffix = self._match_name(ker_name)\n                     c_types, arg_names = self._match_c_sig(c_sig)\n                     num_specs, sizes = self._match_suffix(suffix)\n                     self._add_kernel(\n@@ -54,6 +55,7 @@ def extract_linker_meta(self, header: str):\n                             arg_names=arg_names,\n                             arg_ctypes=c_types,\n                             sizes=sizes,\n+                            sig_hash=sig_hash,\n                             suffix=suffix,\n                             num_specs=num_specs,\n                         ),\n@@ -62,8 +64,8 @@ def extract_linker_meta(self, header: str):\n     def _match_name(self, ker_name: str):\n         m = self.kernel_name.match(ker_name)\n         if _exists(m):\n-            name, suffix = m.group(1), m.group(2)\n-            return name, suffix\n+            name, sig_hash, suffix = m.group(1), m.group(2), m.group(3)\n+            return name, sig_hash, suffix\n         raise LinkerError(f\"{ker_name} is not a valid kernel name\")\n \n     def _match_c_sig(self, c_sig: str):\n@@ -110,7 +112,7 @@ def gen_signature(m):\n \n def make_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     return f\"\"\"\n-CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(metas[-1])});\n+CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(metas[-1])});\n void load_{name}();\n void unload_{name}();\n     \"\"\"\n@@ -119,26 +121,26 @@ def make_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     src = f\"// launcher for: {name}\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n-        src += f\"CUresult {name}_{meta.suffix}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(meta)});\\n\"\n+        src += f\"CUresult {name}_{meta.sig_hash}_{meta.suffix}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(meta)});\\n\"\n     src += \"\\n\"\n \n-    src += f\"CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, unsigned int numWarps, {gen_signature(metas[-1])}){{\"\n+    src += f\"CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(metas[-1])}){{\"\n     src += \"\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n         cond_fn = lambda val, hint: f\"({val} % {hint} == 0)\" if hint == 16 else f\"({val} == {hint})\" if hint == 1 else None\n         conds = \" && \".join([cond_fn(val, hint) for val, hint in zip(meta.arg_names, meta.sizes) if hint is not None])\n         src += f\"  if ({conds})\\n\"\n-        src += f\"    return {name}_{meta.suffix}(stream, gX, gY, gZ, numWarps, {', '.join(meta.arg_names)});\\n\"\n+        src += f\"    return {name}_{meta.sig_hash}_{meta.suffix}(stream, gX, gY, gZ, {', '.join(meta.arg_names)});\\n\"\n     src += \"}\\n\"\n \n     for mode in [\"load\", \"unload\"]:\n         src += f\"\\n// {mode} for: {name}\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"void {mode}_{name}_{meta.suffix}();\\n\"\n+            src += f\"void {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += f\"void {mode}_{name}() {{\"\n         src += \"\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"  {mode}_{name}_{meta.suffix}();\\n\"\n+            src += f\"  {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += \"}\\n\"\n     return src\n "}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 164, "deletions": 86, "changes": 250, "file_content_changes": "@@ -13,6 +13,11 @@\n import triton.language as tl\n \n \n+@triton.jit\n+def max_fn(x, y):\n+    return tl.math.max(x, y)\n+\n+\n @triton.jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,\n@@ -25,68 +30,119 @@ def _fwd_kernel(\n     Z, H, N_CTX,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n+    qvk_offset = off_hz * stride_qh\n+    Q_block_ptr = tl.make_block_ptr(\n+        base=Q + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_qm, stride_qk),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    K_block_ptr = tl.make_block_ptr(\n+        base=K + qvk_offset,\n+        shape=(BLOCK_DMODEL, N_CTX),\n+        strides=(stride_kk, stride_kn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_DMODEL, BLOCK_N),\n+        order=(0, 1)\n+    )\n+    V_block_ptr = tl.make_block_ptr(\n+        base=V + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_vk, stride_vn),\n+        offsets=(0, 0),\n+        block_shape=(BLOCK_N, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n+    O_block_ptr = tl.make_block_ptr(\n+        base=Out + qvk_offset,\n+        shape=(N_CTX, BLOCK_DMODEL),\n+        strides=(stride_om, stride_on),\n+        offsets=(start_m * BLOCK_M, 0),\n+        block_shape=(BLOCK_M, BLOCK_DMODEL),\n+        order=(1, 0)\n+    )\n     # initialize offsets\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     offs_n = tl.arange(0, BLOCK_N)\n-    offs_d = tl.arange(0, BLOCK_DMODEL)\n-    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n-    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    # Initialize pointers to Q, K, V\n-    q_ptrs = Q + off_q\n-    k_ptrs = K + off_k\n-    v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    # causal check on every loop iteration can be expensive\n+    # and peeling the last iteration of the loop does not work well with ptxas\n+    # so we have a mode to do the causal check in a separate kernel entirely\n+    if MODE == 0:  # entire non-causal attention\n+        lo, hi = 0, N_CTX\n+    if MODE == 1:  # entire causal attention\n+        lo, hi = 0, (start_m + 1) * BLOCK_M\n+    if MODE == 2:  # off band-diagonal\n+        lo, hi = 0, start_m * BLOCK_M\n+    if MODE == 3:  # on band-diagonal\n+        l_ptrs = L + off_hz * N_CTX + offs_m\n+        m_ptrs = M + off_hz * N_CTX + offs_m\n+        m_i = tl.load(m_ptrs)\n+        l_i = tl.load(l_ptrs)\n+        acc += tl.load(O_block_ptr).to(tl.float32)\n+        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n+    # credits to: Adam P. Goucher (https://github.com/apgoucher):\n+    # scale sm_scale by 1/log_2(e) and use\n+    # 2^x instead of exp in the loop because CSE and LICM\n+    # don't work as expected with `exp` in the loop\n+    qk_scale = sm_scale * 1.44269504\n     # load q: it will stay in SRAM throughout\n-    q = tl.load(q_ptrs)\n+    q = tl.load(Q_block_ptr)\n+    q = (q * qk_scale).to(tl.float16)\n+    # advance block pointers to first iteration of the loop\n+    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n+    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n     # loop over k, v and update accumulator\n-    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+    for start_n in range(lo, hi, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(k_ptrs)\n+        k = tl.load(K_block_ptr)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k)\n-        qk *= sm_scale\n-        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        # compute new m\n-        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n-        # correct old l\n-        l_prev *= tl.exp(m_prev - m_curr)\n-        # attention weights\n-        p = tl.exp(qk - m_curr[:, None])\n-        l_curr = tl.sum(p, 1) + l_prev\n-        # rescale operands of matmuls\n-        l_rcp = 1. / l_curr\n-        p *= l_rcp[:, None]\n-        acc *= (l_prev * l_rcp)[:, None]\n+        if MODE == 1 or MODE == 3:\n+            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        p = tl.math.exp2(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        alpha = tl.math.exp2(m_i - m_i_new)\n+        beta = tl.math.exp2(m_ij - m_i_new)\n+        l_i *= alpha\n+        l_i_new = l_i + beta * l_ij\n+        # scale p\n+        p_scale = beta / l_i_new\n+        p = p * p_scale[:, None]\n+        # scale acc\n+        acc_scale = l_i / l_i_new\n+        acc = acc * acc_scale[:, None]\n         # update acc\n-        p = p.to(Q.dtype.element_ty)\n-        v = tl.load(v_ptrs)\n+        v = tl.load(V_block_ptr)\n+        p = p.to(tl.float16)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n-        l_prev = l_curr\n-        m_prev = m_curr\n+        l_i = l_i_new\n+        m_i = m_i_new\n         # update pointers\n-        k_ptrs += BLOCK_N * stride_kn\n-        v_ptrs += BLOCK_N * stride_vk\n-    # rematerialize offsets to save registers\n-    start_m = tl.program_id(0)\n-    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n+        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, l_prev)\n-    tl.store(m_ptrs, m_prev)\n-    # initialize pointers to output\n-    offs_n = tl.arange(0, BLOCK_DMODEL)\n-    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n-    out_ptrs = Out + off_o\n-    tl.store(out_ptrs, acc)\n+    tl.store(l_ptrs, l_i)\n+    tl.store(m_ptrs, m_i)\n+    # write back O\n+    tl.store(O_block_ptr, acc.to(tl.float16))\n \n \n @triton.jit\n@@ -122,10 +178,12 @@ def _bwd_kernel(\n     num_block,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     off_hz = tl.program_id(0)\n     off_z = off_hz // H\n     off_h = off_hz % H\n+    qk_scale = sm_scale * 1.44269504\n     # offset pointers for batch/head\n     Q += off_z * stride_qz + off_h * stride_qh\n     K += off_z * stride_qz + off_h * stride_qh\n@@ -135,7 +193,10 @@ def _bwd_kernel(\n     DK += off_z * stride_qz + off_h * stride_qh\n     DV += off_z * stride_qz + off_h * stride_qh\n     for start_n in range(0, num_block):\n-        lo = start_n * BLOCK_M\n+        if MODE == 0:\n+            lo = 0\n+        else:\n+            lo = start_n * BLOCK_M\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n         offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n@@ -163,10 +224,15 @@ def _bwd_kernel(\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.dot(q, tl.trans(k))\n-            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n+            # if MODE == 1:\n+            if MODE == 1:\n+                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+            else:\n+                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+            qk += tl.dot(q, tl.trans(k))\n+            qk *= qk_scale\n             m = tl.load(m_ptrs + offs_m_curr)\n-            p = tl.exp(qk * sm_scale - m[:, None])\n+            p = tl.math.exp2(qk - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n             dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)\n@@ -199,37 +265,42 @@ def _bwd_kernel(\n class _attention(torch.autograd.Function):\n \n     @staticmethod\n-    def forward(ctx, q, k, v, sm_scale):\n+    def forward(ctx, q, k, v, causal, sm_scale):\n         BLOCK = 128\n         # shape constraints\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n         assert Lq == Lk and Lk == Lv\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n-        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+        grid = (triton.cdiv(q.shape[2], 128), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-        num_warps = 4 if Lk <= 64 else 8\n \n-        _fwd_kernel[grid](\n-            q, k, v, sm_scale,\n-            L, m,\n-            o,\n-            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n-            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n-            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=Lk, num_warps=num_warps,\n-            num_stages=2,\n-        )\n-        # print(h.asm[\"ttgir\"])\n+        num_warps = 4 if Lk <= 64 else 8\n+        if causal:\n+            modes = [1] if q.shape[2] <= 2048 else [2, 3]\n+        else:\n+            modes = [0]\n+        for mode in modes:\n+            _fwd_kernel[grid](\n+                q, k, v, sm_scale,\n+                L, m,\n+                o,\n+                q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+                k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+                v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+                o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+                q.shape[0], q.shape[1], q.shape[2],\n+                BLOCK_M=128, BLOCK_N=BLOCK, BLOCK_DMODEL=Lk,\n+                MODE=mode,\n+                num_warps=num_warps,\n+                num_stages=2)\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n+        ctx.causal = causal\n         return o\n \n     @staticmethod\n@@ -242,6 +313,10 @@ def backward(ctx, do):\n         dv = torch.empty_like(v)\n         do_scaled = torch.empty_like(do)\n         delta = torch.empty_like(l)\n+        if ctx.causal:\n+            mode = 1\n+        else:\n+            mode = 0\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n@@ -260,40 +335,38 @@ def backward(ctx, do):\n             ctx.grid[0],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            MODE=mode,\n             num_stages=1,\n         )\n-        # print(h.asm[\"ttgir\"])\n-        return dq, dk, dv, None\n+        return dq, dk, dv, None, None\n \n \n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n-def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n+@pytest.mark.parametrize('causal', [False, True])\n+def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n-    sm_scale = 0.2\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    sm_scale = 0.5\n     dout = torch.randn_like(q)\n     # reference implementation\n     M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n-    for z in range(Z):\n-        for h in range(H):\n-            p[:, :, M == 0] = float(\"-inf\")\n+    if causal:\n+        p[:, :, M == 0] = float(\"-inf\")\n     p = torch.softmax(p.float(), dim=-1).half()\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n     ref_out.backward(dout)\n     ref_dv, v.grad = v.grad.clone(), None\n     ref_dk, k.grad = k.grad.clone(), None\n     ref_dq, q.grad = q.grad.clone(), None\n-    # # triton implementation\n-    tri_out = attention(q, k, v, sm_scale)\n-    # print(ref_out)\n-    # print(tri_out)\n+    # triton implementation\n+    tri_out = attention(q, k, v, causal, sm_scale).half()\n     tri_out.backward(dout)\n     tri_dv, v.grad = v.grad.clone(), None\n     tri_dk, k.grad = k.grad.clone(), None\n@@ -315,19 +388,19 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n # vary seq length for fixed head and batch=4\n configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n-    x_vals=[2**i for i in range(10, 14)],\n+    x_vals=[2**i for i in range(10, 15)],\n     line_arg='provider',\n     line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n     line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n-    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n-) for mode in ['fwd', 'bwd']]\n+    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n+) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n \n \n @triton.testing.perf_report(configs)\n-def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\"cuda\"):\n     assert mode in ['fwd', 'bwd']\n     warmup = 25\n     rep = 100\n@@ -336,25 +409,30 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         sm_scale = 1.3\n-        fn = lambda: attention(q, k, v, sm_scale)\n+        fn = lambda: attention(q, k, v, causal, sm_scale)\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-        return ms\n     if provider == \"flash\":\n         lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n         cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n         cu_seqlens[1:] = lengths.cumsum(0)\n         qkv = torch.randn((BATCH * N_CTX, 3, H, D_HEAD), dtype=dtype, device=device, requires_grad=True)\n-        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n+        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=causal)\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-        return ms\n+    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\n+    total_flops = 2 * flops_per_matmul\n+    if causal:\n+        total_flops *= 0.5\n+    if mode == 'bwd':\n+        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+    return total_flops / ms * 1e-9\n \n \n # only works on post-Ampere GPUs right now"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -202,6 +202,34 @@ tt.func @multi_color(%A : !tt.ptr<f16>) {\n   tt.return\n }\n \n+// This example triggers graph coloring with multiple rounds\n+// CHECK-LABEL: multi_color_multi_rounds\n+tt.func @multi_color_multi_rounds(%arg0: !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 32\n+  %cst = arith.constant dense<0.000000e+00> : tensor<4x4xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 1184, size = 128\n+  %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x4xf16, #A_SHARED>\n+  // CHECK-NEXT: offset = 1312, size = 8192\n+  %cst_1 = arith.constant dense<0.000000e+00> : tensor<1024x4xf16, #A_SHARED>\n+  %cst_2 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  // CHECK-NEXT: scratch offset = 32, size = 1152\n+  %0 = triton_gpu.convert_layout %cst_2 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #AL>\n+  %1 = triton_gpu.convert_layout %cst : (tensor<4x4xf16, #A_SHARED>) -> tensor<4x4xf16, #AL>\n+  // CHECK-NEXT: offset = 11968, size = 128\n+  %cst_3 = arith.constant dense<0.000000e+00> : tensor<2x32xf16, #A_SHARED>\n+  %2 = triton_gpu.convert_layout %cst : (tensor<4x4xf16, #A_SHARED>) -> tensor<4x4xf16, #AL>\n+  // CHECK-NEXT: offset = 0, size = 512\n+  %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %3 = triton_gpu.convert_layout %cst_0 : (tensor<16x4xf16, #A_SHARED>) -> tensor<16x4xf16, #AL>\n+  %4 = triton_gpu.convert_layout %cst_1 : (tensor<1024x4xf16, #A_SHARED>) -> tensor<1024x4xf16, #AL>\n+  // CHECK-NEXT: scratch offset = 0, size = 1152\n+  %5 = triton_gpu.convert_layout %cst_2 : (tensor<16x32xf16, #AL>) -> tensor<16x32xf16, #AL>\n+  %6 = triton_gpu.convert_layout %cst_3 : (tensor<2x32xf16, #A_SHARED>) -> tensor<2x32xf16, #AL>\n+  // CHECK-NEXT: size = 12096\n+  tt.return\n+}\n+\n+\n // CHECK-LABEL: alloc\n tt.func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -67,3 +67,24 @@ tt.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n \n   tt.return\n }\n+\n+\n+// -----\n+\n+tt.func public @select_op(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i1) attributes {noinline = false} {\n+  // CHECK-LABEL: select_op\n+  %cst = arith.constant dense<0.000000e+00> : tensor<128xf32>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+  %2 = tt.addptr %1, %0 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xf32>\n+\n+  // CHECK: %[[splat:.*]] = tt.splat %arg2 : (i1) -> tensor<128xi1, #blocked>\n+  // CHECK-NEXT: %{{.*}} = \"triton_gpu.select\"(%[[splat]], %{{.*}}, %{{.*}}) : (tensor<128xi1, #blocked>, tensor<128xf32, #blocked>, tensor<128xf32, #blocked>) -> tensor<128xf32, #blocked>\n+  %4 = arith.select %arg2, %cst, %3 : tensor<128xf32>\n+\n+  %5 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %0 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n+  tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<128xf32>\n+  tt.return\n+}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "file_content_changes": "@@ -1223,3 +1223,33 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-war\n     tt.return\n   }\n }\n+\n+// -----\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: test_s8_to_bf16_conversion\n+  tt.func @test_s8_to_bf16_conversion(%in: tensor<32xi8, #blocked>) {\n+    // We can't vectorize if we only process\n+    // CHECK-NOT: llvm.inline_asm\n+    // CHECK: llvm.sitofp\n+    // CHECK-NOT: llvm.sitofp\n+    %out = arith.sitofp %in : tensor<32xi8, #blocked> to tensor<32xbf16, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 1]}>\n+#dot = #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: test_s8_to_bf16_vectorized_conversion\n+  tt.func @test_s8_to_bf16_vectorized_conversion(%in: tensor<16x16xi8, #mma>) {\n+    // CHECK-NOT: llvm.sitofp\n+    // 8 elements per thread => we should process 2 vectors of 4\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK-NOT: llvm.inline_asm\n+    %out = arith.sitofp %in : tensor<16x16xi8, #mma> to tensor<16x16xbf16, #mma>\n+    tt.return\n+  }\n+}"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -152,12 +152,18 @@ tt.func @test_canonicalize_masked_store_fail_pattern(%ptr: tensor<8x!tt.ptr<f32>\n }\n \n // CHECK-LABEL: @test_canonicalize_expand_dims\n-tt.func @test_canonicalize_expand_dims(%arg0: tensor<f32>) -> (tensor<1x8xf32>) {\n+tt.func @test_canonicalize_expand_dims(%arg0: tensor<f32>, %arg1: tensor<1xf32>) -> (tensor<1x8xf32>, tensor<8x8xf32>) {\n     %splat = tt.splat %arg0 : (tensor<f32>) -> tensor<8xf32>\n     // CHECK: %{{.*}} = tt.splat %arg0 : (tensor<f32>) -> tensor<1x8xf32>\n     %ed = tt.expand_dims %splat {axis = 0 : i32} : (tensor<8xf32>) -> tensor<1x8xf32>\n \n-    tt.return %ed : tensor<1x8xf32>\n+    // CHECK-NEXT: %[[ed2:.*]] = tt.expand_dims %arg1 {axis = 0 : i32} : (tensor<1xf32>) -> tensor<1x1xf32>\n+    // CHECK-NEXT: %{{.*}} = tt.broadcast %[[ed2]] : (tensor<1x1xf32>) -> tensor<8x8xf32>\n+    %bc = tt.broadcast %arg1 : (tensor<1xf32>) -> tensor<8xf32>\n+    %ed2 = tt.expand_dims %bc {axis = 0 : i32} : (tensor<8xf32>) -> tensor<1x8xf32>\n+    %bc2 = tt.broadcast %ed2 : (tensor<1x8xf32>) -> tensor<8x8xf32>\n+\n+    tt.return %ed, %bc2 : tensor<1x8xf32>, tensor<8x8xf32>\n }\n \n "}, {"filename": "test/Triton/reorder-broadcast.mlir", "status": "added", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -0,0 +1,40 @@\n+// RUN: triton-opt %s -split-input-file -triton-reorder-broadcast | FileCheck %s\n+\n+// CHECK-LABEL: @test_splat_elementwise_pattern\n+tt.func @test_splat_elementwise_pattern(%arg0: f32) -> (tensor<128x128xf32>, tensor<128x128x!tt.ptr<f32>>) {\n+    // CHECK-DAG: %[[a:.*]] = arith.constant 1.000000e+00 : f32\n+    // CHECK-DAG: %[[c1:.*]] = arith.constant 1 : i64\n+    %c1 = arith.constant 1 : i64\n+    %a = arith.constant dense<1.0> : tensor<128x128xf32>\n+\n+    // CHECK-DAG: %[[add:.*]] = arith.addf %arg0, %[[a]] : f32\n+    // CHECK-NEXT: %[[splat:.*]] = tt.splat %[[add]] : (f32) -> tensor<128x128xf32>\n+    %b = tt.splat %arg0 : (f32) -> tensor<128x128xf32>\n+    %add = arith.addf %a, %b : tensor<128x128xf32>\n+\n+\n+    // CHECK-NEXT: %[[ptr:.*]] = tt.int_to_ptr %[[c1]] : i64 -> !tt.ptr<f32>\n+    // CHECK-NEXT: %{{.*}} = tt.splat %[[ptr]] : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+    %c1_t = tt.splat %c1 : (i64) -> tensor<128x128xi64>\n+    %ptr = tt.int_to_ptr %c1_t : tensor<128x128xi64> -> tensor<128x128x!tt.ptr<f32>>\n+\n+    tt.return %add, %ptr : tensor<128x128xf32>, tensor<128x128x!tt.ptr<f32>>\n+}\n+\n+// CHECK-LABEL: @test_broadcast_elementwise_pattern\n+tt.func @test_broadcast_elementwise_pattern(%arg0: tensor<128x1xf32>) -> (tensor<128x128xf32>, tensor<128x32xf32>) {\n+    // CHECK: %[[one:.*]] = arith.constant dense<1.000000e+00> : tensor<128x1xf32>\n+\n+    // CHECK-NEXT: %[[abs:.*]] = math.absf %arg0 : tensor<128x1xf32>\n+    // CHECK-NEXT: %{{.*}} = tt.broadcast %[[abs]] : (tensor<128x1xf32>) -> tensor<128x128xf32>\n+    %broadcast = tt.broadcast %arg0 : (tensor<128x1xf32>) -> tensor<128x128xf32>\n+    %abs = math.absf %broadcast : tensor<128x128xf32>\n+\n+    // CHECK-NEXT: %[[add:.*]] = arith.addf %arg0, %[[one]] : tensor<128x1xf32>\n+    // CHECK-NEXT: %{{.*}} = tt.broadcast %[[add]] : (tensor<128x1xf32>) -> tensor<128x32xf32>\n+    %broadcast2 = tt.broadcast %arg0 : (tensor<128x1xf32>) -> tensor<128x32xf32>\n+    %one = arith.constant dense<1.0> : tensor<128x32xf32>\n+    %add = arith.addf %one, %broadcast2 : tensor<128x32xf32>\n+\n+    tt.return %abs, %add : tensor<128x128xf32>, tensor<128x32xf32>\n+}"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -37,8 +37,8 @@\n // CHECK:   %[[arg_b0_dot_op_0:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   %[[arg_b0_dot_op_1:.*]] = arith.mulf %[[arg_b0_dot_op_0]]\n // CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op_1]], {{.*}}\n-// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remui %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remui %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n@@ -110,8 +110,8 @@ tt.func @matmul_loop(%lb : index, %ub : index, %step : index,\n // CHECK:     %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n // CHECK:     %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:     tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n-// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remui %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remui %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:     %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     triton_gpu.async_wait {num = 2 : i32}\n@@ -179,8 +179,8 @@ tt.func @matmul_loop_nested(%lb : index, %ub : index, %step : index,\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n // CHECK:   tt.dot {{.*}}, %[[arg_b0_dot_op]], {{.*}}\n-// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n-// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remui %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remui %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 1 : i32}\n // CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]"}, {"filename": "test/TritonGPU/reorder-instructions.mlir", "status": "modified", "additions": 29, "deletions": 12, "changes": 41, "file_content_changes": "@@ -9,24 +9,41 @@\n #mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n #shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [0, 1]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n-  tt.func public @convert_cannot_hoist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+  tt.func public @convert_cannot_hoist(%arg0: tensor<32x32x!tt.ptr<f32>, #blocked>) attributes {noinline = false} {\n     %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n     %cst_0 = arith.constant dense<1.230000e+02> : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n-    %0 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n-    %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<32x1xi32, #blocked>\n-    %2 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n-    %3 = tt.expand_dims %2 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x32xi32, #blocked>\n-    %4 = tt.broadcast %1 : (tensor<32x1xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n-    %5 = tt.broadcast %3 : (tensor<1x32xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n-    %6 = arith.addi %4, %5 : tensor<32x32xi32, #blocked>\n-    %7 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n-    %8 = tt.addptr %7, %6 : tensor<32x32x!tt.ptr<f32>, #blocked>, tensor<32x32xi32, #blocked>\n-    %9 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %9 = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n     %10 = triton_gpu.convert_layout %9 : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n     %11 = triton_gpu.convert_layout %10 : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n     %12 = tt.dot %11, %cst_0, %cst {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<32x32xf32, #mma>\n     %13 = triton_gpu.convert_layout %12 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n-    tt.store %8, %13 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf32, #blocked>\n+    tt.store %arg0, %13 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf32, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: sink_convert_idx_1\n+//       CHECK: triton_gpu.convert_layout %{{.*}} : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+//       CHECK: triton_gpu.convert_layout %{{.*}} : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+//       CHECK: tt.dot\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [0, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @sink_convert_idx_1(%arg0: tensor<32x32x!tt.ptr<f32>, #blocked>) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+    %B = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %BS = triton_gpu.convert_layout %B : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n+    %BD = triton_gpu.convert_layout %BS : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+    %cst_0 = arith.constant dense<1.230000e+02> : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+    %A = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %AS = triton_gpu.convert_layout %A : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n+    %AD = triton_gpu.convert_layout %AS : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+    %12 = tt.dot %AD, %BD, %cst {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<32x32xf32, #mma>\n+    %13 = triton_gpu.convert_layout %12 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    tt.store %arg0, %13 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf32, #blocked>\n     tt.return\n   }\n }"}]