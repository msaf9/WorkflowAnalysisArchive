[{"filename": "cmake/FindLLVM.cmake", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -25,7 +25,7 @@\n #  LLVM_VERSION_STRING - Full LLVM version string (e.g. 6.0.0svn).\n #  LLVM_VERSION_BASE_STRING - Base LLVM version string without git/svn suffix (e.g. 6.0.0).\n #\n-# Note: The variable names were chosen in conformance with the offical CMake\n+# Note: The variable names were chosen in conformance with the official CMake\n # guidelines, see ${CMAKE_ROOT}/Modules/readme.txt.\n \n # Try suffixed versions to pick up the newest LLVM install available on Debian\n@@ -196,4 +196,4 @@ include(FindPackageHandleStandardArgs)\n \n find_package_handle_standard_args(LLVM\n     REQUIRED_VARS LLVM_ROOT_DIR\n-    VERSION_VAR LLVM_VERSION_STRING)\n\\ No newline at end of file\n+    VERSION_VAR LLVM_VERSION_STRING)"}, {"filename": "docs/programming-guide/chapter-2/related-work.rst", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -14,7 +14,7 @@ Traditional compilers typically rely on intermediate representations, such as LL\n Program Representation\n +++++++++++++++++++++++\n \n-Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample litterature on linear and integer programming.\n+Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample literature on linear and integer programming.\n \n .. table::\n     :widths: 50 50"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -44,7 +44,7 @@ class PTXInstrExecution;\n //\n // builder.getAllMlirArgs() // get {pVal, iVal, jVal, kVal}\n //\n-// To get the string containing all the contraints with \",\" seperated,\n+// To get the string containing all the constraints with \",\" separated,\n // builder.getConstraints() // get \"=r,r,k\"\n //\n // PTXBuilder can build a PTX asm with multiple instructions, sample code:\n@@ -107,10 +107,10 @@ struct PTXBuilder {\n   // Create a new operand. It will not add to operand list.\n   // @value: the MLIR value bind to this operand.\n   // @constraint: ASM operand constraint, .e.g. \"=r\"\n-  // @formater: extra format to represent this operand in ASM code, default is\n-  //            \"%{0}\".format(operand.idx).\n+  // @formatter: extra format to represent this operand in ASM code, default is\n+  //             \"%{0}\".format(operand.idx).\n   Operand *newOperand(mlir::Value value, StringRef constraint,\n-                      std::function<std::string(int idx)> formater = nullptr);\n+                      std::function<std::string(int idx)> formatter = nullptr);\n \n   // Create a new operand which is written to, that is, the constraint starts\n   // with \"=\", e.g. \"=r\"."}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -20,10 +20,10 @@ std::string strJoin(llvm::ArrayRef<std::string> strs,\n \n PTXInstr::Operand *\n PTXBuilder::newOperand(mlir::Value value, StringRef constraint,\n-                       std::function<std::string(int)> formater) {\n+                       std::function<std::string(int)> formatter) {\n   argArchive.emplace_back(std::make_unique<Operand>(value, constraint));\n   auto *opr = argArchive.back().get();\n-  opr->repr = formater;\n+  opr->repr = formatter;\n   opr->idx = oprCounter++;\n   return opr;\n }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -25,7 +25,7 @@ class LoopPipeliner {\n   /// cache forOp we are working on\n   scf::ForOp forOp;\n \n-  /// cahce YieldOp for this forOp\n+  /// cache YieldOp for this forOp\n   scf::YieldOp yieldOp;\n \n   /// loads to be pipelined"}, {"filename": "lib/driver/llvm.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -103,7 +103,7 @@ static bool find_and_replace(std::string &str, const std::string &begin,\n std::string path_to_ptxas(int &version) {\n   std::vector<std::string> rets;\n   std::string ret;\n-  // search pathes for ptxas\n+  // search paths for ptxas\n   std::vector<std::string> ptxas_prefixes = {\"\", \"/usr/local/cuda/bin/\"};\n   std::string triton_ptxas = tools::getenv(\"TRITON_PTXAS_PATH\");\n   if (!triton_ptxas.empty())"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -222,7 +222,7 @@ void parse_args(py::list &args, py::list do_not_specialize,\n       // copy param\n       std::memcpy(params_ptr, &value, 8);\n       params_ptr += 8;\n-      // udpate cache key\n+      // update cache key\n       cache_key += dtype_cache_key_part(arg.attr(\"dtype\"));\n       cache_key += \"*\";\n       cache_key += \"[multipleof(\";\n@@ -323,7 +323,7 @@ void parse_args(py::list &args, py::list &arg_names, std::string &params,\n       // copy param\n       std::memcpy(params_ptr, &value, 8);\n       params_ptr += 8;\n-      // udpate cache key\n+      // update cache key\n       continue;\n     }\n     // argument is `constexpr`"}, {"filename": "python/tests/test_cast.py", "status": "added", "additions": 56, "deletions": 0, "changes": 56, "file_content_changes": "@@ -0,0 +1,56 @@\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def cast_check():\n+    zero_0d = tl.zeros([], dtype=tl.float32)\n+    zero_1d = tl.zeros([2], dtype=tl.float32)\n+    zero_2d_21 = tl.zeros([2, 1], dtype=tl.float32)\n+    zero_2d_22 = tl.zeros([2, 2], dtype=tl.float32)\n+\n+    # scalar + scalar -> scalar\n+    a0 = 0.0 + 0.0\n+    # scalar + 0D -> 0D\n+    a1 = 0.0 + zero_0d\n+    a2 = zero_0d + 0.0\n+    # scalar + 1D -> 1D\n+    a3 = 0.0 + zero_1d\n+    a4 = zero_1d + 0.0\n+    # scalar + 2D -> 2D\n+    a5 = 0.0 + zero_2d_22\n+    a6 = zero_2d_22 + 0.0\n+\n+    # 0D + 0D -> 0D\n+    b1 = zero_0d + zero_0d\n+    # 0D + 1D -> 1D\n+    b2 = zero_0d + zero_1d\n+    b3 = zero_1d + zero_0d\n+    # 0D + 2D -> 2D\n+    b4 = zero_0d + zero_2d_22\n+    b5 = zero_2d_22 + zero_0d\n+\n+    # 1D + 1D -> 1D\n+    c1 = zero_1d + zero_1d\n+    # 1D + 2D -> 2D\n+    c2 = zero_1d + zero_2d_21\n+    c3 = zero_1d + zero_2d_22\n+    c4 = zero_2d_21 + zero_1d\n+    c5 = zero_2d_22 + zero_1d\n+\n+    # 2D + 2D -> 2D\n+    d1 = zero_2d_21 + zero_2d_21\n+    d2 = zero_2d_22 + zero_2d_22\n+    d3 = zero_2d_21 + zero_2d_22\n+    d4 = zero_2d_22 + zero_2d_21\n+\n+    return a0, a1, a2, a3, a4, a5, a6, b1, b2, b3, b4, b5, c1, c2, c3, c4, c5, d1, d2, d3, d4\n+\n+\n+def test_cast_check():\n+    kernel = triton.compile(cast_check,\n+                            signature=\"\",\n+                            device=0,\n+                            output=\"ttir\")\n+    assert (kernel)\n+    # TODO: Check types of the results"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -53,7 +53,7 @@ def mangle_ty(ty):\n         elt = mangle_ty(ty.scalar)\n         shape = '_'.join(map(str, ty.shape))\n         return f'{elt}S{shape}S'\n-    assert False, \"Unsupport type\"\n+    assert False, \"Unsupported type\"\n \n \n def mangle_fn(name, arg_tys, constants):\n@@ -464,7 +464,7 @@ def visit_While(self, node):\n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n \n-            # condtion (the before region)\n+            # condition (the before region)\n             cond_block = self.builder.create_block()\n             self.builder.set_insertion_point_to_start(cond_block)\n             cond = self.visit(node.test)\n@@ -752,8 +752,11 @@ def make_triton_ir(fn, signature, constants=dict(), attributes=dict()):\n     # create kernel prototype\n     constants = {fn.arg_names.index(name): value for name, value in constants.items()}\n     attributes = {fn.arg_names.index(name): value for name, value in attributes.items()}\n-    arg_types = signature.replace(' ', '').split(',')\n-    arg_types = [str_to_ty(x) for x in arg_types]\n+    if signature.replace(' ', '') != '':\n+        arg_types = signature.replace(' ', '').split(',')\n+        arg_types = [str_to_ty(x) for x in arg_types]\n+    else:\n+        arg_types = []\n     prototype = triton.language.function_type([], arg_types)\n     # visit kernel AST\n     gscope = fn.__globals__.copy()"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -185,7 +185,7 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_float_ty()\n         elif self.name == 'fp64':\n             return builder.get_double_ty()\n-        raise ValueError(f'fail to covert {self} to ir type')\n+        raise ValueError(f'fail to convert {self} to ir type')\n \n     def __str__(self):\n         return self.name\n@@ -239,8 +239,9 @@ def __init__(self, element_ty: dtype, shape: List):\n \n         # Note that block_type's shape is a list of int\n         # while tensor's shape is a list of constexpr.\n-        assert shape\n-        if isinstance(shape[0], constexpr):\n+\n+        # shape can be empty ([]) when an input is a 0D tensor.\n+        if shape and isinstance(shape[0], constexpr):\n             shape = [s.value for s in shape]\n \n         self.shape = shape\n@@ -894,7 +895,7 @@ def where(condition, x, y, _builder=None):\n \n     Note that :code:`x` and :code:`y` are always evaluated regardless of the value of :code:`condition`.\n \n-    If you want to avoid unintented memory operations, use the :code:`mask` arguments in `triton.load` and `triton.store` instead.\n+    If you want to avoid unintended memory operations, use the :code:`mask` arguments in `triton.load` and `triton.store` instead.\n \n     The shape of :code:`x` and :code:`y` are both broadcast to the shape of :code:`condition`.\n     :code:`x` and :code:`y` must have the data type."}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 16, "deletions": 6, "changes": 22, "file_content_changes": "@@ -508,8 +508,21 @@ def broadcast_impl_value(lhs: tl.tensor,\n     elif lhs_ty.is_block() and rhs_ty.is_block():\n         lhs_shape = lhs_ty.get_block_shapes()\n         rhs_shape = rhs_ty.get_block_shapes()\n-        if len(lhs_shape) != len(rhs_shape):\n-            raise ValueError(\"Cannot make_shape_compatible: blocks must have the same rank\")\n+\n+        if len(lhs_shape) < len(rhs_shape):\n+            # Add new axes to lhs\n+            for dim in range(len(lhs_shape), len(rhs_shape)):\n+                lhs = tl.tensor(builder.create_expand_dims(lhs.handle, dim), tl.block_type(lhs_ty.scalar, lhs_shape + [1]))\n+                lhs_ty = lhs.type\n+                lhs_shape = lhs_ty.get_block_shapes()\n+        elif len(rhs_shape) < len(lhs_shape):\n+            # Add new axes to rhs\n+            for dim in range(len(rhs_shape), len(lhs_shape)):\n+                rhs = tl.tensor(builder.create_expand_dims(rhs.handle, dim), tl.block_type(rhs_ty.scalar, rhs_shape + [1]))\n+                rhs_ty = rhs.type\n+                rhs_shape = rhs_ty.get_block_shapes()\n+        assert len(rhs_shape) == len(lhs_shape)\n+\n         ret_shape = []\n         for i in range(len(lhs_shape)):\n             left = lhs_shape[i]\n@@ -962,10 +975,7 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n     for i, s in enumerate(shape):\n         if i != axis:\n             ret_shape.append(s)\n-    if len(ret_shape) == 0:\n-        res_ty = scalar_ty\n-    else:\n-        res_ty = tl.block_type(scalar_ty, ret_shape)\n+    res_ty = tl.block_type(scalar_ty, ret_shape)\n \n     if scalar_ty.is_floating():\n         return tl.tensor(builder.create_reduce(input.handle, FLOAT_OP, axis), res_ty)"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -328,7 +328,7 @@ def dsd_lut(layout, block, step, trans, device):\n     # create increments\n     incs = torch.stack((B_incs, A_incs), dim=1).view(-1).contiguous()\n     # pad by a factor 2*MAX_NUM_STAGES\n-    # to accomodate pre-fetching inside the kernel\n+    # to accommodate pre-fetching inside the kernel\n     pad = torch.zeros(20, device=incs.device, dtype=incs.dtype)\n     incs = torch.cat((incs, pad))\n     # create lut"}, {"filename": "python/triton/tools/disasm.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -104,7 +104,7 @@ def extract(file_path, fun):\n             # peek the next line\n             line = sass_lines[line_idx].decode()\n         # Print sass\n-        # label naming convension: LBB#i\n+        # label naming convention: LBB#i\n         for idx, (ctrl, asm) in enumerate(asm_buffer):\n             # Print label if this is BRA target\n             offset = idx * 16"}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -78,7 +78,7 @@ def softmax_kernel(\n     input_ptrs = row_start_ptr + col_offsets\n     # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n     row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n-    # Substract maximum for numerical stability\n+    # Subtract maximum for numerical stability\n     row_minus_max = row - tl.max(row, axis=0)\n     # Note that exponentials in Triton are fast but approximate (i.e., think __expf in CUDA)\n     numerator = tl.exp(row_minus_max)"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -18,7 +18,7 @@\n # They are notoriously hard to optimize, hence their implementation is generally done by\n # hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n # Unfortunately, these libraries are often proprietary and cannot be easily customized\n-# to accomodate the needs of modern deep learning workloads (e.g., fused activation functions).\n+# to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n # In this tutorial, you will learn how to implement efficient matrix multiplications by\n # yourself with Triton, in a way that is easy to customize and extend.\n #"}]