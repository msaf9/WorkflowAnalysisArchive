[{"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -2,6 +2,7 @@\n import torch\n \n import triton\n+import triton.ops\n \n \n def sparsify_tensor(x, mask, block):"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -2,6 +2,7 @@\n import torch\n \n import triton\n+import triton.ops\n \n \n @pytest.mark.parametrize(\"M, N, dtype, mode\","}, {"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -2,6 +2,7 @@\n import torch\n \n import triton\n+import triton.ops\n \n \n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -4,6 +4,7 @@\n import torch\n \n import triton\n+import triton.ops\n \n \n @pytest.mark.parametrize("}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,5 +1,4 @@\n import torch\n-\n import triton\n import triton.language as tl\n "}]