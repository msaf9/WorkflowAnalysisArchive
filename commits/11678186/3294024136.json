[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -108,8 +108,7 @@ def TT_LoadOp : TT_Op<\"load\",\n                        AttrSizedOperandSegments,\n                        MemoryEffects<[MemRead]>,\n                        TypesMatchWith<\"infer ptr type from result type\",\n-                                      \"result\", \"ptr\",\n-                                      \"getPointerTypeFromTensor($_self)\">,\n+                                      \"result\", \"ptr\", \"getPointerTypeSameShape($_self)\">,\n                        TypesMatchWith<\"infer mask type from result type or none\",\n                                       \"result\", \"mask\", \"getI1SameShape($_self)\",\n                                       \"($_op.getOperands().size() <= 1) || std::equal_to<>()\">,\n@@ -118,7 +117,7 @@ def TT_LoadOp : TT_Op<\"load\",\n                                       \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n     let summary = \"load\";\n \n-    let arguments = (ins TT_PtrTensor:$ptr, Optional<I1Tensor>:$mask, Optional<TT_Type>:$other,\n+    let arguments = (ins TT_PtrLike:$ptr, Optional<TT_BoolLike>:$mask, Optional<TT_Type>:$other,\n                          TT_CacheModifierAttr:$cache, TT_EvictionPolicyAttr:$evict,\n                          BoolAttr:$isVolatile);\n \n@@ -147,13 +146,13 @@ def TT_StoreOp : TT_Op<\"store\",\n                         MemoryEffects<[MemWrite]>,\n                         TypesMatchWith<\"infer ptr type from value type\",\n                                        \"value\", \"ptr\",\n-                                       \"getPointerTypeFromTensor($_self)\">,\n+                                       \"getPointerTypeSameShape($_self)\">,\n                         TypesMatchWith<\"infer mask type from value type\",\n                                        \"value\", \"mask\", \"getI1SameShape($_self)\",\n                                        \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n     let summary = \"store\";\n \n-    let arguments = (ins TT_PtrTensor:$ptr, TT_Type:$value, Optional<I1Tensor>:$mask);\n+    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$value, Optional<TT_BoolLike>:$mask);\n \n     let builders = [\n         OpBuilder<(ins \"Value\":$ptr, \"Value\":$value)>,\n@@ -318,7 +317,7 @@ def TT_ReduceOp : TT_Op<\"reduce\", [NoSideEffect,\n \n     let arguments = (ins TT_RedOpAttr:$redOp, TT_Tensor:$operand, I32Attr:$axis);\n \n-    let results = (outs TT_Tensor:$result);\n+    let results = (outs TT_Type:$result);\n \n     let builders = [\n         OpBuilder<(ins \"triton::RedOp\":$redOp, \"Value\":$operand, \"int\":$axis)>,"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 29, "deletions": 1, "changes": 30, "file_content_changes": "@@ -209,6 +209,33 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     }\n     curr = AxisInfo(contiguity, divisibility, constancy);\n   }\n+\n+  // CmpI\n+  if ((llvm::dyn_cast<arith::CmpIOp>(op) ||\n+       llvm::dyn_cast<triton::gpu::CmpIOp>(op)) &&\n+      op->getResult(0).getType().dyn_cast<TensorType>()) {\n+    auto resTy = op->getResult(0).getType().cast<TensorType>();\n+    short rank = resTy.getRank();\n+    auto lhsInfo = operands[0]->getValue();\n+    auto rhsInfo = operands[1]->getValue();\n+    auto shape = resTy.getShape();\n+\n+    AxisInfo::DimVectorT contiguity, divisibility, constancy;\n+    for (short d = 0; d < rank; ++d) {\n+      if (rhsInfo.getConstancy(d) % lhsInfo.getContiguity(d) == 0 ||\n+          rhsInfo.getConstancy(d) % lhsInfo.getConstancy(d))\n+        constancy.push_back(\n+            gcd(lhsInfo.getDivisibility(d), rhsInfo.getDivisibility(d)));\n+      else\n+        constancy.push_back(1);\n+\n+      divisibility.push_back(shape[d]);\n+      contiguity.push_back(1);\n+    }\n+\n+    curr = AxisInfo(contiguity, divisibility, constancy);\n+  }\n+\n   // UnrealizedConversionCast\n   // This is needed by TritonGPUToLLVM, to get AxisInfo when the graph is\n   // in the process of a PartialConversion, where UnrealizedConversionCast\n@@ -219,7 +246,8 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n   if (curr.getRank() == 0) {\n     return markAllPessimisticFixpoint(op->getResults());\n   }\n-  // join all latice elements\n+\n+  // join all lattice elements\n   ChangeResult result = ChangeResult::NoChange;\n   for (Value value : op->getResults()) {\n     result |= getLatticeElement(value).join(curr);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 225, "deletions": 224, "changes": 449, "file_content_changes": "@@ -759,6 +759,17 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n     return vec;\n   }\n \n+  unsigned getMaskAlignment(Value mask) const {\n+    auto maskOrder = mask.getType()\n+                         .cast<RankedTensorType>()\n+                         .getEncoding()\n+                         .cast<BlockedEncodingAttr>()\n+                         .getOrder();\n+\n+    auto maskAxis = getAxisInfo(mask);\n+    return std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n+  }\n+\n   llvm::Optional<AxisInfo> getAxisInfo(Value val) const {\n     if (auto it = AxisAnalysisPass.lookupLatticeElement(val)) {\n       return it->getValue();\n@@ -771,6 +782,208 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   AxisInfoAnalysis &AxisAnalysisPass;\n };\n \n+struct LoadOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::LoadOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LoadOpConversion(LLVMTypeConverter &converter,\n+                   AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>(converter, benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Value ptr = op.ptr();\n+    Value mask = op.mask();\n+    Value other = op.other();\n+\n+    Value llPtr = adaptor.ptr();\n+    Value llMask = adaptor.mask();\n+    Value llOther = adaptor.other();\n+\n+    auto loc = op->getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    if (!valueTy)\n+      return failure();\n+    Type valueElemTy =\n+        getTypeConverter()->convertType(valueTy.getElementType());\n+\n+    auto [layout, numElems] = getLayout(ptr);\n+\n+    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n+    assert(ptrElems.size() == numElems);\n+    // Determine the vectorization size\n+    size_t vec = getVectorizeSize(ptr, layout);\n+\n+    SmallVector<Value> maskElems;\n+    if (llMask) {\n+      unsigned maskAlignment = getMaskAlignment(mask);\n+      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n+      assert(ptrElems.size() == maskElems.size());\n+\n+      size_t maskAlign = getMaskAlignment(mask);\n+      vec = std::min(vec, maskAlign);\n+    }\n+\n+    const size_t dtsize =\n+        std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n+    const size_t valueElemNbits = dtsize * 8;\n+\n+    const int numVecs = numElems / vec;\n+\n+    // TODO: (goostavz) handle when other is const but not splat, which\n+    //       should be rarely seen\n+    bool otherIsSplatConstInt = false;\n+    DenseElementsAttr constAttr;\n+    int64_t splatVal = 0;\n+    if (valueElemTy.isa<IntegerType>() &&\n+        matchPattern(op.other(), m_Constant(&constAttr)) &&\n+        constAttr.isSplat()) {\n+      otherIsSplatConstInt = true;\n+      splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n+    }\n+\n+    auto otherElems = getLLVMElems(other, llOther, layout, rewriter, loc);\n+\n+    SmallVector<Value> loadedVals;\n+    for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n+      // TODO: optimization when ptr is GEP with constant offset\n+      size_t in_off = 0;\n+\n+      const int maxWordWidth = std::max<int>(32, valueElemNbits);\n+      const int totalWidth = valueElemNbits * vec;\n+      const int width = std::min(totalWidth, maxWordWidth);\n+      const int nWords = std::max(1, totalWidth / width);\n+      const int wordNElems = width / valueElemNbits;\n+      const int vecNElems = totalWidth / valueElemNbits;\n+      assert(wordNElems * nWords * numVecs == numElems);\n+\n+      // TODO(Superjomn) Add cache policy fields to StoreOp.\n+      // TODO(Superjomn) Deal with cache policy here.\n+      const bool hasL2EvictPolicy = false;\n+\n+      PTXBuilder ptxBuilder;\n+      auto &ld = *ptxBuilder.create<PTXIOInstr>(\"ld\");\n+\n+      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);\n+\n+      const std::string readConstraint =\n+          (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n+      const std::string writeConstraint =\n+          (width == 64) ? \"=l\" : ((width == 32) ? \"=r\" : \"=c\");\n+\n+      // prepare asm operands\n+      auto *dstsOpr = ptxBuilder.newListOperand();\n+      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n+        auto *opr = ptxBuilder.newOperand(writeConstraint); // =r operations\n+        dstsOpr->listAppend(opr);\n+      }\n+\n+      auto *addrOpr =\n+          ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n+\n+      // Define the instruction opcode\n+      ld.o(\"volatile\", op.isVolatile())\n+          .global()\n+          .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n+          .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n+          .o(\"L1::evict_first\",\n+             op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n+          .o(\"L1::evict_last\", op.evict() == triton::EvictionPolicy::EVICT_LAST)\n+          .o(\"L1::cache_hint\", hasL2EvictPolicy)\n+          .v(nWords)\n+          .b(width);\n+\n+      PTXBuilder::Operand *evictOpr{};\n+\n+      // Here lack a mlir::Value to bind to this operation, so disabled.\n+      // if (has_l2_evict_policy)\n+      //   evictOpr = ptxBuilder.newOperand(l2Evict, \"l\");\n+\n+      if (!evictOpr)\n+        ld(dstsOpr, addrOpr).predicate(pred, \"b\");\n+      else\n+        ld(dstsOpr, addrOpr, evictOpr).predicate(pred, \"b\");\n+\n+      if (other) {\n+        for (size_t ii = 0; ii < nWords; ++ii) {\n+          PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n+          mov.o(\"u\", width);\n+\n+          size_t size = width / valueElemNbits;\n+\n+          auto vecTy = LLVM::getFixedVectorType(valueElemTy, size);\n+          Value v = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n+          for (size_t s = 0; s < size; ++s) {\n+            Value falseVal = otherElems[vecStart + ii * size + s];\n+            Value sVal = createIndexAttrConstant(\n+                rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n+            v = insert_element(vecTy, v, falseVal, sVal);\n+          }\n+          v = bitcast(IntegerType::get(getContext(), width), v);\n+\n+          PTXInstr::Operand *opr{};\n+          if (otherIsSplatConstInt)\n+            opr = ptxBuilder.newConstantOperand(splatVal);\n+          else\n+            opr = ptxBuilder.newOperand(v, readConstraint);\n+\n+          mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n+        }\n+      }\n+\n+      // ---\n+      // create inline ASM signature\n+      // ---\n+      SmallVector<Type> retTys(nWords, IntegerType::get(getContext(), width));\n+      Type retTy = retTys.size() > 1\n+                       ? LLVM::LLVMStructType::getLiteral(getContext(), retTys)\n+                       : retTys[0];\n+\n+      // TODO: if (has_l2_evict_policy)\n+      auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n+                                                      LLVM::AsmDialect::AD_ATT);\n+      Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n+\n+      // ---\n+      // extract and store return values\n+      // ---\n+      SmallVector<Value> rets;\n+      for (unsigned int ii = 0; ii < nWords; ++ii) {\n+        Value curr;\n+        if (retTy.isa<LLVM::LLVMStructType>()) {\n+          curr = extract_val(IntegerType::get(getContext(), width), ret,\n+                             rewriter.getI64ArrayAttr(ii));\n+        } else {\n+          curr = ret;\n+        }\n+        curr = bitcast(\n+            LLVM::getFixedVectorType(valueElemTy, width / valueElemNbits),\n+            curr);\n+        rets.push_back(curr);\n+      }\n+      int tmp = width / valueElemNbits;\n+      for (size_t ii = 0; ii < vec; ++ii) {\n+        Value vecIdx = createIndexAttrConstant(\n+            rewriter, loc, this->getTypeConverter()->getIndexType(), ii % tmp);\n+        Value loaded = extract_element(valueElemTy, rets[ii / tmp], vecIdx);\n+        loadedVals.push_back(loaded);\n+      }\n+    } // end vec\n+\n+    Type llvmResultStructTy = getTypeConverter()->convertType(valueTy);\n+    Value resultStruct =\n+        getStructFromElements(loc, loadedVals, rewriter, llvmResultStructTy);\n+    rewriter.replaceOp(op, {resultStruct});\n+    return success();\n+  }\n+};\n+\n struct StoreOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::StoreOp>,\n       public LoadStoreConversionBase {\n@@ -814,14 +1027,8 @@ struct StoreOpConversion\n     if (llMask) {\n       maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n       assert(valueElems.size() == maskElems.size());\n-      auto maskOrder = mask.getType()\n-                           .cast<RankedTensorType>()\n-                           .getEncoding()\n-                           .cast<BlockedEncodingAttr>()\n-                           .getOrder();\n-\n-      auto maskAxis = getAxisInfo(mask);\n-      size_t maskAlign = std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n+\n+      size_t maskAlign = getMaskAlignment(mask);\n       vec = std::min(vec, maskAlign);\n     }\n \n@@ -846,15 +1053,10 @@ struct StoreOpConversion\n       // TODO(Superjomn) Deal with cache policy here.\n       const bool hasL2EvictPolicy = false;\n \n-      PTXBuilder ptxBuilder;\n-      auto &ptxStoreInstr = *ptxBuilder.create<PTXIOInstr>(\"st\");\n-\n-      llvm::SmallVector<std::string> asmArgs;\n-\n       Type valArgTy = IntegerType::get(ctx, width);\n       auto wordTy = vec_ty(valueElemTy, wordNElems);\n \n-      auto *asmArgList = ptxBuilder.newListOperand();\n+      SmallVector<std::pair<Value, std::string>> asmArgs;\n       for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n         // llWord is a width-len composition\n         Value llWord = rewriter.create<LLVM::UndefOp>(loc, wordTy);\n@@ -876,23 +1078,25 @@ struct StoreOpConversion\n         llWord = bitcast(valArgTy, llWord);\n         std::string constraint =\n             (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n-        asmArgList->listAppend(ptxBuilder.newOperand(llWord, constraint));\n+        asmArgs.emplace_back(llWord, constraint);\n       }\n \n-      // TODO(Superjomn) Need to check masks before vectorize the load for\n-      // the values share one predicate? Here assume all the mask values are\n-      // the same.\n+      // Prepare the PTX inline asm.\n+      PTXBuilder ptxBuilder;\n+      auto *asmArgList = ptxBuilder.newListOperand(asmArgs);\n+\n       Value maskVal = llMask ? maskElems[vecStart] : int_val(1, 1);\n-      ptxStoreInstr.global().b(width).v(nWords);\n \n       auto *asmAddr =\n           ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n \n+      auto &ptxStoreInstr =\n+          ptxBuilder.create<PTXIOInstr>(\"st\")->global().b(width).v(nWords);\n       ptxStoreInstr(asmAddr, asmArgList).predicate(maskVal, \"b\");\n+\n       Type boolTy = getTypeConverter()->convertType(rewriter.getIntegerType(1));\n       llvm::SmallVector<Type> argTys({boolTy, ptr.getType()});\n-      for (int i = 0; i < nWords; ++i)\n-        argTys.push_back(valArgTy);\n+      argTys.insert(argTys.end(), nWords, valArgTy);\n \n       auto ASMReturnTy = LLVM::LLVMVoidType::get(ctx);\n \n@@ -1065,209 +1269,6 @@ struct MakeRangeOpConversion\n   }\n };\n \n-struct LoadOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>,\n-      public LoadStoreConversionBase {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::LoadOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  LoadOpConversion(LLVMTypeConverter &converter,\n-                   AxisInfoAnalysis &axisAnalysisPass, PatternBenefit benefit)\n-      : ConvertTritonGPUOpToLLVMPattern<triton::LoadOp>(converter, benefit),\n-        LoadStoreConversionBase(axisAnalysisPass) {}\n-\n-  LogicalResult\n-  matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    Value ptr = op.ptr();\n-    Value mask = op.mask();\n-    Value other = op.other();\n-\n-    Value llPtr = adaptor.ptr();\n-    Value llMask = adaptor.mask();\n-    Value llOther = adaptor.other();\n-\n-    auto loc = op->getLoc();\n-    MLIRContext *ctx = rewriter.getContext();\n-\n-    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n-    if (!valueTy)\n-      return failure();\n-    Type valueElemTy =\n-        getTypeConverter()->convertType(valueTy.getElementType());\n-\n-    auto [layout, numElems] = getLayout(ptr);\n-\n-    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n-    assert(ptrElems.size() == numElems);\n-\n-    SmallVector<Value> maskElems;\n-    if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n-      assert(ptrElems.size() == maskElems.size());\n-    }\n-\n-    // Determine the vectorization size\n-    size_t vec = getVectorizeSize(ptr, layout);\n-\n-    const size_t dtsize =\n-        std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n-    const size_t valueElemNbits = dtsize * 8;\n-\n-    const int numVecs = numElems / vec;\n-\n-    // TODO: (goostavz) handle when other is const but not splat, which\n-    //       should be rarely seen\n-    bool otherIsSplatConstInt = false;\n-    DenseElementsAttr constAttr;\n-    int64_t splatVal = 0;\n-    if (valueElemTy.isa<IntegerType>() &&\n-        matchPattern(op.other(), m_Constant(&constAttr)) &&\n-        constAttr.isSplat()) {\n-      otherIsSplatConstInt = true;\n-      splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n-    }\n-\n-    auto otherElems = getLLVMElems(other, llOther, layout, rewriter, loc);\n-\n-    SmallVector<Value> loadedVals;\n-    for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n-      // TODO: optimization when ptr is GEP with constant offset\n-      size_t in_off = 0;\n-\n-      const int maxWordWidth = std::max<int>(32, valueElemNbits);\n-      const int totalWidth = valueElemNbits * vec;\n-      const int width = std::min(totalWidth, maxWordWidth);\n-      const int nWords = std::max(1, totalWidth / width);\n-      const int wordNElems = width / valueElemNbits;\n-      const int vecNElems = totalWidth / valueElemNbits;\n-      assert(wordNElems * nWords * numVecs == numElems);\n-\n-      // TODO(Superjomn) Add cache policy fields to StoreOp.\n-      // TODO(Superjomn) Deal with cache policy here.\n-      const bool hasL2EvictPolicy = false;\n-\n-      PTXBuilder ptxBuilder;\n-      auto &ld = *ptxBuilder.create<PTXIOInstr>(\"ld\");\n-\n-      // TODO(Superjomn) Need to check masks before vectorize the load for all\n-      // the values share one predicate? Here assume all the mask values are\n-      // the same.\n-      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);\n-\n-      const std::string readConstraint =\n-          (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n-      const std::string writeConstraint =\n-          (width == 64) ? \"=l\" : ((width == 32) ? \"=r\" : \"=c\");\n-\n-      // prepare asm operands\n-      auto *dstsOpr = ptxBuilder.newListOperand();\n-      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n-        auto *opr = ptxBuilder.newOperand(writeConstraint); // =r operations\n-        dstsOpr->listAppend(opr);\n-      }\n-\n-      auto *addrOpr =\n-          ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n-\n-      // Define the instruction opcode\n-      ld.o(\"volatile\", op.isVolatile())\n-          .global()\n-          .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n-          .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n-          .o(\"L1::evict_first\",\n-             op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n-          .o(\"L1::evict_last\", op.evict() == triton::EvictionPolicy::EVICT_LAST)\n-          .o(\"L1::cache_hint\", hasL2EvictPolicy)\n-          .v(nWords)\n-          .b(width);\n-\n-      PTXBuilder::Operand *evictOpr{};\n-\n-      // Here lack a mlir::Value to bind to this operation, so disabled.\n-      // if (has_l2_evict_policy)\n-      //   evictOpr = ptxBuilder.newOperand(l2Evict, \"l\");\n-\n-      if (!evictOpr)\n-        ld(dstsOpr, addrOpr).predicate(pred, \"b\");\n-      else\n-        ld(dstsOpr, addrOpr, evictOpr).predicate(pred, \"b\");\n-\n-      if (other) {\n-        for (size_t ii = 0; ii < nWords; ++ii) {\n-          PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n-          mov.o(\"u\", width);\n-\n-          size_t size = width / valueElemNbits;\n-\n-          auto vecTy = LLVM::getFixedVectorType(valueElemTy, size);\n-          Value v = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-          for (size_t s = 0; s < size; ++s) {\n-            Value falseVal = otherElems[vecStart + ii * size + s];\n-            Value sVal = createIndexAttrConstant(\n-                rewriter, loc, this->getTypeConverter()->getIndexType(), s);\n-            v = insert_element(vecTy, v, falseVal, sVal);\n-          }\n-          v = bitcast(IntegerType::get(getContext(), width), v);\n-\n-          PTXInstr::Operand *opr{};\n-          if (otherIsSplatConstInt) {\n-            opr = ptxBuilder.newConstantOperand(splatVal);\n-          } else {\n-            opr = ptxBuilder.newOperand(v, readConstraint);\n-          }\n-\n-          mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n-        }\n-      }\n-\n-      // ---\n-      // create inline ASM signature\n-      // ---\n-      SmallVector<Type> retTys(nWords, IntegerType::get(getContext(), width));\n-      Type retTy = retTys.size() > 1\n-                       ? LLVM::LLVMStructType::getLiteral(getContext(), retTys)\n-                       : retTys[0];\n-\n-      // TODO: if (has_l2_evict_policy)\n-      auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n-                                                      LLVM::AsmDialect::AD_ATT);\n-      Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n-\n-      // ---\n-      // extract and store return values\n-      // ---\n-      SmallVector<Value> rets;\n-      for (unsigned int ii = 0; ii < nWords; ++ii) {\n-        Value curr;\n-        if (retTy.isa<LLVM::LLVMStructType>()) {\n-          curr = extract_val(IntegerType::get(getContext(), width), ret,\n-                             rewriter.getI64ArrayAttr(ii));\n-        } else {\n-          curr = ret;\n-        }\n-        curr = bitcast(\n-            LLVM::getFixedVectorType(valueElemTy, width / valueElemNbits),\n-            curr);\n-        rets.push_back(curr);\n-      }\n-      int tmp = width / valueElemNbits;\n-      for (size_t ii = 0; ii < vec; ++ii) {\n-        Value vecIdx = createIndexAttrConstant(\n-            rewriter, loc, this->getTypeConverter()->getIndexType(), ii % tmp);\n-        Value loaded = extract_element(valueElemTy, rets[ii / tmp], vecIdx);\n-        loadedVals.push_back(loaded);\n-      }\n-    } // end vec\n-\n-    Type llvmResultStructTy = getTypeConverter()->convertType(valueTy);\n-    Value resultStruct =\n-        getStructFromElements(loc, loadedVals, rewriter, llvmResultStructTy);\n-    rewriter.replaceOp(op, {resultStruct});\n-    return success();\n-  }\n-};\n-\n struct GetProgramIdOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::GetProgramIdOp> {\n   using ConvertTritonGPUOpToLLVMPattern<"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 28, "deletions": 20, "changes": 48, "file_content_changes": "@@ -26,14 +26,15 @@ static Type getI32SameShape(Type type) {\n   return i32Type;\n }\n \n-static Type getPointerTypeFromTensor(Type type) {\n+static Type getPointerTypeSameShape(Type type) {\n   if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n     Type elementType = tensorType.getElementType();\n     auto shape = tensorType.getShape();\n     PointerType ptrType = PointerType::get(elementType, 1);\n     return RankedTensorType::get(shape, ptrType, tensorType.getEncoding());\n+  } else {\n+    return PointerType::get(type, 1);\n   }\n-  return Type();\n }\n \n // Parser & printer for assembly forms\n@@ -49,7 +50,7 @@ ParseResult parseLoadOp(OpAsmParser &parser, OperationState &result) {\n   result.addTypes(resultTypes);\n \n   SmallVector<Type> operandTypes;\n-  operandTypes.push_back(getPointerTypeFromTensor(resultTypes[0])); // ptr\n+  operandTypes.push_back(getPointerTypeSameShape(resultTypes[0])); // ptr\n   int hasMask = 0, hasOther = 0;\n   if (allOperands.size() >= 2) {\n     operandTypes.push_back(getI1SameShape(resultTypes[0])); // mask\n@@ -92,8 +93,8 @@ ParseResult parseStoreOp(OpAsmParser &parser, OperationState &result) {\n     return failure();\n \n   SmallVector<Type> operandTypes;\n-  operandTypes.push_back(getPointerTypeFromTensor(valueType)); // ptr\n-  operandTypes.push_back(valueType);                           // value\n+  operandTypes.push_back(getPointerTypeSameShape(valueType)); // ptr\n+  operandTypes.push_back(valueType);                          // value\n   if (allOperands.size() >= 3)\n     operandTypes.push_back(getI1SameShape(valueType)); // mask\n \n@@ -194,26 +195,33 @@ mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n   // infer shape\n   Value arg = operands[0];\n   auto argTy = arg.getType().cast<RankedTensorType>();\n+  auto argEltTy = argTy.getElementType();\n   auto retShape = argTy.getShape().vec();\n   int axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n   retShape.erase(retShape.begin() + axis);\n-  // infer encoding\n-  Attribute argEncoding = argTy.getEncoding();\n-  Attribute retEncoding;\n-  if (argEncoding) {\n-    Dialect &dialect = argEncoding.getDialect();\n-    auto inferLayoutInterface = dyn_cast<DialectInferLayoutInterface>(&dialect);\n-    if (inferLayoutInterface\n-            ->inferReduceOpEncoding(argEncoding, axis, retEncoding)\n-            .failed()) {\n-      llvm::report_fatal_error(\"failed to infer layout for ReduceOp\");\n-      return mlir::failure();\n+  if (retShape.empty()) {\n+    // 0d-tensor -> scalar\n+    inferredReturnTypes.push_back(argEltTy);\n+  } else {\n+    // nd-tensor where n >= 1\n+    // infer encoding\n+    Attribute argEncoding = argTy.getEncoding();\n+    Attribute retEncoding;\n+    if (argEncoding) {\n+      Dialect &dialect = argEncoding.getDialect();\n+      auto inferLayoutInterface =\n+          dyn_cast<DialectInferLayoutInterface>(&dialect);\n+      if (inferLayoutInterface\n+              ->inferReduceOpEncoding(argEncoding, axis, retEncoding)\n+              .failed()) {\n+        llvm::report_fatal_error(\"failed to infer layout for ReduceOp\");\n+        return mlir::failure();\n+      }\n     }\n+    // create type\n+    inferredReturnTypes.push_back(\n+        RankedTensorType::get(retShape, argEltTy, retEncoding));\n   }\n-  // create type\n-  auto argEltTy = argTy.getElementType();\n-  inferredReturnTypes.push_back(\n-      RankedTensorType::get(retShape, argEltTy, retEncoding));\n   return mlir::success();\n }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include <numeric>\n@@ -23,6 +24,11 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     std::sort(order.begin(), order.end(), [&](unsigned x, unsigned y) {\n       return contiguity[x] > contiguity[y];\n     });\n+\n+    int numElems = product(origType.getShape());\n+    int numThreads = numWarps * 32;\n+    int numElemsPerThread = std::max(numElems / numThreads, 1);\n+\n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n     PointerType ptrType = origType.getElementType().cast<PointerType>();\n@@ -31,7 +37,8 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     unsigned maxContig = info.getContiguity(order[0]);\n     unsigned alignment = std::min(maxMultiple, maxContig);\n     unsigned perThread = std::min(alignment, 128 / numBits);\n-    sizePerThread[order[0]] = perThread;\n+    sizePerThread[order[0]] = std::min<int>(perThread, numElemsPerThread);\n+\n     SmallVector<unsigned> dims(rank);\n     std::iota(dims.begin(), dims.end(), 0);\n     // create encoding"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -1112,8 +1112,11 @@ void init_triton_ir(py::module &&m) {\n                  operand.getType().dyn_cast<mlir::RankedTensorType>();\n              std::vector<int64_t> shape = inputTensorType.getShape();\n              shape.erase(shape.begin() + axis);\n-             auto resType = mlir::RankedTensorType::get(\n-                 shape, inputTensorType.getElementType());\n+             mlir::Type resType = inputTensorType.getElementType();\n+             if (!shape.empty()) {\n+               resType = mlir::RankedTensorType::get(\n+                   shape, inputTensorType.getElementType());\n+             }\n              return self.create<mlir::triton::ReduceOp>(loc, resType, redOp,\n                                                         operand, axis);\n            })"}, {"filename": "python/tests/test_cast.py", "status": "removed", "additions": 0, "deletions": 57, "changes": 57, "file_content_changes": "@@ -1,57 +0,0 @@\n-import triton\n-import triton.language as tl\n-\n-\n-# TODO: function with no arguments don't work\n-@triton.jit\n-def cast_check(X):\n-    zero_0d = tl.zeros([], dtype=tl.float32)\n-    zero_1d = tl.zeros([2], dtype=tl.float32)\n-    zero_2d_21 = tl.zeros([2, 1], dtype=tl.float32)\n-    zero_2d_22 = tl.zeros([2, 2], dtype=tl.float32)\n-\n-    # scalar + scalar -> scalar\n-    a0 = 0.0 + 0.0\n-    # scalar + 0D -> 0D\n-    a1 = 0.0 + zero_0d\n-    a2 = zero_0d + 0.0\n-    # scalar + 1D -> 1D\n-    a3 = 0.0 + zero_1d\n-    a4 = zero_1d + 0.0\n-    # scalar + 2D -> 2D\n-    a5 = 0.0 + zero_2d_22\n-    a6 = zero_2d_22 + 0.0\n-\n-    # 0D + 0D -> 0D\n-    b1 = zero_0d + zero_0d\n-    # 0D + 1D -> 1D\n-    b2 = zero_0d + zero_1d\n-    b3 = zero_1d + zero_0d\n-    # 0D + 2D -> 2D\n-    b4 = zero_0d + zero_2d_22\n-    b5 = zero_2d_22 + zero_0d\n-\n-    # 1D + 1D -> 1D\n-    c1 = zero_1d + zero_1d\n-    # 1D + 2D -> 2D\n-    c2 = zero_1d + zero_2d_21\n-    c3 = zero_1d + zero_2d_22\n-    c4 = zero_2d_21 + zero_1d\n-    c5 = zero_2d_22 + zero_1d\n-\n-    # 2D + 2D -> 2D\n-    d1 = zero_2d_21 + zero_2d_21\n-    d2 = zero_2d_22 + zero_2d_22\n-    d3 = zero_2d_21 + zero_2d_22\n-    d4 = zero_2d_22 + zero_2d_21\n-\n-    return a0, a1, a2, a3, a4, a5, a6, b1, b2, b3, b4, b5, c1, c2, c3, c4, c5, d1, d2, d3, d4\n-\n-\n-def test_cast_check():\n-    kernel = triton.compiler._compile(cast_check,\n-                                      signature=\"*fp32\",\n-                                      device=0,\n-                                      output=\"ttgir\")\n-    assert (kernel)\n-    # TODO: Check types of the results"}, {"filename": "python/tests/test_type.py", "status": "added", "additions": 80, "deletions": 0, "changes": 80, "file_content_changes": "@@ -0,0 +1,80 @@\n+import triton\n+import triton.language as tl\n+\n+\n+# TODO: function with no arguments don't work\n+@triton.jit\n+def binop_type_check(X):\n+    # 0d-tensor is not allowed.\n+    # zero_0d = tl.zeros([], dtype=tl.float32)\n+    zero_1d = tl.zeros([2], dtype=tl.float32)\n+    zero_2d_21 = tl.zeros([2, 1], dtype=tl.float32)\n+    zero_2d_22 = tl.zeros([2, 2], dtype=tl.float32)\n+\n+    # scalar + scalar -> scalar\n+    a0 = 0.0 + 0.0\n+    # # scalar + 0D -> 0D\n+    # a1 = 0.0 + zero_0d\n+    # a2 = zero_0d + 0.0\n+    # scalar + 1D -> 1D\n+    a3 = 0.0 + zero_1d\n+    a4 = zero_1d + 0.0\n+    # scalar + 2D -> 2D\n+    a5 = 0.0 + zero_2d_22\n+    a6 = zero_2d_22 + 0.0\n+\n+    # # 0D + 0D -> 0D\n+    # b1 = zero_0d + zero_0d\n+    # # 0D + 1D -> 1D\n+    # b2 = zero_0d + zero_1d\n+    # b3 = zero_1d + zero_0d\n+    # # 0D + 2D -> 2D\n+    # b4 = zero_0d + zero_2d_22\n+    # b5 = zero_2d_22 + zero_0d\n+\n+    # 1D + 1D -> 1D\n+    c1 = zero_1d + zero_1d\n+    # 1D + 2D -> 2D\n+    c2 = zero_1d + zero_2d_21\n+    c3 = zero_1d + zero_2d_22\n+    c4 = zero_2d_21 + zero_1d\n+    c5 = zero_2d_22 + zero_1d\n+\n+    # 2D + 2D -> 2D\n+    d1 = zero_2d_21 + zero_2d_21\n+    d2 = zero_2d_22 + zero_2d_22\n+    d3 = zero_2d_21 + zero_2d_22\n+    d4 = zero_2d_22 + zero_2d_21\n+\n+    # return a0, a1, a2, a3, a4, a5, a6, b1, b2, b3, b4, b5, c1, c2, c3, c4, c5, d1, d2, d3, d4\n+    return a0, a3, a4, a5, a6, c1, c2, c3, c4, c5, d1, d2, d3, d4\n+\n+\n+def test_binop_type_check():\n+    kernel = triton.compiler._compile(binop_type_check,\n+                                      signature=\"*fp32\",\n+                                      device=0,\n+                                      output=\"ttgir\")\n+    assert (kernel)\n+    # TODO: Check types of the results\n+\n+\n+@triton.jit\n+def reduce_type_check(ptr):\n+    v_32 = tl.load(ptr + tl.arange(0, 32))\n+    v_scalar = tl.min(v_32, axis=0)\n+    tl.store(ptr, v_scalar)\n+    v_64x128 = tl.load(ptr + tl.arange(0, 64)[:, None] + tl.arange(0, 128)[None, :])\n+    v_64 = tl.max(v_64x128, axis=1)\n+    tl.store(ptr + tl.arange(0, 64), v_64)\n+    v_128 = tl.max(v_64x128, axis=0)\n+    tl.store(ptr + tl.arange(0, 128), v_128)\n+\n+\n+def test_reduce_type_check():\n+    kernel = triton.compiler._compile(reduce_type_check,\n+                                      signature=\"*fp32\",\n+                                      device=0,\n+                                      output=\"ttgir\")\n+    assert (kernel)\n+    # TODO: Check types of the results"}, {"filename": "python/tests/test_vecadd.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -188,7 +188,7 @@ def test_vecadd_no_scf(num_warps, block_size, shape):\n     [2, 256, (3, 256 + 7)],\n     [4, 256, (3, 256 + 7)],\n ])\n-def test_vecadd__no_scf_masked(num_warps, block_size, shape):\n+def test_vecadd_no_scf_masked(num_warps, block_size, shape):\n     vecadd_no_scf_tester(num_warps, block_size, shape)\n \n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -241,7 +241,9 @@ def __init__(self, element_ty: dtype, shape: List):\n         # while tensor's shape is a list of constexpr.\n \n         # shape can be empty ([]) when an input is a 0D tensor.\n-        if shape and isinstance(shape[0], constexpr):\n+        if not shape:\n+            raise TypeError('0d block_type is forbidden')\n+        if isinstance(shape[0], constexpr):\n             shape = [s.value for s in shape]\n \n         self.shape = shape"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -993,7 +993,11 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n     for i, s in enumerate(shape):\n         if i != axis:\n             ret_shape.append(s)\n-    res_ty = tl.block_type(scalar_ty, ret_shape)\n+    if ret_shape:\n+        res_ty = tl.block_type(scalar_ty, ret_shape)\n+    else:\n+        # 0d-tensor -> scalar\n+        res_ty = scalar_ty\n \n     if scalar_ty.is_floating():\n         return tl.tensor(builder.create_reduce(input.handle, FLOAT_OP, axis), res_ty)"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 89, "deletions": 0, "changes": 89, "file_content_changes": "@@ -50,3 +50,92 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n   tt.store %19, %20, %cst : tensor<128x128xf32>\n   return\n }\n+\n+// -----\n+\n+module {\n+\n+// This is a tiny test for verifying StoreOp-related alignment, It simply store a constant to a buffer.\n+func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n: i32 {tt.divisibility = 16 : i32}) {\n+  // CHECK: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n+  %pid = tt.get_program_id {axis = 0 : i32} : i32\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1]\n+  %c128_i32 = arith.constant 128 : i32\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [1]\n+  %1 = arith.muli %pid, %c128_i32 : i32\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [65536] ; Constancy: [1]\n+  %2 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+ // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [128]\n+  %3 = tt.splat %1 : (i32) -> tensor<128xi32>\n+ // CHECK-NEXT: Contiguity: [128] ; Divisibility: [128] ; Constancy: [1]\n+  %4 = arith.addi %3, %2 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n+  %5 = tt.splat %addr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+  // CHECK-NEXT: Contiguity: [128] ; Divisibility: [16] ; Constancy: [1]\n+  %6 = tt.addptr %5, %4 : tensor<128x!tt.ptr<f32>>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n+  %9 = tt.splat %n : (i32) -> tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [16]\n+  %mask = arith.cmpi slt, %4, %9 : tensor<128xi32>\n+  // CHECK-NEXT: Contiguity: [1] ; Divisibility: [1] ; Constancy: [1]\n+  %cst = arith.constant dense<0.0> : tensor<128xf32>\n+  tt.store %5, %cst, %mask : tensor<128xf32>\n+  return\n+}\n+\n+}\n+\n+// -----\n+\n+// This IR is dumped from vecadd test.\n+// Note, the hint {tt.divisibility = 16 : i32} for %n_elements affects the alignment of mask.\n+func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32 {tt.divisibility = 16 : i32}) {\n+  %c64_i32 = arith.constant 64 : i32\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c64_i32 : i32\n+  %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+  %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n+  %4 = arith.addi %3, %2 : tensor<64xi32>\n+  %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>\n+  %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>\n+  %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n+  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [16] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  %mask = arith.cmpi slt, %4, %9 : tensor<64xi32>\n+  %11 = tt.load %6, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %12 = tt.load %8, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %13 = arith.addf %11, %12 : tensor<64xf32>\n+  %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>> )\n+  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>\n+  tt.store %15, %13, %mask : tensor<64xf32>\n+  return\n+}\n+\n+// -----\n+\n+// This IR is dumped from vecadd test.\n+// Note, there is no divisibility hint for %n_elements, Triton should assume its divisibility to be 1 by default.\n+func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+  %c64_i32 = arith.constant 64 : i32\n+  %0 = tt.get_program_id {axis = 0 : i32} : i32\n+  %1 = arith.muli %0, %c64_i32 : i32\n+  %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+  %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n+  %4 = arith.addi %3, %2 : tensor<64xi32>\n+  %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>\n+  %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>\n+  %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n+  // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [1] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n+  %10 = arith.cmpi slt, %4, %9 : tensor<64xi32>\n+  %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n+  %13 = arith.addf %11, %12 : tensor<64xf32>\n+  %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n+  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>\n+  tt.store %15, %13, %10 : tensor<64xf32>\n+  return\n+}"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 77, "deletions": 0, "changes": 77, "file_content_changes": "@@ -53,3 +53,80 @@ func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n   %2 = tt.addptr %tensor_ptr_1d, %tensor_i32_1d : tensor<16x!tt.ptr<f32>>\n   return\n }\n+\n+func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %mask : i1) {\n+  // Test if Load/Store ops can handle scalar values\n+  %other = arith.constant 0.0e+0 : f32\n+\n+  // load scalar\n+  // CHECK: %[[L0:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : f32\n+  %a = tt.load %ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : f32\n+  // CHECK: %[[L1:.*]] = tt.load %{{.*}}, %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : f32\n+  %b = tt.load %ptr, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : f32\n+  // CHECK: %[[L2:.*]] = tt.load %{{.*}}, %{{.*}}, %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : f32\n+  %c = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : f32\n+\n+  // store scalar\n+  // CHECK: tt.store %{{.*}}, %[[L0]] : f32\n+  tt.store %ptr, %a : f32\n+  // CHECK: tt.store %{{.*}}, %[[L1]], %{{.*}} : f32\n+  tt.store %ptr, %b, %mask : f32\n+  // CHECK: tt.store %{{.*}}, %[[L2]], %{{.*}} : f32\n+  tt.store %ptr, %c, %mask : f32\n+  return\n+}\n+\n+func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n+  // Test if reduce ops infer types correctly\n+\n+  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<2x4xf32>\n+  %a = tt.reduce %v {redOp = 1 : i32, axis = 0 : i32} : tensor<1x2x4xf32> -> tensor<2x4xf32>\n+  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<1x4xf32>\n+  %b = tt.reduce %v {redOp = 1 : i32, axis = 1 : i32} : tensor<1x2x4xf32> -> tensor<1x4xf32>\n+  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<1x2xf32>\n+  %c = tt.reduce %v {redOp = 1 : i32, axis = 2 : i32} : tensor<1x2x4xf32> -> tensor<1x2xf32>\n+  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<1xf32>\n+  %e = tt.reduce %b {redOp = 1 : i32, axis = 1 : i32} : tensor<1x4xf32> -> tensor<1xf32>\n+  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<4xf32>\n+  %f = tt.reduce %a {redOp = 1 : i32, axis = 0 : i32} : tensor<2x4xf32> -> tensor<4xf32>\n+  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> f32\n+  %g = tt.reduce %f {redOp = 1 : i32, axis = 0 : i32} : tensor<4xf32> -> f32\n+\n+  // Avoid optimizations for c, e, and g\n+  %ptr1x2 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<1x2x!tt.ptr<f32>>\n+  %ptr1 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<1x!tt.ptr<f32>>\n+  tt.store %ptr1x2, %c : tensor<1x2xf32>\n+  tt.store %ptr1, %e : tensor<1xf32>\n+  tt.store %ptr, %g : f32\n+  return\n+}\n+\n+func @dot_ops_infer(%ptr: !tt.ptr<f32>, %v : f32) {\n+  // Test if reduce ops infer types correctly\n+  %v128x32 = tt.splat %v : (f32) -> tensor<128x32xf32>\n+  %v32x128 = tt.splat %v : (f32) -> tensor<32x128xf32>\n+  %v128x1 = tt.splat %v : (f32) -> tensor<128x1xf32>\n+  %v1x128 = tt.splat %v : (f32) -> tensor<1x128xf32>\n+\n+  %zero128x128 = arith.constant dense<0.00e+00> : tensor<128x128xf32>\n+  %zero32x32 = arith.constant dense<0.00e+00> : tensor<32x32xf32>\n+  %zero1x1 = arith.constant dense<0.00e+00> : tensor<1x1xf32>\n+\n+  // CHECK: %{{.*}} = tt.dot %{{.*}} -> tensor<128x128xf32>\n+  %r1 = tt.dot %v128x32, %v32x128, %zero128x128 {allowTF32 = true} : tensor<128x32xf32> * tensor<32x128xf32> -> tensor<128x128xf32>\n+  // CHECK: %{{.*}} = tt.dot %{{.*}} -> tensor<32x32xf32>\n+  %r2 = tt.dot %v32x128, %v128x32, %zero32x32 {allowTF32 = true} : tensor<32x128xf32> * tensor<128x32xf32> -> tensor<32x32xf32>\n+  // CHECK: %{{.*}} = tt.dot %{{.*}} -> tensor<128x128xf32>\n+  %r3 = tt.dot %v128x1, %v1x128, %zero128x128 {allowTF32 = true} : tensor<128x1xf32> * tensor<1x128xf32> -> tensor<128x128xf32>\n+  // CHECK: %{{.*}} = tt.dot %{{.*}} -> tensor<1x1xf32>\n+  %r4 = tt.dot %v1x128, %v128x1, %zero1x1 {allowTF32 = true} : tensor<1x128xf32> * tensor<128x1xf32> -> tensor<1x1xf32>\n+\n+  %ptr128x128 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x128x!tt.ptr<f32>>\n+  %ptr32x32 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>>\n+  %ptr1x1 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<1x1x!tt.ptr<f32>>\n+  tt.store %ptr128x128, %r1 : tensor<128x128xf32>\n+  tt.store %ptr32x32, %r2 : tensor<32x32xf32>\n+  tt.store %ptr128x128, %r3 : tensor<128x128xf32>\n+  tt.store %ptr1x1, %r4 : tensor<1x1xf32>\n+  return\n+}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 32, "deletions": 1, "changes": 33, "file_content_changes": "@@ -161,6 +161,37 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n+// This test verifies the vectorization of Load and Store Ops.\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n+// Note, the %n_elements doesn't have a \"tt.divisibility\" hint, so Triton assumes it's divisibility is 1, this should effect the mask's alignment and further restrict the load/store ops' vector width to be 1.\n+module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n+  func @vecadd_masked_vec1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n_elements: i32) {\n+    %c64_i32 = arith.constant 64 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c64_i32 : i32\n+    %2 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked>\n+    %3 = tt.splat %1 : (i32) -> tensor<64xi32, #blocked>\n+    %4 = arith.addi %3, %2 : tensor<64xi32, #blocked>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n+    %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %9 = tt.splat %n_elements : (i32) -> tensor<64xi32, #blocked>\n+    %10 = \"triton_gpu.cmpi\"(%4, %9) {predicate = 2 : i64} : (tensor<64xi32, #blocked>, tensor<64xi32, #blocked>) -> tensor<64xi1, #blocked>\n+    // load op has a vector width = 1 due to the %mask's alignment\n+    // CHECK: ld.global.b32\n+    %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32, #blocked>\n+    %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32, #blocked>\n+    %13 = arith.addf %11, %12 : tensor<64xf32, #blocked>\n+    %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n+    %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    tt.store %15, %13, %10 : tensor<64xf32, #blocked>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: global_load_store_vec8\n@@ -682,4 +713,4 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n     return\n   }\n-}\n\\ No newline at end of file\n+}"}]