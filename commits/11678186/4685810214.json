[{"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -111,8 +111,7 @@ def early_config_prune(configs, named_args):\n         BLOCK_M, BLOCK_N, BLOCK_K, num_stages = \\\n             kw['BLOCK_M'], kw['BLOCK_N'], kw['BLOCK_K'], config.num_stages\n \n-        # TODO: move to `cuda_utils` submodule\n-        max_shared_memory = driver.utils.cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n+        max_shared_memory = driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n         required_shared_memory = (BLOCK_M + BLOCK_N) * BLOCK_K * num_stages * dtsize\n         if required_shared_memory <= max_shared_memory:\n             pruned_configs.append(config)"}]