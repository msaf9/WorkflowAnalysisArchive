[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -1079,10 +1079,9 @@ def format_of(ty):\n     init_module(device);\n   }}\n   void *params[] = {{ {', '.join(f\"&arg{i}\" for i in signature.keys() if i not in constants)} }};\n-  gridX = gridX < 1 ? 1 : gridX;\n-  gridY = gridY < 1 ? 1 : gridY;\n-  gridZ = gridZ < 1 ? 1 : gridZ;\n-  CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*{num_warps}, 1, 1, {name}_shmem, stream, params, 0));\n+  if(gridX*gridY*gridZ > 0){{\n+    CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*{num_warps}, 1, 1, {name}_shmem, stream, params, 0));\n+  }}\n }}\n \n CUdeviceptr getPointer(PyObject *obj, int idx) {{"}, {"filename": "python/triton/runtime/__init__.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1,3 +1,2 @@\n from .autotuner import Config, Heuristics, autotune, heuristics  # noqa: F401\n-from .jit import (JITFunction, KernelInterface, build_kernel,  # noqa: F401\n-                  launch_kernel, version_key)\n+from .jit import (JITFunction, KernelInterface, version_key) # noqa: F401"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 0, "deletions": 173, "changes": 173, "file_content_changes": "@@ -23,59 +23,6 @@\n except ImportError:\n     get_cuda_stream = lambda dev_idx: torch.cuda.current_stream(dev_idx).cuda_stream\n \n-# -----------------------------------------------------------------------------\n-# Binary\n-# -----------------------------------------------------------------------------\n-\n-VALID_BACKENDS: List[str] = (\n-    _triton.runtime.backend.CUDA,\n-)\n-\n-\n-class Binary:\n-    def __init__(self, backend: str, name: str, asm: Dict[str, str], shared_mem: int, num_warps: int):\n-        assert backend in VALID_BACKENDS, \"backend should within [%s], but get a \\\"%s\\\"\" % (', '.join(VALID_BACKENDS), backend)\n-        self.backend = backend\n-        self.name = name\n-        self.asm = asm\n-        self.shared_mem = shared_mem\n-        self.num_warps = num_warps\n-\n-\n-class LoadedBinary:\n-    def __init__(self, device: int, bin: Binary):\n-        module, kernel = _triton.load_binary(bin.backend,\n-                                             bin.name,\n-                                             bin.asm,\n-                                             bin.shared_mem,\n-                                             device)\n-        self.bin = bin\n-        self.asm = bin.asm\n-        self.sass = ''\n-        self.module = module\n-        self.kernel = kernel\n-        self.device = device\n-        self.shared_mem = bin.shared_mem\n-\n-    def __call__(self, stream, args, grid_0, grid_1=1, grid_2=1):\n-        _triton.runtime.enqueue(self.bin.backend, stream, self.kernel,\n-                                grid_0, grid_1, grid_2,\n-                                self.bin.num_warps * 32, 1, 1,\n-                                args, self.bin.shared_mem)\n-\n-    def get_sass(self, fun=None):\n-        if self.sass:\n-            return self.sass\n-        fd, path = tempfile.mkstemp()\n-        try:\n-            with open(fd, 'wb') as cubin:\n-                cubin.write(self.asm['cubin'])\n-            self.sass = extract(path, fun)\n-        finally:\n-            os.remove(path)\n-        self.asm['sass'] = self.sass\n-        return self.sass\n-\n # -----------------------------------------------------------------------------\n # Dependencies Finder\n # -----------------------------------------------------------------------------\n@@ -413,126 +360,6 @@ def __repr__(self):\n         return f\"JITFunction({self.module}:{self.fn.__name__})\"\n \n \n-def pow2_divisor(N):\n-    if N % 16 == 0:\n-        return 16\n-    if N % 8 == 0:\n-        return 8\n-    if N % 4 == 0:\n-        return 4\n-    if N % 2 == 0:\n-        return 2\n-    return 1\n-\n-\n-class _KernelCache:\n-    def __init__(self,\n-                 fn: JITFunction,\n-                 fn_type: str,\n-                 constants: Dict[str, Any],\n-                 num_warps: int = 4,\n-                 num_stages: int = 3):\n-        # hold the arguments for building a kernel\n-        self.fn = fn\n-        self.fn_type = fn_type\n-        self.constants = constants\n-        self.num_warps = num_warps\n-        self.num_stages = num_stages\n-\n-        # kernel compilation cache\n-        self._binary_cache: Optional[LoadedBinary] = None\n-\n-    @property\n-    def binary_cache(self):\n-        return self._binary_cache\n-\n-    def set_binary_cache(self, binary: LoadedBinary):\n-        assert binary\n-        assert not self._binary_cache, \"cannot set binary cache duplicately\"\n-        self._binary_cache = binary\n-\n-\n-def build_kernel(fn: JITFunction,\n-                 fn_type: str,\n-                 constants: Dict[str, Any],\n-                 num_warps: int = 4,\n-                 num_stages: int = 3,\n-                 ) -> _KernelCache:\n-    return _KernelCache(fn, fn_type, constants, num_warps, num_stages)\n-\n-\n-torch_dtype_to_bytes = {\n-    torch.int8: 1,\n-    torch.uint8: 1,\n-\n-    torch.int16: 2,\n-    torch.short: 2,\n-\n-    torch.int: 4,\n-    torch.int32: 4,\n-\n-    torch.long: 8,\n-    torch.int64: 8,\n-\n-    torch.float32: 4,\n-    torch.float: 4,\n-\n-    torch.float16: 2,\n-    torch.half: 2,\n-    torch.bfloat16: 2,\n-    # free to extend\n-}\n-\n-\n-def launch_kernel(kernel: _KernelCache, grid, device, *wargs, **kwargs):\n-    def is_tensor(arg):\n-        return hasattr(arg, 'data_ptr')  # a torch.tensor\n-\n-    # prepare function args for compile\n-    kwargs = {kernel.fn.arg_names.index(name): value for name, value in kwargs.items()}\n-    wargs = list(wargs)\n-    for i, pos in enumerate(sorted(kwargs)):\n-        wargs.insert(pos + i, kwargs[pos])\n-    assert len(wargs) == len(kernel.fn.arg_names), \"Function argument list not match, need %d but get %d args\" % (len(kernel.fn.arg_names), len(wargs))\n-\n-    if not kernel.binary_cache:\n-        # build the kernel cache\n-        backend = _triton.runtime.backend.CUDA\n-\n-        attributes = dict()\n-        for i, arg in enumerate(wargs):\n-            if i in kernel.fn.do_not_specialize:\n-                continue\n-            if isinstance(arg, int):\n-                attributes[i] = pow2_divisor(arg)\n-            elif is_tensor(arg):\n-                assert arg.dtype in torch_dtype_to_bytes\n-                addr = arg.data_ptr()\n-                range_size = _triton.runtime.get_pointer_range_size(addr)\n-                divisibility = min(pow2_divisor(addr), pow2_divisor(range_size)) // torch_dtype_to_bytes[arg.dtype]\n-                attributes[i] = divisibility\n-\n-        attributes_ = dict()\n-        for i, value in attributes.items():\n-            attributes_[kernel.fn.arg_names[i]] = value\n-\n-        cubin, shem_size, kernel_name = compile(kernel.fn, kernel.fn_type, device=device, constants=kernel.constants, attributes=attributes_, num_warps=kernel.num_warps, num_stages=kernel.num_stages, output=\"cubin\")\n-        assert cubin\n-        assert kernel_name\n-\n-        max_shared_memory = _triton.runtime.max_shared_memory(backend, device)\n-        assert shem_size <= max_shared_memory, \"shared memory out of resource, max size is %d, but want %s\" % (max_shared_memory, shem_size)\n-\n-        asm = dict(cubin=cubin)\n-        binary = Binary(backend, kernel_name, asm, shem_size, kernel.num_warps)\n-        loaded_binary = LoadedBinary(device, binary)\n-        kernel.set_binary_cache(loaded_binary)\n-\n-    torch.cuda.set_device(device)\n-    stream = get_cuda_stream(device)\n-\n-    _triton.runtime.launch_binary(kernel.binary_cache, wargs, kernel.fn.do_not_specialize, kernel.fn.arg_names,\n-                                  stream, kernel.num_warps, kernel.num_stages, grid)\n \n \n # -----------------------------------------------------------------------------"}]