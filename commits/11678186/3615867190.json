[{"filename": "include/triton/Analysis/Membar.h", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -29,7 +29,11 @@ class MembarAnalysis {\n   /// The following circumstances are not considered yet:\n   /// - Double buffers\n   /// - N buffers\n-  MembarAnalysis(Allocation *allocation) : allocation(allocation) { run(); }\n+  MembarAnalysis(Allocation *allocation) : allocation(allocation) {}\n+\n+  /// Runs the membar analysis to the given operation, inserts a barrier if\n+  /// necessary.\n+  void run();\n \n private:\n   struct RegionInfo {\n@@ -82,10 +86,6 @@ class MembarAnalysis {\n     }\n   };\n \n-  /// Runs the membar analysis to the given operation, inserts a barrier if\n-  /// necessary.\n-  void run();\n-\n   /// Applies the barrier analysis based on the SCF dialect, in which each\n   /// region has a single basic block only.\n   /// Example:"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -295,6 +295,18 @@ def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n     let assemblyFormat = \"$lhs `,` $rhs attr-dict `:` functional-type(operands, results)\";\n }\n \n+def TT_TransOp : TT_Op<\"trans\", [NoSideEffect,\n+                               SameOperandsAndResultElementType]> {\n+\n+    let summary = \"transpose a tensor\";\n+\n+    let arguments = (ins TT_Tensor:$src);\n+\n+    let results = (outs TT_Tensor:$result);\n+\n+    let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n+}\n+\n //\n // SPMD Ops\n //"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "file_content_changes": "@@ -296,37 +296,37 @@ partitioned between warps.\n // -------------------------------- version = 1 --------------------------- //\n \n For first-gen tensor cores, the implicit warpTileSize is [16, 16].\n-Information about this layout can be found in the official PTX documentation\n+Note: the layout is different from the recommended in PTX ISA\n https://docs.nvidia.com/cuda/parallel-thread-execution/index.html\n (mma.884 section, FP32 accumulator).\n \n For example, the matrix L corresponding to blockTileSize=[32,16] is:\n \n                                warp 0\n --------------------------------/\\-------------------------------\n-[ 0   0   2   2   0   0   2   2    4   4   6   6   4   4   6   6 ]\n-[ 1   1   3   3   1   1   3   3    5   5   7   7   5   5   7   7 ]\n-[ 0   0   2   2   0   0   2   2    4   4   6   6   4   4   6   6 ]\n-[ 1   1   3   3   1   1   3   3    5   5   7   7   5   5   7   7 ]\n-[ 16  16  18  18  16  16  18  18   20  20  22  22  20  20  22  22]\n-[ 17  17  19  19  17  17  19  19   21  21  23  23  21  21  23  23]\n-[ 16  16  18  18  16  16  18  18   20  20  22  22  20  20  22  22]\n-[ 17  17  19  19  17  17  19  19   21  21  23  23  21  21  23  23]\n-[ 8   8   10  10  8   8   10  10   12  12  14  14  12  12  14  14]\n-[ 9   9   11  11  9   9   11  11   13  13  15  15  13  13  15  15]\n-[ ..............................................................\n-[ ..............................................................\n-[ 24  24  26  26  24  24  26  26   28  28  30  30  28  28  30  30]\n-[ 25  25  27  27  25  25  27  27   29  29  31  31  29  29  31  31]\n-\n-                         warp 1 = warp0 + 32\n+[ 0   0   2   2   8   8   10  10   0   0   2   2   8   8   10  10 ]\n+[ 1   1   3   3   9   9   11  11   1   1   3   3   9   9   11  11 ]\n+[ 0   0   2   2   8   8   10  10   0   0   2   2   8   8   10  10 ]\n+[ 1   1   3   3   9   9   11  11   1   1   3   3   9   9   11  11 ]\n+[ 4   4   6   6   12  12  14  14   4   4   6   6   12  12  14  14 ]\n+[ 5   5   7   7   13  13  15  15   5   5   7   7   13  13  15  15 ]\n+[ 4   4   6   6   12  12  14  14   4   4   6   6   12  12  14  14 ]\n+[ 5   5   7   7   13  13  15  15   5   5   7   7   13  13  15  15 ]\n+[ 16  16  18  18  20  20  22  22   16  16  18  18  20  20  22  22 ]\n+[ 17  17  19  19  21  21  23  23   17  17  19  19  21  21  23  23 ]\n+[ 16  16  18  18  20  20  22  22   16  16  18  18  20  20  22  22 ]\n+[ 17  17  19  19  21  21  23  23   17  17  19  19  21  21  23  23 ]\n+[ 24  24  26  26  28  28  30  30   24  24  26  26  28  28  30  30 ]\n+[ 25  25  27  27  29  29  31  31   25  25  27  27  29  29  31  31 ]\n+[ 24  24  26  26  28  28  30  30   24  24  26  26  28  28  30  30 ]\n+[ 25  25  27  27  29  29  31  31   25  25  27  27  29  29  31  31 ]\n+\n+                          warp 1 = warp0 + 32\n --------------------------------/\\-------------------------------\n-[ 32  32  34  34  32  32  34  34   36  36  38  38  36  36  38  38]\n-[ 33  33  35  35  33  33  35  35   37  37  39  39  37  37  39  39]\n-[ ..............................................................\n-[ ..............................................................\n-[ 56  56  58  58  56  56  58  58   60  60  62  62  60  60  62  62]\n-[ 57  57  59  59  57  57  59  59   61  61  63  63  61  61  63  63]\n+[ 32  32  34  34  40  40  42  42   32  32  34  34  40  40  42  42 ]\n+[ 33  33  35  35  41  41  43  43   33  33  35  35  41  41  43  43 ]\n+[ ............................................................... ]\n+\n \n // -------------------------------- version = 2 --------------------------- //\n "}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -26,7 +26,7 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n     // FIXME(Keren): extract and insert are always alias for now\n-    if (isa<tensor::ExtractSliceOp>(op)) {\n+    if (isa<tensor::ExtractSliceOp, triton::TransOp>(op)) {\n       // extract_slice %src\n       aliasInfo = AliasInfo(operands[0]->getValue());\n       pessimistic = false;"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 40, "deletions": 17, "changes": 57, "file_content_changes": "@@ -24,21 +24,43 @@ void MembarAnalysis::dfsOperation(Operation *operation,\n     // scf.if only: two regions\n     // scf.for: one region\n     RegionInfo curRegionInfo;\n-    for (auto &region : operation->getRegions()) {\n-      // Copy the parent info as the current info.\n-      RegionInfo regionInfo = *parentRegionInfo;\n-      for (auto &block : region.getBlocks()) {\n-        assert(region.getBlocks().size() == 1 &&\n-               \"Multiple blocks in a region is not supported\");\n-        for (auto &op : block.getOperations()) {\n-          // Traverse the nested operation.\n-          dfsOperation(&op, &regionInfo, builder);\n+    auto traverseRegions = [&]() -> auto{\n+      for (auto &region : operation->getRegions()) {\n+        // Copy the parent info as the current info.\n+        RegionInfo regionInfo = *parentRegionInfo;\n+        for (auto &block : region.getBlocks()) {\n+          assert(region.getBlocks().size() == 1 &&\n+                 \"Multiple blocks in a region is not supported\");\n+          for (auto &op : block.getOperations()) {\n+            // Traverse the nested operation.\n+            dfsOperation(&op, &regionInfo, builder);\n+          }\n         }\n+        curRegionInfo.join(regionInfo);\n       }\n-      curRegionInfo.join(regionInfo);\n+      // Set the parent region info as the union of the nested region info.\n+      *parentRegionInfo = curRegionInfo;\n+    };\n+\n+    traverseRegions();\n+    if (isa<scf::ForOp>(operation)) {\n+      // scf.for can have two possible inputs: the init value and the\n+      // previous iteration's result. Although we've applied alias analysis,\n+      // there could be unsynced memory accesses on reused memories.\n+      // For example, consider the following code:\n+      // %1 = convert_layout %0: blocked -> shared\n+      // ...\n+      // gpu.barrier\n+      // ...\n+      // %5 = convert_layout %4 : shared -> dot\n+      // %6 = tt.dot %2, %5\n+      // scf.yield\n+      //\n+      // Though %5 could be released before scf.yield, it may shared the same\n+      // memory with %1. So we actually have to insert a barrier before %1 to\n+      // make sure the memory is synced.\n+      traverseRegions();\n     }\n-    // Set the parent region info as the union of the nested region info.\n-    *parentRegionInfo = curRegionInfo;\n   }\n }\n \n@@ -49,8 +71,7 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n     // Do not insert barriers before control flow operations and\n     // alloc/extract/insert\n     // alloc is an allocation op without memory write.\n-    // In contrast, arith.constant is an allocation op with memory write.\n-    // FIXME(Keren): extract is always alias for now\n+    // FIXME(Keren): extract_slice is always alias for now\n     return;\n   }\n \n@@ -60,9 +81,11 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n     return;\n   }\n \n-  if (isa<triton::gpu::AsyncWaitOp>(op)) {\n-    // If the current op is an async wait, we insert a barrier op and sync\n-    // previous reads and writes.\n+  if (isa<triton::gpu::AsyncWaitOp>(op) &&\n+      !isa<gpu::BarrierOp>(op->getNextNode())) {\n+    // If the current op is an async wait and the next op is not a barrier we\n+    // insert a barrier op and sync\n+    regionInfo->sync();\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPointAfter(op);\n     builder->create<gpu::BarrierOp>(op->getLoc());"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -105,7 +105,7 @@ bool maybeSharedAllocationOp(Operation *op) {\n }\n \n bool maybeAliasOp(Operation *op) {\n-  return isa<tensor::ExtractSliceOp>(op) ||\n+  return isa<tensor::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n          isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n          isa<tensor::InsertSliceOp>(op);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 9, "deletions": 13, "changes": 22, "file_content_changes": "@@ -708,19 +708,15 @@ class MMA16816SmemLoader {\n       Type elemTy = type::f32Ty(ctx);\n       Type elemPtrTy = ptr_ty(elemTy);\n       if (kOrder == 1) {\n-        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n-        elems[1] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n-        elems[2] =\n-            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n-        elems[3] =\n-            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n+        elems[1] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n+        elems[2] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n+        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n       } else {\n-        elems[0] = load(gep(elemPtrTy, ptr, i32_val(sOffsetElem)));\n-        elems[2] = load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem)));\n-        elems[1] =\n-            load(gep(elemPtrTy, ptr, i32_val(sOffsetElem + sOffsetArrElem)));\n-        elems[3] =\n-            load(gep(elemPtrTy, ptr2, i32_val(sOffsetElem + sOffsetArrElem)));\n+        elems[0] = load(gep(elemPtrTy, ptr, sOffsetElemVal));\n+        elems[2] = load(gep(elemPtrTy, ptr2, sOffsetElemVal));\n+        elems[1] = load(gep(elemPtrTy, ptr, sOffsetArrElemVal));\n+        elems[3] = load(gep(elemPtrTy, ptr2, sOffsetArrElemVal));\n       }\n       return {elems[0], elems[1], elems[2], elems[3]};\n \n@@ -1140,7 +1136,7 @@ struct MMA16816ConversionHelper {\n   std::function<void(int, int)>\n   getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n                   MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n-                  ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                  SmallVector<int> instrShape, SmallVector<int> matShape,\n                   Value warpId, ValueTable &vals, bool isA) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     // We assumes that the input operand of Dot should be from shared layout."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 93, "deletions": 76, "changes": 169, "file_content_changes": "@@ -1887,9 +1887,9 @@ struct PrintfOpConversion\n \n     Value globalPtr =\n         rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n-    Value stringStart =\n-        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n-                                     globalPtr, mlir::ValueRange({zero, zero}));\n+    Value stringStart = rewriter.create<LLVM::GEPOp>(\n+        UnknownLoc::get(context), int8Ptr, globalPtr,\n+        SmallVector<Value>({zero, zero}));\n \n     Value bufferPtr =\n         rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n@@ -1924,7 +1924,7 @@ struct PrintfOpConversion\n                                                    int8Ptr, allocated);\n     }\n \n-    ValueRange operands{stringStart, bufferPtr};\n+    SmallVector<Value> operands{stringStart, bufferPtr};\n     rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n   }\n };\n@@ -2728,6 +2728,9 @@ struct ConvertLayoutOpConversion\n     auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n     auto inOrd = srcBlockedLayout.getOrder();\n     auto outOrd = dstSharedLayout.getOrder();\n+    if (inOrd != outOrd)\n+      llvm_unreachable(\n+          \"blocked -> shared with different order not yet implemented\");\n     unsigned inVec =\n         inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n     unsigned outVec = dstSharedLayout.getVec();\n@@ -2787,7 +2790,8 @@ struct ConvertLayoutOpConversion\n             getMultiDimIndex<unsigned>(linearRepIdx, reps, inOrd);\n         for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n              ++linearWordIdx) {\n-          // step 1: recover the multidim_index from the index of input_elements\n+          // step 1: recover the multidim_index from the index of\n+          // input_elements\n           auto multiDimWordIdx =\n               getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep, inOrd);\n           SmallVector<Value> multiDimIdx(2);\n@@ -2888,20 +2892,21 @@ struct ConvertLayoutOpConversion\n       } else if (mmaLayout.getVersion() == 1) {\n         multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n         multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 16));\n-        Value partId = udiv(laneId, _4);\n-        Value partIdDiv4 = udiv(partId, _4);\n-        Value partIdRem4 = urem(partId, _4);\n-        Value partRowOffset = mul(udiv(partIdRem4, _2), _8);\n-        partRowOffset = add(mul(partIdDiv4, _4), partRowOffset);\n-        Value partColOffset = mul(urem(partIdRem4, _2), _8);\n-        Value colOffset = add(mul(multiDimWarpId[0], _16), partColOffset);\n-        Value rowOffset = add(mul(multiDimWarpId[1], _16), partRowOffset);\n-        mmaRowIdx[0] = add(urem(laneId, _2), rowOffset);\n+        Value laneIdDiv16 = udiv(laneId, _16);\n+        Value laneIdRem16 = urem(laneId, _16);\n+        Value laneIdRem2 = urem(laneId, _2);\n+        Value laneIdRem16Div8 = udiv(laneIdRem16, _8);\n+        Value laneIdRem16Div4 = udiv(laneIdRem16, _4);\n+        Value laneIdRem16Div4Rem2 = urem(laneIdRem16Div4, _2);\n+        Value laneIdRem4Div2 = udiv(urem(laneId, _4), _2);\n+        mmaRowIdx[0] =\n+            add(add(mul(laneIdDiv16, _8), mul(laneIdRem16Div4Rem2, _4)),\n+                laneIdRem2);\n         mmaRowIdx[1] = add(mmaRowIdx[0], _2);\n-        mmaColIdx[0] = add(mul(udiv(urem(laneId, _4), _2), _2), colOffset);\n+        mmaColIdx[0] = add(mul(laneIdRem16Div8, _4), mul(laneIdRem4Div2, _2));\n         mmaColIdx[1] = add(mmaColIdx[0], _1);\n-        mmaColIdx[2] = add(mmaColIdx[0], _4);\n-        mmaColIdx[3] = add(mmaColIdx[0], idx_val(5));\n+        mmaColIdx[2] = add(mmaColIdx[0], _8);\n+        mmaColIdx[3] = add(mmaColIdx[0], idx_val(9));\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n@@ -3151,10 +3156,6 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n     ConversionPatternRewriter &rewriter) const {\n   auto loc = op.getLoc();\n \n-  // TODO[Keren]: A temporary workaround for an issue from membar pass.\n-  // https://triton-lang.slack.com/archives/C042VBSQWNS/p1669796615860699?thread_ts=1669779203.526739&cid=C042VBSQWNS\n-  barrier();\n-\n   Value src = op.src();\n   Value dst = op.result();\n   auto srcTy = src.getType().cast<RankedTensorType>();\n@@ -3340,10 +3341,10 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   // We cannot get both the operand types(in TypeConverter), here we assume the\n   // types of both the operands are identical here.\n   // TODO[Superjomn]: Find a better way to implement it.\n-  static bool isDotHMMA(TensorType operand, bool allowTF32, int mmaVersion) {\n+  static bool isDotHMMA(TensorType operand, int mmaVersion) {\n     auto elemTy = operand.getElementType();\n     return elemTy.isF16() || elemTy.isBF16() ||\n-           (elemTy.isF32() && allowTF32 && mmaVersion >= 2) ||\n+           (elemTy.isF32() && mmaVersion >= 2) ||\n            (elemTy.isInteger(8) && mmaVersion >= 2);\n   }\n \n@@ -3367,11 +3368,7 @@ Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n   Value src = op.src();\n   Value dst = op.result();\n   auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n-  // TODO[Superjomn]: allowTF32 is not accessible here for it is an attribute of\n-  // an Op instance.\n-  bool allowTF32 = false;\n-  bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, allowTF32,\n-                                           mmaLayout.getVersion());\n+  bool isHMMA = DotOpConversion::isDotHMMA(dstTensorTy, mmaLayout.getVersion());\n \n   auto smemObj = getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n   Value res;\n@@ -3434,25 +3431,16 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   } else if (auto blockedLayout =\n                  dotOperandLayout.getParent()\n                      .dyn_cast_or_null<BlockedEncodingAttr>()) {\n-    // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n-    // is an attribute of DotOp.\n-    bool allowTF32 = false;\n-    bool isFMADot = dstTensorTy.getElementType().isF32() && !allowTF32;\n-    if (isFMADot) {\n-      auto dotOpLayout =\n-          dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-      auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n-      DotOpFMAConversionHelper helper(blockedLayout);\n-      auto thread = getThreadId(rewriter, loc);\n-      if (dotOpLayout.getOpIdx() == 0) { // $a\n-        res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n-                           rewriter);\n-      } else { // $b\n-        res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n-                           rewriter);\n-      }\n-    } else\n-      assert(false && \"Unsupported dot operand layout found\");\n+    auto dotOpLayout = dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+    DotOpFMAConversionHelper helper(blockedLayout);\n+    auto thread = getThreadId(rewriter, loc);\n+    if (dotOpLayout.getOpIdx() == 0) { // $a\n+      res = helper.loadA(src, adaptor.src(), blockedLayout, thread, loc,\n+                         rewriter);\n+    } else { // $b\n+      res = helper.loadB(src, adaptor.src(), blockedLayout, thread, loc,\n+                         rewriter);\n+    }\n   } else {\n     assert(false && \"Unsupported dot operand layout found\");\n   }\n@@ -3563,6 +3551,8 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n \n   // initialize accumulators\n   SmallVector<Value> acc = getElementsFromStruct(loc, loadedC, rewriter);\n+  size_t resSize = acc.size();\n+  SmallVector<Value> resVals(resSize);\n \n   auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n     auto ha = has.at({m, k});\n@@ -3607,8 +3597,13 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n       return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n     };\n \n-    for (unsigned i = 0; i < 8; i++)\n-      acc[idx[i]] = extract_val(f32_ty, res, getIntAttr(i));\n+    for (unsigned i = 0; i < 8; i++) {\n+      Value elem = extract_val(f32_ty, res, getIntAttr(i));\n+      acc[idx[i]] = elem;\n+      // TODO[goostavz]: double confirm this when m/n/k = [32, 32, x] has been\n+      // verified before MMA\n+      resVals[(m * numN / 2 + n) * 8 + i] = elem;\n+    }\n   };\n \n   for (unsigned k = 0; k < NK; k += 4)\n@@ -3617,12 +3612,10 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n         callMMA(m, n, k);\n       }\n \n-  // replace with new packed result\n   Type structTy = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(acc.size(), type::f32Ty(ctx)));\n-  Value res = getStructFromElements(loc, acc, rewriter, structTy);\n+      ctx, SmallVector<Type>(resSize, type::f32Ty(ctx)));\n+  Value res = getStructFromElements(loc, resVals, rewriter, structTy);\n   rewriter.replaceOp(op, res);\n-\n   return success();\n }\n \n@@ -3725,6 +3718,33 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n \n /// ====================== mma codegen end ============================\n \n+/// ====================== trans codegen begin ============================\n+\n+struct TransOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::TransOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::TransOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::TransOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    auto srcSmemObj =\n+        getSharedMemoryObjectFromStruct(loc, adaptor.src(), rewriter);\n+    SmallVector<Value> dstStrides = {srcSmemObj.strides[1],\n+                                     srcSmemObj.strides[0]};\n+    SmallVector<Value> dstOffsets = {srcSmemObj.offsets[1],\n+                                     srcSmemObj.offsets[0]};\n+    auto dstSmemObj =\n+        SharedMemoryObject(srcSmemObj.base, dstStrides, dstOffsets);\n+    auto retVal = getStructFromSharedMemoryObject(loc, dstSmemObj, rewriter);\n+    rewriter.replaceOp(op, retVal);\n+    return success();\n+  }\n+};\n+\n+/// ====================== trans codegen end ============================\n+\n Value convertSplatLikeOpWithMmaLayout(const MmaEncodingAttr &layout,\n                                       Type resType, Type elemType,\n                                       Value constVal,\n@@ -3788,13 +3808,6 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n     auto ctx = type.getContext();\n     Attribute layout = type.getEncoding();\n     auto shape = type.getShape();\n-\n-    // TODO[Keren, Superjomn]: fix it, allowTF32 is not accessible here for it\n-    // is bound to an Op instance.\n-    bool allowTF32 = false;\n-    bool isFMADot = type.getElementType().isF32() && !allowTF32 &&\n-                    layout.dyn_cast_or_null<DotOperandEncodingAttr>();\n-\n     if (layout &&\n         (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n          layout.isa<MmaEncodingAttr>())) {\n@@ -3818,37 +3831,39 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n       return LLVM::LLVMStructType::getLiteral(ctx, types);\n     } else if (auto dotOpLayout =\n                    layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n-      if (isFMADot) { // for parent is blocked layout\n+      if (dotOpLayout.getParent()\n+              .isa<BlockedEncodingAttr>()) { // for parent is blocked layout\n         int numElemsPerThread =\n             DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n \n         return LLVM::LLVMStructType::getLiteral(\n             ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n-\n       } else { // for parent is MMA layout\n         auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n         auto wpt = mmaLayout.getWarpsPerCTA();\n         Type elemTy = convertType(type.getElementType());\n-        auto vecSize = 1;\n-        if (elemTy.getIntOrFloatBitWidth() == 16) {\n-          vecSize = 2;\n-        } else if (elemTy.getIntOrFloatBitWidth() == 8) {\n-          vecSize = 4;\n-        } else {\n-          assert(false && \"Unsupported element type\");\n-        }\n-        Type vecTy = vec_ty(elemTy, vecSize);\n         if (mmaLayout.getVersion() == 2) {\n+          const llvm::DenseMap<int, Type> targetTyMap = {\n+              {32, elemTy},\n+              {16, vec_ty(elemTy, 2)},\n+              {8, vec_ty(elemTy, 4)},\n+          };\n+          Type targetTy;\n+          if (targetTyMap.count(elemTy.getIntOrFloatBitWidth())) {\n+            targetTy = targetTyMap.lookup(elemTy.getIntOrFloatBitWidth());\n+          } else {\n+            assert(false && \"Unsupported element type\");\n+          }\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n             int elems =\n                 MMA16816ConversionHelper::getANumElemsPerThread(type, wpt[0]);\n             return LLVM::LLVMStructType::getLiteral(\n-                ctx, SmallVector<Type>(elems, vecTy));\n+                ctx, SmallVector<Type>(elems, targetTy));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n             int elems =\n                 MMA16816ConversionHelper::getBNumElemsPerThread(type, wpt[1]);\n-            return struct_ty(SmallVector<Type>(elems, vecTy));\n+            return struct_ty(SmallVector<Type>(elems, targetTy));\n           }\n         }\n \n@@ -3978,10 +3993,10 @@ struct InsertSliceAsyncOpConversion\n     // %other\n     SmallVector<Value> otherElems;\n     if (llOther) {\n-      // TODO(Keren): support \"other\" tensor.\n+      // FIXME(Keren): always assume other is 0 for now\n       // It's not necessary for now because the pipeline pass will skip\n       // generating insert_slice_async if the load op has any \"other\" tensor.\n-      assert(false && \"insert_slice_async: Other value not supported yet\");\n+      // assert(false && \"insert_slice_async: Other value not supported yet\");\n       otherElems = getLLVMElems(other, llOther, rewriter, loc);\n       assert(srcElems.size() == otherElems.size());\n     }\n@@ -4552,6 +4567,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n                                                            benefit);\n   patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n+  patterns.add<TransOpConversion>(typeConverter, benefit);\n   patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n \n@@ -4716,7 +4732,8 @@ class ConvertTritonGPUToLLVM\n     decomposeInsertSliceAsyncOp(mod);\n \n     Allocation allocation(mod);\n-    MembarAnalysis membar(&allocation);\n+    MembarAnalysis membarPass(&allocation);\n+    membarPass.run();\n \n     RewritePatternSet scf_patterns(context);\n     mlir::populateLoopToStdConversionPatterns(scf_patterns);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 95, "deletions": 4, "changes": 99, "file_content_changes": "@@ -252,6 +252,51 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n   }\n };\n \n+struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n+\n+  using OpConversionPattern<triton::TransOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::TransOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Value src = adaptor.src();\n+    auto srcType = src.getType().cast<RankedTensorType>();\n+    Attribute srcEncoding = srcType.getEncoding();\n+    if (!srcEncoding)\n+      return failure();\n+    if (!srcEncoding.isa<triton::gpu::SharedEncodingAttr>()) {\n+      // TODO: end-to-end correctness is broken if\n+      // the input is blocked and the output is shared\n+      // with different order. Maybe a backend issue in BlockedToShared?\n+      SmallVector<unsigned> order = {1, 0};\n+      if (auto srcBlockedEncoding =\n+              srcEncoding.dyn_cast<triton::gpu::BlockedEncodingAttr>())\n+        llvm::copy(srcBlockedEncoding.getOrder(), order.begin());\n+      srcEncoding =\n+          triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, order);\n+      srcType = RankedTensorType::get(srcType.getShape(),\n+                                      srcType.getElementType(), srcEncoding);\n+      src = rewriter.create<triton::gpu::ConvertLayoutOp>(src.getLoc(), srcType,\n+                                                          src);\n+    }\n+    auto srcSharedEncoding =\n+        srcEncoding.cast<triton::gpu::SharedEncodingAttr>();\n+    SmallVector<unsigned> retOrder(srcSharedEncoding.getOrder().begin(),\n+                                   srcSharedEncoding.getOrder().end());\n+    SmallVector<int64_t> retShapes(srcType.getShape().begin(),\n+                                   srcType.getShape().end());\n+    std::reverse(retOrder.begin(), retOrder.end());\n+    std::reverse(retShapes.begin(), retShapes.end());\n+    auto retEncoding =\n+        triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, retOrder);\n+    auto retType =\n+        RankedTensorType::get(retShapes, srcType.getElementType(), retEncoding);\n+\n+    rewriter.replaceOpWithNewOp<triton::TransOp>(op, retType, src);\n+    return success();\n+  }\n+};\n+\n struct TritonLoadPattern : public OpConversionPattern<triton::LoadOp> {\n   using OpConversionPattern<triton::LoadOp>::OpConversionPattern;\n \n@@ -390,9 +435,10 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::PtrToIntOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n-      TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-      TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n-      TritonPrintfPattern, TritonAtomicRMWPattern>(typeConverter, context);\n+      TritonTransPattern, TritonExpandDimsPattern, TritonMakeRangePattern,\n+      TritonDotPattern, TritonLoadPattern, TritonStorePattern,\n+      TritonExtElemwisePattern, TritonPrintfPattern, TritonAtomicRMWPattern>(\n+      typeConverter, context);\n }\n \n //\n@@ -456,10 +502,55 @@ struct SCFYieldPattern : public OpConversionPattern<scf::YieldOp> {\n   }\n };\n \n+// This is borrowed from ConvertFIfOpTypes in\n+//    SCF/Transforms/StructuralTypeConversions.cpp\n+class SCFIfPattern : public OpConversionPattern<scf::IfOp> {\n+public:\n+  using OpConversionPattern<scf::IfOp>::OpConversionPattern;\n+  LogicalResult\n+  matchAndRewrite(scf::IfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // TODO: Generalize this to any type conversion, not just 1:1.\n+    //\n+    // We need to implement something more sophisticated here that tracks which\n+    // types convert to which other types and does the appropriate\n+    // materialization logic.\n+    // For example, it's possible that one result type converts to 0 types and\n+    // another to 2 types, so newResultTypes would at least be the right size to\n+    // not crash in the llvm::zip call below, but then we would set the the\n+    // wrong type on the SSA values! These edge cases are also why we cannot\n+    // safely use the TypeConverter::convertTypes helper here.\n+    SmallVector<Type> newResultTypes;\n+    for (auto type : op.getResultTypes()) {\n+      Type newType = typeConverter->convertType(type);\n+      if (!newType)\n+        return rewriter.notifyMatchFailure(op, \"not a 1:1 type conversion\");\n+      newResultTypes.push_back(newType);\n+    }\n+\n+    // See comments in the ForOp pattern for why we clone without regions and\n+    // then inline.\n+    scf::IfOp newOp =\n+        cast<scf::IfOp>(rewriter.cloneWithoutRegions(*op.getOperation()));\n+    rewriter.inlineRegionBefore(op.getThenRegion(), newOp.getThenRegion(),\n+                                newOp.getThenRegion().end());\n+    rewriter.inlineRegionBefore(op.getElseRegion(), newOp.getElseRegion(),\n+                                newOp.getElseRegion().end());\n+\n+    // Update the operands and types.\n+    newOp->setOperands(adaptor.getOperands());\n+    for (auto t : llvm::zip(newOp.getResults(), newResultTypes))\n+      std::get<0>(t).setType(std::get<1>(t));\n+    rewriter.replaceOp(op, newOp.getResults());\n+    return success();\n+  }\n+};\n+\n void populateSCFPatterns(TritonGPUTypeConverter &typeConverter,\n                          RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n-  patterns.add<SCFYieldPattern, SCFForPattern>(typeConverter, context);\n+  patterns.add<SCFYieldPattern, SCFForPattern, SCFIfPattern>(typeConverter,\n+                                                             context);\n }\n \n class ConvertTritonToTritonGPU"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 98, "deletions": 41, "changes": 139, "file_content_changes": "@@ -50,22 +50,25 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n     auto dstType = convert.getType().cast<RankedTensorType>();\n     if (srcType.getEncoding().isa<triton::gpu::BlockedEncodingAttr>() &&\n         dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n-      auto dstDotOperand = dstType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n+      auto dstDotOperand =\n+          dstType.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n       auto dstParent = dstDotOperand.getParent();\n-      if(dstDotOperand.getOpIdx()==1 || \n-         !dstParent.isa<triton::gpu::MmaEncodingAttr>())\n+      if (dstDotOperand.getOpIdx() == 1 ||\n+          !dstParent.isa<triton::gpu::MmaEncodingAttr>())\n         return mlir::failure();\n       auto dstParentMma = dstParent.cast<triton::gpu::MmaEncodingAttr>();\n-      if(dstParentMma.getVersion() == 1 ||\n-         dstParentMma.getWarpsPerCTA()[1] > 1)\n+      if (dstParentMma.getVersion() == 1 ||\n+          dstParentMma.getWarpsPerCTA()[1] > 1)\n         return mlir::failure();\n-      SetVector<Operation*> bwdSlices;\n+      SetVector<Operation *> bwdSlices;\n       mlir::getBackwardSlice(convert.getResult(), &bwdSlices);\n-      if(llvm::find_if(bwdSlices, [](Operation *op) { return isa<triton::DotOp>(op); }) == bwdSlices.end())\n+      if (llvm::find_if(bwdSlices, [](Operation *op) {\n+            return isa<triton::DotOp>(op);\n+          }) == bwdSlices.end())\n         return mlir::failure();\n-      \n-      auto tmpType =\n-          RankedTensorType::get(dstType.getShape(), dstType.getElementType(), dstParentMma);\n+\n+      auto tmpType = RankedTensorType::get(\n+          dstType.getShape(), dstType.getElementType(), dstParentMma);\n       auto tmp = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           convert.getLoc(), tmpType, convert.getOperand());\n       auto newConvert = rewriter.create<triton::gpu::ConvertLayoutOp>(\n@@ -175,6 +178,10 @@ class SimplifyConversion : public mlir::RewritePattern {\n           !isSharedEncoding(convert.getResult())) {\n         return mlir::failure();\n       }\n+      if (isSharedEncoding(convert.getOperand()) &&\n+          isSharedEncoding(convert.getResult())) {\n+        return mlir::failure();\n+      }\n       auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n       auto srcShared =\n           srcType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n@@ -601,10 +608,9 @@ mmaVersionToShapePerWarp(int version, const ArrayRef<int64_t> &shape,\n   }\n }\n \n-\n SmallVector<unsigned, 2> warpsPerTileV1(triton::DotOp dotOp,\n-                                         const ArrayRef<int64_t> shape,\n-                                         int numWarps) {\n+                                        const ArrayRef<int64_t> shape,\n+                                        int numWarps) {\n   SmallVector<unsigned, 2> ret = {1, 1};\n   SmallVector<int64_t, 2> shapePerWarp =\n       mmaVersionToShapePerWarp(1, shape, numWarps);\n@@ -624,39 +630,89 @@ SmallVector<unsigned, 2> warpsPerTileV1(triton::DotOp dotOp,\n }\n \n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n-                                         const ArrayRef<int64_t> shape,\n-                                         int numWarps) {\n-    SetVector<Operation*> slices;\n-    mlir::getForwardSlice(dotOp.getResult(), &slices);\n-    if(llvm::find_if(slices, [](Operation *op) { return isa<triton::DotOp>(op); }) != slices.end())\n-      return {(unsigned)numWarps, 1};\n-    \n-    SmallVector<unsigned, 2> ret = {1, 1};\n-    SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n-    bool changed = false;\n-    // TODO (@daadaada): double-check.\n-    // original logic in\n-    // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n-    // seems buggy for shape = [32, 16] ?\n-    do {\n-      changed = false;\n-      if (ret[0] * ret[1] >= numWarps)\n-        break;\n-      if (shape[0] / shapePerWarp[0] / ret[0] >=\n-          shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n-        if (ret[0] < shape[0] / shapePerWarp[0]) {\n-          ret[0] *= 2;\n-        } else\n-          ret[1] *= 2;\n-      } else {\n+                                        const ArrayRef<int64_t> shape,\n+                                        int numWarps) {\n+  SetVector<Operation *> slices;\n+  mlir::getForwardSlice(dotOp.getResult(), &slices);\n+  if (llvm::find_if(slices, [](Operation *op) {\n+        return isa<triton::DotOp>(op);\n+      }) != slices.end())\n+    return {(unsigned)numWarps, 1};\n+\n+  SmallVector<unsigned, 2> ret = {1, 1};\n+  SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n+  bool changed = false;\n+  // TODO (@daadaada): double-check.\n+  // original logic in\n+  // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L252\n+  // seems buggy for shape = [32, 16] ?\n+  do {\n+    changed = false;\n+    if (ret[0] * ret[1] >= numWarps)\n+      break;\n+    if (shape[0] / shapePerWarp[0] / ret[0] >=\n+        shape[1] / (shapePerWarp[1] * 2) / ret[1]) {\n+      if (ret[0] < shape[0] / shapePerWarp[0]) {\n+        ret[0] *= 2;\n+      } else\n         ret[1] *= 2;\n-      }\n-    } while (true);\n-    return ret;\n+    } else {\n+      ret[1] *= 2;\n+    }\n+  } while (true);\n+  return ret;\n }\n \n } // namespace\n \n+class OptimizeBlockedToShared : public mlir::RewritePattern {\n+public:\n+  OptimizeBlockedToShared(mlir::MLIRContext *context)\n+      : RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(), 1,\n+                       context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto srcType = cvt.getOperand().getType().cast<RankedTensorType>();\n+    auto dstType = cvt.getResult().getType().cast<RankedTensorType>();\n+    auto srcBlockedLayout =\n+        srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n+    auto dstSharedLayout =\n+        dstType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n+    if (!srcBlockedLayout || !dstSharedLayout)\n+      return failure();\n+    if (srcBlockedLayout.getOrder() == dstSharedLayout.getOrder())\n+      return failure();\n+    // For now only works if single use is transpose\n+    // TODO: rematerialize #shared uses\n+    auto users = op->getUsers();\n+    if (std::distance(users.begin(), users.end()) != 1 ||\n+        !isa<triton::TransOp>(*users.begin()))\n+      return failure();\n+\n+    auto tmpShared = triton::gpu::SharedEncodingAttr::get(\n+        op->getContext(), dstSharedLayout.getVec(),\n+        dstSharedLayout.getPerPhase(), dstSharedLayout.getMaxPhase(),\n+        srcBlockedLayout.getOrder());\n+    auto tmpType = RankedTensorType::get(srcType.getShape(),\n+                                         srcType.getElementType(), tmpShared);\n+    auto tmpCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+        op->getLoc(), tmpType, cvt.getOperand());\n+\n+    auto newDstType = RankedTensorType::get(\n+        users.begin()->getResultTypes()[0].cast<RankedTensorType>().getShape(),\n+        srcType.getElementType(), dstSharedLayout);\n+\n+    auto newTrans = rewriter.create<triton::TransOp>(op->getLoc(), newDstType,\n+                                                     tmpCvt.getResult());\n+\n+    rewriter.replaceOp(*users.begin(), newTrans.getResult());\n+    return success();\n+  }\n+};\n+\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n \n@@ -753,6 +809,7 @@ class TritonGPUCombineOpsPass\n \n     mlir::RewritePatternSet patterns(context);\n \n+    patterns.add<OptimizeBlockedToShared>(context);\n     patterns.add<SimplifyConversion>(context);\n     patterns.add<DecomposeDotOperand>(context);\n     patterns.add<RematerializeBackward>(context);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -130,10 +130,10 @@ LogicalResult Prefetcher::initialize() {\n \n   if (dotsInFor.empty())\n     return failure();\n-  \n+\n   // TODO: segfault (original for still has uses)\n   // when used in flash attention that has 2 dots in the loop\n-  if(dotsInFor.size() > 1)\n+  if (dotsInFor.size() > 1)\n     return failure();\n \n   // returns source of cvt"}, {"filename": "python/setup.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -25,8 +25,8 @@ def get_build_type():\n     elif check_env_flag(\"REL_WITH_DEB_INFO\"):\n         return \"RelWithDebInfo\"\n     else:\n-        return \"Debug\"\n-        # TODO(Keren): Restore this before we merge into master\n+        return \"RelWithDebInfo\"\n+        # TODO: change to release when stable enough\n         #return \"Release\"\n \n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 16, "deletions": 7, "changes": 23, "file_content_changes": "@@ -1085,6 +1085,16 @@ void init_triton_ir(py::module &&m) {\n                  mlir::RankedTensorType::get(shape, lhsType.getElementType()),\n                  lhs, rhs);\n            })\n+      .def(\"create_trans\",\n+           [](mlir::OpBuilder &self, mlir::Value &arg) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             auto argType = arg.getType().dyn_cast<mlir::RankedTensorType>();\n+             auto argEltType = argType.getElementType();\n+             std::vector<int64_t> retShape = argType.getShape();\n+             std::reverse(retShape.begin(), retShape.end());\n+             return self.create<mlir::triton::TransOp>(\n+                 loc, mlir::RankedTensorType::get(retShape, argEltType), arg);\n+           })\n       .def(\"create_broadcast\",\n            [](mlir::OpBuilder &self, mlir::Value &arg,\n               std::vector<int64_t> &shape) -> mlir::Value {\n@@ -1251,13 +1261,12 @@ void init_triton_ir(py::module &&m) {\n                                        llvm::StringRef(prefix)),\n                  values);\n            })\n-       // Undef\n-          .def(\"create_undef\",\n-               [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n-               auto loc = self.getUnknownLoc();\n-               return self.create<::mlir::LLVM::UndefOp>(loc, type);\n-          })    \n-       ;\n+      // Undef\n+      .def(\"create_undef\",\n+           [](mlir::OpBuilder &self, mlir::Type &type) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<::mlir::LLVM::UndefOp>(loc, type);\n+           });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")\n       .def(py::init<mlir::MLIRContext *>())"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 140, "deletions": 147, "changes": 287, "file_content_changes": "@@ -667,7 +667,6 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n             tl.atomic_add(Z + off1, z)\n     rs = RandomState(17)\n     x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n-    print(x)\n     # reference result\n     z_ref = np.sum(x, axis=axis, keepdims=False)\n     # triton result\n@@ -677,36 +676,7 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n     kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n-# def test_atomic_cas():\n-#     # 1. make sure that atomic_cas changes the original value (Lock)\n-#     @triton.jit\n-#     def change_value(Lock):\n-#         tl.atomic_cas(Lock, 0, 1)\n-\n-#     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n-#     change_value[(1,)](Lock)\n-\n-#     assert (Lock[0] == 1)\n-\n-#     # 2. only one block enters the critical section\n-#     @triton.jit\n-#     def serialized_add(data, Lock):\n-#         ptrs = data + tl.arange(0, 128)\n-#         while tl.atomic_cas(Lock, 0, 1) == 1:\n-#             pass\n-\n-#         tl.store(ptrs, tl.load(ptrs) + 1.0)\n-\n-#         # release lock\n-#         tl.atomic_xchg(Lock, 0)\n-\n-#     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n-#     data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n-#     ref = torch.full((128,), 64.0)\n-#     serialized_add[(64,)](data, Lock)\n-#     triton.testing.assert_almost_equal(data, ref)\n-\n-def test_simple_atomic_cas():\n+def test_atomic_cas():\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n     def change_value(Lock):\n@@ -717,6 +687,25 @@ def change_value(Lock):\n \n     assert (Lock[0] == 1)\n \n+    # 2. only one block enters the critical section\n+    @triton.jit\n+    def serialized_add(data, Lock):\n+        ptrs = data + tl.arange(0, 128)\n+        while tl.atomic_cas(Lock, 0, 1) == 1:\n+            pass\n+\n+        tl.store(ptrs, tl.load(ptrs) + 1.0)\n+\n+        # release lock\n+        tl.atomic_xchg(Lock, 0)\n+\n+    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+    ref = torch.full((128,), 64.0)\n+    serialized_add[(64,)](data, Lock)\n+    triton.testing.assert_almost_equal(data, ref)\n+\n+\n # # ---------------\n # # test cast\n # # ---------------\n@@ -1077,122 +1066,126 @@ def kernel(X, stride_xm, stride_xn,\n # # ---------------\n \n \n-# @pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n-#                          [(epilogue, allow_tf32, dtype)\n-#                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n-#                           for allow_tf32 in [True, False]\n-#                           for dtype in ['float16']\n-#                           if not (allow_tf32 and (dtype in ['float16']))])\n-# def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n-#     cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n-#     if cc < 80:\n-#         if dtype == 'int8':\n-#             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n-#         elif dtype == 'float32' and allow_tf32:\n-#             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n-\n-#     M, N, K = 128, 128, 64\n-#     num_warps = 8\n-#     trans_a, trans_b = False, False\n-\n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, stride_xm, stride_xk,\n-#                Y, stride_yk, stride_yn,\n-#                W, stride_wn, stride_wl,\n-#                Z, stride_zm, stride_zn,\n-#                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n-#                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n-#                ALLOW_TF32: tl.constexpr,\n-#                DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n-#                TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n-#         off_m = tl.arange(0, BLOCK_M)\n-#         off_n = tl.arange(0, BLOCK_N)\n-#         off_l = tl.arange(0, BLOCK_N)\n-#         off_k = tl.arange(0, BLOCK_K)\n-#         Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n-#         Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n-#         Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n-#         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n-#         z = tl.dot(tl.load(Xs), tl.load(Ys), trans_a=TRANS_A, trans_b=TRANS_B, allow_tf32=ALLOW_TF32)\n-#         if ADD_MATRIX:\n-#             z += tl.load(Zs)\n-#         if ADD_ROWS:\n-#             ZRs = Z + off_m * stride_zm\n-#             z += tl.load(ZRs)[:, None]\n-#         if ADD_COLS:\n-#             ZCs = Z + off_n * stride_zn\n-#             z += tl.load(ZCs)[None, :]\n-#         if DO_SOFTMAX:\n-#             max = tl.max(z, 1)\n-#             z = z - max[:, None]\n-#             num = tl.exp(z)\n-#             den = tl.sum(num, 1)\n-#             z = num / den[:, None]\n-#         if CHAIN_DOT:\n-#             # tl.store(Zs, z)\n-#             # tl.debug_barrier()\n-#             z = tl.dot(z.to(tl.float16), tl.load(Ws), trans_a=TRANS_A)\n-#         tl.store(Zs, z)\n-#     # input\n-#     rs = RandomState(17)\n-#     x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n-#     y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n-#     w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n-#     if allow_tf32:\n-#         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-#         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-#         w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-#     x_tri = to_triton(x, device=device)\n-#     y_tri = to_triton(y, device=device)\n-#     w_tri = to_triton(w, device=device)\n-#     # triton result\n-#     z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n-#     z_tri = to_triton(z, device=device)\n-#     if epilogue == 'trans':\n-#         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n-#     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n-#                          y_tri, y_tri.stride(0), y_tri.stride(1),\n-#                          w_tri, w_tri.stride(0), w_tri.stride(1),\n-#                          z_tri, z_tri.stride(0), z_tri.stride(1),\n-#                          TRANS_A=trans_a, TRANS_B=trans_b,\n-#                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n-#                          ADD_MATRIX=epilogue == 'add-matrix',\n-#                          ADD_ROWS=epilogue == 'add-rows',\n-#                          ADD_COLS=epilogue == 'add-cols',\n-#                          DO_SOFTMAX=epilogue == 'softmax',\n-#                          CHAIN_DOT=epilogue == 'chain-dot',\n-#                          ALLOW_TF32=allow_tf32,\n-#                          num_warps=num_warps)\n-#     # torch result\n-#     x_ref = x.T if trans_a else x\n-#     y_ref = y.T if trans_b else y\n-#     z_ref = np.matmul(x_ref, y_ref)\n-#     if epilogue == 'add-matrix':\n-#         z_ref += z\n-#     if epilogue == 'add-rows':\n-#         z_ref += z[:, 0][:, None]\n-#     if epilogue == 'add-cols':\n-#         z_ref += z[0, :][None, :]\n-#     if epilogue == 'softmax':\n-#         num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n-#         denom = np.sum(num, axis=-1, keepdims=True)\n-#         z_ref = num / denom\n-#     if epilogue == 'chain-dot':\n-#         z_ref = np.matmul(z_ref.T if trans_a else z_ref, w)\n-#     # compare\n-#     # print(z_ref[:,0], z_tri[:,0])\n-#     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n-#     # make sure ld/st are vectorized\n-#     ptx = pgm.asm['ptx']\n-#     assert 'ld.global.v4' in ptx\n-#     assert 'st.global.v4' in ptx\n-#     if allow_tf32:\n-#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n-#     elif dtype == 'float32':\n-#         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n-#     elif dtype == 'int8':\n-#         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n+@pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n+                         [(epilogue, allow_tf32, dtype)\n+                          for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n+                          for allow_tf32 in [True, False]\n+                          for dtype in ['float16']\n+                          if not (allow_tf32 and (dtype in ['float16']))])\n+def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n+    capability = torch.cuda.get_device_capability()\n+    if capability[0] < 80:\n+        if dtype == 'int8':\n+            pytest.skip(\"Only test int8 on devices with sm >= 80\")\n+        elif dtype == 'float32' and allow_tf32:\n+            pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n+\n+    M, N, K = 64, 64, 64\n+    num_warps = 4\n+    trans_a, trans_b = False, False\n+\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, stride_xm, stride_xk,\n+               Y, stride_yk, stride_yn,\n+               W, stride_wn, stride_wl,\n+               Z, stride_zm, stride_zn,\n+               BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n+               ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n+               ALLOW_TF32: tl.constexpr,\n+               DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n+               TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n+        off_m = tl.arange(0, BLOCK_M)\n+        off_n = tl.arange(0, BLOCK_N)\n+        off_l = tl.arange(0, BLOCK_N)\n+        off_k = tl.arange(0, BLOCK_K)\n+        Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n+        Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n+        Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n+        Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n+        x = tl.load(Xs)\n+        y = tl.load(Ys)\n+        x = tl.trans(x) if TRANS_A else x\n+        y = tl.trans(y) if TRANS_B else y\n+        z = tl.dot(x, y, allow_tf32=ALLOW_TF32)\n+        if ADD_MATRIX:\n+            z += tl.load(Zs)\n+        if ADD_ROWS:\n+            ZRs = Z + off_m * stride_zm\n+            z += tl.load(ZRs)[:, None]\n+        if ADD_COLS:\n+            ZCs = Z + off_n * stride_zn\n+            z += tl.load(ZCs)[None, :]\n+        if DO_SOFTMAX:\n+            max = tl.max(z, 1)\n+            z = z - max[:, None]\n+            num = tl.exp(z)\n+            den = tl.sum(num, 1)\n+            z = num / den[:, None]\n+        if CHAIN_DOT:\n+            # tl.store(Zs, z)\n+            # tl.debug_barrier()\n+            z = tl.dot(tl.trans(z.to(tl.float16)), tl.load(Ws))\n+        tl.store(Zs, z)\n+    # input\n+    rs = RandomState(17)\n+    x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n+    y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n+    w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n+    if allow_tf32:\n+        x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+        y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+        w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    w_tri = to_triton(w, device=device)\n+    # triton result\n+    z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n+    z_tri = to_triton(z, device=device)\n+    if epilogue == 'trans':\n+        z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n+    pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n+                         y_tri, y_tri.stride(0), y_tri.stride(1),\n+                         w_tri, w_tri.stride(0), w_tri.stride(1),\n+                         z_tri, z_tri.stride(0), z_tri.stride(1),\n+                         TRANS_A=trans_a, TRANS_B=trans_b,\n+                         BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n+                         ADD_MATRIX=epilogue == 'add-matrix',\n+                         ADD_ROWS=epilogue == 'add-rows',\n+                         ADD_COLS=epilogue == 'add-cols',\n+                         DO_SOFTMAX=epilogue == 'softmax',\n+                         CHAIN_DOT=epilogue == 'chain-dot',\n+                         ALLOW_TF32=allow_tf32,\n+                         num_warps=num_warps)\n+    # torch result\n+    x_ref = x.T if trans_a else x\n+    y_ref = y.T if trans_b else y\n+    z_ref = np.matmul(x_ref, y_ref)\n+    if epilogue == 'add-matrix':\n+        z_ref += z\n+    if epilogue == 'add-rows':\n+        z_ref += z[:, 0][:, None]\n+    if epilogue == 'add-cols':\n+        z_ref += z[0, :][None, :]\n+    if epilogue == 'softmax':\n+        num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n+        denom = np.sum(num, axis=-1, keepdims=True)\n+        z_ref = num / denom\n+    if epilogue == 'chain-dot':\n+        z_ref = np.matmul(z_ref.T, w)\n+    # compare\n+    # print(z_ref[:,0], z_tri[:,0])\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    # make sure ld/st are vectorized\n+    ptx = pgm.asm['ptx']\n+    assert 'ld.global.v4' in ptx\n+    assert 'st.global.v4' in ptx\n+    if allow_tf32:\n+        assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n+    elif dtype == 'float32':\n+        assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' not in ptx\n+    elif dtype == 'int8':\n+        assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n \n # def test_dot_without_load():"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 24, "deletions": 13, "changes": 37, "file_content_changes": "@@ -220,14 +220,17 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n \n \n-@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n-    [32, 32, 16, 4, 32, 32, 16],\n-    [32, 16, 16, 4, 32, 32, 16],\n-    [128, 8, 8, 4, 32, 32, 16],\n-    # TODO[Superjomn]: fix it later\n-    # [127, 41, 43, 4, 32, 32, 16],\n+@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K,allow_tf32', [\n+    [32, 32, 16, 4, 32, 32, 16, False],\n+    [32, 32, 16, 4, 32, 32, 16, True],\n+    [32, 16, 16, 4, 32, 32, 16, False],\n+    [32, 16, 16, 4, 32, 32, 16, True],\n+    [127, 41, 43, 4, 32, 32, 16, False],\n+    [127, 41, 43, 4, 32, 32, 16, True],\n+    [128, 8, 8, 4, 32, 32, 16, False],\n+    [128, 8, 8, 4, 32, 32, 16, True]\n ])\n-def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+def test_gemm_fp32(M, N, K, num_warps, block_M, block_N, block_K, allow_tf32):\n     @triton.jit\n     def matmul_kernel(\n         a_ptr, b_ptr, c_ptr,\n@@ -236,6 +239,7 @@ def matmul_kernel(\n         stride_bk, stride_bn,\n         stride_cm, stride_cn,\n         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+        ALLOW_TF32: tl.constexpr\n     ):\n         pid = tl.program_id(axis=0)\n         # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n@@ -253,10 +257,9 @@ def matmul_kernel(\n         for k in range(0, K, BLOCK_SIZE_K):\n             a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n             b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n-            a = tl.load(a_ptrs, a_mask)\n-            b = tl.load(b_ptrs, b_mask)\n-            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n-            accumulator += tl.dot(a, b, allow_tf32=False)\n+            a = tl.load(a_ptrs, a_mask, other=0.0)\n+            b = tl.load(b_ptrs, b_mask, other=0.0)\n+            accumulator += tl.dot(a, b, allow_tf32=ALLOW_TF32)\n             a_ptrs += BLOCK_SIZE_K * stride_ak\n             b_ptrs += BLOCK_SIZE_K * stride_bk\n             offs_k += BLOCK_SIZE_K\n@@ -267,6 +270,9 @@ def matmul_kernel(\n         c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n         tl.store(c_ptrs, accumulator, c_mask)\n \n+    # Configure the pytorch counterpart\n+    torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n+\n     a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n     c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n@@ -277,11 +283,15 @@ def matmul_kernel(\n                         stride_am=a.stride(0), stride_ak=a.stride(1),\n                         stride_bk=b.stride(0), stride_bn=b.stride(1),\n                         stride_cm=c.stride(0), stride_cn=c.stride(1),\n-                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n+                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K, ALLOW_TF32=allow_tf32)\n \n     golden = torch.matmul(a, b)\n     golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n-    torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))\n+    if allow_tf32:\n+        # TF32 is not accurate enough\n+        torch.testing.assert_close(c, golden, rtol=max(1e-2, 1.5 * golden_rel_err), atol=max(1e-2, 1.5 * golden_abs_err))\n+    else:\n+        torch.testing.assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err))\n \n \n @pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n@@ -290,6 +300,7 @@ def matmul_kernel(\n         [16, 16, 16],\n         [16, 16, 32],\n         [32, 16, 16],\n+        [128, 16, 16],\n         # diabled due to the wrong vec in shared layout, the backend works if right vec passed.\n         # [32, 32, 32],\n     ]"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 14, "deletions": 5, "changes": 19, "file_content_changes": "@@ -359,7 +359,7 @@ def visit_If(self, node):\n             cond = cond.to(triton.language.int1, _builder=self.builder)\n             with enter_sub_region(self) as sr:\n                 liveins, ip_block = sr\n-\n+                liveins_copy = liveins.copy()\n                 then_block = self.builder.create_block()\n                 self.builder.set_insertion_point_to_start(then_block)\n                 self.visit_compound_statement(node.body)\n@@ -394,7 +394,15 @@ def visit_If(self, node):\n                             if then_defs[then_name].type == else_defs[else_name].type:\n                                 names.append(then_name)\n                                 ret_types.append(then_defs[then_name].type)\n-\n+                \n+                # defined in else block but not in then block\n+                # to find in parent scope and yield them\n+                for else_name in else_defs:\n+                    if else_name in liveins and else_name not in then_defs:\n+                        if else_defs[else_name].type == liveins[else_name].type:\n+                            names.append(else_name)\n+                            ret_types.append(else_defs[else_name].type)\n+                            then_defs[else_name] = liveins_copy[else_name]\n                 self.builder.set_insertion_point_to_end(ip_block)\n \n                 if then_defs or node.orelse:  # with else block\n@@ -528,8 +536,7 @@ def visit_While(self, node):\n                                                                 [ty.to_ir(self.builder) for ty in ret_types])\n             loop_block.merge_block_before(after_block)\n             self.builder.set_insertion_point_to_end(after_block)\n-            if len(yields) > 0:\n-                self.builder.create_yield_op([y.handle for y in yields])\n+            self.builder.create_yield_op([y.handle for y in yields])\n \n         # update global uses in while_op\n         for i, name in enumerate(names):\n@@ -1360,7 +1367,7 @@ def make_hash(fn, **kwargs):\n     \"ptx\": ptx_prototype_pattern,\n }\n \n-mlir_arg_type_pattern = r'%\\w+: ([^,\\s]+)(?: \\{\\S+ = \\S+ : \\S+\\})?,?'\n+mlir_arg_type_pattern = r'%\\w+: ([^,^\\)\\s]+)(?: \\{\\S+ = \\S+ : \\S+\\})?,?'\n ptx_arg_type_pattern = r\"\\.param\\s+\\.(\\w+)\"\n arg_type_pattern = {\n     \"ttir\": mlir_arg_type_pattern,\n@@ -1417,7 +1424,9 @@ def compile(fn, **kwargs):\n         import re\n         match = re.search(prototype_pattern[ir], src, re.MULTILINE)\n         name, signature = match.group(1), match.group(2)\n+        print(name, signature)\n         types = re.findall(arg_type_pattern[ir], signature)\n+        print(types)\n         param_tys = [convert_type_repr(ty) for ty in types]\n         signature = {k: v for k, v in enumerate(param_tys)}\n         first_stage = list(stages.keys()).index(ir)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -614,6 +614,7 @@ def __getitem__(self, slices, _builder=None):\n                 assert False, \"unsupported\"\n         return ret\n \n+\n     # x[:, None, :, None]\n     # x = expand_dims(x, axis=1)\n     # x = expand_dims(x, axis=2)\n@@ -737,6 +738,9 @@ def broadcast_to(input, shape, _builder=None):\n     \"\"\"\n     return semantic.broadcast_impl_shape(input, shape, _builder)\n \n+@builtin\n+def trans(input, _builder=None):\n+    return semantic.trans(input, _builder)\n \n @builtin\n def cat(input, other, _builder=None):"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -502,6 +502,11 @@ def cat(lhs: tl.tensor, rhs: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # TODO: check types\n     return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), lhs.type)\n \n+def trans(input: tl.tensor, builder: ir.builder) -> tl.tensor:\n+    if len(input.shape) != 2:\n+        raise ValueError(\"Only 2D tensors can be transposed\")\n+    ret_type = tl.block_type(input.type.scalar, [input.shape[1], input.shape[0]])\n+    return tl.tensor(builder.create_trans(input.handle), ret_type)\n \n def broadcast_impl_shape(input: tl.tensor,\n                          shape: List[int],"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 21, "deletions": 21, "changes": 42, "file_content_changes": "@@ -32,7 +32,7 @@ def _fwd_kernel(\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n     off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n     off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n@@ -50,7 +50,7 @@ def _fwd_kernel(\n         # -- compute qk ----\n         k = tl.load(k_ptrs + start_n * stride_kn)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, k)\n+        qk += tl.dot(q, tl.trans(k))\n         qk *= sm_scale\n         qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n         # -- compute m_ij, p, l_ij\n@@ -165,26 +165,26 @@ def _bwd_kernel(\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.dot(q, k, trans_b=True)\n+            qk = tl.dot(q, tl.trans(k))\n             qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n             m = tl.load(m_ptrs + offs_m_curr)\n             p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n-            dv += tl.dot(p.to(tl.float16), do, trans_a=True)\n+            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n             # compute dp = dot(v, do)\n             Di = tl.load(D_ptrs + offs_m_curr)\n             dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n-            dp += tl.dot(do, v, trans_b=True)\n+            dp += tl.dot(do, tl.trans(v))\n             # compute ds = p * (dp - delta[:, None])\n             ds = p * dp * sm_scale\n             # compute dk = dot(ds.T, q)\n-            dk += tl.dot(ds.to(tl.float16), q, trans_a=True)\n-            # # compute dq\n+            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            # compute dq\n             dq = tl.load(dq_ptrs)\n             dq += tl.dot(ds.to(tl.float16), k)\n             tl.store(dq_ptrs, dq)\n-            # # increment pointers\n+            # increment pointers\n             dq_ptrs += BLOCK_M * stride_qm\n             q_ptrs += BLOCK_M * stride_qm\n             do_ptrs += BLOCK_M * stride_qm\n@@ -273,7 +273,7 @@ def backward(ctx, do):\n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.1).requires_grad_()\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n     k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n     v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n     sm_scale = 0.2\n@@ -287,23 +287,23 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     p = torch.softmax(p.float(), dim=-1).half()\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n-    # ref_out.backward(dout)\n-    # ref_dv, v.grad = v.grad.clone(), None\n-    # ref_dk, k.grad = k.grad.clone(), None\n-    # ref_dq, q.grad = q.grad.clone(), None\n+    ref_out.backward(dout)\n+    ref_dv, v.grad = v.grad.clone(), None\n+    ref_dk, k.grad = k.grad.clone(), None\n+    ref_dq, q.grad = q.grad.clone(), None\n     # # triton implementation\n     tri_out = attention(q, k, v, sm_scale)\n     # print(ref_out)\n     # print(tri_out)\n-    # tri_out.backward(dout)\n-    # tri_dv, v.grad = v.grad.clone(), None\n-    # tri_dk, k.grad = k.grad.clone(), None\n-    # tri_dq, q.grad = q.grad.clone(), None\n+    tri_out.backward(dout)\n+    tri_dv, v.grad = v.grad.clone(), None\n+    tri_dk, k.grad = k.grad.clone(), None\n+    tri_dq, q.grad = q.grad.clone(), None\n     # compare\n     triton.testing.assert_almost_equal(ref_out, tri_out)\n-    # triton.testing.assert_almost_equal(ref_dv, tri_dv)\n-    # triton.testing.assert_almost_equal(ref_dk, tri_dk)\n-    # triton.testing.assert_almost_equal(ref_dq, tri_dq)\n+    triton.testing.assert_almost_equal(ref_dv, tri_dv)\n+    triton.testing.assert_almost_equal(ref_dk, tri_dk)\n+    triton.testing.assert_almost_equal(ref_dq, tri_dq)\n \n BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n # vary seq length for fixed head and batch=4\n@@ -350,4 +350,4 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n         return ms\n \n-bench_flash_attention.run(save_path='.', print_data=True)\n\\ No newline at end of file\n+# bench_flash_attention.run(save_path='.', print_data=True)\n\\ No newline at end of file"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 46, "deletions": 2, "changes": 48, "file_content_changes": "@@ -261,14 +261,58 @@ func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B :\n   %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n-    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n     // CHECK-NEXT: Membar 6\n-    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #AL>\n     scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: Membar 9\n   %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }\n \n+// Although cst2 is not an argument of scf.yield, its memory is reused by cst1.\n+// So we need a barrier both before and after cst1\n+// CHECK-LABEL: for_reuse\n+func @for_reuse(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: Membar 2\n+  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    // CHECK-NEXT: Membar 5\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    // CHECK-NEXT: Membar 7\n+    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  // CHECK-NEXT: Membar 10\n+  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  return\n+}\n+\n+\n+// CHECK-LABEL: for_reuse_nested\n+func @for_reuse_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  // CHECK-NEXT: Membar 2\n+  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    // CHECK-NEXT: Membar 5\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    %a_shared_next, %b_shared_next, %c_shared_next = scf.for %ivv = %lb to %ub step %step iter_args(%a_shared_nested = %a_shared_init, %b_shared_nested = %b_shared_init, %c_shared_nested = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+      // CHECK-NEXT: Membar 7\n+      %cst2 = tt.cat %a_shared_nested, %b_shared_nested {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+      scf.yield %c_shared_nested, %a_shared_nested, %b_shared_nested : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+    }\n+    scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n+  }\n+  // CHECK-NEXT: Membar 11\n+  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n+  return\n+}\n+\n }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -923,6 +923,45 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+#mma = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: matmul_tf32dot\n+  func @matmul_tf32dot(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a:tensor<32x16xf32, #shared>, %b:tensor<16x32xf32, #shared>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    // CHECK-SAME: (f32, f32, f32, f32)\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n+    // CHECK-SAME: (f32, f32, f32, f32)\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<32x16xf32, #shared>) -> tensor<32x16xf32, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<16x32xf32, #shared>) -> tensor<16x32xf32, #dot_operand_b>\n+\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<32x16xf32, #dot_operand_a> * tensor<16x32xf32, #dot_operand_b> -> tensor<32x32xf32, #mma>\n+    %38 = triton_gpu.convert_layout %28 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %38 : tensor<32x32xf32, #blocked>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -26,7 +26,9 @@ struct TestMembarPass\n     auto op_name = SymbolTable::getSymbolName(operation).getValue().str();\n     os << op_name << \"\\n\";\n     Allocation allocation(operation);\n-    MembarAnalysis analysis(&allocation);\n+    MembarAnalysis membarPass(&allocation);\n+    membarPass.run();\n+\n     size_t operationId = 0;\n     operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n       if (isa<gpu::BarrierOp>(op)) {"}]