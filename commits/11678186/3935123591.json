[{"filename": "python/triton/testing.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -151,8 +151,8 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n     torch.cuda.synchronize()\n     estimate_ms = start_event.elapsed_time(end_event) / 5\n     # compute number of warmup and repeat\n-    n_warmup = min(500, max(1, int(warmup / estimate_ms)))\n-    n_repeat = min(500, max(1, int(rep / estimate_ms)))\n+    n_warmup = max(1, int(warmup / estimate_ms))\n+    n_repeat = max(1, int(rep / estimate_ms))\n     # We maintain a buffer of 256 MB that we clear\n     # before each kernel call to make sure that the L2\n     # doesn't contain any input data before the run"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 2, "deletions": 14, "changes": 16, "file_content_changes": "@@ -240,11 +240,7 @@ def matmul_kernel(\n     tl.store(c_ptrs, c, mask=c_mask)\n \n \n-# matmul_kernel = triton.compile(\"./matmul-2.ttgir\", num_warps=8)\n-\n # we can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`\n-\n-\n @triton.jit\n def leaky_relu(x):\n     return tl.where(x >= 0, x, 0.01 * x)\n@@ -279,14 +275,6 @@ def matmul(a, b, activation=None):\n         c.stride(0), c.stride(1),\n         ACTIVATION=activation,\n     )\n-    # grid = (triton.cdiv(M, 128) * triton.cdiv(N, 256), 1, 1)\n-    # matmul_kernel[grid](\n-    #     a.data_ptr(), b.data_ptr(), c.data_ptr(),\n-    #     M, N, K,\n-    #     a.stride(0),\n-    #     b.stride(0),\n-    #     c.stride(0)\n-    # )\n     return c\n \n \n@@ -339,9 +327,9 @@ def benchmark(M, N, K, provider):\n     a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n     b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n     if provider == 'cublas':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=500)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), rep=100)\n     if provider == 'triton':\n-        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=500)\n+        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), rep=100)\n     perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n     return perf(ms), perf(max_ms), perf(min_ms)\n "}]