[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 8, "deletions": 5, "changes": 13, "file_content_changes": "@@ -158,12 +158,15 @@ def test_elementwise(N):\n # Flash-Attention\n #######################\n \n+\n flash_attention_data = {\n     \"a100\": {\n         (4, 48, 4096, 64, 'forward', 'float16'): 0.37,\n         (4, 48, 4096, 64, 'backward', 'float16'): 0.25,\n     }\n }\n+\n+\n @pytest.mark.parametrize(\"Z, H, N_CTX, D_HEAD\", [[4, 48, 4096, 64]])\n @pytest.mark.parametrize(\"mode\", ['forward', 'backward'])\n @pytest.mark.parametrize(\"dtype_str\", ['float16'])\n@@ -187,14 +190,14 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n         fn = lambda: o.backward(do, retain_graph=True)\n     ms = triton.testing.do_bench(fn, percentiles=None, warmup=100, rep=300)\n     # compute flops\n-    flops_per_matmul = 2.*Z*H*N_CTX*N_CTX*D_HEAD*0.5\n-    total_flops = 2*flops_per_matmul\n+    flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n+    total_flops = 2 * flops_per_matmul\n     if is_backward:\n-        total_flops *= 2.5 # 2.0(bwd) + 0.5(recompute)\n-    cur_gpu_perf = total_flops/ms * 1e-9\n+        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n+    cur_gpu_perf = total_flops / ms * 1e-9\n     # maximum flops\n     cur_sm_clock = nvsmi(['clocks.current.sm'])[0]\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n-    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)\n\\ No newline at end of file\n+    triton.testing.assert_almost_equal(cur_gpu_util, ref_gpu_util, decimal=2)"}]