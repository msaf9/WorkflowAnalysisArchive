[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 375, "deletions": 217, "changes": 592, "file_content_changes": "@@ -67,8 +67,6 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n                                           builder.getIntegerAttr(ty, value));\n }\n \n-// Add other specification if needed...\n-\n } // namespace\n \n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n@@ -89,8 +87,12 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define load(...) rewriter.create<LLVM::LoadOp>(loc, __VA_ARGS__)\n #define store(val, ptr) rewriter.create<LLVM::StoreOp>(loc, val, ptr)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n+#define barrier rewriter.create<mlir::gpu::BarrierOp>(loc)\n+#define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define i32_ty rewriter.getIntegerType(32)\n #define vec_ty(type, num) VectorType::get(num, type)\n+#define void_ty LLVM::LLVMVoidType::get(ctx)\n+#define struct_ty(...) LLVM::LLVMStructType::getLiteral(__VA_ARGS__)\n \n // Creator for constant\n #define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n@@ -109,6 +111,18 @@ namespace type = mlir::triton::type;\n \n class TritonGPUToLLVMTypeConverter;\n \n+// TODO: keep these before we have better debug log utilities\n+template <typename T>\n+void print_array(ArrayRef<T> array, const std::string &str) {\n+  std::cout << str << \": \";\n+  for (const T &e : array)\n+    std::cout << e << \",\";\n+  std::cout << std::endl;\n+}\n+template <typename T> void print_scalar(const T &e, const std::string &str) {\n+  std::cout << str << \": \" << e << std::endl;\n+}\n+\n // FuncOpConversion/FuncOpConversionBase is borrowed from\n // https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L276\n // since it is not exposed on header files in mlir v14\n@@ -360,6 +374,13 @@ class ConvertTritonGPUOpToLLVMPattern\n     return threadId;\n   }\n \n+  Value createIndexConst(ConversionPatternRewriter &rewriter, Location loc,\n+                         int64_t value) const {\n+    return rewriter.create<LLVM::ConstantOp>(\n+        loc, this->getTypeConverter()->getIndexType(),\n+        rewriter.getIntegerAttr(rewriter.getIndexType(), value));\n+  }\n+\n   // Convert an \\param index to a multi-dim coordinate given \\param shape and\n   // \\param order.\n   SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n@@ -859,7 +880,7 @@ struct StoreOpConversion\n         asmArgList->listAppend(ptxBuilder.newOperand(llWord, constraint));\n       }\n \n-      // TODO(Superjomn) Need to check masks before vectorize the load for all\n+      // TODO(Superjomn) Need to check masks before vectorize the load for\n       // the values share one predicate? Here assume all the mask values are\n       // the same.\n       Value maskVal = llMask ? maskElems[vecStart] : int_val(1, 1);\n@@ -1060,7 +1081,6 @@ struct LoadOpConversion\n   LogicalResult\n   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-\n     Value ptr = op.ptr();\n     Value mask = op.mask();\n     Value other = op.other();\n@@ -1413,87 +1433,24 @@ struct ConvertLayoutOpConversion\n   LogicalResult\n   matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto loc = op.getLoc();\n     Value src = op.src();\n     Value dst = op.result();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n+    if (srcLayout.isa<BlockedEncodingAttr>() &&\n+        dstLayout.isa<SharedEncodingAttr>()) {\n+      return lowerBlockedToShared(op, adaptor, rewriter);\n+    }\n     if ((!srcLayout.isa<BlockedEncodingAttr>() &&\n          !srcLayout.isa<MmaEncodingAttr>()) ||\n         (!dstLayout.isa<BlockedEncodingAttr>() &&\n          !dstLayout.isa<MmaEncodingAttr>())) {\n       // TODO: to be implemented\n-      llvm::errs() << \"Unsupported ConvertLayout found\";\n       return failure();\n     }\n-    auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n-    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n-    smemBase = bit_cast(elemPtrTy, smemBase);\n-\n-    auto shape = dstTy.getShape();\n-    unsigned rank = dstTy.getRank();\n-    SmallVector<unsigned> numReplicates(rank);\n-    SmallVector<unsigned> inNumCTAsEachRep(rank);\n-    SmallVector<unsigned> outNumCTAsEachRep(rank);\n-    SmallVector<unsigned> inNumCTAs(rank);\n-    SmallVector<unsigned> outNumCTAs(rank);\n-    auto srcShapePerCTA = getShapePerCTA(srcLayout);\n-    auto dstShapePerCTA = getShapePerCTA(dstLayout);\n-    for (unsigned d = 0; d < rank; ++d) {\n-      unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n-      unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n-      unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n-      numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n-      inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n-      outNumCTAsEachRep[d] = maxPerCTA / outPerCTA;\n-      // TODO: confirm this\n-      assert(maxPerCTA % inPerCTA == 0 && maxPerCTA % outPerCTA == 0);\n-      inNumCTAs[d] = ceil<unsigned>(shape[d], inPerCTA);\n-      outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n-    }\n-    // Potentially we need to store for multiple CTAs in this replication\n-    unsigned accumNumReplicates = product<unsigned>(numReplicates);\n-    unsigned elems = getElemsPerThread(srcLayout, srcTy.getShape());\n-    auto vals = getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n-    unsigned inVec = 0;\n-    unsigned outVec = 0;\n-    auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n-\n-    unsigned outElems = getElemsPerThread(dstLayout, shape);\n-    auto outOrd = getOrder(dstLayout);\n-    SmallVector<Value> outVals(outElems);\n-    for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n-      auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n-      rewriter.create<mlir::gpu::BarrierOp>(loc);\n-      if (srcLayout.isa<BlockedEncodingAttr>() ||\n-          srcLayout.isa<MmaEncodingAttr>()) {\n-        processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n-                       multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n-                       smemBase);\n-      } else {\n-        assert(0 && \"ConvertLayout with input layout not implemented\");\n-        return failure();\n-      }\n-      rewriter.create<mlir::gpu::BarrierOp>(loc);\n-      if (dstLayout.isa<BlockedEncodingAttr>() ||\n-          dstLayout.isa<MmaEncodingAttr>()) {\n-        processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n-                       outNumCTAsEachRep, multiDimRepId, outVec, paddedRepShape,\n-                       outOrd, outVals, smemBase);\n-      } else {\n-        assert(0 && \"ConvertLayout with output layout not implemented\");\n-        return failure();\n-      }\n-    }\n-\n-    SmallVector<Type> types(outElems, llvmElemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-    Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n-    rewriter.replaceOp(op, result);\n-    return success();\n+    return lowerDistributedToDistributed(op, adaptor, rewriter);\n   }\n \n private:\n@@ -1508,122 +1465,334 @@ struct ConvertLayoutOpConversion\n     return result;\n   };\n \n-  // shared memory access for blocked or mma layout\n+  // shared memory rd/st for blocked or mma layout with data padding\n   void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n                       bool stNotRd, RankedTensorType type,\n                       ArrayRef<unsigned> numCTAsEachRep,\n                       ArrayRef<unsigned> multiDimRepId, unsigned vec,\n                       ArrayRef<unsigned> paddedRepShape,\n                       ArrayRef<unsigned> outOrd, SmallVector<Value> &vals,\n-                      Value smemBase) const {\n-    unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n-    auto layout = type.getEncoding();\n-    auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n-    auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n-    auto rank = type.getRank();\n-    auto sizePerThread = getSizePerThread(layout);\n-    auto accumSizePerThread = product<unsigned>(sizePerThread);\n-    auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    SmallVector<unsigned> numCTAs(rank);\n-    auto shapePerCTA = getShapePerCTA(layout);\n-    for (unsigned d = 0; d < rank; ++d) {\n-      numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n-    }\n-    auto llvmElemTy = getTypeConverter()->convertType(type.getElementType());\n-    SmallVector<Value> multiDimOffsetFirstElem;\n-    Value mmaGrpId;\n-    Value mmaGrpIdP8;\n-    Value mmaThreadIdInGrpM2;\n-    Value mmaThreadIdInGrpM2P1;\n-    if (blockedLayout) {\n-      multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n-          loc, rewriter, blockedLayout, type.getShape());\n-    } else if (mmaLayout) {\n-      // TODO: simplify these\n-      auto cast = rewriter.create<UnrealizedConversionCastOp>(\n-          loc, TypeRange{llvmIndexTy},\n-          ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(\n-              loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)});\n-      Value threadId = cast.getResult(0);\n-      Value warpSize = createIndexAttrConstant(\n-          rewriter, loc, this->getTypeConverter()->getIndexType(), 32);\n-      Value laneId = rewriter.create<LLVM::URemOp>(loc, threadId, warpSize);\n-      Value fourVal = idx_val(4);\n-      mmaGrpId = rewriter.create<LLVM::UDivOp>(loc, laneId, fourVal);\n-      mmaGrpIdP8 = rewriter.create<LLVM::AddOp>(loc, mmaGrpId, idx_val(8));\n-      Value mmaThreadIdInGrp =\n-          rewriter.create<LLVM::URemOp>(loc, laneId, fourVal);\n-      mmaThreadIdInGrpM2 =\n-          rewriter.create<LLVM::MulOp>(loc, mmaThreadIdInGrp, idx_val(2));\n-      mmaThreadIdInGrpM2P1 =\n-          rewriter.create<LLVM::AddOp>(loc, mmaThreadIdInGrpM2, idx_val(1));\n+                      Value smemBase) const;\n+\n+  // blocked/mma -> blocked/mma.\n+  // Data padding in shared memory to avoid bank confict.\n+  LogicalResult\n+  lowerDistributedToDistributed(triton::gpu::ConvertLayoutOp op,\n+                                OpAdaptor adaptor,\n+                                ConversionPatternRewriter &rewriter) const;\n+\n+  // blocked -> shared.\n+  // Swizzling in shared memory to avoid bank conflict. Normally used for\n+  // A/B operands of dots.\n+  LogicalResult lowerBlockedToShared(triton::gpu::ConvertLayoutOp op,\n+                                     OpAdaptor adaptor,\n+                                     ConversionPatternRewriter &rewriter) const;\n+};\n+\n+void ConvertLayoutOpConversion::processReplica(\n+    Location loc, ConversionPatternRewriter &rewriter, bool stNotRd,\n+    RankedTensorType type, ArrayRef<unsigned> numCTAsEachRep,\n+    ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+    ArrayRef<unsigned> paddedRepShape, ArrayRef<unsigned> outOrd,\n+    SmallVector<Value> &vals, Value smemBase) const {\n+  unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n+  auto layout = type.getEncoding();\n+  auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>();\n+  auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n+  auto rank = type.getRank();\n+  auto sizePerThread = getSizePerThread(layout);\n+  auto accumSizePerThread = product<unsigned>(sizePerThread);\n+  auto llvmIndexTy = getTypeConverter()->getIndexType();\n+  SmallVector<unsigned> numCTAs(rank);\n+  auto shapePerCTA = getShapePerCTA(layout);\n+  for (unsigned d = 0; d < rank; ++d) {\n+    numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n+  }\n+  auto llvmElemTy = getTypeConverter()->convertType(type.getElementType());\n+  SmallVector<Value> multiDimOffsetFirstElem;\n+  SmallVector<Value> mmaColIdx(2);\n+  SmallVector<Value> mmaRowIdx(2);\n+  if (blockedLayout) {\n+    multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n+        loc, rewriter, blockedLayout, type.getShape());\n+  } else if (mmaLayout) {\n+    Value threadId = getThreadId(rewriter, loc);\n+    Value warpSize = idx_val(32);\n+    Value laneId = urem(threadId, warpSize);\n+    Value warpId = udiv(threadId, warpSize);\n+    // auto multiDimWarpId =\n+    //     delinearize(rewriter, loc, warpId, mmaLayout.getWarpsPerCTA());\n+    // TODO: double confirm if its document bug or DotConversion's Bug\n+    SmallVector<Value> multiDimWarpId(2);\n+    multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n+    multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n+    Value four = idx_val(4);\n+    Value mmaGrpId = udiv(laneId, four);\n+    Value mmaGrpIdP8 = add(mmaGrpId, idx_val(8));\n+    Value mmaThreadIdInGrp = urem(laneId, four);\n+    Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, idx_val(2));\n+    Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, idx_val(1));\n+    Value colWarpOffset = mul(multiDimWarpId[0], idx_val(16));\n+    mmaColIdx[0] = add(mmaGrpId, colWarpOffset);\n+    mmaColIdx[1] = add(mmaGrpIdP8, colWarpOffset);\n+    Value rowWarpOffset = mul(multiDimWarpId[1], idx_val(8));\n+    mmaRowIdx[0] = add(mmaThreadIdInGrpM2, rowWarpOffset);\n+    mmaRowIdx[1] = add(mmaThreadIdInGrpM2P1, rowWarpOffset);\n+  }\n+  for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n+    auto multiDimCTAInRepId = getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n+    SmallVector<unsigned> multiDimCTAId(rank);\n+    for (auto it : llvm::enumerate(multiDimCTAInRepId)) {\n+      auto d = it.index();\n+      multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n     }\n-    for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n-      auto multiDimCTAInRepId =\n-          getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n-      SmallVector<unsigned> multiDimCTAId(rank);\n-      for (auto it : llvm::enumerate(multiDimCTAInRepId)) {\n-        auto d = it.index();\n-        multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n-      }\n \n-      unsigned linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs);\n-      // TODO: This is actually redundant index calculation, we should\n-      //       consider of caching the index calculation result in case\n-      //       of performance issue observed.\n-      for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n-        SmallVector<Value> multiDimOffset(rank);\n-        if (blockedLayout) {\n-          SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n-              elemId, blockedLayout.getSizePerThread());\n-          for (unsigned d = 0; d < rank; ++d) {\n-            multiDimOffset[d] = rewriter.create<LLVM::AddOp>(\n-                loc, multiDimOffsetFirstElem[d],\n-                createIndexAttrConstant(rewriter, loc, llvmIndexTy,\n-                                        multiDimCTAInRepId[d] * shapePerCTA[d] +\n-                                            multiDimElemId[d]));\n-          }\n-        } else if (mmaLayout) {\n-          assert(rank == 2);\n-          assert(mmaLayout.getVersion() == 2 &&\n-                 \"mmaLayout ver1 not implemented yet\");\n-          multiDimOffset[0] = elemId < 2 ? mmaGrpId : mmaGrpIdP8;\n-          multiDimOffset[1] =\n-              elemId % 2 == 0 ? mmaThreadIdInGrpM2 : mmaThreadIdInGrpM2P1;\n-        } else {\n-          assert(0 && \"unexpected layout in processReplica\");\n+    unsigned linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs);\n+    // TODO: This is actually redundant index calculation, we should\n+    //       consider of caching the index calculation result in case\n+    //       of performance issue observed.\n+    for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n+      SmallVector<Value> multiDimOffset(rank);\n+      if (blockedLayout) {\n+        SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n+            elemId, blockedLayout.getSizePerThread());\n+        for (unsigned d = 0; d < rank; ++d) {\n+          multiDimOffset[d] =\n+              add(multiDimOffsetFirstElem[d],\n+                  idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                          multiDimElemId[d]));\n         }\n-        Value offset =\n-            linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n-                      reorder<unsigned>(paddedRepShape, outOrd));\n-        auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n-        Value ptr = gep(elemPtrTy, smemBase, offset);\n-        auto vecTy = vec_ty(llvmElemTy, vec);\n-        ptr = bit_cast(LLVM::LLVMPointerType::get(vecTy, 3), ptr);\n-        if (stNotRd) {\n-          Value valVec = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n-          for (unsigned v = 0; v < vec; ++v) {\n-            Value vVal = createIndexAttrConstant(\n-                rewriter, loc, getTypeConverter()->getIndexType(), v);\n-            valVec = insert_element(\n-                vecTy, valVec,\n-                vals[elemId + linearCTAId * accumSizePerThread + v], vVal);\n-          }\n-          store(valVec, ptr);\n-        } else {\n-          Value valVec = load(ptr);\n-          for (unsigned v = 0; v < vec; ++v) {\n-            Value vVal = createIndexAttrConstant(\n-                rewriter, loc, getTypeConverter()->getIndexType(), v);\n-            vals[elemId + linearCTAId * accumSizePerThread + v] =\n-                extract_element(llvmElemTy, valVec, vVal);\n-          }\n+      } else if (mmaLayout) {\n+        assert(rank == 2);\n+        assert(mmaLayout.getVersion() == 2 &&\n+               \"mmaLayout ver1 not implemented yet\");\n+        multiDimOffset[0] = elemId < 2 ? mmaColIdx[0] : mmaColIdx[1];\n+        multiDimOffset[1] = elemId % 2 == 0 ? mmaRowIdx[0] : mmaRowIdx[1];\n+        multiDimOffset[0] = add(\n+            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+        multiDimOffset[1] = add(\n+            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+      } else {\n+        assert(0 && \"unexpected layout in processReplica\");\n+      }\n+      Value offset =\n+          linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n+                    reorder<unsigned>(paddedRepShape, outOrd));\n+      auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+      Value ptr = gep(elemPtrTy, smemBase, offset);\n+      auto vecTy = vec_ty(llvmElemTy, vec);\n+      ptr = bit_cast(ptr_ty(vecTy, 3), ptr);\n+      if (stNotRd) {\n+        Value valVec = undef(vecTy);\n+        for (unsigned v = 0; v < vec; ++v) {\n+          valVec = insert_element(\n+              vecTy, valVec,\n+              vals[elemId + linearCTAId * accumSizePerThread + v], idx_val(v));\n+        }\n+        store(valVec, ptr);\n+      } else {\n+        Value valVec = load(ptr);\n+        for (unsigned v = 0; v < vec; ++v) {\n+          vals[elemId + linearCTAId * accumSizePerThread + v] =\n+              extract_element(llvmElemTy, valVec, idx_val(v));\n         }\n       }\n     }\n   }\n+}\n+\n+LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n+    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto loc = op.getLoc();\n+  Value src = op.src();\n+  Value dst = op.result();\n+  auto srcTy = src.getType().cast<RankedTensorType>();\n+  auto dstTy = dst.getType().cast<RankedTensorType>();\n+  Attribute srcLayout = srcTy.getEncoding();\n+  Attribute dstLayout = dstTy.getEncoding();\n+  auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+  Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n+  auto elemPtrTy = ptr_ty(llvmElemTy, 3);\n+  smemBase = bit_cast(elemPtrTy, smemBase);\n+  auto shape = dstTy.getShape();\n+  unsigned rank = dstTy.getRank();\n+  SmallVector<unsigned> numReplicates(rank);\n+  SmallVector<unsigned> inNumCTAsEachRep(rank);\n+  SmallVector<unsigned> outNumCTAsEachRep(rank);\n+  SmallVector<unsigned> inNumCTAs(rank);\n+  SmallVector<unsigned> outNumCTAs(rank);\n+  auto srcShapePerCTA = getShapePerCTA(srcLayout);\n+  auto dstShapePerCTA = getShapePerCTA(dstLayout);\n+  for (unsigned d = 0; d < rank; ++d) {\n+    unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n+    unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n+    unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n+    numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n+    inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n+    outNumCTAsEachRep[d] = maxPerCTA / outPerCTA;\n+    assert(maxPerCTA % inPerCTA == 0 && maxPerCTA % outPerCTA == 0);\n+    inNumCTAs[d] = ceil<unsigned>(shape[d], inPerCTA);\n+    outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n+  }\n+  // Potentially we need to store for multiple CTAs in this replication\n+  unsigned accumNumReplicates = product<unsigned>(numReplicates);\n+  unsigned elems = getElemsPerThread(srcLayout, srcTy.getShape());\n+  auto vals = getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n+  unsigned inVec = 0;\n+  unsigned outVec = 0;\n+  auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n+\n+  unsigned outElems = getElemsPerThread(dstLayout, shape);\n+  auto outOrd = getOrder(dstLayout);\n+  SmallVector<Value> outVals(outElems);\n+\n+  for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n+    auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n+    barrier;\n+    if (srcLayout.isa<BlockedEncodingAttr>() ||\n+        srcLayout.isa<MmaEncodingAttr>()) {\n+      processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n+                     multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n+                     smemBase);\n+    } else {\n+      assert(0 && \"ConvertLayout with input layout not implemented\");\n+      return failure();\n+    }\n+    barrier;\n+    if (dstLayout.isa<BlockedEncodingAttr>() ||\n+        dstLayout.isa<MmaEncodingAttr>()) {\n+      processReplica(loc, rewriter, /*stNotRd*/ false, dstTy, outNumCTAsEachRep,\n+                     multiDimRepId, outVec, paddedRepShape, outOrd, outVals,\n+                     smemBase);\n+    } else {\n+      assert(0 && \"ConvertLayout with output layout not implemented\");\n+      return failure();\n+    }\n+  }\n+\n+  SmallVector<Type> types(outElems, llvmElemTy);\n+  Type structTy = struct_ty(getContext(), types);\n+  Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n+  rewriter.replaceOp(op, result);\n+\n+  return success();\n };\n \n+LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n+    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto loc = op.getLoc();\n+  Value src = op.src();\n+  Value dst = op.result();\n+  auto srcTy = src.getType().cast<RankedTensorType>();\n+  auto dstTy = dst.getType().cast<RankedTensorType>();\n+  auto srcShape = srcTy.getShape();\n+  assert(srcShape.size() == 2 &&\n+         \"Unexpected rank of ConvertLayout(blocked->shared)\");\n+  auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto inOrd = srcBlockedLayout.getOrder();\n+  auto outOrd = dstSharedLayout.getOrder();\n+  unsigned inVec =\n+      inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n+  unsigned outVec = dstSharedLayout.getVec();\n+  unsigned minVec = std::min(outVec, inVec);\n+  unsigned perPhase = dstSharedLayout.getPerPhase();\n+  unsigned maxPhase = dstSharedLayout.getMaxPhase();\n+  unsigned numElems = getElemsPerThread(srcBlockedLayout, srcShape);\n+  auto inVals = getElementsFromStruct(loc, adaptor.src(), numElems, rewriter);\n+  unsigned srcAccumSizeInThreads =\n+      product<unsigned>(srcBlockedLayout.getSizePerThread());\n+  auto elemTy = srcTy.getElementType();\n+  auto wordTy = vec_ty(elemTy, minVec);\n+\n+  // TODO: [goostavz] We should make a cache for the calculation of\n+  // emitBaseIndexForBlockedLayout in case backend compiler not being able to\n+  // optimize that\n+  SmallVector<Value> multiDimOffsetFirstElem =\n+      emitBaseIndexForBlockedLayout(loc, rewriter, srcBlockedLayout, srcShape);\n+  SmallVector<unsigned> srcShapePerCTA = getShapePerCTA(srcBlockedLayout);\n+  SmallVector<unsigned> reps{ceil<unsigned>(srcShape[0], srcShapePerCTA[0]),\n+                             ceil<unsigned>(srcShape[1], srcShapePerCTA[1])};\n+\n+  // Visit each input value in the order they are placed in inVals\n+  //\n+  // Please note that the order was not awaring of blockLayout.getOrder(),\n+  // thus the adjacent elems may not belong to a same word. This could be\n+  // improved if we update the elements order by emitIndicesForBlockedLayout()\n+  SmallVector<unsigned> wordsInEachRep(2);\n+  wordsInEachRep[0] = inOrd[0] == 0\n+                          ? srcBlockedLayout.getSizePerThread()[0] / minVec\n+                          : srcBlockedLayout.getSizePerThread()[0];\n+  wordsInEachRep[1] = inOrd[0] == 0\n+                          ? srcBlockedLayout.getSizePerThread()[1]\n+                          : srcBlockedLayout.getSizePerThread()[1] / minVec;\n+  Value outVecVal = idx_val(outVec);\n+  Value minVecVal = idx_val(minVec);\n+  Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n+  auto elemPtrTy = ptr_ty(getTypeConverter()->convertType(elemTy), 3);\n+  smemBase = bit_cast(elemPtrTy, smemBase);\n+  unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n+  SmallVector<Value> wordVecs(numWordsEachRep);\n+  for (unsigned i = 0; i < numElems; ++i) {\n+    if (i % srcAccumSizeInThreads == 0) {\n+      // start of a replication\n+      for (unsigned w = 0; w < numWordsEachRep; ++w) {\n+        wordVecs[w] = undef(wordTy);\n+      }\n+    }\n+    unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n+    auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n+        linearIdxInNanoTile, srcBlockedLayout.getSizePerThread());\n+    multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n+    unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n+    unsigned wordVecIdx =\n+        getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep);\n+    wordVecs[wordVecIdx] =\n+        insert_element(wordTy, wordVecs[wordVecIdx], inVals[i], idx_val(pos));\n+\n+    if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n+      // end of replication, store the vectors into shared memory\n+      unsigned linearRepIdx = i / srcAccumSizeInThreads;\n+      auto multiDimRepIdx = getMultiDimIndex<unsigned>(linearRepIdx, reps);\n+      for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n+           ++linearWordIdx) {\n+        // step 1: recover the multidim_index from the index of input_elements\n+        auto multiDimWordIdx =\n+            getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep);\n+        SmallVector<Value> multiDimIdx(2);\n+        auto wordOffset0 = multiDimRepIdx[0] * srcShapePerCTA[0] +\n+                           multiDimWordIdx[0] * (inOrd[0] == 0 ? minVec : 1);\n+        auto wordOffset1 = multiDimRepIdx[1] * srcShapePerCTA[1] +\n+                           multiDimWordIdx[1] * (inOrd[0] == 1 ? minVec : 1);\n+        multiDimIdx[0] = add(multiDimOffsetFirstElem[0], idx_val(wordOffset0));\n+        multiDimIdx[1] = add(multiDimOffsetFirstElem[1], idx_val(wordOffset1));\n+\n+        // step 2: do swizzling\n+        Value remained = urem(multiDimIdx[inOrd[0]], outVecVal);\n+        multiDimIdx[inOrd[0]] = udiv(multiDimIdx[inOrd[0]], outVecVal);\n+        Value off_1 = mul(multiDimIdx[inOrd[1]], idx_val(srcShape[inOrd[0]]));\n+        Value phaseId = udiv(multiDimIdx[inOrd[1]], idx_val(perPhase));\n+        phaseId = urem(phaseId, idx_val(maxPhase));\n+        Value off_0 = xor_(multiDimIdx[inOrd[0]], phaseId);\n+        off_0 = mul(off_0, outVecVal);\n+        remained = udiv(remained, minVecVal);\n+        off_0 = add(off_0, mul(remained, minVecVal));\n+        Value offset = add(off_1, off_0);\n+\n+        // step 3: store\n+        Value smemAddr = gep(elemPtrTy, smemBase, offset);\n+        smemAddr = bit_cast(ptr_ty(wordTy, 3), smemAddr);\n+        store(wordVecs[linearWordIdx], smemAddr);\n+      }\n+    }\n+  }\n+  // TODO: double confirm if the Barrier is necessary here\n+  barrier;\n+  rewriter.replaceOp(op, smemBase);\n+  return success();\n+}\n /// ====================== dot codegen begin ==========================\n \n // Data loader for mma.16816 instruction.\n@@ -1843,7 +2012,7 @@ class MMA16816SmemLoader {\n \n     if (canUseLdmatrix)\n       ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n-    else if (elemBytes == 4 && needTrans) // tf32 & trans\n+    else if (elemBytes == 4 && needTrans)\n       ptrIdx = matIdx[order[0]];\n     else if (elemBytes == 1 && needTrans)\n       ptrIdx = matIdx[order[0]] * 4;\n@@ -2127,10 +2296,6 @@ struct DotOpConversionHelper {\n                     .cast<RankedTensorType>()\n                     .getEncoding()\n                     .cast<MmaEncodingAttr>();\n-\n-    ATensorTy = A.getType().cast<RankedTensorType>();\n-    BTensorTy = B.getType().cast<RankedTensorType>();\n-    DTensorTy = D.getType().cast<RankedTensorType>();\n   }\n \n   // Load SplatLike C which contains a constVal. It simply returns 4 fp32\n@@ -2469,7 +2634,7 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     bool needTrans = kOrder != order[0];\n \n     // (a, b) is the coordinate.\n-    auto load = [&, loader, ptrs, offs, needTrans](int a, int b) {\n+    auto load = [=, &vals, &helper, &ld2](int a, int b) {\n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n           (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n           ptrs, helper.getMatType(), helper.getShemPtrTy());\n@@ -2490,78 +2655,68 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n   };\n \n   std::function<void(int, int)> loadA;\n-  std::function<void(int, int)> loadB = getLoadMatrixFn(\n-      B, adapter.b() /*llTensor*/, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n-      0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n-      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n-\n-  if (aTensorTy.getEncoding()\n-          .dyn_cast<SharedEncodingAttr>()) { // load from smem\n+  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+    // load from smem\n     loadA = getLoadMatrixFn(\n         A, adapter.a() /*llTensor*/, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n         1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShpae*/,\n         {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n-  } else if (auto blockedLayout =\n-                 aTensorTy.getEncoding()\n-                     .dyn_cast<BlockedEncodingAttr>()) { // load from registers,\n-                                                         // used in gemm fuse\n+  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+    // load from registers, used in gemm fuse\n     // TODO(Superjomn) Port the logic.\n     assert(false && \"Loading A from register is not supported yet.\");\n   } else {\n     assert(false && \"A's layout is not supported.\");\n   }\n \n-  const unsigned mStride = numRepN * 2;\n-  SmallVector<Value> fc(numRepM * mStride + numRepN * 2);\n+  std::function<void(int, int)> loadB = getLoadMatrixFn(\n+      B, adapter.b() /*llTensor*/, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n+      0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n+      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n+\n+  const int fcSize = 4 * numRepM * numRepN;\n+  SmallVector<Value> fc(fcSize);\n+\n+  // Currently, we only support a SplatLike C. For the other cases, e.g., C in\n+  // shared layout or blocked layout, we will support them by expanding\n+  // convert_layout.\n+  auto hc = helper.loadSplatLikeC(C, loc, rewriter);\n+  assert(hc.size() == 4UL && \"Only splat-like C is supported now\");\n+  for (int i = 0; i < fc.size(); i++)\n+    fc[i] = hc[0];\n+\n   auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n+    unsigned colsPerThread = numRepN * 2;\n     PTXBuilder builder;\n-\n     auto &mma = *builder.create(helper.getMmaInstr().str());\n-\n     auto retArgs = builder.newListOperand(4, \"=r\");\n-\n     auto aArgs = builder.newListOperand({\n         {ha[{m, k}], \"r\"},\n         {ha[{m + 1, k}], \"r\"},\n         {ha[{m, k + 1}], \"r\"},\n         {ha[{m + 1, k + 1}], \"r\"},\n     });\n-\n     auto bArgs =\n         builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n-\n-    // Currently, we only support a SplatLike C. For the other cases, e.g., C in\n-    // shared layout or blocked layout, we will support them by expanding\n-    // convert_layout.\n-    auto hc = helper.loadSplatLikeC(C, loc, rewriter);\n-    assert(hc.size() == 4UL && \"Only splat-like C is supported now\");\n-\n     auto cArgs = builder.newListOperand();\n-    for (int i = 0; i < hc.size(); ++i) {\n-      cArgs->listAppend(builder.newOperand(\n-          hc[i], std::to_string(i))); // reuse the output registers\n+    for (int i = 0; i < 4; ++i) {\n+      cArgs->listAppend(builder.newOperand(fc[m * colsPerThread + 4 * n + i],\n+                                           std::to_string(i)));\n+      // reuse the output registers\n     }\n-\n     mma(retArgs, aArgs, bArgs, cArgs);\n-\n     Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n \n     auto getIntAttr = [&](int v) {\n       return ArrayAttr::get(ctx, {IntegerAttr::get(i32_ty, v)});\n     };\n \n-    fc[(m + 0) * mStride + (n * 2 + 0)] =\n-        extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(0));\n-    fc[(m + 0) * mStride + (n * 2 + 1)] =\n-        extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(1));\n-    fc[(m + 1) * mStride + (n * 2 + 0)] =\n-        extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(2));\n-    fc[(m + 1) * mStride + (n * 2 + 1)] =\n-        extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(3));\n+    for (int i = 0; i < 4; i++)\n+      fc[m * colsPerThread + 4 * n + i] =\n+          extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n   };\n \n   // Main program\n-\n   for (unsigned k = 0; k < numRepK; ++k) {\n     for (unsigned m = 0; m < numRepM; ++m)\n       loadA(2 * m, 2 * k);\n@@ -2741,6 +2896,9 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n          \"Inliner pass is expected before TritonGPUToLLVM\");\n   b.setInsertionPointToStart(&funcs[0].getBody().front());\n   smem = b.create<LLVM::AddressOfOp>(loc, global);\n+  auto ptrTy =\n+      LLVM::LLVMPointerType::get(typeConverter.convertType(b.getI8Type()), 3);\n+  smem = b.create<LLVM::BitcastOp>(loc, ptrTy, smem);\n }\n \n } // namespace"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -87,7 +87,6 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n-\n   return shape;\n }\n \n@@ -104,7 +103,7 @@ SmallVector<unsigned> getOrder(const Attribute &layout) {\n     assert(0 && \"Unimplemented usage of getOrder\");\n     return {};\n   }\n-}\n+};\n \n } // namespace gpu\n } // namespace triton\n@@ -215,9 +214,12 @@ unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n }\n \n unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n-  int threads = product(getWarpsPerCTA());\n-  int numElem = product(shape);\n-  return numElem / threads;\n+  size_t rank = shape.size();\n+  assert(rank == 2 && \"Unexpected rank of mma layout\");\n+  assert(getVersion() == 2 && \"mmaLayout version = 1 is not implemented yet\");\n+  unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n+  unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n+  return elemsCol * elemsRow;\n }\n \n unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {"}, {"filename": "python/tests/test_gemm.py", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "@@ -0,0 +1,52 @@\n+import pytest\n+import torch\n+from torch.testing import assert_allclose\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def matmul_kernel(\n+    a_ptr, b_ptr, c_ptr,\n+    stride_am, stride_ak,\n+    stride_bk, stride_bn,\n+    stride_cm, stride_cn,\n+    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr\n+):\n+    offs_m = tl.arange(0, M)\n+    offs_n = tl.arange(0, N)\n+    offs_k = tl.arange(0, K)\n+    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n+    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n+    a = tl.load(a_ptrs)\n+    b = tl.load(b_ptrs)\n+\n+    c = tl.dot(a, b)\n+\n+    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n+    tl.store(c_ptrs, c)\n+\n+# TODO: num_warps could only be 4 for now\n+\n+\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS', [\n+    [128, 256, 32, 4],\n+    [256, 128, 16, 4],\n+    [128, 16, 32, 4],\n+    [32, 128, 64, 4],\n+])\n+def test_gemm_impl(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS):\n+    a = torch.randn((SIZE_M, SIZE_K), device='cuda', dtype=torch.float16)\n+    b = torch.randn((SIZE_K, SIZE_N), device='cuda', dtype=torch.float16)\n+    c = torch.empty((SIZE_M, SIZE_N), device=a.device, dtype=torch.float32)\n+    grid = lambda META: (1, )\n+    matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c,\n+                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                        M=SIZE_M, N=SIZE_N, K=SIZE_K,\n+                        num_warps=NUM_WARPS)\n+    golden = torch.matmul(a, b)\n+    torch.set_printoptions(profile=\"full\")\n+    assert_allclose(c, golden, rtol=1e-3, atol=1e-3)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -910,7 +910,7 @@ def ptx_get_version(cuda_version) -> int:\n \n \n def path_to_ptxas():\n-    prefixes = [os.environ.get(\"TRITON_PTXAS_PATH\", \"\"), \"\", \"/usr/local/cuda/\"]\n+    prefixes = [os.environ.get(\"TRITON_PTXAS_PATH\", \"\"), \"\", os.environ.get('CUDA_PATH', default_cuda_dir())]\n     for prefix in prefixes:\n         ptxas = os.path.join(prefix, \"bin\", \"ptxas\")\n         if os.path.exists(ptxas):"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -515,3 +515,20 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     return\n   }\n }\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<16384 x i8>\n+  // CHECK-LABEL: convert_layout_blocked_shared\n+  func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    return\n+  }\n+}\n\\ No newline at end of file"}]