[{"filename": "python/test/unit/tools/test_aot.py", "status": "modified", "additions": 62, "deletions": 45, "changes": 107, "file_content_changes": "@@ -128,48 +128,65 @@ def kernel(C, A, B,\n \n \n def test_compile_link_matmul():\n-    # with tempfile.TemporaryDirectory() as tmp_dir:\n-    tmp_dir = tempfile.mkdtemp()\n-    kernel_path = os.path.join(tmp_dir, \"kernel.py\")\n-    with open(kernel_path, \"w\") as file:\n-        file.write(kernel_src)\n-\n-    kernel_utils_path = os.path.join(tmp_dir, \"kernel_utils.py\")\n-    with open(kernel_utils_path, \"w\") as file:\n-        file.write(kernel_utils_src)\n-\n-    compile_path = Path(triton.tools.__path__[0]) / \"compile.py\"\n-    dtype = \"fp16\"\n-    M, N, K = 16, 16, 16\n-    BM, BN, BK = 16, 16, 16\n-    # hints = [\":16\", \"\"]\n-    hints = [\":16\"]\n-    for ha in hints:\n-        for hb in hints:\n-            sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, 1, i32{hb}, 1, i32:16, 1, {BM}, {BN}, {BK}'\n-            name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n-            subprocess.run([\"python\", compile_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", tmp_dir + \"/\" + name, kernel_path], check=True)\n-\n-    link_path = Path(triton.tools.__path__[0]) / \"link.py\"\n-    subprocess.run([\"python\", link_path] + glob.glob(tmp_dir + \"/*.h\") + [\"-o\", tmp_dir + \"/kernel\"], check=True)\n-\n-    test_path = os.path.join(tmp_dir, \"test.c\")\n-    with open(test_path, \"w\") as file:\n-        file.write(test_src)\n-    subprocess.run([\"gcc\"] + glob.glob(tmp_dir + \"/*.c\") + [\"-I\", \"/usr/local/cuda/include/\"] + [\"-o\", tmp_dir + \"/test\", \"-L\", \"/usr/lib/wsl/lib/\", \"-l\", \"cuda\"], check=True)\n-\n-    # create data\n-    a = np.random.randn(M * K).astype(np.float16).reshape((M, K))\n-    b = np.random.randn(M * K).astype(np.float16).reshape((K, N))\n-\n-    a_path = os.path.join(tmp_dir, \"a.csv\")\n-    b_path = os.path.join(tmp_dir, \"b.csv\")\n-    c_path = os.path.join(tmp_dir, \"c.csv\")\n-    for x, path in [(a, a_path), (b, b_path)]:\n-        x.view(np.int16).ravel().tofile(path, sep=\",\")\n-\n-    subprocess.run([os.path.join(tmp_dir, \"test\"), a_path, b_path, c_path], check=True)\n-    c = np.genfromtxt(c_path, delimiter=\",\", dtype=np.int32)\n-    c_tri = c.reshape((M, N)).view(np.float32)\n-    c_ref = np.matmul(a.astype(np.float32), b.astype(np.float32))\n-    np.testing.assert_allclose(c_tri, c_ref * c_ref, atol=1e-4, rtol=0.)\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+        kernel_path = os.path.join(tmp_dir, \"kernel.py\")\n+        with open(kernel_path, \"w\") as file:\n+            file.write(kernel_src)\n+\n+        kernel_utils_path = os.path.join(tmp_dir, \"kernel_utils.py\")\n+        with open(kernel_utils_path, \"w\") as file:\n+            file.write(kernel_utils_src)\n+\n+        compile_path = Path(triton.tools.__path__[0]) / \"compile.py\"\n+        dtype = \"fp16\"\n+        M, N, K = 16, 16, 16\n+        BM, BN, BK = 16, 16, 16\n+        # hints = [\":16\", \"\"]\n+        hints = [\":16\"]\n+        for ha in hints:\n+            for hb in hints:\n+                sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, 1, i32{hb}, 1, i32:16, 1, {BM}, {BN}, {BK}'\n+                name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n+                subprocess.run([\"python\", compile_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", tmp_dir + \"/\" + name, kernel_path], check=True)\n+\n+        link_path = Path(triton.tools.__path__[0]) / \"link.py\"\n+        subprocess.run([\"python\", link_path] + glob.glob(tmp_dir + \"/*.h\") + [\"-o\", tmp_dir + \"/kernel\"], check=True)\n+\n+        test_path = os.path.join(tmp_dir, \"test.c\")\n+        with open(test_path, \"w\") as file:\n+            file.write(test_src)\n+        subprocess.run([\"gcc\"] + glob.glob(tmp_dir + \"/*.c\") + [\"-I\", \"/usr/local/cuda/include/\"] + [\"-o\", tmp_dir + \"/test\", \"-L\", \"/usr/lib/wsl/lib/\", \"-l\", \"cuda\"], check=True)\n+\n+        # create data\n+        a = np.random.randn(M * K).astype(np.float16).reshape((M, K))\n+        b = np.random.randn(M * K).astype(np.float16).reshape((K, N))\n+\n+        a_path = os.path.join(tmp_dir, \"a.csv\")\n+        b_path = os.path.join(tmp_dir, \"b.csv\")\n+        c_path = os.path.join(tmp_dir, \"c.csv\")\n+        for x, path in [(a, a_path), (b, b_path)]:\n+            x.view(np.int16).ravel().tofile(path, sep=\",\")\n+\n+        subprocess.run([os.path.join(tmp_dir, \"test\"), a_path, b_path, c_path], check=True)\n+        c = np.genfromtxt(c_path, delimiter=\",\", dtype=np.int32)\n+        c_tri = c.reshape((M, N)).view(np.float32)\n+        c_ref = np.matmul(a.astype(np.float32), b.astype(np.float32))\n+        np.testing.assert_allclose(c_tri, c_ref * c_ref, atol=1e-4, rtol=0.)\n+\n+\n+def test_ttgir_to_ptx():\n+    src = \"\"\"\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @sum_kernel_0d1d(%arg0: !tt.ptr<i32>, %arg1: !tt.ptr<i32>) {\n+    tt.return\n+  }\n+}\n+\"\"\"\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+        kernel_path = os.path.join(tmp_dir, \"empty_kernel.ttgir\")\n+        with open(kernel_path, \"w\") as fp:\n+            fp.write(src)\n+        k = triton.compile(kernel_path, cc=80)\n+        ptx = k.asm[\"ptx\"]\n+        assert \".target sm_80\" in ptx\n+        assert \".address_size 64\" in ptx"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "removed", "additions": 0, "deletions": 16, "changes": 16, "file_content_changes": "@@ -1,16 +0,0 @@\n-// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir --sm=80 | FileCheck %s\n-\n-// == LLVM IR check begin ==\n-// CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'\n-// CHECK: define void @test_empty_kernel\n-// CHECK: !nvvm.annotations\n-// CHECK: !{ptr @test_empty_kernel, !\"maxntidx\", i32 128}\n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-\n-tt.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n-\n-  tt.return\n-}\n-\n-}"}, {"filename": "test/Target/tritongpu_to_llvmir_noinline.mlir", "status": "removed", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "@@ -1,22 +0,0 @@\n-// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir --sm=80 | FileCheck %s\n-\n-// == LLVM IR check begin ==\n-// CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'\n-// CHECK: define void @test_func\n-// CHECK: define void @test_kernel\n-// CHECK: tail call void @test_func\n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-\n-tt.func @test_func(%lb : index, %A : !tt.ptr<f16>) attributes { noinline = true } {\n-  %0 = arith.constant 1.0 : f16\n-  tt.store %A, %0 : f16\n-  tt.return\n-}\n-\n-tt.func @test_kernel(%lb : index, %A : !tt.ptr<f16>) {\n-  tt.call @test_func(%lb, %A) : (index, !tt.ptr<f16>) -> ()\n-  tt.return\n-}\n-\n-}"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "removed", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -1,14 +0,0 @@\n-// RUN: %PYTHON -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n-// CHECK-LABEL: // Generated by LLVM NVPTX Back-End\n-// CHECK: .version 6.3\n-// CHECK: .target sm_80\n-// CHECK: .address_size 64\n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-\n-tt.func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n-\n-  tt.return\n-}\n-\n-}"}]