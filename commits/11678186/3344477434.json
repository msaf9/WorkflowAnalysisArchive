[{"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -292,13 +292,11 @@ struct PTXCpAsyncWaitGroupInstr : public PTXCpAsyncInstrBase {\n \n struct PTXCpAsyncLoadInstr : public PTXCpAsyncInstrBase {\n   explicit PTXCpAsyncLoadInstr(PTXBuilder *builder,\n-                               triton::CacheModifier modifier,\n-                               triton::EvictionPolicy policy)\n+                               triton::CacheModifier modifier)\n       : PTXCpAsyncInstrBase(builder) {\n     o(triton::stringifyCacheModifier(modifier).str());\n     o(\"shared\");\n     o(\"global\");\n-    o(\"L2::\" + triton::stringifyEvictionPolicy(policy).str());\n   }\n };\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -21,7 +21,7 @@ namespace mlir {\n namespace triton {\n namespace gpu {\n \n-unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape);\n+unsigned getElemsPerThread(Type type);\n \n SmallVector<unsigned> getSizePerThread(Attribute layout);\n "}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -1,6 +1,7 @@\n #ifndef TRITON_TARGET_LLVMIRTRANSLATION_H\n #define TRITON_TARGET_LLVMIRTRANSLATION_H\n #include <memory>\n+#include <vector>\n \n namespace llvm {\n class Module;\n@@ -14,6 +15,11 @@ class ModuleOp;\n namespace mlir {\n namespace triton {\n \n+// add external dependent libs\n+void addExternalLibs(mlir::ModuleOp &module,\n+                     const std::vector<std::string> &names,\n+                     const std::vector<std::string> &paths);\n+\n // Translate TritonGPU dialect to LLVMIR, return null if failed.\n std::unique_ptr<llvm::Module>\n translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "file_content_changes": "@@ -42,18 +42,14 @@ void MembarAnalysis::dfsOperation(Operation *operation,\n \n void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n                               OpBuilder *builder) {\n-  if (op->getNumResults() < 1)\n-    return;\n-\n   if (isa<scf::ForOp>(op) || isa<scf::IfOp>(op) || isa<scf::YieldOp>(op) ||\n       isa<triton::gpu::ExtractSliceOp>(op) ||\n-      isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n       isa<triton::gpu::AllocTensorOp>(op)) {\n     // Do not insert barriers before control flow operations and\n     // alloc/extract/insert\n     // alloc is an allocation op without memory write.\n     // In contrast, arith.constant is an allocation op with memory write.\n-    // FIXME(Keren): extract and insert are always alias for now\n+    // FIXME(Keren): extract is always alias for now\n     return;\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 307, "deletions": 351, "changes": 658, "file_content_changes": "@@ -6,14 +6,17 @@\n #include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n #include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n #include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n+#include \"mlir/Conversion/SCFToStandard/SCFToStandard.h\"\n #include \"mlir/Conversion/StandardToLLVM/ConvertStandardToLLVM.h\"\n #include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n #include \"mlir/Dialect/GPU/GPUDialect.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Membar.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n@@ -117,7 +120,7 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define f32_ty rewriter.getF32Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n #define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n-#define void_ty LLVM::LLVMVoidType::get(ctx)\n+#define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n #define struct_ty(...) LLVM::LLVMStructType::getLiteral(__VA_ARGS__)\n \n // Creator for constant\n@@ -317,6 +320,10 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<::mlir::ReturnOp> {\n Value getStructFromElements(Location loc, ValueRange resultVals,\n                             ConversionPatternRewriter &rewriter,\n                             Type structType) {\n+  if (!structType.isa<LLVM::LLVMStructType>()) {\n+    return *resultVals.begin();\n+  }\n+\n   Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n   for (auto v : llvm::enumerate(resultVals)) {\n     llvmStruct = insert_val(structType, llvmStruct, v.value(),\n@@ -369,13 +376,17 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n   auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n   auto *valOpr = builder.newOperand(val, c);\n   st(ptrOpr, valOpr).predicate(pred, \"b\");\n-  return builder.launch(rewriter, loc, void_ty);\n+  return builder.launch(rewriter, loc, void_ty(ctx));\n }\n \n struct ConvertTritonGPUOpToLLVMPatternBase {\n   static SmallVector<Value>\n   getElementsFromStruct(Location loc, Value llvmStruct,\n                         ConversionPatternRewriter &rewriter) {\n+    if (llvmStruct.getType().isIntOrIndexOrFloat() ||\n+        llvmStruct.getType().isa<triton::PointerType>() ||\n+        llvmStruct.getType().isa<LLVM::LLVMPointerType>())\n+      return {llvmStruct};\n     ArrayRef<Type> types =\n         llvmStruct.getType().cast<LLVM::LLVMStructType>().getBody();\n     SmallVector<Value> results(types.size());\n@@ -685,7 +696,7 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n     auto layout = tensorTy.getEncoding();\n     auto srcType = typeConverter->convertType(elemType);\n     auto llSrc = bitcast(constVal, srcType);\n-    size_t elemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n+    size_t elemsPerThread = getElemsPerThread(tensorTy);\n     llvm::SmallVector<Value> elems(elemsPerThread, llSrc);\n     llvm::SmallVector<Type> elemTypes(elems.size(), srcType);\n     auto structTy =\n@@ -767,64 +778,49 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n \n   // Get corresponding LLVM element values of \\param value.\n   SmallVector<Value> getLLVMElems(Value value, Value llValue,\n-                                  const BlockedEncodingAttr &layout,\n                                   ConversionPatternRewriter &rewriter,\n                                   Location loc) const {\n     if (!value)\n       return {};\n-\n-    auto shape = value.getType().cast<RankedTensorType>().getShape();\n+    if (!llValue.getType().isa<LLVM::LLVMStructType>())\n+      return {llValue};\n     // Here, we assume that all inputs should have a blockedLayout\n     auto valueVals = getElementsFromStruct(loc, llValue, rewriter);\n     return valueVals;\n   }\n \n-  // Get the blocked layout.\n-  std::tuple<BlockedEncodingAttr, unsigned> getLayout(Value val) const {\n-    auto ty = val.getType().cast<RankedTensorType>();\n-    // Here, we assume that all inputs should have a blockedLayout\n-    auto layout = ty.getEncoding().dyn_cast<BlockedEncodingAttr>();\n-    assert(layout && \"unexpected layout in getLayout\");\n-    auto shape = ty.getShape();\n-    unsigned valueElems = layout.getElemsPerThread(shape);\n-    return {layout, valueElems};\n-  }\n-\n-  unsigned getAlignment(Value val, const BlockedEncodingAttr &layout) const {\n-    auto axisInfo = getAxisInfo(val);\n-    auto order = layout.getOrder();\n-    unsigned maxMultiple = axisInfo->getDivisibility(order[0]);\n-    unsigned maxContig = axisInfo->getContiguity(order[0]);\n-    unsigned alignment = std::min(maxMultiple, maxContig);\n-    return alignment;\n-  }\n+  unsigned getVectorSize(Value ptr) const {\n+    auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return 1;\n+    auto layout = tensorTy.getEncoding();\n+    auto shape = tensorTy.getShape();\n \n-  unsigned getVectorizeSize(Value ptr,\n-                            const BlockedEncodingAttr &layout) const {\n     auto axisInfo = getAxisInfo(ptr);\n     // Here order should be ordered by contiguous first, so the first element\n     // should have the largest contiguous.\n-    auto order = layout.getOrder();\n+    auto order = getOrder(layout);\n     unsigned align = getAlignment(ptr, layout);\n \n-    auto ty = ptr.getType().dyn_cast<RankedTensorType>();\n-    assert(ty);\n-    auto shape = ty.getShape();\n-\n-    unsigned contigPerThread = layout.getSizePerThread()[order[0]];\n+    unsigned contigPerThread = getSizePerThread(layout)[order[0]];\n     unsigned vec = std::min(align, contigPerThread);\n     vec = std::min<unsigned>(shape[order[0]], vec);\n \n     return vec;\n   }\n \n-  unsigned getMaskAlignment(Value mask) const {\n-    auto maskOrder = mask.getType()\n-                         .cast<RankedTensorType>()\n-                         .getEncoding()\n-                         .cast<BlockedEncodingAttr>()\n-                         .getOrder();\n+  unsigned getAlignment(Value val, const Attribute &layout) const {\n+    auto axisInfo = getAxisInfo(val);\n+    auto order = getOrder(layout);\n+    unsigned maxMultiple = axisInfo->getDivisibility(order[0]);\n+    unsigned maxContig = axisInfo->getContiguity(order[0]);\n+    unsigned alignment = std::min(maxMultiple, maxContig);\n+    return alignment;\n+  }\n \n+  unsigned getMaskAlignment(Value mask) const {\n+    auto tensorTy = mask.getType().cast<RankedTensorType>();\n+    auto maskOrder = getOrder(tensorTy.getEncoding());\n     auto maskAxis = getAxisInfo(mask);\n     return std::max<int>(maskAxis->getConstancy(maskOrder[0]), 1);\n   }\n@@ -855,46 +851,39 @@ struct LoadOpConversion\n   LogicalResult\n   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n+    MLIRContext *ctx = rewriter.getContext();\n+    auto loc = op->getLoc();\n+\n+    // original values\n     Value ptr = op.ptr();\n     Value mask = op.mask();\n     Value other = op.other();\n \n+    // adaptor values\n     Value llPtr = adaptor.ptr();\n     Value llMask = adaptor.mask();\n     Value llOther = adaptor.other();\n \n-    auto loc = op->getLoc();\n-    MLIRContext *ctx = rewriter.getContext();\n-\n-    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n-    if (!valueTy)\n-      return failure();\n-    Type valueElemTy =\n-        getTypeConverter()->convertType(valueTy.getElementType());\n-\n-    auto [layout, numElems] = getLayout(ptr);\n-\n-    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n-    assert(ptrElems.size() == numElems);\n     // Determine the vectorization size\n-    size_t vec = getVectorizeSize(ptr, layout);\n+    Type valueTy = op.getResult().getType();\n+    Type valueElemTy = getElementTypeOrSelf(valueTy);\n+    unsigned vec = getVectorSize(ptr);\n+    unsigned numElems = getElemsPerThread(ptr.getType());\n+    if (llMask)\n+      vec = std::min<size_t>(vec, getMaskAlignment(mask));\n+\n+    // Get the LLVM values for pointers\n+    auto ptrElems = getLLVMElems(ptr, llPtr, rewriter, loc);\n+    assert(ptrElems.size() == numElems);\n \n+    // Get the LLVM values for mask\n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      unsigned maskAlignment = getMaskAlignment(mask);\n-      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n-      assert(ptrElems.size() == maskElems.size());\n-\n-      size_t maskAlign = getMaskAlignment(mask);\n-      vec = std::min(vec, maskAlign);\n+      maskElems = getLLVMElems(mask, llMask, rewriter, loc);\n+      assert(maskElems.size() == numElems);\n     }\n \n-    const size_t dtsize =\n-        std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n-    const size_t valueElemNbits = dtsize * 8;\n-\n-    const int numVecs = numElems / vec;\n-\n+    // Get the LLVM values for `other`\n     // TODO: (goostavz) handle when other is const but not splat, which\n     //       should be rarely seen\n     bool otherIsSplatConstInt = false;\n@@ -905,8 +894,12 @@ struct LoadOpConversion\n       otherIsSplatConstInt = true;\n       splatVal = constAttr.getSplatValue<APInt>().getSExtValue();\n     }\n+    auto otherElems = getLLVMElems(other, llOther, rewriter, loc);\n \n-    auto otherElems = getLLVMElems(other, llOther, layout, rewriter, loc);\n+    // vectorized iteration through all the pointer/mask/other elements\n+    const int valueElemNbits =\n+        std::max(8u, valueElemTy.getIntOrFloatBitWidth());\n+    const int numVecs = numElems / vec;\n \n     SmallVector<Value> loadedVals;\n     for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n@@ -1066,30 +1059,23 @@ struct StoreOpConversion\n     auto loc = op->getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n \n-    auto valueTy = value.getType().dyn_cast<RankedTensorType>();\n-    if (!valueTy) {\n-      store(llValue, llPtr);\n-      rewriter.eraseOp(op);\n-      return success();\n-    }\n-\n-    Type valueElemTy =\n-        getTypeConverter()->convertType(valueTy.getElementType());\n+    auto valueTy = value.getType();\n+    Type valueElemTy = getElementTypeOrSelf(valueTy);\n \n-    auto [layout, numElems] = getLayout(ptr);\n+    unsigned vec = getVectorSize(ptr);\n+    unsigned numElems = getElemsPerThread(ptr.getType());\n \n-    auto ptrElems = getLLVMElems(ptr, llPtr, layout, rewriter, loc);\n-    auto valueElems = getLLVMElems(value, llValue, layout, rewriter, loc);\n+    auto ptrElems = getLLVMElems(ptr, llPtr, rewriter, loc);\n+    auto valueElems = getLLVMElems(value, llValue, rewriter, loc);\n     assert(ptrElems.size() == valueElems.size());\n \n     // Determine the vectorization size\n-    size_t vec = getVectorizeSize(ptr, layout);\n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, layout, rewriter, loc);\n+      maskElems = getLLVMElems(mask, llMask, rewriter, loc);\n       assert(valueElems.size() == maskElems.size());\n \n-      size_t maskAlign = getMaskAlignment(mask);\n+      unsigned maskAlign = getMaskAlignment(mask);\n       vec = std::min(vec, maskAlign);\n     }\n \n@@ -1152,14 +1138,14 @@ struct StoreOpConversion\n           ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n \n       auto &ptxStoreInstr =\n-          ptxBuilder.create<PTXIOInstr>(\"st\")->global().b(width).v(nWords);\n+          ptxBuilder.create<PTXIOInstr>(\"st\")->global().v(nWords).b(width);\n       ptxStoreInstr(asmAddr, asmArgList).predicate(maskVal, \"b\");\n \n       Type boolTy = getTypeConverter()->convertType(rewriter.getIntegerType(1));\n       llvm::SmallVector<Type> argTys({boolTy, ptr.getType()});\n       argTys.insert(argTys.end(), nWords, valArgTy);\n \n-      auto ASMReturnTy = LLVM::LLVMVoidType::get(ctx);\n+      auto ASMReturnTy = void_ty(ctx);\n \n       ptxBuilder.launch(rewriter, loc, ASMReturnTy);\n     }\n@@ -1229,8 +1215,9 @@ struct BroadcastOpConversion\n     for (auto it : llvm::enumerate(broadcastDims)) {\n       // Incase there are multiple indices in the src that is actually\n       // calculating the same element, srcLogicalShape may not need to be 1.\n-      // Such as the case when src of shape [256, 1], and with a blocked layout:\n-      // sizePerThread: [1, 4];  threadsPerWarp: [1, 32]; warpsPerCTA: [1, 2]\n+      // Such as the case when src of shape [256, 1], and with a blocked\n+      // layout: sizePerThread: [1, 4];  threadsPerWarp: [1, 32]; warpsPerCTA:\n+      // [1, 2]\n       int64_t d = resultLogicalShape[it.value()] / srcLogicalShape[it.value()];\n       broadcastSizes[it.index()] = d;\n       duplicates *= d;\n@@ -1240,10 +1227,10 @@ struct BroadcastOpConversion\n       duplicates *= d;\n     }\n \n-    unsigned srcElems = srcLayout.getElemsPerThread(srcShape);\n+    unsigned srcElems = getElemsPerThread(srcTy);\n     auto elemTy = resultTy.getElementType();\n     auto srcVals = getElementsFromStruct(loc, src, rewriter);\n-    unsigned resultElems = resultLayout.getElemsPerThread(resultShape);\n+    unsigned resultElems = getElemsPerThread(resultTy);\n     SmallVector<Value> resultVals(resultElems);\n     for (unsigned i = 0; i < srcElems; ++i) {\n       auto srcMultiDim = getMultiDimIndex<int64_t>(i, srcLogicalShape);\n@@ -1262,8 +1249,10 @@ struct BroadcastOpConversion\n       }\n     }\n     auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n+\n     Value resultStruct =\n         getStructFromElements(loc, resultVals, rewriter, llvmStructTy);\n+\n     rewriter.replaceOp(op, {resultStruct});\n     return success();\n   }\n@@ -1395,7 +1384,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n \n   auto smemShape = getScratchConfigForReduce(op);\n \n-  unsigned srcElems = getElemsPerThread(srcLayout, srcShape);\n+  unsigned srcElems = getElemsPerThread(srcTy);\n   auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n   auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n \n@@ -1452,7 +1441,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     auto resultLayout = resultTy.getEncoding();\n     auto resultShape = resultTy.getShape();\n \n-    unsigned resultElems = getElemsPerThread(resultLayout, resultShape);\n+    unsigned resultElems = getElemsPerThread(resultTy);\n     auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n     assert(resultIndices.size() == resultElems);\n \n@@ -1504,7 +1493,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   unsigned sizeIntraWarps = threadsPerWarp[axis];\n   unsigned sizeInterWarps = warpsPerCTA[axis];\n \n-  unsigned srcElems = getElemsPerThread(srcLayout, srcShape);\n+  unsigned srcElems = getElemsPerThread(srcTy);\n   auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcShape);\n   auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n \n@@ -1592,7 +1581,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n     auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n     auto resultShape = resultTy.getShape();\n \n-    unsigned resultElems = getElemsPerThread(resultLayout, resultShape);\n+    unsigned resultElems = getElemsPerThread(resultTy);\n     auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n     assert(resultIndices.size() == resultElems);\n \n@@ -1639,7 +1628,7 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n     auto resultShape = resultTy.getShape();\n-    unsigned elems = getElemsPerThread(resultTy.getEncoding(), resultShape);\n+    unsigned elems = getElemsPerThread(resultTy);\n     Type elemTy =\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n@@ -1718,7 +1707,7 @@ struct AddPtrOpConversion\n           resultTensorTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n       assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n       auto resultShape = resultTensorTy.getShape();\n-      unsigned elems = resultLayout.getElemsPerThread(resultShape);\n+      unsigned elems = getElemsPerThread(resultTy);\n       Type elemTy =\n           getTypeConverter()->convertType(resultTensorTy.getElementType());\n       SmallVector<Type> types(elems, elemTy);\n@@ -1775,8 +1764,8 @@ struct ExtractSliceOpConversion\n     auto srcLayout = srcTy.getEncoding().dyn_cast<SharedEncodingAttr>();\n     assert(srcLayout && \"Unexpected resultLayout in ExtractSliceOpConversion\");\n \n-    // axis > 0 will result in non-contiguous memory access if the result tensor\n-    // is an alias of the source tensor.\n+    // axis > 0 will result in non-contiguous memory access if the result\n+    // tensor is an alias of the source tensor.\n     auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n     assert(axis == 0 && \"extract_slice: Only axis=0 is supported for now\");\n \n@@ -1798,221 +1787,77 @@ struct ExtractSliceOpConversion\n   }\n };\n \n-// TODO: rewrite Ternary/Binary/Unary as Elementwise\n-\n // A CRTP style of base class.\n template <typename SourceOp, typename DestOp, typename ConcreteT>\n-class BinaryOpConversionBase\n+class ElementwiseOpConversionBase\n     : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n public:\n   using OpAdaptor = typename SourceOp::Adaptor;\n \n-  explicit BinaryOpConversionBase(LLVMTypeConverter &typeConverter,\n-                                  PatternBenefit benefit = 1)\n+  explicit ElementwiseOpConversionBase(LLVMTypeConverter &typeConverter,\n+                                       PatternBenefit benefit = 1)\n       : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n   LogicalResult\n   matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto resultTy = op.getType().template dyn_cast<RankedTensorType>();\n-    // ArithmeticToLLVM will handle the lowering of scalar ArithOps\n-    if (!resultTy)\n-      return failure();\n-\n+    auto resultTy = op.getType();\n     Location loc = op->getLoc();\n-    auto resultLayout =\n-        resultTy.getEncoding().template dyn_cast<BlockedEncodingAttr>();\n-    auto resultShape = resultTy.getShape();\n-    assert(resultLayout && \"Unexpected resultLayout in BinaryOpConversion\");\n-    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n-    Type elemTy =\n-        this->getTypeConverter()->convertType(resultTy.getElementType());\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n-\n-    auto *concreteThis = static_cast<const ConcreteT *>(this);\n-    auto lhss = this->getElementsFromStruct(loc, concreteThis->getLhs(adaptor),\n-                                            rewriter);\n-    auto rhss = this->getElementsFromStruct(loc, concreteThis->getRhs(adaptor),\n-                                            rewriter);\n-    SmallVector<Value> resultVals(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = concreteThis->createDestOp(op, rewriter, elemTy, lhss[i],\n-                                                 rhss[i], loc);\n-    }\n-    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n-    rewriter.replaceOp(op, view);\n-    return success();\n-  }\n-};\n-\n-template <typename SourceOp, typename DestOp>\n-struct BinaryOpConversion\n-    : public BinaryOpConversionBase<SourceOp, DestOp,\n-                                    BinaryOpConversion<SourceOp, DestOp>> {\n-\n-  explicit BinaryOpConversion(LLVMTypeConverter &typeConverter,\n-                              PatternBenefit benefit = 1)\n-      : BinaryOpConversionBase<SourceOp, DestOp,\n-                               BinaryOpConversion<SourceOp, DestOp>>(\n-            typeConverter, benefit) {}\n-\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-  // An interface to support variant DestOp builder.\n-  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n-                      Type elemTy, Value lhs, Value rhs, Location loc) const {\n-    return rewriter.create<DestOp>(loc, elemTy, lhs, rhs);\n-  }\n-\n-  // Get the left operand of the op.\n-  Value getLhs(OpAdaptor adaptor) const { return adaptor.getLhs(); }\n-  // Get the right operand of the op.\n-  Value getRhs(OpAdaptor adaptor) const { return adaptor.getRhs(); }\n-};\n-\n-//\n-// Ternary\n-//\n-\n-template <typename SourceOp, typename DestOp, typename ConcreteT>\n-class TernaryOpConversionBase\n-    : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n-public:\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-\n-  explicit TernaryOpConversionBase(LLVMTypeConverter &typeConverter,\n-                                   PatternBenefit benefit = 1)\n-      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n-\n-  LogicalResult\n-  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto resultTy = op.getType().template dyn_cast<RankedTensorType>();\n-    // ArithmeticToLLVM will handle the lowering of scalar ArithOps\n-    if (!resultTy)\n-      return failure();\n \n-    Location loc = op->getLoc();\n-    auto resultLayout =\n-        resultTy.getEncoding().template dyn_cast<BlockedEncodingAttr>();\n-    auto resultShape = resultTy.getShape();\n-    assert(resultLayout && \"Unexpected resultLayout in TernaryOpConversion\");\n-    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n-    Type elemTy =\n-        this->getTypeConverter()->convertType(resultTy.getElementType());\n+    unsigned elems = getElemsPerThread(resultTy);\n+    auto resultElementTy = getElementTypeOrSelf(resultTy);\n+    Type elemTy = this->getTypeConverter()->convertType(resultElementTy);\n     SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+    Type structTy = this->getTypeConverter()->convertType(resultTy);\n \n     auto *concreteThis = static_cast<const ConcreteT *>(this);\n-    auto lhss =\n-        this->getElementsFromStruct(loc, adaptor.getOperands()[0], rewriter);\n-    auto rhss =\n-        this->getElementsFromStruct(loc, adaptor.getOperands()[1], rewriter);\n-    auto thss =\n-        this->getElementsFromStruct(loc, adaptor.getOperands()[2], rewriter);\n+    auto operands = getOperands(rewriter, adaptor, elems, loc);\n     SmallVector<Value> resultVals(elems);\n     for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] = concreteThis->createDestOp(op, rewriter, elemTy, lhss[i],\n-                                                 rhss[i], thss[i], loc);\n+      resultVals[i] = concreteThis->createDestOp(op, adaptor, rewriter, elemTy,\n+                                                 operands[i], loc);\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n     return success();\n   }\n-};\n-\n-template <typename SourceOp, typename DestOp>\n-struct TernaryOpConversion\n-    : public TernaryOpConversionBase<SourceOp, DestOp,\n-                                     TernaryOpConversion<SourceOp, DestOp>> {\n-\n-  explicit TernaryOpConversion(LLVMTypeConverter &typeConverter,\n-                               PatternBenefit benefit = 1)\n-      : TernaryOpConversionBase<SourceOp, DestOp,\n-                                TernaryOpConversion<SourceOp, DestOp>>(\n-            typeConverter, benefit) {}\n-\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-  // An interface to support variant DestOp builder.\n-  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n-                      Type elemTy, Value lhs, Value rhs, Value th,\n-                      Location loc) const {\n-    return rewriter.create<DestOp>(loc, elemTy, lhs, rhs, th);\n-  }\n-};\n-\n-//\n-// Unary\n-//\n-\n-template <typename SourceOp, typename DestOp, typename ConcreteT>\n-class UnaryOpConversionBase : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n-\n-public:\n-  using OpAdaptor = typename SourceOp::Adaptor;\n-\n-  explicit UnaryOpConversionBase(LLVMTypeConverter &typeConverter,\n-                                 PatternBenefit benefit = 1)\n-      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n-\n-  LogicalResult\n-  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto resultTy = op.getType().template dyn_cast<RankedTensorType>();\n-\n-    // ArithmeticToLLVM will handle the lowering of scalar ArithOps\n-    if (!resultTy)\n-      return failure();\n-\n-    Location loc = op->getLoc();\n-    auto resultLayout =\n-        resultTy.getEncoding().template dyn_cast<BlockedEncodingAttr>();\n-    auto resultShape = resultTy.getShape();\n-    assert(resultLayout && \"Unexpected resultLayout in UnaryOpConversion\");\n-    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n-    Type elemTy =\n-        this->getTypeConverter()->convertType(resultTy.getElementType());\n-    SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n \n-    auto *concreteThis = static_cast<const ConcreteT *>(this);\n-    auto srcs = this->getElementsFromStruct(loc, concreteThis->getSrc(adaptor),\n-                                            rewriter);\n-    SmallVector<Value> resultVals(elems);\n-    for (unsigned i = 0; i < elems; ++i) {\n-      resultVals[i] =\n-          concreteThis->createDestOp(op, rewriter, elemTy, srcs[i], loc);\n+protected:\n+  SmallVector<SmallVector<Value>>\n+  getOperands(ConversionPatternRewriter &rewriter, OpAdaptor adaptor,\n+              const unsigned elems, Location loc) const {\n+    SmallVector<SmallVector<Value>> operands(elems);\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n+      for (int i = 0; i < elems; ++i) {\n+        operands[i].push_back(sub_operands[i]);\n+      }\n     }\n-    Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n-    rewriter.replaceOp(op, view);\n-    return success();\n+    return operands;\n   }\n };\n \n template <typename SourceOp, typename DestOp>\n-struct UnaryOpConversion\n-    : public UnaryOpConversionBase<SourceOp, DestOp,\n-                                   UnaryOpConversion<SourceOp, DestOp>> {\n-\n-  explicit UnaryOpConversion(LLVMTypeConverter &typeConverter,\n-                             PatternBenefit benefit = 1)\n-      : UnaryOpConversionBase<SourceOp, DestOp,\n-                              UnaryOpConversion<SourceOp, DestOp>>(\n+struct ElementwiseOpConversion\n+    : public ElementwiseOpConversionBase<\n+          SourceOp, DestOp, ElementwiseOpConversion<SourceOp, DestOp>> {\n+  using Base =\n+      ElementwiseOpConversionBase<SourceOp, DestOp,\n+                                  ElementwiseOpConversion<SourceOp, DestOp>>;\n+  using Base::Base;\n+  using OpAdaptor = typename Base::OpAdaptor;\n+\n+  explicit ElementwiseOpConversion(LLVMTypeConverter &typeConverter,\n+                                   PatternBenefit benefit = 1)\n+      : ElementwiseOpConversionBase<SourceOp, DestOp, ElementwiseOpConversion>(\n             typeConverter, benefit) {}\n \n-  using OpAdaptor = typename SourceOp::Adaptor;\n   // An interface to support variant DestOp builder.\n-  DestOp createDestOp(SourceOp op, ConversionPatternRewriter &rewriter,\n-                      Type elemTy, Value src, Location loc) const {\n-    return rewriter.create<DestOp>(loc, elemTy, src);\n-  }\n-\n-  // Get the source operand of the op.\n-  Value getSrc(OpAdaptor adaptor) const {\n-    auto operands = adaptor.getOperands();\n-    if (operands.size() > 1)\n-      llvm::report_fatal_error(\"unary operator has more than one operand\");\n-    return operands.front();\n+  DestOp createDestOp(SourceOp op, OpAdaptor adaptor,\n+                      ConversionPatternRewriter &rewriter, Type elemTy,\n+                      ValueRange operands, Location loc) const {\n+    return rewriter.create<DestOp>(loc, elemTy, operands,\n+                                   adaptor.getAttributes().getValue());\n   }\n };\n \n@@ -2021,25 +1866,22 @@ struct UnaryOpConversion\n //\n \n struct CmpIOpConversion\n-    : public BinaryOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n-                                    CmpIOpConversion> {\n-  explicit CmpIOpConversion(LLVMTypeConverter &typeConverter,\n-                            PatternBenefit benefit = 1)\n-      : BinaryOpConversionBase(typeConverter, benefit) {}\n+    : public ElementwiseOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n+                                         CmpIOpConversion> {\n+  using Base = ElementwiseOpConversionBase<triton::gpu::CmpIOp, LLVM::ICmpOp,\n+                                           CmpIOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n \n   // An interface to support variant DestOp builder.\n-  LLVM::ICmpOp createDestOp(triton::gpu::CmpIOp op,\n+  LLVM::ICmpOp createDestOp(triton::gpu::CmpIOp op, OpAdaptor adaptor,\n                             ConversionPatternRewriter &rewriter, Type elemTy,\n-                            Value lhs, Value rhs, Location loc) const {\n+                            ValueRange operands, Location loc) const {\n     return rewriter.create<LLVM::ICmpOp>(\n-        loc, elemTy, ArithCmpIPredicteToLLVM(op.predicate()), lhs, rhs);\n+        loc, elemTy, ArithCmpIPredicteToLLVM(op.predicate()), operands[0],\n+        operands[1]);\n   }\n \n-  // Get the left operand of the op.\n-  Value getLhs(OpAdaptor adaptor) const { return adaptor.lhs(); }\n-  // Get the right operand of the op.\n-  Value getRhs(OpAdaptor adaptor) const { return adaptor.rhs(); }\n-\n   static LLVM::ICmpPredicate\n   ArithCmpIPredicteToLLVM(arith::CmpIPredicate predicate) {\n     switch (predicate) {\n@@ -2065,25 +1907,22 @@ struct CmpIOpConversion\n };\n \n struct CmpFOpConversion\n-    : public BinaryOpConversionBase<triton::gpu::CmpFOp, LLVM::FCmpOp,\n-                                    CmpFOpConversion> {\n-  explicit CmpFOpConversion(LLVMTypeConverter &typeConverter,\n-                            PatternBenefit benefit = 1)\n-      : BinaryOpConversionBase(typeConverter, benefit) {}\n+    : public ElementwiseOpConversionBase<triton::gpu::CmpFOp, LLVM::FCmpOp,\n+                                         CmpFOpConversion> {\n+  using Base = ElementwiseOpConversionBase<triton::gpu::CmpFOp, LLVM::FCmpOp,\n+                                           CmpFOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n \n   // An interface to support variant DestOp builder.\n-  LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op,\n+  LLVM::FCmpOp createDestOp(triton::gpu::CmpFOp op, OpAdaptor adaptor,\n                             ConversionPatternRewriter &rewriter, Type elemTy,\n-                            Value lhs, Value rhs, Location loc) const {\n+                            ValueRange operands, Location loc) const {\n     return rewriter.create<LLVM::FCmpOp>(\n-        loc, elemTy, ArithCmpFPredicteToLLVM(op.predicate()), lhs, rhs);\n+        loc, elemTy, ArithCmpFPredicteToLLVM(op.predicate()), operands[0],\n+        operands[1]);\n   }\n \n-  // Get the left operand of the op.\n-  Value getLhs(OpAdaptor adaptor) const { return adaptor.lhs(); }\n-  // Get the right operand of the op.\n-  Value getRhs(OpAdaptor adaptor) const { return adaptor.rhs(); }\n-\n   static LLVM::FCmpPredicate\n   ArithCmpFPredicteToLLVM(arith::CmpFPredicate predicate) {\n     switch (predicate) {\n@@ -2369,13 +2208,13 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   }\n   // Potentially we need to store for multiple CTAs in this replication\n   unsigned accumNumReplicates = product<unsigned>(numReplicates);\n-  unsigned elems = getElemsPerThread(srcLayout, srcTy.getShape());\n+  unsigned elems = getElemsPerThread(srcTy);\n   auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n   unsigned inVec = 0;\n   unsigned outVec = 0;\n   auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n \n-  unsigned outElems = getElemsPerThread(dstLayout, shape);\n+  unsigned outElems = getElemsPerThread(dstTy);\n   auto outOrd = getOrder(dstLayout);\n   SmallVector<Value> outVals(outElems);\n \n@@ -2433,7 +2272,7 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   unsigned minVec = std::min(outVec, inVec);\n   unsigned perPhase = dstSharedLayout.getPerPhase();\n   unsigned maxPhase = dstSharedLayout.getMaxPhase();\n-  unsigned numElems = getElemsPerThread(srcBlockedLayout, srcShape);\n+  unsigned numElems = getElemsPerThread(srcTy);\n   auto inVals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n   unsigned srcAccumSizeInThreads =\n       product<unsigned>(srcBlockedLayout.getSizePerThread());\n@@ -2611,7 +2450,8 @@ class MMA16816SmemLoader {\n     Value c = urem(lane, i32_val(8));\n     Value s = udiv(lane, i32_val(8)); // sub-warp-id\n \n-    // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a warp\n+    // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n+    // warp\n     Value s0 = urem(s, i32_val(2));\n     Value s1 = udiv(s, i32_val(2));\n \n@@ -2758,8 +2598,8 @@ class MMA16816SmemLoader {\n       llvm::report_fatal_error(\"unsupported mma type found\");\n \n     // The main difference with the original triton code is we removed the\n-    // prefetch-related logic here for the upstream optimizer phase should take\n-    // care with it, and that is transparent in dot conversion.\n+    // prefetch-related logic here for the upstream optimizer phase should\n+    // take care with it, and that is transparent in dot conversion.\n     auto getPtr = [&](int idx) { return ptrs[idx]; };\n \n     Value ptr = getPtr(ptrIdx);\n@@ -2770,7 +2610,8 @@ class MMA16816SmemLoader {\n           matIdx[order[1]] * sMatStride * sMatShape * sTileStride * elemBytes;\n       PTXBuilder builder;\n \n-      // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a thread.\n+      // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n+      // thread.\n       auto resArgs = builder.newListOperand(4, \"=r\");\n       auto addrArg = builder.newAddrOperand(ptr, \"r\", sOffset);\n \n@@ -3094,7 +2935,7 @@ struct DotOpMmaV2ConversionHelper {\n \n   // Get the M and N of mma instruction shape.\n   static std::tuple<int, int> getInstrShapeMN() {\n-    // According to DotOpMmaV2ConversionHelper::mmaInstrShape, all the M,N are\n+    // According to DotOpConversionHelper::mmaInstrShape, all the M,N are\n     // {16,8}\n     return {16, 8};\n   }\n@@ -4300,8 +4141,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n     if (layout &&\n         (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n          layout.isa<MmaEncodingAttr>())) {\n-      unsigned numElementsPerThread =\n-          getElemsPerThread(layout, type.getShape());\n+      unsigned numElementsPerThread = getElemsPerThread(type);\n       SmallVector<Type, 4> types(numElementsPerThread,\n                                  convertType(type.getElementType()));\n       return LLVM::LLVMStructType::getLiteral(ctx, types);\n@@ -4376,7 +4216,7 @@ struct AsyncWaitOpConversion\n \n     auto ctx = op.getContext();\n     auto loc = op.getLoc();\n-    auto voidTy = LLVM::LLVMVoidType::get(ctx);\n+    auto voidTy = void_ty(ctx);\n     auto ret = ptxBuilder.launch(rewriter, loc, voidTy);\n \n     // Safe to remove the op since it doesn't have any return value.\n@@ -4414,7 +4254,7 @@ struct InsertSliceAsyncOpConversion\n \n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto resTy = dst.getType().cast<RankedTensorType>();\n-    auto resElemTy = resTy.getElementType();\n+    auto resElemTy = getTypeConverter()->convertType(resTy.getElementType());\n     auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n     auto resSharedLayout = resTy.getEncoding().cast<SharedEncodingAttr>();\n     auto srcShape = srcTy.getShape();\n@@ -4428,14 +4268,14 @@ struct InsertSliceAsyncOpConversion\n     Value llIndex = adaptor.index();\n \n     // %src\n-    auto srcElems = getLLVMElems(src, llSrc, srcBlockedLayout, rewriter, loc);\n+    auto srcElems = getLLVMElems(src, llSrc, rewriter, loc);\n \n     // %dst\n     auto axis = op->getAttrOfType<IntegerAttr>(\"axis\").getInt();\n     assert(axis == 0 && \"insert_slice_async: Only axis=0 is supported for now\");\n     auto dstBase = createIndexAttrConstant(rewriter, loc,\n                                            getTypeConverter()->getIndexType(),\n-                                           product<int64_t>(resTy.getShape()));\n+                                           product<int64_t>(srcTy.getShape()));\n     Value offset = mul(llIndex, dstBase);\n     auto dstPtrTy = LLVM::LLVMPointerType::get(\n         getTypeConverter()->convertType(resTy.getElementType()), 3);\n@@ -4444,7 +4284,7 @@ struct InsertSliceAsyncOpConversion\n     // %mask\n     SmallVector<Value> maskElems;\n     if (llMask) {\n-      maskElems = getLLVMElems(mask, llMask, srcBlockedLayout, rewriter, loc);\n+      maskElems = getLLVMElems(mask, llMask, rewriter, loc);\n       assert(srcElems.size() == maskElems.size());\n     }\n \n@@ -4455,15 +4295,14 @@ struct InsertSliceAsyncOpConversion\n       // It's not necessary for now because the pipeline pass will skip\n       // generating insert_slice_async if the load op has any \"other\" tensor.\n       assert(false && \"insert_slice_async: Other value not supported yet\");\n-      otherElems =\n-          getLLVMElems(other, llOther, srcBlockedLayout, rewriter, loc);\n+      otherElems = getLLVMElems(other, llOther, rewriter, loc);\n       assert(srcElems.size() == otherElems.size());\n     }\n \n-    unsigned inVec = getVectorizeSize(src, srcBlockedLayout);\n+    unsigned inVec = getVectorSize(src);\n     unsigned outVec = resSharedLayout.getVec();\n     unsigned minVec = std::min(outVec, inVec);\n-    unsigned numElems = getElemsPerThread(srcBlockedLayout, srcShape);\n+    unsigned numElems = getElemsPerThread(srcTy);\n     unsigned perPhase = resSharedLayout.getPerPhase();\n     unsigned maxPhase = resSharedLayout.getMaxPhase();\n     auto sizePerThread = srcBlockedLayout.getSizePerThread();\n@@ -4546,44 +4385,130 @@ struct InsertSliceAsyncOpConversion\n       auto numWordElems = bitWidth / resElemTy.getIntOrFloatBitWidth();\n \n       // XXX(Keren): Tune CG and CA here.\n+      auto byteWidth = bitWidth / 8;\n       CacheModifier srcCacheModifier =\n-          bitWidth == 128 ? CacheModifier::CG : CacheModifier::CA;\n-      assert(bitWidth == 128 || bitWidth == 64 || bitWidth == 32);\n+          byteWidth == 16 ? CacheModifier::CG : CacheModifier::CA;\n+      assert(byteWidth == 16 || byteWidth == 8 || byteWidth == 4);\n+      auto resByteWidth = resElemTy.getIntOrFloatBitWidth() / 8;\n \n-      for (int wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n+      auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n+      for (unsigned wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n         PTXBuilder ptxBuilder;\n-        auto &copyAsyncOp = *ptxBuilder.create<PTXCpAsyncLoadInstr>(\n-            srcCacheModifier, op.evict());\n-\n-        auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n-        auto *dstOperand =\n-            ptxBuilder.newAddrOperand(tileOffset, \"r\", baseOffset);\n-        auto *srcOperand = ptxBuilder.newAddrOperand(srcElems[vecIdx], \"l\");\n-        auto *copySize = ptxBuilder.newConstantOperand(bitWidth);\n+        auto wordElemIdx = wordIdx * numWordElems;\n+        auto &copyAsyncOp =\n+            *ptxBuilder.create<PTXCpAsyncLoadInstr>(srcCacheModifier);\n+        auto *dstOperand = ptxBuilder.newAddrOperand(\n+            tileOffset, \"r\", (wordElemIdx + baseOffset) * resByteWidth);\n+        auto *srcOperand =\n+            ptxBuilder.newAddrOperand(srcElems[elemIdx + wordElemIdx], \"l\");\n+        auto *copySize = ptxBuilder.newConstantOperand(byteWidth);\n         auto *srcSize = copySize;\n         if (op.mask()) {\n           // We don't use predicate in this case, setting src-size to 0\n           // if there's any mask. cp.async will automatically fill the\n           // remaining slots with 0 if cp-size > src-size.\n           // XXX(Keren): Always assume other = 0 for now.\n-          auto selectOp = select(maskElems[vecIdx + wordIdx * numWordElems],\n-                                 i32_val(bitWidth), i32_val(0));\n+          auto selectOp = select(maskElems[elemIdx + wordElemIdx],\n+                                 i32_val(byteWidth), i32_val(0));\n           srcSize = ptxBuilder.newOperand(selectOp, \"r\");\n         }\n         copyAsyncOp(dstOperand, srcOperand, copySize, srcSize);\n-        ptxBuilder.launch(rewriter, loc, LLVM::LLVMVoidType::get(getContext()));\n+        ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n       }\n     }\n \n     PTXBuilder ptxBuilder;\n     ptxBuilder.create<PTXCpAsyncCommitGroupInstr>()->operator()();\n-    auto ret =\n-        ptxBuilder.launch(rewriter, loc, LLVM::LLVMVoidType::get(getContext()));\n-    rewriter.replaceOp(op, ret);\n+    ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n+    rewriter.replaceOp(op, llDst);\n     return success();\n   }\n };\n \n+struct ExtElemwiseOpConversion\n+    : public ElementwiseOpConversionBase<\n+          triton::ExtElemwiseOp, LLVM::LLVMFuncOp, ExtElemwiseOpConversion> {\n+  using Base =\n+      ElementwiseOpConversionBase<triton::ExtElemwiseOp, LLVM::LLVMFuncOp,\n+                                  ExtElemwiseOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(triton::ExtElemwiseOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+    StringRef funcName = op.symbol();\n+    if (funcName.empty())\n+      llvm::errs() << \"ExtElemwiseOpConversion\";\n+\n+    Type funcType = getFunctionType(elemTy, operands);\n+    LLVM::LLVMFuncOp funcOp =\n+        appendOrGetFuncOp(rewriter, op, funcName, funcType);\n+    return rewriter.create<LLVM::CallOp>(loc, funcOp, operands).getResult(0);\n+  }\n+\n+private:\n+  Type getFunctionType(Type resultType, ValueRange operands) const {\n+    SmallVector<Type> operandTypes(operands.getTypes());\n+    return LLVM::LLVMFunctionType::get(resultType, operandTypes);\n+  }\n+\n+  LLVM::LLVMFuncOp appendOrGetFuncOp(ConversionPatternRewriter &rewriter,\n+                                     triton::ExtElemwiseOp op,\n+                                     StringRef funcName, Type funcType) const {\n+    using LLVM::LLVMFuncOp;\n+\n+    auto funcAttr = StringAttr::get(op->getContext(), funcName);\n+    Operation *funcOp = SymbolTable::lookupNearestSymbolFrom(op, funcAttr);\n+    if (funcOp)\n+      return cast<LLVMFuncOp>(*funcOp);\n+\n+    mlir::OpBuilder b(op->getParentOfType<LLVMFuncOp>());\n+    auto ret = b.create<LLVMFuncOp>(op->getLoc(), funcName, funcType);\n+    ret.getOperation()->setAttr(\n+        \"libname\", StringAttr::get(op->getContext(), op.libname()));\n+    ret.getOperation()->setAttr(\n+        \"libpath\", StringAttr::get(op->getContext(), op.libpath()));\n+    return ret;\n+  }\n+};\n+\n+struct FDivOpConversion\n+    : ElementwiseOpConversionBase<mlir::arith::DivFOp, LLVM::InlineAsmOp,\n+                                  FDivOpConversion> {\n+  using Base = ElementwiseOpConversionBase<mlir::arith::DivFOp,\n+                                           LLVM::InlineAsmOp, FDivOpConversion>;\n+  using Base::Base;\n+  using Adaptor = typename Base::OpAdaptor;\n+\n+  Value createDestOp(mlir::arith::DivFOp op, OpAdaptor adaptor,\n+                     ConversionPatternRewriter &rewriter, Type elemTy,\n+                     ValueRange operands, Location loc) const {\n+\n+    PTXBuilder ptxBuilder;\n+    auto &fdiv = *ptxBuilder.create<PTXInstr>(\"div\");\n+    unsigned bitwidth = elemTy.getIntOrFloatBitWidth();\n+    if (32 == bitwidth) {\n+      fdiv.o(\"full\").o(\"f32\");\n+      auto res = ptxBuilder.newOperand(\"=r\");\n+      auto lhs = ptxBuilder.newOperand(operands[0], \"r\");\n+      auto rhs = ptxBuilder.newOperand(operands[1], \"r\");\n+      fdiv(res, lhs, rhs);\n+    } else if (64 == bitwidth) {\n+      fdiv.o(\"rn\").o(\"f64\");\n+      auto res = ptxBuilder.newOperand(\"=l\");\n+      auto lhs = ptxBuilder.newOperand(operands[0], \"l\");\n+      auto rhs = ptxBuilder.newOperand(operands[1], \"l\");\n+      fdiv(res, lhs, rhs);\n+    } else {\n+      assert(0 && bitwidth && \"not supported\");\n+    }\n+\n+    Value ret = ptxBuilder.launch(rewriter, loc, elemTy, false);\n+    return ret;\n+  }\n+};\n+\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -4596,12 +4521,13 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<AsyncWaitOpConversion>(typeConverter, benefit);\n \n #define POPULATE_TERNARY_OP(SRC_OP, DST_OP)                                    \\\n-  patterns.add<TernaryOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n   POPULATE_TERNARY_OP(triton::gpu::SelectOp, LLVM::SelectOp);\n #undef POPULATE_TERNARY_OP\n \n #define POPULATE_BINARY_OP(SRC_OP, DST_OP)                                     \\\n-  patterns.add<BinaryOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+\n   POPULATE_BINARY_OP(arith::SubIOp, LLVM::SubOp) // -\n   POPULATE_BINARY_OP(arith::SubFOp, LLVM::FSubOp)\n   POPULATE_BINARY_OP(arith::AddIOp, LLVM::AddOp) // +\n@@ -4625,7 +4551,8 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<CmpIOpConversion>(typeConverter, benefit);\n   patterns.add<CmpFOpConversion>(typeConverter, benefit);\n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\\n-  patterns.add<UnaryOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+  patterns.add<ElementwiseOpConversion<SRC_OP, DST_OP>>(typeConverter, benefit);\n+\n   POPULATE_UNARY_OP(arith::TruncIOp, LLVM::TruncOp)\n   POPULATE_UNARY_OP(arith::TruncFOp, LLVM::FPTruncOp)\n   POPULATE_UNARY_OP(arith::ExtSIOp, LLVM::SExtOp)\n@@ -4635,11 +4562,20 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   POPULATE_UNARY_OP(arith::UIToFPOp, LLVM::UIToFPOp)\n   POPULATE_UNARY_OP(arith::SIToFPOp, LLVM::SIToFPOp)\n   POPULATE_UNARY_OP(arith::ExtFOp, LLVM::FPExtOp)\n+  POPULATE_UNARY_OP(math::LogOp, math::LogOp)\n+  POPULATE_UNARY_OP(math::CosOp, math::CosOp)\n+  POPULATE_UNARY_OP(math::SinOp, math::SinOp)\n+  POPULATE_UNARY_OP(math::SqrtOp, math::SqrtOp)\n+  POPULATE_UNARY_OP(math::ExpOp, math::ExpOp)\n   POPULATE_UNARY_OP(triton::BitcastOp, LLVM::BitcastOp)\n   POPULATE_UNARY_OP(triton::IntToPtrOp, LLVM::IntToPtrOp)\n   POPULATE_UNARY_OP(triton::PtrToIntOp, LLVM::PtrToIntOp)\n #undef POPULATE_UNARY_OP\n \n+  patterns.add<FDivOpConversion>(typeConverter, benefit);\n+\n+  patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n+\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n   patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n@@ -4679,21 +4615,39 @@ class ConvertTritonGPUToLLVM\n \n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    // step 1: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // step 2: Allocate for shared memories\n-    // step 3: Convert the rest of ops via partial conversion\n-    // The reason for a seperation between 1/3 is that, step 2 is out of\n+    // step 1: Allocate shared memories and insert barriers\n+    // setp 2: Convert SCF to CFG\n+    // step 3: Convert FuncOp to LLVMFuncOp via partial conversion\n+    // step 4: Convert the rest of ops via partial conversion\n+    // The reason for putting step 1 before step 2 is that the membar analysis\n+    // currently only supports SCF but not CFG.\n+    // The reason for a seperation between 1/4 is that, step 3 is out of\n     // the scope of Dialect Conversion, thus we need to make sure the smem\n-    // is not revised during the conversion of step 3.\n+    // is not revised during the conversion of step 4.\n+    Allocation allocation(mod);\n+    MembarAnalysis membar(&allocation);\n+\n+    RewritePatternSet scf_patterns(context);\n+    mlir::populateLoopToStdConversionPatterns(scf_patterns);\n+    mlir::ConversionTarget scf_target(*context);\n+    scf_target.addIllegalOp<scf::ForOp, scf::IfOp, scf::ParallelOp,\n+                            scf::WhileOp, scf::ExecuteRegionOp>();\n+    scf_target.markUnknownOpDynamicallyLegal([](Operation *) { return true; });\n+    if (failed(\n+            applyPartialConversion(mod, scf_target, std::move(scf_patterns))))\n+      return signalPassFailure();\n+\n     RewritePatternSet func_patterns(context);\n     func_patterns.add<FuncOpConversion>(typeConverter, numWarps, 1 /*benefit*/);\n     if (failed(\n             applyPartialConversion(mod, funcTarget, std::move(func_patterns))))\n       return signalPassFailure();\n \n-    Allocation allocation(mod);\n     auto axisAnalysis = runAxisAnalysis(mod);\n     initSharedMemory(allocation.getSharedMemorySize(), typeConverter);\n+    mod->setAttr(\"triton_gpu.shared\",\n+                 mlir::IntegerAttr::get(mlir::IntegerType::get(context, 32),\n+                                        allocation.getSharedMemorySize()));\n \n     // We set a higher benefit here to ensure triton's patterns runs before\n     // arith patterns for some encoding not supported by the community\n@@ -4736,9 +4690,11 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n   OpBuilder b(mod.getBodyRegion());\n   auto loc = mod.getLoc();\n   auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n-  auto arrayTy = LLVM::LLVMArrayType::get(elemTy, size);\n+  // Set array size 0 and external linkage indicates that we use dynamic shared\n+  // allocation to allow a larger shared memory size for each kernel.\n+  auto arrayTy = LLVM::LLVMArrayType::get(elemTy, 0);\n   auto global = b.create<LLVM::GlobalOp>(\n-      loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::Internal,\n+      loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::External,\n       \"global_smem\", /*value=*/Attribute(),\n       /*alignment=*/0, mlir::gpu::GPUDialect::getWorkgroupAddressSpace());\n   SmallVector<LLVM::LLVMFuncOp> funcs;"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 12, "deletions": 5, "changes": 17, "file_content_changes": "@@ -131,6 +131,16 @@ void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n }\n \n //-- LoadOp --\n+static Type getLoadOpResultType(::mlir::OpBuilder &builder, Type ptrType) {\n+  auto ptrTensorType = ptrType.dyn_cast<RankedTensorType>();\n+  if (!ptrTensorType)\n+    return ptrType.cast<PointerType>().getPointeeType();\n+  auto shape = ptrTensorType.getShape();\n+  Type elementType =\n+      ptrTensorType.getElementType().cast<PointerType>().getPointeeType();\n+  return RankedTensorType::get(shape, elementType);\n+}\n+\n void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                    ::mlir::Value ptr, ::mlir::triton::CacheModifier cache,\n                    ::mlir::triton::EvictionPolicy evict, bool isVolatile) {\n@@ -150,11 +160,8 @@ void LoadOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                    ::mlir::Value ptr, ::mlir::Value mask, ::mlir::Value other,\n                    ::mlir::triton::CacheModifier cache,\n                    ::mlir::triton::EvictionPolicy evict, bool isVolatile) {\n-  TensorType ptrType = ptr.getType().cast<TensorType>();\n-  Type elementType =\n-      ptrType.getElementType().cast<PointerType>().getPointeeType();\n-  auto shape = ptrType.getShape();\n-  Type resultType = RankedTensorType::get(shape, elementType);\n+  Type resultType = getLoadOpResultType(builder, ptr.getType());\n+\n   state.addOperands(ptr);\n   if (mask) {\n     state.addOperands(mask);"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -43,7 +43,12 @@ static Type getPointeeType(Type type) {\n namespace gpu {\n \n // TODO: Inheritation of layout attributes\n-unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n+unsigned getElemsPerThread(Type type) {\n+  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n+    return 1;\n+  auto tensorType = type.cast<RankedTensorType>();\n+  auto layout = tensorType.getEncoding();\n+  auto shape = tensorType.getShape();\n   size_t rank = shape.size();\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return blockedLayout.getElemsPerThread(shape);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "file_content_changes": "@@ -103,16 +103,21 @@ class SimplifyConversion : public mlir::RewritePattern {\n     if (!arg)\n       return mlir::failure();\n     // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n-    // cvt(insert_slice(x), type2) -> extract_slice(cvt(x, type2))\n     auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n     if (alloc_tensor) {\n       rewriter.replaceOpWithNewOp<triton::gpu::AllocTensorOp>(\n           op, op->getResult(0).getType());\n       return mlir::success();\n     }\n+    // cvt(insert_slice(x), type2) -> insert_slice(cvt(x, type2))\n     auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n     if (insert_slice) {\n       auto newType = op->getResult(0).getType();\n+      // Ensure that the new insert_slice op is placed in the same place as the\n+      // old insert_slice op. Otherwise, the new insert_slice op may be placed\n+      // after the async_wait op, which is not allowed.\n+      OpBuilder::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPoint(insert_slice);\n       auto new_arg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           op->getLoc(), newType, insert_slice.dst());\n       rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n@@ -126,6 +131,11 @@ class SimplifyConversion : public mlir::RewritePattern {\n     auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n     if (extract_slice) {\n       auto origType = extract_slice.src().getType().cast<RankedTensorType>();\n+      // Ensure that the new extract_slice op is placed in the same place as the\n+      // old extract_slice op. Otherwise, the new extract_slice op may be placed\n+      // after the async_wait op, which is not allowed.\n+      OpBuilder::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPoint(extract_slice);\n       auto newType = RankedTensorType::get(\n           origType.getShape(), origType.getElementType(),\n           op->getResult(0).getType().cast<RankedTensorType>().getEncoding());"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -78,6 +78,9 @@ class LoopPipeliner {\n   /// emit pipelined loads (before loop body)\n   void emitPrologue();\n \n+  /// emit pipelined loads (after loop body)\n+  void emitEpilogue();\n+\n   /// create the new ForOp (add new args & insert prefetched ops)\n   scf::ForOp createNewForOp();\n \n@@ -362,6 +365,23 @@ void LoopPipeliner::emitPrologue() {\n         loadStageBuffer[loadOp][numStages - 1], loopIterIdx, /*axis*/ 0);\n     loadsExtract[loadOp] = extractSlice;\n   }\n+  // bump up loopIterIdx, this is used for getting the correct slice for the\n+  // *next* iteration\n+  loopIterIdx = builder.create<arith::AddIOp>(\n+      loopIterIdx.getLoc(), loopIterIdx,\n+      builder.create<arith::ConstantIntOp>(loopIterIdx.getLoc(), 1, 32));\n+}\n+\n+void LoopPipeliner::emitEpilogue() {\n+  // If there's any outstanding async copies, we need to wait for them.\n+  // TODO(Keren): We may want to completely avoid the async copies in the last\n+  // few iterations by setting is_masked attribute to true. We don't want to use\n+  // the mask operand because it's a tensor but not a scalar.\n+  OpBuilder builder(forOp);\n+  OpBuilder::InsertionGuard g(builder);\n+  builder.setInsertionPointAfter(forOp);\n+  Operation *asyncWait =\n+      builder.create<triton::gpu::AsyncWaitOp>(forOp.getLoc(), 0);\n }\n \n scf::ForOp LoopPipeliner::createNewForOp() {\n@@ -581,6 +601,8 @@ struct PipelinePass : public TritonGPUPipelineBase<PipelinePass> {\n \n       scf::ForOp newForOp = pipeliner.createNewForOp();\n \n+      pipeliner.emitEpilogue();\n+\n       // replace the original loop\n       for (unsigned i = 0; i < forOp->getNumResults(); ++i)\n         forOp->getResult(i).replaceAllUsesWith(newForOp->getResult(i));"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 73, "deletions": 1, "changes": 74, "file_content_changes": "@@ -16,6 +16,9 @@\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n #include \"triton/tools/sys/getenv.hpp\"\n #include \"llvm/IR/Constants.h\"\n+#include \"llvm/IRReader/IRReader.h\"\n+#include \"llvm/Linker/Linker.h\"\n+#include \"llvm/Support/SourceMgr.h\"\n \n namespace mlir {\n namespace triton {\n@@ -136,23 +139,92 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnChange=*/true,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n-  pm.addPass(mlir::createLowerToCFGPass());\n   pm.addPass(createConvertTritonGPUToLLVMPass());\n   // Conanicalize to eliminate the remaining UnrealizedConversionCastOp\n   pm.addPass(mlir::createCanonicalizerPass());\n+  pm.addPass(mlir::createCSEPass()); // Simplify the IR to improve readability.\n+  pm.addPass(mlir::createSymbolDCEPass());\n+  pm.addPass(mlir::createCanonicalizerPass());\n \n   if (failed(pm.run(module))) {\n     llvm::errs() << \"Pass execution failed\";\n     return nullptr;\n   }\n \n+  std::map<std::string, std::string> extern_libs;\n+  SmallVector<LLVM::LLVMFuncOp> funcs;\n+  module.walk([&](LLVM::LLVMFuncOp func) {\n+    if (func.isExternal())\n+      funcs.push_back(func);\n+  });\n+\n+  for (auto &func : funcs) {\n+    if (func.getOperation()->hasAttr(\"libname\")) {\n+      auto name =\n+          func.getOperation()->getAttr(\"libname\").dyn_cast<StringAttr>();\n+      auto path =\n+          func.getOperation()->getAttr(\"libpath\").dyn_cast<StringAttr>();\n+      if (name) {\n+        std::string lib_name = name.str();\n+        extern_libs[lib_name] = path.str();\n+      }\n+    }\n+  }\n+\n+  if (module.getOperation()->hasAttr(\"triton_gpu.externs\")) {\n+    auto dict = module.getOperation()\n+                    ->getAttr(\"triton_gpu.externs\")\n+                    .dyn_cast<DictionaryAttr>();\n+    for (auto &attr : dict) {\n+      extern_libs[attr.getName().strref().trim().str()] =\n+          attr.getValue().dyn_cast<StringAttr>().strref().trim().str();\n+    }\n+  }\n+\n   auto llvmir = translateLLVMToLLVMIR(llvmContext, module);\n   if (!llvmir) {\n     llvm::errs() << \"Translate to LLVM IR failed\";\n+    return nullptr;\n+  }\n+\n+  llvm::SMDiagnostic err;\n+  for (auto &lib : extern_libs) {\n+    auto ext_mod = llvm::parseIRFile(lib.second, err, *llvmContext);\n+    if (!ext_mod) {\n+      llvm::errs() << \"Failed to load extern lib \" << lib.first;\n+      return nullptr;\n+    }\n+    ext_mod->setTargetTriple(llvmir->getTargetTriple());\n+    ext_mod->setDataLayout(llvmir->getDataLayout());\n+\n+    if (llvm::Linker::linkModules(*llvmir, std::move(ext_mod))) {\n+      llvm::errs() << \"Failed to link extern lib \" << lib.first;\n+      return nullptr;\n+    }\n   }\n \n   return llvmir;\n }\n \n+void addExternalLibs(mlir::ModuleOp &module,\n+                     const std::vector<std::string> &names,\n+                     const std::vector<std::string> &paths) {\n+  if (names.empty() || names.size() != paths.size())\n+    return;\n+\n+  llvm::SmallVector<NamedAttribute, 2> attrs;\n+\n+  for (size_t i = 0; i < names.size(); ++i) {\n+    auto name = StringAttr::get(module->getContext(), names[i]);\n+    auto path = StringAttr::get(module->getContext(), paths[i]);\n+    NamedAttribute attr(name, path);\n+    attrs.push_back(attr);\n+  }\n+\n+  DictionaryAttr dict = DictionaryAttr::get(module->getContext(), attrs);\n+  module.getOperation()->setAttr(\"triton_gpu.externs\", dict);\n+  return;\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -1257,8 +1257,8 @@ void init_triton_translation(py::module &m) {\n   using ret = py::return_value_policy;\n \n   m.def(\"get_shared_memory_size\", [](mlir::ModuleOp module) {\n-    auto pass = std::make_unique<mlir::Allocation>(module);\n-    return pass->getSharedMemorySize();\n+    return module->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.shared\")\n+        .getInt();\n   });\n \n   m.def(\n@@ -1335,6 +1335,12 @@ void init_triton_translation(py::module &m) {\n           py::bytes bytes(cubin);\n           return bytes;\n         });\n+\n+  m.def(\"add_external_libs\",\n+        [](mlir::ModuleOp &op, const std::vector<std::string> &names,\n+           const std::vector<std::string> &paths) {\n+          ::mlir::triton::addExternalLibs(op, names, paths);\n+        });\n }\n \n void init_triton(py::module &m) {"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 142, "deletions": 139, "changes": 281, "file_content_changes": "@@ -556,45 +556,45 @@ def make_ptr_str(name, shape):\n # # ---------------\n \n \n-# @triton.jit\n-# def fn(a, b):\n-#     return a + b, \\\n-#         a - b, \\\n-#         a * b\n+@triton.jit\n+def fn(a, b):\n+    return a + b, \\\n+        a - b, \\\n+        a * b\n \n \n-# def test_tuples():\n-#     device = 'cuda'\n+def test_tuples():\n+    device = 'cuda'\n \n-#     @triton.jit\n-#     def with_fn(X, Y, A, B, C):\n-#         x = tl.load(X)\n-#         y = tl.load(Y)\n-#         a, b, c = fn(x, y)\n-#         tl.store(A, a)\n-#         tl.store(B, b)\n-#         tl.store(C, c)\n+    @triton.jit\n+    def with_fn(X, Y, A, B, C):\n+        x = tl.load(X)\n+        y = tl.load(Y)\n+        a, b, c = fn(x, y)\n+        tl.store(A, a)\n+        tl.store(B, b)\n+        tl.store(C, c)\n \n-#     @triton.jit\n-#     def without_fn(X, Y, A, B, C):\n-#         x = tl.load(X)\n-#         y = tl.load(Y)\n-#         a, b, c = x + y, x - y, x * y\n-#         tl.store(A, a)\n-#         tl.store(B, b)\n-#         tl.store(C, c)\n-\n-#     x = torch.tensor([1.3], device=device, dtype=torch.float32)\n-#     y = torch.tensor([1.9], device=device, dtype=torch.float32)\n-#     a_tri = torch.tensor([0], device=device, dtype=torch.float32)\n-#     b_tri = torch.tensor([0], device=device, dtype=torch.float32)\n-#     c_tri = torch.tensor([0], device=device, dtype=torch.float32)\n-#     for kernel in [with_fn, without_fn]:\n-#         kernel[(1, )](x, y, a_tri, b_tri, c_tri, num_warps=1)\n-#         a_ref, b_ref, c_ref = x + y, x - y, x * y\n-#         assert a_tri == a_ref\n-#         assert b_tri == b_ref\n-#         assert c_tri == c_ref\n+    @triton.jit\n+    def without_fn(X, Y, A, B, C):\n+        x = tl.load(X)\n+        y = tl.load(Y)\n+        a, b, c = x + y, x - y, x * y\n+        tl.store(A, a)\n+        tl.store(B, b)\n+        tl.store(C, c)\n+\n+    x = torch.tensor([1.3], device=device, dtype=torch.float32)\n+    y = torch.tensor([1.9], device=device, dtype=torch.float32)\n+    a_tri = torch.tensor([0], device=device, dtype=torch.float32)\n+    b_tri = torch.tensor([0], device=device, dtype=torch.float32)\n+    c_tri = torch.tensor([0], device=device, dtype=torch.float32)\n+    for kernel in [with_fn, without_fn]:\n+        kernel[(1, )](x, y, a_tri, b_tri, c_tri, num_warps=1)\n+        a_ref, b_ref, c_ref = x + y, x - y, x * y\n+        assert a_tri == a_ref\n+        assert b_tri == b_ref\n+        assert c_tri == c_ref\n \n \n # # ---------------\n@@ -709,75 +709,77 @@ def make_ptr_str(name, shape):\n # # ---------------\n \n \n-# @pytest.mark.parametrize(\"dtype_x, dtype_z, bitcast\", [\n-#     (dtype_x, dtype_z, False)\n-#     for dtype_x in dtypes\n-#     for dtype_z in dtypes\n-# ] + [\n-#     ('float32', 'bfloat16', False),\n-#     ('bfloat16', 'float32', False),\n-#     ('float32', 'int32', True),\n-#     ('float32', 'int1', False),\n-# ] + [\n-#     (f'uint{x}', f'int{x}', True) for x in [8, 16, 32, 64]\n-# ] + [\n-#     (f'int{x}', f'uint{x}', True) for x in [8, 16, 32, 64]\n-# ])\n-# def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n-#     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n-#     x0 = 43 if dtype_x in int_dtypes else 43.5\n-#     if dtype_x in float_dtypes and dtype_z == 'int1':\n-#         x0 = 0.5\n-#     if dtype_x.startswith('bfloat'):\n-#         x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n-#     else:\n-#         x = np.array([x0], dtype=getattr(np, dtype_x))\n-#         x_tri = to_triton(x)\n+@pytest.mark.parametrize(\"dtype_x, dtype_z, bitcast\", [\n+    (dtype_x, dtype_z, False)\n+    for dtype_x in dtypes\n+    for dtype_z in dtypes\n+] + [\n+    # TODO:\n+    # ('float32', 'bfloat16', False),\n+    # ('bfloat16', 'float32', False),\n+    ('float32', 'int32', True),\n+    # TODO:\n+    # ('float32', 'int1', False),\n+] + [\n+    (f'uint{x}', f'int{x}', True) for x in [8, 16, 32, 64]\n+] + [\n+    (f'int{x}', f'uint{x}', True) for x in [8, 16, 32, 64]\n+])\n+def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n+    # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n+    x0 = 43 if dtype_x in int_dtypes else 43.5\n+    if dtype_x in float_dtypes and dtype_z == 'int1':\n+        x0 = 0.5\n+    if dtype_x.startswith('bfloat'):\n+        x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n+    else:\n+        x = np.array([x0], dtype=getattr(np, dtype_x))\n+        x_tri = to_triton(x)\n \n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, Z, BITCAST: tl.constexpr):\n-#         x = tl.load(X)\n-#         z = x.to(Z.dtype.element_ty, bitcast=BITCAST)\n-#         tl.store(Z, z)\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BITCAST: tl.constexpr):\n+        x = tl.load(X)\n+        z = x.to(Z.dtype.element_ty, bitcast=BITCAST)\n+        tl.store(Z, z)\n \n-#     dtype_z_np = dtype_z if dtype_z != 'int1' else 'bool_'\n-#     # triton result\n-#     if dtype_z.startswith('bfloat'):\n-#         z_tri = torch.empty((1,), dtype=getattr(torch, dtype_z), device=device)\n-#     else:\n-#         z_tri = to_triton(np.empty((1, ), dtype=getattr(np, dtype_z_np)), device=device)\n-#     kernel[(1, )](x_tri, z_tri, BITCAST=bitcast)\n-#     # torch result\n-#     if dtype_z.startswith('bfloat') or dtype_x.startswith('bfloat'):\n-#         assert bitcast is False\n-#         z_ref = x_tri.to(z_tri.dtype)\n-#         assert z_tri == z_ref\n-#     else:\n-#         if bitcast:\n-#             z_ref = x.view(getattr(np, dtype_z_np))\n-#         else:\n-#             z_ref = x.astype(getattr(np, dtype_z_np))\n-#         assert to_numpy(z_tri) == z_ref\n+    dtype_z_np = dtype_z if dtype_z != 'int1' else 'bool_'\n+    # triton result\n+    if dtype_z.startswith('bfloat'):\n+        z_tri = torch.empty((1,), dtype=getattr(torch, dtype_z), device=device)\n+    else:\n+        z_tri = to_triton(np.empty((1, ), dtype=getattr(np, dtype_z_np)), device=device)\n+    kernel[(1, )](x_tri, z_tri, BITCAST=bitcast)\n+    # torch result\n+    if dtype_z.startswith('bfloat') or dtype_x.startswith('bfloat'):\n+        assert bitcast is False\n+        z_ref = x_tri.to(z_tri.dtype)\n+        assert z_tri == z_ref\n+    else:\n+        if bitcast:\n+            z_ref = x.view(getattr(np, dtype_z_np))\n+        else:\n+            z_ref = x.astype(getattr(np, dtype_z_np))\n+        assert to_numpy(z_tri) == z_ref\n \n \n-# def test_store_bool():\n-#     \"\"\"Tests that boolean True is stored as 1\"\"\"\n-#     @triton.jit\n-#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n-#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-#         mask = offsets < n_elements\n-#         input = tl.load(input_ptr + offsets, mask=mask)\n-#         output = input\n-#         tl.store(output_ptr + offsets, output, mask=mask)\n+def test_store_bool():\n+    \"\"\"Tests that boolean True is stored as 1\"\"\"\n+    @triton.jit\n+    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        input = tl.load(input_ptr + offsets, mask=mask)\n+        output = input\n+        tl.store(output_ptr + offsets, output, mask=mask)\n \n-#     src = torch.tensor([True, False], dtype=torch.bool, device='cuda')\n-#     n_elements = src.numel()\n-#     dst = torch.empty_like(src)\n-#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-#     copy_kernel[grid](src, dst, n_elements, BLOCK_SIZE=1024)\n+    src = torch.tensor([True, False], dtype=torch.bool, device='cuda')\n+    n_elements = src.numel()\n+    dst = torch.empty_like(src)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    copy_kernel[grid](src, dst, n_elements, BLOCK_SIZE=1024)\n \n-#     assert (to_numpy(src).view('uint8') == to_numpy(dst).view('uint8')).all()\n+    assert (to_numpy(src).view('uint8') == to_numpy(dst).view('uint8')).all()\n \n \n # @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n@@ -990,48 +992,49 @@ def make_ptr_str(name, shape):\n # # ---------------\n \n \n-# @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n-#                          [(dtype, shape, perm)\n-#                           for dtype in ['bfloat16', 'float16', 'float32']\n-#                              for shape in [(64, 64), (128, 128)]\n-#                              for perm in [(1, 0)]])\n-# def test_permute(dtype_str, shape, perm, device='cuda'):\n-#     check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+@pytest.mark.parametrize(\"dtype_str, shape, perm\",\n+                         [(dtype, shape, perm)\n+                          # TODO: bfloat16\n+                          for dtype in ['float16', 'float32']\n+                             for shape in [(64, 64), (128, 128)]\n+                             for perm in [(1, 0)]])\n+def test_permute(dtype_str, shape, perm, device='cuda'):\n+    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n \n-#     # triton kernel\n-#     @triton.jit\n-#     def kernel(X, stride_xm, stride_xn,\n-#                Z, stride_zm, stride_zn,\n-#                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n-#         off_m = tl.arange(0, BLOCK_M)\n-#         off_n = tl.arange(0, BLOCK_N)\n-#         Xs = X + off_m[:, None] * stride_xm + off_n[None, :] * stride_xn\n-#         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n-#         tl.store(Zs, tl.load(Xs))\n-#     # input\n-#     x = numpy_random(shape, dtype_str=dtype_str)\n-#     # triton result\n-#     z_tri = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n-#     z_tri_contiguous = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n-#     x_tri = to_triton(x, device=device, dst_type=dtype_str)\n-#     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n-#                          z_tri, z_tri.stride(1), z_tri.stride(0),\n-#                          BLOCK_M=shape[0], BLOCK_N=shape[1])\n-#     pgm_contiguous = kernel[(1, 1)](x_tri, x_tri.stride(1), x_tri.stride(0),\n-#                                     z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n-#                                     BLOCK_M=shape[0], BLOCK_N=shape[1])\n-#     # numpy result\n-#     z_ref = x.transpose(*perm)\n-#     # compare\n-#     triton.testing.assert_almost_equal(z_tri, z_ref)\n-#     triton.testing.assert_almost_equal(z_tri_contiguous, z_ref)\n-#     # parse ptx to make sure ld/st are vectorized\n-#     ptx = pgm.asm['ptx']\n-#     assert 'ld.global.v4' in ptx\n-#     assert 'st.global.v4' in ptx\n-#     ptx = pgm_contiguous.asm['ptx']\n-#     assert 'ld.global.v4' in ptx\n-#     assert 'st.global.v4' in ptx\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, stride_xm, stride_xn,\n+               Z, stride_zm, stride_zn,\n+               BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n+        off_m = tl.arange(0, BLOCK_M)\n+        off_n = tl.arange(0, BLOCK_N)\n+        Xs = X + off_m[:, None] * stride_xm + off_n[None, :] * stride_xn\n+        Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n+        tl.store(Zs, tl.load(Xs))\n+    # input\n+    x = numpy_random(shape, dtype_str=dtype_str)\n+    # triton result\n+    z_tri = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n+    z_tri_contiguous = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n+    x_tri = to_triton(x, device=device, dst_type=dtype_str)\n+    pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n+                         z_tri, z_tri.stride(1), z_tri.stride(0),\n+                         BLOCK_M=shape[0], BLOCK_N=shape[1])\n+    pgm_contiguous = kernel[(1, 1)](x_tri, x_tri.stride(1), x_tri.stride(0),\n+                                    z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n+                                    BLOCK_M=shape[0], BLOCK_N=shape[1])\n+    # numpy result\n+    z_ref = x.transpose(*perm)\n+    # compare\n+    triton.testing.assert_almost_equal(z_tri, z_ref)\n+    triton.testing.assert_almost_equal(z_tri_contiguous, z_ref)\n+    # parse ptx to make sure ld/st are vectorized\n+    ptx = pgm.asm['ptx']\n+    assert 'ld.global.v4' in ptx\n+    assert 'st.global.v4' in ptx\n+    ptx = pgm_contiguous.asm['ptx']\n+    assert 'ld.global.v4' in ptx\n+    assert 'st.global.v4' in ptx\n \n # # ---------------\n # # test dot"}, {"filename": "python/tests/test_elementwise.py", "status": "added", "additions": 189, "deletions": 0, "changes": 189, "file_content_changes": "@@ -0,0 +1,189 @@\n+import tempfile\n+from inspect import Parameter, Signature\n+\n+import _testcapi\n+import pytest\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+torch_type = {\n+    \"bool\": torch.bool,\n+    \"int32\": torch.int32,\n+    \"float32\": torch.float32,\n+    \"float64\": torch.float64\n+}\n+\n+torch_ops = {\n+    \"log\": \"log\",\n+    \"cos\": \"cos\",\n+    \"sin\": \"sin\",\n+    \"sqrt\": \"sqrt\",\n+    \"abs\": \"abs\",\n+    \"exp\": \"exp\",\n+    \"sigmoid\": \"sigmoid\",\n+    \"umulhi\": None,\n+    \"cdiv\": None,\n+    \"fdiv\": \"div\",\n+    \"minimum\": \"minimum\",\n+    \"maximum\": \"maximum\",\n+    \"where\": \"where\",\n+}\n+\n+libdevice = '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'\n+\n+\n+def get_tensor(shape, data_type, b_positive=False):\n+    x = None\n+    if data_type.startswith('int'):\n+        x = torch.randint(2**31 - 1, shape, dtype=torch_type[data_type], device='cuda')\n+    elif data_type.startswith('bool'):\n+        x = torch.randint(1, shape, dtype=torch_type[data_type], device='cuda')\n+    else:\n+        x = torch.randn(shape, dtype=torch_type[data_type], device='cuda')\n+\n+    if b_positive:\n+        x = torch.abs(x)\n+\n+    return x\n+\n+\n+@pytest.mark.parametrize('expr, output_type, input0_type',\n+                         [('log', 'float32', 'float32'),\n+                          ('log', 'float64', 'float64'),\n+                             ('cos', 'float32', 'float32'),\n+                             ('cos', 'float64', 'float64'),\n+                             ('sin', 'float32', 'float32'),\n+                             ('sin', 'float64', 'float64'),\n+                             ('sqrt', 'float32', 'float32'),\n+                             ('sqrt', 'float64', 'float64'),\n+                             ('abs', 'float32', 'float32'),\n+                             ('exp', 'float32', 'float32'),\n+                             ('sigmoid', 'float32', 'float32'),\n+                          ])\n+def test_single_input(expr, output_type, input0_type):\n+    src = f\"\"\"\n+def kernel(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = tl.{expr}(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\"\"\"\n+    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n+    fp.write(src)\n+    fp.flush()\n+\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        pass\n+    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n+    parameters = []\n+    parameters.append(Parameter(\"X\", 1))\n+    parameters.append(Parameter(\"Y\", 1))\n+    parameters.append(Parameter(\"BLOCK\", 1))\n+    kernel.__signature__ = Signature(parameters=parameters)\n+    kernel = triton.jit(kernel)\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x = get_tensor(shape, input0_type, expr == 'log' or expr == 'sqrt')\n+    # triton result\n+    y = torch.zeros(shape, dtype=torch_type[output_type], device=\"cuda\")\n+    kernel[(1,)](x, y, BLOCK=shape[0], extern_libs={\"libdevice\": libdevice})\n+    # reference result\n+    y_ref = getattr(torch, torch_ops[expr])(x)\n+    # compare\n+    assert_close(y, y_ref)\n+\n+\n+@pytest.mark.parametrize('expr, output_type, input0_type, input1_type',\n+                         [('umulhi', 'int32', 'int32', 'int32'),\n+                          ('cdiv', 'int32', 'int32', 'int32'),\n+                             ('fdiv', 'float32', 'float32', 'float32'),\n+                             ('minimum', 'float32', 'float32', 'float32'),\n+                             ('maximum', 'float32', 'float32', 'float32'),\n+                          ])\n+def test_two_input(expr, output_type, input0_type, input1_type):\n+    src = f\"\"\"\n+def kernel(X0, X1, Y, BLOCK: tl.constexpr):\n+    x0 = tl.load(X0 + tl.arange(0, BLOCK))\n+    x1 = tl.load(X1 + tl.arange(0, BLOCK))\n+    y = tl.{expr}(x0, x1)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\"\"\"\n+    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n+    fp.write(src)\n+    fp.flush()\n+\n+    def kernel(X0, X1, Y, BLOCK: tl.constexpr):\n+        pass\n+    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n+    parameters = []\n+    parameters.append(Parameter(\"X0\", 1))\n+    parameters.append(Parameter(\"X1\", 1))\n+    parameters.append(Parameter(\"Y\", 1))\n+    parameters.append(Parameter(\"BLOCK\", 1))\n+    kernel.__signature__ = Signature(parameters=parameters)\n+    kernel = triton.jit(kernel)\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x0 = get_tensor(shape, input0_type)\n+    x1 = get_tensor(shape, input1_type)\n+\n+    # triton result\n+    y = torch.zeros(shape, dtype=torch_type[output_type], device=\"cuda\")\n+    kernel[(1,)](x0, x1, y, BLOCK=shape[0], extern_libs={\"libdevice\": libdevice})\n+    # reference result\n+\n+    if expr == \"cdiv\":\n+        y_ref = (x0 + x1 - 1) // x1\n+    elif expr == \"umulhi\":\n+        y_ref = ((x0.to(torch.int64) * x1) >> 32).to(torch.int32)\n+    else:\n+        y_ref = getattr(torch, torch_ops[expr])(x0, x1)\n+    # compare\n+    assert_close(y, y_ref)\n+\n+\n+@pytest.mark.parametrize('expr, output_type, input0_type, input1_type, input2_type',\n+                         [('where', \"int32\", \"bool\", \"int32\", \"int32\"), ])\n+def test_three_input(expr, output_type, input0_type, input1_type, input2_type):\n+    src = f\"\"\"\n+def kernel(X0, X1, X2, Y, BLOCK: tl.constexpr):\n+    x0 = tl.load(X0 + tl.arange(0, BLOCK))\n+    x1 = tl.load(X1 + tl.arange(0, BLOCK))\n+    x2 = tl.load(X2 + tl.arange(0, BLOCK))\n+    y = tl.{expr}(x0, x1, x2)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\"\"\"\n+    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n+    fp.write(src)\n+    fp.flush()\n+\n+    def kernel(X0, X1, X2, Y, BLOCK: tl.constexpr):\n+        pass\n+    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n+    parameters = []\n+    parameters.append(Parameter(\"X0\", 1))\n+    parameters.append(Parameter(\"X1\", 1))\n+    parameters.append(Parameter(\"X2\", 1))\n+    parameters.append(Parameter(\"Y\", 1))\n+    parameters.append(Parameter(\"BLOCK\", 1))\n+    kernel.__signature__ = Signature(parameters=parameters)\n+    kernel = triton.jit(kernel)\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x0 = get_tensor(shape, input0_type)\n+    x1 = get_tensor(shape, input1_type)\n+    x2 = get_tensor(shape, input1_type)\n+\n+    # triton result\n+    y = torch.zeros(shape, dtype=torch_type[output_type], device=\"cuda\")\n+    kernel[(1,)](x0, x1, x2, y, BLOCK=shape[0], extern_libs={\"libdevice\": libdevice})\n+    # reference result\n+\n+    y_ref = getattr(torch, torch_ops[expr])(x0, x1, x2)\n+    # compare\n+    assert_close(y, y_ref)"}, {"filename": "python/tests/test_ext_elemwise.py", "status": "added", "additions": 178, "deletions": 0, "changes": 178, "file_content_changes": "@@ -0,0 +1,178 @@\n+\n+import pytest\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n+    [4, 256, 1],\n+    [4, 1024, 256],\n+])\n+def test_sin_no_mask(num_warps, block_size, iter_size):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               block_size,\n+               iter_size: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+        for i in range(0, block_size, iter_size):\n+            offset = pid * block_size + tl.arange(0, iter_size)\n+            x_ptrs = x_ptr + offset\n+            x = tl.load(x_ptrs)\n+            y = tl.libdevice.sin(x)\n+            y_ptrs = y_ptr + offset\n+            tl.store(y_ptrs, y)\n+\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+\n+    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    y = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // (block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n+\n+    golden_y = torch.sin(x)\n+    assert_close(y, golden_y, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n+    [4, 256, 1],\n+    [4, 1024, 256],\n+])\n+def test_fmin_no_mask(num_warps, block_size, iter_size):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               block_size,\n+               iter_size: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+        for i in range(0, block_size, iter_size):\n+            offset = pid * block_size + tl.arange(0, iter_size)\n+            x_ptrs = x_ptr + offset\n+            y_ptrs = y_ptr + offset\n+\n+            x = tl.load(x_ptrs)\n+            y = tl.load(y_ptrs)\n+            z = tl.libdevice.min(x, y)\n+            z_ptrs = z_ptr + offset\n+            tl.store(z_ptrs, z)\n+\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+            z_ptr += iter_size\n+\n+    x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n+    z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // (block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n+\n+    golden_z = torch.minimum(x, y)\n+    assert_close(z, golden_z, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize('num_warps, block_size, iter_size', [\n+    [4, 256, 1],\n+    [4, 1024, 256],\n+])\n+def test_fmad_rn_no_mask(num_warps, block_size, iter_size):\n+    @triton.jit\n+    def kernel(x_ptr,\n+               y_ptr,\n+               z_ptr,\n+               w_ptr,\n+               block_size,\n+               iter_size: tl.constexpr):\n+        pid = tl.program_id(axis=0)\n+        for i in range(0, block_size, iter_size):\n+            offset = pid * block_size + tl.arange(0, iter_size)\n+            x_ptrs = x_ptr + offset\n+            y_ptrs = y_ptr + offset\n+            z_ptrs = z_ptr + offset\n+\n+            x = tl.load(x_ptrs)\n+            y = tl.load(y_ptrs)\n+            z = tl.load(z_ptrs)\n+\n+            w = tl.libdevice.fma_rn(x, y, z)\n+            w_ptrs = w_ptr + offset\n+            tl.store(w_ptrs, w)\n+\n+            x_ptr += iter_size\n+            y_ptr += iter_size\n+            z_ptr += iter_size\n+            w_ptr += iter_size\n+\n+    x = torch.randn((block_size,), device='cuda', dtype=torch.float64)\n+    y = torch.randn((block_size,), device='cuda', dtype=torch.float64)\n+    z = torch.randn((block_size,), device='cuda', dtype=torch.float64)\n+    w = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n+\n+    grid = lambda EA: (x.shape.numel() // (block_size),)\n+    kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, w_ptr=w,\n+                 block_size=x.shape[0], iter_size=iter_size, num_warps=num_warps)\n+\n+    golden_w = x * y + z\n+    assert_close(w, golden_w, rtol=1e-7, atol=1e-7)\n+\n+\n+@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+                         [('int32', 'libdevice.ffs', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n+                          ('int32', 'libdevice.ffs', '')])\n+def test_libdevice(dtype_str, expr, lib_path):\n+    src = f\"\"\"\n+def kernel(X, Y, BLOCK: tl.constexpr):\n+    x = tl.load(X + tl.arange(0, BLOCK))\n+    y = tl.{expr}(x)\n+    tl.store(Y + tl.arange(0, BLOCK), y)\n+\"\"\"\n+    import tempfile\n+    from inspect import Parameter, Signature\n+\n+    import _testcapi\n+\n+    fp = tempfile.NamedTemporaryFile(mode='w', suffix=\".py\")\n+    fp.write(src)\n+    fp.flush()\n+\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        pass\n+    kernel.__code__ = _testcapi.code_newempty(fp.name, \"kernel\", 1)\n+    parameters = []\n+    parameters.append(Parameter(\"X\", 1))\n+    parameters.append(Parameter(\"Y\", 1))\n+    parameters.append(Parameter(\"BLOCK\", 1))\n+    kernel.__signature__ = Signature(parameters=parameters)\n+    kernel = triton.jit(kernel)\n+\n+    torch_type = {\n+        \"int32\": torch.int32,\n+        \"float32\": torch.float32,\n+        \"float64\": torch.float64\n+    }\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x = None\n+    if dtype_str == \"int32\":\n+        x = torch.randint(2**31 - 1, shape, dtype=torch_type[dtype_str], device=\"cuda\")\n+    else:\n+        x = torch.randn(shape, dtype=torch_type[dtype_str], device=\"cuda\")\n+    if expr == 'libdevice.ffs':\n+        y_ref = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+        for i in range(shape[0]):\n+            y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n+\n+    # triton result\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    kernel[(1,)](x, y, BLOCK=shape[0], extern_libs={\"libdevice\": lib_path})\n+    # compare\n+    assert_close(y, y_ref)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "file_content_changes": "@@ -36,6 +36,7 @@ def str_to_ty(name):\n         \"bf16\": triton.language.bfloat16,\n         \"fp32\": triton.language.float32,\n         \"fp64\": triton.language.float64,\n+        \"i1\": triton.language.int1,\n         \"i8\": triton.language.int8,\n         \"i16\": triton.language.int16,\n         \"i32\": triton.language.int32,\n@@ -45,7 +46,6 @@ def str_to_ty(name):\n         \"u32\": triton.language.uint32,\n         \"u64\": triton.language.uint64,\n         \"B\": triton.language.int1,\n-        \"i1\": triton.language.int1,\n     }\n     return tys[name]\n \n@@ -875,7 +875,7 @@ def optimize_tritongpu_ir(mod, num_stages):\n     pm.enable_debug()\n     # Get error in backend due to wrong conversion in expanding async-related instruction.\n     # TODO[Superjomn]: Open it when fixed.\n-    # pm.add_tritongpu_pipeline_pass(num_stages)\n+    pm.add_tritongpu_pipeline_pass(num_stages)\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n     pm.add_coalesce_pass()\n@@ -888,6 +888,13 @@ def optimize_tritongpu_ir(mod, num_stages):\n     return mod\n \n \n+def add_external_libs(mod, libs):\n+    for name, path in libs.items():\n+        if len(name) == 0 or len(path) == 0:\n+            return\n+    _triton.add_external_libs(mod, list(libs.keys()), list(libs.values()))\n+\n+\n def make_llvm_ir(mod):\n     return _triton.translate_triton_gpu_to_llvmir(mod)\n \n@@ -986,6 +993,8 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     module = optimize_tritongpu_ir(module, num_stages)\n     if output == \"ttgir\":\n         return module.str()\n+    if extern_libs:\n+        add_external_libs(module, extern_libs)\n \n     # llvm-ir\n     llvm_ir = make_llvm_ir(module)"}, {"filename": "python/triton/language/libdevice.10.bc", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -226,7 +226,6 @@ def fdiv(input: tl.tensor,\n         raise ValueError(\"both operands of fdiv must have floating poscalar type\")\n     input, other = binary_op_type_checking_impl(input, other, builder, False, False, False, True)\n     ret = builder.create_fdiv(input.handle, other.handle)\n-    ret.set_fdiv_ieee_rounding(ieee_rounding)\n     return tl.tensor(ret, input.type)\n \n \n@@ -1074,7 +1073,8 @@ def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n \n def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     x, y = binary_op_type_checking_impl(x, y, builder)\n-    return tl.tensor(builder.create_umulhi(x.handle, y.handle), x.type)\n+    from . import libdevice\n+    return libdevice.mulhi(x, y, _builder=builder)\n \n \n def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "file_content_changes": "@@ -153,7 +153,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n \n     // Store 4 elements to global with single one vectorized store instruction\n-    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n     return\n   }\n@@ -222,8 +222,8 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n \n     // Store 8 elements to global with two vectorized store instruction\n-    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n-    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n     return\n   }\n@@ -326,7 +326,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #shared0 = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem\n+  // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_alloc_tensor\n   func @basic_alloc_tensor() {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -343,7 +343,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #shared0 = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem\n+  // CHECK: llvm.mlir.global external @global_smem\n   // CHECK-LABEL: basic_extract_slice\n   func @basic_extract_slice() {\n     // CHECK: %[[BASE0:.*]] = llvm.mlir.addressof @global_smem\n@@ -382,10 +382,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #slice2d1 = #triton_gpu.slice<{dim = 1, parent=#block2}>\n #slice3d0 = #triton_gpu.slice<{dim = 0, parent=#block3}>\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#A = #triton_gpu.shared<{vec = 4, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_insert_slice_async_v4\n-  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}) {\n+  func @basic_insert_slice_async_v4(%arg0: !tt.ptr<f32> {tt.divisibility = 8 : i32}) {\n     %off0_ = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #slice2d1>\n     %off1_ = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<64xi32, #slice3d0>\n     %off0 = tt.expand_dims %off0_ {axis = 1 : i32} : (tensor<16xi32, #slice2d1>) -> tensor<16x1xi32, #block2>\n@@ -404,9 +404,9 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n-    // CHECK-SAME: cp.async.cg.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x80, 0x80\n+    // CHECK-SAME: cp.async.cg.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x10, 0x10\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n-    // CHECK-SAME: cp.async.cg.shared.global.L2::evict_normal [ ${{.*}} + 8 ], [ ${{.*}} + 0 ], 0x80, 0x80\n+    // CHECK-SAME: cp.async.cg.shared.global [ ${{.*}} + 16 ], [ ${{.*}} + 0 ], 0x10, 0x10\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x64x!tt.ptr<f32>, #AL> -> tensor<2x16x64xf32, #A>\n@@ -445,13 +445,13 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x32x!tt.ptr<f32>, #AL> -> tensor<2x16x32xf32, #A>\n@@ -489,21 +489,21 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %index = arith.constant 1 : i32\n \n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 0 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: cp.async.ca.shared.global.L2::evict_normal [ ${{.*}} + 512 ], [ ${{.*}} + 0 ], 0x20, 0x20\n+    // CHECK-SAME: cp.async.ca.shared.global [ ${{.*}} + 2048 ], [ ${{.*}} + 0 ], 0x4, 0x4\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: cp.async.commit_group\n     %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32x!tt.ptr<f32>, #AL> -> tensor<2x32x32xf32, #A>\n@@ -545,7 +545,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [0, 1]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<1088 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked\n   func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -593,7 +593,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [16, 2], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<1280 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_vec\n   func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -617,7 +617,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<640 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_blocked_multi_rep\n   func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n@@ -708,7 +708,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n #mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<2560 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mma_block\n   func @convert_layout_mma_blocked(%arg0: tensor<32x16xf32, #mma>) {\n     // CHECK: nvvm.barrier0\n@@ -729,7 +729,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<16384 x i8>\n+  // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_blocked_shared\n   func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: llvm.store"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -22,7 +22,7 @@\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n // CHECK: %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]], %[[CONSTANT_0]]\n // CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n-// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n@@ -78,7 +78,7 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n // CHECK:   %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]], %[[CONSTANT_0]]\n // CHECK:   %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n-// CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n+// CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:     tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n@@ -131,7 +131,7 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n // CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK: triton_gpu.async_wait {num = 1 : i32}\n // CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n-// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n // CHECK:   tt.dot {{.*}}, %[[arg_b0]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]"}]