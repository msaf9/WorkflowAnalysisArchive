[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -77,6 +77,9 @@ SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   return result;\n }\n \n+bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n+                        triton::gpu::DotOperandEncodingAttr &dotOperandLayout);\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -58,6 +58,13 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   auto dstTy = op.result().getType().cast<RankedTensorType>();\n   Attribute srcLayout = srcTy.getEncoding();\n   Attribute dstLayout = dstTy.getEncoding();\n+\n+  // MmaToDotShortcut doesn't use shared mem\n+  if (auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>())\n+    if (auto dotOperandLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>())\n+      if (isMmaToDotShortcut(mmaLayout, dotOperandLayout))\n+        return {};\n+\n   assert(srcLayout && dstLayout &&\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n   auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -48,6 +48,12 @@ SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n   auto axis = op.axis();\n   SmallVector<SmallVector<unsigned>> smemShapes(3);\n \n+  auto argLayout = srcTy.getEncoding();\n+  auto argLayoutMma = argLayout.dyn_cast<triton::gpu::MmaEncodingAttr>();\n+  if (argLayoutMma && argLayoutMma.getVersionMajor() == 2 &&\n+      triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n+    return {{1, 1}, {1, 1}};\n+\n   /// shared memory block0\n   smemShapes[0] = convertType<unsigned>(getSrcShape());\n   smemShapes[0][axis] = getInterWarpSize();\n@@ -148,4 +154,14 @@ std::string getValueOperandName(Value value, AsmState &state) {\n   return opName;\n }\n \n+bool isMmaToDotShortcut(triton::gpu::MmaEncodingAttr &mmaLayout,\n+                        triton::gpu::DotOperandEncodingAttr &dotOperandLayout) {\n+  // dot_op<opIdx=0, parent=#mma> = #mma\n+  // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n+  return mmaLayout.getVersionMajor() == 2 &&\n+         mmaLayout.getWarpsPerCTA()[1] == 1 &&\n+         dotOperandLayout.getOpIdx() == 0 &&\n+         dotOperandLayout.getParent() == mmaLayout;\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 9, "changes": 10, "file_content_changes": "@@ -1,5 +1,6 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"DotOpHelpers.h\"\n+#include \"Utility.h\"\n \n using ::mlir::LLVM::DotOpFMAConversionHelper;\n using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n@@ -17,15 +18,6 @@ using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n-bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n-                        DotOperandEncodingAttr &dotOperandLayout) {\n-  // dot_op<opIdx=0, parent=#mma> = #mma\n-  // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n-  return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n-         dotOperandLayout.getOpIdx() == 0 &&\n-         dotOperandLayout.getParent() == mmaLayout;\n-}\n-\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.h", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -8,9 +8,6 @@ using namespace mlir::triton;\n \n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n \n-bool isMmaToDotShortcut(MmaEncodingAttr &mmaLayout,\n-                        DotOperandEncodingAttr &dotOperandLayout);\n-\n void populateConvertLayoutOpToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 0, "deletions": 10, "changes": 10, "file_content_changes": "@@ -345,16 +345,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return ret;\n   }\n \n-  bool isMmaToDotShortcut(\n-      MmaEncodingAttr &mmaLayout,\n-      triton::gpu::DotOperandEncodingAttr &dotOperandLayout) const {\n-    // dot_op<opIdx=0, parent=#mma> = #mma\n-    // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>\n-    return mmaLayout.getWarpsPerCTA()[1] == 1 &&\n-           dotOperandLayout.getOpIdx() == 0 &&\n-           dotOperandLayout.getParent() == mmaLayout;\n-  }\n-\n   void storeDistributedToShared(Value src, Value llSrc,\n                                 ArrayRef<Value> dstStrides,\n                                 ArrayRef<SmallVector<Value>> srcIndices,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -88,7 +88,7 @@ class TritonGPUReorderInstructionsPass\n       if (!dstEncoding)\n         return;\n       int opIdx = dstEncoding.getOpIdx();\n-      if (opIdx != 1)\n+      if (opIdx != 0)\n         return;\n       if (op->getUsers().empty())\n         return;"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 32, "deletions": 34, "changes": 66, "file_content_changes": "@@ -15,7 +15,7 @@\n @triton.jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,\n-    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to work around a compiler bug\n+    L, M,\n     Out,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n@@ -32,58 +32,55 @@ def _fwd_kernel(\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n     off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n+    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n     off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n     k_ptrs = K + off_k\n     v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n     # load q: it will stay in SRAM throughout\n     q = tl.load(q_ptrs)\n     # loop over k, v and update accumulator\n     for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n-        # start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(k_ptrs + start_n * stride_kn)\n+        k = tl.load(k_ptrs)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, tl.trans(k))\n+        qk += tl.dot(q, k)\n         qk *= sm_scale\n-        qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n-        # -- compute m_ij, p, l_ij\n-        m_ij = tl.max(qk, 1)\n-        p = tl.exp(qk - m_ij[:, None])\n-        l_ij = tl.sum(p, 1)\n-        # -- update m_i and l_i\n-        m_i_new = tl.maximum(m_i, m_ij)\n-        alpha = tl.exp(m_i - m_i_new)\n-        beta = tl.exp(m_ij - m_i_new)\n-        l_i_new = alpha * l_i + beta * l_ij\n-        # -- update output accumulator --\n-        # scale p\n-        p_scale = beta / l_i_new\n-        p = p * p_scale[:, None]\n-        # scale acc\n-        acc_scale = l_i / l_i_new * alpha\n-        acc = acc * acc_scale[:, None]\n+        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        # compute new m\n+        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n+        # correct old l\n+        l_prev *= tl.exp(m_prev - m_curr)\n+        # attention weights\n+        p = tl.exp(qk - m_curr[:, None])\n+        l_curr = tl.sum(p, 1) + l_prev\n+        # rescale operands of matmuls\n+        l_rcp = 1. / l_curr\n+        p *= l_rcp\n+        acc *= (l_prev * l_rcp)[:, None]\n         # update acc\n-        v = tl.load(v_ptrs + start_n * stride_vk)\n         p = p.to(tl.float16)\n+        v = tl.load(v_ptrs)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n-        l_i = l_i_new\n-        m_i = m_i_new\n+        l_prev = l_curr\n+        m_prev = m_curr\n+        # update pointers\n+        k_ptrs += BLOCK_N * stride_kn\n+        v_ptrs += BLOCK_N * stride_vk\n     # rematerialize offsets to save registers\n     start_m = tl.program_id(0)\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, l_i)\n-    tl.store(m_ptrs, m_i)\n+    tl.store(l_ptrs, l_prev)\n+    tl.store(m_ptrs, m_prev)\n     # initialize pointers to output\n     offs_n = tl.arange(0, BLOCK_DMODEL)\n     off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n@@ -209,14 +206,13 @@ def forward(ctx, q, k, v, sm_scale):\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n         grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n-        tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         num_warps = 4 if Lk <= 64 else 8\n \n         _fwd_kernel[grid](\n             q, k, v, sm_scale,\n-            tmp, L, m,\n+            L, m,\n             o,\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n@@ -316,15 +312,15 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n # vary seq length for fixed head and batch=4\n configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n-    x_vals=[2**i for i in range(10, 15)],\n+    x_vals=[2**i for i in range(10, 14)],\n     line_arg='provider',\n     line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n     line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n     args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n-) for mode in ['fwd']]\n+) for mode in ['fwd', 'bwd']]\n \n \n @triton.testing.perf_report(configs)\n@@ -357,4 +353,6 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n         return ms\n \n-# bench_flash_attention.run(save_path='.', print_data=True)\n+\n+# only works on post-Ampere GPUs right now\n+bench_flash_attention.run(save_path='.', print_data=True)"}, {"filename": "test/lib/Analysis/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -6,4 +6,5 @@ add_mlir_library(TritonTestAnalysis\n \n   LINK_LIBS PUBLIC\n   TritonAnalysis\n+  ${dialect_libs}\n )\n\\ No newline at end of file"}]