[{"filename": "python/test/unit/runtime/test_launch.py", "status": "modified", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -1,5 +1,12 @@\n+from typing import Tuple\n+\n import gc\n import tracemalloc\n+import time\n+import os\n+import sys\n+import importlib\n+import textwrap\n \n import torch\n \n@@ -33,3 +40,65 @@ def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n         assert end - begin < 1000\n     finally:\n         tracemalloc.stop()\n+\n+def test_kernel_launch_latency() -> None:\n+    def define_empty_kernel(file_path, num_tensor_args):\n+        arg_str = \",\".join([f\"arg{i}: torch.Tensor\" for i in range(num_tensor_args)])\n+        arg_str += \", n_elements: int, BLOCK_SIZE: tl.constexpr\"\n+        func_str = f\"\"\"\n+        import torch\n+\n+        import triton\n+        import triton.language as tl\n+\n+        @triton.jit\n+        def empty_kernel({arg_str}):\n+            pass\n+        \"\"\"\n+        with open(file_path, \"w\") as f:\n+            f.write(textwrap.dedent(func_str))\n+\n+    def import_empty_kernel(file_path):\n+        directory, filename = os.path.split(file_path)\n+        module_name, _ = os.path.splitext(filename)\n+        sys.path.insert(0, directory)\n+\n+        module = importlib.import_module(module_name)\n+        empty_kernel = module.empty_kernel\n+        return empty_kernel\n+\n+    def empty(*kernel_args: Tuple[torch.Tensor]):\n+        first_arg = kernel_args[0]\n+        n_elements = first_arg.numel()\n+        grid = (triton.cdiv(n_elements, 1024),)\n+        # Warmup\n+        empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=torch.cuda.current_device())\n+        torch.cuda.synchronize()\n+        # Measure launch overhead at steady state\n+        num_runs = 1000\n+        latency = []\n+        for i in range(num_runs):\n+            single_start_time = time.time()\n+            empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=torch.cuda.current_device())\n+            single_end_time = time.time()\n+            latency.append((single_end_time - single_start_time) * 1e6)\n+\n+        latency.sort()\n+        mid = len(latency) // 2\n+        median = (latency[mid] + latency[~mid]) / 2\n+        # print(f\"Median latency (usec) = {median}\")\n+        assert median < 35, \"Kernel launch time has increased!\"\n+\n+    # Define a jitted empty_kernel in /tmp and import\n+    file_path = '/tmp/empty_kernel.py'\n+    num_tensor_args = 40\n+    define_empty_kernel(file_path, num_tensor_args)\n+    empty_kernel = import_empty_kernel(file_path)\n+\n+    # Initialize random tensors for the empty_kernel\n+    torch.manual_seed(0)\n+    size = 1024\n+    kernel_args = (torch.rand(size, device='cuda') for i in range(num_tensor_args))\n+    \n+    # Run empty, which would run empty_kernel internally\n+    empty(*kernel_args)\n\\ No newline at end of file"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 14, "deletions": 4, "changes": 18, "file_content_changes": "@@ -247,14 +247,23 @@ def _make_launcher(self):\n         for i, arg in enumerate(regular_args):\n             if i in self.do_not_specialize:\n                 continue\n-            specializations += [f'({arg}.data_ptr() % {JITFunction.divisibility} == 0) if hasattr({arg}, \"data_ptr\") '\n+            arg_annotation = self.__annotations__.get(arg, None)\n+            if not arg_annotation:\n+                specializations += [f'({arg}.data_ptr() % {JITFunction.divisibility} == 0) if hasattr({arg}, \"data_ptr\") '\n                                 f'else ({arg} % {JITFunction.divisibility} == 0, {arg} == 1) if isinstance({arg}, int) '\n                                 f'else (False,)']\n+            elif arg_annotation == 'torch.Tensor':\n+                specializations += [f'({arg}.data_ptr() % {JITFunction.divisibility} == 0)']\n+            elif arg_annotation == 'int':\n+                specializations += [f'({arg} % {JITFunction.divisibility} == 0, {arg} == 1)']\n+            else:\n+                specializations += ['(False,)']\n+\n         spec_keys = ', '.join(specializations)\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n \n         src = f\"\"\"\n-def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False):\n+def {self.fn.__name__}({', '.join(self.arg_names)}, grid, device=None, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False):\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n@@ -268,8 +277,9 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     grid_0 = grid[0]\n     grid_1 = grid[1] if grid_size > 1 else 1\n     grid_2 = grid[2] if grid_size > 2 else 1\n-    device = get_current_device()\n-    set_current_device(device)\n+    if device is None:\n+        device = get_current_device()\n+        set_current_device(device)\n     if stream is None and not warmup:\n       stream = get_cuda_stream(device)\n     try:"}]