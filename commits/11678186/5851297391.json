[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -91,9 +91,11 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '1' && env.ENABLE_MMA_V3 == '1'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper/test_flashattention.py\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n+          #run hopper/test_flashattention.py to avoid out of gpu memory\n+          python3 -m pytest hopper/test_flashattention.py\n \n       - name: Run python tests on CUDA with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/FenceInsertion.cpp", "status": "modified", "additions": 23, "deletions": 10, "changes": 33, "file_content_changes": "@@ -3,6 +3,7 @@\n #include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n #include \"llvm/Support/Debug.h\"\n \n //===----------------------------------------------------------------------===//\n@@ -32,16 +33,20 @@ struct FenceInsertionPass\n   FenceInsertionPass(int computeCapability) {\n     this->computeCapability = computeCapability;\n   }\n-  // TODO: support more patterns to insert fences\n-  // only support insertion between convert layout ops and dot ops to protect\n-  // flashattention\n+  // TODO: support more general patterns to insert fences. eg. any op(generic)\n+  // to shared in use-def chain which refers by async proxy. We have generic(\n+  // convertlayout with sts/stmatix) + fence + async(wgmma/tma store) up to now\n   void runOnOperation() override {\n     // Only insert fences for compute capability 9.0\n     if (computeCapability < 90)\n       return;\n+    // ENABLE_MMA_V3\n+    if (!::triton::tools::getBoolEnv(\"ENABLE_MMA_V3\"))\n+      return;\n     ModuleOp mod = getOperation();\n     mod.walk([&](Operation *op) {\n       if (isa<tt::DotOp>(op)) {\n+        OpBuilder builder(op);\n         auto a = op->getOperand(0);\n         auto b = op->getOperand(1);\n         auto mmaEncoding = op->getResult(0)\n@@ -50,19 +55,27 @@ struct FenceInsertionPass\n                                .getEncoding()\n                                .dyn_cast<ttg::MmaEncodingAttr>();\n         auto isHopperEncoding = mmaEncoding && mmaEncoding.isHopper();\n-        if (isHopperEncoding && (isa<ttg::ConvertLayoutOp>(a.getDefiningOp()) &&\n-                                 ttg::isSharedEncoding(a)) ||\n-            (isa<ttg::ConvertLayoutOp>(b.getDefiningOp()) &&\n-             ttg::isSharedEncoding(b))) {\n-\n-          // TODO: check whether cluster fence is needed\n-          OpBuilder builder(op);\n+        if (isHopperEncoding && (canReachGeneric(a) || canReachGeneric(b))) {\n           builder.create<ttng::FenceAsyncSharedOp>(op->getLoc(),\n                                                    false /*bCluster*/);\n         }\n       }\n     });\n   }\n+\n+private:\n+  bool canReachGeneric(Value operand) {\n+    auto op = operand.getDefiningOp();\n+    if (!op)\n+      return false;\n+    if (isa<ttg::ConvertLayoutOp>(op) && ttg::isSharedEncoding(operand))\n+      return true;\n+    for (auto v : op->getOperands()) {\n+      if (canReachGeneric(v))\n+        return true;\n+    }\n+    return false;\n+  }\n };\n \n } // namespace"}, {"filename": "python/test/unit/hopper/test_flashattention.py", "status": "modified", "additions": 9, "deletions": 8, "changes": 17, "file_content_changes": "@@ -368,14 +368,15 @@ def backward(ctx, do):\n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 128, 64),\n-                                                 #  (4, 48, 256, 64),\n-                                                 #  (4, 48, 512, 64),\n-                                                 #  (4, 48, 1024, 64),\n-                                                 #  (4, 48, 2048, 64),\n-                                                 #  (4, 48, 4096, 64),\n-                                                 #  (4, 48, 8192, 64), out of memory\n-                                                 ])\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [\n+    (4, 48, 128, 64),\n+    (4, 48, 256, 64),\n+    (4, 48, 512, 64),\n+    (4, 48, 1024, 64),\n+    (4, 48, 2048, 64),\n+    (4, 48, 4096, 64),\n+    #  (4, 48, 8192, 64), out of memory\n+])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"requires arch 9+\")\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     torch.manual_seed(20)"}]