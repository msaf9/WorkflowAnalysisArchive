[{"filename": ".github/CODEOWNERS", "status": "added", "additions": 55, "deletions": 0, "changes": 55, "file_content_changes": "@@ -0,0 +1,55 @@\n+# These owners will be the default owners for everything in\n+# the repo. Unless a later match takes precedence,\n+# @global-owner1 and @global-owner2 will be requested for\n+# review when someone opens a pull request.\n+*       @ptillet\n+\n+# --------\n+# Analyses\n+# --------\n+# Alias analysis\n+include/triton/Analysis/Alias.h @Jokeren\n+lib/Analysis/Alias.cpp @Jokeren\n+# Allocation analysis\n+include/triton/Analysis/Allocation.h @Jokeren\n+lib/Analysis/Allocation.cpp @Jokeren\n+# Membar analysis\n+include/triton/Analysis/Membar.h @Jokeren\n+lib/Analysis/Membar.cpp @Jokeren\n+# AxisInfo analysis\n+include/triton/Analysis/AxisInfo.h @ptillet\n+lib/Analysis/AxisInfo.cpp @ptillet\n+# Utilities\n+include/triton/Analysis/Utility.h @Jokeren\n+lib/Analysis/Utility.cpp @Jokeren\n+\n+# ----------\n+# Dialects\n+# ----------\n+# Pipeline pass\n+lib/Dialect/TritonGPU/Transforms/Pipeline.cpp @daadaada\n+# Coalesce pass\n+lib/Dialect/TritonGPU/Transforms/Coalesce.cpp @ptillet\n+# Layout simplification pass\n+lib/Dialect/TritonGPU/Transforms/Combine.cpp @ptillet\n+\n+# -----------\n+# Conversions\n+# -----------\n+# TritonGPUToLLVM\n+include/triton/Conversion/TritonGPUToLLVM/ @goostavz @Superjomn\n+lib/Conversions/TritonGPUToLLVM @goostavz @Superjomn\n+# TritonToTritonGPU\n+include/triton/Conversion/TritonToTritonGPU/ @daadaada\n+lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp @daadaada\n+\n+\n+# -------\n+# Targets\n+# -------\n+# LLVMIR\n+include/triton/Target/LLVMIR/ @goostavz @Superjomn\n+lib/Target/LLVMIR @goostavz @Superjomn\n+# PTX\n+include/triton/Target/PTX/ @goostavz @Superjomn\n+lib/Target/PTX @goostavz @Superjomn"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -184,7 +184,6 @@ target_link_libraries(triton\n   TritonAnalysis\n   TritonTransforms\n   TritonGPUTransforms\n-  TritonDriver\n   TritonLLVMIR\n   TritonPTX\n   ${dialect_libs}"}, {"filename": "bin/CMakeLists.txt", "status": "modified", "additions": 32, "deletions": 32, "changes": 64, "file_content_changes": "@@ -26,35 +26,35 @@ target_link_libraries(triton-opt PRIVATE\n mlir_check_all_link_libraries(triton-opt)\n \n \n-add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n-llvm_update_compile_flags(triton-translate)\n-target_link_libraries(triton-translate PRIVATE\n-        TritonAnalysis\n-        TritonTransforms\n-        TritonGPUTransforms\n-        TritonLLVMIR\n-        TritonDriver\n-        ${dialect_libs}\n-        ${conversion_libs}\n-        # tests\n-        TritonTestAnalysis\n-\n-        LLVMCore\n-        LLVMSupport\n-        LLVMOption\n-        LLVMCodeGen\n-        LLVMAsmParser\n-\n-        # MLIR core\n-        MLIROptLib\n-        MLIRIR\n-        MLIRPass\n-        MLIRSupport\n-        MLIRTransforms\n-        MLIRExecutionEngine\n-        MLIRMathToLLVM\n-        MLIRTransformUtils\n-        MLIRLLVMToLLVMIRTranslation\n-        MLIRNVVMToLLVMIRTranslation\n-        )\n-mlir_check_all_link_libraries(triton-translate)\n+# add_llvm_executable(triton-translate triton-translate.cpp PARTIAL_SOURCES_INTENDED)\n+#llvm_update_compile_flags(triton-translate)\n+# target_link_libraries(triton-translate PRIVATE\n+#         TritonAnalysis\n+#         TritonTransforms\n+#         TritonGPUTransforms\n+#         TritonLLVMIR\n+#         TritonDriver\n+#         ${dialect_libs}\n+#         ${conversion_libs}\n+#         # tests\n+#         TritonTestAnalysis\n+\n+#         LLVMCore\n+#         LLVMSupport\n+#         LLVMOption\n+#         LLVMCodeGen\n+#         LLVMAsmParser\n+\n+#         # MLIR core\n+#         MLIROptLib\n+#         MLIRIR\n+#         MLIRPass\n+#         MLIRSupport\n+#         MLIRTransforms\n+#         MLIRExecutionEngine\n+#         MLIRMathToLLVM\n+#         MLIRTransformUtils\n+#         MLIRLLVMToLLVMIRTranslation\n+#         MLIRNVVMToLLVMIRTranslation\n+#         )\n+# mlir_check_all_link_libraries(triton-translate)"}, {"filename": "include/triton/Target/PTX/PTXTranslation.h", "status": "modified", "additions": 5, "deletions": 22, "changes": 27, "file_content_changes": "@@ -1,34 +1,17 @@\n #ifndef TRITON_TARGET_PTXTRANSLATION_H\n #define TRITON_TARGET_PTXTRANSLATION_H\n \n-#include \"triton/driver/dispatch.h\"\n-\n+#include <memory>\n #include <string>\n \n-namespace mlir {\n-\n-class ModuleOp;\n-\n-} // namespace mlir\n+namespace llvm {\n+class Module;\n+} // namespace llvm\n \n namespace triton {\n \n-template <CUdevice_attribute attr> int cuGetInfo(CUdevice device) {\n-  int res;\n-  driver::dispatch::cuDeviceGetAttribute(&res, attr, device);\n-  return res;\n-}\n-\n-void getCuCCAndVersionFromDevice(uint64_t device, int *cc, int *version,\n-                                 std::string *ptxasPath);\n-\n // Translate TritonGPU IR to PTX code.\n-std::tuple<std::string, // ptx code\n-           size_t,      // PTX cc\n-           int,         // PTX version\n-           std::string  // ptxas path\n-           >\n-translateTritonGPUToPTX(mlir::ModuleOp module, uint64_t device);\n+std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version);\n \n } // namespace triton\n "}, {"filename": "include/triton/driver/dispatch.h", "status": "removed", "additions": 0, "deletions": 376, "changes": 376, "file_content_changes": "@@ -1,376 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_DRIVER_DISPATCH_H_\n-#define _TRITON_DRIVER_DISPATCH_H_\n-\n-#include <dlfcn.h>\n-#include <type_traits>\n-\n-// CUDA Backend\n-#include \"triton/external/CUDA/cuda.h\"\n-#include \"triton/external/CUDA/nvml.h\"\n-\n-//// HIP backend\n-//#define __HIP_PLATFORM_AMD__\n-#include \"triton/external/hip.h\"\n-\n-// Exceptions\n-#include <iostream>\n-#include <stdexcept>\n-\n-namespace llvm {\n-class PassRegistry;\n-class Module;\n-} // namespace llvm\n-\n-namespace triton {\n-namespace driver {\n-\n-class cu_context;\n-\n-template <class T> void check(T) {}\n-void check(CUresult err);\n-void check(hipError_t err);\n-\n-class dispatch {\n-protected:\n-  template <class F> struct return_type;\n-\n-  template <class R, class... A> struct return_type<R (*)(A...)> {\n-    typedef R type;\n-  };\n-\n-  typedef bool (*f_init_t)();\n-\n-  template <f_init_t initializer, typename FunPtrT, typename... Args>\n-  static typename return_type<FunPtrT>::type\n-  f_impl(void *&lib_h, FunPtrT, void *&cache, const char *name, Args... args) {\n-    initializer();\n-    if (cache == nullptr) {\n-      cache = dlsym(lib_h, name);\n-      if (cache == 0) {\n-#ifdef __EXCEPTIONS\n-        throw std::runtime_error(\"dlsym unable to load function\");\n-#else\n-        std::cerr << \"Triton: dlsym unable to load function `\" << name << \"`\"\n-                  << std::endl;\n-        std::abort();\n-#endif\n-      }\n-    }\n-    FunPtrT fptr;\n-    *reinterpret_cast<void **>(&fptr) = cache;\n-    typename return_type<FunPtrT>::type res = (*fptr)(args...);\n-    check(res);\n-    return res;\n-  }\n-\n-public:\n-  static void release();\n-  // Nvidia\n-  static bool nvmlinit();\n-  static bool cuinit();\n-  // AMD\n-  static bool hipinit();\n-\n-  /* ------------------- *\n-   * CUDA\n-   * ------------------- */\n-  // context management\n-  static CUresult cuInit(unsigned int Flags);\n-  static CUresult cuCtxDestroy_v2(CUcontext ctx);\n-  static CUresult cuCtxCreate_v2(CUcontext *pctx, unsigned int flags,\n-                                 CUdevice dev);\n-  static CUresult cuCtxPushCurrent_v2(CUcontext ctx);\n-  static CUresult cuCtxPopCurrent_v2(CUcontext *pctx);\n-  static CUresult cuCtxGetDevice(CUdevice *result);\n-  static CUresult cuCtxEnablePeerAccess(CUcontext peerContext,\n-                                        unsigned int flags);\n-  static CUresult cuDriverGetVersion(int *driverVersion);\n-  // device management\n-  static CUresult cuDeviceGet(CUdevice *device, int ordinal);\n-  static CUresult cuDeviceGetName(char *name, int len, CUdevice dev);\n-  static CUresult cuDeviceGetPCIBusId(char *id, int len, CUdevice dev);\n-  static CUresult cuDeviceGetAttribute(int *pi, CUdevice_attribute attrib,\n-                                       CUdevice dev);\n-  static CUresult cuDeviceGetCount(int *count);\n-  // link management\n-  static CUresult cuLinkAddData_v2(CUlinkState state, CUjitInputType type,\n-                                   void *data, size_t size, const char *name,\n-                                   unsigned int numOptions,\n-                                   CUjit_option *options, void **optionValues);\n-  static CUresult cuLinkCreate_v2(unsigned int numOptions,\n-                                  CUjit_option *options, void **optionValues,\n-                                  CUlinkState *stateOut);\n-  static CUresult cuLinkComplete(CUlinkState state, void **cubinOut,\n-                                 size_t *sizeOut);\n-  static CUresult cuLinkDestroy(CUlinkState state);\n-  // module management\n-  static CUresult cuModuleGetGlobal_v2(CUdeviceptr *dptr, size_t *bytes,\n-                                       CUmodule hmod, const char *name);\n-  static CUresult cuModuleLoad(CUmodule *module, const char *fname);\n-  static CUresult cuModuleLoadData(CUmodule *module, const void *image);\n-  static CUresult cuModuleUnload(CUmodule hmod);\n-  static CUresult cuModuleLoadDataEx(CUmodule *module, const void *image,\n-                                     unsigned int numOptions,\n-                                     CUjit_option *options,\n-                                     void **optionValues);\n-  static CUresult cuModuleGetFunction(CUfunction *hfunc, CUmodule hmod,\n-                                      const char *name);\n-  // stream management\n-  static CUresult cuStreamCreate(CUstream *phStream, unsigned int Flags);\n-  static CUresult cuStreamSynchronize(CUstream hStream);\n-  static CUresult cuStreamGetCtx(CUstream hStream, CUcontext *pctx);\n-  static CUresult cuStreamDestroy_v2(CUstream hStream);\n-  static CUresult cuLaunchKernel(CUfunction f, unsigned int gridDimX,\n-                                 unsigned int gridDimY, unsigned int gridDimZ,\n-                                 unsigned int blockDimX, unsigned int blockDimY,\n-                                 unsigned int blockDimZ,\n-                                 unsigned int sharedMemBytes, CUstream hStream,\n-                                 void **kernelParams, void **extra);\n-  // function management\n-  static CUresult cuFuncGetAttribute(int *pi, CUfunction_attribute attrib,\n-                                     CUfunction hfunc);\n-  static CUresult cuFuncSetAttribute(CUfunction hfunc,\n-                                     CUfunction_attribute attrib, int value);\n-  static CUresult cuFuncSetCacheConfig(CUfunction hfunc, CUfunc_cache config);\n-  // memory management\n-  static CUresult cuMemAlloc_v2(CUdeviceptr *dptr, size_t bytesize);\n-  static CUresult cuPointerGetAttribute(void *data,\n-                                        CUpointer_attribute attribute,\n-                                        CUdeviceptr ptr);\n-  static CUresult cuMemsetD8Async(CUdeviceptr dst, unsigned char x, size_t N,\n-                                  CUstream stream);\n-  static CUresult cuMemcpyDtoH_v2(void *dstHost, CUdeviceptr srcDevice,\n-                                  size_t ByteCount);\n-  static CUresult cuMemFree_v2(CUdeviceptr dptr);\n-  static CUresult cuMemcpyDtoHAsync_v2(void *dstHost, CUdeviceptr srcDevice,\n-                                       size_t ByteCount, CUstream hStream);\n-  static CUresult cuMemcpyHtoDAsync_v2(CUdeviceptr dstDevice,\n-                                       const void *srcHost, size_t ByteCount,\n-                                       CUstream hStream);\n-  static CUresult cuMemcpyHtoD_v2(CUdeviceptr dstDevice, const void *srcHost,\n-                                  size_t ByteCount);\n-  // event management\n-  static CUresult cuEventCreate(CUevent *phEvent, unsigned int Flags);\n-  static CUresult cuEventElapsedTime(float *pMilliseconds, CUevent hStart,\n-                                     CUevent hEnd);\n-  static CUresult cuEventRecord(CUevent hEvent, CUstream hStream);\n-  static CUresult cuEventDestroy_v2(CUevent hEvent);\n-\n-  /* ------------------- *\n-   * NVML\n-   * ------------------- */\n-  static nvmlReturn_t nvmlDeviceGetHandleByPciBusId_v2(const char *pciBusId,\n-                                                       nvmlDevice_t *device);\n-  static nvmlReturn_t nvmlDeviceGetClockInfo(nvmlDevice_t device,\n-                                             nvmlClockType_t type,\n-                                             unsigned int *clock);\n-  static nvmlReturn_t nvmlDeviceGetMaxClockInfo(nvmlDevice_t device,\n-                                                nvmlClockType_t type,\n-                                                unsigned int *clock);\n-  static nvmlReturn_t nvmlDeviceSetApplicationsClocks(nvmlDevice_t device,\n-                                                      unsigned int mem_clock,\n-                                                      unsigned int sm_clock);\n-\n-  /* ------------------- *\n-   * HIP\n-   * ------------------- */\n-  // context management\n-  static hipError_t hipInit(unsigned int Flags);\n-  static hipError_t hipCtxDestroy(hipCtx_t ctx);\n-  static hipError_t hipCtxCreate(hipCtx_t *pctx, unsigned int flags,\n-                                 hipDevice_t dev);\n-  static hipError_t hipCtxPushCurrent(hipCtx_t ctx);\n-  static hipError_t hipCtxPopCurrent(hipCtx_t *pctx);\n-  static hipError_t hipCtxGetDevice(hipDevice_t *result);\n-  static hipError_t hipCtxEnablePeerAccess(hipCtx_t peerContext,\n-                                           unsigned int flags);\n-  static hipError_t hipDriverGetVersion(int *driverVersion);\n-  // device management\n-  static hipError_t hipGetDevice(hipDevice_t *device, int ordinal);\n-  static hipError_t hipDeviceGetName(char *name, int len, hipDevice_t dev);\n-  static hipError_t hipDeviceGetPCIBusId(char *id, int len, hipDevice_t dev);\n-  static hipError_t hipDeviceGetAttribute(int *pi, hipDeviceAttribute_t attrib,\n-                                          hipDevice_t dev);\n-  static hipError_t hipGetDeviceCount(int *count);\n-  // module management\n-  static hipError_t hipModuleGetGlobal(hipDeviceptr_t *dptr, size_t *bytes,\n-                                       hipModule_t hmod, const char *name);\n-  static hipError_t hipModuleLoad(hipModule_t *module, const char *fname);\n-  static hipError_t hipModuleLoadData(hipModule_t *module, const void *image);\n-  static hipError_t hipModuleUnload(hipModule_t hmod);\n-  static hipError_t hipModuleLoadDataEx(hipModule_t *module, const void *image,\n-                                        unsigned int numOptions,\n-                                        hipJitOption *options,\n-                                        void **optionValues);\n-  static hipError_t hipModuleGetFunction(hipFunction_t *hfunc, hipModule_t hmod,\n-                                         const char *name);\n-  // stream management\n-  static hipError_t hipStreamCreate(hipStream_t *phStream, unsigned int Flags);\n-  static hipError_t hipStreamSynchronize(hipStream_t hStream);\n-  static hipError_t hipStreamDestroy(hipStream_t hStream);\n-  static hipError_t\n-  hipModuleLaunchKernel(hipFunction_t f, unsigned int gridDimX,\n-                        unsigned int gridDimY, unsigned int gridDimZ,\n-                        unsigned int blockDimX, unsigned int blockDimY,\n-                        unsigned int blockDimZ, unsigned int sharedMemBytes,\n-                        hipStream_t hStream, void **kernelParams, void **extra);\n-  // function management\n-  static hipError_t hipFuncGetAttributes(hipFuncAttributes *attrib,\n-                                         void *hfunc);\n-  static hipError_t hipFuncSetAttribute(hipFunction_t hfunc,\n-                                        hipFuncAttribute attrib, int value);\n-  static hipError_t hipFuncSetCacheConfig(hipFunction_t hfunc,\n-                                          hipFuncCache_t config);\n-  // memory management\n-  static hipError_t hipMalloc(hipDeviceptr_t *dptr, size_t bytesize);\n-  static hipError_t hipPointerGetAttribute(void *data,\n-                                           CUpointer_attribute attribute,\n-                                           hipDeviceptr_t ptr);\n-  static hipError_t hipMemsetD8Async(hipDeviceptr_t dst, unsigned char x,\n-                                     size_t N, hipStream_t stream);\n-  static hipError_t hipMemcpyDtoH(void *dstHost, hipDeviceptr_t srcDevice,\n-                                  size_t ByteCount);\n-  static hipError_t hipFree(hipDeviceptr_t dptr);\n-  static hipError_t hipMemcpyDtoHAsync(void *dstHost, hipDeviceptr_t srcDevice,\n-                                       size_t ByteCount, hipStream_t hStream);\n-  static hipError_t hipMemcpyHtoDAsync(hipDeviceptr_t dstDevice,\n-                                       const void *srcHost, size_t ByteCount,\n-                                       hipStream_t hStream);\n-  static hipError_t hipMemcpyHtoD(hipDeviceptr_t dstDevice, const void *srcHost,\n-                                  size_t ByteCount);\n-  // event management\n-  static hipError_t hipEventCreate(hipEvent_t *phEvent, unsigned int Flags);\n-  static hipError_t hipEventElapsedTime(float *pMilliseconds, hipEvent_t hStart,\n-                                        hipEvent_t hEnd);\n-  static hipError_t hipEventRecord(hipEvent_t hEvent, hipStream_t hStream);\n-  static hipError_t hipEventDestroy(hipEvent_t hEvent);\n-\n-private:\n-  // Libraries\n-  static void *cuda_;\n-  static void *nvml_;\n-  static void *hip_;\n-\n-  /* ------------------- *\n-   * CUDA\n-   * ------------------- */\n-  // context management\n-  static void *cuCtxGetCurrent_;\n-  static void *cuCtxSetCurrent_;\n-  static void *cuCtxDestroy_v2_;\n-  static void *cuCtxCreate_v2_;\n-  static void *cuCtxGetDevice_;\n-  static void *cuCtxPushCurrent_v2_;\n-  static void *cuCtxPopCurrent_v2_;\n-  static void *cuCtxEnablePeerAccess_;\n-  static void *cuDriverGetVersion_;\n-  static void *cuInit_;\n-  // device management\n-  static void *cuDeviceGet_;\n-  static void *cuDeviceGetName_;\n-  static void *cuDeviceGetPCIBusId_;\n-  static void *cuDeviceGetAttribute_;\n-  static void *cuDeviceGetCount_;\n-  // link management\n-  static void *cuLinkAddData_v2_;\n-  static void *cuLinkCreate_v2_;\n-  static void *cuLinkDestroy_;\n-  static void *cuLinkComplete_;\n-  // module management\n-  static void *cuModuleGetGlobal_v2_;\n-  static void *cuModuleLoad_;\n-  static void *cuModuleUnload_;\n-  static void *cuModuleLoadDataEx_;\n-  static void *cuModuleLoadData_;\n-  static void *cuModuleGetFunction_;\n-  // stream management\n-  static void *cuStreamCreate_;\n-  static void *cuStreamSynchronize_;\n-  static void *cuStreamDestroy_v2_;\n-  static void *cuStreamGetCtx_;\n-  static void *cuLaunchKernel_;\n-  // function management\n-  static void *cuFuncGetAttribute_;\n-  static void *cuFuncSetAttribute_;\n-  static void *cuFuncSetCacheConfig_;\n-  // memory management\n-  static void *cuMemcpyDtoH_v2_;\n-  static void *cuMemFree_v2_;\n-  static void *cuMemcpyDtoHAsync_v2_;\n-  static void *cuMemcpyHtoDAsync_v2_;\n-  static void *cuMemcpyHtoD_v2_;\n-  static void *cuMemAlloc_v2_;\n-  static void *cuMemsetD8Async_;\n-  static void *cuPointerGetAttribute_;\n-  // event management\n-  static void *cuEventCreate_;\n-  static void *cuEventElapsedTime_;\n-  static void *cuEventRecord_;\n-  static void *cuEventDestroy_v2_;\n-\n-  /* ------------------- *\n-   * NVML\n-   * ------------------- */\n-  static void *nvmlInit_v2_;\n-  static void *nvmlDeviceGetHandleByPciBusId_v2_;\n-  static void *nvmlDeviceGetClockInfo_;\n-  static void *nvmlDeviceGetMaxClockInfo_;\n-  static void *nvmlDeviceSetApplicationsClocks_;\n-\n-  /* ------------------- *\n-   * HIP\n-   * ------------------- */\n-  // context management\n-  static void *hipInit_;\n-  static void *hipCtxDestroy_;\n-  static void *hipCtxCreate_;\n-  static void *hipCtxPushCurrent_;\n-  static void *hipCtxPopCurrent_;\n-  static void *hipCtxGetDevice_;\n-  static void *hipCtxEnablePeerAccess_;\n-  static void *hipDriverGetVersion_;\n-  // device management\n-  static void *hipGetDevice_;\n-  static void *hipDeviceGetName_;\n-  static void *hipDeviceGetPCIBusId_;\n-  static void *hipDeviceGetAttribute_;\n-  static void *hipGetDeviceCount_;\n-  // module management\n-  static void *hipModuleGetGlobal_;\n-  static void *hipModuleLoad_;\n-  static void *hipModuleLoadData_;\n-  static void *hipModuleUnload_;\n-  static void *hipModuleLoadDataEx_;\n-  static void *hipModuleGetFunction_;\n-  // stream management\n-  static void *hipStreamCreate_;\n-  static void *hipStreamSynchronize_;\n-  static void *hipStreamDestroy_;\n-  static void *hipModuleLaunchKernel_;\n-  ;\n-  // function management\n-  static void *hipFuncGetAttributes_;\n-  static void *hipFuncSetAttribute_;\n-  static void *hipFuncSetCacheConfig_;\n-  // memory management\n-  static void *hipMalloc_;\n-  static void *hipPointerGetAttribute_;\n-  static void *hipMemsetD8Async_;\n-  static void *hipMemcpyDtoH_;\n-  static void *hipFree_;\n-  static void *hipMemcpyDtoHAsync_;\n-  static void *hipMemcpyHtoDAsync_;\n-  static void *hipMemcpyHtoD_;\n-  // event management\n-  static void *hipEventCreate_;\n-  static void *hipEventElapsedTime_;\n-  static void *hipEventRecord_;\n-  static void *hipEventDestroy_;\n-};\n-\n-} // namespace driver\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/driver/error.h", "status": "removed", "additions": 0, "deletions": 254, "changes": 254, "file_content_changes": "@@ -1,254 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_DRIVER_ERROR_H_\n-#define _TRITON_DRIVER_ERROR_H_\n-\n-#include \"triton/driver/dispatch.h\"\n-#include <exception>\n-\n-namespace triton {\n-\n-namespace driver {\n-\n-namespace exception {\n-\n-namespace nvrtc {\n-\n-#define TRITON_CREATE_NVRTC_EXCEPTION(name, msg)                               \\\n-  class name : public std::exception {                                         \\\n-  public:                                                                      \\\n-    const char *what() const throw() override { return \"NVRTC: Error- \" msg; } \\\n-  }\n-\n-TRITON_CREATE_NVRTC_EXCEPTION(out_of_memory, \"out of memory\");\n-TRITON_CREATE_NVRTC_EXCEPTION(program_creation_failure,\n-                              \"program creation failure\");\n-TRITON_CREATE_NVRTC_EXCEPTION(invalid_input, \"invalid input\");\n-TRITON_CREATE_NVRTC_EXCEPTION(invalid_program, \"invalid program\");\n-TRITON_CREATE_NVRTC_EXCEPTION(invalid_option, \"invalid option\");\n-TRITON_CREATE_NVRTC_EXCEPTION(compilation, \"compilation\");\n-TRITON_CREATE_NVRTC_EXCEPTION(builtin_operation_failure,\n-                              \"builtin operation failure\");\n-TRITON_CREATE_NVRTC_EXCEPTION(unknown_error, \"unknown error\");\n-\n-#undef TRITON_CREATE_NVRTC_EXCEPTION\n-} // namespace nvrtc\n-\n-namespace cuda {\n-class base : public std::exception {};\n-\n-#define TRITON_CREATE_CUDA_EXCEPTION(name, msg)                                \\\n-  class name : public base {                                                   \\\n-  public:                                                                      \\\n-    const char *what() const throw() override { return \"CUDA: Error- \" msg; }  \\\n-  }\n-\n-TRITON_CREATE_CUDA_EXCEPTION(invalid_value, \"invalid value\");\n-TRITON_CREATE_CUDA_EXCEPTION(out_of_memory, \"out of memory\");\n-TRITON_CREATE_CUDA_EXCEPTION(not_initialized, \"not initialized\");\n-TRITON_CREATE_CUDA_EXCEPTION(deinitialized, \"deinitialized\");\n-TRITON_CREATE_CUDA_EXCEPTION(profiler_disabled, \"profiler disabled\");\n-TRITON_CREATE_CUDA_EXCEPTION(profiler_not_initialized,\n-                             \"profiler not initialized\");\n-TRITON_CREATE_CUDA_EXCEPTION(profiler_already_started,\n-                             \"profiler already started\");\n-TRITON_CREATE_CUDA_EXCEPTION(profiler_already_stopped,\n-                             \"profiler already stopped\");\n-TRITON_CREATE_CUDA_EXCEPTION(no_device, \"no device\");\n-TRITON_CREATE_CUDA_EXCEPTION(invalid_device, \"invalid device\");\n-TRITON_CREATE_CUDA_EXCEPTION(invalid_image, \"invalid image\");\n-TRITON_CREATE_CUDA_EXCEPTION(invalid_context, \"invalid context\");\n-TRITON_CREATE_CUDA_EXCEPTION(context_already_current,\n-                             \"context already current\");\n-TRITON_CREATE_CUDA_EXCEPTION(map_failed, \"map failed\");\n-TRITON_CREATE_CUDA_EXCEPTION(unmap_failed, \"unmap failed\");\n-TRITON_CREATE_CUDA_EXCEPTION(array_is_mapped, \"array is mapped\");\n-TRITON_CREATE_CUDA_EXCEPTION(already_mapped, \"already mapped\");\n-TRITON_CREATE_CUDA_EXCEPTION(no_binary_for_gpu, \"no binary for gpu\");\n-TRITON_CREATE_CUDA_EXCEPTION(already_acquired, \"already acquired\");\n-TRITON_CREATE_CUDA_EXCEPTION(not_mapped, \"not mapped\");\n-TRITON_CREATE_CUDA_EXCEPTION(not_mapped_as_array, \"not mapped as array\");\n-TRITON_CREATE_CUDA_EXCEPTION(not_mapped_as_pointer, \"not mapped as pointer\");\n-TRITON_CREATE_CUDA_EXCEPTION(ecc_uncorrectable, \"ecc uncorrectable\");\n-TRITON_CREATE_CUDA_EXCEPTION(unsupported_limit, \"unsupported limit\");\n-TRITON_CREATE_CUDA_EXCEPTION(context_already_in_use, \"context already in use\");\n-TRITON_CREATE_CUDA_EXCEPTION(peer_access_unsupported,\n-                             \"peer access unsupported\");\n-TRITON_CREATE_CUDA_EXCEPTION(invalid_ptx, \"invalid ptx\");\n-TRITON_CREATE_CUDA_EXCEPTION(invalid_graphics_context,\n-                             \"invalid graphics context\");\n-TRITON_CREATE_CUDA_EXCEPTION(invalid_source, \"invalid source\");\n-TRITON_CREATE_CUDA_EXCEPTION(file_not_found, \"file not found\");\n-TRITON_CREATE_CUDA_EXCEPTION(shared_object_symbol_not_found,\n-                             \"shared object symbol not found\");\n-TRITON_CREATE_CUDA_EXCEPTION(shared_object_init_failed,\n-                             \"shared object init failed\");\n-TRITON_CREATE_CUDA_EXCEPTION(operating_system, \"operating system\");\n-TRITON_CREATE_CUDA_EXCEPTION(invalid_handle, \"invalid handle\");\n-TRITON_CREATE_CUDA_EXCEPTION(not_found, \"not found\");\n-TRITON_CREATE_CUDA_EXCEPTION(not_ready, \"not ready\");\n-TRITON_CREATE_CUDA_EXCEPTION(illegal_address, \"illegal address\");\n-TRITON_CREATE_CUDA_EXCEPTION(launch_out_of_resources,\n-                             \"launch out of resources\");\n-TRITON_CREATE_CUDA_EXCEPTION(launch_timeout, \"launch timeout\");\n-TRITON_CREATE_CUDA_EXCEPTION(launch_incompatible_texturing,\n-                             \"launch incompatible texturing\");\n-TRITON_CREATE_CUDA_EXCEPTION(peer_access_already_enabled,\n-                             \"peer access already enabled\");\n-TRITON_CREATE_CUDA_EXCEPTION(peer_access_not_enabled,\n-                             \"peer access not enabled\");\n-TRITON_CREATE_CUDA_EXCEPTION(primary_context_active, \"primary context active\");\n-TRITON_CREATE_CUDA_EXCEPTION(context_is_destroyed, \"context is destroyed\");\n-TRITON_CREATE_CUDA_EXCEPTION(assert_error, \"assert\");\n-TRITON_CREATE_CUDA_EXCEPTION(too_many_peers, \"too many peers\");\n-TRITON_CREATE_CUDA_EXCEPTION(host_memory_already_registered,\n-                             \"host memory already registered\");\n-TRITON_CREATE_CUDA_EXCEPTION(host_memory_not_registered,\n-                             \"hot memory not registered\");\n-TRITON_CREATE_CUDA_EXCEPTION(hardware_stack_error, \"hardware stack error\");\n-TRITON_CREATE_CUDA_EXCEPTION(illegal_instruction, \"illegal instruction\");\n-TRITON_CREATE_CUDA_EXCEPTION(misaligned_address, \"misaligned address\");\n-TRITON_CREATE_CUDA_EXCEPTION(invalid_address_space, \"invalid address space\");\n-TRITON_CREATE_CUDA_EXCEPTION(invalid_pc, \"invalid pc\");\n-TRITON_CREATE_CUDA_EXCEPTION(launch_failed, \"launch failed\");\n-TRITON_CREATE_CUDA_EXCEPTION(not_permitted, \"not permitted\");\n-TRITON_CREATE_CUDA_EXCEPTION(not_supported, \"not supported\");\n-TRITON_CREATE_CUDA_EXCEPTION(unknown, \"unknown\");\n-\n-#undef TRITON_CREATE_CUDA_EXCEPTION\n-} // namespace cuda\n-\n-namespace cublas {\n-class base : public std::exception {};\n-\n-#define TRITON_CREATE_CUBLAS_EXCEPTION(name, msg)                              \\\n-  class name : public base {                                                   \\\n-  public:                                                                      \\\n-    const char *what() const throw() override {                                \\\n-      return \"CUBLAS: Error- \" msg;                                            \\\n-    }                                                                          \\\n-  }\n-\n-TRITON_CREATE_CUBLAS_EXCEPTION(not_initialized, \"not initialized\");\n-TRITON_CREATE_CUBLAS_EXCEPTION(alloc_failed, \"alloc failed\");\n-TRITON_CREATE_CUBLAS_EXCEPTION(invalid_value, \"invalid value\");\n-TRITON_CREATE_CUBLAS_EXCEPTION(arch_mismatch, \"arch mismatch\");\n-TRITON_CREATE_CUBLAS_EXCEPTION(mapping_error, \"mapping error\");\n-TRITON_CREATE_CUBLAS_EXCEPTION(execution_failed, \"execution failed\");\n-TRITON_CREATE_CUBLAS_EXCEPTION(internal_error, \"internal error\");\n-TRITON_CREATE_CUBLAS_EXCEPTION(not_supported, \"not supported\");\n-TRITON_CREATE_CUBLAS_EXCEPTION(license_error, \"license error\");\n-TRITON_CREATE_CUBLAS_EXCEPTION(unknown, \"unknown\");\n-\n-#undef TRITON_CREATE_CUBLAS_EXCEPTION\n-} // namespace cublas\n-\n-namespace cudnn {\n-#define TRITON_CREATE_CUDNN_EXCEPTION(name, msg)                               \\\n-  class name : public std::exception {                                         \\\n-  public:                                                                      \\\n-    const char *what() const throw() override { return \"CUDNN: Error- \" msg; } \\\n-  }\n-\n-TRITON_CREATE_CUDNN_EXCEPTION(not_initialized, \"not initialized\");\n-TRITON_CREATE_CUDNN_EXCEPTION(alloc_failed, \"allocation failed\");\n-TRITON_CREATE_CUDNN_EXCEPTION(bad_param, \"bad param\");\n-TRITON_CREATE_CUDNN_EXCEPTION(internal_error, \"internal error\");\n-TRITON_CREATE_CUDNN_EXCEPTION(invalid_value, \"invalid value\");\n-TRITON_CREATE_CUDNN_EXCEPTION(arch_mismatch, \"arch mismatch\");\n-TRITON_CREATE_CUDNN_EXCEPTION(mapping_error, \"mapping error\");\n-TRITON_CREATE_CUDNN_EXCEPTION(execution_failed, \"execution failed\");\n-TRITON_CREATE_CUDNN_EXCEPTION(not_supported, \"not supported\");\n-TRITON_CREATE_CUDNN_EXCEPTION(license_error, \"license error\");\n-TRITON_CREATE_CUDNN_EXCEPTION(runtime_prerequisite_missing,\n-                              \"prerequisite missing\");\n-TRITON_CREATE_CUDNN_EXCEPTION(runtime_in_progress, \"runtime in progress\");\n-TRITON_CREATE_CUDNN_EXCEPTION(runtime_fp_overflow, \"runtime fp overflow\");\n-} // namespace cudnn\n-\n-namespace hip {\n-class base : public std::exception {};\n-\n-#define TRITON_CREATE_HIP_EXCEPTION(name, msg)                                 \\\n-  class name : public base {                                                   \\\n-  public:                                                                      \\\n-    const char *what() const throw() override { return \"HIP: Error- \" msg; }   \\\n-  }\n-\n-TRITON_CREATE_HIP_EXCEPTION(invalid_value, \"invalid value\");\n-TRITON_CREATE_HIP_EXCEPTION(out_of_memory, \"out of memory\");\n-TRITON_CREATE_HIP_EXCEPTION(not_initialized, \"not initialized\");\n-TRITON_CREATE_HIP_EXCEPTION(deinitialized, \"deinitialized\");\n-TRITON_CREATE_HIP_EXCEPTION(profiler_disabled, \"profiler disabled\");\n-TRITON_CREATE_HIP_EXCEPTION(profiler_not_initialized,\n-                            \"profiler not initialized\");\n-TRITON_CREATE_HIP_EXCEPTION(profiler_already_started,\n-                            \"profiler already started\");\n-TRITON_CREATE_HIP_EXCEPTION(profiler_already_stopped,\n-                            \"profiler already stopped\");\n-TRITON_CREATE_HIP_EXCEPTION(no_device, \"no device\");\n-TRITON_CREATE_HIP_EXCEPTION(invalid_device, \"invalid device\");\n-TRITON_CREATE_HIP_EXCEPTION(invalid_image, \"invalid image\");\n-TRITON_CREATE_HIP_EXCEPTION(invalid_context, \"invalid context\");\n-TRITON_CREATE_HIP_EXCEPTION(context_already_current, \"context already current\");\n-TRITON_CREATE_HIP_EXCEPTION(map_failed, \"map failed\");\n-TRITON_CREATE_HIP_EXCEPTION(unmap_failed, \"unmap failed\");\n-TRITON_CREATE_HIP_EXCEPTION(array_is_mapped, \"array is mapped\");\n-TRITON_CREATE_HIP_EXCEPTION(already_mapped, \"already mapped\");\n-TRITON_CREATE_HIP_EXCEPTION(no_binary_for_gpu, \"no binary for gpu\");\n-TRITON_CREATE_HIP_EXCEPTION(already_acquired, \"already acquired\");\n-TRITON_CREATE_HIP_EXCEPTION(not_mapped, \"not mapped\");\n-TRITON_CREATE_HIP_EXCEPTION(not_mapped_as_array, \"not mapped as array\");\n-TRITON_CREATE_HIP_EXCEPTION(not_mapped_as_pointer, \"not mapped as pointer\");\n-TRITON_CREATE_HIP_EXCEPTION(ecc_uncorrectable, \"ecc uncorrectable\");\n-TRITON_CREATE_HIP_EXCEPTION(unsupported_limit, \"unsupported limit\");\n-TRITON_CREATE_HIP_EXCEPTION(context_already_in_use, \"context already in use\");\n-TRITON_CREATE_HIP_EXCEPTION(peer_access_unsupported, \"peer access unsupported\");\n-TRITON_CREATE_HIP_EXCEPTION(invalid_ptx, \"invalid ptx\");\n-TRITON_CREATE_HIP_EXCEPTION(invalid_graphics_context,\n-                            \"invalid graphics context\");\n-TRITON_CREATE_HIP_EXCEPTION(invalid_source, \"invalid source\");\n-TRITON_CREATE_HIP_EXCEPTION(file_not_found, \"file not found\");\n-TRITON_CREATE_HIP_EXCEPTION(shared_object_symbol_not_found,\n-                            \"shared object symbol not found\");\n-TRITON_CREATE_HIP_EXCEPTION(shared_object_init_failed,\n-                            \"shared object init failed\");\n-TRITON_CREATE_HIP_EXCEPTION(operating_system, \"operating system\");\n-TRITON_CREATE_HIP_EXCEPTION(invalid_handle, \"invalid handle\");\n-TRITON_CREATE_HIP_EXCEPTION(not_found, \"not found\");\n-TRITON_CREATE_HIP_EXCEPTION(not_ready, \"not ready\");\n-TRITON_CREATE_HIP_EXCEPTION(illegal_address, \"illegal address\");\n-TRITON_CREATE_HIP_EXCEPTION(launch_out_of_resources, \"launch out of resources\");\n-TRITON_CREATE_HIP_EXCEPTION(launch_timeout, \"launch timeout\");\n-TRITON_CREATE_HIP_EXCEPTION(launch_incompatible_texturing,\n-                            \"launch incompatible texturing\");\n-TRITON_CREATE_HIP_EXCEPTION(peer_access_already_enabled,\n-                            \"peer access already enabled\");\n-TRITON_CREATE_HIP_EXCEPTION(peer_access_not_enabled, \"peer access not enabled\");\n-TRITON_CREATE_HIP_EXCEPTION(primary_context_active, \"primary context active\");\n-TRITON_CREATE_HIP_EXCEPTION(context_is_destroyed, \"context is destroyed\");\n-TRITON_CREATE_HIP_EXCEPTION(assert_error, \"assert\");\n-TRITON_CREATE_HIP_EXCEPTION(too_many_peers, \"too many peers\");\n-TRITON_CREATE_HIP_EXCEPTION(host_memory_already_registered,\n-                            \"host memory already registered\");\n-TRITON_CREATE_HIP_EXCEPTION(host_memory_not_registered,\n-                            \"hot memory not registered\");\n-TRITON_CREATE_HIP_EXCEPTION(hardware_stack_error, \"hardware stack error\");\n-TRITON_CREATE_HIP_EXCEPTION(illegal_instruction, \"illegal instruction\");\n-TRITON_CREATE_HIP_EXCEPTION(misaligned_address, \"misaligned address\");\n-TRITON_CREATE_HIP_EXCEPTION(invalid_address_space, \"invalid address space\");\n-TRITON_CREATE_HIP_EXCEPTION(invalid_pc, \"invalid pc\");\n-TRITON_CREATE_HIP_EXCEPTION(launch_failed, \"launch failed\");\n-TRITON_CREATE_HIP_EXCEPTION(not_permitted, \"not permitted\");\n-TRITON_CREATE_HIP_EXCEPTION(not_supported, \"not supported\");\n-TRITON_CREATE_HIP_EXCEPTION(invalid_symbol, \"invalid symbol\");\n-TRITON_CREATE_HIP_EXCEPTION(unknown, \"unknown\");\n-\n-#undef TRITON_CREATE_CUDA_EXCEPTION\n-} // namespace hip\n-\n-} // namespace exception\n-} // namespace driver\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/driver/llvm.h", "status": "removed", "additions": 0, "deletions": 22, "changes": 22, "file_content_changes": "@@ -1,22 +0,0 @@\n-#include \"triton/external/CUDA/cuda.h\"\n-#include \"triton/external/hip.h\"\n-#include <string>\n-\n-namespace llvm {\n-class Module;\n-}\n-\n-namespace triton {\n-namespace driver {\n-\n-void init_llvm();\n-std::string path_to_ptxas(int &version);\n-std::string llir_to_ptx(llvm::Module *module, int cc, int version);\n-std::string ptx_to_cubin(const std::string &ptx, const std::string &ptxas_path,\n-                         int cc);\n-CUmodule ptx_to_cumodule(const std::string &ptx, int cc);\n-std::string llir_to_amdgpu(llvm::Module *module, const std::string &proc);\n-hipModule_t amdgpu_to_hipmodule(const std::string &path);\n-\n-} // namespace driver\n-} // namespace triton"}, {"filename": "include/triton/external/CUDA/cuda.h", "status": "removed", "additions": 0, "deletions": 18994, "changes": 18994, "file_content_changes": "N/A"}, {"filename": "include/triton/external/CUDA/nvml.h", "status": "removed", "additions": 0, "deletions": 6281, "changes": 6281, "file_content_changes": "N/A"}, {"filename": "include/triton/external/half.hpp", "status": "removed", "additions": 0, "deletions": 3067, "changes": 3067, "file_content_changes": "N/A"}, {"filename": "include/triton/external/hip.h", "status": "removed", "additions": 0, "deletions": 293, "changes": 293, "file_content_changes": "@@ -1,293 +0,0 @@\n-#ifndef __external_hip_h__\n-#define __external_hip_h__\n-\n-/*\n- * @brief hipError_t\n- * @enum\n- * @ingroup Enumerations\n- */\n-// Developer note - when updating these, update the hipErrorName and hipErrorString functions in\n-// NVCC and HCC paths Also update the hipCUDAErrorTohipError function in NVCC path.\n-\n-// Ignoring error-code return values from hip APIs is discouraged. On C++17,\n-// we can make that yield a warning\n-\n-/*\n- * @brief hipError_t\n- * @enum\n- * @ingroup Enumerations\n- */\n-// Developer note - when updating these, update the hipErrorName and hipErrorString functions in\n-// NVCC and HCC paths Also update the hipCUDAErrorTohipError function in NVCC path.\n-\n-#include <cstddef>\n-\n-typedef enum hipError_t {\n-    hipSuccess = 0,  ///< Successful completion.\n-    hipErrorInvalidValue = 1,  ///< One or more of the parameters passed to the API call is NULL\n-                               ///< or not in an acceptable range.\n-    hipErrorOutOfMemory = 2,\n-    // Deprecated\n-    hipErrorMemoryAllocation = 2,  ///< Memory allocation error.\n-    hipErrorNotInitialized = 3,\n-    // Deprecated\n-    hipErrorInitializationError = 3,\n-    hipErrorDeinitialized = 4,\n-    hipErrorProfilerDisabled = 5,\n-    hipErrorProfilerNotInitialized = 6,\n-    hipErrorProfilerAlreadyStarted = 7,\n-    hipErrorProfilerAlreadyStopped = 8,\n-    hipErrorInvalidConfiguration = 9,\n-    hipErrorInvalidPitchValue = 12,\n-    hipErrorInvalidSymbol = 13,\n-    hipErrorInvalidDevicePointer = 17,  ///< Invalid Device Pointer\n-    hipErrorInvalidMemcpyDirection = 21,  ///< Invalid memory copy direction\n-    hipErrorInsufficientDriver = 35,\n-    hipErrorMissingConfiguration = 52,\n-    hipErrorPriorLaunchFailure = 53,\n-    hipErrorInvalidDeviceFunction = 98,\n-    hipErrorNoDevice = 100,  ///< Call to hipGetDeviceCount returned 0 devices\n-    hipErrorInvalidDevice = 101,  ///< DeviceID must be in range 0...#compute-devices.\n-    hipErrorInvalidImage = 200,\n-    hipErrorInvalidContext = 201,  ///< Produced when input context is invalid.\n-    hipErrorContextAlreadyCurrent = 202,\n-    hipErrorMapFailed = 205,\n-    // Deprecated\n-    hipErrorMapBufferObjectFailed = 205,  ///< Produced when the IPC memory attach failed from ROCr.\n-    hipErrorUnmapFailed = 206,\n-    hipErrorArrayIsMapped = 207,\n-    hipErrorAlreadyMapped = 208,\n-    hipErrorNoBinaryForGpu = 209,\n-    hipErrorAlreadyAcquired = 210,\n-    hipErrorNotMapped = 211,\n-    hipErrorNotMappedAsArray = 212,\n-    hipErrorNotMappedAsPointer = 213,\n-    hipErrorECCNotCorrectable = 214,\n-    hipErrorUnsupportedLimit = 215,\n-    hipErrorContextAlreadyInUse = 216,\n-    hipErrorPeerAccessUnsupported = 217,\n-    hipErrorInvalidKernelFile = 218,  ///< In CUDA DRV, it is CUDA_ERROR_INVALID_PTX\n-    hipErrorInvalidGraphicsContext = 219,\n-    hipErrorInvalidSource = 300,\n-    hipErrorFileNotFound = 301,\n-    hipErrorSharedObjectSymbolNotFound = 302,\n-    hipErrorSharedObjectInitFailed = 303,\n-    hipErrorOperatingSystem = 304,\n-    hipErrorInvalidHandle = 400,\n-    // Deprecated\n-    hipErrorInvalidResourceHandle = 400,  ///< Resource handle (hipEvent_t or hipStream_t) invalid.\n-    hipErrorNotFound = 500,\n-    hipErrorNotReady = 600,  ///< Indicates that asynchronous operations enqueued earlier are not\n-                             ///< ready.  This is not actually an error, but is used to distinguish\n-                             ///< from hipSuccess (which indicates completion).  APIs that return\n-                             ///< this error include hipEventQuery and hipStreamQuery.\n-    hipErrorIllegalAddress = 700,\n-    hipErrorLaunchOutOfResources = 701,  ///< Out of resources error.\n-    hipErrorLaunchTimeOut = 702,\n-    hipErrorPeerAccessAlreadyEnabled =\n-        704,  ///< Peer access was already enabled from the current device.\n-    hipErrorPeerAccessNotEnabled =\n-        705,  ///< Peer access was never enabled from the current device.\n-    hipErrorSetOnActiveProcess = 708,\n-    hipErrorAssert = 710,  ///< Produced when the kernel calls assert.\n-    hipErrorHostMemoryAlreadyRegistered =\n-        712,  ///< Produced when trying to lock a page-locked memory.\n-    hipErrorHostMemoryNotRegistered =\n-        713,  ///< Produced when trying to unlock a non-page-locked memory.\n-    hipErrorLaunchFailure =\n-        719,  ///< An exception occurred on the device while executing a kernel.\n-    hipErrorCooperativeLaunchTooLarge =\n-        720,  ///< This error indicates that the number of blocks launched per grid for a kernel\n-              ///< that was launched via cooperative launch APIs exceeds the maximum number of\n-              ///< allowed blocks for the current device\n-    hipErrorNotSupported = 801,  ///< Produced when the hip API is not supported/implemented\n-    hipErrorUnknown = 999,  //< Unknown error.\n-    // HSA Runtime Error Codes start here.\n-    hipErrorRuntimeMemory = 1052,  ///< HSA runtime memory call returned error.  Typically not seen\n-                                   ///< in production systems.\n-    hipErrorRuntimeOther = 1053,  ///< HSA runtime call other than memory returned error.  Typically\n-                                  ///< not seen in production systems.\n-    hipErrorTbd  ///< Marker that more error codes are needed.\n-} hipError_t;\n-\n-\n-typedef struct ihipCtx_t* hipCtx_t;\n-\n-// Note many APIs also use integer deviceIds as an alternative to the device pointer:\n-typedef int hipDevice_t;\n-\n-typedef enum hipDeviceP2PAttr {\n-  hipDevP2PAttrPerformanceRank = 0,\n-  hipDevP2PAttrAccessSupported,\n-  hipDevP2PAttrNativeAtomicSupported,\n-  hipDevP2PAttrHipArrayAccessSupported\n-} hipDeviceP2PAttr;\n-\n-typedef struct ihipStream_t* hipStream_t;\n-\n-#define hipIpcMemLazyEnablePeerAccess 0\n-\n-#define HIP_IPC_HANDLE_SIZE 64\n-\n-typedef struct hipIpcMemHandle_st {\n-    char reserved[HIP_IPC_HANDLE_SIZE];\n-} hipIpcMemHandle_t;\n-\n-typedef struct hipIpcEventHandle_st {\n-    char reserved[HIP_IPC_HANDLE_SIZE];\n-} hipIpcEventHandle_t;\n-\n-typedef struct ihipModule_t* hipModule_t;\n-\n-typedef struct ihipModuleSymbol_t* hipFunction_t;\n-\n-typedef struct hipFuncAttributes {\n-    int binaryVersion;\n-    int cacheModeCA;\n-    size_t constSizeBytes;\n-    size_t localSizeBytes;\n-    int maxDynamicSharedSizeBytes;\n-    int maxThreadsPerBlock;\n-    int numRegs;\n-    int preferredShmemCarveout;\n-    int ptxVersion;\n-    size_t sharedSizeBytes;\n-} hipFuncAttributes;\n-\n-typedef struct ihipEvent_t* hipEvent_t;\n-\n-/*\n- * @brief hipDeviceAttribute_t\n- * @enum\n- * @ingroup Enumerations\n- */\n-typedef enum hipDeviceAttribute_t {\n-    hipDeviceAttributeMaxThreadsPerBlock,       ///< Maximum number of threads per block.\n-    hipDeviceAttributeMaxBlockDimX,             ///< Maximum x-dimension of a block.\n-    hipDeviceAttributeMaxBlockDimY,             ///< Maximum y-dimension of a block.\n-    hipDeviceAttributeMaxBlockDimZ,             ///< Maximum z-dimension of a block.\n-    hipDeviceAttributeMaxGridDimX,              ///< Maximum x-dimension of a grid.\n-    hipDeviceAttributeMaxGridDimY,              ///< Maximum y-dimension of a grid.\n-    hipDeviceAttributeMaxGridDimZ,              ///< Maximum z-dimension of a grid.\n-    hipDeviceAttributeMaxSharedMemoryPerBlock,  ///< Maximum shared memory available per block in\n-                                                ///< bytes.\n-    hipDeviceAttributeTotalConstantMemory,      ///< Constant memory size in bytes.\n-    hipDeviceAttributeWarpSize,                 ///< Warp size in threads.\n-    hipDeviceAttributeMaxRegistersPerBlock,  ///< Maximum number of 32-bit registers available to a\n-                                             ///< thread block. This number is shared by all thread\n-                                             ///< blocks simultaneously resident on a\n-                                             ///< multiprocessor.\n-    hipDeviceAttributeClockRate,             ///< Peak clock frequency in kilohertz.\n-    hipDeviceAttributeMemoryClockRate,       ///< Peak memory clock frequency in kilohertz.\n-    hipDeviceAttributeMemoryBusWidth,        ///< Global memory bus width in bits.\n-    hipDeviceAttributeMultiprocessorCount,   ///< Number of multiprocessors on the device.\n-    hipDeviceAttributeComputeMode,           ///< Compute mode that device is currently in.\n-    hipDeviceAttributeL2CacheSize,  ///< Size of L2 cache in bytes. 0 if the device doesn't have L2\n-                                    ///< cache.\n-    hipDeviceAttributeMaxThreadsPerMultiProcessor,  ///< Maximum resident threads per\n-                                                    ///< multiprocessor.\n-    hipDeviceAttributeComputeCapabilityMajor,       ///< Major compute capability version number.\n-    hipDeviceAttributeComputeCapabilityMinor,       ///< Minor compute capability version number.\n-    hipDeviceAttributeConcurrentKernels,  ///< Device can possibly execute multiple kernels\n-                                          ///< concurrently.\n-    hipDeviceAttributePciBusId,           ///< PCI Bus ID.\n-    hipDeviceAttributePciDeviceId,        ///< PCI Device ID.\n-    hipDeviceAttributeMaxSharedMemoryPerMultiprocessor,  ///< Maximum Shared Memory Per\n-                                                         ///< Multiprocessor.\n-    hipDeviceAttributeIsMultiGpuBoard,                   ///< Multiple GPU devices.\n-    hipDeviceAttributeIntegrated,                        ///< iGPU\n-    hipDeviceAttributeCooperativeLaunch,                 ///< Support cooperative launch\n-    hipDeviceAttributeCooperativeMultiDeviceLaunch,      ///< Support cooperative launch on multiple devices\n-    hipDeviceAttributeMaxTexture1DWidth,    ///< Maximum number of elements in 1D images\n-    hipDeviceAttributeMaxTexture2DWidth,    ///< Maximum dimension width of 2D images in image elements\n-    hipDeviceAttributeMaxTexture2DHeight,   ///< Maximum dimension height of 2D images in image elements\n-    hipDeviceAttributeMaxTexture3DWidth,    ///< Maximum dimension width of 3D images in image elements\n-    hipDeviceAttributeMaxTexture3DHeight,   ///< Maximum dimensions height of 3D images in image elements\n-    hipDeviceAttributeMaxTexture3DDepth,    ///< Maximum dimensions depth of 3D images in image elements\n-\n-    hipDeviceAttributeHdpMemFlushCntl,      ///< Address of the HDP_MEM_COHERENCY_FLUSH_CNTL register\n-    hipDeviceAttributeHdpRegFlushCntl,      ///< Address of the HDP_REG_COHERENCY_FLUSH_CNTL register\n-\n-    hipDeviceAttributeMaxPitch,             ///< Maximum pitch in bytes allowed by memory copies\n-    hipDeviceAttributeTextureAlignment,     ///<Alignment requirement for textures\n-    hipDeviceAttributeTexturePitchAlignment, ///<Pitch alignment requirement for 2D texture references bound to pitched memory;\n-    hipDeviceAttributeKernelExecTimeout,    ///<Run time limit for kernels executed on the device\n-    hipDeviceAttributeCanMapHostMemory,     ///<Device can map host memory into device address space\n-    hipDeviceAttributeEccEnabled,           ///<Device has ECC support enabled\n-\n-    hipDeviceAttributeCooperativeMultiDeviceUnmatchedFunc,        ///< Supports cooperative launch on multiple\n-                                                                  ///devices with unmatched functions\n-    hipDeviceAttributeCooperativeMultiDeviceUnmatchedGridDim,     ///< Supports cooperative launch on multiple\n-                                                                  ///devices with unmatched grid dimensions\n-    hipDeviceAttributeCooperativeMultiDeviceUnmatchedBlockDim,    ///< Supports cooperative launch on multiple\n-                                                                  ///devices with unmatched block dimensions\n-    hipDeviceAttributeCooperativeMultiDeviceUnmatchedSharedMem,   ///< Supports cooperative launch on multiple\n-                                                                  ///devices with unmatched shared memories\n-    hipDeviceAttributeAsicRevision,         ///< Revision of the GPU in this device\n-    hipDeviceAttributeManagedMemory,        ///< Device supports allocating managed memory on this system\n-    hipDeviceAttributeDirectManagedMemAccessFromHost, ///< Host can directly access managed memory on\n-                                                      /// the device without migration\n-    hipDeviceAttributeConcurrentManagedAccess,  ///< Device can coherently access managed memory\n-                                                /// concurrently with the CPU\n-    hipDeviceAttributePageableMemoryAccess,     ///< Device supports coherently accessing pageable memory\n-                                                /// without calling hipHostRegister on it\n-    hipDeviceAttributePageableMemoryAccessUsesHostPageTables, ///< Device accesses pageable memory via\n-                                                              /// the host's page tables\n-    hipDeviceAttributeCanUseStreamWaitValue ///< '1' if Device supports hipStreamWaitValue32() and\n-                                            ///< hipStreamWaitValue64() , '0' otherwise.\n-\n-} hipDeviceAttribute_t;\n-\n-typedef void* hipDeviceptr_t;\n-\n-/*\n- * @brief hipJitOption\n- * @enum\n- * @ingroup Enumerations\n- */\n-typedef enum hipJitOption {\n-    hipJitOptionMaxRegisters = 0,\n-    hipJitOptionThreadsPerBlock,\n-    hipJitOptionWallTime,\n-    hipJitOptionInfoLogBuffer,\n-    hipJitOptionInfoLogBufferSizeBytes,\n-    hipJitOptionErrorLogBuffer,\n-    hipJitOptionErrorLogBufferSizeBytes,\n-    hipJitOptionOptimizationLevel,\n-    hipJitOptionTargetFromContext,\n-    hipJitOptionTarget,\n-    hipJitOptionFallbackStrategy,\n-    hipJitOptionGenerateDebugInfo,\n-    hipJitOptionLogVerbose,\n-    hipJitOptionGenerateLineInfo,\n-    hipJitOptionCacheMode,\n-    hipJitOptionSm3xOpt,\n-    hipJitOptionFastCompile,\n-    hipJitOptionNumOptions\n-} hipJitOption;\n-\n-/**\n- * @warning On AMD devices and some Nvidia devices, these hints and controls are ignored.\n- */\n-typedef enum hipFuncAttribute {\n-    hipFuncAttributeMaxDynamicSharedMemorySize = 8,\n-    hipFuncAttributePreferredSharedMemoryCarveout = 9,\n-    hipFuncAttributeMax\n-} hipFuncAttribute;\n-\n-/**\n- * @warning On AMD devices and some Nvidia devices, these hints and controls are ignored.\n- */\n-typedef enum hipFuncCache_t {\n-    hipFuncCachePreferNone,    ///< no preference for shared memory or L1 (default)\n-    hipFuncCachePreferShared,  ///< prefer larger shared memory and smaller L1 cache\n-    hipFuncCachePreferL1,      ///< prefer larger L1 cache and smaller shared memory\n-    hipFuncCachePreferEqual,   ///< prefer equal size L1 cache and shared memory\n-} hipFuncCache_t;\n-\n-\n-#define HIP_LAUNCH_PARAM_BUFFER_POINTER ((void*)0x01)\n-#define HIP_LAUNCH_PARAM_BUFFER_SIZE ((void*)0x02)\n-#define HIP_LAUNCH_PARAM_END ((void*)0x03)\n-\n-#endif\n\\ No newline at end of file"}, {"filename": "include/triton/tools/bench.hpp", "status": "removed", "additions": 0, "deletions": 57, "changes": 57, "file_content_changes": "@@ -1,57 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_TOOLS_BENCH_H_\n-#define _TRITON_TOOLS_BENCH_H_\n-\n-#include \"triton/driver/device.h\"\n-#include \"triton/driver/stream.h\"\n-#include <algorithm>\n-#include <chrono>\n-#include <functional>\n-\n-namespace triton {\n-namespace tools {\n-\n-class timer {\n-  typedef std::chrono::high_resolution_clock high_resolution_clock;\n-  typedef std::chrono::nanoseconds nanoseconds;\n-\n-public:\n-  explicit timer(bool run = false) {\n-    if (run)\n-      start();\n-  }\n-\n-  void start() { _start = high_resolution_clock::now(); }\n-\n-  nanoseconds get() const {\n-    return std::chrono::duration_cast<nanoseconds>(\n-        high_resolution_clock::now() - _start);\n-  }\n-\n-private:\n-  high_resolution_clock::time_point _start;\n-};\n-\n-inline double bench(std::function<void()> const &op, driver::stream *stream,\n-                    size_t warmup = 10, size_t repeat = 200) {\n-  timer tmr;\n-  std::vector<size_t> times;\n-  double total_time = 0;\n-  for (size_t i = 0; i < warmup; i++)\n-    op();\n-  stream->synchronize();\n-  tmr.start();\n-  for (size_t i = 0; i < repeat; i++) {\n-    op();\n-  }\n-  stream->synchronize();\n-  return (float)tmr.get().count() / repeat;\n-\n-  //  return *std::min_element(times.begin(), times.end());\n-}\n-\n-} // namespace tools\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/tools/graph.h", "status": "removed", "additions": 0, "deletions": 68, "changes": 68, "file_content_changes": "@@ -1,68 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_TOOLS_THREAD_GRAPH_H_\n-#define _TRITON_TOOLS_THREAD_GRAPH_H_\n-\n-#include <iostream>\n-#include <map>\n-#include <set>\n-#include <vector>\n-\n-namespace triton {\n-namespace tools {\n-\n-template <class node_t> class graph {\n-  typedef std::map<node_t, std::set<node_t>> edges_t;\n-\n-public:\n-  typedef std::map<size_t, std::vector<node_t>> cmap_t;\n-  typedef std::map<node_t, size_t> nmap_t;\n-\n-private:\n-  void connected_components_impl(node_t x, std::set<node_t> &nodes,\n-                                 nmap_t *nmap, cmap_t *cmap, int id) const {\n-    if (nmap)\n-      (*nmap)[x] = id;\n-    if (cmap)\n-      (*cmap)[id].push_back(x);\n-    if (nodes.find(x) != nodes.end()) {\n-      nodes.erase(x);\n-      for (const node_t &y : edges_.at(x))\n-        connected_components_impl(y, nodes, nmap, cmap, id);\n-    }\n-  }\n-\n-public:\n-  void connected_components(cmap_t *cmap, nmap_t *nmap) const {\n-    if (cmap)\n-      cmap->clear();\n-    if (nmap)\n-      nmap->clear();\n-    std::set<node_t> nodes = nodes_;\n-    unsigned id = 0;\n-    while (!nodes.empty()) {\n-      connected_components_impl(*nodes.begin(), nodes, nmap, cmap, id++);\n-    }\n-  }\n-\n-  void add_edge(node_t x, node_t y) {\n-    nodes_.insert(x);\n-    nodes_.insert(y);\n-    edges_[x].insert(y);\n-    edges_[y].insert(x);\n-  }\n-\n-  void clear() {\n-    nodes_.clear();\n-    edges_.clear();\n-  }\n-\n-private:\n-  std::set<node_t> nodes_;\n-  edges_t edges_;\n-};\n-\n-} // namespace tools\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/tools/sha1.hpp", "status": "removed", "additions": 0, "deletions": 172, "changes": 172, "file_content_changes": "@@ -1,172 +0,0 @@\n-/*\n- Copyright (c) 2011, Micael Hildenborg\n- All rights reserved.\n- Redistribution and use in source and binary forms, with or without\n- modification, are permitted provided that the following conditions are met:\n-    * Redistributions of source code must retain the above copyright\n-      notice, this list of conditions and the following disclaimer.\n-    * Redistributions in binary form must reproduce the above copyright\n-      notice, this list of conditions and the following disclaimer in the\n-      documentation and/or other materials provided with the distribution.\n-    * Neither the name of Micael Hildenborg nor the\n-      names of its contributors may be used to endorse or promote products\n-      derived from this software without specific prior written permission.\n- THIS SOFTWARE IS PROVIDED BY Micael Hildenborg ''AS IS'' AND ANY\n- EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n- WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n- DISCLAIMED. IN NO EVENT SHALL Micael Hildenborg BE LIABLE FOR ANY\n- DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n- (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n- LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n- ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n- (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n- SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n- */\n-\n-/*\n- Contributors:\n- Gustav\n- Several members in the gamedev.se forum.\n- Gregory Petrosyan\n- */\n-\n-#ifndef _TRITON_TOOLS_SHA1_HPP_\n-#define _TRITON_TOOLS_SHA1_HPP_\n-\n-namespace sha1 {\n-namespace // local\n-{\n-// Rotate an integer value to left.\n-inline unsigned int rol(const unsigned int value, const unsigned int steps) {\n-  return ((value << steps) | (value >> (32 - steps)));\n-}\n-\n-// Sets the first 16 integers in the buffert to zero.\n-// Used for clearing the W buffert.\n-inline void clearWBuffert(unsigned int *buffert) {\n-  for (int pos = 16; --pos >= 0;) {\n-    buffert[pos] = 0;\n-  }\n-}\n-\n-inline void innerHash(unsigned int *result, unsigned int *w) {\n-  unsigned int a = result[0];\n-  unsigned int b = result[1];\n-  unsigned int c = result[2];\n-  unsigned int d = result[3];\n-  unsigned int e = result[4];\n-\n-  int round = 0;\n-\n-#define sha1macro(func, val)                                                   \\\n-  {                                                                            \\\n-    const unsigned int t = rol(a, 5) + (func) + e + val + w[round];            \\\n-    e = d;                                                                     \\\n-    d = c;                                                                     \\\n-    c = rol(b, 30);                                                            \\\n-    b = a;                                                                     \\\n-    a = t;                                                                     \\\n-  }\n-\n-  while (round < 16) {\n-    sha1macro((b & c) | (~b & d), 0x5a827999)++ round;\n-  }\n-  while (round < 20) {\n-    w[round] =\n-        rol((w[round - 3] ^ w[round - 8] ^ w[round - 14] ^ w[round - 16]), 1);\n-    sha1macro((b & c) | (~b & d), 0x5a827999)++ round;\n-  }\n-  while (round < 40) {\n-    w[round] =\n-        rol((w[round - 3] ^ w[round - 8] ^ w[round - 14] ^ w[round - 16]), 1);\n-    sha1macro(b ^ c ^ d, 0x6ed9eba1)++ round;\n-  }\n-  while (round < 60) {\n-    w[round] =\n-        rol((w[round - 3] ^ w[round - 8] ^ w[round - 14] ^ w[round - 16]), 1);\n-    sha1macro((b & c) | (b & d) | (c & d), 0x8f1bbcdc)++ round;\n-  }\n-  while (round < 80) {\n-    w[round] =\n-        rol((w[round - 3] ^ w[round - 8] ^ w[round - 14] ^ w[round - 16]), 1);\n-    sha1macro(b ^ c ^ d, 0xca62c1d6)++ round;\n-  }\n-\n-#undef sha1macro\n-\n-  result[0] += a;\n-  result[1] += b;\n-  result[2] += c;\n-  result[3] += d;\n-  result[4] += e;\n-}\n-} // namespace\n-\n-inline void calc(const void *src, const int bytelength, unsigned char *hash) {\n-  // Init the result array.\n-  unsigned int result[5] = {0x67452301, 0xefcdab89, 0x98badcfe, 0x10325476,\n-                            0xc3d2e1f0};\n-\n-  // Cast the void src pointer to be the byte array we can work with.\n-  const unsigned char *sarray = (const unsigned char *)src;\n-\n-  // The reusable round buffer\n-  unsigned int w[80];\n-\n-  // Loop through all complete 64byte blocks.\n-  const int endOfFullBlocks = bytelength - 64;\n-  int endCurrentBlock;\n-  int currentBlock = 0;\n-\n-  while (currentBlock <= endOfFullBlocks) {\n-    endCurrentBlock = currentBlock + 64;\n-\n-    // Init the round buffer with the 64 byte block data.\n-    for (int roundPos = 0; currentBlock < endCurrentBlock; currentBlock += 4) {\n-      // This line will swap endian on big endian and keep endian on little\n-      // endian.\n-      w[roundPos++] = (unsigned int)sarray[currentBlock + 3] |\n-                      (((unsigned int)sarray[currentBlock + 2]) << 8) |\n-                      (((unsigned int)sarray[currentBlock + 1]) << 16) |\n-                      (((unsigned int)sarray[currentBlock]) << 24);\n-    }\n-    innerHash(result, w);\n-  }\n-\n-  // Handle the last and not full 64 byte block if existing.\n-  endCurrentBlock = bytelength - currentBlock;\n-  clearWBuffert(w);\n-  int lastBlockBytes = 0;\n-  for (; lastBlockBytes < endCurrentBlock; ++lastBlockBytes) {\n-    w[lastBlockBytes >> 2] |=\n-        (unsigned int)sarray[lastBlockBytes + currentBlock]\n-        << ((3 - (lastBlockBytes & 3)) << 3);\n-  }\n-  w[lastBlockBytes >> 2] |= 0x80 << ((3 - (lastBlockBytes & 3)) << 3);\n-  if (endCurrentBlock >= 56) {\n-    innerHash(result, w);\n-    clearWBuffert(w);\n-  }\n-  w[15] = bytelength << 3;\n-  innerHash(result, w);\n-\n-  // Store hash in result pointer, and make sure we get in in the correct order\n-  // on both endian models.\n-  for (int hashByte = 20; --hashByte >= 0;) {\n-    hash[hashByte] =\n-        (result[hashByte >> 2] >> (((3 - hashByte) & 0x3) << 3)) & 0xff;\n-  }\n-}\n-\n-inline void toHexString(const unsigned char *hash, char *hexstring) {\n-  const char hexDigits[] = {\"0123456789abcdef\"};\n-\n-  for (int hashByte = 20; --hashByte >= 0;) {\n-    hexstring[hashByte << 1] = hexDigits[(hash[hashByte] >> 4) & 0xf];\n-    hexstring[(hashByte << 1) + 1] = hexDigits[hash[hashByte] & 0xf];\n-  }\n-  hexstring[40] = 0;\n-}\n-} // namespace sha1\n-\n-#endif"}, {"filename": "include/triton/tools/sys/exec.hpp", "status": "removed", "additions": 0, "deletions": 42, "changes": 42, "file_content_changes": "@@ -1,42 +0,0 @@\n-#ifndef TRITON_TOOLS_SYS_EXEC_HPP\n-#define TRITON_TOOLS_SYS_EXEC_HPP\n-\n-#include <cstdio>\n-#include <iostream>\n-#include <memory>\n-#include <stdexcept>\n-#include <string>\n-\n-namespace triton {\n-namespace tools {\n-\n-#ifdef _WIN32\n-#define popen _popen\n-#define pclose _pclose\n-#endif\n-\n-#ifndef WEXITSTATUS\n-#define WEXITSTATUS(stat_val) ((unsigned)(stat_val)&255)\n-#endif\n-\n-int exec(const std::string &cmd, std::string &result) {\n-  char buffer[128];\n-  FILE *pipe = popen(cmd.c_str(), \"r\");\n-  if (!pipe)\n-    return 0;\n-  result.clear();\n-  try {\n-    while (fgets(buffer, sizeof buffer, pipe) != NULL)\n-      result += buffer;\n-  } catch (...) {\n-    pclose(pipe);\n-    return 0;\n-  }\n-  int status = pclose(pipe);\n-  return WEXITSTATUS(status);\n-}\n-\n-} // namespace tools\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/tools/sys/mkdir.hpp", "status": "removed", "additions": 0, "deletions": 70, "changes": 70, "file_content_changes": "@@ -1,70 +0,0 @@\n-/*\n- * Copyright (c) 2015, PHILIPPE TILLET. All rights reserved.\n- *\n- * This file is part of ISAAC.\n- *\n- * ISAAC is free software; you can redistribute it and/or\n- * modify it under the terms of the GNU Lesser General Public\n- * License as published by the Free Software Foundation; either\n- * version 2.1 of the License, or (at your option) any later version.\n- *\n- * This library is distributed in the hope that it will be useful,\n- * but WITHOUT ANY WARRANTY; without even the implied warranty of\n- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n- * Lesser General Public License for more details.\n- *\n- * You should have received a copy of the GNU Lesser General Public\n- * License along with this library; if not, write to the Free Software\n- * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,\n- * MA 02110-1301  USA\n- */\n-\n-#ifndef TDL_TOOLS_SYS_MKDIR_HPP\n-#define TDL_TOOLS_SYS_MKDIR_HPP\n-\n-#include <cstdlib>\n-#include <cstring>\n-#include <errno.h>\n-#include <string>\n-#include <sys/stat.h>\n-#if defined(_WIN32)\n-#include <direct.h>\n-#endif\n-\n-namespace triton {\n-\n-namespace tools {\n-\n-inline int mkdir(std::string const &path) {\n-#if defined(_WIN32)\n-  return _mkdir(path.c_str());\n-#else\n-  return ::mkdir(path.c_str(), 0777);\n-#endif\n-}\n-\n-inline int mkpath(std::string const &path) {\n-  int status = 0;\n-  size_t pp = 0;\n-  size_t sp;\n-  while ((sp = path.find('/', pp)) != std::string::npos) {\n-    if (sp != pp) {\n-      status = mkdir(path.substr(0, sp));\n-    }\n-    pp = sp + 1;\n-  }\n-  return (status == 0 || errno == EEXIST) ? 0 : -1;\n-}\n-\n-inline int mtime(std::string const &path) {\n-  struct stat st;\n-  if (stat(path.c_str(), &st) != 0)\n-    return 0;\n-  return st.st_mtime;\n-}\n-\n-} // namespace tools\n-\n-} // namespace triton\n-\n-#endif"}, {"filename": "include/triton/tools/thread_pool.h", "status": "removed", "additions": 0, "deletions": 81, "changes": 81, "file_content_changes": "@@ -1,81 +0,0 @@\n-#pragma once\n-\n-#ifndef _TRITON_TOOLS_THREAD_POOL_H_\n-#define _TRITON_TOOLS_THREAD_POOL_H_\n-\n-#include <condition_variable>\n-#include <functional>\n-#include <future>\n-#include <memory>\n-#include <mutex>\n-#include <queue>\n-#include <stdexcept>\n-#include <thread>\n-#include <vector>\n-\n-class ThreadPool {\n-public:\n-  ThreadPool(size_t threads) : stop(false) {\n-    for (size_t i = 0; i < threads; ++i)\n-      workers.emplace_back([this] {\n-        for (;;) {\n-          std::function<void()> task;\n-          {\n-            std::unique_lock<std::mutex> lock(this->queue_mutex);\n-            this->condition.wait(\n-                lock, [this] { return this->stop || !this->tasks.empty(); });\n-            if (this->stop && this->tasks.empty())\n-              return;\n-            task = std::move(this->tasks.front());\n-            this->tasks.pop();\n-          }\n-          task();\n-        }\n-      });\n-  }\n-\n-  template <class F, class... Args>\n-  auto enqueue(F &&f, Args &&...args)\n-      -> std::future<typename std::result_of<F(Args...)>::type> {\n-    using return_type = typename std::result_of<F(Args...)>::type;\n-\n-    auto task = std::make_shared<std::packaged_task<return_type()>>(\n-        std::bind(std::forward<F>(f), std::forward<Args>(args)...));\n-\n-    std::future<return_type> res = task->get_future();\n-    {\n-      std::unique_lock<std::mutex> lock(queue_mutex);\n-\n-      // don't allow enqueueing after stopping the pool\n-      if (stop)\n-        throw std::runtime_error(\"enqueue on stopped ThreadPool\");\n-\n-      tasks.emplace([task]() { (*task)(); });\n-    }\n-    condition.notify_one();\n-    return res;\n-  }\n-\n-  ~ThreadPool() {\n-    {\n-      std::unique_lock<std::mutex> lock(queue_mutex);\n-      stop = true;\n-    }\n-    condition.notify_all();\n-    for (std::thread &worker : workers)\n-      worker.join();\n-  }\n-\n-private:\n-  // need to keep track of threads so we can join them\n-  std::vector<std::thread> workers;\n-  // the task queue\n-  std::queue<std::function<void()>> tasks;\n-\n-  // synchronization\n-  std::mutex queue_mutex;\n-  std::condition_variable condition;\n-  bool stop;\n-};\n-\n-#endif"}, {"filename": "lib/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,5 +1,4 @@\n # add_subdirectory(codegen)\n-add_subdirectory(driver)\n add_subdirectory(Analysis)\n add_subdirectory(Conversion)\n add_subdirectory(Dialect)"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -13,7 +13,6 @@\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n-#include \"triton/driver/llvm.h\"\n #include \"triton/tools/sys/getenv.hpp\"\n #include \"llvm/IR/Constants.h\"\n \n@@ -99,7 +98,6 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module) {\n   }\n \n   // Initialize LLVM targets.\n-  ::triton::driver::init_llvm();\n   mlir::ExecutionEngine::setupTargetTriple(llvmModule.get());\n \n   auto optPipeline = mlir::makeOptimizingTransformer("}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 118, "deletions": 20, "changes": 138, "file_content_changes": "@@ -11,31 +11,129 @@\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n-#include \"triton/driver/dispatch.h\"\n-#include \"triton/driver/llvm.h\"\n+\n+#include \"llvm/ExecutionEngine/ExecutionEngine.h\"\n+#include \"llvm/ExecutionEngine/SectionMemoryManager.h\"\n+#include \"llvm/IR/IRBuilder.h\"\n+#include \"llvm/IR/IRPrintingPasses.h\"\n+#include \"llvm/IR/LegacyPassManager.h\"\n+#include \"llvm/IR/Module.h\"\n+#include \"llvm/IR/Verifier.h\"\n+#include \"llvm/MC/TargetRegistry.h\"\n+#include \"llvm/Support/CodeGen.h\"\n+#include \"llvm/Support/CommandLine.h\"\n+#include \"llvm/Support/SourceMgr.h\"\n+#include \"llvm/Support/TargetSelect.h\"\n+#include \"llvm/Support/raw_ostream.h\"\n+#include \"llvm/Target/TargetMachine.h\"\n+#include \"llvm/Target/TargetOptions.h\"\n+#include \"llvm/Transforms/Scalar.h\"\n+#include \"llvm/Transforms/Utils/Cloning.h\"\n+#include <regex>\n \n namespace triton {\n \n-void getCuCCAndVersionFromDevice(uint64_t device, int *cc, int *version,\n-                                 std::string *ptxasPath) {\n-  CUdevice dev = (CUdevice)device;\n-  size_t major = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR>(dev);\n-  size_t minor = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR>(dev);\n-  *cc = major * 10 + minor;\n-  *ptxasPath = driver::path_to_ptxas(*version); // assign version\n+extern \"C\" {\n+int set_curterm(char *nterm) { return 0; }\n+int del_curterm(char *nterm) { return 0; }\n+int tigetnum(char *capname) { return 0; }\n+int setupterm(char *term, int fildes, int *errret) { return 0; }\n+}\n+\n+static void init_llvm() {\n+  LLVMInitializeNVPTXTargetInfo();\n+  LLVMInitializeNVPTXTarget();\n+  LLVMInitializeNVPTXTargetMC();\n+  LLVMInitializeNVPTXAsmPrinter();\n+}\n+\n+static bool find_and_replace(std::string &str, const std::string &begin,\n+                             const std::string &end,\n+                             const std::string &target) {\n+  size_t start_replace = str.find(begin);\n+  if (start_replace == std::string::npos)\n+    return false;\n+  size_t end_replace = str.find(end, start_replace);\n+  if (end_replace == std::string::npos)\n+    return false;\n+  str.replace(start_replace, end_replace + 1 - start_replace, target);\n+  return true;\n+}\n+\n+static std::string llir_to_ptx(llvm::Module *module, int capability, int ptx) {\n+  // LLVM version in use may not officially support target hardware\n+  int max_nvvm_cc = 75;\n+  int max_nvvm_ptx = 74;\n+  // options\n+  auto options = llvm::cl::getRegisteredOptions();\n+  auto *short_ptr =\n+      static_cast<llvm::cl::opt<bool> *>(options[\"nvptx-short-ptr\"]);\n+  assert(short_ptr);\n+  short_ptr->setValue(true);\n+  // compute capability\n+  std::string sm = \"sm_\" + std::to_string(capability);\n+  // max PTX version\n+  int ptx_major = ptx / 10;\n+  int ptx_minor = ptx % 10;\n+  // create\n+  llvm::SmallVector<char, 0> buffer;\n+  std::string triple = \"nvptx64-nvidia-cuda\";\n+  std::string proc = \"sm_\" + std::to_string(std::min(capability, max_nvvm_cc));\n+  std::string layout = \"\";\n+  std::string features = \"\";\n+  // std::string features = \"+ptx\" + std::to_string(std::min(ptx,\n+  // max_nvvm_ptx));\n+  init_llvm();\n+  // verify and store llvm\n+  llvm::legacy::PassManager pm;\n+  pm.add(llvm::createVerifierPass());\n+  pm.run(*module);\n+  // module->print(llvm::outs(), nullptr);\n+\n+  // create machine\n+  module->setTargetTriple(triple);\n+  std::string error;\n+  auto target =\n+      llvm::TargetRegistry::lookupTarget(module->getTargetTriple(), error);\n+  llvm::TargetOptions opt;\n+  opt.AllowFPOpFusion = llvm::FPOpFusion::Fast;\n+  opt.UnsafeFPMath = false;\n+  opt.NoInfsFPMath = false;\n+  opt.NoNaNsFPMath = true;\n+  llvm::TargetMachine *machine = target->createTargetMachine(\n+      module->getTargetTriple(), proc, features, opt, llvm::Reloc::PIC_,\n+      llvm::None, llvm::CodeGenOpt::Aggressive);\n+  // set data layout\n+  if (layout.empty())\n+    module->setDataLayout(machine->createDataLayout());\n+  else\n+    module->setDataLayout(layout);\n+  // emit machine code\n+  for (llvm::Function &f : module->functions())\n+    f.addFnAttr(llvm::Attribute::AlwaysInline);\n+  llvm::legacy::PassManager pass;\n+  llvm::raw_svector_ostream stream(buffer);\n+  // emit\n+  machine->addPassesToEmitFile(pass, stream, nullptr,\n+                               llvm::CodeGenFileType::CGFT_AssemblyFile);\n+  pass.run(*module);\n+\n+  // post-process\n+  std::string result(buffer.begin(), buffer.end());\n+  find_and_replace(result, \".version\", \"\\n\",\n+                   \".version \" + std::to_string(ptx_major) + \".\" +\n+                       std::to_string(ptx_minor) + \"\\n\");\n+  find_and_replace(result, \".target\", \"\\n\", \".target \" + sm + \"\\n\");\n+  while (find_and_replace(result, \"\\t// begin inline asm\", \"\\n\", \"\"))\n+    ;\n+  while (find_and_replace(result, \"\\t// end inline asm\", \"\\n\", \"\"))\n+    ;\n+  return result;\n }\n \n-std::tuple<std::string, size_t, int, std::string>\n-translateTritonGPUToPTX(mlir::ModuleOp module, uint64_t device) {\n-  int cc;\n-  int version;\n-  std::string ptxasPath;\n-  getCuCCAndVersionFromDevice(device, &cc, &version, &ptxasPath);\n-\n-  llvm::LLVMContext ctx;\n-  auto llModule = mlir::triton::translateTritonGPUToLLVMIR(&ctx, module);\n-  auto ptxCode = driver::llir_to_ptx(llModule.get(), cc, version);\n-  return std::make_tuple(ptxCode, cc, version, ptxasPath);\n+std::string translateLLVMIRToPTX(llvm::Module &module, int cc, int version) {\n+  auto ptxCode = llir_to_ptx(&module, cc, version);\n+  return ptxCode;\n }\n \n } // namespace triton"}, {"filename": "lib/driver/CMakeLists.txt", "status": "removed", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -1,5 +0,0 @@\n-add_library(TritonDriver\n-  dispatch.cc\n-  error.cc\n-  llvm.cc\n-)"}, {"filename": "lib/driver/dispatch.cc", "status": "removed", "additions": 0, "deletions": 395, "changes": 395, "file_content_changes": "@@ -1,395 +0,0 @@\n-/* Copyright 2015-2017 Philippe Tillet\n- *\n- * Permission is hereby granted, free of charge, to any person obtaining\n- * a copy of this software and associated documentation files\n- * (the \"Software\"), to deal in the Software without restriction,\n- * including without limitation the rights to use, copy, modify, merge,\n- * publish, distribute, sublicense, and/or sell copies of the Software,\n- * and to permit persons to whom the Software is furnished to do so,\n- * subject to the following conditions:\n- *\n- * The above copyright notice and this permission notice shall be\n- * included in all copies or substantial portions of the Software.\n- *\n- * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n- * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n- * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n- * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n- */\n-\n-#include \"triton/driver/dispatch.h\"\n-\n-namespace triton {\n-namespace driver {\n-\n-// Helpers for function definition\n-#define DEFINE0(init, hlib, ret, fname)                                        \\\n-  ret dispatch::fname() {                                                      \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname);              \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE1(init, hlib, ret, fname, t1)                                    \\\n-  ret dispatch::fname(t1 a) {                                                  \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a);           \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE2(init, hlib, ret, fname, t1, t2)                                \\\n-  ret dispatch::fname(t1 a, t2 b) {                                            \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b);        \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE3(init, hlib, ret, fname, t1, t2, t3)                            \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c) {                                      \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c);     \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE4(init, hlib, ret, fname, t1, t2, t3, t4)                        \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c, t4 d) {                                \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c, d);  \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE5(init, hlib, ret, fname, t1, t2, t3, t4, t5)                    \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c, t4 d, t5 e) {                          \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c, d,   \\\n-                                  e);                                          \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE6(init, hlib, ret, fname, t1, t2, t3, t4, t5, t6)                \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c, t4 d, t5 e, t6 f) {                    \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c, d,   \\\n-                                  e, f);                                       \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE7(init, hlib, ret, fname, t1, t2, t3, t4, t5, t6, t7)            \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c, t4 d, t5 e, t6 f, t7 g) {              \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c, d,   \\\n-                                  e, f, g);                                    \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE8(init, hlib, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8)        \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c, t4 d, t5 e, t6 f, t7 g, t8 h) {        \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c, d,   \\\n-                                  e, f, g, h);                                 \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE9(init, hlib, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9)    \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c, t4 d, t5 e, t6 f, t7 g, t8 h, t9 i) {  \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c, d,   \\\n-                                  e, f, g, h, i);                              \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE10(init, hlib, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9,   \\\n-                 t10)                                                          \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c, t4 d, t5 e, t6 f, t7 g, t8 h, t9 i,    \\\n-                      t10 j) {                                                 \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c, d,   \\\n-                                  e, f, g, h, i, j);                           \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE11(init, hlib, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9,   \\\n-                 t10, t11)                                                     \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c, t4 d, t5 e, t6 f, t7 g, t8 h, t9 i,    \\\n-                      t10 j, t11 k) {                                          \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c, d,   \\\n-                                  e, f, g, h, i, j, k);                        \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE13(init, hlib, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9,   \\\n-                 t10, t11, t12, t13)                                           \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c, t4 d, t5 e, t6 f, t7 g, t8 h, t9 i,    \\\n-                      t10 j, t11 k, t12 l, t13 m) {                            \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c, d,   \\\n-                                  e, f, g, h, i, j, k, l, m);                  \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-#define DEFINE19(init, hlib, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9,   \\\n-                 t10, t11, t12, t13, t14, t15, t16, t17, t18, t19)             \\\n-  ret dispatch::fname(t1 a, t2 b, t3 c, t4 d, t5 e, t6 f, t7 g, t8 h, t9 i,    \\\n-                      t10 j, t11 k, t12 l, t13 m, t14 n, t15 o, t16 p, t17 q,  \\\n-                      t18 r, t19 s) {                                          \\\n-    return f_impl<dispatch::init>(hlib, fname, fname##_, #fname, a, b, c, d,   \\\n-                                  e, f, g, h, i, j, k, l, m, n, o, p, q, r,    \\\n-                                  s);                                          \\\n-  }                                                                            \\\n-  void *dispatch::fname##_;\n-\n-/* ------------------- *\n- * CUDA\n- * ------------------- */\n-\n-bool dispatch::cuinit() {\n-  if (cuda_ == nullptr) {\n-#ifdef _WIN32\n-    cuda_ = dlopen(\"cudart64_110.dll\", RTLD_LAZY);\n-#else\n-    cuda_ = dlopen(\"libcuda.so\", RTLD_LAZY);\n-    if (!cuda_)\n-      cuda_ = dlopen(\"libcuda.so.1\", RTLD_LAZY);\n-#endif\n-    if (!cuda_)\n-      throw std::runtime_error(\"Could not find `libcuda.so`. Make sure it is \"\n-                               \"in your LD_LIBRARY_PATH.\");\n-  }\n-  if (cuda_ == nullptr)\n-    return false;\n-  CUresult (*fptr)(unsigned int);\n-  cuInit_ = dlsym(cuda_, \"cuInit\");\n-  *reinterpret_cast<void **>(&fptr) = cuInit_;\n-  CUresult res = (*fptr)(0);\n-  check(res);\n-  return true;\n-}\n-\n-#define CUDA_DEFINE1(ret, fname, t1) DEFINE1(cuinit, cuda_, ret, fname, t1)\n-#define CUDA_DEFINE2(ret, fname, t1, t2)                                       \\\n-  DEFINE2(cuinit, cuda_, ret, fname, t1, t2)\n-#define CUDA_DEFINE3(ret, fname, t1, t2, t3)                                   \\\n-  DEFINE3(cuinit, cuda_, ret, fname, t1, t2, t3)\n-#define CUDA_DEFINE4(ret, fname, t1, t2, t3, t4)                               \\\n-  DEFINE4(cuinit, cuda_, ret, fname, t1, t2, t3, t4)\n-#define CUDA_DEFINE5(ret, fname, t1, t2, t3, t4, t5)                           \\\n-  DEFINE5(cuinit, cuda_, ret, fname, t1, t2, t3, t4, t5)\n-#define CUDA_DEFINE6(ret, fname, t1, t2, t3, t4, t5, t6)                       \\\n-  DEFINE6(cuinit, cuda_, ret, fname, t1, t2, t3, t4, t5, t6)\n-#define CUDA_DEFINE7(ret, fname, t1, t2, t3, t4, t5, t6, t7)                   \\\n-  DEFINE7(cuinit, cuda_, ret, fname, t1, t2, t3, t4, t5, t6, t7)\n-#define CUDA_DEFINE8(ret, fname, t1, t2, t3, t4, t5, t6, t7, t8)               \\\n-  DEFINE8(cuinit, cuda_, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8)\n-#define CUDA_DEFINE9(ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9)           \\\n-  DEFINE9(cuinit, cuda_, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9)\n-#define CUDA_DEFINE10(ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10)     \\\n-  DEFINE10(cuinit, cuda_, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10)\n-#define CUDA_DEFINE11(ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10,     \\\n-                      t11)                                                     \\\n-  DEFINE11(cuinit, cuda_, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, \\\n-           t11)\n-\n-// context management\n-CUDA_DEFINE1(CUresult, cuCtxDestroy_v2, CUcontext)\n-CUDA_DEFINE3(CUresult, cuCtxCreate_v2, CUcontext *, unsigned int, CUdevice)\n-CUDA_DEFINE1(CUresult, cuCtxGetDevice, CUdevice *)\n-CUDA_DEFINE2(CUresult, cuCtxEnablePeerAccess, CUcontext, unsigned int)\n-CUDA_DEFINE1(CUresult, cuInit, unsigned int)\n-CUDA_DEFINE1(CUresult, cuDriverGetVersion, int *)\n-// device management\n-CUDA_DEFINE2(CUresult, cuDeviceGet, CUdevice *, int)\n-CUDA_DEFINE3(CUresult, cuDeviceGetName, char *, int, CUdevice)\n-CUDA_DEFINE3(CUresult, cuDeviceGetPCIBusId, char *, int, CUdevice)\n-CUDA_DEFINE3(CUresult, cuDeviceGetAttribute, int *, CUdevice_attribute,\n-             CUdevice)\n-CUDA_DEFINE1(CUresult, cuDeviceGetCount, int *)\n-\n-// link management\n-CUDA_DEFINE8(CUresult, cuLinkAddData_v2, CUlinkState, CUjitInputType, void *,\n-             size_t, const char *, unsigned int, CUjit_option *, void **);\n-CUDA_DEFINE4(CUresult, cuLinkCreate_v2, unsigned int, CUjit_option *, void **,\n-             CUlinkState *);\n-CUDA_DEFINE1(CUresult, cuLinkDestroy, CUlinkState);\n-CUDA_DEFINE3(CUresult, cuLinkComplete, CUlinkState, void **, size_t *);\n-// module management\n-CUDA_DEFINE4(CUresult, cuModuleGetGlobal_v2, CUdeviceptr *, size_t *, CUmodule,\n-             const char *)\n-CUDA_DEFINE2(CUresult, cuModuleLoad, CUmodule *, const char *)\n-CUDA_DEFINE1(CUresult, cuModuleUnload, CUmodule)\n-CUDA_DEFINE2(CUresult, cuModuleLoadData, CUmodule *, const void *)\n-CUDA_DEFINE5(CUresult, cuModuleLoadDataEx, CUmodule *, const void *,\n-             unsigned int, CUjit_option *, void **)\n-CUDA_DEFINE3(CUresult, cuModuleGetFunction, CUfunction *, CUmodule,\n-             const char *)\n-// stream management\n-CUDA_DEFINE2(CUresult, cuStreamCreate, CUstream *, unsigned int)\n-CUDA_DEFINE1(CUresult, cuStreamSynchronize, CUstream)\n-CUDA_DEFINE1(CUresult, cuStreamDestroy_v2, CUstream)\n-CUDA_DEFINE2(CUresult, cuStreamGetCtx, CUstream, CUcontext *)\n-CUDA_DEFINE11(CUresult, cuLaunchKernel, CUfunction, unsigned int, unsigned int,\n-              unsigned int, unsigned int, unsigned int, unsigned int,\n-              unsigned int, CUstream, void **, void **)\n-// function management\n-CUDA_DEFINE3(CUresult, cuFuncGetAttribute, int *, CUfunction_attribute,\n-             CUfunction)\n-CUDA_DEFINE3(CUresult, cuFuncSetAttribute, CUfunction, CUfunction_attribute,\n-             int)\n-CUDA_DEFINE2(CUresult, cuFuncSetCacheConfig, CUfunction, CUfunc_cache)\n-// memory management\n-CUDA_DEFINE3(CUresult, cuMemcpyDtoH_v2, void *, CUdeviceptr, size_t)\n-CUDA_DEFINE1(CUresult, cuMemFree_v2, CUdeviceptr)\n-CUDA_DEFINE4(CUresult, cuMemcpyDtoHAsync_v2, void *, CUdeviceptr, size_t,\n-             CUstream)\n-CUDA_DEFINE4(CUresult, cuMemcpyHtoDAsync_v2, CUdeviceptr, const void *, size_t,\n-             CUstream)\n-CUDA_DEFINE3(CUresult, cuMemcpyHtoD_v2, CUdeviceptr, const void *, size_t)\n-CUDA_DEFINE2(CUresult, cuMemAlloc_v2, CUdeviceptr *, size_t)\n-CUDA_DEFINE3(CUresult, cuPointerGetAttribute, void *, CUpointer_attribute,\n-             CUdeviceptr)\n-CUDA_DEFINE4(CUresult, cuMemsetD8Async, CUdeviceptr, unsigned char, size_t,\n-             CUstream)\n-// event management\n-CUDA_DEFINE2(CUresult, cuEventCreate, CUevent *, unsigned int)\n-CUDA_DEFINE3(CUresult, cuEventElapsedTime, float *, CUevent, CUevent)\n-CUDA_DEFINE2(CUresult, cuEventRecord, CUevent, CUstream)\n-CUDA_DEFINE1(CUresult, cuEventDestroy_v2, CUevent)\n-\n-/* ------------------- *\n- * NVML\n- * ------------------- */\n-bool dispatch::nvmlinit() {\n-#ifdef _WIN32\n-  if (nvml_ == nullptr)\n-    nvml_ = dlopen(\"nvml.dll\", RTLD_LAZY);\n-#else\n-  if (nvml_ == nullptr)\n-    nvml_ = dlopen(\"libnvidia-ml.so\", RTLD_LAZY);\n-#endif\n-  nvmlReturn_t (*fptr)();\n-  nvmlInit_v2_ = dlsym(nvml_, \"nvmlInit_v2\");\n-  *reinterpret_cast<void **>(&fptr) = nvmlInit_v2_;\n-  nvmlReturn_t res = (*fptr)();\n-  check(res);\n-  return res;\n-}\n-\n-#define NVML_DEFINE0(ret, fname) DEFINE0(nvmlinit, nvml_, ret, fname)\n-#define NVML_DEFINE1(ret, fname, t1) DEFINE1(nvmlinit, nvml_, ret, fname, t1)\n-#define NVML_DEFINE2(ret, fname, t1, t2)                                       \\\n-  DEFINE2(nvmlinit, nvml_, ret, fname, t1, t2)\n-#define NVML_DEFINE3(ret, fname, t1, t2, t3)                                   \\\n-  DEFINE3(nvmlinit, nvml_, ret, fname, t1, t2, t3)\n-\n-NVML_DEFINE2(nvmlReturn_t, nvmlDeviceGetHandleByPciBusId_v2, const char *,\n-             nvmlDevice_t *)\n-NVML_DEFINE3(nvmlReturn_t, nvmlDeviceGetClockInfo, nvmlDevice_t,\n-             nvmlClockType_t, unsigned int *)\n-NVML_DEFINE3(nvmlReturn_t, nvmlDeviceGetMaxClockInfo, nvmlDevice_t,\n-             nvmlClockType_t, unsigned int *)\n-NVML_DEFINE3(nvmlReturn_t, nvmlDeviceSetApplicationsClocks, nvmlDevice_t,\n-             unsigned int, unsigned int)\n-\n-/* ------------------- *\n- * HIP\n- * ------------------- */\n-bool dispatch::hipinit() {\n-  if (hip_ == nullptr)\n-    hip_ = dlopen(\"libamdhip64.so\", RTLD_LAZY);\n-  if (hip_ == nullptr)\n-    return false;\n-  hipError_t (*fptr)();\n-  hipInit_ = dlsym(hip_, \"hipInit\");\n-  *reinterpret_cast<void **>(&fptr) = hipInit_;\n-  hipError_t res = (*fptr)();\n-  check(res);\n-  return res;\n-}\n-\n-#define HIP_DEFINE1(ret, fname, t1) DEFINE1(hipinit, hip_, ret, fname, t1)\n-#define HIP_DEFINE2(ret, fname, t1, t2)                                        \\\n-  DEFINE2(hipinit, hip_, ret, fname, t1, t2)\n-#define HIP_DEFINE3(ret, fname, t1, t2, t3)                                    \\\n-  DEFINE3(hipinit, hip_, ret, fname, t1, t2, t3)\n-#define HIP_DEFINE4(ret, fname, t1, t2, t3, t4)                                \\\n-  DEFINE4(hipinit, hip_, ret, fname, t1, t2, t3, t4)\n-#define HIP_DEFINE5(ret, fname, t1, t2, t3, t4, t5)                            \\\n-  DEFINE5(hipinit, hip_, ret, fname, t1, t2, t3, t4, t5)\n-#define HIP_DEFINE6(ret, fname, t1, t2, t3, t4, t5, t6)                        \\\n-  DEFINE6(hipinit, hip_, ret, fname, t1, t2, t3, t4, t5, t6)\n-#define HIP_DEFINE7(ret, fname, t1, t2, t3, t4, t5, t6, t7)                    \\\n-  DEFINE7(hipinit, hip_, ret, fname, t1, t2, t3, t4, t5, t6, t7)\n-#define HIP_DEFINE8(ret, fname, t1, t2, t3, t4, t5, t6, t7, t8)                \\\n-  DEFINE8(hipinit, hip_, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8)\n-#define HIP_DEFINE9(ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9)            \\\n-  DEFINE9(hipinit, hip_, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9)\n-#define HIP_DEFINE10(ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10)      \\\n-  DEFINE10(hipinit, hip_, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10)\n-#define HIP_DEFINE11(ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, t11) \\\n-  DEFINE11(hipinit, hip_, ret, fname, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, \\\n-           t11)\n-\n-// context management\n-HIP_DEFINE1(hipError_t, hipCtxDestroy, hipCtx_t)\n-HIP_DEFINE3(hipError_t, hipCtxCreate, hipCtx_t *, unsigned int, hipDevice_t)\n-HIP_DEFINE1(hipError_t, hipCtxGetDevice, hipDevice_t *)\n-HIP_DEFINE1(hipError_t, hipCtxPushCurrent, hipCtx_t)\n-HIP_DEFINE1(hipError_t, hipCtxPopCurrent, hipCtx_t *)\n-HIP_DEFINE2(hipError_t, hipCtxEnablePeerAccess, hipCtx_t, unsigned int)\n-HIP_DEFINE1(hipError_t, hipInit, unsigned int)\n-HIP_DEFINE1(hipError_t, hipDriverGetVersion, int *)\n-// device management\n-HIP_DEFINE2(hipError_t, hipGetDevice, hipDevice_t *, int)\n-HIP_DEFINE3(hipError_t, hipDeviceGetName, char *, int, hipDevice_t)\n-HIP_DEFINE3(hipError_t, hipDeviceGetPCIBusId, char *, int, hipDevice_t)\n-HIP_DEFINE3(hipError_t, hipDeviceGetAttribute, int *, hipDeviceAttribute_t,\n-            hipDevice_t)\n-HIP_DEFINE1(hipError_t, hipGetDeviceCount, int *)\n-// module management\n-HIP_DEFINE4(hipError_t, hipModuleGetGlobal, hipDeviceptr_t *, size_t *,\n-            hipModule_t, const char *)\n-HIP_DEFINE2(hipError_t, hipModuleLoad, hipModule_t *, const char *)\n-HIP_DEFINE1(hipError_t, hipModuleUnload, hipModule_t)\n-HIP_DEFINE2(hipError_t, hipModuleLoadData, hipModule_t *, const void *)\n-HIP_DEFINE5(hipError_t, hipModuleLoadDataEx, hipModule_t *, const void *,\n-            unsigned int, hipJitOption *, void **)\n-HIP_DEFINE3(hipError_t, hipModuleGetFunction, hipFunction_t *, hipModule_t,\n-            const char *)\n-// stream management\n-HIP_DEFINE2(hipError_t, hipStreamCreate, hipStream_t *, unsigned int)\n-HIP_DEFINE1(hipError_t, hipStreamSynchronize, hipStream_t)\n-HIP_DEFINE1(hipError_t, hipStreamDestroy, hipStream_t)\n-HIP_DEFINE11(hipError_t, hipModuleLaunchKernel, hipFunction_t, unsigned int,\n-             unsigned int, unsigned int, unsigned int, unsigned int,\n-             unsigned int, unsigned int, hipStream_t, void **, void **)\n-// function management\n-HIP_DEFINE2(hipError_t, hipFuncGetAttributes, hipFuncAttributes *, void *)\n-HIP_DEFINE2(hipError_t, hipFuncSetCacheConfig, hipFunction_t, hipFuncCache_t)\n-// memory management\n-HIP_DEFINE3(hipError_t, hipMemcpyDtoH, void *, hipDeviceptr_t, size_t)\n-HIP_DEFINE1(hipError_t, hipFree, hipDeviceptr_t)\n-HIP_DEFINE4(hipError_t, hipMemcpyDtoHAsync, void *, hipDeviceptr_t, size_t,\n-            hipStream_t)\n-HIP_DEFINE4(hipError_t, hipMemcpyHtoDAsync, hipDeviceptr_t, const void *,\n-            size_t, hipStream_t)\n-HIP_DEFINE3(hipError_t, hipMemcpyHtoD, hipDeviceptr_t, const void *, size_t)\n-HIP_DEFINE2(hipError_t, hipMalloc, hipDeviceptr_t *, size_t)\n-HIP_DEFINE3(hipError_t, hipPointerGetAttribute, void *, CUpointer_attribute,\n-            hipDeviceptr_t)\n-HIP_DEFINE4(hipError_t, hipMemsetD8Async, hipDeviceptr_t, unsigned char, size_t,\n-            hipStream_t)\n-// event management\n-HIP_DEFINE2(hipError_t, hipEventCreate, hipEvent_t *, unsigned int)\n-HIP_DEFINE3(hipError_t, hipEventElapsedTime, float *, hipEvent_t, hipEvent_t)\n-HIP_DEFINE2(hipError_t, hipEventRecord, hipEvent_t, hipStream_t)\n-HIP_DEFINE1(hipError_t, hipEventDestroy, hipEvent_t)\n-\n-/* ------------------- *\n- * COMMON\n- * ------------------- */\n-\n-// Release\n-void dispatch::release() {\n-  if (cuda_) {\n-    dlclose(cuda_);\n-    cuda_ = nullptr;\n-  }\n-}\n-\n-void *dispatch::cuda_;\n-void *dispatch::nvml_;\n-void *dispatch::nvmlInit_v2_;\n-void *dispatch::hip_;\n-\n-} // namespace driver\n-} // namespace triton"}, {"filename": "lib/driver/error.cc", "status": "removed", "additions": 0, "deletions": 270, "changes": 270, "file_content_changes": "@@ -1,270 +0,0 @@\n-/* Copyright 2015-2017 Philippe Tillet\n- *\n- * Permission is hereby granted, free of charge, to any person obtaining\n- * a copy of this software and associated documentation files\n- * (the \"Software\"), to deal in the Software without restriction,\n- * including without limitation the rights to use, copy, modify, merge,\n- * publish, distribute, sublicense, and/or sell copies of the Software,\n- * and to permit persons to whom the Software is furnished to do so,\n- * subject to the following conditions:\n- *\n- * The above copyright notice and this permission notice shall be\n- * included in all copies or substantial portions of the Software.\n- *\n- * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n- * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n- * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n- * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n- */\n-\n-#include \"triton/driver/error.h\"\n-\n-namespace triton {\n-namespace driver {\n-\n-void check(CUresult err) {\n-  using namespace exception::cuda;\n-  switch (err) {\n-  case CUDA_SUCCESS:\n-    break;\n-  case CUDA_ERROR_INVALID_VALUE:\n-    throw invalid_value();\n-  case CUDA_ERROR_OUT_OF_MEMORY:\n-    throw out_of_memory();\n-  case CUDA_ERROR_NOT_INITIALIZED:\n-    throw not_initialized();\n-  case CUDA_ERROR_DEINITIALIZED:\n-    throw deinitialized();\n-  case CUDA_ERROR_PROFILER_DISABLED:\n-    throw profiler_disabled();\n-  case CUDA_ERROR_PROFILER_NOT_INITIALIZED:\n-    throw profiler_not_initialized();\n-  case CUDA_ERROR_PROFILER_ALREADY_STARTED:\n-    throw profiler_already_started();\n-  case CUDA_ERROR_PROFILER_ALREADY_STOPPED:\n-    throw profiler_already_stopped();\n-  case CUDA_ERROR_NO_DEVICE:\n-    throw no_device();\n-  case CUDA_ERROR_INVALID_DEVICE:\n-    throw invalid_device();\n-  case CUDA_ERROR_INVALID_IMAGE:\n-    throw invalid_image();\n-  case CUDA_ERROR_INVALID_CONTEXT:\n-    throw invalid_context();\n-  case CUDA_ERROR_CONTEXT_ALREADY_CURRENT:\n-    throw context_already_current();\n-  case CUDA_ERROR_MAP_FAILED:\n-    throw map_failed();\n-  case CUDA_ERROR_UNMAP_FAILED:\n-    throw unmap_failed();\n-  case CUDA_ERROR_ARRAY_IS_MAPPED:\n-    throw array_is_mapped();\n-  case CUDA_ERROR_ALREADY_MAPPED:\n-    throw already_mapped();\n-  case CUDA_ERROR_NO_BINARY_FOR_GPU:\n-    throw no_binary_for_gpu();\n-  case CUDA_ERROR_ALREADY_ACQUIRED:\n-    throw already_acquired();\n-  case CUDA_ERROR_NOT_MAPPED:\n-    throw not_mapped();\n-  case CUDA_ERROR_NOT_MAPPED_AS_ARRAY:\n-    throw not_mapped_as_array();\n-  case CUDA_ERROR_NOT_MAPPED_AS_POINTER:\n-    throw not_mapped_as_pointer();\n-  case CUDA_ERROR_ECC_UNCORRECTABLE:\n-    throw ecc_uncorrectable();\n-  case CUDA_ERROR_UNSUPPORTED_LIMIT:\n-    throw unsupported_limit();\n-  case CUDA_ERROR_CONTEXT_ALREADY_IN_USE:\n-    throw context_already_in_use();\n-  case CUDA_ERROR_PEER_ACCESS_UNSUPPORTED:\n-    throw peer_access_unsupported();\n-  case CUDA_ERROR_INVALID_PTX:\n-    throw invalid_ptx();\n-  case CUDA_ERROR_INVALID_GRAPHICS_CONTEXT:\n-    throw invalid_graphics_context();\n-  case CUDA_ERROR_INVALID_SOURCE:\n-    throw invalid_source();\n-  case CUDA_ERROR_FILE_NOT_FOUND:\n-    throw file_not_found();\n-  case CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND:\n-    throw shared_object_symbol_not_found();\n-  case CUDA_ERROR_SHARED_OBJECT_INIT_FAILED:\n-    throw shared_object_init_failed();\n-  case CUDA_ERROR_OPERATING_SYSTEM:\n-    throw operating_system();\n-  case CUDA_ERROR_INVALID_HANDLE:\n-    throw invalid_handle();\n-  case CUDA_ERROR_NOT_FOUND:\n-    throw not_found();\n-  case CUDA_ERROR_NOT_READY:\n-    throw not_ready();\n-  case CUDA_ERROR_ILLEGAL_ADDRESS:\n-    throw illegal_address();\n-  case CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES:\n-    throw launch_out_of_resources();\n-  case CUDA_ERROR_LAUNCH_TIMEOUT:\n-    throw launch_timeout();\n-  case CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING:\n-    throw launch_incompatible_texturing();\n-  case CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED:\n-    throw peer_access_already_enabled();\n-  case CUDA_ERROR_PEER_ACCESS_NOT_ENABLED:\n-    throw peer_access_not_enabled();\n-  case CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE:\n-    throw primary_context_active();\n-  case CUDA_ERROR_CONTEXT_IS_DESTROYED:\n-    throw context_is_destroyed();\n-  case CUDA_ERROR_ASSERT:\n-    throw assert_error();\n-  case CUDA_ERROR_TOO_MANY_PEERS:\n-    throw too_many_peers();\n-  case CUDA_ERROR_HOST_MEMORY_ALREADY_REGISTERED:\n-    throw host_memory_already_registered();\n-  case CUDA_ERROR_HOST_MEMORY_NOT_REGISTERED:\n-    throw host_memory_not_registered();\n-  case CUDA_ERROR_HARDWARE_STACK_ERROR:\n-    throw hardware_stack_error();\n-  case CUDA_ERROR_ILLEGAL_INSTRUCTION:\n-    throw illegal_instruction();\n-  case CUDA_ERROR_MISALIGNED_ADDRESS:\n-    throw misaligned_address();\n-  case CUDA_ERROR_INVALID_ADDRESS_SPACE:\n-    throw invalid_address_space();\n-  case CUDA_ERROR_INVALID_PC:\n-    throw invalid_pc();\n-  case CUDA_ERROR_LAUNCH_FAILED:\n-    throw launch_failed();\n-  case CUDA_ERROR_NOT_PERMITTED:\n-    throw not_permitted();\n-  case CUDA_ERROR_NOT_SUPPORTED:\n-    throw not_supported();\n-  case CUDA_ERROR_UNKNOWN:\n-    throw unknown();\n-  default:\n-    throw unknown();\n-  }\n-}\n-\n-void check(hipError_t error) {\n-  using namespace exception::hip;\n-  switch (error) {\n-  case hipSuccess:\n-    break;\n-  case hipErrorInvalidValue:\n-    throw invalid_value();\n-  case hipErrorMemoryAllocation:\n-    throw out_of_memory();\n-  case hipErrorNotInitialized:\n-    throw not_initialized();\n-  case hipErrorDeinitialized:\n-    throw deinitialized();\n-  case hipErrorProfilerDisabled:\n-    throw profiler_disabled();\n-  case hipErrorProfilerNotInitialized:\n-    throw profiler_not_initialized();\n-  case hipErrorProfilerAlreadyStarted:\n-    throw profiler_already_started();\n-  case hipErrorProfilerAlreadyStopped:\n-    throw profiler_already_stopped();\n-  case hipErrorNoDevice:\n-    throw no_device();\n-  case hipErrorInvalidSymbol:\n-    throw invalid_symbol();\n-  case hipErrorInvalidDevice:\n-    throw invalid_device();\n-  case hipErrorInvalidImage:\n-    throw invalid_image();\n-  case hipErrorInvalidContext:\n-    throw invalid_context();\n-  case hipErrorContextAlreadyCurrent:\n-    throw context_already_current();\n-  case hipErrorMapFailed:\n-    throw map_failed();\n-  case hipErrorUnmapFailed:\n-    throw unmap_failed();\n-  case hipErrorArrayIsMapped:\n-    throw array_is_mapped();\n-  case hipErrorAlreadyMapped:\n-    throw already_mapped();\n-  case hipErrorNoBinaryForGpu:\n-    throw no_binary_for_gpu();\n-  case hipErrorAlreadyAcquired:\n-    throw already_acquired();\n-  case hipErrorNotMapped:\n-    throw not_mapped();\n-  case hipErrorNotMappedAsArray:\n-    throw not_mapped_as_array();\n-  case hipErrorNotMappedAsPointer:\n-    throw not_mapped_as_pointer();\n-  case hipErrorECCNotCorrectable:\n-    throw ecc_uncorrectable();\n-  case hipErrorUnsupportedLimit:\n-    throw unsupported_limit();\n-  case hipErrorContextAlreadyInUse:\n-    throw context_already_in_use();\n-  case hipErrorPeerAccessUnsupported:\n-    throw peer_access_unsupported();\n-  case hipErrorInvalidKernelFile:\n-    throw invalid_ptx();\n-  case hipErrorInvalidGraphicsContext:\n-    throw invalid_graphics_context();\n-  case hipErrorInvalidSource:\n-    throw invalid_source();\n-  case hipErrorFileNotFound:\n-    throw file_not_found();\n-  case hipErrorSharedObjectSymbolNotFound:\n-    throw shared_object_symbol_not_found();\n-  case hipErrorSharedObjectInitFailed:\n-    throw shared_object_init_failed();\n-  case hipErrorOperatingSystem:\n-    throw operating_system();\n-  case hipErrorInvalidResourceHandle:\n-    throw invalid_handle();\n-  case hipErrorNotFound:\n-    throw not_found();\n-  case hipErrorNotReady:\n-    throw not_ready();\n-  case hipErrorIllegalAddress:\n-    throw illegal_address();\n-  case hipErrorLaunchOutOfResources:\n-    throw launch_out_of_resources();\n-  case hipErrorLaunchTimeOut:\n-    throw launch_timeout();\n-  // case hipErrorLaunchIncompatibleTexturing  : throw\n-  // launch_incompatible_texturing();\n-  case hipErrorPeerAccessAlreadyEnabled:\n-    throw peer_access_already_enabled();\n-  case hipErrorPeerAccessNotEnabled:\n-    throw peer_access_not_enabled();\n-  // case hipErrorPrimaryContextActive         : throw primary_context_active();\n-  // case hipErrorContextIsDestroyed           : throw context_is_destroyed();\n-  case hipErrorAssert:\n-    throw assert_error();\n-  // case hipErrorTooManyPeers                 : throw too_many_peers();\n-  case hipErrorHostMemoryAlreadyRegistered:\n-    throw host_memory_already_registered();\n-  case hipErrorHostMemoryNotRegistered:\n-    throw host_memory_not_registered();\n-  // case hipErrorHardwareStackError           : throw hardware_stack_error();\n-  // case hipErrorIllegalInstruction            : throw illegal_instruction();\n-  // case hipErrorMisalignedAddress             : throw misaligned_address();\n-  // case hipErrorInvalidAddressSpace          : throw invalid_address_space();\n-  // case hipErrorInvalidPc                     : throw invalid_pc();\n-  case hipErrorLaunchFailure:\n-    throw launch_failed();\n-  // case hipErrorNotPermitted                  : throw not_permitted();\n-  case hipErrorNotSupported:\n-    throw not_supported();\n-  case hipErrorUnknown:\n-    throw unknown();\n-  default:\n-    throw unknown();\n-  }\n-}\n-\n-} // namespace driver\n-} // namespace triton"}, {"filename": "lib/driver/llvm.cc", "status": "removed", "additions": 0, "deletions": 392, "changes": 392, "file_content_changes": "@@ -1,392 +0,0 @@\n-/* Copyright 2015-2017 Philippe Tillet\n- *\n- * Permission is hereby granted, free of charge, to any person obtaining\n- * a copy of this software and associated documentation files\n- * (the \"Software\"), to deal in the Software without restriction,\n- * including without limitation the rights to use, copy, modify, merge,\n- * publish, distribute, sublicense, and/or sell copies of the Software,\n- * and to permit persons to whom the Software is furnished to do so,\n- * subject to the following conditions:\n- *\n- * The above copyright notice and this permission notice shall be\n- * included in all copies or substantial portions of the Software.\n- *\n- * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n- * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n- * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n- * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n- */\n-#include <fstream>\n-\n-#if defined __has_include\n-#if __has_include(<unistd.h>)\n-#include <unistd.h>\n-#endif\n-#endif\n-\n-#include \"triton/driver/dispatch.h\"\n-#include \"triton/driver/error.h\"\n-#include \"triton/driver/llvm.h\"\n-#include \"triton/tools/sha1.hpp\"\n-#include \"triton/tools/sys/exec.hpp\"\n-#include \"triton/tools/sys/getenv.hpp\"\n-#include \"triton/tools/sys/mkdir.hpp\"\n-#include \"llvm/ExecutionEngine/ExecutionEngine.h\"\n-#include \"llvm/ExecutionEngine/SectionMemoryManager.h\"\n-#include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/IR/IRPrintingPasses.h\"\n-#include \"llvm/IR/LegacyPassManager.h\"\n-#include \"llvm/IR/Module.h\"\n-#include \"llvm/IR/Verifier.h\"\n-#include \"llvm/MC/TargetRegistry.h\"\n-#include \"llvm/Support/CodeGen.h\"\n-#include \"llvm/Support/CommandLine.h\"\n-#include \"llvm/Support/SourceMgr.h\"\n-#include \"llvm/Support/TargetSelect.h\"\n-#include \"llvm/Support/raw_ostream.h\"\n-#include \"llvm/Target/TargetMachine.h\"\n-#include \"llvm/Target/TargetOptions.h\"\n-#include \"llvm/Transforms/Scalar.h\"\n-#include \"llvm/Transforms/Utils/Cloning.h\"\n-#include <memory>\n-#include <regex>\n-\n-// begin AMD stuff\n-#include \"llvm/ADT/StringRef.h\"\n-#include \"llvm/Analysis/TargetLibraryInfo.h\"\n-#include \"llvm/Support/FileSystem.h\"\n-#include \"llvm/Support/FormattedStream.h\"\n-#include \"llvm/Support/Program.h\"\n-#include \"llvm/Support/ToolOutputFile.h\"\n-// end AMD stuff\n-\n-extern \"C\" {\n-int set_curterm(char *nterm) { return 0; }\n-int del_curterm(char *nterm) { return 0; }\n-int tigetnum(char *capname) { return 0; }\n-int setupterm(char *term, int fildes, int *errret) { return 0; }\n-}\n-\n-namespace triton {\n-namespace driver {\n-\n-void init_llvm() {\n-  LLVMInitializeNVPTXTargetInfo();\n-  LLVMInitializeNVPTXTarget();\n-  LLVMInitializeNVPTXTargetMC();\n-  LLVMInitializeNVPTXAsmPrinter();\n-  LLVMInitializeAMDGPUTargetInfo();\n-  LLVMInitializeAMDGPUTarget();\n-  LLVMInitializeAMDGPUTargetMC();\n-  LLVMInitializeAMDGPUAsmPrinter();\n-}\n-\n-/* ------------------------ */\n-//         CUDA             //\n-/* ------------------------ */\n-static bool find_and_replace(std::string &str, const std::string &begin,\n-                             const std::string &end,\n-                             const std::string &target) {\n-  size_t start_replace = str.find(begin);\n-  if (start_replace == std::string::npos)\n-    return false;\n-  size_t end_replace = str.find(end, start_replace);\n-  if (end_replace == std::string::npos)\n-    return false;\n-  str.replace(start_replace, end_replace + 1 - start_replace, target);\n-  return true;\n-}\n-\n-std::string path_to_ptxas(int &version) {\n-  std::vector<std::string> rets;\n-  std::string ret;\n-  // search paths for ptxas\n-  std::vector<std::string> ptxas_prefixes = {\"\", \"/usr/local/cuda/bin/\"};\n-  std::string triton_ptxas = tools::getenv(\"TRITON_PTXAS_PATH\");\n-  if (!triton_ptxas.empty())\n-    ptxas_prefixes.insert(ptxas_prefixes.begin(), triton_ptxas);\n-  // see what path for ptxas are valid\n-  std::vector<std::string> working_ptxas;\n-  for (const std::string &prefix : ptxas_prefixes) {\n-    std::string ptxas = prefix + \"ptxas\";\n-    bool works = tools::exec(ptxas + \" --version 2>&1\", ret) == 0;\n-    if (works) {\n-      working_ptxas.push_back(ptxas);\n-      rets.push_back(ret);\n-    }\n-  }\n-  // error if no working ptxas was found\n-  if (working_ptxas.empty())\n-    throw std::runtime_error(\"`ptxas` was searched in TRITON_PTXAS_PATH, \"\n-                             \"/usr/local/cuda/bin/ or PATH\"\n-                             \" but a working version could not be found.\");\n-  std::string ptxas = working_ptxas.front();\n-  // parse version\n-  std::regex version_regex(\"release (\\\\d+)\\\\.(\\\\d+)\");\n-  std::smatch match;\n-  bool found = false;\n-  // currently choosing the first ptxas. Other logics can be implemented in\n-  // future\n-  size_t i = 0;\n-  while (i < rets.size()) {\n-    if (std::regex_search(rets[i], match, version_regex)) {\n-      int major = std::stoi(match[1]);\n-      int minor = std::stoi(match[2]);\n-      version = major * 1000 + minor * 10;\n-      found = true;\n-      break;\n-    }\n-    ++i;\n-  }\n-  if (not found) {\n-    throw std::runtime_error(\"Error in parsing version\");\n-  }\n-  return working_ptxas[i];\n-}\n-\n-int vptx(int version) {\n-  if (version >= 11040)\n-    return 74;\n-  if (version >= 11030)\n-    return 73;\n-  if (version >= 11020)\n-    return 72;\n-  if (version >= 11010)\n-    return 71;\n-  if (version >= 11000)\n-    return 70;\n-  if (version >= 10020)\n-    return 65;\n-  if (version >= 10010)\n-    return 64;\n-  if (version >= 10000)\n-    return 63;\n-  throw std::runtime_error(\"Triton requires CUDA 10+\");\n-}\n-\n-std::string llir_to_ptx(llvm::Module *module, int cc, int version) {\n-  // LLVM version in use may not officially support target hardware\n-  int max_nvvm_cc = 75;\n-  int max_nvvm_ptx = 74;\n-  // options\n-  auto options = llvm::cl::getRegisteredOptions();\n-  auto *short_ptr =\n-      static_cast<llvm::cl::opt<bool> *>(options[\"nvptx-short-ptr\"]);\n-  assert(short_ptr);\n-  short_ptr->setValue(true);\n-  // compute capability\n-  std::string sm = \"sm_\" + std::to_string(cc);\n-  // max PTX version\n-  int ptx = vptx(version);\n-  int ptx_major = ptx / 10;\n-  int ptx_minor = ptx % 10;\n-  // create\n-  llvm::SmallVector<char, 0> buffer;\n-  std::string triple = \"nvptx64-nvidia-cuda\";\n-  std::string proc = \"sm_\" + std::to_string(std::min(cc, max_nvvm_cc));\n-  std::string layout = \"\";\n-  std::string features = \"\";\n-  // std::string features = \"+ptx\" + std::to_string(std::min(ptx,\n-  // max_nvvm_ptx));\n-  init_llvm();\n-  // verify and store llvm\n-  llvm::legacy::PassManager pm;\n-  pm.add(llvm::createVerifierPass());\n-  pm.run(*module);\n-  // module->print(llvm::outs(), nullptr);\n-\n-  // create machine\n-  module->setTargetTriple(triple);\n-  std::string error;\n-  auto target =\n-      llvm::TargetRegistry::lookupTarget(module->getTargetTriple(), error);\n-  llvm::TargetOptions opt;\n-  opt.AllowFPOpFusion = llvm::FPOpFusion::Fast;\n-  opt.UnsafeFPMath = false;\n-  opt.NoInfsFPMath = false;\n-  opt.NoNaNsFPMath = true;\n-  llvm::TargetMachine *machine = target->createTargetMachine(\n-      module->getTargetTriple(), proc, features, opt, llvm::Reloc::PIC_,\n-      llvm::None, llvm::CodeGenOpt::Aggressive);\n-  // set data layout\n-  if (layout.empty())\n-    module->setDataLayout(machine->createDataLayout());\n-  else\n-    module->setDataLayout(layout);\n-  // emit machine code\n-  for (llvm::Function &f : module->functions())\n-    f.addFnAttr(llvm::Attribute::AlwaysInline);\n-  llvm::legacy::PassManager pass;\n-  llvm::raw_svector_ostream stream(buffer);\n-  // emit\n-  machine->addPassesToEmitFile(pass, stream, nullptr,\n-                               llvm::CodeGenFileType::CGFT_AssemblyFile);\n-  pass.run(*module);\n-\n-  // post-process\n-  std::string result(buffer.begin(), buffer.end());\n-  find_and_replace(result, \".version\", \"\\n\",\n-                   \".version \" + std::to_string(ptx_major) + \".\" +\n-                       std::to_string(ptx_minor) + \"\\n\");\n-  find_and_replace(result, \".target\", \"\\n\", \".target \" + sm + \"\\n\");\n-  while (find_and_replace(result, \"\\t// begin inline asm\", \"\\n\", \"\"))\n-    ;\n-  while (find_and_replace(result, \"\\t// end inline asm\", \"\\n\", \"\"))\n-    ;\n-  return result;\n-}\n-\n-std::string ptx_to_cubin(const std::string &ptx, const std::string &ptxas,\n-                         int cc) {\n-  // compile ptx with ptxas\n-  char _fsrc[L_tmpnam];\n-  char _flog[L_tmpnam];\n-  std::tmpnam(_fsrc);\n-  std::tmpnam(_flog);\n-  std::string fsrc = _fsrc;\n-  std::string flog = _flog;\n-  std::string fbin = fsrc + \".o\";\n-  const char *_fbin = fbin.c_str();\n-  std::ofstream ofs(fsrc);\n-  ofs << ptx << std::endl;\n-  ofs.close();\n-  std::string cmd;\n-  int err;\n-  cmd = ptxas + \" -v --gpu-name=sm_\" + std::to_string(cc) + \" \" + fsrc +\n-        \" -o \" + fsrc + \".o 2> \" + flog;\n-  err = system(cmd.c_str());\n-  if (err != 0) {\n-    std::ifstream _log(_flog);\n-    std::string log(std::istreambuf_iterator<char>(_log), {});\n-    unlink(_fsrc);\n-    unlink(_flog);\n-    throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" + log);\n-  }\n-  CUmodule ret;\n-  std::ifstream _cubin(_fbin, std::ios::binary);\n-  std::string cubin(std::istreambuf_iterator<char>(_cubin), {});\n-  _cubin.close();\n-  unlink(_fsrc);\n-  unlink(_flog);\n-  unlink(_fbin);\n-  dispatch::cuModuleLoadData(&ret, cubin.c_str());\n-  return cubin;\n-}\n-\n-/* ------------------------ */\n-//         HIP              //\n-/* ------------------------ */\n-\n-std::string llir_to_amdgpu(llvm::Module *module, const std::string &_proc) {\n-  init_llvm();\n-\n-  //  proc = std::get<0>(GetFeatureStrFromGCNArchName(rocminfo));\n-  //  features = std::get<1>(GetFeatureStrFromGCNArchName(rocminfo));\n-\n-  // create\n-  llvm::SmallVector<char, 0> buffer;\n-  std::string triple = \"amdgcn-amd-amdhsa\";\n-  std::string layout = \"\";\n-  std::string features;\n-  std::string proc = \"gfx908\";\n-  // verify and store llvm\n-  llvm::legacy::PassManager pm;\n-  pm.add(llvm::createVerifierPass());\n-  pm.run(*module);\n-  // create machine\n-  module->setTargetTriple(triple);\n-  std::string error;\n-  auto target =\n-      llvm::TargetRegistry::lookupTarget(module->getTargetTriple(), error);\n-  llvm::TargetOptions opt;\n-  opt.AllowFPOpFusion = llvm::FPOpFusion::Fast;\n-  opt.UnsafeFPMath = false;\n-  opt.NoInfsFPMath = false;\n-  opt.NoNaNsFPMath = true;\n-  llvm::TargetMachine *machine = target->createTargetMachine(\n-      module->getTargetTriple(), proc, features, opt, llvm::Reloc::PIC_,\n-      llvm::None, llvm::CodeGenOpt::Aggressive);\n-  // set data layout\n-  if (layout.empty())\n-    module->setDataLayout(machine->createDataLayout());\n-  else\n-    module->setDataLayout(layout);\n-  // emit machine code\n-  for (llvm::Function &f : module->functions())\n-    f.addFnAttr(llvm::Attribute::AlwaysInline);\n-  llvm::legacy::PassManager pass;\n-  llvm::raw_svector_ostream stream(buffer);\n-\n-  // create dump files\n-  std::string module_name = module->getModuleIdentifier();\n-  std::error_code ec;\n-\n-  // Save GCN ISA binary.\n-  std::string isabin_path =\n-      std::string(\"/tmp/\") + module_name + std::string(\".o\");\n-  std::unique_ptr<llvm::raw_fd_ostream> isabin_fs(\n-      new llvm::raw_fd_ostream(isabin_path, ec, llvm::sys::fs::OF_Text));\n-  if (ec) {\n-    std::cout << isabin_path << \" was not created. error code: \" << ec\n-              << std::endl;\n-  }\n-\n-  // emit\n-  machine->addPassesToEmitFile(pass, *isabin_fs, nullptr,\n-                               llvm::CGFT_ObjectFile);\n-  pass.run(*module);\n-  // Save GCN ISA.\n-  std::string amdgcn_path =\n-      std::string(\"/tmp/\") + module_name + std::string(\".gcn\");\n-  std::string result(buffer.begin(), buffer.end());\n-  std::ofstream amdgcn(amdgcn_path);\n-  amdgcn << result;\n-  amdgcn.close();\n-\n-  // generate HASCO file\n-  std::string hsaco_path =\n-      std::string(\"/tmp/\") + module_name + std::string(\".hsaco\");\n-  std::string error_message;\n-  int lld_result =\n-      llvm::sys::ExecuteAndWait(\"/opt/rocm/llvm/bin/ld.lld\",\n-                                {\"/opt/rocm/llvm/bin/ld.lld\", \"-flavor\", \"gnu\",\n-                                 \"-shared\", \"-o\", hsaco_path, isabin_path},\n-                                llvm::None, {}, 0, 0, &error_message);\n-  if (lld_result) {\n-    std::cout << \"ld.lld execute fail: \" << std::endl;\n-    std::cout << error_message << std::endl;\n-    std::cout << lld_result << std::endl;\n-  }\n-\n-  return hsaco_path;\n-}\n-\n-hipModule_t amdgpu_to_hipmodule(const std::string &path) {\n-  // Read HSACO.\n-  std::ifstream hsaco_file(path, std::ios::binary | std::ios::ate);\n-  std::ifstream::pos_type hsaco_file_size = hsaco_file.tellg();\n-\n-  std::vector<unsigned char> hsaco(hsaco_file_size);\n-  hsaco_file.seekg(0, std::ios::beg);\n-  hsaco_file.read(reinterpret_cast<char *>(&hsaco[0]), hsaco_file_size);\n-  hsaco_file.close();\n-  hipJitOption opt[] = {hipJitOptionErrorLogBufferSizeBytes,\n-                        hipJitOptionErrorLogBuffer,\n-                        hipJitOptionInfoLogBufferSizeBytes,\n-                        hipJitOptionInfoLogBuffer, hipJitOptionLogVerbose};\n-  const unsigned int errbufsize = 8192;\n-  const unsigned int logbufsize = 8192;\n-  char _err[errbufsize];\n-  char _log[logbufsize];\n-  void *optval[] = {(void *)(uintptr_t)errbufsize, (void *)_err,\n-                    (void *)(uintptr_t)logbufsize, (void *)_log, (void *)1};\n-  hipModule_t ret;\n-  dispatch::hipModuleLoadDataEx(&ret, hsaco.data(), 5, opt, optval);\n-  return ret;\n-}\n-\n-} // namespace driver\n-} // namespace triton"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 109, "deletions": 559, "changes": 668, "file_content_changes": "@@ -1,7 +1,4 @@\n-\ufeff#include \"triton/driver/error.h\"\n-#include \"triton/driver/llvm.h\"\n-\n-#include \"mlir/IR/Builders.h\"\n+\ufeff#include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/Verifier.h\"\n@@ -10,6 +7,9 @@\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Transforms/Passes.h\"\n \n+#include \"mlir/Parser.h\"\n+#include \"mlir/Support/FileUtilities.h\"\n+\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPU.h\"\n@@ -24,10 +24,14 @@\n #include \"llvm/IR/LegacyPassManager.h\"\n #include \"llvm/IR/Module.h\"\n #include \"llvm/IR/Verifier.h\"\n+#include \"llvm/IRReader/IRReader.h\"\n #include \"llvm/Support/raw_ostream.h\"\n \n+#include \"llvm/Support/SourceMgr.h\"\n+\n #include <Python.h>\n #include <cctype>\n+#include <fstream>\n #include <optional>\n #include <pybind11/buffer_info.h>\n #include <pybind11/functional.h>\n@@ -40,510 +44,22 @@\n #include <string>\n \n namespace py = pybind11;\n-// namespace ir = triton::ir;\n-namespace drv = triton::driver;\n-\n-using triton::cuGetInfo;\n \n enum backend_t {\n   HOST,\n   CUDA,\n   ROCM,\n };\n \n-void cu_enable_peer_access(uint64_t peer_ptr) {\n-  CUcontext context;\n-  drv::dispatch::cuPointerGetAttribute(&context, CU_POINTER_ATTRIBUTE_CONTEXT,\n-                                       peer_ptr);\n-  try {\n-    drv::dispatch::cuCtxEnablePeerAccess(context, 0);\n-  } catch (drv::exception::cuda::peer_access_already_enabled) {\n-  }\n-}\n-\n-void host_enqueue(uint64_t stream, uint64_t kernel, uint64_t grid_0,\n-                  uint64_t grid_1, uint64_t grid_2, uint64_t block_0,\n-                  uint64_t block_1, uint64_t block_2, void *args_ptr,\n-                  size_t args_size, int64_t shared_mem) {\n-  throw std::runtime_error(\"unsupported\");\n-  // auto hst = kernel->module()->hst();\n-  // hst_->futures->reserve(hst_->futures->size() + grid[0]*grid[1]*grid[2]);\n-  // char* params = new char[args_size];\n-  // std::memcpy((void*)params, (void*)args, args_size);\n-  // for(size_t i = 0; i < grid[0]; i++)\n-  //   for(size_t j = 0; j < grid[1]; j++)\n-  //     for(size_t k = 0; k < grid[2]; k++)\n-  //       hst_->futures->emplace_back(hst_->pool->enqueue(hst->fn,\n-  //       (char**)params, int32_t(i), int32_t(j), int32_t(k)));\n-}\n-\n-void cu_enqueue(uint64_t stream, uint64_t kernel, uint64_t grid_0,\n-                uint64_t grid_1, uint64_t grid_2, uint64_t block_0,\n-                uint64_t block_1, uint64_t block_2, void *args_ptr,\n-                size_t args_size, int64_t shared_mem) {\n-  void *config[] = {CU_LAUNCH_PARAM_BUFFER_POINTER, (void *)args_ptr,\n-                    CU_LAUNCH_PARAM_BUFFER_SIZE, &args_size,\n-                    CU_LAUNCH_PARAM_END};\n-  drv::dispatch::cuLaunchKernel((CUfunction)kernel, grid_0, grid_1, grid_2,\n-                                block_0, block_1, block_2, shared_mem,\n-                                (CUstream)stream, nullptr, config);\n-}\n-\n-long pow2_divisor(long N) {\n-  if (N % 16 == 0)\n-    return 16;\n-  if (N % 8 == 0)\n-    return 8;\n-  if (N % 4 == 0)\n-    return 4;\n-  if (N % 2 == 0)\n-    return 2;\n-  return 1;\n-}\n-\n-// Returns something like \"int16\", whether dtype is a torch.dtype or\n-// triton.language.dtype.\n-std::string dtype_cache_key_part(const py::object &dtype) {\n-  if (py::hasattr(dtype, \"cache_key_part\")) {\n-    // Presumed to be a triton.language.dtype.\n-    return std::string(py::str(py::getattr(dtype, \"cache_key_part\")));\n-  } else {\n-    // Remove 'torch.' prefix from repr of torch.dtype.\n-    py::object repr = py::repr(dtype);\n-    size_t repr_len = PyUnicode_GET_LENGTH(repr.ptr());\n-    const char *repr_ptr = (const char *)PyUnicode_1BYTE_DATA(repr.ptr());\n-    if (repr_len <= 6 || strncmp(repr_ptr, \"torch.\", 6)) {\n-      throw std::logic_error(\"invalid dtype: \" +\n-                             std::string(repr_ptr, repr_len));\n-    }\n-    return std::string(repr_ptr + 6, repr_len - 6);\n-  }\n-}\n-\n-size_t get_pointer_range_size(uint64_t addr) {\n-  if (addr == 0)\n-    return 0;\n-  size_t size;\n-  drv::dispatch::cuPointerGetAttribute(&size, CU_POINTER_ATTRIBUTE_RANGE_SIZE,\n-                                       (CUdeviceptr)addr);\n-  return size;\n-}\n-\n-// Launch\n-void parse_args(py::list &args, py::list do_not_specialize,\n-                const std::string &func_key, py::list &arg_names,\n-                std::string &cache_key, std::string &params,\n-                size_t &params_size, py::dict constants, int num_warps,\n-                int num_stages) {\n-  size_t len = PyList_Size(args.ptr());\n-  params.reserve(8 * len); // 8 max bytes by argument\n-  char *params_ptr = &params[0];\n-  cache_key = func_key;\n-  cache_key += \"-\" + std::to_string(num_warps);\n-  cache_key += \"-\" + std::to_string(num_stages);\n-  cache_key += \"-\";\n-  for (int i = 0; i < len; i++) {\n-    cache_key += \"_\";\n-    py::int_ py_i = py::int_(i);\n-    bool specialize = !do_not_specialize.contains(py_i);\n-    py::object arg = args[i];\n-    auto arg_ptr = arg.ptr();\n-\n-    // argument is `long`\n-    if (PyLong_Check(arg_ptr)) {\n-      int overflow;\n-      long long value = PyLong_AsLongLongAndOverflow(arg_ptr, &overflow);\n-      // values equal to 1 are specialized\n-      if (specialize && (value == 1)) {\n-        cache_key += \"1\";\n-        continue;\n-      }\n-      // int32, uint32, int64, and uint64 have different kernels\n-      if (!overflow && -0x8000'0000LL <= value && value <= 0x7FFF'FFFFLL) {\n-        cache_key += \"int32\";\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 3) & (-4));\n-        std::memcpy(params_ptr, &value, 4);\n-        params_ptr += 4;\n-      } else if (!overflow && 0x8000'0000LL <= value &&\n-                 value <= 0xFFFF'FFFFLL) {\n-        cache_key += \"uint32\";\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 3) & (-4));\n-        std::memcpy(params_ptr, &value, 4);\n-        params_ptr += 4;\n-      } else if (!overflow) {\n-        cache_key += \"int64\";\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 7) & (-8));\n-        std::memcpy(params_ptr, &value, 8);\n-        params_ptr += 8;\n-      } else {\n-        if (PyErr_Occurred()) {\n-          throw std::logic_error(\"An error occurred?\");\n-        }\n-        unsigned long long unsigned_value = PyLong_AsUnsignedLongLong(arg_ptr);\n-        if (PyErr_Occurred()) {\n-          throw std::runtime_error(\"integer overflow in argument: \" +\n-                                   std::string(py::str(arg)));\n-        }\n-        cache_key += \"uint64\";\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 7) & (-8));\n-        std::memcpy(params_ptr, &unsigned_value, 8);\n-        params_ptr += 8;\n-      }\n-      if (!specialize)\n-        continue;\n-      // values divisible by small powers of 2 are specialized\n-      cache_key += \"[multipleof(\";\n-      cache_key += std::to_string(pow2_divisor(value));\n-      cache_key += \")]\";\n-      continue;\n-    }\n-    // argument is `float`\n-    if (PyFloat_Check(arg_ptr)) {\n-      cache_key += \"float32\";\n-      float value = PyFloat_AsDouble(arg_ptr);\n-      params_ptr = (char *)(((uintptr_t)params_ptr + 3) & (-4));\n-      std::memcpy(params_ptr, &value, 4);\n-      params_ptr += 4;\n-      continue;\n-    }\n-    // argument is `bool`\n-    if (PyBool_Check(arg_ptr)) {\n-      cache_key += \"bool\";\n-      bool value = arg_ptr == Py_True ? true : false;\n-      std::memcpy(params_ptr, &value, 1);\n-      params_ptr += 1;\n-      continue;\n-    }\n-    // argument is tensor\n-    if (py::hasattr(arg, \"data_ptr\")) {\n-      py::object data_ptr = arg.attr(\"data_ptr\")();\n-      long value = data_ptr.cast<long>();\n-      params_ptr = (char *)(((uintptr_t)params_ptr + 7) & (-8));\n-      // copy param\n-      std::memcpy(params_ptr, &value, 8);\n-      params_ptr += 8;\n-      // update cache key\n-      cache_key += dtype_cache_key_part(arg.attr(\"dtype\"));\n-      cache_key += \"*\";\n-      cache_key += \"[multipleof(\";\n-      size_t range_size = get_pointer_range_size(value);\n-      cache_key += std::to_string(\n-          std::min(pow2_divisor(value), pow2_divisor(range_size)));\n-      cache_key += \")]\";\n-      continue;\n-    }\n-    // argument is `constexpr`\n-    if (py::hasattr(arg, \"value\")) {\n-      py::object value = arg.attr(\"value\");\n-      py::object name = arg_names[i];\n-      constants[name] = value;\n-      py::object repr = py::repr(value);\n-      const char *start = (const char *)PyUnicode_1BYTE_DATA(repr.ptr());\n-      size_t len = PyUnicode_GET_LENGTH(repr.ptr());\n-      cache_key += std::string(start, len);\n-      continue;\n-    }\n-    std::string ty_str =\n-        arg.attr(\"__class__\").attr(\"__name__\").cast<std::string>();\n-    if (ty_str == \"NoneType\") {\n-      cache_key += \"None\";\n-      continue;\n-    }\n-    std::string err_msg = \"Received type '\" + ty_str + \"' for argument \" +\n-                          std::to_string(i) + \".\" +\n-                          \" Only int, float, bool, torch.Tensor, and \"\n-                          \"triton.language.constexpr are supported.\";\n-    throw std::runtime_error(err_msg);\n-  }\n-  params_size = (std::ptrdiff_t)(params_ptr - &params[0]);\n-}\n-\n-void parse_args(py::list &args, py::list &arg_names, std::string &params,\n-                size_t &params_size, py::dict constants) {\n-  size_t len = PyList_Size(args.ptr());\n-  params.reserve(8 * len); // 8 max bytes by argument\n-  char *params_ptr = params.data();\n-  for (int i = 0; i < len; i++) {\n-    py::object arg = args[i];\n-    auto arg_ptr = arg.ptr();\n-\n-    if (PyLong_Check(arg_ptr)) {\n-      int overflow{};\n-      long long value = PyLong_AsLongLongAndOverflow(arg_ptr, &overflow);\n-\n-      if (!overflow && -0x8000'0000LL <= value && value <= 0x7FFF'FFFFLL) {\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 3) & (-4));\n-        std::memcpy(params_ptr, &value, 4);\n-        params_ptr += 4;\n-      } else if (!overflow && 0x8000'0000LL <= value &&\n-                 value <= 0xFFFF'FFFFLL) {\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 3) & (-4));\n-        std::memcpy(params_ptr, &value, 4);\n-        params_ptr += 4;\n-      } else if (!overflow) {\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 7) & (-8));\n-        std::memcpy(params_ptr, &value, 8);\n-        params_ptr += 8;\n-      } else {\n-        if (PyErr_Occurred()) {\n-          throw std::logic_error(\"An error occurred?\");\n-        }\n-        unsigned long long unsigned_value = PyLong_AsUnsignedLongLong(arg_ptr);\n-        if (PyErr_Occurred()) {\n-          throw std::runtime_error(\"integer overflow in argument: \" +\n-                                   std::string(py::str(arg)));\n-        }\n-        params_ptr = (char *)(((uintptr_t)params_ptr + 7) & (-8));\n-        std::memcpy(params_ptr, &unsigned_value, 8);\n-        params_ptr += 8;\n-      }\n-      continue;\n-    }\n-\n-    if (PyFloat_Check(arg_ptr)) {\n-      float value = PyFloat_AsDouble(arg_ptr);\n-      params_ptr = (char *)(((uintptr_t)params_ptr + 3) & (-4));\n-      std::memcpy(params_ptr, &value, 4);\n-      params_ptr += 4;\n-      continue;\n-    }\n-\n-    // argument is `bool`\n-    if (PyBool_Check(arg_ptr)) {\n-      bool value = arg_ptr == Py_True ? true : false;\n-      std::memcpy(params_ptr, &value, 1);\n-      params_ptr += 1;\n-      continue;\n-    }\n-    // argument is torch.tensor, get data_ptr as memory address\n-    if (py::hasattr(arg, \"data_ptr\")) {\n-      py::object data_ptr = arg.attr(\"data_ptr\")();\n-      long value = data_ptr.cast<long>();\n-      params_ptr = (char *)(((uintptr_t)params_ptr + 7) & (-8));\n-      // copy param\n-      std::memcpy(params_ptr, &value, 8);\n-      params_ptr += 8;\n-      // update cache key\n-      continue;\n-    }\n-    // argument is `constexpr`\n-    if (py::hasattr(arg, \"value\")) {\n-      py::object value = arg.attr(\"value\");\n-      py::object name = arg_names[i];\n-      constants[name] = value;\n-      continue;\n-    }\n-    // argument is `LoadedBinary`\n-    if (py::hasattr(arg, \"get_sass\")) {\n-      // Do nothing, just a placeholder here to indicate validity.\n-      continue;\n-    }\n-\n-    std::string ty_str =\n-        arg.attr(\"__class__\").attr(\"__name__\").cast<std::string>();\n-    std::string err_msg = \"Received type '\" + ty_str + \"' for argument \" +\n-                          std::to_string(i) + \".\" +\n-                          \" Only int, float, bool, torch.Tensor, and \"\n-                          \"triton.language.constexpr are supported.\";\n-    throw std::runtime_error(err_msg);\n-  }\n-\n-  params_size = (std::ptrdiff_t)(params_ptr - &params[0]);\n-}\n-\n void init_triton_runtime(py::module &&m) {\n   // wrap backend_t\n   py::enum_<backend_t>(m, \"backend\")\n       .value(\"HOST\", HOST)\n       .value(\"CUDA\", CUDA)\n       // .value(\"ROCM\", ROCM)\n       .export_values();\n-\n-  // enable peer-to-peer\n-  m.def(\"enable_peer_access\", [](backend_t backend, uint64_t peer_ptr) {\n-    if (backend != CUDA)\n-      throw std::runtime_error(\"P2P only supported on CUDA devices!\");\n-    cu_enable_peer_access(peer_ptr);\n-  });\n-\n-  // get range size for the given pointer\n-  m.def(\"get_pointer_range_size\", &get_pointer_range_size);\n-\n-  // cache key\n-  m.def(\"launch\", [](py::list args, py::list do_not_specialize,\n-                     const std::string &func_key, py::list &arg_names,\n-                     py::object device, py::int_ stream, py::dict bin_cache,\n-                     py::int_ num_warps, py::int_ num_stages,\n-                     py::function add_to_cache, py::object grid) {\n-    // parse arguments to compute cache key, compile-time constants and packed\n-    // kernel arguments\n-    long _num_warps = PyLong_AsLong(num_warps.ptr());\n-    long _num_stages = PyLong_AsLong(num_stages.ptr());\n-    std::string cache_key;\n-    std::string params;\n-    size_t params_size;\n-    py::dict constants;\n-    parse_args(args, do_not_specialize, func_key, arg_names, cache_key, params,\n-               params_size, constants, _num_warps, _num_stages);\n-\n-    // get cached binary\n-    py::str key(cache_key);\n-    py::bool_ noop = false;\n-    if (!bin_cache.contains(key)) {\n-      noop = add_to_cache(key, args, device, num_warps, num_stages);\n-    }\n-    if (noop)\n-      return (py::object)py::none();\n-    py::object bin = bin_cache[key];\n-\n-    // get grid\n-    py::sequence seq;\n-    if (!PySequence_Check(grid.ptr()))\n-      seq = grid(constants);\n-    else\n-      seq = grid;\n-    int size = seq.size();\n-    int grid_0 = py::cast<int>(seq[0]);\n-    int grid_1 = size < 2 ? 1 : py::cast<int>(seq[1]);\n-    int grid_2 = size < 3 ? 1 : py::cast<int>(seq[2]);\n-\n-    // enqueue\n-    uint64_t kernel = py::cast<uint64_t>(bin.attr(\"kernel\"));\n-    uint64_t shared_mem = py::cast<uint64_t>(bin.attr(\"shared_mem\"));\n-\n-    // actually launch\n-    void *config[] = {CU_LAUNCH_PARAM_BUFFER_POINTER, params.data(),\n-                      CU_LAUNCH_PARAM_BUFFER_SIZE, &params_size,\n-                      CU_LAUNCH_PARAM_END};\n-    uint64_t _stream = PyLong_AsLong(stream.ptr());\n-    if (grid_0 * grid_1 * grid_2 > 0) {\n-      // release the gil in case the enqueue blocks\n-      // cuda will block if too many ops are enqueued\n-      py::gil_scoped_release allow_threads;\n-      drv::dispatch::cuLaunchKernel((CUfunction)kernel, grid_0, grid_1, grid_2,\n-                                    _num_warps * 32, 1, 1, shared_mem,\n-                                    (CUstream)_stream, nullptr, config);\n-    }\n-    return bin;\n-  });\n-\n-  m.def(\"cc\", [](backend_t backend, uint64_t device) -> int {\n-    if (backend == CUDA) {\n-      CUdevice dev = (CUdevice)device;\n-      int major = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR>(dev);\n-      int minor = cuGetInfo<CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR>(dev);\n-      return major * 10 + minor;\n-    }\n-    return -1;\n-  });\n-\n-  m.def(\"launch_binary\", [](py::object binary, py::list args,\n-                            py::list do_not_specialize, py::list arg_names,\n-                            py::int_ stream, py::int_ num_warps,\n-                            py::int_ num_stages, py::object grid) {\n-    long _num_warps = PyLong_AsLong(num_warps.ptr());\n-    long _num_stages = PyLong_AsLong(num_stages.ptr());\n-\n-    // get grid\n-    py::sequence seq;\n-    py::dict constants;\n-    std::string params;\n-    size_t params_size{};\n-    parse_args(args, arg_names, params, params_size, constants);\n-    if (!PySequence_Check(grid.ptr()))\n-      seq = grid(constants);\n-    else\n-      seq = grid;\n-\n-    int size = seq.size();\n-    int grid_0 = py::cast<int>(seq[0]);\n-    int grid_1 = size < 2 ? 1 : py::cast<int>(seq[1]);\n-    int grid_2 = size < 3 ? 1 : py::cast<int>(seq[2]);\n-\n-    uint64_t kernel = py::cast<uint64_t>(binary.attr(\"kernel\"));\n-    uint64_t shared_mem = py::cast<uint64_t>(binary.attr(\"shared_mem\"));\n-\n-    // actually launch\n-    void *config[] = {CU_LAUNCH_PARAM_BUFFER_POINTER, params.data(),\n-                      CU_LAUNCH_PARAM_BUFFER_SIZE, &params_size,\n-                      CU_LAUNCH_PARAM_END};\n-    uint64_t _stream = PyLong_AsLong(stream.ptr());\n-    const int numGrids = grid_0 * grid_1 * grid_2;\n-    if (numGrids) {\n-      // release the gil in case the enqueue blocks\n-      // cuda will block if too many ops are enqueued\n-      py::gil_scoped_release allow_threads;\n-      drv::dispatch::cuLaunchKernel((CUfunction)kernel, grid_0, grid_1, grid_2,\n-                                    _num_warps * 32, 1, 1, shared_mem,\n-                                    (CUstream)_stream, nullptr, config);\n-    }\n-    return binary;\n-  });\n-\n-  // query maximum shared memory\n-  m.def(\"max_shared_memory\", [](backend_t backend, uint64_t device) {\n-    if (backend == HOST)\n-      return 0;\n-    if (backend == CUDA)\n-      return cuGetInfo<CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN>(\n-          device);\n-    return -1;\n-  });\n-\n-  // query DRAM & L2 cache\n-  m.def(\"memory_clock_rate\", [](backend_t backend, uint64_t device) {\n-    if (backend == CUDA)\n-      return cuGetInfo<CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE>(device);\n-    return -1;\n-  });\n-  m.def(\"global_memory_bus_width\", [](backend_t backend, uint64_t device) {\n-    if (backend == CUDA)\n-      return cuGetInfo<CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH>(device);\n-    return -1;\n-  });\n-  m.def(\"l2_cache_size\", [](backend_t backend, uint64_t device) {\n-    if (backend == CUDA)\n-      return cuGetInfo<CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE>(device);\n-    return -1;\n-  });\n-\n-  // query clock rate (in kilohertz)\n-  m.def(\"clock_rate\", [](backend_t backend, uint64_t device) {\n-    if (backend == CUDA)\n-      return cuGetInfo<CU_DEVICE_ATTRIBUTE_CLOCK_RATE>(device);\n-    return -1;\n-  });\n-\n-  m.def(\"num_sm\", [](backend_t backend, uint64_t device) {\n-    if (backend == CUDA)\n-      return cuGetInfo<CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT>(device);\n-    return -1;\n-  });\n-\n-  // enqueue\n-  m.def(\"enqueue\",\n-        [](backend_t backend, uint64_t stream, uint64_t kernel, uint64_t grid_0,\n-           uint64_t grid_1, uint64_t grid_2, uint64_t block_0, uint64_t block_1,\n-           uint64_t block_2, const std::string &args, int64_t shared_mem) {\n-          void *args_ptr = (void *)args.data();\n-          size_t args_size = args.size();\n-          // release the gil in case the enqueue blocks\n-          // cuda will block if too many ops are enqueued\n-          py::gil_scoped_release allow_threads;\n-          if (backend == HOST)\n-            host_enqueue(stream, kernel, grid_0, grid_1, grid_2, block_0,\n-                         block_1, block_2, args_ptr, args_size, shared_mem);\n-          if (backend == CUDA)\n-            cu_enqueue(stream, kernel, grid_0, grid_1, grid_2, block_0, block_1,\n-                       block_2, args_ptr, args_size, shared_mem);\n-        });\n }\n \n-/*****************************************************************************/\n-/* Python bindings for triton::codegen                                       */\n-/*****************************************************************************/\n-typedef std::map<std::string, py::object> asm_map_t;\n-\n /*****************************************************************************/\n /* Python bindings for triton::ir                                            */\n /*****************************************************************************/\n@@ -783,6 +299,38 @@ void init_triton_ir(py::module &&m) {\n              return self.lookupSymbol<mlir::FuncOp>(funcName);\n            });\n \n+  m.def(\n+      \"parse_mlir_module\",\n+      [](const std::string &inputFilename, mlir::MLIRContext &context) {\n+        // open file\n+        std::string errorMessage;\n+        auto input = mlir::openInputFile(inputFilename, &errorMessage);\n+        if (!input)\n+          throw std::runtime_error(errorMessage);\n+\n+        // initialize registry\n+        mlir::DialectRegistry registry;\n+        registry.insert<mlir::triton::TritonDialect,\n+                        mlir::triton::gpu::TritonGPUDialect,\n+                        mlir::math::MathDialect, mlir::arith::ArithmeticDialect,\n+                        mlir::StandardOpsDialect, mlir::scf::SCFDialect>();\n+\n+        context.appendDialectRegistry(registry);\n+        context.loadAllAvailableDialects();\n+        context.allowUnregisteredDialects();\n+\n+        // parse module\n+        llvm::SourceMgr sourceMgr;\n+        sourceMgr.AddNewSourceBuffer(std::move(input), llvm::SMLoc());\n+        mlir::OwningOpRef<mlir::ModuleOp> module(\n+            mlir::parseSourceFile(sourceMgr, &context));\n+        if (!module)\n+          throw std::runtime_error(\"Parse MLIR file failed.\");\n+\n+        return module->clone();\n+      },\n+      ret::take_ownership);\n+\n   py::class_<mlir::FuncOp, mlir::OpState>(m, \"function\")\n       // .def_property_readonly(\"attrs\", &ir::function::attrs)\n       // .def(\"add_attr\", &ir::function::add_attr);\n@@ -1643,84 +1191,86 @@ void init_triton_ir(py::module &&m) {\n }\n \n void init_triton_translation(py::module &m) {\n-  m.def(\"translate_triton_gpu_to_llvmir\", [](mlir::ModuleOp op) -> std::string {\n-    llvm::LLVMContext llvmContext;\n-    auto llvmModule =\n-        ::mlir::triton::translateTritonGPUToLLVMIR(&llvmContext, op);\n \n-    std::string str;\n-    llvm::raw_string_ostream os(str);\n-    llvmModule->print(os, nullptr);\n-    os.flush();\n-    return str;\n-  });\n+  using ret = py::return_value_policy;\n \n-  m.def(\"translate_triton_gpu_to_ptx\",\n-        [](mlir::ModuleOp module, uint64_t device)\n-            -> std::tuple<std::string /*ptx code*/, size_t /*shem size*/> {\n-          auto [ptxCode, cc, version, ptxasPath] =\n-              triton::translateTritonGPUToPTX(module, device);\n+  m.def(\"get_shared_memory_size\", [](mlir::ModuleOp module) {\n+    auto pass = std::make_unique<mlir::Allocation>(module);\n+    return pass->getSharedMemorySize();\n+  });\n \n-          mlir::PassManager pm(module->getContext());\n-          auto pass = std::make_unique<mlir::Allocation>(module);\n-          size_t size = pass->getSharedMemorySize();\n+  m.def(\n+      \"translate_triton_gpu_to_llvmir\",\n+      [](mlir::ModuleOp op) {\n+        llvm::LLVMContext llvmContext;\n+        auto llvmModule =\n+            ::mlir::triton::translateTritonGPUToLLVMIR(&llvmContext, op);\n+\n+        std::string str;\n+        llvm::raw_string_ostream os(str);\n+        llvmModule->print(os, nullptr);\n+        os.flush();\n+        return str;\n+      },\n+      ret::take_ownership);\n \n-          return std::make_tuple(ptxCode, size);\n-        });\n+  m.def(\n+      \"translate_llvmir_to_ptx\",\n+      [](const std::string llvmIR, int capability, int version) -> std::string {\n+        // create LLVM module from C++\n+        llvm::LLVMContext context;\n+        std::unique_ptr<llvm::MemoryBuffer> buffer =\n+            llvm::MemoryBuffer::getMemBuffer(llvmIR.c_str());\n+        llvm::SMDiagnostic error;\n+        std::unique_ptr<llvm::Module> module =\n+            llvm::parseIR(buffer->getMemBufferRef(), error, context);\n+        // translate module to PTX\n+        auto ptxCode =\n+            triton::translateLLVMIRToPTX(*module, capability, version);\n+        return ptxCode;\n+      },\n+      ret::take_ownership);\n \n   m.def(\"compile_ptx_to_cubin\",\n-        [](const std::string &ptxCode, uint64_t device) -> py::object {\n+        [](const std::string &ptxCode, const std::string &ptxasPath,\n+           int capability) -> py::object {\n           py::gil_scoped_release allow_threads;\n-          int version;\n-          int cc;\n-          std::string ptxasPath;\n-          triton::getCuCCAndVersionFromDevice(device, &cc, &version,\n-                                              &ptxasPath);\n \n-          std::string cubin = drv::ptx_to_cubin(ptxCode, ptxasPath, cc);\n+          // compile ptx with ptxas\n+          char _fsrc[L_tmpnam];\n+          char _flog[L_tmpnam];\n+          std::tmpnam(_fsrc);\n+          std::tmpnam(_flog);\n+          std::string fsrc = _fsrc;\n+          std::string flog = _flog;\n+          std::string fbin = fsrc + \".o\";\n+          const char *_fbin = fbin.c_str();\n+          std::ofstream ofs(fsrc);\n+          ofs << ptxCode << std::endl;\n+          ofs.close();\n+          std::string cmd;\n+          int err;\n+          cmd = ptxasPath + \" -v --gpu-name=sm_\" + std::to_string(capability) +\n+                \" \" + fsrc + \" -o \" + fsrc + \".o 2> \" + flog;\n+          err = system(cmd.c_str());\n+          if (err != 0) {\n+            std::ifstream _log(_flog);\n+            std::string log(std::istreambuf_iterator<char>(_log), {});\n+            unlink(_fsrc);\n+            unlink(_flog);\n+            throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n+                                     log);\n+          }\n+          std::ifstream _cubin(_fbin, std::ios::binary);\n+          std::string cubin(std::istreambuf_iterator<char>(_cubin), {});\n+          _cubin.close();\n+          unlink(_fsrc);\n+          unlink(_flog);\n+          unlink(_fbin);\n+\n           py::bytes bytes(cubin);\n           return bytes;\n         });\n-\n-  m.def(\n-      \"load_binary\",\n-      [](const std::string &name, const std::string &data,\n-         size_t n_shared_bytes, uint64_t device) {\n-        py::gil_scoped_release allow_threads;\n-        // create driver handles\n-        CUfunction fun;\n-        CUmodule mod;\n-        drv::dispatch::cuModuleLoadData(&mod, data.c_str());\n-        drv::dispatch::cuModuleGetFunction(&fun, mod, name.c_str());\n-        // get allocated registers and spilled registers from the function\n-        int n_regs = 0;\n-        int n_spills = 0;\n-        drv::dispatch::cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS,\n-                                          fun);\n-        drv::dispatch::cuFuncGetAttribute(\n-            &n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun);\n-        n_spills /= 4;\n-        // set dynamic shared memory if necessary\n-        int shared_optin;\n-        drv::dispatch::cuDeviceGetAttribute(\n-            &shared_optin,\n-            CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device);\n-        if (n_shared_bytes > 49152 && shared_optin > 49152) {\n-          drv::dispatch::cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED);\n-          int shared_total, shared_static;\n-          drv::dispatch::cuDeviceGetAttribute(\n-              &shared_total,\n-              CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, device);\n-          drv::dispatch::cuFuncGetAttribute(\n-              &shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun);\n-          drv::dispatch::cuFuncSetAttribute(\n-              fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n-              shared_optin - shared_static);\n-        }\n-        return std::make_tuple((uint64_t)mod, (uint64_t)fun, (uint64_t)n_regs,\n-                               (uint64_t)n_spills);\n-      },\n-      py::return_value_policy::take_ownership);\n }\n \n void init_triton(py::module &m) {"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 186, "deletions": 8, "changes": 194, "file_content_changes": "@@ -7,6 +7,7 @@\n import io\n import json\n import os\n+import re\n import shutil\n import subprocess\n import sys\n@@ -843,25 +844,29 @@ def optimize_tritongpu_ir(mod, num_stages):\n     return mod\n \n \n-def make_ptx(mod: Any, device: int) -> Tuple[str, int]:\n+def make_llvm_ir(mod):\n+    return _triton.translate_triton_gpu_to_llvmir(mod)\n+\n+\n+def make_ptx(mod: Any, compute_capability: int, ptx_version: int) -> Tuple[str, int]:\n     '''\n     Translate TritonGPU module to PTX code.\n     :param mod: a TritonGPU dialect module\n     :return:\n         - PTX code\n         - shared memory alloaction size\n     '''\n-    return _triton.translate_triton_gpu_to_ptx(mod, device)\n+    return _triton.translate_llvmir_to_ptx(mod, compute_capability, ptx_version)\n \n \n-def make_cubin(ptx, device):\n+def make_cubin(ptx: str, ptxas: str, compute_capability: int):\n     '''\n     Compile TritonGPU module to cubin.\n     :param ptx: ptx code\n     :param device: CUDA device\n     :return: str\n     '''\n-    return _triton.compile_ptx_to_cubin(ptx, device)\n+    return _triton.compile_ptx_to_cubin(ptx, ptxas, compute_capability)\n \n \n def ptx_get_kernel_name(ptx: str) -> str:\n@@ -877,6 +882,46 @@ def ptx_get_kernel_name(ptx: str) -> str:\n             return line.split()[-1]\n \n \n+@functools.lru_cache\n+def ptx_get_version(cuda_version) -> int:\n+    '''\n+    Get the highest PTX version supported by the current CUDA driver.\n+    '''\n+    assert isinstance(cuda_version, str)\n+    major, minor = map(int, cuda_version.split('.'))\n+    version = major * 1000 + minor * 10\n+    if version >= 11040:\n+        return 74\n+    if version >= 11030:\n+        return 73\n+    if version >= 11020:\n+        return 72\n+    if version >= 11010:\n+        return 71\n+    if version >= 11000:\n+        return 70\n+    if version >= 10020:\n+        return 65\n+    if version >= 10010:\n+        return 64\n+    if version >= 10000:\n+        return 63\n+    raise RuntimeError(\"Triton only support CUDA 10.0 or higher\")\n+\n+\n+def path_to_ptxas():\n+    prefixes = [os.environ.get(\"TRITON_PTXAS_PATH\", \"\"), \"\", \"/usr/local/cuda/\"]\n+    for prefix in prefixes:\n+        ptxas = os.path.join(prefix, \"bin\", \"ptxas\")\n+        if os.path.exists(ptxas):\n+            result = subprocess.check_output([ptxas, \"--version\"], stderr=subprocess.STDOUT)\n+            if result is not None:\n+                version = re.search(r\".*release (\\d+\\.\\d+).*\", result.decode(\"utf-8\"), flags=re.MULTILINE)\n+                if version is not None:\n+                    return ptxas, version.group(1)\n+    raise RuntimeError(\"Cannot find ptxas\")\n+\n+\n instance_descriptor = namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"], defaults=[set(), set()])\n \n \n@@ -895,17 +940,24 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     # tritongpu-ir\n     module = make_tritongpu_ir(module, num_warps)\n     module = optimize_tritongpu_ir(module, num_stages)\n-\n     if output == \"ttgir\":\n         return module.str()\n \n+    # llvm-ir\n+    llvm_ir = make_llvm_ir(module)\n+\n     assert device >= 0, \"device should be provided.\"\n-    ptx, shem_size = make_ptx(module, device)\n+    ptxas, cuda_version = path_to_ptxas()\n+    compute_capability = torch.cuda.get_device_capability(device)\n+    compute_capability = compute_capability[0] * 10 + compute_capability[1]\n+    ptx_version = ptx_get_version(cuda_version)\n+    ptx = make_ptx(llvm_ir, compute_capability, ptx_version)\n+    shem_size = _triton.get_shared_memory_size(module)\n     kernel_name = ptx_get_kernel_name(ptx)\n     if output == \"ptx\":\n         return ptx, shem_size, kernel_name\n \n-    cubin = make_cubin(ptx, device)\n+    cubin = make_cubin(ptx, ptxas, compute_capability)\n     if output == \"cubin\":\n         return cubin, ptx, shem_size, kernel_name\n \n@@ -980,6 +1032,7 @@ def format_of(ty):\n     src = f\"\"\"\n #include \\\"cuda.h\\\"\n #include <Python.h>\n+\n static inline void gpuAssert(CUresult code, const char *file, int line)\n {{\n    if (code != CUDA_SUCCESS)\n@@ -993,13 +1046,16 @@ def format_of(ty):\n       PyErr_SetString(PyExc_RuntimeError, err);\n    }}\n }}\n+\n #define CUDA_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}\n+\n void _launch(int gridX, int gridY, int gridZ, int num_warps, int shared_memory, CUstream stream, CUfunction function, {arg_decls}) {{\n   void *params[] = {{ {', '.join(f\"&arg{i}\" for i in signature.keys() if i not in constants)} }};\n   if(gridX*gridY*gridZ > 0){{\n     CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*num_warps, 1, 1, shared_memory, stream, params, 0));\n   }}\n }}\n+\n static inline CUdeviceptr getPointer(PyObject *obj, int idx) {{\n   if (PyLong_Check(obj)) {{\n     return (CUdeviceptr)PyLong_AsUnsignedLongLong(obj);\n@@ -1021,6 +1077,7 @@ def format_of(ty):\n   PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n   return (CUdeviceptr)0;\n }}\n+\n static PyObject* launch(PyObject* self, PyObject* args) {{\n   int gridX, gridY, gridZ;\n   uint64_t _stream;\n@@ -1039,17 +1096,20 @@ def format_of(ty):\n   Py_INCREF(Py_None);\n   return Py_None;\n }}\n+\n static PyMethodDef ModuleMethods[] = {{\n   {{\"launch\", launch, METH_VARARGS, \"Entry point for all kernels with this signature\"}},\n   {{NULL, NULL, 0, NULL}} // sentinel\n }};\n+\n static struct PyModuleDef ModuleDef = {{\n   PyModuleDef_HEAD_INIT,\n   \\\"launcher\\\",\n   NULL, //documentation\n   -1, //size\n   ModuleMethods\n }};\n+\n PyMODINIT_FUNC PyInit_launcher(void) {{\n   PyObject *m = PyModule_Create(&ModuleDef);\n   if(m == NULL) {{\n@@ -1256,7 +1316,10 @@ def __init__(self, fn_name, so_path, cache_dir):\n             self.asm[\"ptx\"] = f.read()\n \n         device = torch.cuda.current_device()\n-        mod, func, n_regs, n_spills = _triton.load_binary(metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n+        global cuda_utils\n+        if cuda_utils is None:\n+            cuda_utils = CudaUtils()\n+        mod, func, n_regs, n_spills = cuda_utils.load_binary(metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n         self.cu_module = mod\n         self.cu_function = func\n \n@@ -1266,3 +1329,118 @@ def runner(*args, stream=None):\n                 stream = torch.cuda.current_stream().cuda_stream\n             self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function, *args)\n         return\n+\n+\n+class CudaUtils(object):\n+\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(CudaUtils, cls).__new__(cls)\n+        return cls.instance\n+\n+    def _generate_src(self):\n+        return \"\"\"\n+        #include <cuda.h>\n+\n+        #include \\\"cuda.h\\\"\n+        #include <Python.h>\n+\n+        static inline void gpuAssert(CUresult code, const char *file, int line)\n+        {\n+           if (code != CUDA_SUCCESS)\n+           {\n+              const char* prefix = \"Triton Error [CUDA]: \";\n+              const char* str;\n+              cuGetErrorString(code, &str);\n+              char err[1024] = {0};\n+              strcat(err, prefix);\n+              strcat(err, str);\n+              PyErr_SetString(PyExc_RuntimeError, err);\n+           }\n+        }\n+\n+        #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n+\n+        static PyObject* loadBinary(PyObject* self, PyObject* args) {\n+            const char* name;\n+            const char* data;\n+            Py_ssize_t data_size;\n+            int shared;\n+            int device;\n+            if(!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared, &device)) {\n+                return NULL;\n+            }\n+            CUfunction fun;\n+            CUmodule mod;\n+            int32_t n_regs = 0;\n+            int32_t n_spills = 0;\n+            Py_BEGIN_ALLOW_THREADS;\n+            // create driver handles\n+            CUDA_CHECK(cuModuleLoadData(&mod, data));\n+            CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n+            // get allocated registers and spilled registers from the function\n+            CUDA_CHECK(cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS, fun));\n+            CUDA_CHECK(cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun));\n+            n_spills /= 4;\n+            // set dynamic shared memory if necessary\n+            int shared_optin;\n+            CUDA_CHECK(cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device));\n+            if (shared > 49152 && shared_optin > 49152) {\n+              CUDA_CHECK(cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED));\n+              int shared_total, shared_static;\n+              CUDA_CHECK(cuDeviceGetAttribute(&shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, device));\n+              CUDA_CHECK(cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n+              CUDA_CHECK(cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static));\n+            }\n+            Py_END_ALLOW_THREADS;\n+\n+            if(PyErr_Occurred()) {\n+              return NULL;\n+            }\n+            return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs, n_spills);\n+        }\n+\n+        static PyMethodDef ModuleMethods[] = {\n+          {\"load_binary\", loadBinary, METH_VARARGS, \"Load provided cubin into CUDA driver\"},\n+          {NULL, NULL, 0, NULL} // sentinel\n+        };\n+\n+        static struct PyModuleDef ModuleDef = {\n+          PyModuleDef_HEAD_INIT,\n+          \\\"cuda_utils\\\",\n+          NULL, //documentation\n+          -1, //size\n+          ModuleMethods\n+        };\n+\n+        PyMODINIT_FUNC PyInit_cuda_utils(void) {\n+          PyObject *m = PyModule_Create(&ModuleDef);\n+          if(m == NULL) {\n+            return NULL;\n+          }\n+          PyModule_AddFunctions(m, ModuleMethods);\n+          return m;\n+        }\n+        \"\"\"\n+\n+    def __init__(self):\n+        src = self._generate_src()\n+        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n+        cache = CacheManager(key)\n+        fname = \"cuda_utils.so\"\n+        if not cache.has_file(fname):\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                src_path = os.path.join(tmpdir, \"main.c\")\n+                with open(src_path, \"w\") as f:\n+                    f.write(src)\n+                so = _build(\"cuda_utils\", src_path, tmpdir)\n+                with open(so, \"rb\") as f:\n+                    cache.put(f.read(), fname, binary=True)\n+        import importlib.util\n+        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache._make_path(fname))\n+        mod = importlib.util.module_from_spec(spec)\n+        spec.loader.exec_module(mod)\n+        self.load_binary = mod.load_binary\n+\n+\n+cuda_utils = None"}, {"filename": "python/triton/tools/aot.py", "status": "added", "additions": 61, "deletions": 0, "changes": 61, "file_content_changes": "@@ -0,0 +1,61 @@\n+import argparse\n+\n+import triton\n+import triton._C.libtriton.triton as libtriton\n+\n+if __name__ == '__main__':\n+\n+    # valid source and target formats\n+    VALID_FORMATS = ['llvm-ir', 'ptx', 'triton-ir', 'triton-gpu-ir']\n+\n+    # set up the argument parser\n+    # TODO: conditional requirements\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument('src', help=\"Source file to compile\")\n+    parser.add_argument('--target', required=True,\n+                        help=\"Target format, one of: \" + ', '.join(VALID_FORMATS))\n+    parser.add_argument('--sm', type=int, help=\"Compute capability to compile for\")\n+    parser.add_argument('--ptx-version', type=int, help=\"PTX version to compile for\")\n+\n+    # parse the args\n+    args = parser.parse_args()\n+\n+    # TODO: clean-up and re-use triton.compiler primitive functions\n+    # check for validity of format arguments\n+    if args.target not in VALID_FORMATS:\n+        print(\"Invalid target format: \" + args.target)\n+        exit(0)\n+\n+    # parse source file to MLIR module\n+    context = libtriton.ir.context()\n+    module = libtriton.ir.parse_mlir_module(args.src, context)\n+    module.context = context\n+\n+    # optimizer triton-ir\n+    module = triton.compiler.optimize_triton_ir(module)\n+    if args.target == 'triton-ir':\n+        print(module.str())\n+        exit(0)\n+\n+    # triton-ir -> triton-gpu-ir\n+    module = triton.compiler.make_tritongpu_ir(module, num_warps=4)\n+    module = triton.compiler.optimize_tritongpu_ir(module, num_stages=3)\n+    if args.target == 'triton-gpu-ir':\n+        print(module.str())\n+        exit(0)\n+\n+    # triton-gpu-ir -> llvm-ir\n+    module = triton.compiler.make_llvm_ir(module)\n+    if args.target == 'llvm-ir':\n+        print(module)\n+        exit(0)\n+\n+    if not args.sm:\n+        raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n+    if not args.ptx_version:\n+        raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")\n+\n+    # llvm-ir -> ptx\n+    module = triton.compiler.make_ptx(module, compute_capability=args.sm, ptx_version=args.ptx_version)\n+    assert args.target == 'ptx'\n+    print(module)"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-translate %s --target=llvmir | FileCheck %s\n+// RUN: python3 -m triton.tools.aot %s --target=llvm-ir | FileCheck %s\n \n // == LLVM IR check begin ==\n // CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1,5 +1,4 @@\n-// RUN: triton-translate %s --target=ptx --sm=80 --ptx-version=10000 | FileCheck %s\n-\n+// RUN: python3 -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n // CHECK-LABEL: // Generated by LLVM NVPTX Back-End\n // CHECK: .version 6.3\n // CHECK: .target sm_80"}]