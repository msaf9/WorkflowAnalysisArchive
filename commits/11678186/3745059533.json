[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -250,7 +250,7 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n \n @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n     (dtype_x, dtype_y, op)\n-    for op in ['+', '-', '*', '/', '%']  \n+    for op in ['+', '-', '*', '/', '%']\n     for dtype_x in dtypes_with_bfloat16\n     for dtype_y in dtypes_with_bfloat16\n ])"}, {"filename": "python/test/unit/operators/test_blocksparse.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -2,7 +2,6 @@\n import torch\n \n import triton\n-import triton._C.libtriton.triton as _triton\n \n \n @pytest.mark.parametrize(\"MODE\", [\"sdd\", \"dds\", \"dsd\"])"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -2,7 +2,6 @@\n import torch\n \n import triton\n-import triton._C.libtriton.triton as _triton\n \n \n @pytest.mark.parametrize(\"M, N, dtype, mode\","}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -4,7 +4,6 @@\n import torch\n \n import triton\n-import triton._C.libtriton.triton as _triton\n \n \n @pytest.mark.parametrize("}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -3,9 +3,7 @@\n \n from ..impl import (\n     ir,\n-    builtin,\n )\n-from . import core, extern, libdevice, random\n from .core import (\n     abs,\n     arange,"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 8, "changes": 9, "file_content_changes": "@@ -238,13 +238,8 @@ def mod(input: tl.tensor,\n     # float % float\n     if scalar_ty.is_floating():\n         # input - input.div(other, rounding_mode=\"floor\") * other\n-<<<<<<< HEAD\n         ret = sub(input, mul(floor(fdiv(input, other, False, builder), builder),\n                              other, builder),\n-=======\n-        ret = sub(input , mul(floor(fdiv(input, other, False, builder), builder), \n-                              other, builder), \n->>>>>>> origin/triton-mlir\n                   builder)\n         return ret\n     # % int\n@@ -1117,9 +1112,6 @@ def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     from . import libdevice\n     return libdevice.mulhi(x, y, _builder=builder)\n \n-def floor(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n-    from . import libdevice\n-    return libdevice.floor(x, _builder=builder)\n \n def floor(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     from . import libdevice\n@@ -1133,6 +1125,7 @@ def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n def log(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_log(x.handle), x.type)\n \n+\n def cos(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_cos(x.handle), x.type)\n "}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -99,7 +99,6 @@ def estimate_matmul_time(\n \n \n def early_config_prune(configs, named_args):\n-    backend = _triton.runtime.backend.CUDA\n     device = torch.cuda.current_device()\n     capability = torch.cuda.get_device_capability()\n     # BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, num_warps, num_stages"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -309,14 +309,20 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     triton.testing.assert_almost_equal(ref_dq, tri_dq)\n \n \n+try:\n+    from flash_attn.flash_attn_interface import flash_attn_func\n+    HAS_FLASH = True\n+except BaseException:\n+    HAS_FLASH = False\n+\n BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n # vary seq length for fixed head and batch=4\n configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n     x_vals=[2**i for i in range(10, 16)],\n     line_arg='provider',\n-    line_vals=['triton'],\n-    line_names=['Triton'],\n+    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',"}]