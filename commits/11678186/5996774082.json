[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -62,6 +62,11 @@ jobs:\n         run: |\n           echo \"PATH=${HOME}/.local/bin:${PATH}\" >> \"${GITHUB_ENV}\"\n \n+      - name: Check pre-commit\n+        run: |\n+          python3 -m pip install --upgrade pre-commit\n+          python3 -m pre_commit run --all-files --verbose\n+\n       - name: Install Triton\n         if: ${{ env.BACKEND == 'CUDA'}}\n         run: |"}, {"filename": "README.md", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "file_content_changes": "@@ -9,6 +9,32 @@\n ------------------- |\n [![Documentation](https://github.com/openai/triton/actions/workflows/documentation.yml/badge.svg)](https://triton-lang.org/)\n \n+# Triton Developer Conference Registration Open\n+The Triton Developer Conference will be held in a hybrid mode at the Microsoft Silicon Valley Campus in Mountain View, California. The conference will be held on September 20th from 10am to 4pm, followed by a reception till 5:30 pm. Please use the link below to register to attend either in-person or virtually online.\n+\n+Registration Link for Triton Developer Conference is [here](https://forms.office.com/r/m4jQXShDts)\n+\n+Tentative Agenda for the conference (subject to change):\n+\n+|Time    |Title  |Speaker\n+|--------|-------|-------|\n+|10:00 AM|Welcome|Kevin Scott (Microsoft)|\n+|10:20 AM|The Triton Compiler: Past, Present and Future|Phil Tillet (OpenAI)|\n+|11:00 AM|**Break**||\n+|11:20 AM|Hopper support in Triton|Gustav Zhu (Nvidia)|\n+|11:40 AM|Bringing Triton to AMD GPUs|Jason Furmanek, Lixun Zhang (AMD)|\n+|12:00 PM|Intel XPU Backend for Triton|Eikan Wang (Intel)|\n+|12:20 PM|Vectorization of Triton Kernels for Qualcomm Hexagon Backend|Javed Absar (Qualcomm)|\n+|12:30 PM|**Lunch**||\n+|1:40 PM |Triton for MTIA|Roman Levenstein et al, (Meta)|\n+|2:00 PM |Using Triton IR for high-performance fusions in XLA|George Karpenkov (Google)|\n+|2:20 PM |Triton for All: Triton as a device-independent language|Ian Bearman (Microsoft)|\n+|2:40 PM|**Break**||\n+|3:00 PM|PyTorch 2.0 and TorchInductor|Jason Ansel, Horace He (Meta)|\n+|3:20 PM|Pallas: A JAX Kernel Language|Sharad Vikram (Google)|\n+|3:40 PM|Writing Grouped GEMMs in Triton|Vinod Grover (Nvidia)|\n+|4:00 PM|**Reception**||\n+\n \n # Triton\n "}, {"filename": "docs/conf.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -101,11 +101,12 @@ def documenter(app, obj, parent):\n     'gallery_dirs': 'getting-started/tutorials',\n     'filename_pattern': '',\n     # XXX: Temporarily disable fused attention tutorial on V100\n-    'ignore_pattern': r'__init__\\.py',\n+    'ignore_pattern': r'(__init__\\.py|09.*\\.py|10.*\\.py)',\n     'within_subsection_order': FileNameSortKey,\n     'reference_url': {\n         'sphinx_gallery': None,\n-    }\n+    },\n+    'abort_on_example_error': True,\n }\n \n # Add any paths that contain templates here, relative to this directory.\n@@ -144,7 +145,7 @@ def documenter(app, obj, parent):\n #\n # This is also used if you do content translation via gettext catalogs.\n # Usually you set \"language\" from the command line for these cases.\n-language = None\n+language = 'en'\n \n # List of patterns, relative to source directory, that match files and\n # directories to ignore when looking for source files."}, {"filename": "docs/meetups/08-22-2023.md", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -11,3 +11,31 @@\n 5. Intel working on the CPU backend for Triton.\n 6. AMD updates\n 7. Open discussion\n+\n+##### Minutes:\n+Recording link [here](https://drive.google.com/file/d/19Nnc0i7zUyn-ni2RSFHbPHHiPkYU96Mz/view)\n+\n+1. H100 updates:\n+   - Preliminary support is merged, disabled by default, can be enabled with env variables\n+   - Supports latest tensor cores, FP8s. Support for Flash Attention on the main branch coming soon.\n+   - Performance is very good on Matmuls, 80-90% of cublas on large Matmuls right now, will eventually reach parity with cublas. Above 600 teraflops on fp16 on xxm card, cublas is 670 on random input data. FP8 is twice that, around 1.2 petaflops.\n+   - Hopper support includes the full FP8 support for compute.\n+2. Triton release plan update\n+   - No specific dates for now, plan is to release before end of 2023.\n+   - Will move to 3.0 release due to minor backward compatibility breaking changes. For eg. Will move compiler options in the indexing operators as hardcoded operators in the kernel, will bump the major version.\n+   - Functionally the main goal will be to have 3rd party plugins for Intel and AMD gpus.\n+   - May synchronise with a PyTorch release so that PyTorch can benefit from the latest features, however continuous integration workflow is the default release cadence expected.\n+   - Will switch the default behavior to optimized mode for the release, needs more discussion with Nvidia.\n+   - Will expose flags for a user to enable kernel selection themselves.\n+   - Open question: Pytorch hasn\u2019t rebased to latest triton, it is close to PyTorch code freeze \u2013 will PyTorch still sync with Triton 2.0? Will we have another release to support triton 2.0?\n+   - Community can start with the latest stable branch and rebase 3rd party plugin on top of that. OAI has no resources to commit to, but community can contribute.\n+3. Linalg updates\n+   - Discussion on Github for Linalg as a middle layer between the language and target hardware. Includes support for block pointers and modulo operators.\n+   - Please join the conversation [here](https://github.com/openai/triton/discussions/1842)\n+   - Branch pushed is behind the tip, will work on getting it caught up on the tip.\n+4. Intel GPU Backend status update.\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Intel%20XPU%20Backend%20for%20Triton%20-%20Update%20-%200823.pptx)\n+5. Intel working on the CPU backend for Triton.\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Intel%20XPU%20Backend%20for%20Triton%20-%20Update%20-%200823.pptx)\n+6. AMD updates\n+   - Please refer to slides [here](https://github.com/openai/triton/blob/main/docs/meetups/Triton_AMD_update_0823.pdf)."}, {"filename": "docs/meetups/Intel XPU Backend for Triton - Update - 0823.pptx", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "docs/meetups/Triton_AMD_update_0823.pdf", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -137,7 +137,7 @@ def TT_LoadOp : TT_Op<\"load\",\n                       [SameLoadStoreOperandsAndResultShape,\n                        SameLoadStoreOperandsAndResultEncoding,\n                        AttrSizedOperandSegments,\n-                       MemoryEffects<[MemRead]>,\n+                       DeclareOpInterfaceMethods<MemoryEffectsOpInterface>,\n                        TypesMatchWith<\"infer ptr type from result type\",\n                                       \"result\", \"ptr\", \"$_self\",\n                                       \"mlir::OpTrait::impl::verifyLoadStorePointerAndValueType\">,\n@@ -498,6 +498,8 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [Pure]> {\n     let results = (outs TT_IntTensor:$result);\n \n     let assemblyFormat = \"attr-dict `:` type($result)\";\n+\n+    let hasFolder = 1;\n }\n \n //"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -758,9 +758,8 @@ struct ConvertLayoutOpConversion\n                            i32_val(srcShape[0] / instrShape[0]));\n \n       unsigned inVec =\n-          inOrd == outOrd\n-              ? triton::gpu::getContigPerThread(mmaLayout)[inOrd[0]]\n-              : 1;\n+          inOrd == outOrd ? triton::gpu::getContigPerThread(mmaLayout)[inOrd[0]]\n+                          : 1;\n       unsigned outVec = dstSharedLayout.getVec();\n       unsigned minVec = std::min(outVec, inVec);\n       assert(minVec == 2);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -128,7 +128,8 @@ Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n   for (int i = 0; i < aNumPtr; ++i) {\n     aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n   }\n-  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n+  auto elemTy = typeConverter->convertType(\n+      A.getType().cast<RankedTensorType>().getElementType());\n \n   Type ptrTy = ptr_ty(elemTy, 3);\n   SmallVector<Value> aPtrs(aNumPtr);\n@@ -192,7 +193,8 @@ Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n   for (int i = 0; i < bNumPtr; ++i) {\n     bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n   }\n-  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n+  auto elemTy = typeConverter->convertType(\n+      B.getType().cast<RankedTensorType>().getElementType());\n \n   Type ptrTy = ptr_ty(elemTy, 3);\n   SmallVector<Value> bPtrs(bNumPtr);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "file_content_changes": "@@ -237,9 +237,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n           {ha[{m, k + 1}], \"r\"},\n           {ha[{m + 1, k + 1}], \"r\"},\n       });\n-      auto bArgs2 = builder.newListOperand({\n-          {hb[{n, k + 1}], \"r\"}\n-      });\n+      auto bArgs2 = builder.newListOperand({{hb[{n, k + 1}], \"r\"}});\n       mma(retArgs, aArgs1, bArgs1, cArgs);\n       mma(retArgs, aArgs2, bArgs2, cArgs);\n     } else {\n@@ -333,4 +331,3 @@ LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n                               ConversionPatternRewriter &rewriter) {\n   return convertMMA(op, adaptor, typeConverter, rewriter, false /*isTuring*/);\n }\n-"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -190,6 +190,12 @@ static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n   assert(inEncoding == ouEncoding);\n   if (!inEncoding)\n     return values;\n+  // If the parent of the dot operand is in block encoding, we don't need to\n+  // reorder elements\n+  auto parentEncoding =\n+      dyn_cast<triton::gpu::MmaEncodingAttr>(ouEncoding.getParent());\n+  if (!parentEncoding)\n+    return values;\n   size_t inBitWidth = inTensorTy.getElementType().getIntOrFloatBitWidth();\n   size_t ouBitWidth = ouTensorTy.getElementType().getIntOrFloatBitWidth();\n   auto ouEltTy = ouTensorTy.getElementType();"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -80,6 +80,16 @@ void LoadOp::print(OpAsmPrinter &printer) {\n   printer.printStrippedAttrOrType(getResult().getType());\n }\n \n+void LoadOp::getEffects(\n+    SmallVectorImpl<SideEffects::EffectInstance<MemoryEffects::Effect>>\n+        &effects) {\n+  effects.emplace_back(MemoryEffects::Read::get(), getPtr(),\n+                       SideEffects::DefaultResource::get());\n+  if (getIsVolatile())\n+    effects.emplace_back(MemoryEffects::Write::get(),\n+                         SideEffects::DefaultResource::get());\n+}\n+\n ParseResult StoreOp::parse(OpAsmParser &parser, OperationState &result) {\n   // Parse operands\n   SmallVector<OpAsmParser::UnresolvedOperand, 4> allOperands;\n@@ -418,6 +428,16 @@ LogicalResult mlir::triton::DotOp::verify() {\n                                                      bEncoding);\n }\n \n+//-- MakeRangeOp --\n+OpFoldResult MakeRangeOp::fold(FoldAdaptor adaptor) {\n+  // make_range(start, start + 1) -> constant(start)\n+  if (adaptor.getStart() + 1 == adaptor.getEnd()) {\n+    auto shapedType = getType().cast<ShapedType>();\n+    return SplatElementsAttr::get(shapedType, adaptor.getStartAttr());\n+  }\n+  return {};\n+}\n+\n //-- ReduceOp --\n static mlir::LogicalResult\n inferReduceReturnShape(const RankedTensorType &argTy, const Type &retEltTy,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 36, "deletions": 2, "changes": 38, "file_content_changes": "@@ -102,6 +102,8 @@ warpsPerTileV3(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps,\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n   mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n+  mutable llvm::SmallVector<llvm::SetVector<Operation *>> dotOpSetVector;\n+  mutable llvm::SmallVector<unsigned> mmaV3InstrNs;\n \n   static bool bwdFilter(Operation *op) {\n     return op->getNumOperands() == 1 &&\n@@ -144,6 +146,36 @@ class BlockedToMMA : public mlir::RewritePattern {\n     }\n   }\n \n+  unsigned getMmaV3InstrN(tt::DotOp dotOp, unsigned currN) const {\n+    auto type = dotOp.getResult().getType().cast<RankedTensorType>();\n+    if (type.getEncoding().isa<MmaEncodingAttr>())\n+      return currN;\n+    for (size_t i = 0; i < dotOpSetVector.size(); ++i) {\n+      if (dotOpSetVector[i].count(dotOp.getOperation()) > 0)\n+        return mmaV3InstrNs[i];\n+    }\n+\n+    SetVector<Operation *> slices;\n+    mlir::getForwardSlice(dotOp.getResult(), &slices);\n+    mlir::getBackwardSlice(dotOp.getOperation(), &slices);\n+    unsigned N = currN;\n+    llvm::SetVector<Operation *> dotOpSet;\n+    for (Operation *iter : slices) {\n+      if (auto nextDotOp = dyn_cast<tt::DotOp>(iter)) {\n+        auto type = nextDotOp.getResult().getType().cast<RankedTensorType>();\n+        auto AType = nextDotOp.getOperand(0).getType().cast<RankedTensorType>();\n+        auto shapePerCTA = ttg::getShapePerCTA(type);\n+        auto instrShape = mmaVersionToInstrShape(3, shapePerCTA, AType);\n+        dotOpSet.insert(iter);\n+        if (instrShape[1] < N)\n+          N = instrShape[1];\n+      }\n+    }\n+    mmaV3InstrNs.push_back(N);\n+    dotOpSetVector.push_back(dotOpSet);\n+    return N;\n+  }\n+\n   static Value getMMAv3Operand(Value v, mlir::PatternRewriter &rewriter,\n                                int opIdx) {\n     auto cvtOp = dyn_cast_or_null<ttg::ConvertLayoutOp>(v.getDefiningOp());\n@@ -201,6 +233,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n     auto instrShape =\n         mmaVersionToInstrShape(versionMajor, retShapePerCTA, AType);\n+    if (versionMajor == 3)\n+      instrShape[1] = getMmaV3InstrN(dotOp, instrShape[1]);\n \n     // operands\n     Value a = dotOp.getA();\n@@ -246,8 +280,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n       auto warpsPerTile = getWarpsPerTile(dotOp, retShapePerCTA, versionMajor,\n                                           numWarps, instrShape);\n       mmaEnc = ttg::MmaEncodingAttr::get(oldRetType.getContext(), versionMajor,\n-                                         versionMinor, warpsPerTile,\n-                                         CTALayout, instrShape);\n+                                         versionMinor, warpsPerTile, CTALayout,\n+                                         instrShape);\n     }\n     auto newRetType = RankedTensorType::get(\n         oldRetType.getShape(), oldRetType.getElementType(), mmaEnc);"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 36, "deletions": 0, "changes": 36, "file_content_changes": "@@ -54,6 +54,7 @@\n #include <fstream>\n #include <optional>\n #include <pybind11/buffer_info.h>\n+#include <pybind11/embed.h>\n #include <pybind11/functional.h>\n #include <pybind11/pybind11.h>\n #include <pybind11/stl.h>\n@@ -172,6 +173,30 @@ class TritonOpBuilder {\n   bool lineInfoEnabled = !triton::tools::getBoolEnv(\"TRITON_DISABLE_LINE_INFO\");\n };\n \n+static std::string locationToString(mlir::Location loc) {\n+  std::string str;\n+  llvm::raw_string_ostream os(str);\n+  loc.print(os);\n+  os.flush(); // Make sure all the content is dumped into the 'str' string\n+  return str;\n+}\n+\n+static void outputWarning(mlir::Location loc, const std::string &msg) {\n+  std::string locStr = locationToString(loc);\n+\n+  py::exec(\n+      R\"(\n+import warnings\n+\n+def custom_showwarning(message, category, filename, lineno, file=None, line=None):\n+    print(f\"UserWarning: {message}\")\n+\n+warnings.showwarning = custom_showwarning\n+warnings.warn(f\"{loc}: {msg}\")\n+)\",\n+      py::globals(), py::dict(py::arg(\"loc\") = locStr, py::arg(\"msg\") = msg));\n+}\n+\n /*****************************************************************************/\n /* Python bindings for triton::ir                                            */\n /*****************************************************************************/\n@@ -596,6 +621,17 @@ void init_triton_ir(py::module &&m) {\n                  newBlock->erase();\n                }\n              });\n+             // 2. Check if the result of tl.advance is used\n+             self.walk([&](mlir::Operation *op) {\n+               if (mlir::isa<mlir::triton::AdvanceOp>(op) &&\n+                   op->getResult(0).use_empty())\n+                 outputWarning(op->getLoc(), \"The result of tl.advance is not \"\n+                                             \"being used. Note that tl.advance \"\n+                                             \"does not have any side effects. \"\n+                                             \"To move the block pointer, you \"\n+                                             \"need to assign the result of \"\n+                                             \"tl.advance to a variable.\");\n+             });\n            })\n       .def_property_readonly(\"type\", &mlir::triton::FuncOp::getFunctionType)\n       .def(\"reset_type\", &mlir::triton::FuncOp::setType);"}, {"filename": "python/test/unit/hopper/test_gemm.py", "status": "modified", "additions": 87, "deletions": 76, "changes": 163, "file_content_changes": "@@ -211,102 +211,113 @@ def matmul_kernel(\n \n \n @pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_OUTPUT,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n-                         [(128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n-                          for shape_w_c in [\n-                             # badcase from cublas-important-layers\n-                             [4096, 1, 1024, False, False, True],\n-                             [2048, 204, 1000, True, False, True],\n-                             [4096, 1, 1024, False, False, False],\n-                             [2048, 204, 1000, True, False, False],\n-                         ]\n+                         [\n+                             # corner shapes\n+                             (128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n+                             for shape_w_c in [\n+                                 [4096, 1, 1024, False, False, True],\n+                                 [2048, 204, 1000, True, False, True],\n+                                 [4096, 1, 1024, False, False, False],\n+                                 [2048, 204, 1000, True, False, False],\n+                             ]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              # softmax works for one CTA\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 64, 64, 64],\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [16, 16, 64, 4, 1, 16, 16, 64],\n-                             [64, 64, 32, 8, 1, 64, 64, 64],\n-                             [128, 128, 64, 4, 1, 128, 128, 128],\n-                         ]\n+                         ] + [\n+                             # softmax epilogue\n+                             (*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 64, 64, 64],\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [16, 16, 64, 4, 1, 16, 16, 64],\n+                                 [64, 64, 32, 8, 1, 64, 64, 64],\n+                                 [128, 128, 64, 4, 1, 128, 128, 128],\n+                             ]\n                              for epilogue in ['softmax']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n-                             for trans_output in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n+                             for trans_output in [False,]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 128, 128, 64],\n-                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n-                             # for chain-dot\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [64, 64, 16, 4, 1, None, None, None],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 16, 64, 4, 1, 128, 128, 64],\n-                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4, 8] for num_ctas in [1, 2]],\n-                             # repeat\n-                             [64, 64, 32, 8, 1, 128, 256, 64],\n-                             [64, 64, 16, 8, 2, 128, 128, 64],\n-                             # irregular shape\n-                             [128, 128, 64, 4, 1, 500, 200, 128],\n-                             [128, 128, 64, 4, 2, 513, 193, 192],\n-                         ]\n+                         ] + [\n+                             # loop over epilogues besides of softmax\n+                             (*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 128, 128, 64],\n+                                 *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n+                                 # for chain-dot\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [64, 64, 16, 4, 1, None, None, None],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 16, 64, 4, 1, 128, 128, 64],\n+                                 *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4, 8] for num_ctas in [1, 2]],\n+                                 # repeat\n+                                 [64, 64, 32, 8, 1, 128, 256, 64],\n+                                 [64, 64, 16, 8, 2, 128, 128, 64],\n+                                 # irregular shape\n+                                 [128, 128, 64, 4, 1, 500, 200, 128],\n+                                 [128, 128, 64, 4, 2, 513, 193, 192],\n+                             ]\n                              for epilogue in ['none', 'add-matrix', 'add-rows', 'add-cols', 'chain-dot']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n-                             for trans_output in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n+                             for trans_output in [False,]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n                              if not (epilogue == 'chain-dot' and (shape_w_c[6] is not None or shape_w_c[1] != shape_w_c[6]))\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 32, 4, 1, 128, 256, 64],\n-                             [128, 128, 16, 4, 4, 512, 256, 64],\n-                             [128, 256, 32, 4, 8, 256, 256, 192],\n-                             [512, 256, 32, 4, 8, 1024, 256, 192],\n-                             # BLOCK_K >= 128\n-                             [64, 128, 128, 4, 1, 512, 256, 256],\n-                             [128, 128, 128, 4, 1, 256, 256, 192],\n-                             [128, 128, 128, 4, 2, 256, 256, 192],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 32, 32, 4, 1, 128, 256, 64],\n-                             [32, 32, 16, 4, 1, 256, 256, 192],\n-                             [16, 32, 64, 4, 4, 512, 256, 64],\n-                         ]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                         ] + [\n+                             # loop over tile shapes and transpose combinations\n+                             (*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 32, 4, 1, 128, 256, 64],\n+                                 [128, 128, 16, 4, 4, 512, 256, 64],\n+                                 [128, 256, 32, 4, 8, 256, 256, 192],\n+                                 [512, 256, 32, 4, 8, 1024, 256, 192],\n+                                 # BLOCK_K >= 128\n+                                 [64, 128, 128, 4, 1, 512, 256, 256],\n+                                 [128, 128, 128, 4, 1, 256, 256, 192],\n+                                 [128, 128, 128, 4, 2, 256, 256, 192],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 32, 32, 4, 1, 128, 256, 64],\n+                                 [32, 32, 16, 4, 1, 256, 256, 192],\n+                                 [16, 32, 64, 4, 4, 512, 256, 64],\n+                             ]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n                              for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                              # loop over instr shapes\n-                              for n in [16, 32, 64, 128, 256]\n-                              for trans_output in [False, True]\n-                              for out_dtype in ['float16', 'float32']\n-                              for use_tma_store in [False, True]\n-                              for num_stages in [2, 4, 5, 7]\n-                              for enable_ws in [False, True]\n-                              ] + [(*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                                   # irregular shapes\n-                                   for shape_w_c in [\n-                                       [128, 128, 64, 4, 1],\n-                                       [256, 128, 64, 4, 2],\n-                                       [128, 128, 128, 4, 2],\n-                              ]\n-                             for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for trans_output in [False, True]\n-                             for out_dtype in ['float16', 'float32']\n+                         ] + [\n+                             # loop over instr shapes & pipeline stages\n+                             (64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for n in [16, 32, 64, 128, 256]\n+                             for trans_output in [False,]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n+                             for num_stages in [2, 4, 5, 7]\n+                             for enable_ws in [False, True]\n+                         ] + [\n+                             # irregular shapes\n+                             (*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [128, 128, 64, 4, 1],\n+                                 [256, 128, 64, 4, 2],\n+                                 [128, 128, 128, 4, 2],\n+                             ]\n+                             for shape in [\n+                                 [512, 360, 1024],\n+                                 [360, 4096, 512],\n+                             ]\n+                             for trans_output in [False,]\n+                             for out_dtype in ['float32',]\n                              for use_tma_store in [False, True]\n-                             for num_stages in [2, 3, 4]\n+                             for num_stages in [3, 4]\n                              for enable_ws in [False, True]\n                          ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 39, "deletions": 32, "changes": 71, "file_content_changes": "@@ -696,9 +696,9 @@ def full_static_persistent_matmul_kernel(\n \n @pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n                          [\n+                             # corner shapes\n                              (128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n                              for shape_w_c in [\n-                                 # bad from cublas-important-layers\n                                  [4096, 1, 1024, False, False],\n                                  [2048, 204, 1000, True, False],\n                                  [16, 524288, 32, False, True],\n@@ -707,6 +707,7 @@ def full_static_persistent_matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for enable_ws in [True]\n                          ] + [\n+                             # softmax epilogue\n                              (*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                              # softmax works for one CTA\n                              for shape_w_c in [\n@@ -720,11 +721,12 @@ def full_static_persistent_matmul_kernel(\n                              for epilogue in ['softmax']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n                          ] + [\n+                             # loop over tile shapes and transpose combinations\n                              (*shape_w_c, trans_a, trans_b, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              for shape_w_c in [\n                                  [64, 64, 32, 4, 1, 128, 256, 64],\n@@ -740,58 +742,63 @@ def full_static_persistent_matmul_kernel(\n                                  [32, 32, 16, 4, 1, 256, 256, 192],\n                                  [16, 32, 64, 4, 4, 512, 256, 64],\n                              ]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                             for out_dtype in ['float32',]\n+                             for use_tma_store in [False,]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n-                              for shape_w_c in [\n-                             [64, 64, 16, 4, 1, 128, 128, 64],\n-                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1, 2, 4]],\n-                             # for chain-dot\n-                             [128, 128, 64, 4, 1, None, None, None],\n-                             [64, 64, 16, 4, 1, None, None, None],\n-                             # small BLOCK_M and BLOCK_K\n-                             [16, 16, 64, 4, 1, 128, 128, 64],\n-                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1, 2]],\n-                             #  # TODO: enable when num_warps != 4 is supported.\n-                             #  # repeat\n-                             #  # [64, 64, 32, 8, 1, 128, 256, 64],\n-                             #  # [64, 64, 16, 8, 2, 128, 128, 64],\n-                             # irregular shape\n-                             [128, 128, 64, 4, 1, 500, 200, 128],\n-                             [128, 128, 64, 4, 1, 513, 193, 192],\n-                         ]\n+                         ] + [\n+                             # loop over epilogues besides of softmax\n+                             (*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                             for shape_w_c in [\n+                                 [64, 64, 16, 4, 1, 128, 128, 64],\n+                                 *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1, 2, 4]],\n+                                 # for chain-dot\n+                                 [128, 128, 64, 4, 1, None, None, None],\n+                                 [64, 64, 16, 4, 1, None, None, None],\n+                                 # small BLOCK_M and BLOCK_K\n+                                 [16, 16, 64, 4, 1, 128, 128, 64],\n+                                 *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1, 2]],\n+                                 #  # TODO: enable when num_warps != 4 is supported.\n+                                 #  # repeat\n+                                 #  # [64, 64, 32, 8, 1, 128, 256, 64],\n+                                 #  # [64, 64, 16, 8, 2, 128, 128, 64],\n+                                 # irregular shape\n+                                 [128, 128, 64, 4, 1, 500, 200, 128],\n+                                 [128, 128, 64, 4, 1, 513, 193, 192],\n+                             ]\n                              for epilogue in ['none', 'add-matrix', 'add-rows', 'add-cols', 'chain-dot']\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n-                             for trans_a in [False, True]\n-                             for trans_b in [False, True]\n+                             for trans_a in [False,]\n+                             for trans_b in [True,]\n                              for num_stages in [3]\n                              for enable_ws in [True]\n                              if not (epilogue == 'chain-dot' and (shape_w_c[5] is not None or shape_w_c[0] != shape_w_c[1]))\n                          ] + [\n+                             # loop over instr shapes & pipeline stages\n                              (64, n, 16, 4, 1, 512, 256, 256, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n-                             # loop over instr shapes\n                              for n in [16, 32, 64, 128, 256]\n-                             for out_dtype in ['float16', 'float32']\n-                             for use_tma_store in [False, True]\n+                             for out_dtype in ['float32']\n+                             for use_tma_store in [False,]\n                              for num_stages in [2, 4, 5, 7]\n                              for enable_ws in [True]\n                          ] + [\n-                             (*shape_w_c, *shape, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              # irregular shapes\n+                             (*shape_w_c, *shape, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              for shape_w_c in [\n                                  [128, 128, 64, 4, 1],\n                                  [256, 128, 64, 4, 2],\n                                  [128, 128, 128, 4, 2]\n                              ]\n-                             for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for out_dtype in ['float16', 'float32']\n+                             for shape in [\n+                                 [512, 360, 1024],\n+                                 [360, 4096, 512],\n+                             ]\n+                             for out_dtype in ['float32']\n                              for use_tma_store in [False, True]\n-                             for num_stages in [2, 3, 4]\n+                             for num_stages in [3, 4]\n                              for enable_ws in [True]\n                          ]\n                          )"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 11, "deletions": 7, "changes": 18, "file_content_changes": "@@ -2172,12 +2172,12 @@ def kernel(X, stride_xm, stride_xk,\n                Y, stride_yk, stride_yn,\n                W, stride_wn, stride_wl,\n                Z, stride_zm, stride_zn,\n-               out_dtype: tl.constexpr,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n                ALLOW_TF32: tl.constexpr,\n                DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n-               COL_A: tl.constexpr, COL_B: tl.constexpr):\n+               COL_A: tl.constexpr, COL_B: tl.constexpr,\n+               out_dtype: tl.constexpr = tl.float32):\n         off_m = tl.arange(0, BLOCK_M)\n         off_n = tl.arange(0, BLOCK_N)\n         off_l = tl.arange(0, BLOCK_N)\n@@ -2251,7 +2251,6 @@ def kernel(X, stride_xm, stride_xk,\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n                          w_tri, w_tri.stride(0), w_tri.stride(1),\n                          z_tri, z_tri.stride(0), z_tri.stride(1),\n-                         out_dtype,\n                          COL_A=col_a, COL_B=col_b,\n                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n@@ -2260,7 +2259,8 @@ def kernel(X, stride_xm, stride_xk,\n                          DO_SOFTMAX=epilogue == 'softmax',\n                          CHAIN_DOT=epilogue == 'chain-dot',\n                          ALLOW_TF32=allow_tf32,\n-                         num_warps=num_warps, num_ctas=num_ctas)\n+                         num_warps=num_warps, num_ctas=num_ctas,\n+                         out_dtype=out_dtype)\n     if epilogue == 'softmax' and (in_dtype != 'float32' or allow_tf32):\n         ptx = pgm.asm[\"ptx\"]\n         start = ptx.find(\"shfl.sync\")\n@@ -2313,9 +2313,9 @@ def kernel(X, stride_xm, stride_xk,\n         assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.tf32.tf32', ptx)\n     elif in_dtype == 'float16' and out_dtype == tl.float32:\n         if capability[0] == 7 and capability[1] == 5:  # Turing\n-          assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.f16.f16', ptx)\n+            assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f32.f16.f16', ptx)\n         else:\n-          assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f32.f16.f16', ptx)\n+            assert re.search(r'[mma|wgmma.mma_async].sync.aligned.m\\d+n\\d+k16(?:.row.col)?.f32.f16.f16', ptx)\n     elif in_dtype == 'float16' and out_dtype == tl.float16:\n         if capability[0] == 7 and capability[1] == 5:  # Turing\n             assert re.search(r'mma.sync.aligned.m\\d+n\\d+k8(?:.row.col)?.f16.f16.f16', ptx)\n@@ -2331,6 +2331,7 @@ def test_dot_mulbroadcastred(in_dtype, device):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         pytest.skip(\"Requires sm >= 80 to run\")\n+\n     @triton.jit\n     def kernel(Z, X, Y,\n                M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n@@ -2430,6 +2431,7 @@ def kernel(out_ptr):\n def test_dot_without_load(dtype_str, device):\n     capability = torch.cuda.get_device_capability()\n     allow_tf32 = capability[0] > 7\n+\n     @triton.jit\n     def _kernel(out, ALLOW_TF32: tl.constexpr):\n         a = GENERATE_TEST_HERE\n@@ -2900,6 +2902,8 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n # TODO(Keren): if_exp_dynamic\n+\n+\n @pytest.mark.parametrize(\"if_type\", [\"if\", \"if_and_dynamic\", \"if_exp_static\", \"if_and_static\"])\n def test_if(if_type, device):\n \n@@ -3084,7 +3088,7 @@ def kernel(X, Y, Z, n: tl.constexpr, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"num_ctas\", num_ctas_list)\n def test_inline_asm_packed(num_ctas, device):\n     check_cuda_only(device)\n-    \n+\n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n         x = tl.load(X + tl.arange(0, BLOCK))"}, {"filename": "python/test/unit/language/test_line_info.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -116,5 +116,3 @@ def test_line_info(func: str):\n         assert (check_file_lines(file_lines, \"standard.py\", 33))\n         assert (check_file_lines(file_lines, \"standard.py\", 34))\n         assert (check_file_lines(file_lines, \"standard.py\", 36))\n-        # core.py is changed frequently, so we only check if it exists\n-        assert (check_file_lines(file_lines, \"core.py\", -1))"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 53, "deletions": 40, "changes": 93, "file_content_changes": "@@ -26,61 +26,61 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\n-    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE\",\n+    \"BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE, ALLOW_TF32\",\n     itertools.chain(\n         *[\n             [\n                 # 1 warp\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 16, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 32, 16, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 16, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 32, 32, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 16, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (16, 64, 64, 1, 1, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 2 warp\n-                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (64, 32, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 64, 64, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 32, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 64, 16, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 32, 1, 2, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 4 warp\n-                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 64, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (64, 128, 16, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 32, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (128, 32, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (32, 128, 64, 1, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # 8 warp\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n+                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE, True),\n                 # variable input\n-                (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE),\n-                (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE, True),\n+                (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE, True),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n         *[\n             [\n-                (16, 16, 16, 1, 1, STAGES, 32, 32, 80, AT, BT, DTYPE, DTYPE),\n-                (64, 32, 64, 1, 2, STAGES, 128, 64, 128, AT, BT, DTYPE, DTYPE),\n-                (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE),\n-                (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE),\n+                (16, 16, 16, 1, 1, STAGES, 32, 32, 80, AT, BT, DTYPE, DTYPE, True),\n+                (64, 32, 64, 1, 2, STAGES, 128, 64, 128, AT, BT, DTYPE, DTYPE, True),\n+                (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE, True),\n+                (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE, True),\n+                (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE, True),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [4]\n         ],\n         # mixed-precision\n         *[\n             [\n-                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, True),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, True),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE, True),\n             ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float8e5\"),\n                                      (\"float8e4nv\", \"float8e4nv\"),\n                                      (\"float8e5\", \"float8e4nv\"),\n@@ -92,10 +92,23 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                                      (\"bfloat16\", \"float32\"),\n                                      (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n         ],\n+        # mixed-precision block layout\n+        *[\n+            [\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, False),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE, False),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE, False),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4nv\", \"float16\"),\n+                                     (\"float16\", \"float8e5\"),\n+                                     (\"float16\", \"float32\"),\n+                                     (\"float32\", \"float16\"),\n+                                     (\"bfloat16\", \"float32\"),\n+                                     (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n+        ],\n         *[\n             # float8e4b15 only supports row-col layout\n             [\n-                (128, 128, 32, 1, 4, 2, None, None, None, False, True, ADTYPE, BDTYPE),\n+                (128, 128, 32, 1, 4, 2, None, None, None, False, True, ADTYPE, BDTYPE, True),\n             ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n                                      (\"float8e4b15\", \"float16\"),\n                                      (\"float16\", \"float8e4b15\"),\n@@ -105,7 +118,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n         ]\n     ),\n )\n-def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE):\n+def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT, ADTYPE, BDTYPE, ALLOW_TF32):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n@@ -173,7 +186,7 @@ def init_input(m, n, dtype):\n             a = triton.reinterpret(a, getattr(tl, ADTYPE))\n         if b_fp8:\n             b = triton.reinterpret(b, getattr(tl, BDTYPE))\n-        tt_c = triton.ops.matmul(a, b)\n+        tt_c = triton.ops.matmul(a, b, None, ALLOW_TF32)\n         torch.testing.assert_allclose(th_c, tt_c, atol=0, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/test/unit/tools/test_aot.py", "status": "modified", "additions": 138, "deletions": 37, "changes": 175, "file_content_changes": "@@ -23,26 +23,43 @@ def mul(x, y):\n import kernel_utils\n \n @triton.jit\n-def kernel(C, A, B,\n+def kernel(C, A, B, M, N, K,\n           stride_cm, stride_cn,\n           stride_am, stride_ak,\n           stride_bk, stride_bn,\n           BLOCK_M: tl.constexpr,\n           BLOCK_N: tl.constexpr,\n           BLOCK_K: tl.constexpr):\n-  ms = tl.arange(0, BLOCK_M)\n-  ns = tl.arange(0, BLOCK_N)\n-  ks = tl.arange(0, BLOCK_K)\n-  a = tl.load(A + ms[:, None] * stride_am + ks[None, :] * stride_ak)\n-  b = tl.load(B + ks[:, None] * stride_bk + ns[None, :] * stride_bn)\n-  c = tl.dot(a, b)\n-  c = kernel_utils.mul(c, c)\n-  tl.store(C + ms[:, None] * stride_cm + ns[None, :] * stride_cn, c)\n+  pid_m = tl.program_id(0)\n+  pid_n = tl.program_id(1)\n+\n+  offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n+  offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n+  offs_k = tl.arange(0, BLOCK_K)\n+  a_ptrs = A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+  b_ptrs = B + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+  accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n+  for k in range(0, tl.cdiv(K, BLOCK_K)):\n+      # Load the next block of A and B, generate a mask by checking the K dimension.\n+      # If it is out of bounds, set it to 0.\n+      a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_K, other=0.0)\n+      b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)\n+      # We accumulate along the K dimension.\n+      accumulator += tl.dot(a, b)\n+      # Advance the ptrs to the next K block.\n+      a_ptrs += BLOCK_K * stride_ak\n+      b_ptrs += BLOCK_K * stride_bk\n+\n+  c = kernel_utils.mul(accumulator, accumulator)\n+  # Write back the block of the output matrix C with masks.\n+  offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+  offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n+  c_ptrs = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n+  tl.store(c_ptrs, c)\n \"\"\"\n \n-\n-def gen_test_bin(dir, M, N, K, BM, BN, BK):\n-    test_src = '''\n+test_utils_src = '''\n #include <cuda.h>\n #include <stdio.h>\n #include <stdint.h>\n@@ -78,10 +95,23 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n     fclose(file);\n }'''\n \n-    test_src += f'''\n+\n+def gen_kernel_library(dir, libname):\n+    c_files = glob.glob(os.path.join(dir, \"*.c\"))\n+    subprocess.run([\"gcc\"] + c_files + [\"-I\", cuda_include_dir(),\n+                                        \"-c\", \"-fPIC\"],\n+                   check=True, cwd=dir)\n+    o_files = glob.glob(os.path.join(dir, \"*.o\"))\n+    subprocess.run([\"gcc\"] + o_files + [\"-shared\",\n+                                        \"-o\", libname,\n+                                        \"-L\", libcuda_dirs()[0]],\n+                   check=True, cwd=dir)\n+\n+\n+def gen_test_bin(dir, M, N, K, exe=\"test\", algo_id=0):\n+    test_src = f'''\n int main(int argc, char **argv) {{\n   int M = {M}, N = {N}, K = {K};\n-  int BM = {M}, BN = {N}, BK = {K};\n \n   // initialize CUDA handles\n   CUdevice dev;\n@@ -96,7 +126,7 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n   cuMemAlloc(&B, K * N * 2);\n   cuMemAlloc(&C, M * N * 4);\n   cuStreamCreate(&stream, 0);\n-  load_matmul_fp16xfp16_16x16x16();\n+  load_matmul_fp16();\n \n   // initialize input data\n   int16_t hA[M*K];\n@@ -110,7 +140,13 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n \n   // launch kernel\n   cuStreamSynchronize(stream);\n-  CUresult ret = matmul_fp16xfp16_16x16x16(stream, M/BM, N/BN, 1, C, A, B, N, 1, K, 1, N, 1);\n+  CUresult ret;\n+  int algo_id = {algo_id};\n+  if (algo_id == 0) {{\n+    ret = matmul_fp16_default(stream, C, A, B, M, N, K, N, 1, K, 1, N, 1);\n+  }} else {{\n+    ret = matmul_fp16(stream, C, A, B, M, N, K, N, 1, K, 1, N, 1, {algo_id});\n+  }}\n   if (ret != 0) fprintf(stderr, \"kernel launch failed\\\\n\");\n   assert(ret == 0);\n \n@@ -123,41 +159,51 @@ def gen_test_bin(dir, M, N, K, BM, BN, BK):\n   write_buffer_to_csv(argv[3], hC, M*N);\n \n   // free cuda handles\n-  unload_matmul_fp16xfp16_16x16x16();\n+  unload_matmul_fp16();\n   cuMemFree(A);\n   cuMemFree(B);\n   cuMemFree(C);\n   cuCtxDestroy(ctx);\n }}\n '''\n-\n+    src = test_utils_src + test_src\n     with open(os.path.join(dir, \"test.c\"), \"w\") as file:\n-        file.write(test_src)\n-    c_files = glob.glob(os.path.join(dir, \"*.c\"))\n-    subprocess.run([\"gcc\"] + c_files + [\"-I\", cuda_include_dir(),\n-                                        \"-L\", libcuda_dirs()[0],\n-                                        \"-l\", \"cuda\",\n-                                        \"-o\", \"test\"], check=True, cwd=dir)\n+        file.write(src)\n+    subprocess.run([\"gcc\"] + [\"test.c\",\n+                              \"-I\", cuda_include_dir(),\n+                              \"-L\", libcuda_dirs()[0],\n+                              \"-l\", \"cuda\",\n+                              \"-L\", dir,\n+                              \"-l\", \"kernel\",\n+                              \"-o\", exe], check=True, cwd=dir)\n \n \n-def generate_matmul_launcher(dir, dtype, BM, BN, BK, ha_hb_hints):\n+def write_triton_kernels(dir, src, util_src):\n     kernel_path = os.path.join(dir, \"kernel.py\")\n     with open(kernel_path, \"w\") as file:\n-        file.write(kernel_src)\n+        file.write(src)\n \n     kernel_utils_path = os.path.join(dir, \"kernel_utils.py\")\n     with open(kernel_utils_path, \"w\") as file:\n-        file.write(kernel_utils_src)\n+        file.write(util_src)\n \n+    return kernel_path\n+\n+\n+def compile_aot_kernels(dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints):\n     compiler_path = os.path.join(triton.tools.__path__[0], \"compile.py\")\n-    linker_path = os.path.join(triton.tools.__path__[0], \"link.py\")\n \n     # compile all desired configs\n     for ha in ha_hb_hints:\n         for hb in ha_hb_hints:\n-            sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32{ha}, i32:1, i32{hb}, i32:1, i32:16, i32:1, {BM}, {BN}, {BK}'\n-            name = f\"matmul_{dtype}x{dtype}_{BM}x{BN}x{BK}\"\n-            subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", kernel_path], check=True, cwd=dir)\n+            sig = f'*fp32:16, *{dtype}:16, *{dtype}:16, i32, i32, i32, i32{ha}, i32:1, i32{hb}, i32:1, i32:16, i32:1, {BM}, {BN}, {BK}'\n+            name = f\"matmul_{dtype}\"\n+            grid = f'M/{BM}, N/{BN}, 1'\n+            subprocess.run([sys.executable, compiler_path, \"-n\", \"kernel\", \"--signature\", sig, \"--out-name\", name, \"-o\", name, \"-w\", \"1\", \"-g\", grid, kernel_path], check=True, cwd=dir)\n+\n+\n+def link_aot_kernels(dir):\n+    linker_path = os.path.join(triton.tools.__path__[0], \"link.py\")\n \n     # link all desired configs\n     h_files = glob.glob(os.path.join(dir, \"*.h\"))\n@@ -183,17 +229,22 @@ def test_compile_link_matmul():\n         dtype = \"fp16\"\n         BM, BN, BK = 16, 16, 16\n \n-        generate_matmul_launcher(tmp_dir, dtype, BM, BN, BK, ha_hb_hints=[\"\", \":16\"])\n+        kernel_path = write_triton_kernels(tmp_dir, kernel_src, kernel_utils_src)\n+        compile_aot_kernels(tmp_dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints=[\"\", \":16\"])\n+        link_aot_kernels(tmp_dir)\n \n         # compile test case\n         M, N, K = 16, 16, 16\n-        gen_test_bin(tmp_dir, M, N, K, BM, BN, BK)\n+        gen_kernel_library(tmp_dir, \"libkernel.so\")\n+        gen_test_bin(tmp_dir, M, N, K)\n \n         # initialize test data\n         a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n \n         # run test case\n-        subprocess.run([\"./test\", a_path, b_path, c_path], check=True, cwd=tmp_dir)\n+        env = os.environ.copy()\n+        env[\"LD_LIBRARY_PATH\"] = tmp_dir\n+        subprocess.run([\"./test\", a_path, b_path, c_path], env=env, check=True, cwd=tmp_dir)\n \n         # read data and compare against reference\n         c = np.genfromtxt(c_path, delimiter=\",\", dtype=np.int32)\n@@ -209,23 +260,73 @@ def test_launcher_has_no_available_kernel():\n         dtype = \"fp16\"\n         BM, BN, BK = 16, 16, 16\n \n-        generate_matmul_launcher(tmp_dir, dtype, BM, BN, BK, ha_hb_hints=[\":1\"])\n+        kernel_path = write_triton_kernels(tmp_dir, kernel_src, kernel_utils_src)\n+        compile_aot_kernels(tmp_dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints=[\":1\"])\n+        link_aot_kernels(tmp_dir)\n \n         # compile test case\n         M, N, K = 16, 16, 16\n-        gen_test_bin(tmp_dir, M, N, K, BM, BN, BK)\n+        gen_kernel_library(tmp_dir, \"libkernel.so\")\n+        gen_test_bin(tmp_dir, M, N, K)\n \n         # initialize test data\n         a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n \n         # run test case\n-        result = subprocess.run([\"./test\", a_path, b_path, c_path], cwd=tmp_dir, capture_output=True, text=True)\n+        env = os.environ.copy()\n+        env[\"LD_LIBRARY_PATH\"] = tmp_dir\n+        result = subprocess.run([\"./test\", a_path, b_path, c_path], env=env, cwd=tmp_dir, capture_output=True, text=True)\n \n         # It should fail since the launcher requires all the strides be 1 while they are not.\n         assert result.returncode == -6\n         assert \"kernel launch failed\" in result.stderr\n \n \n+def test_compile_link_autotune_matmul():\n+    np.random.seed(3)\n+\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+\n+        dtype = \"fp16\"\n+\n+        kernel_path = write_triton_kernels(tmp_dir, kernel_src, kernel_utils_src)\n+\n+        tile_sizes = [\n+            [16, 16, 16],\n+            [32, 32, 16],\n+            [32, 32, 32],\n+            [64, 64, 32],\n+        ]\n+\n+        for ts in tile_sizes:\n+            BM, BN, BK = ts[0], ts[1], ts[2]\n+            compile_aot_kernels(tmp_dir, kernel_path, dtype, BM, BN, BK, ha_hb_hints=[\"\", \":16\"])\n+\n+        link_aot_kernels(tmp_dir)\n+\n+        gen_kernel_library(tmp_dir, \"libkernel.so\")\n+\n+        # compile test case\n+        M, N, K = 64, 64, 64\n+        # initialize test data\n+        a, b, a_path, b_path, c_path = generate_matmul_test_data(tmp_dir, M, N, K)\n+        c_ref = np.matmul(a.astype(np.float32), b.astype(np.float32))\n+\n+        for algo_id in range(len(tile_sizes)):\n+            # generate and run test case\n+            test_name = f\"test_{algo_id}\"\n+            gen_test_bin(tmp_dir, M, N, K, exe=test_name, algo_id=algo_id)\n+\n+            env = os.environ.copy()\n+            env[\"LD_LIBRARY_PATH\"] = tmp_dir\n+            subprocess.run([f\"./{test_name}\", a_path, b_path, c_path], check=True, cwd=tmp_dir, env=env)\n+\n+            # read data and compare against reference\n+            c = np.genfromtxt(c_path, delimiter=\",\", dtype=np.int32)\n+            c_tri = c.reshape((M, N)).view(np.float32)\n+            np.testing.assert_allclose(c_tri, c_ref * c_ref, atol=1e-4, rtol=1e-4)\n+\n+\n def test_ttgir_to_ptx():\n     src = \"\"\"\n module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32, \"triton_gpu.num-ctas\" = 1 : i32} {"}, {"filename": "python/triton/compiler/__init__.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -1,4 +1,5 @@\n-from .compiler import CompiledKernel, compile, instance_descriptor\n+from .compiler import (CompiledKernel, compile, get_arch_default_num_stages,\n+                       get_arch_default_num_warps, instance_descriptor)\n from .errors import CompilationError\n \n-__all__ = [\"compile\", \"instance_descriptor\", \"CompiledKernel\", \"CompilationError\"]\n+__all__ = [\"compile\", \"instance_descriptor\", \"CompiledKernel\", \"CompilationError\", \"get_arch_default_num_warps\", \"get_arch_default_num_stages\"]"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 35, "deletions": 3, "changes": 38, "file_content_changes": "@@ -368,6 +368,32 @@ def get_architecture_descriptor(capability):\n     return capability\n \n \n+def get_arch_default_num_warps(device_type):\n+    if device_type in [\"cuda\", \"hip\"]:\n+        num_warps = 4\n+    else:\n+        _device_backend = get_backend(device_type)\n+        assert _device_backend\n+        arch = _device_backend.get_architecture_descriptor()\n+        num_warps = arch[\"num_warps\"]\n+\n+    return num_warps\n+\n+\n+def get_arch_default_num_stages(device_type, capability=None):\n+    if device_type in [\"cuda\", \"hip\"]:\n+        arch = get_architecture_descriptor(capability)\n+        is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n+        num_stages = 3 if is_cuda and arch >= 75 else 2\n+    else:\n+        _device_backend = get_backend(device_type)\n+        assert _device_backend\n+        arch = _device_backend.get_architecture_descriptor()\n+        num_stages = arch[\"num_stages\"]\n+\n+    return num_stages\n+\n+\n def add_rocm_stages(arch, extern_libs, stages):\n     extern_libs.update(get_amdgcn_bitcode_paths(arch))\n \n@@ -397,9 +423,10 @@ def compile(fn, **kwargs):\n     # Get device type to decide which backend should be used\n     device_type = kwargs.get(\"device_type\", \"cuda\")\n     _device_backend = get_backend(device_type)\n+    capability = kwargs.get(\"cc\", None)\n \n     if device_type in [\"cuda\", \"hip\"]:\n-        arch = get_architecture_descriptor(kwargs.get(\"cc\", None))\n+        arch = get_architecture_descriptor(capability)\n     else:\n         _device_backend = get_backend(device_type)\n         assert _device_backend\n@@ -409,9 +436,10 @@ def compile(fn, **kwargs):\n     is_hip = device_type in [\"cuda\", \"hip\"] and not is_cuda\n     context = ir.context()\n     constants = kwargs.get(\"constants\", dict())\n-    num_warps = kwargs.get(\"num_warps\", 4)\n+    num_warps = kwargs.get(\"num_warps\", get_arch_default_num_warps(device_type))\n+    assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n     num_ctas = kwargs.get(\"num_ctas\", 1)\n-    num_stages = kwargs.get(\"num_stages\", 3 if is_cuda and arch >= 75 else 2)\n+    num_stages = kwargs.get(\"num_stages\", get_arch_default_num_stages(device_type, capability=capability))\n     # TODO[shuhaoj]: Default should be to enable warp specialization once possible\n     enable_warp_specialization = kwargs.get(\"enable_warp_specialization\", False)\n     # TODO[shuhaoj]: persistent can be decoupled with warp specialization\n@@ -445,6 +473,10 @@ def compile(fn, **kwargs):\n     elif is_hip:\n         add_rocm_stages(arch, extern_libs, stages)\n     else:\n+        # pass the user's configuration to the backend device.\n+        arch[\"num_warps\"] = num_warps\n+        arch[\"num_stages\"] = num_stages\n+        arch[\"num_ctas\"] = num_ctas\n         _device_backend.add_stages(arch, extern_libs, stages)\n \n     # find out the signature of the function"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 14, "deletions": 13, "changes": 27, "file_content_changes": "@@ -183,6 +183,7 @@ def format_of(ty):\n       }}\n \n       PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n+      ptr_info.valid = false;\n       return ptr_info;\n     }}\n \n@@ -202,17 +203,17 @@ def format_of(ty):\n         return NULL;\n       }}\n \n-      if (launch_enter_hook != Py_None) {{\n-        PyObject_CallObject(launch_enter_hook, args);\n+      if (launch_enter_hook != Py_None && !PyObject_CallObject(launch_enter_hook, args)) {{\n+        return NULL;\n       }}\n \n       // raise exception asap\n       {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n+      Py_BEGIN_ALLOW_THREADS;\n       _launch(gridX, gridY, gridZ, num_warps, shared_memory, (hipStream_t)_stream, (hipFunction_t)_function{', ' + ', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\" for i, ty in signature.items()) if len(signature) > 0 else ''});\n-      if (launch_exit_hook != Py_None) {{\n-        PyObject_CallObject(launch_exit_hook, args);\n-      }}\n-      if (PyErr_Occurred()) {{\n+      Py_END_ALLOW_THREADS;\n+\n+      if (launch_exit_hook != Py_None && !PyObject_CallObject(launch_exit_hook, args)) {{\n         return NULL;\n       }}\n \n@@ -365,6 +366,7 @@ def format_of(ty):\n     return ptr_info;\n   }}\n   PyErr_SetString(PyExc_TypeError, \"Pointer argument must be either uint64 or have data_ptr method\");\n+  ptr_info.valid = false;\n   return ptr_info;\n }}\n \n@@ -386,22 +388,21 @@ def format_of(ty):\n     return NULL;\n   }}\n \n-  if (launch_enter_hook != Py_None) {{\n-    PyObject_CallObject(launch_enter_hook, args);\n+  if (launch_enter_hook != Py_None && !PyObject_CallObject(launch_enter_hook, args)) {{\n+    return NULL;\n   }}\n \n \n   // raise exception asap\n   {\"; \".join([f\"DevicePtrInfo ptr_info{i} = getPointer(_arg{i}, {i}); if (!ptr_info{i}.valid) return NULL;\" if ty[0] == \"*\" else \"\" for i, ty in signature.items()])};\n+  Py_BEGIN_ALLOW_THREADS;\n   _launch(gridX, gridY, gridZ, num_warps, num_ctas, clusterDimX, clusterDimY, clusterDimZ, shared_memory, (CUstream)_stream, (CUfunction)_function{', ' + ', '.join(f\"ptr_info{i}.dev_ptr\" if ty[0]==\"*\" else f\"_arg{i}\"for i, ty in signature.items()) if len(signature) > 0 else ''});\n+  Py_END_ALLOW_THREADS;\n \n-  if (launch_exit_hook != Py_None) {{\n-    PyObject_CallObject(launch_exit_hook, args);\n-  }}\n-\n-  if(PyErr_Occurred()) {{\n+  if (launch_exit_hook != Py_None && !PyObject_CallObject(launch_exit_hook, args)) {{\n     return NULL;\n   }}\n+\n   // return None\n   Py_INCREF(Py_None);\n   return Py_None;"}, {"filename": "python/triton/hopper_lib/libhopper_helpers.bc", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -4,11 +4,21 @@\n from . import math\n from . import extra\n from .standard import (\n+    argmax,\n+    argmin,\n     cdiv,\n+    cumprod,\n+    cumsum,\n+    max,\n+    maximum,\n+    min,\n+    minimum,\n     sigmoid,\n     softmax,\n+    sum,\n     ravel,\n     swizzle2d,\n+    xor_sum,\n     zeros,\n     zeros_like,\n )\n@@ -17,8 +27,6 @@\n     abs,\n     advance,\n     arange,\n-    argmin,\n-    argmax,\n     associative_scan,\n     atomic_add,\n     atomic_and,\n@@ -35,8 +43,6 @@\n     cat,\n     constexpr,\n     cos,\n-    cumprod,\n-    cumsum,\n     debug_barrier,\n     device_assert,\n     device_print,\n@@ -63,12 +69,8 @@\n     load,\n     log,\n     make_block_ptr,\n-    max,\n     max_constancy,\n     max_contiguous,\n-    maximum,\n-    min,\n-    minimum,\n     multiple_of,\n     num_programs,\n     pi32_t,\n@@ -81,7 +83,6 @@\n     static_assert,\n     static_print,\n     store,\n-    sum,\n     static_range,\n     tensor,\n     trans,\n@@ -94,7 +95,6 @@\n     view,\n     void,\n     where,\n-    xor_sum,\n )\n from .random import (\n     pair_uniform_to_normal,"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 5, "deletions": 191, "changes": 196, "file_content_changes": "@@ -6,8 +6,7 @@\n from typing import Callable, List, Sequence, TypeVar\n \n from .._C.libtriton.triton import ir\n-from ..runtime.jit import jit\n-from . import math, semantic\n+from . import semantic\n \n T = TypeVar('T')\n \n@@ -205,6 +204,10 @@ def is_int(self):\n     def is_bool(self):\n         return self.is_int1()\n \n+    @staticmethod\n+    def is_dtype(type_str):\n+        return type_str in dtype.SINT_TYPES + dtype.UINT_TYPES + dtype.FP_TYPES + dtype.OTHER_TYPES\n+\n     @staticmethod\n     def is_void():\n         raise RuntimeError(\"Not implemented\")\n@@ -1412,170 +1415,6 @@ def _reduce_with_indices(input, axis, combine_fn, _builder=None, _generator=None\n     return rvalue, rindices\n \n \n-@jit\n-def minimum(x, y):\n-    \"\"\"\n-    Computes the element-wise minimum of :code:`x` and :code:`y`.\n-\n-    :param input: the first input tensor\n-    :type input: Block\n-    :param other: the second input tensor\n-    :type other: Block\n-    \"\"\"\n-    return math.min(x, y)\n-\n-\n-@jit\n-def maximum(x, y):\n-    \"\"\"\n-    Computes the element-wise maximum of :code:`x` and :code:`y`.\n-\n-    :param input: the first input tensor\n-    :type input: Block\n-    :param other: the second input tensor\n-    :type other: Block\n-    \"\"\"\n-    return math.max(x, y)\n-\n-# max and argmax\n-\n-\n-@jit\n-def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n-    if tie_break_left:\n-        tie = value1 == value2 and index1 < index2\n-    else:\n-        tie = False\n-    gt = value1 > value2 or tie\n-    v_ret = where(gt, value1, value2)\n-    i_ret = where(gt, index1, index2)\n-    return v_ret, i_ret\n-\n-\n-@jit\n-def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n-    return _argmax_combine(value1, index1, value2, index2, True)\n-\n-\n-@jit\n-def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n-    return _argmax_combine(value1, index1, value2, index2, False)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"maximum\",\n-                       return_indices_arg=\"return_indices\",\n-                       tie_break_arg=\"return_indices_tie_break_left\")\n-def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n-    input = _promote_reduction_input(input)\n-    if return_indices:\n-        if return_indices_tie_break_left:\n-            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n-        else:\n-            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n-    else:\n-        if constexpr(input.dtype.primitive_bitwidth) < 32:\n-            if constexpr(input.dtype.is_floating()):\n-                input = input.to(float32)\n-            else:\n-                assert input.dtype.is_integer_type()\n-                input = input.to(int32)\n-        return reduce(input, axis, maximum)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n-def argmax(input, axis, tie_break_left=True):\n-    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n-    return ret\n-\n-# min and argmin\n-\n-\n-@jit\n-def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n-    if tie_break_left:\n-        tie = value1 == value2 and index1 < index2\n-    else:\n-        tie = False\n-    lt = value1 < value2 or tie\n-    value_ret = where(lt, value1, value2)\n-    index_ret = where(lt, index1, index2)\n-    return value_ret, index_ret\n-\n-\n-@jit\n-def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n-    return _argmin_combine(value1, index1, value2, index2, True)\n-\n-\n-@jit\n-def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n-    return _argmin_combine(value1, index1, value2, index2, False)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"minimum\",\n-                       return_indices_arg=\"return_indices\",\n-                       tie_break_arg=\"return_indices_tie_break_left\")\n-def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n-    input = _promote_reduction_input(input)\n-    if return_indices:\n-        if return_indices_tie_break_left:\n-            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n-        else:\n-            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n-    else:\n-        if constexpr(input.dtype.primitive_bitwidth) < 32:\n-            if constexpr(input.dtype.is_floating()):\n-                input = input.to(float32)\n-            else:\n-                assert input.dtype.is_integer_type()\n-                input = input.to(int32)\n-        return reduce(input, axis, minimum)\n-\n-\n-@jit\n-@_add_reduction_docstr(\"minimum index\",\n-                       tie_break_arg=\"tie_break_left\")\n-def argmin(input, axis, tie_break_left=True):\n-    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n-    return ret\n-\n-\n-@jit\n-def _sum_combine(a, b):\n-    return a + b\n-\n-# sum\n-\n-\n-@jit\n-@_add_reduction_docstr(\"sum\")\n-def sum(input, axis=None):\n-    input = _promote_reduction_input(input)\n-    return reduce(input, axis, _sum_combine)\n-\n-\n-@jit\n-def _xor_combine(a, b):\n-    return a ^ b\n-\n-\n-# xor sum\n-\n-@builtin\n-@_add_reduction_docstr(\"xor sum\")\n-def xor_sum(input, axis=None, _builder=None, _generator=None):\n-    scalar_ty = input.type.scalar\n-    if not scalar_ty.is_int():\n-        raise ValueError(\"xor_sum only supported for integers\")\n-\n-    input = _promote_reduction_input(input, _builder=_builder)\n-    return reduce(input, axis, _xor_combine,\n-                  _builder=_builder, _generator=_generator)\n-\n-\n # -----------------------\n # Scans\n # -----------------------\n@@ -1626,31 +1465,6 @@ def make_combine_region(scan_op):\n     axis = _constexpr_to_value(axis)\n     return semantic.associative_scan(input, axis, make_combine_region, _builder)\n \n-# cumsum\n-\n-\n-@jit\n-@_add_scan_docstr(\"cumsum\")\n-def cumsum(input, axis=0):\n-    # todo rename this to a generic function name\n-    input = _promote_reduction_input(input)\n-    return associative_scan(input, axis, _sum_combine)\n-\n-# cumprod\n-\n-\n-@jit\n-def _prod_combine(a, b):\n-    return a * b\n-\n-\n-@jit\n-@_add_scan_docstr(\"cumprod\")\n-def cumprod(input, axis=0):\n-    # todo rename this to a generic function name\n-    input = _promote_reduction_input(input)\n-    return associative_scan(input, axis, _prod_combine)\n-\n # -----------------------\n # Compiler Hint Ops\n # -----------------------"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -1,5 +1,6 @@\n from ..runtime.jit import jit\n from . import core as tl\n+from . import standard\n \n PHILOX_KEY_A: tl.constexpr = 0x9E3779B9\n PHILOX_KEY_B: tl.constexpr = 0xBB67AE85\n@@ -141,7 +142,7 @@ def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n @jit\n def pair_uniform_to_normal(u1, u2):\n     \"\"\"Box-Muller transform\"\"\"\n-    u1 = tl.maximum(1.0e-7, u1)\n+    u1 = standard.maximum(1.0e-7, u1)\n     th = 6.283185307179586 * u2\n     r = tl.sqrt(-2.0 * tl.log(u1))\n     return r * tl.cos(th), r * tl.sin(th)"}, {"filename": "python/triton/language/standard.py", "status": "modified", "additions": 193, "deletions": 4, "changes": 197, "file_content_changes": "@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from ..runtime.jit import jit\n-from . import core\n+from . import core, math\n \n # -----------------------\n # Standard library\n@@ -30,9 +30,9 @@ def sigmoid(x):\n @jit\n @core._add_math_1arg_docstr(\"softmax\")\n def softmax(x, ieee_rounding=False):\n-    z = x - core.max(x, 0)\n+    z = x - max(x, 0)\n     num = core.exp(z)\n-    den = core.sum(num, 0)\n+    den = sum(num, 0)\n     return core.fdiv(num, den, ieee_rounding)\n \n \n@@ -73,7 +73,7 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n     # row-index of the first element of this group\n     off_i = group_id * size_g\n     # last group may have fewer rows\n-    size_g = core.minimum(size_i - off_i, size_g)\n+    size_g = minimum(size_i - off_i, size_g)\n     # new row and column indices\n     new_i = off_i + (ij % size_g)\n     new_j = (ij % size_gj) // size_g\n@@ -96,3 +96,192 @@ def zeros(shape, dtype):\n @jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)\n+\n+\n+@jit\n+def minimum(x, y):\n+    \"\"\"\n+    Computes the element-wise minimum of :code:`x` and :code:`y`.\n+\n+    :param input: the first input tensor\n+    :type input: Block\n+    :param other: the second input tensor\n+    :type other: Block\n+    \"\"\"\n+    return math.min(x, y)\n+\n+\n+@jit\n+def maximum(x, y):\n+    \"\"\"\n+    Computes the element-wise maximum of :code:`x` and :code:`y`.\n+\n+    :param input: the first input tensor\n+    :type input: Block\n+    :param other: the second input tensor\n+    :type other: Block\n+    \"\"\"\n+    return math.max(x, y)\n+\n+# max and argmax\n+\n+\n+@jit\n+def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    gt = value1 > value2 or tie\n+    v_ret = core.where(gt, value1, value2)\n+    i_ret = core.where(gt, index1, index2)\n+    return v_ret, i_ret\n+\n+\n+@jit\n+def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"maximum\",\n+                            return_indices_arg=\"return_indices\",\n+                            tie_break_arg=\"return_indices_tie_break_left\")\n+def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n+    input = core._promote_reduction_input(input)\n+    if return_indices:\n+        if return_indices_tie_break_left:\n+            return core._reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n+        else:\n+            return core._reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n+    else:\n+        if core.constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if core.constexpr(input.dtype.is_floating()):\n+                input = input.to(core.float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(core.int32)\n+        return core.reduce(input, axis, maximum)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n+def argmax(input, axis, tie_break_left=True):\n+    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n+    return ret\n+\n+# min and argmin\n+\n+\n+@jit\n+def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    lt = value1 < value2 or tie\n+    value_ret = core.where(lt, value1, value2)\n+    index_ret = core.where(lt, index1, index2)\n+    return value_ret, index_ret\n+\n+\n+@jit\n+def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"minimum\",\n+                            return_indices_arg=\"return_indices\",\n+                            tie_break_arg=\"return_indices_tie_break_left\")\n+def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n+    input = core._promote_reduction_input(input)\n+    if return_indices:\n+        if return_indices_tie_break_left:\n+            return core._reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n+        else:\n+            return core._reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n+    else:\n+        if core.constexpr(input.dtype.primitive_bitwidth) < 32:\n+            if core.constexpr(input.dtype.is_floating()):\n+                input = input.to(core.float32)\n+            else:\n+                assert input.dtype.is_integer_type()\n+                input = input.to(core.int32)\n+        return core.reduce(input, axis, minimum)\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"minimum index\",\n+                            tie_break_arg=\"tie_break_left\")\n+def argmin(input, axis, tie_break_left=True):\n+    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n+    return ret\n+\n+\n+@jit\n+def _sum_combine(a, b):\n+    return a + b\n+\n+# sum\n+\n+\n+@jit\n+@core._add_reduction_docstr(\"sum\")\n+def sum(input, axis=None):\n+    input = core._promote_reduction_input(input)\n+    return core.reduce(input, axis, _sum_combine)\n+\n+\n+@jit\n+def _xor_combine(a, b):\n+    return a ^ b\n+\n+# xor sum\n+\n+\n+@core.builtin\n+@core._add_reduction_docstr(\"xor sum\")\n+def xor_sum(input, axis=None, _builder=None, _generator=None):\n+    scalar_ty = input.type.scalar\n+    if not scalar_ty.is_int():\n+        raise ValueError(\"xor_sum only supported for integers\")\n+\n+    input = core._promote_reduction_input(input, _builder=_builder)\n+    return core.reduce(input, axis, _xor_combine,\n+                       _builder=_builder, _generator=_generator)\n+\n+# cumsum\n+\n+\n+@jit\n+@core._add_scan_docstr(\"cumsum\")\n+def cumsum(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = core._promote_reduction_input(input)\n+    return core.associative_scan(input, axis, _sum_combine)\n+\n+# cumprod\n+\n+\n+@jit\n+def _prod_combine(a, b):\n+    return a * b\n+\n+\n+@jit\n+@core._add_scan_docstr(\"cumprod\")\n+def cumprod(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = core._promote_reduction_input(input)\n+    return core.associative_scan(input, axis, _prod_combine)"}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -81,6 +81,7 @@ def _kernel(A, B, C, M, N, K,\n             stride_bk, stride_bn,\n             stride_cm, stride_cn,\n             dot_out_dtype: tl.constexpr,\n+            allow_tf32: tl.constexpr,\n             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n             GROUP_M: tl.constexpr, SPLIT_K: tl.constexpr, EVEN_K: tl.constexpr, AB_DTYPE: tl.constexpr\n             ):\n@@ -117,7 +118,7 @@ def _kernel(A, B, C, M, N, K,\n         if AB_DTYPE:\n             a = a.to(C.dtype.element_ty)\n             b = b.to(C.dtype.element_ty)\n-        acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n+        acc += tl.dot(a, b, out_dtype=dot_out_dtype, allow_tf32=allow_tf32)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n     acc = acc.to(C.dtype.element_ty)\n@@ -139,7 +140,7 @@ class _matmul(torch.autograd.Function):\n     _locks = {}\n \n     @staticmethod\n-    def _call(a, b, dot_out_dtype):\n+    def _call(a, b, dot_out_dtype, allow_tf32):\n         device = a.device\n         # handle non-contiguous inputs if necessary\n         if a.stride(0) > 1 and a.stride(1) > 1:\n@@ -180,12 +181,13 @@ def _call(a, b, dot_out_dtype):\n                       b.stride(0), b.stride(1),\n                       c.stride(0), c.stride(1),\n                       dot_out_dtype=dot_out_dtype,\n+                      allow_tf32=allow_tf32,\n                       GROUP_M=8, AB_DTYPE=ab_dtype)\n         return c\n \n     @staticmethod\n-    def forward(ctx, a, b, dot_out_dtype=None):\n-        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype)\n+    def forward(ctx, a, b, dot_out_dtype=None, allow_tf32=True):\n+        return _matmul._call(a, b, dot_out_dtype=dot_out_dtype, allow_tf32=allow_tf32)\n \n \n matmul = _matmul.apply"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 12, "deletions": 3, "changes": 15, "file_content_changes": "@@ -17,9 +17,7 @@ static inline void gpuAssert(CUresult code, const char *file, int line) {\n \n #define CUDA_CHECK(ans)                                                        \\\n   {                                                                            \\\n-    gpuAssert((ans), __FILE__, __LINE__);                                      \\\n-    if (PyErr_Occurred())                                                      \\\n-      return NULL;                                                             \\\n+    { gpuAssert((ans), __FILE__, __LINE__); }                                  \\\n   }\n \n #define ADD_ENUM_ITEM(value)                                                   \\\n@@ -234,6 +232,8 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n   int32_t n_spills = 0;\n   // create driver handles\n   CUcontext pctx = 0;\n+\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuCtxGetCurrent(&pctx));\n   if (!pctx) {\n     CUDA_CHECK(cuDevicePrimaryCtxRetain(&pctx, device));\n@@ -264,6 +264,7 @@ static PyObject *loadBinary(PyObject *self, PyObject *args) {\n         cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n                            shared_optin - shared_static));\n   }\n+  Py_END_ALLOW_THREADS;\n \n   if (PyErr_Occurred()) {\n     return NULL;\n@@ -281,7 +282,9 @@ static PyObject *memAlloc(PyObject *self, PyObject *args) {\n     return NULL; // Error parsing arguments\n   }\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemAlloc(&dptr, bytesize));\n+  Py_END_ALLOW_THREADS;\n \n   return PyLong_FromUnsignedLongLong((unsigned long long)dptr);\n }\n@@ -300,7 +303,9 @@ static PyObject *memcpyHtoD(PyObject *self, PyObject *args) {\n   dstDevice = (CUdeviceptr)dstDevicePtr;\n   srcHost = (const void *)srcHostPtr;\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemcpyHtoD(dstDevice, srcHost, byteCount));\n+  Py_END_ALLOW_THREADS;\n \n   Py_RETURN_NONE;\n }\n@@ -312,7 +317,9 @@ static PyObject *memFree(PyObject *self, PyObject *args) {\n     return NULL; // Error parsing arguments\n   }\n \n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuMemFree(dptr));\n+  Py_END_ALLOW_THREADS;\n \n   Py_RETURN_NONE;\n }\n@@ -400,10 +407,12 @@ static PyObject *tensorMapEncodeTiled(PyObject *self, PyObject *args) {\n     cuTensorMapEncodeTiledHandle = getCuTensorMapEncodeTiledHandle();\n   }\n   // Call the function\n+  Py_BEGIN_ALLOW_THREADS;\n   CUDA_CHECK(cuTensorMapEncodeTiledHandle(\n       tensorMap, tensorDataType, tensorRank, globalAddress, globalDim,\n       globalStrides, boxDim, elementStrides, interleave, swizzle, l2Promotion,\n       oobFill));\n+  Py_END_ALLOW_THREADS;\n \n   // Clean up\n   free(globalDim);"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 18, "deletions": 10, "changes": 28, "file_content_changes": "@@ -13,6 +13,7 @@\n \n from .._C.libtriton.triton import TMAInfos\n from ..common.backend import get_backend, path_to_ptxas\n+from ..language.core import dtype\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n TRITON_VERSION = \"2.1.0\"\n@@ -358,19 +359,16 @@ def _make_launcher(self):\n \n         spec_keys = ', '.join(specializations)\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n-        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n+        args_signature = ', '.join(name if dflt == inspect._empty else f'{name} = triton.language.dtype(\\'{dflt}\\')' if dtype.is_dtype(f'{dflt}') else f'{name} = {dflt}' for name, dflt in zip(self.arg_names, self.arg_defaults))\n         args_signature = args_signature + ', ' if len(args_signature) > 0 else ''\n \n         src = f\"\"\"\n-def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_stages=3, enable_warp_specialization=False, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n-    from ..compiler import compile, CompiledKernel\n+import triton\n+def {self.fn.__name__}({args_signature}grid=None, num_warps=None, num_ctas=1, num_stages=None, enable_warp_specialization=False, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n+    from ..compiler import compile, CompiledKernel, get_arch_default_num_warps, get_arch_default_num_stages\n     sig_key = {f'{sig_keys},' if len(sig_keys) > 0 else ()}\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n-    key = (version_key, sig_key, constexpr_key, spec_key, num_warps, num_ctas, num_stages, enable_warp_specialization, self.debug)\n-    if not extern_libs is None:\n-      key = (key, tuple(extern_libs.items()))\n-    assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n     assert num_ctas > 0\n     assert grid is not None\n     if callable(grid):\n@@ -403,6 +401,15 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_s\n         else:\n             stream = device_backend.get_stream()\n \n+    if num_warps is None:\n+        num_warps = get_arch_default_num_warps(device_type)\n+    if num_stages is None:\n+        num_stages = get_arch_default_num_stages(device_type)\n+\n+    key = (version_key, sig_key, constexpr_key, spec_key, num_warps, num_ctas, num_stages, enable_warp_specialization, self.debug)\n+    if not extern_libs is None:\n+      key = (key, tuple(extern_libs.items()))\n+\n     bin = cache[device].get(key, None)\n     if bin is not None:\n       # build dict of constant values\n@@ -461,9 +468,6 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         self.arg_names = [v.name for v in signature.parameters.values()]\n         self.arg_defaults = [v.default for v in signature.parameters.values()]\n         self.has_defaults = any(v != inspect._empty for v in self.arg_defaults)\n-        # specialization hints\n-        self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n-        self.do_not_specialize = {self.arg_names.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # function source code (without decorators)\n         self.src = textwrap.dedent(inspect.getsource(fn))\n         self.src = self.src[self.src.find(\"def\"):]\n@@ -480,6 +484,10 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         self.__annotations__ = {name: _normalize_ty(ty) for name, ty in fn.__annotations__.items()}\n         # index of constexprs\n         self.constexprs = [self.arg_names.index(name) for name, ty in self.__annotations__.items() if 'constexpr' in ty]\n+        # specialization hints\n+        regular_args = [arg for i, arg in enumerate(self.arg_names) if i not in self.constexprs]\n+        self.do_not_specialize = [] if do_not_specialize is None else do_not_specialize\n+        self.do_not_specialize = {regular_args.index(arg) if isinstance(arg, str) else arg for arg in self.do_not_specialize}\n         # tma info\n         self.tensormaps_info = TMAInfos()\n         # launcher"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 40, "deletions": 32, "changes": 72, "file_content_changes": "@@ -3,6 +3,7 @@\n import subprocess\n import sys\n from contextlib import contextmanager\n+from typing import Any, Dict, List\n \n from . import language as tl\n from ._C.libtriton.triton import runtime\n@@ -201,37 +202,41 @@ class Benchmark:\n \n     def __init__(\n         self,\n-        x_names,\n-        x_vals,\n-        line_arg,\n-        line_vals,\n-        line_names,\n-        plot_name,\n-        args,\n-        xlabel='',\n-        ylabel='',\n-        x_log=False,\n-        y_log=False,\n+        x_names: List[str],\n+        x_vals: List[Any],\n+        line_arg: str,\n+        line_vals: List[Any],\n+        line_names: List[str],\n+        plot_name: str,\n+        args: Dict[str, Any],\n+        xlabel: str = '',\n+        ylabel: str = '',\n+        x_log: bool = False,\n+        y_log: bool = False,\n         color=None,\n         styles=None,\n     ):\n         \"\"\"\n-        Constructor\n+        Constructor.\n+        x_vals can be a list of scalars or a list of tuples/lists. If x_vals is a list\n+        of scalars and there are multiple x_names, all arguments will have the same value.\n+        If x_vals is a list of tuples/lists, each element should have the same length as\n+        x_names.\n \n-        :param x_names: Name of the arguments that should appear on the x axis of the plot. If the list contains more than one element, all the arguments are assumed to have the same value.\n+        :param x_names: Name of the arguments that should appear on the x axis of the plot.\n         :type x_names: List[str]\n         :param x_vals: List of values to use for the arguments in :code:`x_names`.\n         :type x_vals: List[Any]\n         :param line_arg: Argument name for which different values correspond to different lines in the plot.\n         :type line_arg: str\n         :param line_vals: List of values to use for the arguments in :code:`line_arg`.\n-        :type line_vals: List[str]\n+        :type line_vals: List[Any]\n         :param line_names: Label names for the different lines.\n         :type line_names: List[str]\n         :param plot_name: Name of the plot.\n         :type plot_name: str\n-        :param args: List of arguments to remain fixed throughout the benchmark.\n-        :type args: List[str]\n+        :param args: Dictionary of keyword arguments to remain fixed throughout the benchmark.\n+        :type args: Dict[str, Any]\n         :param xlabel: Label for the x axis of the plot.\n         :type xlabel: str, optional\n         :param ylabel: Label for the y axis of the plot.\n@@ -261,23 +266,25 @@ def __init__(self, fn, benchmarks):\n         self.fn = fn\n         self.benchmarks = benchmarks\n \n-    def _run(self, bench, save_path, show_plots, print_data):\n+    def _run(self, bench: Benchmark, save_path: str, show_plots: bool, print_data: bool):\n         import os\n \n         import matplotlib.pyplot as plt\n         import pandas as pd\n         y_mean = bench.line_names\n         y_min = [f'{x}-min' for x in bench.line_names]\n         y_max = [f'{x}-max' for x in bench.line_names]\n-        x_names_str = str(bench.x_names)\n-        df = pd.DataFrame(columns=[x_names_str] + y_mean + y_min + y_max)\n+        x_names = list(bench.x_names)\n+        df = pd.DataFrame(columns=x_names + y_mean + y_min + y_max)\n         for x in bench.x_vals:\n-            if not isinstance(x, list):\n-                x = [x]\n-            if len(x) == 1:\n-                x = x * len(bench.x_names)\n-            x_str = str(x)\n-            x_args = {x_name: x_in for x_name, x_in in zip(bench.x_names, x)}\n+            # x can be a single value or a sequence of values.\n+            if not isinstance(x, (list, tuple)):\n+                x = [x for _ in x_names]\n+\n+            if len(x) != len(x_names):\n+                raise ValueError(f\"Expected {len(x_names)} values, got {x}\")\n+            x_args = dict(zip(x_names, x))\n+\n             row_mean, row_min, row_max = [], [], []\n             for y in bench.line_vals:\n                 ret = self.fn(**x_args, **{bench.line_arg: y}, **bench.args)\n@@ -288,23 +295,24 @@ def _run(self, bench, save_path, show_plots, print_data):\n                 row_mean += [y_mean]\n                 row_min += [y_min]\n                 row_max += [y_max]\n-            df.loc[len(df)] = [x_str] + row_mean + row_min + row_max\n+            df.loc[len(df)] = list(x) + row_mean + row_min + row_max\n+\n         if bench.plot_name:\n             plt.figure()\n             ax = plt.subplot()\n-            x = x_names_str\n+            # Plot first x value on x axis if there are multiple.\n+            first_x = x_names[0]\n             for i, y in enumerate(bench.line_names):\n                 y_min, y_max = df[y + '-min'], df[y + '-max']\n                 col = bench.styles[i][0] if bench.styles else None\n                 sty = bench.styles[i][1] if bench.styles else None\n-                ax.plot(df[x], df[y], label=y, color=col, ls=sty)\n+                ax.plot(df[first_x], df[y], label=y, color=col, ls=sty)\n                 if not y_min.isnull().all() and not y_max.isnull().all():\n                     y_min = y_min.astype(float)\n                     y_max = y_max.astype(float)\n-                    ax.fill_between(df[x], y_min, y_max, alpha=0.15, color=col)\n+                    ax.fill_between(df[first_x], y_min, y_max, alpha=0.15, color=col)\n             ax.legend()\n-            xlabel = bench.xlabel if bench.xlabel else \" = \".join(bench.x_names)\n-            ax.set_xlabel(xlabel)\n+            ax.set_xlabel(bench.xlabel or first_x)\n             ax.set_ylabel(bench.ylabel)\n             # ax.set_title(bench.plot_name)\n             ax.set_xscale(\"log\" if bench.x_log else \"linear\")\n@@ -313,7 +321,7 @@ def _run(self, bench, save_path, show_plots, print_data):\n                 plt.show()\n             if save_path:\n                 plt.savefig(os.path.join(save_path, f\"{bench.plot_name}.png\"))\n-        df = df[[x_names_str] + bench.line_names]\n+        df = df[x_names + bench.line_names]\n         if print_data:\n             print(bench.plot_name + ':')\n             print(df)"}, {"filename": "python/triton/tools/compile.c", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -54,9 +54,12 @@ void load_{kernel_name}() {{\n /*\n {kernel_docstring}\n */\n-CUresult {kernel_name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {signature}) {{\n+CUresult {kernel_name}(CUstream stream, {signature}) {{\n     if ({kernel_name}_func == NULL)\n        load_{kernel_name}();\n+    unsigned int gX = {gridX};\n+    unsigned int gY = {gridY};\n+    unsigned int gZ = {gridZ};\n     void *args[{num_args}] = {{ {arg_pointers} }};\n     // TODO: shared memory\n     if(gX * gY * gZ > 0)"}, {"filename": "python/triton/tools/compile.h", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -10,7 +10,5 @@\n \n void unload_{kernel_name}(void);\n void load_{kernel_name}(void);\n-// tt-linker: {kernel_name}:{full_signature}\n-CUresult{_placeholder} {kernel_name}(CUstream stream, unsigned int gX,\n-                                     unsigned int gY, unsigned int gZ,\n-                                     {signature});\n+// tt-linker: {kernel_name}:{full_signature}:{algo_info}\n+CUresult{_placeholder} {kernel_name}(CUstream stream, {signature});"}, {"filename": "python/triton/tools/compile.py", "status": "modified", "additions": 14, "deletions": 3, "changes": 17, "file_content_changes": "@@ -43,10 +43,11 @@\n     parser.add_argument(\"path\", help=\"Path to Python source containing desired kernel in its scope. File will be executed.\")\n     parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\", help=\"Name of the kernel to compile\", required=True)\n     parser.add_argument(\"--num-warps\", \"-w\", type=int, default=1, help=\"Number of warps to launch the kernel\")\n-    parser.add_argument(\"--num-stages\", \"-ns\", type=int, default=3, help=\"Number of stages meta-parameter for the kernel\")\n+    parser.add_argument(\"--num-stages\", \"-ns\", type=int, default=3, help=\"Number of stages (meta-parameter of the kernel)\")\n     parser.add_argument(\"--out-name\", \"-on\", type=str, default=None, help=\"Out name for the compiled kernel\")\n     parser.add_argument(\"--out-path\", \"-o\", type=Path, default=None, help=\"Out filename\")\n     parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\", required=True)\n+    parser.add_argument(\"--grid\", \"-g\", type=str, help=\"Launch grid of the kernel\", required=True)\n     args = parser.parse_args()\n \n     out_name = args.out_name if args.out_name else args.kernel_name\n@@ -59,6 +60,8 @@\n     mod = importlib.util.module_from_spec(spec)\n     spec.loader.exec_module(mod)\n     kernel = getattr(mod, args.kernel_name)\n+    grid = args.grid.split(\",\")\n+    assert len(grid) == 3\n \n     # validate and parse signature\n     signature = list(map(lambda s: s.strip(\" \"), args.signature.split(\",\")))\n@@ -68,7 +71,8 @@ def hash_signature(signature: List[str]):\n         m.update(\" \".join(signature).encode())\n         return m.hexdigest()[:8]\n \n-    sig_hash = hash_signature(signature)\n+    meta_sig = f\"warps{args.num_warps}xstages{args.num_stages}\"\n+    sig_hash = hash_signature(signature + [meta_sig])\n \n     def constexpr(s):\n         try:\n@@ -88,6 +92,9 @@ def constexpr(s):\n     constexprs = {i: constexpr(s) for i, s in enumerate(signature)}\n     constexprs = {k: v for k, v in constexprs.items() if v is not None}\n     signature = {i: s.split(\":\")[0] for i, s in enumerate(signature) if i not in constexprs}\n+    const_sig = 'x'.join([str(v) for v in constexprs.values()])\n+    doc_string = [f\"{kernel.arg_names[i]}={constexprs[i]}\" for i in constexprs.keys()]\n+    doc_string += [f\"num_warps={args.num_warps}\", f\"num_stages={args.num_stages}\"]\n \n     # compile ast into cubin\n     for h in hints.values():\n@@ -119,9 +126,13 @@ def constexpr(s):\n         \"full_signature\": \", \".join([f\"{ty_to_cpp(signature[i])} {kernel.arg_names[i]}\" for i in signature.keys()]),\n         \"arg_pointers\": \", \".join([f\"&{arg}\" for arg in arg_names]),\n         \"num_args\": len(arg_names),\n-        \"kernel_docstring\": \"\",\n+        \"kernel_docstring\": doc_string,\n         \"shared\": ccinfo.shared,\n         \"num_warps\": args.num_warps,\n+        \"algo_info\": '_'.join([const_sig, meta_sig]),\n+        \"gridX\": grid[0],\n+        \"gridY\": grid[1],\n+        \"gridZ\": grid[2],\n         \"_placeholder\": \"\",\n     }\n     for ext in ['h', 'c']:"}, {"filename": "python/triton/tools/link.py", "status": "modified", "additions": 109, "deletions": 15, "changes": 124, "file_content_changes": "@@ -15,10 +15,12 @@ class LinkerError(Exception):\n \n @dataclass\n class KernelLinkerMeta:\n+    orig_kernel_name: str\n     arg_names: Sequence[str]\n     arg_ctypes: Sequence[str]\n     sizes: Sequence[Union[int, None]]\n     sig_hash: str\n+    triton_suffix: str\n     suffix: str\n     num_specs: int\n     \"\"\" number of specialized arguments \"\"\"\n@@ -29,8 +31,8 @@ def __init__(self) -> None:\n         import re\n \n         # [kernel_name, c signature]\n-        self.linker_directives = re.compile(\"//[\\\\s]*tt-linker:[\\\\s]*([\\\\w]+):(.+)\")\n-        # [name, suffix]\n+        self.linker_directives = re.compile(\"//[\\\\s]*tt-linker:[\\\\s]*([\\\\w]+):(.+):(.+)\")\n+        # [name, hash, suffix]\n         self.kernel_name = re.compile(\"^([\\\\w]+)_([\\\\w]+)_([\\\\w]+)$\")\n         # [(type, name)]\n         self.c_sig = re.compile(\"[\\\\s]*(\\\\w+)\\\\s(\\\\w+)[,]?\")\n@@ -45,17 +47,19 @@ def extract_linker_meta(self, header: str):\n             if ln.startswith(\"//\"):\n                 m = self.linker_directives.match(ln)\n                 if _exists(m):\n-                    ker_name, c_sig = m.group(1), m.group(2)\n+                    ker_name, c_sig, algo_info = m.group(1), m.group(2), m.group(3)\n                     name, sig_hash, suffix = self._match_name(ker_name)\n                     c_types, arg_names = self._match_c_sig(c_sig)\n                     num_specs, sizes = self._match_suffix(suffix, c_sig)\n                     self._add_kernel(\n-                        name,\n+                        \"_\".join([name, algo_info]),\n                         KernelLinkerMeta(\n+                            orig_kernel_name=name,\n                             arg_names=arg_names,\n                             arg_ctypes=c_types,\n                             sizes=sizes,\n                             sig_hash=sig_hash,\n+                            triton_suffix=suffix,\n                             suffix=suffix,\n                             num_specs=num_specs,\n                         ),\n@@ -126,44 +130,107 @@ def gen_signature(m):\n     return sig\n \n \n-def make_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n+# generate declarations of kernels with meta-parameter and constant values\n+def make_algo_decls(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     return f\"\"\"\n-CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature_with_full_args(metas[-1])});\n+CUresult {name}(CUstream stream, {gen_signature_with_full_args(metas[-1])});\n void load_{name}();\n void unload_{name}();\n     \"\"\"\n \n \n-def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n+# generate declarations of kernels with meta-parameter and constant values\n+def make_global_decl(meta: KernelLinkerMeta) -> str:\n+    return f\"\"\"\n+CUresult {meta.orig_kernel_name}_default(CUstream stream, {gen_signature_with_full_args(meta)});\n+CUresult {meta.orig_kernel_name}(CUstream stream, {gen_signature_with_full_args(meta)}, int algo_id);\n+void load_{meta.orig_kernel_name}();\n+void unload_{meta.orig_kernel_name}();\n+    \"\"\"\n+\n+\n+# generate dispatcher function for kernels with different meta-parameter and constant values\n+def make_default_algo_kernel(meta: KernelLinkerMeta) -> str:\n+    src = f\"CUresult {meta.orig_kernel_name}_default(CUstream stream, {gen_signature_with_full_args(meta)}){{\\n\"\n+    src += f\"  return {meta.orig_kernel_name}(stream, {', '.join(meta.arg_names)}, 0);\\n\"\n+    src += \"}\\n\"\n+    return src\n+\n+\n+# generate dispatcher function for kernels with different integer value hints\n+def make_kernel_hints_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n     src = f\"// launcher for: {name}\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n-        src += f\"CUresult {name}_{meta.sig_hash}_{meta.suffix}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature(meta)});\\n\"\n+        src += f\"CUresult {meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}(CUstream stream, {gen_signature(meta)});\\n\"\n     src += \"\\n\"\n \n-    src += f\"CUresult {name}(CUstream stream, unsigned int gX, unsigned int gY, unsigned int gZ, {gen_signature_with_full_args(metas[-1])}){{\"\n+    src += f\"CUresult {name}(CUstream stream, {gen_signature_with_full_args(metas[-1])}){{\"\n     src += \"\\n\"\n     for meta in sorted(metas, key=lambda m: -m.num_specs):\n         cond_fn = lambda val, hint: f\"({val} % {hint} == 0)\" if hint == 16 else f\"({val} == {hint})\" if hint == 1 else None\n         conds = \" && \".join([cond_fn(val, hint) for val, hint in zip(meta.arg_names, meta.sizes) if hint is not None])\n         src += f\"  if ({conds})\\n\"\n         arg_names = [arg for arg, hint in zip(meta.arg_names, meta.sizes) if hint != 1]\n-        src += f\"    return {name}_{meta.sig_hash}_{meta.suffix}(stream, gX, gY, gZ, {', '.join(arg_names)});\\n\"\n+        src += f\"    return {meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}(stream, {', '.join(arg_names)});\\n\"\n     src += \"\\n\"\n     src += \"  return CUDA_ERROR_INVALID_VALUE;\\n\"\n     src += \"}\\n\"\n \n     for mode in [\"load\", \"unload\"]:\n         src += f\"\\n// {mode} for: {name}\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"void {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n+            src += f\"void {mode}_{meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += f\"void {mode}_{name}() {{\"\n         src += \"\\n\"\n         for meta in sorted(metas, key=lambda m: -m.num_specs):\n-            src += f\"  {mode}_{name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n+            src += f\"  {mode}_{meta.orig_kernel_name}_{meta.sig_hash}_{meta.suffix}();\\n\"\n         src += \"}\\n\"\n     return src\n \n \n+# generate dispatcher function for kernels with different meta-parameter and constant values\n+def make_kernel_meta_const_dispatcher(meta: KernelLinkerMeta) -> str:\n+    src = f\"CUresult {meta.orig_kernel_name}(CUstream stream, {gen_signature_with_full_args(meta)}, int algo_id){{\\n\"\n+    src += f\"  assert (algo_id < (int)sizeof({meta.orig_kernel_name}_kernels));\\n\"\n+    src += f\"  return {meta.orig_kernel_name}_kernels[algo_id](stream, {', '.join(meta.arg_names)});\\n\"\n+    src += \"}\\n\"\n+    return src\n+\n+\n+# generate definition of function pointers of kernel dispatchers based on meta-parameter and constant values\n+def make_func_pointers(names: str, meta: KernelLinkerMeta) -> str:\n+    # the table of hint dispatchers\n+    src = f\"typedef CUresult (*kernel_func_t)(CUstream stream, {gen_signature_with_full_args(meta)});\\n\"\n+    src += f\"kernel_func_t {meta.orig_kernel_name}_kernels[] = {{\\n\"\n+    for name in names:\n+        src += f\"  {name},\\n\"\n+    src += \"};\\n\"\n+    return src\n+\n+\n+# generate definition for load/unload functions for kernels with different meta-parameter and constant values\n+def make_kernel_load_def(names: str, meta: KernelLinkerMeta) -> str:\n+    src = \"\"\n+    for mode in [\"load\", \"unload\"]:\n+        src += f\"void {mode}_{meta.orig_kernel_name}(void){{\\n\"\n+        for name in names:\n+            src += f\"  {mode}_{name}();\\n\"\n+        src += \"}\\n\\n\"\n+    return src\n+\n+\n+def make_get_num_algos_decl(meta: KernelLinkerMeta) -> str:\n+    src = f\"int {meta.orig_kernel_name}_get_num_algos(void);\"\n+    return src\n+\n+\n+def make_get_num_algos_def(meta: KernelLinkerMeta) -> str:\n+    src = f\"int {meta.orig_kernel_name}_get_num_algos(void){{\\n\"\n+    src += f\"  return (int)sizeof({meta.orig_kernel_name}_kernels);\\n\"\n+    src += \"}\\n\"\n+    return src\n+\n+\n desc = \"\"\"\n Triton ahead-of-time linker:\n \n@@ -198,16 +265,43 @@ def make_kernel_dispatcher(name: str, metas: Sequence[KernelLinkerMeta]) -> str:\n         parser.extract_linker_meta(h_str)\n \n     # generate headers\n-    decls = [make_decls(name, meta) for name, meta in parser.kernels.items()]\n+    algo_decls = [make_algo_decls(name, meta) for name, meta in parser.kernels.items()]\n+    meta_lists = [meta for name, meta in parser.kernels.items()]\n+    meta = meta_lists[0][0]\n+    get_num_algos_decl = make_get_num_algos_decl(meta)\n+    global_decl = make_global_decl(meta)\n     with args.out.with_suffix(\".h\").open(\"w\") as fp:\n-        fp.write(\"#include <cuda.h>\\n\" + \"\\n\".join(decls))\n+        out = \"#include <cuda.h>\\n\"\n+        out += \"\\n\".join(algo_decls)\n+        out += \"\\n\"\n+        out += get_num_algos_decl\n+        out += \"\\n\"\n+        out += global_decl\n+        fp.write(out)\n \n     # generate source\n-    defs = [make_kernel_dispatcher(name, meta) for name, meta in parser.kernels.items()]\n+    defs = [make_kernel_hints_dispatcher(name, meta) for name, meta in parser.kernels.items()]\n+    names = [name for name in parser.kernels.keys()]\n+    func_pointers_def = make_func_pointers(names, meta)\n+    meta_const_def = make_kernel_meta_const_dispatcher(meta)\n+    load_unload_def = make_kernel_load_def(names, meta)\n+    get_num_algos_def = make_get_num_algos_def(meta)\n+    default_algo_kernel = make_default_algo_kernel(meta)\n     with args.out.with_suffix(\".c\").open(\"w\") as fp:\n         out = \"\"\n         out += \"#include <cuda.h>\\n\"\n         out += \"#include <stdint.h>\\n\"\n+        out += \"#include <assert.h>\\n\"\n         out += \"\\n\"\n         out += \"\\n\".join(defs)\n+        out += \"\\n\"\n+        out += func_pointers_def\n+        out += \"\\n\"\n+        out += get_num_algos_def\n+        out += \"\\n\"\n+        out += meta_const_def\n+        out += \"\\n\"\n+        out += load_unload_def\n+        out += \"\\n\"\n+        out += default_algo_kernel\n         fp.write(out)"}, {"filename": "python/tutorials/09-experimental-tma-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -32,6 +32,11 @@\n import triton\n import triton.language as tl\n \n+if torch.cuda.get_device_capability()[0] < 9:\n+    import sys\n+    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n+    sys.exit(0)\n+\n \n @triton.autotune(\n     configs=["}, {"filename": "python/tutorials/10-experimental-tma-store-matrix-multiplication.py", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -32,6 +32,11 @@\n import triton\n import triton.language as tl\n \n+if torch.cuda.get_device_capability()[0] < 9:\n+    import sys\n+    print(\"Skipping TMA benchmark for GPU with compute capability < 9\")\n+    sys.exit(0)\n+\n \n @triton.autotune(\n     configs=["}, {"filename": "test/Triton/canonicalize.mlir", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -0,0 +1,27 @@\n+// RUN: triton-opt %s -split-input-file -canonicalize | FileCheck %s\n+\n+// CHECK-LABEL: dead_load\n+tt.func @dead_load(%ptr: tensor<32x128x!tt.ptr<f16>>) {\n+  %mask = arith.constant dense<true> : tensor<32x128xi1>\n+  %other = arith.constant dense<0.00e+00> : tensor<32x128xf16>\n+  // CHECK-NOT: tt.load {{.*}} isVolatile = false\n+  //     CHECK: tt.load {{.*}} isVolatile = true\n+  %a = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16>\n+  %b = tt.load %ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isVolatile = true} : tensor<32x128xf16>\n+  tt.return\n+}\n+\n+\n+// CHECK-LABEL: make_range\n+tt.func @make_range() -> (tensor<128x1xi32>, tensor<1xi32>) {\n+  // CHECK-DAG: %[[c:.*]] = arith.constant dense<0> : tensor<128x1xi32>\n+  %a = tt.make_range {end = 1 : i32, start = 0 : i32} : tensor<1xi32>\n+  %b = tt.expand_dims %a {axis = 1 : i32} : (tensor<1xi32>) -> tensor<1x1xi32>\n+  %c = tt.broadcast %b : (tensor<1x1xi32>) -> tensor<128x1xi32>\n+\n+  // CHECK-DAG: %[[d:.*]] = arith.constant dense<1> : tensor<1xi32>\n+  %d = tt.make_range {end = 2 : i32, start = 1 : i32} : tensor<1xi32>\n+\n+  // CHECK-DAG: tt.return %[[c]], %[[d]] : tensor<128x1xi32>, tensor<1xi32>\n+  tt.return %c, %d : tensor<128x1xi32>, tensor<1xi32>\n+}"}]