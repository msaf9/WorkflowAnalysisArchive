[{"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 29, "deletions": 13, "changes": 42, "file_content_changes": "@@ -30,8 +30,8 @@ class TT_Op<string mnemonic, list<Trait> traits = []> :\n //   fptoui, fptosi, uitofp, sitofp,\n //   extf, tructf,\n //   extui, extsi, tructi\n-def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape, \n-                                         SameOperandsAndResultEncoding, \n+def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n+                                         SameOperandsAndResultEncoding,\n                                          NoSideEffect,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast int64 to pointer\";\n@@ -43,7 +43,7 @@ def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape,\n     let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n }\n \n-def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape, \n+def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n                                          SameOperandsAndResultEncoding,\n                                          NoSideEffect,\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -57,7 +57,7 @@ def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape,\n }\n \n // arith.bitcast doesn't support pointers\n-def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape, \n+def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n                                      SameOperandsAndResultEncoding,\n                                      NoSideEffect,\n                                      /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -72,7 +72,7 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n     // TODO: Add verifier\n }\n \n-def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape, \n+def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n                                    SameOperandsAndResultEncoding,\n                                    NoSideEffect,\n                                    /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n@@ -99,7 +99,7 @@ def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n //\n \n def TT_AddPtrOp : TT_Op<\"addptr\",\n-                     [NoSideEffect, \n+                     [NoSideEffect,\n                      SameOperandsAndResultShape,\n                      SameOperandsAndResultEncoding,\n                       TypesMatchWith<\"result type matches ptr type\",\n@@ -224,7 +224,7 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [SameOperandsAndResultShape,\n //\n // Shape Manipulation Ops\n //\n-def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect, \n+def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n                                  SameOperandsAndResultElementType]> {\n     let summary = \"splat\";\n \n@@ -237,8 +237,8 @@ def TT_SplatOp : TT_Op<\"splat\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect, \n-                                            DeclareOpInterfaceMethods<InferTypeOpInterface>, \n+def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n+                                            DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                                             SameOperandsAndResultElementType]> {\n     let summary = \"expand_dims\";\n \n@@ -249,7 +249,7 @@ def TT_ExpandDimsOp : TT_Op<\"expand_dims\", [NoSideEffect,\n     let assemblyFormat = \"$src attr-dict `:` functional-type(operands, results)\";\n }\n \n-def TT_ViewOp : TT_Op<\"view\", [NoSideEffect, \n+def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n                                SameOperandsAndResultElementType]> {\n     let summary = \"view\";\n \n@@ -261,7 +261,7 @@ def TT_ViewOp : TT_Op<\"view\", [NoSideEffect,\n \n }\n \n-def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect, \n+def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n                                          SameOperandsAndResultElementType]> {\n     let summary = \"broadcast. No left-padding as of now.\";\n \n@@ -274,7 +274,7 @@ def TT_BroadcastOp : TT_Op<\"broadcast\", [NoSideEffect,\n     let hasFolder = 1;\n }\n \n-def TT_CatOp : TT_Op<\"cat\", [NoSideEffect, \n+def TT_CatOp : TT_Op<\"cat\", [NoSideEffect,\n                              SameOperandsAndResultElementType]> {\n     let summary = \"concatenate 2 tensors\";\n \n@@ -307,7 +307,7 @@ def TT_GetNumProgramsOp : TT_Op<\"get_num_programs\", [NoSideEffect]> {\n //\n // Dot Op\n //\n-def TT_DotOp : TT_Op<\"dot\", [NoSideEffect, \n+def TT_DotOp : TT_Op<\"dot\", [NoSideEffect,\n                              DeclareOpInterfaceMethods<InferTypeOpInterface>,\n                              TypesMatchWith<\"result's type matches accumulator's type\",\n                                             \"d\", \"c\", \"$_self\">]> {\n@@ -385,4 +385,20 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [NoSideEffect]> {\n     let assemblyFormat = \"attr-dict `:` type($result)\";\n }\n \n+//\n+// Make PrintfOp\n+//\n+def TT_PrintfOp : TT_Op<\"printf\", [MemoryEffects<[MemWrite]>]>,\n+  Arguments<(ins StrAttr:$prefix,\n+                Variadic<AnyTypeOf<[TT_Type]>>:$args)> {\n+  let summary = \"Device-side printf, as in CUDA for debugging\";\n+  let description = [{\n+    `tt.printf` takes a literal string prefix and an arbitrary number of scalar or tensor arguments that should be printed.\n+    format are generated automatically from the arguments.\n+  }];\n+  let assemblyFormat = [{\n+    $prefix attr-dict ($args^ `:` type($args))?\n+  }];\n+}\n+\n #endif // Triton_OPS"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 188, "deletions": 0, "changes": 188, "file_content_changes": "@@ -119,9 +119,11 @@ Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, short width,\n #define barrier() rewriter.create<mlir::gpu::BarrierOp>(loc)\n #define undef(...) rewriter.create<LLVM::UndefOp>(loc, __VA_ARGS__)\n #define i32_ty rewriter.getIntegerType(32)\n+#define ui32_ty rewriter.getIntegerType(32, false)\n #define f16_ty rewriter.getF16Type()\n #define i8_ty rewriter.getIntegerType(8)\n #define f32_ty rewriter.getF32Type()\n+#define f64_ty rewriter.getF64Type()\n #define vec_ty(type, num) VectorType::get(num, type)\n #define f32_val(...) LLVM::createConstantF32(loc, rewriter, __VA_ARGS__)\n #define void_ty(ctx) LLVM::LLVMVoidType::get(ctx)\n@@ -1808,6 +1810,7 @@ class ElementwiseOpConversionBase\n     }\n     Value view = getStructFromElements(loc, resultVals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n+\n     return success();\n   }\n \n@@ -4542,6 +4545,190 @@ struct FDivOpConversion\n   }\n };\n \n+struct PrintfOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+    SmallVector<Value, 16> operands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n+      for (auto elem : sub_operands) {\n+        operands.push_back(elem);\n+      }\n+    }\n+    std::string formatStr;\n+    llvm::raw_string_ostream os(formatStr);\n+    os << op.prefix();\n+    if (operands.size() > 0) {\n+      os << getFormatSubstr(operands[0]);\n+    }\n+\n+    for (size_t i = 1; i < operands.size(); ++i) {\n+      os << \", \" << getFormatSubstr(operands[i]);\n+    }\n+    llPrintf(formatStr, operands, rewriter);\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+  // get format specific for each input value\n+  // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n+  std::string getFormatSubstr(Value value) const {\n+    Type type = value.getType();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+\n+    if (type.isa<LLVM::LLVMPointerType>()) {\n+      return \"%p\";\n+    } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n+      return \"%f\";\n+    } else if (type.isSignedInteger()) {\n+      return \"%i\";\n+    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n+      return \"%u\";\n+    }\n+    assert(false && \"not supported type\");\n+  }\n+\n+  // declare vprintf(i8*, i8*) as external function\n+  LLVM::LLVMFuncOp\n+  getVprintfDeclaration(ConversionPatternRewriter &rewriter) const {\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    StringRef funcName(\"vprintf\");\n+    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n+    if (funcOp)\n+      return cast<LLVM::LLVMFuncOp>(*funcOp);\n+\n+    auto *context = rewriter.getContext();\n+\n+    SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n+                               ptr_ty(IntegerType::get(context, 8))};\n+    auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n+\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPointToStart(moduleOp.getBody());\n+\n+    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n+                                             funcType);\n+  }\n+\n+  // extend integer to int32, extend float to float64\n+  // this comes from vprintf alignment requirements.\n+  std::pair<Type, Value> promoteValue(ConversionPatternRewriter &rewriter,\n+                                      Value value) const {\n+    auto *context = rewriter.getContext();\n+    auto type = value.getType();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+    Value newOp = value;\n+    Type newType = type;\n+\n+    bool bUnsigned = type.isUnsignedInteger();\n+    if (type.isIntOrIndex() && width < 32) {\n+      if (bUnsigned) {\n+        newType = ui32_ty;\n+        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      } else {\n+        newType = i32_ty;\n+        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      }\n+    } else if (type.isBF16() || type.isF16() || type.isF32()) {\n+      newType = f64_ty;\n+      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n+                                             value);\n+    }\n+\n+    return {newType, newOp};\n+  }\n+\n+  void llPrintf(StringRef msg, ValueRange args,\n+                ConversionPatternRewriter &rewriter) const {\n+    static const char formatStringPrefix[] = \"printfFormat_\";\n+    assert(!msg.empty() && \"printf with empty string not support\");\n+    Type int8Ptr = ptr_ty(i8_ty);\n+\n+    auto *context = rewriter.getContext();\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    auto funcOp = getVprintfDeclaration(rewriter);\n+\n+    Value one = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n+    Value zero = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n+\n+    unsigned stringNumber = 0;\n+    SmallString<16> stringConstName;\n+    do {\n+      stringConstName.clear();\n+      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n+    } while (moduleOp.lookupSymbol(stringConstName));\n+\n+    llvm::SmallString<64> formatString(msg);\n+    formatString.push_back('\\n');\n+    formatString.push_back('\\0');\n+    size_t formatStringSize = formatString.size_in_bytes();\n+    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n+\n+    LLVM::GlobalOp global;\n+    {\n+      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPointToStart(moduleOp.getBody());\n+      global = rewriter.create<LLVM::GlobalOp>(\n+          UnknownLoc::get(context), globalType,\n+          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n+          rewriter.getStringAttr(formatString));\n+    }\n+\n+    Value globalPtr =\n+        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n+    Value stringStart =\n+        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n+                                     globalPtr, mlir::ValueRange({zero, zero}));\n+\n+    Value bufferPtr =\n+        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n+\n+    SmallVector<Value, 16> newArgs;\n+    if (args.size() >= 1) {\n+      SmallVector<Type> argTypes;\n+      for (auto arg : args) {\n+        Type newType;\n+        Value newArg;\n+        std::tie(newType, newArg) = promoteValue(rewriter, arg);\n+        argTypes.push_back(newType);\n+        newArgs.push_back(newArg);\n+      }\n+\n+      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n+      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n+                                                       ptr_ty(structTy), one,\n+                                                       /*alignment=*/0);\n+\n+      for (const auto &entry : llvm::enumerate(newArgs)) {\n+        auto index = rewriter.create<LLVM::ConstantOp>(\n+            UnknownLoc::get(context), i32_ty,\n+            rewriter.getI32IntegerAttr(entry.index()));\n+        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n+            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n+            allocated, ArrayRef<Value>{zero, index});\n+        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n+                                       fieldPtr);\n+      }\n+      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n+                                                   int8Ptr, allocated);\n+    }\n+\n+    ValueRange operands{stringStart, bufferPtr};\n+    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n+  }\n+};\n+\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -4628,6 +4815,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n                                                            benefit);\n   patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n+  patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n \n class ConvertTritonGPUToLLVM"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 14, "deletions": 2, "changes": 16, "file_content_changes": "@@ -339,6 +339,18 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   }\n };\n \n+struct TritonPrintfPattern : public OpConversionPattern<triton::PrintfOp> {\n+  using OpConversionPattern<PrintfOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(PrintfOp op, typename PrintfOp::Adaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    rewriter.replaceOpWithNewOp<triton::PrintfOp>(op, op.prefixAttr(),\n+                                                  adaptor.getOperands());\n+    return success();\n+  }\n+};\n+\n void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n                             RewritePatternSet &patterns) {\n   MLIRContext *context = patterns.getContext();\n@@ -350,8 +362,8 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n       TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n       TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-      TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern>(\n-      typeConverter, context);\n+      TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n+      TritonPrintfPattern>(typeConverter, context);\n }\n \n //"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -1185,6 +1185,16 @@ void init_triton_ir(py::module &&m) {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::SelectOp>(loc, condition, trueValue,\n                                                 falseValue);\n+           })\n+      .def(\"create_printf\",\n+           [](mlir::OpBuilder &self, const std::string &prefix,\n+              const std::vector<mlir::Value> &values) -> void {\n+             auto loc = self.getUnknownLoc();\n+             self.create<mlir::triton::PrintfOp>(\n+                 loc,\n+                 mlir::StringAttr::get(self.getContext(),\n+                                       llvm::StringRef(prefix)),\n+                 values);\n            });\n \n   py::class_<mlir::PassManager>(m, \"pass_manager\")"}, {"filename": "python/tests/printf_helper.py", "status": "added", "additions": 56, "deletions": 0, "changes": 56, "file_content_changes": "@@ -0,0 +1,56 @@\n+import torch\n+from torch.testing import assert_close\n+\n+import triton\n+import triton.language as tl\n+\n+torch_type = {\n+    \"bool\": torch.bool,\n+    'int8': torch.int8,\n+    'uint8': torch.uint8,\n+    'int16': torch.int16,\n+    \"int32\": torch.int32,\n+    'int64': torch.long,\n+    'float16': torch.float16,\n+    'bfloat16': torch.bfloat16,\n+    \"float32\": torch.float32,\n+    \"float64\": torch.float64\n+}\n+\n+\n+def get_tensor(shape, data_type, b_positive=False):\n+    x = None\n+    if data_type.startswith('int'):\n+        x = torch.arange(0, shape[0], dtype=torch_type[data_type], device='cuda')\n+    else:\n+        x = torch.arange(0, shape[0], dtype=torch_type[data_type], device='cuda')\n+\n+    return x\n+\n+# @pytest.mark.parametrize('data_type',\n+#                          [(\"int8\"),\n+#                           ('int16'),\n+#                           ('int32'),\n+#                           (\"int64\"),\n+#                           ('float16'),\n+#                           (\"float32\"),\n+#                           (\"float64\")])\n+\n+\n+def printf(data_type):\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        tl.printf(\"\", x)\n+        tl.store(Y + tl.arange(0, BLOCK), x)\n+\n+    shape = (128, )\n+    # limit the range of integers so that the sum does not overflow\n+    x = get_tensor(shape, data_type)\n+    y = torch.zeros(shape, dtype=x.dtype, device=\"cuda\")\n+    kernel[(1,)](x, y, BLOCK=shape[0])\n+    assert_close(y, x)\n+\n+\n+printf(\"float16\")\n+printf(\"int8\")"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 43, "deletions": 48, "changes": 91, "file_content_changes": "@@ -144,7 +144,7 @@ def kernel(Z, X, SIZE: tl.constexpr):\n     # triton result\n     x_tri = to_triton(x, device=device, dst_type=dtype_x)\n     z_tri = to_triton(np.empty_like(z_ref), device=device, dst_type=dtype_x)\n-    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4)\n+    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4, extern_libs={\"libdevice\": \"/usr/local/cuda/nvvm/libdevice/libdevice.10.bc\"})\n     # compare\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n@@ -463,17 +463,12 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n # # test math ops\n # # ----------------\n \n-# TODO: Math module\n-# # @pytest.mark.parametrize(\"expr\", [\n-# #     'exp', 'log', 'cos', 'sin'\n-# # ])\n \n-\n-# @pytest.mark.parametrize(\"expr\", [\n-#     'exp', 'log', 'cos', 'sin'\n-# ])\n-# def test_math_op(expr, device='cuda'):\n-#     _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n+@pytest.mark.parametrize(\"expr\", [\n+    'exp', 'log', 'cos', 'sin'\n+])\n+def test_math_op(expr, device='cuda'):\n+    _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n \n # # ----------------\n@@ -1545,43 +1540,43 @@ def _kernel(dst):\n # # -------------\n \n \n-# @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-#                          [('int32', 'libdevice.ffs', ''),\n-#                           ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n-#                           ('float64', 'libdevice.norm4d', '')])\n-# def test_libdevice(dtype_str, expr, lib_path):\n+@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+                         [('int32', 'libdevice.ffs', ''),\n+                          ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n+                          ('float64', 'libdevice.norm4d', '')])\n+def test_libdevice(dtype_str, expr, lib_path):\n \n-#     @triton.jit\n-#     def kernel(X, Y, BLOCK: tl.constexpr):\n-#         x = tl.load(X + tl.arange(0, BLOCK))\n-#         y = GENERATE_TEST_HERE\n-#         tl.store(Y + tl.arange(0, BLOCK), y)\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        y = GENERATE_TEST_HERE\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n \n-#     shape = (128, )\n-#     rs = RandomState(17)\n-#     # limit the range of integers so that the sum does not overflow\n-#     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n-\n-#     if expr == 'libdevice.ffs':\n-#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n-#         y_ref = np.zeros(shape, dtype=x.dtype)\n-#         for i in range(shape[0]):\n-#             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n-#     elif expr == 'libdevice.pow':\n-#         # numpy does not allow negative factors in power, so we use abs()\n-#         x = np.abs(x)\n-#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n-#         y_ref = np.power(x, x)\n-#     elif expr == 'libdevice.norm4d':\n-#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n-#         y_ref = np.sqrt(4 * np.power(x, 2))\n+    shape = (128, )\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n \n-#     x_tri = to_triton(x)\n-#     # triton result\n-#     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n-#     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n-#     # compare\n-#     if expr == 'libdevice.ffs':\n-#         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n-#     else:\n-#         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n+    if expr == 'libdevice.ffs':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n+        y_ref = np.zeros(shape, dtype=x.dtype)\n+        for i in range(shape[0]):\n+            y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n+    elif expr == 'libdevice.pow':\n+        # numpy does not allow negative factors in power, so we use abs()\n+        x = np.abs(x)\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+        y_ref = np.power(x, x)\n+    elif expr == 'libdevice.norm4d':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n+        y_ref = np.sqrt(4 * np.power(x, 2))\n+\n+    x_tri = to_triton(x)\n+    # triton result\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    # compare\n+    if expr == 'libdevice.ffs':\n+        np.testing.assert_equal(y_ref, to_numpy(y_tri))\n+    else:\n+        np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)"}, {"filename": "python/tests/test_printf.py", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -0,0 +1,21 @@\n+import os\n+import subprocess\n+\n+dir_path = os.path.dirname(os.path.realpath(__file__))\n+printf_path = os.path.join(dir_path, \"printf_helper.py\")\n+\n+\n+def test_printf():\n+    proc = subprocess.Popen([\"python\", printf_path], stdout=subprocess.PIPE, shell=False)\n+    (outs, err) = proc.communicate()\n+    outs = outs.split()\n+    new_lines = set()\n+    for line in outs:\n+        try:\n+            value = int(float(line))\n+            new_lines.add(value)\n+        except Exception as e:\n+            print(e)\n+    for i in range(128):\n+        assert i in new_lines\n+    assert len(new_lines) == 128"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -1197,3 +1197,22 @@ def swizzle2d(i, j, size_i, size_j, size_g):\n @triton.jit\n def zeros_like(input):\n     return zeros(input.shape, input.dtype)\n+\n+\n+@builtin\n+def printf(prefix, *args, _builder=None):\n+    import string\n+    new_prefix = prefix\n+    if isinstance(prefix, constexpr):\n+        new_prefix = prefix.value\n+    assert isinstance(new_prefix, str), f\"{new_prefix} is not string\"\n+    b_ascii = True\n+    for ch in new_prefix:\n+        if ch not in string.printable:\n+            b_ascii = False\n+            break\n+    assert b_ascii, f\"{new_prefix} is not an ascii string\"\n+    new_args = []\n+    for arg in args:\n+        new_args.append(_to_tensor(arg, _builder))\n+    return semantic.printf(new_prefix, new_args, _builder)"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1123,3 +1123,10 @@ def max_contiguous(x: tl.tensor, values: List[int]) -> tl.tensor:\n \n def debug_barrier(builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_barrier(''), tl.void)\n+\n+\n+def printf(prefix: str, args: List[tl.tensor], builder: ir.builder) -> tl.tensor:\n+    new_args = []\n+    for arg in args:\n+        new_args.append(arg.handle)\n+    return tl.tensor(builder.create_printf(prefix, new_args), tl.void)"}]