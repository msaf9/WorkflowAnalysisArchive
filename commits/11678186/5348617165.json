[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 12, "deletions": 4, "changes": 16, "file_content_changes": "@@ -77,7 +77,7 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n     AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n                      \"ArrayRef<int64_t>\":$shape,\n                      \"ArrayRef<unsigned>\":$order,\n-                     \"Type\":$eltTy), [{\n+                     \"unsigned\":$typeWidthInBit), [{\n         auto mmaEnc = dotOpEnc.getParent().dyn_cast<MmaEncodingAttr>();\n \n         if(!mmaEnc)\n@@ -87,7 +87,7 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         int opIdx = dotOpEnc.getOpIdx();\n \n         // number of rows per phase\n-        int perPhase = 128 / (shape[order[0]] * (eltTy.getIntOrFloatBitWidth() / 8));\n+        int perPhase = 128 / (shape[order[0]] * (typeWidthInBit / 8));\n         perPhase = std::max<int>(perPhase, 1);\n \n         // index of the inner dimension in `order`\n@@ -109,9 +109,9 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         // ---- begin Ampere ----\n         if (mmaEnc.isAmpere()) {\n           std::vector<size_t> matShape = {8, 8,\n-                                          2 * 64 / eltTy.getIntOrFloatBitWidth()};\n+                                          2 * 64 / typeWidthInBit};\n           // for now, disable swizzle when using transposed int8 tensor cores\n-          if (eltTy.isInteger(8) && order[0] == inner)\n+          if (typeWidthInBit == 8 && order[0] == inner)\n             return $_get(context, 1, 1, 1, order);\n \n           // --- handle A operand ---\n@@ -135,6 +135,14 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n \n         // ---- not implemented ----\n         llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n+    }]>,\n+\n+    AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n+                     \"ArrayRef<int64_t>\":$shape,\n+                     \"ArrayRef<unsigned>\":$order,\n+                     \"Type\":$eltTy), [{\n+      unsigned bitwidth = eltTy.getIntOrFloatBitWidth();\n+      return get(context, dotOpEnc, shape, order, bitwidth);\n     }]>\n   ];\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 118, "deletions": 0, "changes": 118, "file_content_changes": "@@ -14,6 +14,7 @@ using triton::DotOp;\n using triton::gpu::ConvertLayoutOp;\n using triton::gpu::DotOperandEncodingAttr;\n using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SharedEncodingAttr;\n using triton::gpu::SliceEncodingAttr;\n \n // convert(trans(convert(arg)))\n@@ -192,6 +193,119 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n \n } // namespace\n \n+static bool isConvertToDotEncoding(Operation *op) {\n+  auto convertLayout = llvm::dyn_cast<ConvertLayoutOp>(op);\n+  if (!convertLayout)\n+    return false;\n+  auto tensorType =\n+      convertLayout.getResult().getType().cast<RankedTensorType>();\n+  return tensorType.getEncoding().isa<DotOperandEncodingAttr>();\n+}\n+\n+static ConvertLayoutOp updateConvert(OpBuilder &builder, ConvertLayoutOp cvt,\n+                                     IRMapping &mapping, Type smallestType) {\n+  auto cvtDstTy = cvt.getResult().getType().cast<RankedTensorType>();\n+  auto cvtDstEnc = cvtDstTy.getEncoding().cast<DotOperandEncodingAttr>();\n+  Value operand = cvt.getOperand();\n+  if (mapping.contains(operand))\n+    operand = mapping.lookup(operand);\n+  auto newDstTy = RankedTensorType::get(\n+      cvtDstTy.getShape(), cvtDstTy.getElementType(),\n+      DotOperandEncodingAttr::get(cvtDstEnc.getContext(), cvtDstEnc.getOpIdx(),\n+                                  cvtDstEnc.getParent(), smallestType));\n+  auto newCvt =\n+      builder.create<ConvertLayoutOp>(cvt.getLoc(), newDstTy, operand);\n+  mapping.map(cvt.getResult(), newCvt.getResult());\n+  return newCvt;\n+}\n+\n+// Update kWidth based on the smallestType found in the given convert ops and\n+// propagate the type change.\n+static void\n+updateDotEncodingLayout(SmallVector<ConvertLayoutOp> &convertsToDotEncoding,\n+                        Type smallestType) {\n+  IRMapping mapping;\n+  OpBuilder builder(smallestType.getContext());\n+  SetVector<Operation *> slices(convertsToDotEncoding.begin(),\n+                                convertsToDotEncoding.end());\n+  // Collect all the operations where the type needs to be propagated.\n+  for (auto cvt : convertsToDotEncoding) {\n+    auto filter = [&](Operation *op) {\n+      for (Value operand : op->getOperands()) {\n+        auto tensorType = operand.getType().dyn_cast<RankedTensorType>();\n+        if (tensorType &&\n+            tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n+          return true;\n+      }\n+      return false;\n+    };\n+    mlir::getForwardSlice(cvt.getResult(), &slices, {filter});\n+  }\n+  // Apply the type change by walking ops in toological order.\n+  slices = mlir::topologicalSort(slices);\n+  for (Operation *op : slices) {\n+    builder.setInsertionPoint(op);\n+    if (isConvertToDotEncoding(op)) {\n+      auto cvt = cast<ConvertLayoutOp>(op);\n+      ConvertLayoutOp newCvt =\n+          updateConvert(builder, cvt, mapping, smallestType);\n+      continue;\n+    }\n+    auto *newOp = cloneWithInferType(builder, op, mapping);\n+    for (auto [result, newResult] :\n+         llvm::zip(op->getResults(), newOp->getResults())) {\n+      result.replaceUsesWithIf(newResult, [&](OpOperand &operand) {\n+        return slices.count(operand.getOwner()) == 0;\n+      });\n+    }\n+  }\n+  for (Operation *op : llvm::reverse(slices))\n+    op->erase();\n+}\n+\n+// Change the layout of dotOperand layout to use the kWidth from the smallest\n+// loaded type. This allows better code generation for mixed-mode matmul.\n+static void optimizeKWidth(triton::FuncOp func) {\n+  SmallVector<ConvertLayoutOp> convertsToDotEncoding;\n+  Type smallestType;\n+  func->walk([&](triton::LoadOp loadOp) {\n+    if (!loadOp.getResult().hasOneUse())\n+      return;\n+    Operation *use = *loadOp.getResult().getUsers().begin();\n+\n+    // Advance to the first conversion as long as the use resides in shared\n+    // memory and it has a single use itself\n+    while (use) {\n+      if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n+        break;\n+      auto tensorType =\n+          use->getResult(0).getType().dyn_cast<RankedTensorType>();\n+      if (!tensorType || !tensorType.getEncoding().isa<SharedEncodingAttr>())\n+        break;\n+      use = *use->getResult(0).getUsers().begin();\n+    }\n+\n+    auto convertLayout = llvm::dyn_cast<ConvertLayoutOp>(use);\n+    if (!convertLayout)\n+      return;\n+    auto tensorType =\n+        convertLayout.getResult().getType().cast<RankedTensorType>();\n+    if (!tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n+      return;\n+    convertsToDotEncoding.push_back(convertLayout);\n+\n+    // Update the smallest type.\n+    auto ty = loadOp.getType().cast<RankedTensorType>();\n+    Type eltTy = ty.getElementType();\n+    if (!smallestType ||\n+        (eltTy.getIntOrFloatBitWidth() < smallestType.getIntOrFloatBitWidth()))\n+      smallestType = eltTy;\n+  });\n+  if (!smallestType)\n+    return;\n+  updateDotEncodingLayout(convertsToDotEncoding, smallestType);\n+}\n+\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n@@ -216,6 +330,10 @@ class TritonGPUOptimizeDotOperandsPass\n       signalPassFailure();\n     if (fixupLoops(m).failed())\n       signalPassFailure();\n+\n+    // Change the layout of dotOperand layout to use the kWidth from the\n+    // smallest loaded type.\n+    m->walk([](triton::FuncOp func) { optimizeKWidth(func); });\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 8, "deletions": 30, "changes": 38, "file_content_changes": "@@ -111,9 +111,6 @@ class LoopPipeliner {\n \n   /// Loads to be pipelined\n   SetVector<Value> validLoads;\n-  /// Smallest data-type for each load (used to optimize swizzle and\n-  /// (create DotOpEncoding layout)\n-  DenseMap<Value, Type> loadsSmallestType;\n   /// The value that each load will be mapped to (after layout conversion)\n   DenseMap<Value, Value> loadsMapping;\n   /// load => buffer\n@@ -485,21 +482,6 @@ Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n }\n \n void LoopPipeliner::createBufferTypes() {\n-  // We need to find the smallest common dtype since this determines the layout\n-  // of `mma.sync` operands in mixed-precision mode\n-  Type smallestType;\n-  for (auto loadCvt : loadsMapping) {\n-    auto loadOp = loadCvt.first;\n-    auto ty = loadOp.getType().cast<RankedTensorType>();\n-    Type eltTy = ty.getElementType();\n-    if (!smallestType ||\n-        (eltTy.getIntOrFloatBitWidth() < smallestType.getIntOrFloatBitWidth()))\n-      smallestType = eltTy;\n-  }\n-\n-  for (auto loadCvt : loadsMapping)\n-    loadsSmallestType[loadCvt.first] = smallestType;\n-\n   for (auto loadCvt : loadsMapping) {\n     auto loadOp = loadCvt.first;\n     Value cvt = loadCvt.second;\n@@ -511,9 +493,12 @@ void LoopPipeliner::createBufferTypes() {\n     SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n                                      ty.getShape().end());\n     bufferShape.insert(bufferShape.begin(), numStages);\n-    auto sharedEnc = ttg::SharedEncodingAttr::get(\n-        ty.getContext(), dotOpEnc, ty.getShape(),\n-        ttg::getOrder(ty.getEncoding()), loadsSmallestType[loadOp]);\n+    unsigned bitWidth = dotOpEnc.getMMAv2kWidth()\n+                            ? 32 / dotOpEnc.getMMAv2kWidth()\n+                            : ty.getElementType().getIntOrFloatBitWidth();\n+    auto sharedEnc =\n+        ttg::SharedEncodingAttr::get(ty.getContext(), dotOpEnc, ty.getShape(),\n+                                     ttg::getOrder(ty.getEncoding()), bitWidth);\n     loadsBufferType[loadOp] =\n         RankedTensorType::get(bufferShape, ty.getElementType(), sharedEnc);\n   }\n@@ -789,19 +774,12 @@ scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n     // we replace the use new load use with a convert layout\n     size_t i = std::distance(validLoads.begin(), it);\n     auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n-    auto cvtDstEnc =\n-        cvtDstTy.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n-    if (!cvtDstEnc) {\n+    if (!cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n       builder.clone(op, mapping);\n       continue;\n     }\n-    auto newDstTy = RankedTensorType::get(\n-        cvtDstTy.getShape(), cvtDstTy.getElementType(),\n-        ttg::DotOperandEncodingAttr::get(\n-            cvtDstEnc.getContext(), cvtDstEnc.getOpIdx(), cvtDstEnc.getParent(),\n-            loadsSmallestType[op.getOperand(0)]));\n     auto cvt = builder.create<ttg::ConvertLayoutOp>(\n-        op.getResult(0).getLoc(), newDstTy,\n+        op.getResult(0).getLoc(), cvtDstTy,\n         newForOp.getRegionIterArgs()[loadIdx + i]);\n     mapping.map(op.getResult(0), cvt.getResult());\n   }"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -15,10 +15,12 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // CHECK: tt.func @push_elementwise1\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n-// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n-// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]]\n-// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n-// CHECK: %[[C:.*]] = tt.dot %[[AF16]]\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %{{.*}} : {{.*}} tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[C:.*]] = tt.dot %[[AF16]], %[[BCVT]]\n+// CHECK-SAME: tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<16x16xf32, #mma>\n // CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n tt.func @push_elementwise1(\n                    %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},"}]