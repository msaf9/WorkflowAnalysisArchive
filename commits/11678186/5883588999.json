[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -123,6 +123,8 @@ def check_type_supported(dtype, device):\n         cc = torch.cuda.get_device_capability()\n         if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n             pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+        if cc[0] < 9 and (dtype is tl.float8e4 or dtype == \"float8e4\"):\n+            pytest.skip(\"float8e4 is only supported on NVGPU with cc >= 90\")\n \n \n class MmaLayout:\n@@ -1425,6 +1427,7 @@ def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n         - conversion tri_fp8 = convert(input=tri_fp16, out=out_dtype) matches the original\n     this is only possible if both conversions are correct\n     \"\"\"\n+    check_type_supported(in_dtype, device)\n     check_type_supported(out_dtype, device)\n \n     @triton.jit"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -108,6 +108,8 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n     if capability[0] < 8 and (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\"):\n         pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n+    if capability[0] < 9 and (ADTYPE == \"float8e4\" or BDTYPE == \"float8e4\"):\n+        pytest.skip(\"Only test float8e4 on devices with sm >= 90\")\n     if (ADTYPE == \"bfloat16\" or BDTYPE == \"bfloat16\") and SPLIT_K != 1:\n         pytest.skip(\"bfloat16 matmuls don't allow split_k for now\")\n     torch.manual_seed(0)"}]