[{"filename": "python/triton/testing.py", "status": "modified", "additions": 74, "deletions": 0, "changes": 74, "file_content_changes": "@@ -358,6 +358,80 @@ def get_max_tensorcore_tflops(dtype: torch.dtype, backend=None, device=None, clo\n     tflops = num_subcores * clock_rate * ops_per_sub_core * 1e-9\n     return tflops\n \n+# create decorator that wraps test function into\n+# a cuda-memcheck system call\n+\n+\n+def cuda_memcheck(**target_kwargs):\n+    def decorator(test_fn):\n+        @functools.wraps(test_fn)\n+        def wrapper(*args, **kwargs):\n+            import psutil\n+            ppid_name = psutil.Process(os.getppid()).name()\n+            run_cuda_memcheck = target_kwargs.items() <= kwargs.items()\n+            if run_cuda_memcheck and ppid_name != \"cuda-memcheck\":\n+                path = os.path.realpath(test_fn.__globals__[\"__file__\"])\n+                # get path of current file\n+                env = {\"PATH\": os.environ[\"PATH\"], \"PYTORCH_NO_CUDA_MEMORY_CACHING\": \"1\"}\n+                assert 'request' in kwargs, \"memcheck'ed test must have a (possibly unused) `request` fixture\"\n+                test_id = kwargs['request'].node.callspec.id\n+                cmd = f\"{path}::{test_fn.__name__}[{test_id}]\"\n+                out = subprocess.run([\"cuda-memcheck\", \"pytest\", \"-vs\", cmd], capture_output=True, env=env)\n+                assert out.returncode == 0, \"cuda-memcheck returned an error: bounds checking failed\"\n+                assert \"ERROR SUMMARY: 0 errors\" in str(out.stdout)\n+            else:\n+                test_fn(*args, **kwargs)\n+        return wrapper\n+    return decorator\n+\n+\n+def nvsmi_attr(attrs):\n+    attrs = \",\".join(attrs)\n+    cmd = [\n+        \"nvidia-smi\",\n+        \"-i\",\n+        \"0\",\n+        \"--query-gpu=\" + attrs,\n+        \"--format=csv,noheader,nounits\",\n+    ]\n+    out = subprocess.check_output(cmd)\n+    ret = out.decode(sys.stdout.encoding).split(\",\")\n+    ret = [int(x) for x in ret]\n+    return ret\n+\n+\n+@contextmanager\n+def set_gpu_clock(ref_sm_clock=1350, ref_mem_clock=1215):\n+    try:\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-pm\", \"1\"])\n+        subprocess.check_output(\n+            [\n+                \"nvidia-smi\",\n+                \"-i\",\n+                \"0\",\n+                f\"--lock-gpu-clocks={ref_sm_clock},{ref_sm_clock}\",\n+            ]\n+        )\n+        subprocess.check_output(\n+            [\n+                \"nvidia-smi\",\n+                \"-i\",\n+                \"0\",\n+                f\"--lock-memory-clocks={ref_mem_clock},{ref_mem_clock}\",\n+            ]\n+        )\n+        cur_sm_clock = nvsmi_attr([\"clocks.current.sm\"])[0]\n+        cur_mem_clock = nvsmi_attr([\"clocks.current.memory\"])[0]\n+        assert abs(cur_sm_clock - ref_sm_clock) < 10, f\"GPU SMs must run at {ref_sm_clock} MHz\"\n+        assert abs(cur_mem_clock - ref_mem_clock) < 10, f\"GPU SMs must run at {ref_mem_clock} MHz\"\n+        tflops = 1e-6 * 2 * 108 * 4 * 256 * ref_sm_clock\n+        gbps = 640 * 2 * ref_mem_clock * 1e-3\n+        yield tflops, gbps\n+    finally:\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-pm\", \"0\"])\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-rgc\"])\n+        subprocess.check_output([\"nvidia-smi\", \"-i\", \"0\", \"-rmc\"])\n+\n \n def get_max_simd_tflops(dtype: torch.dtype, backend=None, device=None):\n     if not backend:"}]