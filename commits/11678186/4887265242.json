[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 8, "deletions": 9, "changes": 17, "file_content_changes": "@@ -435,20 +435,19 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n       auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n       auto order = triton::gpu::getOrder(layout);\n       auto shapePerCTA = triton::gpu::getShapePerCTA(layout, shape);\n+      Value warpSize = i32_val(32);\n+      Value laneId = urem(tid, warpSize);\n+      Value warpId = udiv(tid, warpSize);\n+      SmallVector<Value> multiDimWarpId =\n+          delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+      SmallVector<Value> multiDimThreadId =\n+          delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n       for (unsigned dim = 0; dim < rank; ++dim) {\n         // if there is no data replication across threads on this dimension\n         if (shape[dim] >= shapePerCTA[dim])\n           continue;\n         // Otherwise, we need to mask threads that will replicate data on this\n-        // dimension\n-        Value warpSize = i32_val(32);\n-        Value laneId = urem(tid, warpSize);\n-        Value warpId = udiv(tid, warpSize);\n-        SmallVector<Value> multiDimWarpId =\n-            delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n-        SmallVector<Value> multiDimThreadId =\n-            delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n-        // Calculate the thread index on this dimension for the CTA\n+        // dimension. Calculate the thread index on this dimension for the CTA\n         Value threadDim =\n             add(mul(multiDimWarpId[dim], i32_val(threadsPerWarp[dim])),\n                 multiDimThreadId[dim]);"}]