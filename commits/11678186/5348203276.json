[{"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 59, "deletions": 51, "changes": 110, "file_content_changes": "@@ -78,16 +78,15 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n \n public:\n   MoveOpAfterLayoutConversion(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, context) {}\n \n   static mlir::LogicalResult\n   isBlockedToDotOperand(mlir::Operation *op,\n                         triton::gpu::DotOperandEncodingAttr &retEncoding,\n                         triton::gpu::BlockedEncodingAttr &srcEncoding) {\n-    if (!op)\n+    auto cvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(op);\n+    if (!cvt)\n       return failure();\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n     auto srcTy = cvt.getOperand().getType().cast<RankedTensorType>();\n     auto retTy = cvt.getResult().getType().dyn_cast<RankedTensorType>();\n     retEncoding =\n@@ -135,58 +134,67 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n-    triton::gpu::DotOperandEncodingAttr retEncoding;\n-    triton::gpu::BlockedEncodingAttr srcEncoding;\n-    if (isBlockedToDotOperand(op, retEncoding, srcEncoding).failed())\n-      return mlir::failure();\n-\n+    auto dotOp = cast<triton::DotOp>(op);\n     // only supports dot NT\n-    auto users = cvt->getUsers();\n-    auto dotOp = dyn_cast_or_null<DotOp>(*users.begin());\n-    if (!dotOp)\n-      return failure();\n     if (!isDotNT(dotOp))\n       return failure();\n-\n-    // don't move things around when cvt operand is a block arg\n-    Operation *argOp = cvt.getOperand().getDefiningOp();\n-    if (!argOp)\n-      return failure();\n-    //\n-    SetVector<Operation *> processed;\n-    SetVector<Attribute> layout;\n-    llvm::MapVector<Value, Attribute> toConvert;\n-    int numCvts = simulateBackwardRematerialization(cvt, processed, layout,\n-                                                    toConvert, retEncoding);\n-    if (numCvts > 1 || toConvert.size() == 1)\n-      return failure();\n-    for (Operation *op : processed) {\n-      if (op->getNumOperands() != 1)\n+    bool changed = false;\n+    for (Value operand : {dotOp.getOperand(0), dotOp.getOperand(1)}) {\n+      auto cvt = operand.getDefiningOp<triton::gpu::ConvertLayoutOp>();\n+      triton::gpu::DotOperandEncodingAttr retEncoding;\n+      triton::gpu::BlockedEncodingAttr srcEncoding;\n+      bool failed =\n+          isBlockedToDotOperand(cvt, retEncoding, srcEncoding).failed();\n+      assert(!failed);\n+\n+      // don't move things around when cvt operand is a block arg\n+      Operation *argOp = cvt.getOperand().getDefiningOp();\n+      if (!argOp)\n+        continue;\n+      SetVector<Operation *> processed;\n+      SetVector<Attribute> layout;\n+      llvm::MapVector<Value, Attribute> toConvert;\n+      int numCvts = simulateBackwardRematerialization(cvt, processed, layout,\n+                                                      toConvert, retEncoding);\n+      if (numCvts > 1 || toConvert.size() == 1)\n         continue;\n-      auto srcTy = op->getOperand(0).getType().cast<RankedTensorType>();\n-      auto dstTy = op->getResult(0).getType().cast<RankedTensorType>();\n-      // we don't want to push conversions backward if there is a downcast\n-      // since it would result in more shared memory traffic\n-      if (srcTy.getElementType().getIntOrFloatBitWidth() >\n-          dstTy.getElementType().getIntOrFloatBitWidth())\n-        return failure();\n-      // we only push back when the first op in the chain has a load operand\n-      if ((op == processed.back()) &&\n-          !isa<triton::LoadOp>(op->getOperand(0).getDefiningOp()))\n-        return failure();\n-      // we don't want to use ldmatrix for 8-bit data that requires trans\n-      // since Nvidia GPUs can't do it efficiently\n-      int kOrder = retEncoding.getOpIdx() ^ 1;\n-      bool isTrans = kOrder != srcEncoding.getOrder()[0];\n-      bool isInt8 = srcTy.getElementType().getIntOrFloatBitWidth() == 8;\n-      if (isTrans && isInt8)\n-        return failure();\n+      bool replaceOperand = true;\n+      for (Operation *op : processed) {\n+        if (op->getNumOperands() != 1)\n+          continue;\n+        auto srcTy = op->getOperand(0).getType().cast<RankedTensorType>();\n+        auto dstTy = op->getResult(0).getType().cast<RankedTensorType>();\n+        // we don't want to push conversions backward if there is a downcast\n+        // since it would result in more shared memory traffic\n+        if (srcTy.getElementType().getIntOrFloatBitWidth() >\n+            dstTy.getElementType().getIntOrFloatBitWidth()) {\n+          replaceOperand = false;\n+          break;\n+        }\n+        // we only push back when the first op in the chain has a load operand\n+        if ((op == processed.back()) &&\n+            !isa<triton::LoadOp>(op->getOperand(0).getDefiningOp())) {\n+          replaceOperand = false;\n+          break;\n+        }\n+        // we don't want to use ldmatrix for 8-bit data that requires trans\n+        // since Nvidia GPUs can't do it efficiently\n+        int kOrder = retEncoding.getOpIdx() ^ 1;\n+        bool isTrans = kOrder != srcEncoding.getOrder()[0];\n+        bool isInt8 = srcTy.getElementType().getIntOrFloatBitWidth() == 8;\n+        if (isTrans && isInt8) {\n+          replaceOperand = false;\n+          break;\n+        }\n+      }\n+      if (!replaceOperand)\n+        continue;\n+      IRMapping mapping;\n+      rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n+      rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n+      changed = true;\n     }\n-    IRMapping mapping;\n-    rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n-    rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n-    return mlir::success();\n+    return mlir::success(changed);\n   }\n };\n "}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 35, "deletions": 0, "changes": 35, "file_content_changes": "@@ -126,3 +126,38 @@ tt.func @push_elementwise5(\n }\n \n }\n+\n+// -----\n+\n+#blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+// CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+// CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+// CHECK: #[[MMA:.*]] = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+\n+// CHECK: tt.func @push_convert_both_operands\n+// CHECK: %[[ALOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BA]]>\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] : (tensor<16x16xf16, #[[BA]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 1}>>\n+// CHECK: %[[BLOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BB]]>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %[[BLOAD]] : (tensor<16x16xf16, #[[BB]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 1}>>\n+// CHECK: %[[AEXT:.*]] = arith.extf %[[ACVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 1}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 1}>>\n+// CHECK: %[[BEXT:.*]] = arith.extf %[[BCVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 1}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 1}>>\n+// CHECK: tt.dot %[[AEXT]], %[[BEXT]], %{{.*}} {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 1}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 1}>> -> tensor<16x16xf32, #mma>\n+tt.func @push_convert_both_operands(\n+                   %pa: tensor<16x16x!tt.ptr<f16>, #blockedA> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #blockedB> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #mma>) -> tensor<16x16xf32, #mma>{\n+  %a = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedA>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedB>\n+  %ae = arith.extf %a : tensor<16x16xf16, #blockedA> to tensor<16x16xf32, #blockedA>\n+  %be = arith.extf %b : tensor<16x16xf16, #blockedB> to tensor<16x16xf32, #blockedB>\n+  %al = triton_gpu.convert_layout %ae : (tensor<16x16xf32, #blockedA>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+  %bl = triton_gpu.convert_layout %be : (tensor<16x16xf32, #blockedB>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+  %r = tt.dot %al, %bl, %c {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<16x16xf32, #mma>\n+  tt.return %r : tensor<16x16xf32, #mma>\n+}\n+\n+}"}]