[{"filename": "python/tests/test_core.py", "status": "modified", "additions": 43, "deletions": 48, "changes": 91, "file_content_changes": "@@ -144,7 +144,7 @@ def kernel(Z, X, SIZE: tl.constexpr):\n     # triton result\n     x_tri = to_triton(x, device=device, dst_type=dtype_x)\n     z_tri = to_triton(np.empty_like(z_ref), device=device, dst_type=dtype_x)\n-    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4)\n+    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4, extern_libs={\"libdevice\": \"/usr/local/cuda/nvvm/libdevice/libdevice.10.bc\"})\n     # compare\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n@@ -463,17 +463,12 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n # # test math ops\n # # ----------------\n \n-# TODO: Math module\n-# # @pytest.mark.parametrize(\"expr\", [\n-# #     'exp', 'log', 'cos', 'sin'\n-# # ])\n \n-\n-# @pytest.mark.parametrize(\"expr\", [\n-#     'exp', 'log', 'cos', 'sin'\n-# ])\n-# def test_math_op(expr, device='cuda'):\n-#     _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n+@pytest.mark.parametrize(\"expr\", [\n+    'exp', 'log', 'cos', 'sin'\n+])\n+def test_math_op(expr, device='cuda'):\n+    _test_unary('float32', f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n \n # # ----------------\n@@ -1545,43 +1540,43 @@ def _kernel(dst):\n # # -------------\n \n \n-# @pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n-#                          [('int32', 'libdevice.ffs', ''),\n-#                           ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n-#                           ('float64', 'libdevice.norm4d', '')])\n-# def test_libdevice(dtype_str, expr, lib_path):\n+@pytest.mark.parametrize(\"dtype_str, expr, lib_path\",\n+                         [('int32', 'libdevice.ffs', ''),\n+                          ('float32', 'libdevice.pow', '/usr/local/cuda/nvvm/libdevice/libdevice.10.bc'),\n+                          ('float64', 'libdevice.norm4d', '')])\n+def test_libdevice(dtype_str, expr, lib_path):\n \n-#     @triton.jit\n-#     def kernel(X, Y, BLOCK: tl.constexpr):\n-#         x = tl.load(X + tl.arange(0, BLOCK))\n-#         y = GENERATE_TEST_HERE\n-#         tl.store(Y + tl.arange(0, BLOCK), y)\n+    @triton.jit\n+    def kernel(X, Y, BLOCK: tl.constexpr):\n+        x = tl.load(X + tl.arange(0, BLOCK))\n+        y = GENERATE_TEST_HERE\n+        tl.store(Y + tl.arange(0, BLOCK), y)\n \n-#     shape = (128, )\n-#     rs = RandomState(17)\n-#     # limit the range of integers so that the sum does not overflow\n-#     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n-\n-#     if expr == 'libdevice.ffs':\n-#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n-#         y_ref = np.zeros(shape, dtype=x.dtype)\n-#         for i in range(shape[0]):\n-#             y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n-#     elif expr == 'libdevice.pow':\n-#         # numpy does not allow negative factors in power, so we use abs()\n-#         x = np.abs(x)\n-#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n-#         y_ref = np.power(x, x)\n-#     elif expr == 'libdevice.norm4d':\n-#         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n-#         y_ref = np.sqrt(4 * np.power(x, 2))\n+    shape = (128, )\n+    rs = RandomState(17)\n+    # limit the range of integers so that the sum does not overflow\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n \n-#     x_tri = to_triton(x)\n-#     # triton result\n-#     y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n-#     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n-#     # compare\n-#     if expr == 'libdevice.ffs':\n-#         np.testing.assert_equal(y_ref, to_numpy(y_tri))\n-#     else:\n-#         np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n+    if expr == 'libdevice.ffs':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.ffs(x)'})\n+        y_ref = np.zeros(shape, dtype=x.dtype)\n+        for i in range(shape[0]):\n+            y_ref[i] = (int(x[i]) & int(-x[i])).bit_length()\n+    elif expr == 'libdevice.pow':\n+        # numpy does not allow negative factors in power, so we use abs()\n+        x = np.abs(x)\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.pow(x, x)'})\n+        y_ref = np.power(x, x)\n+    elif expr == 'libdevice.norm4d':\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.libdevice.norm4d(x, x, x, x)'})\n+        y_ref = np.sqrt(4 * np.power(x, 2))\n+\n+    x_tri = to_triton(x)\n+    # triton result\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n+    # compare\n+    if expr == 'libdevice.ffs':\n+        np.testing.assert_equal(y_ref, to_numpy(y_tri))\n+    else:\n+        np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)"}]