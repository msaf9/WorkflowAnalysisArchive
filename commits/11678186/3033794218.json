[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1317,7 +1317,7 @@ def kernel(VALUE, X):\n     x = torch.tensor([3.14159], device='cuda')\n \n     if overflow:\n-        with pytest.raises(RuntimeError, match='integer overflow'):\n+        with pytest.raises(OverflowError, match='int too large'):\n             kernel[(1, )](value, x)\n     else:\n         kernel[(1, )](value, x)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "file_content_changes": "@@ -864,7 +864,7 @@ def ptx_get_kernel_name(ptx: str) -> str:\n         if line.startswith('// .globl'):\n             return line.split()[-1]\n \n-def _compile(fn, signature: str, device: int = -1, constants=dict(), specialization=_triton.code_gen.instance_descriptor(), num_warps: int = 4, num_stages: int = 3, output: str = \"ttgir\") -> Tuple[str, int, str]:\n+def _compile(fn, signature: str, device: int = -1, constants=dict(), specialization=_triton.code_gen.instance_descriptor(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, output: str = \"ttgir\") -> Tuple[str, int, str]:\n     valid_outputs = (\"ttir\", \"ttgir\", \"ptx\", \"cubin\")\n     assert output in valid_outputs, \"output should be one of [%s], but get \\\"%s\\\"\" % (','.join(valid_outputs), output)\n \n@@ -876,7 +876,8 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     assert output == \"cubin\"\n     assert torch.version.hip is None\n     backend = _triton.runtime.backend.CUDA\n-    extern_libs = {}\n+    if extern_libs is None:\n+      extern_libs = dict()\n     name, asm, shared_mem = _triton.code_gen.compile_ttir(backend, module, device, num_warps, num_stages, extern_libs)\n     return asm, shared_mem, name\n   \n@@ -1026,6 +1027,7 @@ def _extracted_type(ty):\n         return \"long\"\n       return {\n         'u32': 'uint32_t',\n+        'u64': 'uint64_t',\n         'fp32': 'float',\n         'fp64': 'double',\n       }[ty]\n@@ -1192,7 +1194,7 @@ def make_shared_object(fn, signature, num_warps, binaries, tmpdir):\n         return f.read()\n \n \n-def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, configs = None):\n+def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs = None, configs = None):\n     # we get the kernel, i.e. the first function generated in the module\n     if configs is None:\n       assert False, \"automatic specialization is not supported yet\"\n@@ -1208,7 +1210,7 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n     # compile all the configs\n     binaries = []\n     for config in configs:\n-        binaries.append(_compile(fn, signature, device, constants, config, num_warps, num_stages, \"cubin\"))\n+        binaries.append(_compile(fn, signature, device, constants, config, num_warps, num_stages, extern_libs, \"cubin\"))\n     # generate and compile glue code into shared object\n     with tempfile.TemporaryDirectory() as tmpdir:\n       so = make_shared_object(fn, signature, num_warps, binaries, tmpdir)"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 12, "deletions": 7, "changes": 19, "file_content_changes": "@@ -158,10 +158,12 @@ def _key_of(arg):\n         if hasattr(arg, \"dtype\"):\n             return arg.dtype\n         elif isinstance(arg, int):\n-            if -0x80000000 <= arg and arg <= 0x7FFFFFFF:\n+            if -2**31 <= arg and arg <= 2**31 - 1:\n               return \"i32\"\n-            elif 0x80000000 <= arg and arg <= 0xFFFFFFFF:\n+            elif 2**31 <= arg and arg <= 2**32 - 1:\n               return \"u32\"\n+            elif 2**63 <= arg and arg <= 2**64 - 1:\n+              return \"u64\"\n             else:\n               return \"i64\"\n         elif isinstance(arg, float):\n@@ -226,7 +228,7 @@ def _make_signature(self, key):\n         constants = {i: k for i, k in enumerate(key) if i in self.constexprs}\n         return signature, constants\n     \n-    def _call_hook(self, key, signature, device, constants, num_warps, num_stages, configs):\n+    def _call_hook(self, key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n         if JITFunction.cache_hook is None:\n           return False\n         # TODO: assemble compilation-key into human-readable format\n@@ -244,7 +246,7 @@ def warmup(kwargs):\n                 self.cache[key] = bin\n \n         kwargs = dict(fn=self, signature=signature, device=device, constants=constants, \n-                     num_warps=num_warps, num_stages=num_stages, \n+                     num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs,\n                      configs=configs)\n \n         return JITFunction.cache_hook(key=key, repr=repr, fn=LegacyCompiler(), compile={\"key\": key, **kwargs}, is_manual_warmup=False, already_compiled=False)\n@@ -268,10 +270,13 @@ def _make_launcher(self):\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n \n         src = f\"\"\"\n-def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, stream=None):\n+def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None):\n     sig_key =  ({regular_keys}, {constexpr_keys})\n     spec_key = ({spec_keys})\n     key = (version_key, sig_key, spec_key)\n+    if not extern_libs is None:\n+      key = (key, tuple(extern_libs.items()))\n+    assert num_warps > 0 and (num_warps & (num_warps - 1)) == 0, \"num_warps must be a power of 2\"\n     if callable(grid):\n         grid = grid({{{grid_args}}})\n     grid_size = len(grid)\n@@ -291,8 +296,8 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n       constants |= {{i: None for i, arg in enumerate(args) if arg is None}}\n       configs = [self._get_config(*args)]\n       device = 0\n-      if not self._call_hook(key, signature, device, constants, num_warps, num_stages, configs):\n-        bin = triton.compile(self, signature, device, constants, num_warps, num_stages, configs=configs)\n+      if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n+        bin = triton.compile(self, signature, device, constants, num_warps, num_stages, extern_libs=extern_libs, configs=configs)\n         bin.c_wrapper(grid_0, grid_1, grid_2, stream, *args)\n         self.cache[key] = bin\n         return bin"}]