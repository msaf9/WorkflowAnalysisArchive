[{"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -198,7 +198,7 @@ def forward(ctx, q, k, v, sm_scale):\n         # only support for Ampere now\n         capability = torch.cuda.get_device_capability()\n         if capability[0] < 8:\n-            raise RuntimeError(\"Flash attention currently only supported for compute capability < 80\")\n+            raise RuntimeError(\"Flash attention currently only supported for compute capability >= 80\")\n         BLOCK = 128\n         # shape constraints\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]"}]