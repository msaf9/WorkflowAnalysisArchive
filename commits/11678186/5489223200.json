[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 50, "deletions": 14, "changes": 64, "file_content_changes": "@@ -14,7 +14,7 @@ using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n-\n+static int counter = 0;\n // Forward declarations\n namespace SharedToDotOperandMMAv1 {\n using CoordTy = SmallVector<Value>;\n@@ -65,7 +65,7 @@ bool canUseLdMatrix(RankedTensorType srcTy, RankedTensorType dstTy) {\n     isDstMmaV1 = mmaLayout.isVolta();\n   }\n   return dstLayout.isa<MmaEncodingAttr>() && !isDstMmaV1 &&\n-         triton::gpu::getWarpsPerCTA(dstLayout)[1] == 1 &&\n+         // triton::gpu::getWarpsPerCTA(dstLayout)[1] == 1 &&\n          srcTy.getElementType().getIntOrFloatBitWidth() != 32;\n }\n struct ConvertLayoutOpConversion\n@@ -282,11 +282,17 @@ struct ConvertLayoutOpConversion\n           else if (isPtr)\n             currVal = ptrtoint(llvmElemTy, currVal);\n           word = insert_element(wordTy, word, currVal, i32_val(i % minVec));\n+          // if (counter == 1)\n+          //   mlir::LLVM::vprintf(\"store - tid: %d, currVal: %f\",\n+          //                       {tid_val(), currVal}, rewriter);\n \n           if (i % minVec == minVec - 1) {\n \n             Value smemAddr = sharedPtrs[linearCTAIdInRep * accumSizePerThread +\n                                         i / minVec * minVec];\n+            if (counter == 1)\n+              mlir::LLVM::vprintf(\"store - tid: %d, smemAddr: %d, currVal: %f\",\n+                                  {tid_val(), smemAddr, currVal}, rewriter);\n \n             smemAddr = bitcast(smemAddr, ptr_ty(wordTy, 3));\n \n@@ -325,14 +331,16 @@ struct ConvertLayoutOpConversion\n                          ArrayRef<unsigned> repShape, ArrayRef<unsigned> outOrd,\n                          SmallVector<Value> &vals, Value smemBase,\n                          triton::gpu::SharedEncodingAttr sharedLayout) const {\n+    std::cout << \"repShape = \" << repShape[0] << \", \" << repShape[1]\n+              << std::endl;\n     auto rank = type.getRank();\n     auto layout = type.getEncoding();\n     auto accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n     auto sizePerThread = getSizePerThread(layout);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTAs(rank);\n     auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n-    shapePerCTA[1] = 16; // todo: hack\n+    shapePerCTA[1] = 16 * triton::gpu::getWarpsPerCTA(layout)[1]; // todo: hack\n     auto order = getOrder(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n       numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n@@ -355,16 +363,22 @@ struct ConvertLayoutOpConversion\n     auto smemStrides = getStridesFromShapeAndOrder(\n         convertType<int64_t>(repShape), getOrder(sharedLayout), loc, rewriter);\n \n-    SmallVector<Value> smemOffsetVals = {i32_val(0), i32_val(0)};\n+    for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ctaId++) {\n+      Value warp = udiv(tid_val(), i32_val(32));\n \n-    SharedMemoryObject smemObj(smemBase, smemStrides, smemOffsetVals);\n-    Value res = SharedToDotOperandMMAv2::convertLayout(\n-        dstDotOp.getOpIdx(), rewriter, loc, sharedTy, dstDotOp, smemObj,\n-        getTypeConverter(), tid_val());\n+      SmallVector<Value> multiDimWarpId = delinearize(\n+          rewriter, loc, warp, triton::gpu::getWarpsPerCTA(layout), order);\n+      SmallVector<Value> smemOffsetVals = {i32_val(0),\n+                                           mul(multiDimWarpId[1], i32_val(16))};\n \n-    SmallVector<Value> valsInt32 =\n-        getTypeConverter()->unpackLLElements(loc, res, rewriter, i32_ty);\n-    for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ctaId++) {\n+      SharedMemoryObject smemObj(smemBase, smemStrides, smemOffsetVals);\n+      Value res = SharedToDotOperandMMAv2::convertLayout(\n+          dstDotOp.getOpIdx(), rewriter, loc, sharedTy, dstDotOp, smemObj,\n+          getTypeConverter(), tid_val());\n+\n+      SmallVector<Value> valsInt32 =\n+          getTypeConverter()->unpackLLElements(loc, res, rewriter, i32_ty);\n+      std::cout << \"valsInt32.size() = \" << valsInt32.size() << std::endl;\n       auto multiDimCTAInRepId =\n           getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n       SmallVector<unsigned> multiDimCTAId(rank);\n@@ -376,6 +390,12 @@ struct ConvertLayoutOpConversion\n           getLinearIndex<unsigned>(multiDimCTAId, numCTAs, order);\n       auto linearCTAIdInRep =\n           getLinearIndex<unsigned>(multiDimCTAInRepId, numCTAs, order);\n+      std::cout << \"multiDimCTAId = \" << multiDimCTAId[0] << \", \"\n+                << multiDimCTAId[1] << std::endl;\n+      std::cout << \"numCTAs = \" << numCTAs[0] << \", \" << numCTAs[1]\n+                << std::endl;\n+      std::cout << \"ctaId: \" << ctaId << \" linearCTAId: \" << linearCTAId\n+                << std::endl;\n       castInt32To2xF16(vals, valsInt32[ctaId * 4], linearCTAId * 8 + 0, loc,\n                        rewriter);\n       castInt32To2xF16(vals, valsInt32[ctaId * 4 + 2], linearCTAId * 8 + 2, loc,\n@@ -385,6 +405,20 @@ struct ConvertLayoutOpConversion\n       castInt32To2xF16(vals, valsInt32[ctaId * 4 + 3], linearCTAId * 8 + 6, loc,\n                        rewriter);\n     }\n+    mlir::LLVM::vprintf(\n+        \"loaded - tid: %d, vals: %f, %f, %f, %f, %f, %f, %f, %f\",\n+        {\n+            tid_val(),\n+            vals[0],\n+            vals[1],\n+            vals[2],\n+            vals[3],\n+            vals[4],\n+            vals[5],\n+            vals[6],\n+            vals[7],\n+        },\n+        rewriter);\n     // Value currVal = extract_element(llvmElemTy, valVec, i32_val(v));\n     // if (isInt1)\n     //   currVal =\n@@ -534,7 +568,8 @@ struct ConvertLayoutOpConversion\n     auto srcShapePerCTA = getShapePerCTA(srcLayout, srcTy.getShape());\n     auto dstShapePerCTA = getShapePerCTA(dstLayout, shape);\n     if (useLdMatrix) {\n-      dstShapePerCTA[1] = 16; // todo: hack\n+      dstShapePerCTA[1] =\n+          16 * triton::gpu::getWarpsPerCTA(dstLayout)[1]; // todo: hack\n     }\n \n     // For Volta, all the coords for a CTA are calculated.\n@@ -573,6 +608,7 @@ struct ConvertLayoutOpConversion\n     unsigned outElems = getTotalElemsPerThread(dstTy);\n     auto outOrd = getOrder(dstLayout);\n     SmallVector<Value> outVals(outElems);\n+    std::cout << \"outElems: \" << outElems << \"\\n\";\n \n     unsigned inVec = 0;\n     unsigned outVec = 0;\n@@ -591,7 +627,7 @@ struct ConvertLayoutOpConversion\n       // maxPhase\n       repShape = getScratchConfigForCvtLayout(op, inVec, outVec, false);\n       if (useLdMatrix)\n-        repShape[1] = ceil<unsigned>(repShape[1], 16) * 16; // todo: hack\n+        repShape[1] = dstShapePerCTA[1]; // todo: hack\n       auto dstDotOp = triton::gpu::DotOperandEncodingAttr::get(\n           getContext(), 0, dstLayout, dstTy.getElementType());\n       sharedLayout = triton::gpu::SharedEncodingAttr::get(\n@@ -645,7 +681,7 @@ struct ConvertLayoutOpConversion\n     Value result =\n         getTypeConverter()->packLLElements(loc, outVals, rewriter, dstTy);\n     rewriter.replaceOp(op, result);\n-\n+    counter++;\n     return success();\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 49, "deletions": 22, "changes": 71, "file_content_changes": "@@ -16,6 +16,16 @@ using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n+SmallVector<Value> castInt32To2xF16(Value i32Val, Location loc,\n+                                    ConversionPatternRewriter &rewriter) {\n+  SmallVector<Value> ret;\n+  auto floatVectorType = vec_ty(f16_ty, 2);\n+  auto vec = rewriter.create<LLVM::BitcastOp>(loc, floatVectorType, i32Val);\n+  ret.push_back(extract_element(f16_ty, vec, i32_val(0)));\n+  ret.push_back(extract_element(f16_ty, vec, i32_val(1)));\n+  return ret;\n+}\n+\n // Data loader for mma.16816 instruction.\n class MMA16816SmemLoader {\n public:\n@@ -31,9 +41,9 @@ class MMA16816SmemLoader {\n   // lane = thread % 32\n   // warpOff = (thread/32) % warpsPerTile(0)\n   llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane,\n-                                          Value cSwizzleOffset) {\n+                                          Value cSwizzleOffset, Value thread) {\n     if (canUseLdmatrix)\n-      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n+      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset, thread);\n     else\n       return computeLdsMatOffs(warpOff, lane, cSwizzleOffset);\n     return {};\n@@ -44,16 +54,16 @@ class MMA16816SmemLoader {\n   // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n   // mapped to.\n   SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n-                                            Value cSwizzleOffset);\n+                                            Value cSwizzleOffset, Value thread);\n   // compute 8-bit matrix offset.\n   SmallVector<Value> computeLdsMatOffs(Value warpOff, Value lane,\n                                        Value cSwizzleOffset);\n \n   // Load 4 matrices and returns 4 vec<2> elements.\n   std::tuple<Value, Value, Value, Value> loadX4(int mat0, int mat1,\n                                                 ArrayRef<Value> ptrs,\n-                                                Type matTy,\n-                                                Type shemPtrTy) const;\n+                                                Type matTy, Type shemPtrTy,\n+                                                Value thread) const;\n \n private:\n   SmallVector<uint32_t> order;\n@@ -96,7 +106,7 @@ class MMA16816SmemLoader {\n \n SmallVector<Value>\n MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n-                                           Value cSwizzleOffset) {\n+                                           Value cSwizzleOffset, Value thread) {\n   // 4x4 matrices\n   Value rowInMat = urem(lane, i32_val(8)); // row in the 8x8 matrix\n   Value matIndex =\n@@ -166,6 +176,8 @@ MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n     Value contiguousIndexSwizzled = xor_(contiguousIndex, phase);\n     offs[i] = add(mul(contiguousIndexSwizzled, i32_val(contiguousMatShape)),\n                   mul(rowOffset, stridedSmemOffset));\n+    mlir::LLVM::vprintf(std::to_string(i) + \" tid: %d, offs[i]: %d\",\n+                        {thread, offs[i]}, rewriter);\n   }\n \n   return offs;\n@@ -265,7 +277,7 @@ SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value warpOff,\n \n std::tuple<Value, Value, Value, Value>\n MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> ptrs, Type matTy,\n-                           Type shemPtrTy) const {\n+                           Type shemPtrTy, Value thread) const {\n   assert(mat0 % 2 == 0 && mat1 % 2 == 0 && \"smem matrix load must be aligned\");\n   int matIdx[2] = {mat0, mat1};\n \n@@ -275,7 +287,7 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> ptrs, Type matTy,\n     ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n   else\n     ptrIdx = matIdx[order[0]] * 4 / elemBytes;\n-\n+  std::cout << \"ptrIdx = \" << ptrIdx << std::endl;\n   // The main difference with the original triton code is we removed the\n   // prefetch-related logic here for the upstream optimizer phase should\n   // take care with it, and that is transparent in dot conversion.\n@@ -305,7 +317,8 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> ptrs, Type matTy,\n         mul(i32_val(matIdx[order[1]] * stridedLoadMatOffset * stridedMatShape),\n             stridedSmemOffset);\n     Value readPtr = gep(shemPtrTy, ptr, stridedOffset);\n-\n+    mlir::LLVM::vprintf(\"tid: %d, ptr: %d, stridedOffset: %d, readPtr: %d\",\n+                        {thread, ptr, stridedOffset, readPtr}, rewriter);\n     PTXBuilder builder;\n     // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n     // thread.\n@@ -475,14 +488,13 @@ Value composeValuesToDotOperandLayoutStruct(\n   return result;\n }\n \n-std::function<void(int, int)>\n-getLoadMatrixFn(RankedTensorType tensorTy, const SharedMemoryObject &smemObj,\n-                MmaEncodingAttr mmaLayout, int warpsPerTile, uint32_t kOrder,\n-                int kWidth, SmallVector<int> instrShape,\n-                SmallVector<int> matShape, Value warpId, Value lane,\n-                ValueTable &vals, bool isA,\n-                TritonGPUToLLVMTypeConverter *typeConverter,\n-                ConversionPatternRewriter &rewriter, Location loc) {\n+std::function<void(int, int)> getLoadMatrixFn(\n+    RankedTensorType tensorTy, const SharedMemoryObject &smemObj,\n+    MmaEncodingAttr mmaLayout, int warpsPerTile, uint32_t kOrder, int kWidth,\n+    SmallVector<int> instrShape, SmallVector<int> matShape, Value warpId,\n+    Value lane, ValueTable &vals, bool isA,\n+    TritonGPUToLLVMTypeConverter *typeConverter,\n+    ConversionPatternRewriter &rewriter, Location loc, Value thread) {\n   Type eltTy = tensorTy.getElementType();\n   // We assumes that the input operand of Dot should be from shared layout.\n   // TODO(Superjomn) Consider other layouts if needed later.\n@@ -502,20 +514,26 @@ getLoadMatrixFn(RankedTensorType tensorTy, const SharedMemoryObject &smemObj,\n     // Offset of a slice within the original tensor in shared memory\n     Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n     SmallVector<Value> offs =\n-        loader.computeOffsets(warpId, lane, cSwizzleOffset);\n+        loader.computeOffsets(warpId, lane, cSwizzleOffset, thread);\n     // initialize pointers\n     const int numPtrs = loader.getNumPtrs();\n     SmallVector<Value> ptrs(numPtrs);\n     Value smemBase = smemObj.getBaseBeforeSlice(order[0], loc, rewriter);\n     Type smemPtrTy = getSharedMemPtrTy(eltTy);\n-    for (int i = 0; i < numPtrs; ++i)\n+    for (int i = 0; i < numPtrs; ++i) {\n       ptrs[i] = bitcast(gep(smemPtrTy, smemBase, offs[i]), smemPtrTy);\n+      mlir::LLVM::vprintf(\n+          std::to_string(i) +\n+              \" Here - tid: %d, smemBase: %d, offs[i]: %d, ptrs[i]: %d\",\n+          {thread, smemBase, offs[i], ptrs[i]}, rewriter);\n+    }\n+\n     // actually load from shared memory\n     auto matTy = LLVM::LLVMStructType::getLiteral(eltTy.getContext(),\n                                                   SmallVector<Type>(4, i32_ty));\n     auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n         (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, ptrs,\n-        matTy, getSharedMemPtrTy(eltTy));\n+        matTy, getSharedMemPtrTy(eltTy), thread);\n     if (!isA)\n       std::swap(ha1, ha2);\n     // the following is incorrect\n@@ -529,6 +547,14 @@ getLoadMatrixFn(RankedTensorType tensorTy, const SharedMemoryObject &smemObj,\n     vals[{a + 1, b}] = ha1;\n     vals[{a, b + 1}] = ha2;\n     vals[{a + 1, b + 1}] = ha3;\n+    auto vec0 = castInt32To2xF16(ha0, loc, rewriter);\n+    auto vec1 = castInt32To2xF16(ha1, loc, rewriter);\n+    auto vec2 = castInt32To2xF16(ha2, loc, rewriter);\n+    auto vec3 = castInt32To2xF16(ha3, loc, rewriter);\n+    mlir::LLVM::vprintf(\"tid: %d, vals: %f, %f, %f, %f, %f, %f, %f, %f\",\n+                        {thread, vec0[0], vec0[1], vec1[0], vec1[1], vec2[0],\n+                         vec2[1], vec3[0], vec3[1]},\n+                        rewriter);\n   };\n \n   return load;\n@@ -578,18 +604,19 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc,\n         1 /*kOrder*/, kWidth, {mmaInstrM, mmaInstrK} /*instrShape*/,\n         {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, lane /*laneId*/,\n         vals /*vals*/, isA /*isA*/, typeConverter /* typeConverter */,\n-        rewriter /*rewriter*/, loc /*loc*/);\n+        rewriter /*rewriter*/, loc /*loc*/, thread);\n   else\n     loadFn = getLoadMatrixFn(\n         tensorTy, smemObj, mmaLayout, warpsPerTile /*warpsPerTile*/,\n         0 /*kOrder*/, kWidth, {mmaInstrK, mmaInstrN} /*instrShape*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, lane /*laneId*/,\n         vals /*vals*/, isA /*isA*/, typeConverter /* typeConverter */,\n-        rewriter /*rewriter*/, loc /*loc*/);\n+        rewriter /*rewriter*/, loc /*loc*/, thread);\n \n   // Perform loading.\n   int numRepOuter = isA ? numRep[0] : std::max<int>(numRep[1] / 2, 1);\n   int numRepK = isA ? numRep[1] : numRep[0];\n+  numRepK = 1;\n   for (int m = 0; m < numRepOuter; ++m)\n     for (int k = 0; k < numRepK; ++k)\n       loadFn(2 * m, 2 * k);"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -3064,11 +3064,11 @@ def isLayoutMmaV2(layout):\n \n \n def test_swizzling():\n-    M, N = 256, 256\n+    M, N = 32, 32\n     dtype = 'float16'\n     ir = f\"\"\"\n     #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}}>\n-    #mma = #triton_gpu.mma<{{versionMajor=2, versionMinor=0, warpsPerCTA=[4, 1]}}>\n+    #mma = #triton_gpu.mma<{{versionMajor=2, versionMinor=0, warpsPerCTA=[2, 2]}}>\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n   tt.func public @kernel_0d1d(%arg0: !tt.ptr<f16> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<f16> {{tt.divisibility = 16 : i32}}) {{\n     %cst = arith.constant dense<{M}> : tensor<{M}x1xi32, #blocked>"}]