[{"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -401,7 +401,8 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n LogicalResult mlir::triton::DotOp::verify() {\n   auto aTy = getOperand(0).getType().cast<RankedTensorType>();\n   auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n-  if (aTy.getIntOrFloatBitWidth() != bTy.getIntOrFloatBitWidth())\n+  if (aTy.getElementType().getIntOrFloatBitWidth() !=\n+      bTy.getElementType().getIntOrFloatBitWidth())\n     return emitError(\n         \"element types of operands A and B must have same bit width\");\n   auto aEncoding = aTy.getEncoding();"}, {"filename": "test/Conversion/invalid.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -5,7 +5,7 @@\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   tt.func @convert_dot(%A: tensor<16x16xf32, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n-    // expected-error@+1 {{element types of operands A and B must match}}\n+    // expected-error@+1 {{element types of operands A and B must have same bit width}}\n     %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n         tensor<16x16xf32, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n     tt.return"}]