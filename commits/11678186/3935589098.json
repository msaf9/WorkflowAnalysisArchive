[{"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -1394,7 +1394,9 @@ struct MMA16816ConversionHelper {\n       unsigned colsPerThread = numRepN * 2;\n       PTXBuilder builder;\n       auto &mma = *builder.create(helper.getMmaInstr().str());\n-      auto retArgs = builder.newListOperand(4, \"=f\");\n+      // using =r for float32 works but leads to less readable ptx.\n+      bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n+      auto retArgs = builder.newListOperand(4, isIntMMA ? \"=r\" : \"=f\");\n       auto aArgs = builder.newListOperand({\n           {ha[{m, k}], \"r\"},\n           {ha[{m + 1, k}], \"r\"},"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -177,7 +177,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       PTXBuilder builder;\n       auto idx = getIdx(m, n);\n \n-      auto *resOprs = builder.newListOperand(8, \"=f\");\n+      // note: using \"=f\" for float leads to cleaner PTX\n+      bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n+      auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n       auto *AOprs = builder.newListOperand({\n           {ha.first, \"r\"},\n           {ha.second, \"r\"},"}]