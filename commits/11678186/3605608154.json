[{"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 16, "deletions": 5, "changes": 21, "file_content_changes": "@@ -685,6 +685,13 @@ class OptimizeBlockedToShared : public mlir::RewritePattern {\n       return failure();\n     if (srcBlockedLayout.getOrder() == dstSharedLayout.getOrder())\n       return failure();\n+    // For now only works if single use is transpose\n+    // TODO: rematerialize #shared uses\n+    auto users = op->getUsers();\n+    if (std::distance(users.begin(), users.end()) != 1 ||\n+        !isa<triton::TransOp>(*users.begin()))\n+      return failure();\n+\n     auto tmpShared = triton::gpu::SharedEncodingAttr::get(\n         op->getContext(), dstSharedLayout.getVec(),\n         dstSharedLayout.getPerPhase(), dstSharedLayout.getMaxPhase(),\n@@ -693,9 +700,15 @@ class OptimizeBlockedToShared : public mlir::RewritePattern {\n                                          srcType.getElementType(), tmpShared);\n     auto tmpCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op->getLoc(), tmpType, cvt.getOperand());\n-    auto newCvt = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        op->getLoc(), dstType, tmpCvt.getResult());\n-    rewriter.replaceOp(op, newCvt.getResult());\n+\n+    auto newDstType = RankedTensorType::get(\n+        users.begin()->getResultTypes()[0].cast<RankedTensorType>().getShape(),\n+        srcType.getElementType(), dstSharedLayout);\n+\n+    auto newTrans = rewriter.create<triton::TransOp>(op->getLoc(), newDstType,\n+                                                     tmpCvt.getResult());\n+\n+    rewriter.replaceOp(*users.begin(), newTrans.getResult());\n     return success();\n   }\n };\n@@ -792,8 +805,6 @@ class TritonGPUCombineOpsPass\n     MLIRContext *context = &getContext();\n     ModuleOp m = getOperation();\n \n-    // llvm::outs() << m << \"\\n\";\n-\n     mlir::RewritePatternSet patterns(context);\n \n     patterns.add<OptimizeBlockedToShared>(context);"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -32,7 +32,7 @@ def _fwd_kernel(\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n     off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n     off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n@@ -50,7 +50,7 @@ def _fwd_kernel(\n         # -- compute qk ----\n         k = tl.load(k_ptrs + start_n * stride_kn)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, k)\n+        qk += tl.dot(q, tl.trans(k))\n         qk *= sm_scale\n         qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n         # -- compute m_ij, p, l_ij\n@@ -165,26 +165,26 @@ def _bwd_kernel(\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.dot(q, k, trans_b=True)\n+            qk = tl.dot(q, tl.trans(k))\n             qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n             m = tl.load(m_ptrs + offs_m_curr)\n             p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n-            dv += tl.dot(p.to(tl.float16), do, trans_a=True)\n+            dv += tl.dot(tl.trans(p.to(tl.float16)), do)\n             # compute dp = dot(v, do)\n             Di = tl.load(D_ptrs + offs_m_curr)\n             dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n-            dp += tl.dot(do, v, trans_b=True)\n+            dp += tl.dot(do, tl.trans(v))\n             # compute ds = p * (dp - delta[:, None])\n             ds = p * dp * sm_scale\n             # compute dk = dot(ds.T, q)\n-            dk += tl.dot(ds.to(tl.float16), q, trans_a=True)\n-            # # compute dq\n+            dk += tl.dot(tl.trans(ds.to(tl.float16)), q)\n+            # compute dq\n             dq = tl.load(dq_ptrs)\n             dq += tl.dot(ds.to(tl.float16), k)\n             tl.store(dq_ptrs, dq)\n-            # # increment pointers\n+            # increment pointers\n             dq_ptrs += BLOCK_M * stride_qm\n             q_ptrs += BLOCK_M * stride_qm\n             do_ptrs += BLOCK_M * stride_qm\n@@ -273,7 +273,7 @@ def backward(ctx, do):\n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.1).requires_grad_()\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n     k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n     v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n     sm_scale = 0.2\n@@ -302,8 +302,8 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     # compare\n     triton.testing.assert_almost_equal(ref_out, tri_out)\n     triton.testing.assert_almost_equal(ref_dv, tri_dv)\n-    # triton.testing.assert_almost_equal(ref_dk, tri_dk)\n-    # triton.testing.assert_almost_equal(ref_dq, tri_dq)\n+    triton.testing.assert_almost_equal(ref_dk, tri_dk)\n+    triton.testing.assert_almost_equal(ref_dq, tri_dq)\n \n BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n # vary seq length for fixed head and batch=4"}]