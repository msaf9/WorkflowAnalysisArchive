[{"filename": "CMakeLists.txt", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -132,14 +132,13 @@ endif()\n # Python module\n if(TRITON_BUILD_PYTHON_MODULE)\n     message(STATUS \"Adding Python module\")\n+    set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+    include_directories(\".\" ${PYTHON_SRC_PATH})\n     if (PYTHON_INCLUDE_DIRS)\n-      set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n-      include_directories(\".\" ${PYTHON_SRC_PATH} ${PYTHON_INCLUDE_DIRS})\n-      link_directories(${PYTHON_LINK_DIRS})\n+      include_directories(${PYTHON_INCLUDE_DIRS})\n     else()\n       find_package(Python3 REQUIRED COMPONENTS Development)\n-      set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n-      include_directories(\".\" ${PYTHON_SRC_PATH} ${Python3_INCLUDE_DIRS})\n+      include_directories(${Python3_INCLUDE_DIRS})\n       link_directories(${Python3_LIBRARY_DIRS})\n       link_libraries(${Python3_LIBRARIES})\n       add_link_options(${Python3_LINK_OPTIONS})\n@@ -169,7 +168,9 @@ list(APPEND CMAKE_MODULE_PATH \"${LLVM_CMAKE_DIR}\")\n include(TableGen) # required by AddMLIR\n include(AddLLVM)\n include(AddMLIR)\n-# include(HandleLLVMOptions) # human-friendly error message\n+\n+# Disable warnings that show up in external code (gtest;pybind11)\n+set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror -Wno-covered-switch-default\")\n \n include_directories(${MLIR_INCLUDE_DIRS})\n include_directories(${LLVM_INCLUDE_DIRS})\n@@ -192,7 +193,6 @@ get_property(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)\n get_property(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)\n \n target_link_libraries(triton\n-  ${PYTHON_LIBRARIES}\n   TritonAnalysis\n   TritonTransforms\n   TritonGPUTransforms"}, {"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -142,7 +142,7 @@ class Allocation {\n     BufferT(BufferKind kind) : BufferT(kind, 0, 0) {}\n     BufferT(BufferKind kind, size_t size) : BufferT(kind, size, 0) {}\n     BufferT(BufferKind kind, size_t size, size_t offset)\n-        : kind(kind), size(size), offset(offset), id(nextId++) {}\n+        : kind(kind), id(nextId++), size(size), offset(offset) {}\n \n     bool intersects(const BufferT &other) const {\n       return Interval<size_t>(offset, offset + size)"}, {"filename": "include/triton/Analysis/AxisInfo.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -28,8 +28,8 @@ class AxisInfo {\n            DimVectorT knownConstancy)\n       : contiguity(knownContiguity), divisibility(knownDivisibility),\n         constancy(knownConstancy), rank(contiguity.size()) {\n-    assert(knownDivisibility.size() == rank);\n-    assert(knownConstancy.size() == rank);\n+    assert(knownDivisibility.size() == (size_t)rank);\n+    assert(knownConstancy.size() == (size_t)rank);\n   }\n \n   // Accessors"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -15,9 +15,9 @@ class Location;\n namespace triton {\n using llvm::StringRef;\n \n-class PTXInstr;\n-class PTXInstrCommon;\n-class PTXInstrExecution;\n+struct PTXInstr;\n+struct PTXInstrCommon;\n+struct PTXInstrExecution;\n \n // PTXBuilder helps to manage a PTX asm program consists of one or multiple\n // instructions.\n@@ -83,7 +83,7 @@ struct PTXBuilder {\n     Operand() = default;\n     Operand(const Operation &) = delete;\n     Operand(Value value, StringRef constraint)\n-        : value(value), constraint(constraint) {}\n+        : constraint(constraint), value(value) {}\n \n     bool isList() const { return !value && constraint.empty(); }\n \n@@ -120,15 +120,15 @@ struct PTXBuilder {\n   Operand *newListOperand(unsigned count, mlir::Value val,\n                           const std::string &constraint) {\n     auto *list = newOperand();\n-    for (int i = 0; i < count; ++i) {\n+    for (unsigned i = 0; i < count; ++i) {\n       list->listAppend(newOperand(val, constraint));\n     }\n     return list;\n   }\n \n   Operand *newListOperand(unsigned count, const std::string &constraint) {\n     auto *list = newOperand();\n-    for (int i = 0; i < count; ++i) {\n+    for (unsigned i = 0; i < count; ++i) {\n       list->listAppend(newOperand(constraint));\n     }\n     return list;\n@@ -172,8 +172,8 @@ struct PTXBuilder {\n     return argArchive.back().get();\n   }\n \n-  friend class PTXInstr;\n-  friend class PTXInstrCommon;\n+  friend struct PTXInstr;\n+  friend struct PTXInstrCommon;\n \n protected:\n   llvm::SmallVector<std::unique_ptr<Operand>, 6> argArchive;\n@@ -209,7 +209,7 @@ struct PTXInstrCommon {\n   PTXBuilder *builder{};\n   llvm::SmallVector<std::string, 4> instrParts;\n \n-  friend class PTXInstrExecution;\n+  friend struct PTXInstrExecution;\n };\n \n template <class ConcreteT> struct PTXInstrBase : public PTXInstrCommon {\n@@ -309,7 +309,7 @@ struct PTXInstrExecution {\n   PTXInstrExecution() = default;\n   explicit PTXInstrExecution(PTXInstrCommon *instr,\n                              llvm::ArrayRef<Operand *> oprs)\n-      : instr(instr), argsInOrder(oprs.begin(), oprs.end()) {}\n+      : argsInOrder(oprs.begin(), oprs.end()), instr(instr) {}\n \n   // Prefix a predicate to the instruction.\n   PTXInstrExecution &predicate(mlir::Value value, StringRef constraint = \"b\") {"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -11,16 +11,12 @@ class ModuleOp;\n template <typename T> class OperationPass;\n \n class TritonLLVMConversionTarget : public ConversionTarget {\n-  mlir::LLVMTypeConverter &typeConverter;\n-\n public:\n   explicit TritonLLVMConversionTarget(MLIRContext &ctx,\n                                       mlir::LLVMTypeConverter &typeConverter);\n };\n \n class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n-  mlir::LLVMTypeConverter &typeConverter;\n-\n public:\n   explicit TritonLLVMFunctionConversionTarget(\n       MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter);"}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -26,11 +26,11 @@ class DialectInferLayoutInterface\n   DialectInferLayoutInterface(Dialect *dialect) : Base(dialect) {}\n \n   virtual LogicalResult\n-  inferReduceOpEncoding(Attribute operandEncoding, int axis,\n+  inferReduceOpEncoding(Attribute operandEncoding, unsigned axis,\n                         Attribute &resultEncoding) const = 0;\n \n   virtual LogicalResult\n-  inferExpandDimsOpEncoding(Attribute operandEncoding, int axis,\n+  inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n                             Attribute &resultEncoding) const = 0;\n };\n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -21,7 +21,6 @@ class TritonGPUTypeConverter : public TypeConverter {\n };\n \n class TritonGPUConversionTarget : public ConversionTarget {\n-  TritonGPUTypeConverter &typeConverter;\n \n public:\n   explicit TritonGPUConversionTarget(MLIRContext &ctx,"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -82,7 +82,6 @@ SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n   auto srcShape = srcTy.getShape();\n-  auto rank = srcShape.size();\n   auto axis = op.axis();\n \n   bool fast_reduce = axis == 1; // FIXME(Qingyi): The fastest-changing dimension"}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 4, "deletions": 10, "changes": 14, "file_content_changes": "@@ -66,7 +66,7 @@ AxisInfo AxisInfo::join(const AxisInfo &lhs, const AxisInfo &rhs) {\n   DimVectorT retContiguity;\n   DimVectorT retDivisibility;\n   DimVectorT retConstancy;\n-  for (size_t d = 0; d < lhs.getRank(); ++d) {\n+  for (int d = 0; d < lhs.getRank(); ++d) {\n     retContiguity.push_back(gcd(lhs.getContiguity(d), rhs.getContiguity(d)));\n     retDivisibility.push_back(\n         gcd(lhs.getDivisibility(d), rhs.getDivisibility(d)));\n@@ -88,7 +88,7 @@ AxisInfo AxisInfoAnalysis::visitBinaryOp(\n   AxisInfo::DimVectorT newContiguity;\n   AxisInfo::DimVectorT newDivisibility;\n   AxisInfo::DimVectorT newConstancy;\n-  for (size_t d = 0; d < rank; ++d) {\n+  for (int d = 0; d < rank; ++d) {\n     newContiguity.push_back(getContiguity(lhsInfo, rhsInfo, d));\n     newDivisibility.push_back(getDivisibility(lhsInfo, rhsInfo, d));\n     newConstancy.push_back(getConstancy(lhsInfo, rhsInfo, d));\n@@ -167,7 +167,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     AxisInfo::DimVectorT contiguity;\n     AxisInfo::DimVectorT divisibility;\n     AxisInfo::DimVectorT constancy;\n-    for (size_t d = 0; d < retTy.getRank(); ++d) {\n+    for (int d = 0; d < retTy.getRank(); ++d) {\n       contiguity.push_back(1);\n       divisibility.push_back(opInfo.getDivisibility(0));\n       constancy.push_back(retTy.getShape()[d]);\n@@ -176,12 +176,6 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n   }\n   // expandDims\n   if (auto expandDims = llvm::dyn_cast<triton::ExpandDimsOp>(op)) {\n-    Type _retTy = *op->result_type_begin();\n-    Type _opTy = *op->operand_type_begin();\n-    TensorType retTy = _retTy.cast<TensorType>();\n-    TensorType opTy = _opTy.cast<TensorType>();\n-    ArrayRef<int64_t> retShape = retTy.getShape();\n-    ArrayRef<int64_t> opShape = opTy.getShape();\n     AxisInfo opInfo = operands[0]->getValue();\n     AxisInfo::DimVectorT contiguity = opInfo.getContiguity();\n     AxisInfo::DimVectorT divisibility = opInfo.getDivisibility();\n@@ -203,7 +197,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     AxisInfo::DimVectorT contiguity;\n     AxisInfo::DimVectorT divisibility;\n     AxisInfo::DimVectorT constancy;\n-    for (size_t d = 0; d < retTy.getRank(); ++d) {\n+    for (int d = 0; d < retTy.getRank(); ++d) {\n       contiguity.push_back(opShape[d] == 1 ? 1 : opInfo.getContiguity(d));\n       divisibility.push_back(opInfo.getDivisibility(d));\n       constancy.push_back(opShape[d] == 1 ? retShape[d] : 1);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -28,11 +28,10 @@ bool maybeSharedAllocationOp(Operation *op) {\n }\n \n std::string getValueOperandName(Value value, AsmState &state) {\n-  auto *op = value.getDefiningOp();\n   std::string opName;\n   llvm::raw_string_ostream ss(opName);\n   value.printAsOperand(ss, state);\n-  return std::move(opName);\n+  return opName;\n }\n \n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 42, "deletions": 80, "changes": 122, "file_content_changes": "@@ -258,7 +258,6 @@ struct FuncOpConversionBase : public ConvertOpToLLVMPattern<FuncOp> {\n /// FuncOp legalization pattern that converts MemRef arguments to pointers to\n /// MemRef descriptors (LLVM struct data types) containing all the MemRef type\n /// information.\n-static constexpr StringRef kEmitIfaceAttrName = \"llvm.emit_c_interface\";\n struct FuncOpConversion : public FuncOpConversionBase {\n   FuncOpConversion(LLVMTypeConverter &converter, int numWarps,\n                    PatternBenefit benefit)\n@@ -296,7 +295,6 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<::mlir::ReturnOp> {\n   LogicalResult\n   matchAndRewrite(ReturnOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Location loc = op->getLoc();\n     unsigned numArguments = op.getNumOperands();\n \n     // Currently, Triton kernel function always return nothing.\n@@ -486,7 +484,6 @@ class ConvertTritonGPUOpToLLVMPattern\n                                 ConversionPatternRewriter &rewriter,\n                                 const BlockedEncodingAttr &blocked_layout,\n                                 ArrayRef<int64_t> shape) const {\n-    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n     Value threadId = getThreadId(rewriter, loc);\n     Value warpSize = idx_val(32);\n     Value laneId = urem(threadId, warpSize);\n@@ -658,7 +655,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto bufferId = allocation->getBufferId(value);\n     assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n     size_t offset = allocation->getOffset(bufferId);\n-    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n     Value offVal = idx_val(offset);\n     Value base = gep(ptrTy, smem, offVal);\n     return base;\n@@ -688,7 +684,6 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n   auto tensorTy = resType.cast<RankedTensorType>();\n   if (tensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n-    auto layout = tensorTy.getEncoding();\n     auto srcType = typeConverter->convertType(elemType);\n     auto llSrc = bitcast(srcType, constVal);\n     size_t elemsPerThread = getElemsPerThread(tensorTy);\n@@ -846,7 +841,6 @@ struct LoadOpConversion\n   LogicalResult\n   matchAndRewrite(triton::LoadOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    MLIRContext *ctx = rewriter.getContext();\n     auto loc = op->getLoc();\n \n     // original values\n@@ -901,12 +895,11 @@ struct LoadOpConversion\n       // TODO: optimization when ptr is GEP with constant offset\n       size_t in_off = 0;\n \n-      const int maxWordWidth = std::max<int>(32, valueElemNbits);\n-      const int totalWidth = valueElemNbits * vec;\n-      const int width = std::min(totalWidth, maxWordWidth);\n-      const int nWords = std::max(1, totalWidth / width);\n-      const int wordNElems = width / valueElemNbits;\n-      const int vecNElems = totalWidth / valueElemNbits;\n+      const size_t maxWordWidth = std::max<size_t>(32, valueElemNbits);\n+      const size_t totalWidth = valueElemNbits * vec;\n+      const size_t width = std::min(totalWidth, maxWordWidth);\n+      const size_t nWords = std::max<size_t>(1, totalWidth / width);\n+      const size_t wordNElems = width / valueElemNbits;\n       assert(wordNElems * nWords * numVecs == numElems);\n \n       // TODO(Superjomn) Add cache policy fields to StoreOp.\n@@ -925,7 +918,7 @@ struct LoadOpConversion\n \n       // prepare asm operands\n       auto *dstsOpr = ptxBuilder.newListOperand();\n-      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n+      for (size_t wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n         auto *opr = ptxBuilder.newOperand(writeConstraint); // =r operations\n         dstsOpr->listAppend(opr);\n       }\n@@ -992,8 +985,8 @@ struct LoadOpConversion\n                        : retTys[0];\n \n       // TODO: if (has_l2_evict_policy)\n-      auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n-                                                      LLVM::AsmDialect::AD_ATT);\n+      // auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n+      //                                                 LLVM::AsmDialect::AD_ATT);\n       Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n \n       // ---\n@@ -1084,27 +1077,25 @@ struct StoreOpConversion\n       // TODO: optimization when ptr is AddPtr with constant offset\n       size_t in_off = 0;\n \n-      const int maxWordWidth = std::max<int>(32, valueElemNbits);\n-      const int totalWidth = valueElemNbits * vec;\n-      const int width = std::min(totalWidth, maxWordWidth);\n-      const int nWords = std::max(1, totalWidth / width);\n-      const int wordNElems = width / valueElemNbits;\n-      const int vecNElems = totalWidth / valueElemNbits;\n+      const size_t maxWordWidth = std::max<size_t>(32, valueElemNbits);\n+      const size_t totalWidth = valueElemNbits * vec;\n+      const size_t width = std::min(totalWidth, maxWordWidth);\n+      const size_t nWords = std::max<size_t>(1, totalWidth / width);\n+      const size_t wordNElems = width / valueElemNbits;\n       assert(wordNElems * nWords * numVecs == numElems);\n \n       // TODO(Superjomn) Add cache policy fields to StoreOp.\n       // TODO(Superjomn) Deal with cache policy here.\n-      const bool hasL2EvictPolicy = false;\n \n       Type valArgTy = IntegerType::get(ctx, width);\n       auto wordTy = vec_ty(valueElemTy, wordNElems);\n \n       SmallVector<std::pair<Value, std::string>> asmArgs;\n-      for (int wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n+      for (size_t wordIdx = 0; wordIdx < nWords; ++wordIdx) {\n         // llWord is a width-len composition\n         Value llWord = rewriter.create<LLVM::UndefOp>(loc, wordTy);\n         // Insert each value element to the composition\n-        for (int elemIdx = 0; elemIdx < wordNElems; ++elemIdx) {\n+        for (size_t elemIdx = 0; elemIdx < wordNElems; ++elemIdx) {\n           const size_t elemOffset = vecStart + wordIdx * wordNElems + elemIdx;\n           assert(elemOffset < valueElems.size());\n           Value elem = valueElems[elemOffset];\n@@ -1224,7 +1215,6 @@ struct BroadcastOpConversion\n     }\n \n     unsigned srcElems = getElemsPerThread(srcTy);\n-    auto elemTy = resultTy.getElementType();\n     auto srcVals = getElementsFromStruct(loc, src, rewriter);\n     unsigned resultElems = getElemsPerThread(resultTy);\n     SmallVector<Value> resultVals(resultElems);\n@@ -1286,8 +1276,6 @@ struct ReduceOpConversion\n LogicalResult\n ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n-  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-  auto rank = srcTy.getShape().size();\n   if (op.axis() == 1) // FIXME(Qingyi): The fastest-changing dimension\n     return matchAndRewriteFast(op, adaptor, rewriter);\n   return matchAndRewriteBasic(op, adaptor, rewriter);\n@@ -1336,7 +1324,6 @@ void ReduceOpConversion::accumulate(ConversionPatternRewriter &rewriter,\n \n Value ReduceOpConversion::shflSync(ConversionPatternRewriter &rewriter,\n                                    Location loc, Value val, int i) const {\n-  MLIRContext *ctx = rewriter.getContext();\n   unsigned bits = val.getType().getIntOrFloatBitWidth();\n \n   if (bits == 64) {\n@@ -1443,7 +1430,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n \n     barrier();\n     SmallVector<Value> resultVals(resultElems);\n-    for (int i = 0; i < resultElems; i++) {\n+    for (size_t i = 0; i < resultElems; i++) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n       Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n@@ -1475,7 +1462,6 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n   auto srcShape = srcTy.getShape();\n-  auto srcOrder = srcLayout.getOrder();\n \n   auto threadsPerWarp = srcLayout.getThreadsPerWarp();\n   auto warpsPerCTA = srcLayout.getWarpsPerCTA();\n@@ -1583,7 +1569,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n \n     barrier();\n     SmallVector<Value> resultVals(resultElems);\n-    for (int i = 0; i < resultElems; i++) {\n+    for (size_t i = 0; i < resultElems; i++) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, i32_val(0));\n       Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n@@ -1623,7 +1609,6 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n     // due to MLIR's restrictions\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n-    auto resultShape = resultTy.getShape();\n     unsigned elems = getElemsPerThread(resultTy);\n     Type elemTy =\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n@@ -1702,7 +1687,6 @@ struct AddPtrOpConversion\n       auto resultLayout =\n           resultTensorTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n       assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n-      auto resultShape = resultTensorTy.getShape();\n       unsigned elems = getElemsPerThread(resultTy);\n       Type elemTy =\n           getTypeConverter()->convertType(resultTensorTy.getElementType());\n@@ -1825,7 +1809,7 @@ class ElementwiseOpConversionBase\n     SmallVector<SmallVector<Value>> operands(elems);\n     for (auto operand : adaptor.getOperands()) {\n       auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n-      for (int i = 0; i < elems; ++i) {\n+      for (size_t i = 0; i < elems; ++i) {\n         operands[i].push_back(sub_operands[i]);\n       }\n     }\n@@ -1935,6 +1919,7 @@ struct CmpFOpConversion\n       __PRED_ENUM(ORD, ord);\n       __PRED_ENUM(UEQ, ueq);\n       __PRED_ENUM(UGT, ugt);\n+      __PRED_ENUM(UGE, uge);\n       __PRED_ENUM(ULT, ult);\n       __PRED_ENUM(ULE, ule);\n       __PRED_ENUM(UNE, une);\n@@ -2038,7 +2023,6 @@ void ConvertLayoutOpConversion::processReplica(\n   auto rank = type.getRank();\n   auto sizePerThread = getSizePerThread(layout);\n   auto accumSizePerThread = product<unsigned>(sizePerThread);\n-  auto llvmIndexTy = getTypeConverter()->getIndexType();\n   SmallVector<unsigned> numCTAs(rank);\n   auto shapePerCTA = getShapePerCTA(layout);\n   for (unsigned d = 0; d < rank; ++d) {\n@@ -2057,7 +2041,6 @@ void ConvertLayoutOpConversion::processReplica(\n     multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n         loc, rewriter, blockedLayout, type.getShape());\n   } else if (sliceLayout) {\n-    unsigned dim = sliceLayout.getDim();\n     auto parent = sliceLayout.getParent();\n     if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n       SmallVector<int64_t> paddedShape =\n@@ -2215,7 +2198,7 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   }\n   // Potentially we need to store for multiple CTAs in this replication\n   unsigned accumNumReplicates = product<unsigned>(numReplicates);\n-  unsigned elems = getElemsPerThread(srcTy);\n+  // unsigned elems = getElemsPerThread(srcTy);\n   auto vals = getElementsFromStruct(loc, adaptor.src(), rewriter);\n   unsigned inVec = 0;\n   unsigned outVec = 0;\n@@ -2382,17 +2365,17 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n // Data loader for mma.16816 instruction.\n class MMA16816SmemLoader {\n public:\n-  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, int kOrder,\n+  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n                      ArrayRef<int64_t> tileShape, ArrayRef<int> instrShape,\n                      ArrayRef<int> matShape, int perPhase, int maxPhase,\n                      int elemBytes, ConversionPatternRewriter &rewriter,\n                      TypeConverter *typeConverter, const Location &loc)\n-      : wpt(wpt), order(order.begin(), order.end()), kOrder(kOrder),\n+      : order(order.begin(), order.end()), kOrder(kOrder),\n         tileShape(tileShape.begin(), tileShape.end()),\n         instrShape(instrShape.begin(), instrShape.end()),\n         matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n-        maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter),\n-        typeConverter(typeConverter), loc(loc), ctx(rewriter.getContext()) {\n+        maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n+        ctx(rewriter.getContext()) {\n     cMatShape = matShape[order[0]];\n     sMatShape = matShape[order[1]];\n \n@@ -2591,7 +2574,6 @@ class MMA16816SmemLoader {\n     assert(mat0 % 2 == 0 && mat1 % 2 == 0 &&\n            \"smem matrix load must be aligned\");\n     int matIdx[2] = {mat0, mat1};\n-    int k = matIdx[kOrder];\n \n     int ptrIdx{-1};\n \n@@ -2611,7 +2593,6 @@ class MMA16816SmemLoader {\n \n     Value ptr = getPtr(ptrIdx);\n \n-    Value resV4;\n     if (canUseLdmatrix) {\n       int sOffset =\n           matIdx[order[1]] * sMatStride * sMatShape * sTileStride * elemBytes;\n@@ -2742,7 +2723,6 @@ class MMA16816SmemLoader {\n   }\n \n private:\n-  int wpt;\n   SmallVector<uint32_t> order;\n   int kOrder;\n   SmallVector<int64_t> tileShape;\n@@ -2752,7 +2732,6 @@ class MMA16816SmemLoader {\n   int maxPhase;\n   int elemBytes;\n   ConversionPatternRewriter &rewriter;\n-  TypeConverter *typeConverter{};\n   const Location &loc;\n   MLIRContext *ctx{};\n \n@@ -2801,14 +2780,9 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   LogicalResult\n   matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    Location loc = op->getLoc();\n     // D = A * B + C\n     Value A = op.a();\n-    Value B = op.b();\n-    Value C = op.c();\n     Value D = op.getResult();\n-    MLIRContext *ctx = op->getContext();\n-    bool allowTF32 = op.allowTF32();\n \n     // Here we assume the DotOp's operands always comes from shared memory.\n     auto AShape = A.getType().cast<RankedTensorType>().getShape();\n@@ -2966,8 +2940,6 @@ struct DotOpConversionHelper {\n     Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n     Type i8x4Pack4Ty =\n         LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n-    Type i32Pack4Ty = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(4, type::i32Ty(ctx)));\n \n     switch (mmaType) {\n     case TensorCoreType::FP32_FP16_FP16_FP32:\n@@ -3077,7 +3049,6 @@ struct DotOpConversionHelper {\n     auto bTy = B.getType().cast<RankedTensorType>();\n     // d = a*b + c\n     auto dTy = op.d().getType().cast<RankedTensorType>();\n-    auto mmaLayout = dTy.getEncoding().cast<MmaEncodingAttr>();\n \n     if (dTy.getElementType().isF32()) {\n       if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n@@ -3183,9 +3154,9 @@ struct MMA16816ConversionHelper {\n   MMA16816ConversionHelper(MmaEncodingAttr mmaLayout, Value thread,\n                            ConversionPatternRewriter &rewriter,\n                            TypeConverter *typeConverter, Location loc)\n-      : mmaLayout(mmaLayout), helper(mmaLayout), rewriter(rewriter),\n-        typeConverter(typeConverter), loc(loc), ctx(mmaLayout.getContext()),\n-        thread(thread) {\n+      : mmaLayout(mmaLayout), thread(thread), helper(mmaLayout),\n+        rewriter(rewriter), typeConverter(typeConverter), loc(loc),\n+        ctx(mmaLayout.getContext()) {\n     wpt = mmaLayout.getWarpsPerCTA();\n \n     Value _32 = i32_val(32);\n@@ -3296,8 +3267,8 @@ struct MMA16816ConversionHelper {\n     }\n \n     // step1. Perform loading.\n-    for (unsigned m = 0; m < numRepM; ++m)\n-      for (unsigned k = 0; k < numRepK; ++k)\n+    for (int m = 0; m < numRepM; ++m)\n+      for (int k = 0; k < numRepK; ++k)\n         loadFn(2 * m, 2 * k);\n \n     // step2. Format the values to LLVM::Struct to passing to mma codegen.\n@@ -3320,8 +3291,8 @@ struct MMA16816ConversionHelper {\n         0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n-    for (unsigned n = 0; n < std::max(numRepN / 2, 1); ++n) {\n-      for (unsigned k = 0; k < numRepK; ++k)\n+    for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+      for (int k = 0; k < numRepK; ++k)\n         loadFn(2 * n, 2 * k);\n     }\n \n@@ -3357,17 +3328,12 @@ struct MMA16816ConversionHelper {\n     helper.deduceMmaType(op);\n \n     auto aTensorTy = a.getType().cast<RankedTensorType>();\n-    auto bTensorTy = b.getType().cast<RankedTensorType>();\n-    auto cTensorTy = c.getType().cast<RankedTensorType>();\n     auto dTensorTy = d.getType().cast<RankedTensorType>();\n \n     auto aShape = aTensorTy.getShape();\n     auto dShape = dTensorTy.getShape();\n \n-    int NK = aShape[1];\n     // shape / shape_per_cta\n-    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n     int numRepM = getNumRepM(aTensorTy, dShape[0]);\n     int numRepN = getNumRepN(aTensorTy, dShape[1]);\n     int numRepK = getNumRepK(aTensorTy, aShape[1]);\n@@ -3410,9 +3376,9 @@ struct MMA16816ConversionHelper {\n             extract_val(type::f32Ty(ctx), mmaOut, getIntAttr(i));\n     };\n \n-    for (unsigned k = 0; k < numRepK; ++k)\n-      for (unsigned m = 0; m < numRepM; ++m)\n-        for (unsigned n = 0; n < numRepN; ++n)\n+    for (int k = 0; k < numRepK; ++k)\n+      for (int m = 0; m < numRepM; ++m)\n+        for (int n = 0; n < numRepN; ++n)\n           callMma(2 * m, n, 2 * k);\n \n     // replace with new packed result\n@@ -3427,7 +3393,7 @@ struct MMA16816ConversionHelper {\n private:\n   std::function<void(int, int)>\n   getLoadMatrixFn(Value tensor, Value llTensor, MmaEncodingAttr mmaLayout,\n-                  int wpt, int kOrder, ArrayRef<int> instrShape,\n+                  int wpt, uint32_t kOrder, ArrayRef<int> instrShape,\n                   ArrayRef<int> matShape, Value warpId,\n                   ValueTable &vals) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n@@ -3501,8 +3467,8 @@ struct MMA16816ConversionHelper {\n   Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n                                               int n1) const {\n     std::vector<Value> elems;\n-    for (unsigned m = 0; m < n0; ++m)\n-      for (unsigned k = 0; k < n1; ++k) {\n+    for (int m = 0; m < n0; ++m)\n+      for (int k = 0; k < n1; ++k) {\n         elems.push_back(vals.at({2 * m, 2 * k}));\n         elems.push_back(vals.at({2 * m, 2 * k + 1}));\n         elems.push_back(vals.at({2 * m + 1, 2 * k}));\n@@ -3544,10 +3510,8 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   auto loc = op.getLoc();\n   Value src = op.src();\n   Value dst = op.result();\n-  auto srcTensorTy = src.getType().cast<RankedTensorType>();\n   auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n \n-  auto sharedLayout = srcTensorTy.getEncoding().cast<SharedEncodingAttr>();\n   auto dotOperandLayout =\n       dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n   MmaEncodingAttr mmaLayout =\n@@ -3726,7 +3690,7 @@ struct AsyncWaitOpConversion\n     auto ctx = op.getContext();\n     auto loc = op.getLoc();\n     auto voidTy = void_ty(ctx);\n-    auto ret = ptxBuilder.launch(rewriter, loc, voidTy);\n+    ptxBuilder.launch(rewriter, loc, voidTy);\n \n     // Safe to remove the op since it doesn't have any return value.\n     rewriter.eraseOp(op);\n@@ -3815,12 +3779,10 @@ struct InsertSliceAsyncOpConversion\n     unsigned perPhase = resSharedLayout.getPerPhase();\n     unsigned maxPhase = resSharedLayout.getMaxPhase();\n     auto sizePerThread = srcBlockedLayout.getSizePerThread();\n-    auto threadsPerWarp = srcBlockedLayout.getThreadsPerWarp();\n-    auto warpsPerCTA = srcBlockedLayout.getWarpsPerCTA();\n     auto threadsPerCTA = getThreadsPerCTA(srcBlockedLayout);\n \n     auto inOrder = srcBlockedLayout.getOrder();\n-    auto outOrder = resSharedLayout.getOrder();\n+\n     // If perPhase * maxPhase > threadsPerCTA, we need to swizzle over\n     // elements across phases. If perPhase * maxPhase == threadsPerCTA,\n     // swizzle is not allowd\n@@ -3901,7 +3863,7 @@ struct InsertSliceAsyncOpConversion\n       auto resByteWidth = resElemTy.getIntOrFloatBitWidth() / 8;\n \n       auto tileOffset = tileOffsetMap[{tileVecIdxRow, tileVecIdxCol}];\n-      for (unsigned wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n+      for (size_t wordIdx = 0; wordIdx < numWords; ++wordIdx) {\n         PTXBuilder ptxBuilder;\n         auto wordElemIdx = wordIdx * numWordElems;\n         auto &copyAsyncOp =\n@@ -4223,7 +4185,7 @@ namespace mlir {\n \n TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n     MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter)\n-    : ConversionTarget(ctx), typeConverter(typeConverter) {\n+    : ConversionTarget(ctx) {\n   addLegalDialect<LLVM::LLVMDialect>();\n   addLegalDialect<NVVM::NVVMDialect>();\n   // addIllegalDialect<triton::TritonDialect>();\n@@ -4235,7 +4197,7 @@ TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n \n TritonLLVMFunctionConversionTarget::TritonLLVMFunctionConversionTarget(\n     MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter)\n-    : ConversionTarget(ctx), typeConverter(typeConverter) {\n+    : ConversionTarget(ctx) {\n   addLegalDialect<LLVM::LLVMDialect>();\n   // addLegalDialect<NVVM::NVVMDialect>();\n   addIllegalOp<mlir::FuncOp>();"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 9, "deletions": 16, "changes": 25, "file_content_changes": "@@ -21,9 +21,7 @@ template <class Op> class GenericOpPattern : public OpConversionPattern<Op> {\n   matchAndRewrite(Op op, typename Op::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    Op res =\n-        rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands());\n-\n+    rewriter.replaceOpWithNewOp<Op>(op, retType, adaptor.getOperands());\n     return success();\n   }\n };\n@@ -37,9 +35,8 @@ class ArithCmpPattern : public OpConversionPattern<SrcOp> {\n   matchAndRewrite(SrcOp op, typename SrcOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    DstOp res =\n-        rewriter.replaceOpWithNewOp<DstOp>(op, retType, adaptor.getPredicate(),\n-                                           adaptor.getLhs(), adaptor.getRhs());\n+    rewriter.replaceOpWithNewOp<DstOp>(op, retType, adaptor.getPredicate(),\n+                                       adaptor.getLhs(), adaptor.getRhs());\n     return success();\n   }\n };\n@@ -129,10 +126,9 @@ class StdSelectPattern : public OpConversionPattern<SelectOp> {\n   matchAndRewrite(SelectOp op, typename SelectOp::Adaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    triton::gpu::SelectOp res =\n-        rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n-            op, retType, adaptor.getCondition(), adaptor.getTrueValue(),\n-            adaptor.getFalseValue());\n+    rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n+        op, retType, adaptor.getCondition(), adaptor.getTrueValue(),\n+        adaptor.getFalseValue());\n     return success();\n   }\n };\n@@ -204,9 +200,6 @@ struct TritonExpandDimsPattern\n         triton::gpu::BlockedEncodingAttr::get(getContext(), retSizePerThread,\n                                               retThreadsPerWarp, retWarpsPerCTA,\n                                               retOrder);\n-    // return type\n-    RankedTensorType retType =\n-        RankedTensorType::get(retShape, argType.getElementType(), retEncoding);\n     // convert operand to slice of return type\n     Attribute newArgEncoding = triton::gpu::SliceEncodingAttr::get(\n         getContext(), op.axis(), retEncoding);\n@@ -252,7 +245,7 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n                                            bType.getElementType(), encoding);\n       b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), dstType, b);\n     }\n-    auto newDot = rewriter.replaceOpWithNewOp<triton::DotOp>(\n+    rewriter.replaceOpWithNewOp<triton::DotOp>(\n         op, retType, a, b, adaptor.c(), adaptor.allowTF32(), adaptor.transA(),\n         adaptor.transB());\n     return success();\n@@ -279,7 +272,7 @@ struct TritonStorePattern : public OpConversionPattern<triton::StoreOp> {\n   LogicalResult\n   matchAndRewrite(triton::StoreOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto newOp = rewriter.replaceOpWithNewOp<triton::StoreOp>(\n+    rewriter.replaceOpWithNewOp<triton::StoreOp>(\n         op, adaptor.ptr(), adaptor.value(), adaptor.mask());\n     return success();\n   }\n@@ -340,7 +333,7 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   LogicalResult\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    auto newOp = rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n+    rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n         op, adaptor.redOp(), adaptor.operand(), adaptor.axis());\n     return success();\n   }"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "file_content_changes": "@@ -49,7 +49,6 @@ unsigned getElemsPerThread(Type type) {\n   auto tensorType = type.cast<RankedTensorType>();\n   auto layout = tensorType.getEncoding();\n   auto shape = tensorType.getShape();\n-  size_t rank = shape.size();\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return blockedLayout.getElemsPerThread(shape);\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n@@ -109,15 +108,15 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n   SmallVector<unsigned> shape;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-    for (int d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n+    for (unsigned d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n       shape.push_back(blockedLayout.getSizePerThread()[d] *\n                       blockedLayout.getThreadsPerWarp()[d] *\n                       blockedLayout.getWarpsPerCTA()[d]);\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     unsigned dim = sliceLayout.getDim();\n     auto parent = sliceLayout.getParent();\n     if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n-      for (int d = 0, n = blockedParent.getOrder().size(); d < n; ++d) {\n+      for (unsigned d = 0, n = blockedParent.getOrder().size(); d < n; ++d) {\n         if (d == dim)\n           continue;\n         shape.push_back(blockedParent.getSizePerThread()[d] *\n@@ -258,7 +257,6 @@ SliceEncodingAttr::paddedShape(ArrayRef<int64_t> shape) const {\n unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n   auto parent = getParent();\n-  unsigned dim = getDim();\n   if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n     assert(rank == blockedParent.getSizePerThread().size() - 1 &&\n            \"unexpected rank in SliceEncodingAttr::getElemsPerThread\");\n@@ -512,11 +510,11 @@ mlir::LogicalResult ExtractSliceOp::inferReturnTypes(\n   auto encoding = srcType.getEncoding();\n   auto srcShape = srcType.getShape();\n   auto axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n-  if (axis < 0 || axis > srcShape.size())\n+  if (axis < 0 || (size_t)axis > srcShape.size())\n     return failure();\n   SmallVector<int64_t, 4> dstShape;\n-  for (int i = 0; i < srcShape.size(); i++)\n-    if (i != axis)\n+  for (size_t i = 0; i < srcShape.size(); i++)\n+    if (i != (size_t)axis)\n       dstShape.push_back(srcShape[i]);\n   auto returnType =\n       RankedTensorType::get(dstShape, srcType.getElementType(), encoding);\n@@ -578,15 +576,17 @@ struct TritonGPUInferLayoutInterface\n     : public triton::DialectInferLayoutInterface {\n   using DialectInferLayoutInterface::DialectInferLayoutInterface;\n \n-  LogicalResult inferReduceOpEncoding(Attribute operandEncoding, int axis,\n-                                      Attribute &resultEncoding) const {\n+  LogicalResult\n+  inferReduceOpEncoding(Attribute operandEncoding, unsigned axis,\n+                        Attribute &resultEncoding) const override {\n     resultEncoding = SliceEncodingAttr::get(getDialect()->getContext(), axis,\n                                             operandEncoding);\n     return success();\n   }\n \n-  LogicalResult inferExpandDimsOpEncoding(Attribute operandEncoding, int axis,\n-                                          Attribute &resultEncoding) const {\n+  LogicalResult\n+  inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n+                            Attribute &resultEncoding) const override {\n     auto sliceEncoding = operandEncoding.dyn_cast<SliceEncodingAttr>();\n     if (!sliceEncoding) {\n       llvm::report_fatal_error("}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 5, "deletions": 12, "changes": 17, "file_content_changes": "@@ -87,7 +87,6 @@ class SimplifyConversion : public mlir::RewritePattern {\n     if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n       return mlir::failure();\n     auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n-    auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n     auto dstType = convert.getType().cast<RankedTensorType>();\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accomodate fused attention\n@@ -219,10 +218,10 @@ Operation *cloneWithInferType(mlir::PatternRewriter &rewriter, Operation *op,\n   auto typeInfer = dyn_cast<InferTypeOpInterface>(newOp);\n   if (typeInfer) {\n     SmallVector<Type, 1> newType;\n-    auto sucess = typeInfer.inferReturnTypes(\n+    auto success = typeInfer.inferReturnTypes(\n         newOp->getContext(), newOp->getLoc(), newOp->getOperands(),\n         newOp->getAttrDictionary(), newOp->getRegions(), newType);\n-    if (success)\n+    if (succeeded(success))\n       newOp->getResult(0).setType(newType.front());\n   }\n   return newOp;\n@@ -364,10 +363,6 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n   rematerializeForLoop(mlir::PatternRewriter &rewriter, scf::ForOp &forOp,\n                        size_t i, RankedTensorType newType,\n                        triton::gpu::ConvertLayoutOp origConversion) const {\n-\n-    auto newEncoding = newType.cast<RankedTensorType>().getEncoding();\n-    auto ctx = forOp.getContext();\n-    auto isInLoop = [&](Operation *op) { return op->getParentOp() == forOp; };\n     // Rewrite init argument\n     Type origType = forOp.getInitArgs()[i].getType();\n     SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n@@ -418,11 +413,10 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n     return newResults;\n   }\n \n-  mlir::LogicalResult matchAndRewrite(mlir::Operation *op,\n-                                      mlir::PatternRewriter &rewriter) const {\n-\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n     auto forOp = cast<scf::ForOp>(op);\n-    auto isInLoop = [&](Operation *op) { return op->getParentOp() == forOp; };\n     auto iterArgs = forOp.getRegionIterArgs();\n     for (auto iterArg : llvm::enumerate(iterArgs)) {\n       // if (iterArg.index() != 1)\n@@ -480,7 +474,6 @@ class RematerializeForward : public mlir::RewritePattern {\n     auto forOp = dyn_cast<scf::ForOp>(cvt->getParentOp());\n     if (!forOp)\n       return mlir::failure();\n-    auto yieldOp = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n     auto isInLoop = [&](Operation *op) { return op->getParentOp() == forOp; };\n \n     SetVector<Operation *> cvtSlices;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 11, "deletions": 28, "changes": 39, "file_content_changes": "@@ -17,11 +17,6 @@ using namespace mlir;\n \n namespace {\n class LoopPipeliner {\n-  /// comments on numStages:\n-  ///   [0, numStages-1) are in the prologue\n-  ///   numStages-1 is appended after the loop body\n-  int numStages;\n-\n   /// cache forOp we are working on\n   scf::ForOp forOp;\n \n@@ -43,6 +38,11 @@ class LoopPipeliner {\n   ///\n   Value loopIterIdx;\n \n+  /// comments on numStages:\n+  ///   [0, numStages-1) are in the prologue\n+  ///   numStages-1 is appended after the loop body\n+  int numStages;\n+\n   /// value (in loop) => value at stage N\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n \n@@ -58,9 +58,6 @@ class LoopPipeliner {\n \n   Value lookupOrDefault(Value origin, int stage);\n \n-  /// return true if this op uses any of `loads`\n-  bool isDirectUserOfAsyncLoad(Operation &op);\n-\n   /// returns a empty buffer of size <numStages, ...>\n   triton::gpu::AllocTensorOp allocateEmptyBuffer(Operation *op,\n                                                  OpBuilder &builder);\n@@ -84,7 +81,7 @@ class LoopPipeliner {\n   /// create the new ForOp (add new args & insert prefetched ops)\n   scf::ForOp createNewForOp();\n \n-  friend class PipelinePass;\n+  friend struct PipelinePass;\n };\n \n // helpers\n@@ -123,19 +120,6 @@ void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n   }\n }\n \n-bool LoopPipeliner::isDirectUserOfAsyncLoad(Operation &op) {\n-  for (Value loadOp : loads) {\n-    assert(loadOp.hasOneUse() &&\n-           \"load should only have one use (ConvertLayout)\");\n-    Value loadUseResult = loadOp.getUsers().begin()->getResult(0);\n-    for (Value opOperand : op.getOperands()) {\n-      if (opOperand == loadUseResult)\n-        return true;\n-    }\n-  }\n-  return false;\n-}\n-\n triton::gpu::AllocTensorOp\n LoopPipeliner::allocateEmptyBuffer(Operation *op, OpBuilder &builder) {\n   // allocate a buffer for each pipelined tensor\n@@ -356,8 +340,8 @@ void LoopPipeliner::emitPrologue() {\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n   // async.wait & extract_slice\n-  Operation *asyncWait = builder.create<triton::gpu::AsyncWaitOp>(\n-      loads[0].getLoc(), loads.size() * (numStages - 2));\n+  builder.create<triton::gpu::AsyncWaitOp>(loads[0].getLoc(),\n+                                           loads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (Value loadOp : loads) {\n     Value extractSlice = builder.create<triton::gpu::ExtractSliceOp>(\n@@ -380,8 +364,7 @@ void LoopPipeliner::emitEpilogue() {\n   OpBuilder builder(forOp);\n   OpBuilder::InsertionGuard g(builder);\n   builder.setInsertionPointAfter(forOp);\n-  Operation *asyncWait =\n-      builder.create<triton::gpu::AsyncWaitOp>(forOp.getLoc(), 0);\n+  builder.create<triton::gpu::AsyncWaitOp>(forOp.getLoc(), 0);\n }\n \n scf::ForOp LoopPipeliner::createNewForOp() {\n@@ -575,8 +558,8 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   yieldValues.push_back(loopIterIdx);\n \n   builder.setInsertionPointToEnd(newForOp.getBody());\n-  auto test = builder.create<scf::YieldOp>(\n-      forOp.getBody()->getTerminator()->getLoc(), yieldValues);\n+  builder.create<scf::YieldOp>(forOp.getBody()->getTerminator()->getLoc(),\n+                               yieldValues);\n   return newForOp;\n }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Swizzle.cpp", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -30,7 +30,7 @@ struct SwizzlePass : public TritonGPUSwizzleBase<SwizzlePass> {\n                           (ty.getElementType().getIntOrFloatBitWidth() / 8));\n     perPhase = std::max<int>(perPhase, 1);\n     // index of the inner dimension in `order`\n-    int inner = (opIdx == 0) ? 0 : 1;\n+    size_t inner = (opIdx == 0) ? 0 : 1;\n     if (version == 1) {\n       int maxPhase = (order[inner] == 1 ? 8 : 4) / perPhase;\n       // TODO: handle rep (see\n@@ -67,7 +67,6 @@ struct SwizzlePass : public TritonGPUSwizzleBase<SwizzlePass> {\n \n   void runOnOperation() override {\n     Operation *op = getOperation();\n-    MLIRContext *context = &getContext();\n     op->walk([&](triton::DotOp dotOp) -> void {\n       OpBuilder builder(dotOp);\n       auto _retEncoding ="}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -73,7 +73,7 @@ TritonGPUTypeConverter::TritonGPUTypeConverter(MLIRContext *context,\n //\n TritonGPUConversionTarget::TritonGPUConversionTarget(\n     MLIRContext &context, TritonGPUTypeConverter &typeConverter)\n-    : ConversionTarget(context), typeConverter(typeConverter) {\n+    : ConversionTarget(context) {\n   // TODO: we should also verify ops of TritonGPUDialect\n   addLegalDialect<triton::gpu::TritonGPUDialect>();\n \n@@ -90,7 +90,7 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n   });\n \n   // We have requirements for the data layouts\n-  addDynamicallyLegalOp<triton::DotOp>([this](triton::DotOp dotOp) -> bool {\n+  addDynamicallyLegalOp<triton::DotOp>([](triton::DotOp dotOp) -> bool {\n     Attribute aEncoding =\n         dotOp.a().getType().cast<RankedTensorType>().getEncoding();\n     Attribute bEncoding ="}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -63,7 +63,7 @@ static bool find_and_replace(std::string &str, const std::string &begin,\n static std::string llir_to_ptx(llvm::Module *module, int capability, int ptx) {\n   // LLVM version in use may not officially support target hardware\n   int max_nvvm_cc = 75;\n-  int max_nvvm_ptx = 74;\n+  // int max_nvvm_ptx = 74;\n   // options\n   auto options = llvm::cl::getRegisteredOptions();\n   auto *short_ptr ="}, {"filename": "python/setup.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -1,5 +1,4 @@\n import distutils\n-import distutils.spawn\n import os\n import platform\n import re\n@@ -136,14 +135,15 @@ def build_extension(self, ext):\n         if not os.path.exists(llvm_build_dir):\n             os.makedirs(llvm_build_dir)\n         # python directories\n-        python_include_dirs = [distutils.sysconfig.get_python_inc()] + ['/usr/local/cuda/include']\n+        python_include_dir = distutils.sysconfig.get_python_inc()\n         cmake_args = [\n+            \"-DLLVM_ENABLE_WERROR=ON\",\n             \"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\" + extdir,\n             \"-DTRITON_BUILD_TUTORIALS=OFF\",\n             \"-DTRITON_BUILD_PYTHON_MODULE=ON\",\n             # '-DPYTHON_EXECUTABLE=' + sys.executable,\n             # '-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON',\n-            \"-DPYTHON_INCLUDE_DIRS=\" + \";\".join(python_include_dirs),\n+            \"-DPYTHON_INCLUDE_DIRS=\" + python_include_dir,\n             \"-DLLVM_EXTERNAL_LIT=\" + lit_dir\n         ] + thirdparty_cmake_args\n "}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 17, "deletions": 19, "changes": 36, "file_content_changes": "@@ -26,6 +26,7 @@\n #include \"llvm/IR/Module.h\"\n #include \"llvm/IR/Verifier.h\"\n #include \"llvm/IRReader/IRReader.h\"\n+#include \"llvm/Support/FileUtilities.h\"\n #include \"llvm/Support/raw_ostream.h\"\n \n #include \"llvm/Support/SourceMgr.h\"\n@@ -1301,39 +1302,36 @@ void init_triton_translation(py::module &m) {\n           py::gil_scoped_release allow_threads;\n \n           // compile ptx with ptxas\n-          char _fsrc[L_tmpnam];\n-          char _flog[L_tmpnam];\n-          std::tmpnam(_fsrc);\n-          std::tmpnam(_flog);\n-          std::string fsrc = _fsrc;\n-          std::string flog = _flog;\n-          std::string fbin = fsrc + \".o\";\n+          llvm::SmallString<64> fsrc;\n+          llvm::SmallString<64> flog;\n+          llvm::sys::fs::createTemporaryFile(\"compile-ptx-src\", \"\", fsrc);\n+          llvm::sys::fs::createTemporaryFile(\"compile-ptx-log\", \"\", flog);\n+          std::string fbin = std::string(fsrc) + \".o\";\n+          llvm::FileRemover srcRemover(fsrc);\n+          llvm::FileRemover logRemover(flog);\n+          llvm::FileRemover binRemover(fbin);\n+          const char *_fsrc = fsrc.c_str();\n+          const char *_flog = flog.c_str();\n           const char *_fbin = fbin.c_str();\n-          std::ofstream ofs(fsrc);\n+          std::ofstream ofs(_fsrc);\n           ofs << ptxCode << std::endl;\n           ofs.close();\n           std::string cmd;\n           int err;\n           cmd = ptxasPath + \" -v --gpu-name=sm_\" + std::to_string(capability) +\n-                \" \" + fsrc + \" -o \" + fsrc + \".o 2> \" + flog;\n+                \" \" + _fsrc + \" -o \" + _fsrc + \".o 2> \" + _flog;\n           err = system(cmd.c_str());\n           if (err != 0) {\n             std::ifstream _log(_flog);\n             std::string log(std::istreambuf_iterator<char>(_log), {});\n-            unlink(_fsrc);\n-            unlink(_flog);\n             throw std::runtime_error(\"Internal Triton PTX codegen error: \\n\" +\n                                      log);\n           }\n           std::ifstream _cubin(_fbin, std::ios::binary);\n           std::string cubin(std::istreambuf_iterator<char>(_cubin), {});\n           _cubin.close();\n-          unlink(_fsrc);\n-          unlink(_flog);\n-          unlink(_fbin);\n-\n           py::bytes bytes(cubin);\n-          return bytes;\n+          return std::move(bytes);\n         });\n \n   m.def(\"add_external_libs\",\n@@ -1345,8 +1343,8 @@ void init_triton_translation(py::module &m) {\n \n void init_triton(py::module &m) {\n   py::module subm = m.def_submodule(\"triton\");\n-  // init_triton_codegen(std::move(subm.def_submodule(\"code_gen\")));\n-  init_triton_runtime(std::move(subm.def_submodule(\"runtime\")));\n-  init_triton_ir(std::move(subm.def_submodule(\"ir\")));\n+  // init_triton_codegen(subm.def_submodule(\"code_gen\"));\n+  init_triton_runtime(subm.def_submodule(\"runtime\"));\n+  init_triton_ir(subm.def_submodule(\"ir\"));\n   init_triton_translation(subm);\n }"}]