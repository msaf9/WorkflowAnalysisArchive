[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -114,7 +114,7 @@ jobs:\n           cd python/test/regression\n           sudo nvidia-smi -i 0 -pm 1\n           sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n-          python3 -m pytest -vs . --reruns 10\n+          python3 -m pytest -vs . --reruns 10 --reruns-delay 1\n           sudo nvidia-smi -i 0 -rgc\n \n   Integration-Tests-Third-Party:"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -16,7 +16,7 @@ def nvsmi(attrs):\n     return ret\n \n \n-def do_bench_cudagraph(fn, n_warmup=10, n_repeat=20, grad_to_none=None):\n+def do_bench_cudagraph(fn, n_warmup=20, n_repeat=40, grad_to_none=None):\n     import torch\n     \"\"\"\n     Benchmark the runtime of the provided function.\n@@ -56,6 +56,7 @@ def do_bench_cudagraph(fn, n_warmup=10, n_repeat=20, grad_to_none=None):\n         torch.cuda.synchronize()\n         for i in range(n_warmup):\n             fn()\n+        torch.cuda.synchronize()\n         for i in range(n_repeat):\n             # we don't want `fn` to accumulate gradient values\n             # if it contains a backward pass. So we clear the"}]