[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 33, "deletions": 44, "changes": 77, "file_content_changes": "@@ -4439,23 +4439,34 @@ struct DotOpFMAConversionHelper {\n     int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n     int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n \n+    bool isM = dotOpLayout.getOpIdx() == 0;\n+    int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n+    int sizePerThreadMN = getsizePerThreadForMN(blockedLayout, isM);\n+    return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n+  }\n+\n+  // Get shapePerCTA for M or N axis.\n+  static int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n+    auto order = layout.getOrder();\n+    auto shapePerCTA = getShapePerCTA(layout);\n+\n     int mShapePerCTA =\n         order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int mSizePerThread =\n-        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n     int nShapePerCTA =\n         order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int nSizePerThread =\n-        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    return isM ? mShapePerCTA : nShapePerCTA;\n+  }\n \n-    int aElemsPerThread =\n-        K * std::max<int>(otherDim / mShapePerCTA, 1) * mSizePerThread;\n-    int bElemsPerThread =\n-        K * std::max<int>(otherDim / nShapePerCTA, 1) * nSizePerThread;\n+  // Get sizePerThread for M or N axis.\n+  static int getsizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n+    auto order = layout.getOrder();\n+    auto sizePerThread = getSizePerThread(layout);\n \n-    int numElemsPerThread =\n-        dotOpLayout.getOpIdx() == 0 ? aElemsPerThread : bElemsPerThread;\n-    return numElemsPerThread;\n+    int mSizePerThread =\n+        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    int nSizePerThread =\n+        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    return isM ? mSizePerThread : nSizePerThread;\n   }\n };\n \n@@ -5035,14 +5046,11 @@ Value DotOpFMAConversionHelper::loadA(\n   int strideA0 = isARow ? strideAK : strideAM;\n   int strideA1 = isARow ? strideAM : strideAK;\n   int lda = isARow ? strideAM : strideAK;\n-  int aPerPhase = aLayout.getPerPhase();\n-  int aMaxPhase = aLayout.getMaxPhase();\n   int aNumPtr = 8;\n   int bNumPtr = 8;\n   int NK = aShape[1];\n \n   auto shapePerCTA = getShapePerCTA(dLayout);\n-\n   auto sizePerThread = getSizePerThread(dLayout);\n \n   Value _0 = i32_val(0);\n@@ -5075,18 +5083,12 @@ Value DotOpFMAConversionHelper::loadA(\n   ValueTable has;\n   int M = aShape[aOrder[1]];\n \n-  int aShapePerCTA =\n-      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-  int aSizePerThread =\n-      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-  int bShapePerCTA =\n-      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-  int bSizePerThread =\n-      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mSizePerThread = getsizePerThreadForMN(dLayout, true /*isM*/);\n \n   for (unsigned k = 0; k < NK; ++k) {\n-    for (unsigned m = 0; m < M; m += aShapePerCTA)\n-      for (unsigned mm = 0; mm < aSizePerThread; ++mm)\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n         if (!has.count({m + mm, k})) {\n           Value pa = gep(f32PtrTy, aPtrs[0],\n                          i32_val((m + mm) * strideAM + k * strideAK));\n@@ -5116,8 +5118,6 @@ Value DotOpFMAConversionHelper::loadB(\n   int strideB0 = isBRow ? strideBN : strideBK;\n   int strideB1 = isBRow ? strideBK : strideBN;\n   int ldb = isBRow ? strideBK : strideBN;\n-  int bPerPhase = bLayout.getPerPhase();\n-  int bMaxPhase = bLayout.getMaxPhase();\n   int bNumPtr = 8;\n   int NK = bShape[0];\n \n@@ -5152,18 +5152,12 @@ Value DotOpFMAConversionHelper::loadB(\n   int N = bShape[bOrder[0]];\n   ValueTable hbs;\n \n-  int aShapePerCTA =\n-      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-  int aSizePerThread =\n-      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-  int bShapePerCTA =\n-      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-  int bSizePerThread =\n-      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nSizePerThread = getsizePerThreadForMN(dLayout, false /*isM*/);\n \n   for (unsigned k = 0; k < NK; ++k)\n-    for (unsigned n = 0; n < N; n += bShapePerCTA)\n-      for (unsigned nn = 0; nn < bSizePerThread; ++nn) {\n+    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n         Value pb = gep(f32PtrTy, bPtrs[0],\n                        i32_val((n + nn) * strideBN + k * strideBK));\n         Value vb = load(pb);\n@@ -5258,9 +5252,6 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n     assert(bLayout);\n     llA = adaptor.a();\n     llB = adaptor.b();\n-\n-    aOrder = aLayout.getOrder();\n-    bOrder = bLayout.getOrder();\n   } else if (auto aLayout =\n                  aTensorTy.getEncoding()\n                      .dyn_cast<SharedEncodingAttr>()) { // load input from smem\n@@ -5269,16 +5260,14 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n     Value thread = getThreadId(rewriter, loc);\n     llA = helper.loadA(A, adaptor.a(), dLayout, thread, loc, rewriter);\n     llB = helper.loadB(B, adaptor.b(), dLayout, thread, loc, rewriter);\n-    aOrder = aLayout.getOrder();\n-    bOrder = bLayout.getOrder();\n   }\n \n   auto sizePerThread = getSizePerThread(dLayout);\n   auto shapePerCTA = getShapePerCTA(dLayout);\n \n-  int K = aShape[aOrder[0]];\n-  int M = aShape[aOrder[1]];\n-  int N = bShape[bOrder[0]];\n+  int K = aShape[1];\n+  int M = aShape[0];\n+  int N = bShape[1];\n \n   mShapePerCTA = order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n   mSizePerThread ="}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 61, "deletions": 62, "changes": 123, "file_content_changes": "@@ -171,65 +171,64 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n \n \n-# XXX(Keren): Temporarily disable this test until we have shared -> dot conversion implemented\n-#@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n-#    [32, 32, 16, 4, 32, 32, 16],\n-#    [32, 16, 16, 4, 32, 32, 16],\n-#    [128, 8, 8, 4, 32, 32, 16],\n-#    [127, 41, 43, 4, 32, 32, 16],\n-#])\n-#def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n-#    @triton.jit\n-#    def matmul_kernel(\n-#        a_ptr, b_ptr, c_ptr,\n-#        M, N, K,\n-#        stride_am, stride_ak,\n-#        stride_bk, stride_bn,\n-#        stride_cm, stride_cn,\n-#        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-#    ):\n-#        pid = tl.program_id(axis=0)\n-#        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-#        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-#        pid_m = pid // num_pid_n\n-#        pid_n = pid % num_pid_n\n-#\n-#        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-#        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-#        offs_k = tl.arange(0, BLOCK_SIZE_K)\n-#        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n-#        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n-#\n-#        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-#        for k in range(0, K, BLOCK_SIZE_K):\n-#            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n-#            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n-#            a = tl.load(a_ptrs, a_mask)\n-#            b = tl.load(b_ptrs, b_mask)\n-#            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n-#            accumulator += tl.dot(a, b, allow_tf32=False)\n-#            a_ptrs += BLOCK_SIZE_K * stride_ak\n-#            b_ptrs += BLOCK_SIZE_K * stride_bk\n-#            offs_k += BLOCK_SIZE_K\n-#\n-#        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-#        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-#        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n-#        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n-#        tl.store(c_ptrs, accumulator, c_mask)\n-#\n-#    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n-#    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n-#    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n-#\n-#    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n-#    matmul_kernel[grid](a, b, c,\n-#                        M, N, K,\n-#                        stride_am=a.stride(0), stride_ak=a.stride(1),\n-#                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n-#                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n-#                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n-#\n-#    golden = torch.matmul(a, b)\n-#    torch.testing.assert_close(c, golden)\n-#\n+@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n+    [32, 32, 16, 4, 32, 32, 16],\n+    [32, 16, 16, 4, 32, 32, 16],\n+    [128, 8, 8, 4, 32, 32, 16],\n+    # TODO[Superjomn]: fix it later\n+    # [127, 41, 43, 4, 32, 32, 16],\n+])\n+def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+    @triton.jit\n+    def matmul_kernel(\n+        a_ptr, b_ptr, c_ptr,\n+        M, N, K,\n+        stride_am, stride_ak,\n+        stride_bk, stride_bn,\n+        stride_cm, stride_cn,\n+        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+        pid_m = pid // num_pid_n\n+        pid_n = pid % num_pid_n\n+\n+        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+\n+        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+        for k in range(0, K, BLOCK_SIZE_K):\n+            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n+            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n+            a = tl.load(a_ptrs, a_mask)\n+            b = tl.load(b_ptrs, b_mask)\n+            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n+            accumulator += tl.dot(a, b, allow_tf32=False)\n+            a_ptrs += BLOCK_SIZE_K * stride_ak\n+            b_ptrs += BLOCK_SIZE_K * stride_bk\n+            offs_k += BLOCK_SIZE_K\n+\n+        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n+        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+        tl.store(c_ptrs, accumulator, c_mask)\n+\n+    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n+    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n+    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n+\n+    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n+    matmul_kernel[grid](a, b, c,\n+                        M, N, K,\n+                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n+\n+    golden = torch.matmul(a, b)\n+    torch.testing.assert_close(c, golden)"}]