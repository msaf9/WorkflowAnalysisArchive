[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 176, "deletions": 7, "changes": 183, "file_content_changes": "@@ -360,6 +360,13 @@ class ConvertTritonGPUOpToLLVMPattern\n     return threadId;\n   }\n \n+  Value createIndexConst(ConversionPatternRewriter &rewriter, Location loc,\n+                         int64_t value) const {\n+    return rewriter.create<LLVM::ConstantOp>(\n+        loc, this->getTypeConverter()->getIndexType(),\n+        rewriter.getIntegerAttr(rewriter.getIndexType(), value));\n+  }\n+\n   // Convert an \\param index to a multi-dim coordinate given \\param shape and\n   // \\param order.\n   SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n@@ -1420,12 +1427,15 @@ struct ConvertLayoutOpConversion\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n+    if (srcLayout.isa<BlockedEncodingAttr>() &&\n+        dstLayout.isa<SharedEncodingAttr>()) {\n+      return lowerBlockedToShared(op, adaptor, rewriter);\n+    }\n     if ((!srcLayout.isa<BlockedEncodingAttr>() &&\n          !srcLayout.isa<MmaEncodingAttr>()) ||\n         (!dstLayout.isa<BlockedEncodingAttr>() &&\n          !dstLayout.isa<MmaEncodingAttr>())) {\n       // TODO: to be implemented\n-      llvm::errs() << \"Unsupported ConvertLayout found\";\n       return failure();\n     }\n     auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n@@ -1622,10 +1632,164 @@ struct ConvertLayoutOpConversion\n       }\n     }\n   }\n+\n+  LogicalResult\n+  lowerBlockedToShared(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                       ConversionPatternRewriter &rewriter) const {\n+    op.dump();\n+    auto loc = op.getLoc();\n+    Value src = op.src();\n+    Value dst = op.result();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    auto srcShape = srcTy.getShape();\n+    assert(srcShape.size() == 2 &&\n+           \"Unexpected rank of ConvertLayout(blocked->shared)\");\n+    auto srcBlockedLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto dstSharedLayout = dstTy.getEncoding().cast<SharedEncodingAttr>();\n+    auto inOrd = srcBlockedLayout.getOrder();\n+    auto outOrd = dstSharedLayout.getOrder();\n+    unsigned inVec =\n+        inOrd == outOrd ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 1;\n+    unsigned outVec = dstSharedLayout.getVec();\n+    unsigned minVec = std::min(outVec, inVec);\n+    unsigned perPhase = dstSharedLayout.getPerPhase();\n+    unsigned maxPhase = dstSharedLayout.getMaxPhase();\n+    unsigned numElems = getElemsPerThread(srcBlockedLayout, srcShape);\n+    auto inVals = getElementsFromStruct(loc, adaptor.src(), numElems, rewriter);\n+    unsigned srcAccumSizeInThreads =\n+        product<unsigned>(srcBlockedLayout.getSizePerThread());\n+    auto elemTy = srcTy.getElementType();\n+    auto wordTy = VectorType::get(minVec, elemTy);\n+\n+    // TODO: [goostavz] We should make a cache for the calculation of\n+    // emitBaseIndexForBlockedLayout in case backend compiler not being able to\n+    // optimize that\n+    SmallVector<Value> multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n+        loc, rewriter, srcBlockedLayout, srcShape);\n+    SmallVector<unsigned> srcShapePerCTA = getShapePerCTA(srcBlockedLayout);\n+    SmallVector<unsigned> reps{ceil<unsigned>(srcShape[0], srcShapePerCTA[0]),\n+                               ceil<unsigned>(srcShape[1], srcShapePerCTA[1])};\n+\n+    // Visit each input value in the order they are placed in inVals\n+    //\n+    // Please note that the order was not awaring of blockLayout.getOrder(),\n+    // thus the adjacent elems may not belong to a same word. This could be\n+    // improved if we update the elements order by emitIndicesForBlockedLayout()\n+    SmallVector<unsigned> wordsInEachRep(2);\n+    wordsInEachRep[0] = inOrd[0] == 0\n+                            ? srcBlockedLayout.getSizePerThread()[0] / minVec\n+                            : srcBlockedLayout.getSizePerThread()[0];\n+    wordsInEachRep[1] = inOrd[0] == 0\n+                            ? srcBlockedLayout.getSizePerThread()[1]\n+                            : srcBlockedLayout.getSizePerThread()[1] / minVec;\n+    Value outVecVal = createIndexConst(rewriter, loc, outVec);\n+    Value minVecVal = createIndexConst(rewriter, loc, minVec);\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, dst);\n+    auto elemPtrTy =\n+        LLVM::LLVMPointerType::get(getTypeConverter()->convertType(elemTy), 3);\n+    smemBase = rewriter.create<LLVM::BitcastOp>(loc, elemPtrTy, smemBase);\n+    unsigned numWordsEachRep = product<unsigned>(wordsInEachRep);\n+    SmallVector<Value> wordVecs(numWordsEachRep);\n+    for (unsigned i = 0; i < numElems; ++i) {\n+      if (i % srcAccumSizeInThreads == 0) {\n+        // start of a replication\n+        for (unsigned w = 0; w < numWordsEachRep; ++w) {\n+          wordVecs[w] = rewriter.create<LLVM::UndefOp>(loc, wordTy);\n+        }\n+      }\n+      unsigned linearIdxInNanoTile = i % srcAccumSizeInThreads;\n+      auto multiDimIdxInNanoTile = getMultiDimIndex<unsigned>(\n+          linearIdxInNanoTile, srcBlockedLayout.getSizePerThread());\n+      multiDimIdxInNanoTile[inOrd[0]] /= minVec;\n+      unsigned pos = multiDimIdxInNanoTile[inOrd[0]] % minVec;\n+      unsigned wordVecIdx =\n+          getLinearIndex<unsigned>(multiDimIdxInNanoTile, wordsInEachRep);\n+      wordVecs[wordVecIdx] = rewriter.create<LLVM::InsertElementOp>(\n+          loc, wordTy, wordVecs[wordVecIdx], inVals[i],\n+          createIndexConst(rewriter, loc, pos));\n+\n+      if (i % srcAccumSizeInThreads == srcAccumSizeInThreads - 1) {\n+        // end of replication, store the vectors into shared memory\n+        unsigned linearRepIdx = i / srcAccumSizeInThreads;\n+        auto multiDimRepIdx = getMultiDimIndex<unsigned>(linearRepIdx, reps);\n+        for (unsigned linearWordIdx = 0; linearWordIdx < numWordsEachRep;\n+             ++linearWordIdx) {\n+          // step 1: recover the multidim_index from the index of input_elements\n+          // row = multiDimOffsetFirstElem[0] +\n+          //       multiDimRepIdx[0] * shapePerCTA[0] + multiDimWordIdx[0] *\n+          //       (inOrd[0] == 0) ? minVec : 1\n+          // col = multiDimOffsetFirstElem[1] +\n+          //       multiDimRepIdx[1] * shapePerCTA[1] + multiDimWordIdx[1] *\n+          //       (inOrd[0] == 1) ? minVec : 1\n+          auto multiDimWordIdx =\n+              getMultiDimIndex<unsigned>(linearWordIdx, wordsInEachRep);\n+          SmallVector<Value> multiDimIdx(2);\n+          multiDimIdx[0] = rewriter.create<LLVM::AddOp>(\n+              loc, multiDimOffsetFirstElem[0],\n+              createIndexConst(rewriter, loc,\n+                               multiDimRepIdx[0] * srcShapePerCTA[0] +\n+                                   multiDimWordIdx[0] *\n+                                       (inOrd[0] == 0 ? minVec : 1)));\n+          multiDimIdx[1] = rewriter.create<LLVM::AddOp>(\n+              loc, multiDimOffsetFirstElem[1],\n+              createIndexConst(rewriter, loc,\n+                               multiDimRepIdx[1] * srcShapePerCTA[1] +\n+                                   multiDimWordIdx[1] *\n+                                       (inOrd[0] == 1 ? minVec : 1)));\n+\n+          // step 2: do swizzling\n+          Value remained = rewriter.create<LLVM::URemOp>(\n+              loc, multiDimIdx[inOrd[0]], outVecVal);\n+          multiDimIdx[inOrd[0]] = rewriter.create<LLVM::UDivOp>(\n+              loc, multiDimIdx[inOrd[0]], outVecVal);\n+          Value off_1 = rewriter.create<LLVM::MulOp>(\n+              loc, multiDimIdx[inOrd[1]],\n+              createIndexConst(rewriter, loc, srcShape[inOrd[0]]));\n+          Value phaseId = rewriter.create<LLVM::UDivOp>(\n+              loc, multiDimIdx[inOrd[1]],\n+              createIndexConst(rewriter, loc, perPhase));\n+          phaseId = rewriter.create<LLVM::URemOp>(\n+              loc, phaseId, createIndexConst(rewriter, loc, maxPhase));\n+          Value off_0 =\n+              rewriter.create<LLVM::XOrOp>(loc, multiDimIdx[inOrd[0]], phaseId);\n+          off_0 = rewriter.create<LLVM::MulOp>(loc, off_0, outVecVal);\n+          remained = rewriter.create<LLVM::UDivOp>(loc, remained, minVecVal);\n+          off_0 = rewriter.create<LLVM::AddOp>(\n+              loc, off_0,\n+              rewriter.create<LLVM::MulOp>(loc, remained, minVecVal));\n+          Value offset = rewriter.create<LLVM::AddOp>(loc, off_1, off_0);\n+\n+          // step 3: store\n+          Value smemAddr =\n+              rewriter.create<LLVM::GEPOp>(loc, elemPtrTy, smemBase, offset);\n+          smemAddr = rewriter.create<LLVM::BitcastOp>(\n+              loc, LLVM::LLVMPointerType::get(wordTy, 3), smemAddr);\n+          rewriter.create<LLVM::StoreOp>(loc, wordVecs[linearWordIdx],\n+                                         smemAddr);\n+        }\n+      }\n+    }\n+    // TODO: double confirm if the Barrier is necessary here\n+    rewriter.create<mlir::gpu::BarrierOp>(loc);\n+    rewriter.replaceOp(op, smemBase);\n+    return success();\n+  }\n };\n \n /// ====================== dot codegen begin ==========================\n \n+template <typename T>\n+void print_array(ArrayRef<T> array, const std::string &str) {\n+  std::cout << str << \": \";\n+  for (const T &e : array)\n+    std::cout << e << \",\";\n+  std::cout << std::endl;\n+}\n+template <typename T> void print_scalar(const T &e, const std::string &str) {\n+  std::cout << str << \": \" << e << std::endl;\n+}\n+\n // Data loader for mma.16816 instruction.\n class MMA16816SmemLoader {\n public:\n@@ -1841,13 +2005,13 @@ class MMA16816SmemLoader {\n \n     int ptrIdx{-1};\n \n-    if (canUseLdmatrix)\n+    if (canUseLdmatrix) {\n       ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n-    else if (elemBytes == 4 && needTrans) // tf32 & trans\n+    } else if (elemBytes == 4 && needTrans) { // tf32 & trans\n       ptrIdx = matIdx[order[0]];\n-    else if (elemBytes == 1 && needTrans)\n+    } else if (elemBytes == 1 && needTrans) {\n       ptrIdx = matIdx[order[0]] * 4;\n-    else\n+    } else\n       llvm::report_fatal_error(\"unsupported mma type found\");\n \n     // The main difference with the original triton code is we removed the\n@@ -2469,7 +2633,7 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n     bool needTrans = kOrder != order[0];\n \n     // (a, b) is the coordinate.\n-    auto load = [&, loader, ptrs, offs, needTrans](int a, int b) {\n+    auto load = [&, kOrder, loader, ptrs, offs, needTrans](int a, int b) {\n       auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n           (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n           ptrs, helper.getMatType(), helper.getShemPtrTy());\n@@ -2512,7 +2676,9 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adapter,\n   }\n \n   const unsigned mStride = numRepN * 2;\n-  SmallVector<Value> fc(numRepM * mStride + numRepN * 2);\n+  const int fcSize =\n+      ((2 * (numRepM - 1)) + 1) * mStride + 2 * (numRepN - 1) + 1 + 1;\n+  SmallVector<Value> fc(fcSize);\n   auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n     PTXBuilder builder;\n \n@@ -2741,6 +2907,9 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n          \"Inliner pass is expected before TritonGPUToLLVM\");\n   b.setInsertionPointToStart(&funcs[0].getBody().front());\n   smem = b.create<LLVM::AddressOfOp>(loc, global);\n+  auto ptrTy =\n+      LLVM::LLVMPointerType::get(typeConverter.convertType(b.getI8Type()), 3);\n+  smem = b.create<LLVM::BitcastOp>(loc, ptrTy, smem);\n }\n \n } // namespace"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -87,7 +87,6 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n-\n   return shape;\n }\n \n@@ -104,7 +103,7 @@ SmallVector<unsigned> getOrder(const Attribute &layout) {\n     assert(0 && \"Unimplemented usage of getOrder\");\n     return {};\n   }\n-}\n+};\n \n } // namespace gpu\n } // namespace triton\n@@ -215,9 +214,12 @@ unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n }\n \n unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n-  int threads = product(getWarpsPerCTA());\n-  int numElem = product(shape);\n-  return numElem / threads;\n+  size_t rank = shape.size();\n+  assert(rank == 2 && \"Unexpected rank of mma layout\");\n+  assert(getVersion() == 2 && \"mmaLayout version = 1 is not implemented yet\");\n+  unsigned elemsCol = ceil<unsigned>(shape[0], 16 * getWarpsPerCTA()[0]) * 2;\n+  unsigned elemsRow = ceil<unsigned>(shape[1], 8 * getWarpsPerCTA()[1]) * 2;\n+  return elemsCol * elemsRow;\n }\n \n unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -910,7 +910,7 @@ def ptx_get_version(cuda_version) -> int:\n \n \n def path_to_ptxas():\n-    prefixes = [os.environ.get(\"TRITON_PTXAS_PATH\", \"\"), \"\", \"/usr/local/cuda/\"]\n+    prefixes = [os.environ.get(\"TRITON_PTXAS_PATH\", \"\"), \"\", os.environ.get('CUDA_PATH', default_cuda_dir())]\n     for prefix in prefixes:\n         ptxas = os.path.join(prefix, \"bin\", \"ptxas\")\n         if os.path.exists(ptxas):\n@@ -952,6 +952,7 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     compute_capability = compute_capability[0] * 10 + compute_capability[1]\n     ptx_version = ptx_get_version(cuda_version)\n     ptx = make_ptx(llvm_ir, compute_capability, ptx_version)\n+    print(ptx)\n     shem_size = _triton.get_shared_memory_size(module)\n     kernel_name = ptx_get_kernel_name(ptx)\n     if output == \"ptx\":"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -515,3 +515,20 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     return\n   }\n }\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<16384 x i8>\n+  // CHECK-LABEL: convert_layout_blocked_shared\n+  func @convert_layout_blocked_shared(%arg0: tensor<128x32xf32, #blocked0>) {\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<8xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n+    return\n+  }\n+}\n\\ No newline at end of file"}]