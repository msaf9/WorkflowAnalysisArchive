[{"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -1,8 +1,8 @@\n import pytest\n import torch\n \n-# import triton\n-# import triton.ops\n+import triton\n+import triton.ops\n \n \n @pytest.mark.parametrize(\"M, N, dtype, mode\",\n@@ -17,20 +17,22 @@ def test_op(M, N, dtype, mode):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8 and dtype == \"bfloat16\":\n         pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n+    if capability[0] > 8:\n+        pytest.skip(\"Skip hopper\")\n     dtype = {'bfloat16': torch.bfloat16, 'float16': torch.float16, 'float32': torch.float32}[dtype]\n     # create inputs\n     x = torch.randn(M, N, dtype=dtype, device='cuda', requires_grad=True)\n     idx = 4 + torch.ones(M, dtype=torch.int64, device='cuda')\n     # forward pass\n-    # tt_y = triton.ops.cross_entropy(x, idx)\n+    tt_y = triton.ops.cross_entropy(x, idx)\n     th_y = torch.nn.CrossEntropyLoss(reduction=\"none\")(x, idx)\n     if mode == 'forward':\n-        torch.testing.assert_allclose(th_y, th_y)\n+        torch.testing.assert_allclose(th_y, tt_y)\n     # backward pass\n     elif mode == 'backward':\n-        dy = torch.randn_like(th_y)\n+        dy = torch.randn_like(tt_y)\n         # triton backward\n-        th_y.backward(dy)\n+        tt_y.backward(dy)\n         tt_dx = x.grad.clone()\n         # torch backward\n         x.grad.zero_()"}]