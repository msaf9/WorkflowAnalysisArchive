[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 22, "deletions": 18, "changes": 40, "file_content_changes": "@@ -106,25 +106,29 @@ MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n   Value kMatArr = kOrder == 1 ? s1 : s0;\n   Value nkMatArr = kOrder == 1 ? s0 : s1;\n \n-  // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n-  // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n-  //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n-  //   |0 0 1 1 2 2|\n-  //\n-  // for B(kOrder=0) is\n-  //   |0 0|  -> 0,1,2 are the warpids\n-  //   |1 1|\n-  //   |2 2|\n+  // Matrix coordinates inside a CTA,\n+  // the matrix layout is [2wpt[0], 2] for A and [2, 2wpt[1]] for B.\n+  // e.g., Setting wpt=4, the data layout for A(kOrder=1) is\n+  //   |0 0|  -> 0,1,2,3 are the warpids\n   //   |0 0|\n   //   |1 1|\n+  //   |1 1|\n   //   |2 2|\n+  //   |2 2|\n+  //   |3 3|\n+  //   |3 3|\n+  //\n+  // for B(kOrder=0) is\n+  //   |0 1 2 3 0 1 2 3| -> 0,1,2,3 are the warpids\n+  //   |0 1 2 3 0 1 2 3|\n   // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n   // address (s0,s1) annotates.\n \n   Value matOff[2];\n   matOff[kOrder ^ 1] =\n-      add(mul(warpId, i32_val(warpOffStride)),   // warp offset\n-          mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n+      add(mul(warpId, i32_val(warpOffStride)), // warp offset (kOrder=1)\n+          mul(nkMatArr,\n+              i32_val(matArrStride))); // matrix offset inside a warp (kOrder=1)\n   matOff[kOrder] = kMatArr;\n \n   // Physical offset (before swizzling)\n@@ -138,7 +142,13 @@ MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n \n   SmallVector<Value> offs(numPtrs);\n   Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-  Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+  // To prevent out-of-bound access of B when wpt * 16 > tile_size.\n+  // In such a case, we need to wrap around the offset of B.\n+  // |0 1 2 3 0 1 2 3| -> | 0(0) 1(1) 2(2) 3(3) |\n+  // |0 1 2 3 0 1 2 3|    | 0(0) 1(1) 2(2) 3(3) |\n+  //          ~~~~~~~ out-of-bound access\n+  Value sOff = urem(add(sOffInMat, mul(sMatOff, i32_val(sMatShape))),\n+                    i32_val(tileShape[order[1]]));\n   for (int i = 0; i < numPtrs; ++i) {\n     Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n     cMatOffI = xor_(cMatOffI, phase);\n@@ -631,12 +641,6 @@ Value loadB(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n   SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n                              tensorTy.getShape().end());\n \n-  // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n-  bool transB = false;\n-  if (transB) {\n-    std::swap(shape[0], shape[1]);\n-  }\n-\n   int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n   int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -1283,6 +1283,7 @@ def kernel(X, stride_xm, stride_xn,\n                                            [64, 128, 128, 4],\n                                            [32, 128, 64, 2],\n                                            [64, 64, 32, 4],\n+                                           [32, 32, 128, 16],\n                                            [128, 128, 64, 2],\n                                            [64, 128, 128, 2]]\n                           for allow_tf32 in [True]\n@@ -1436,7 +1437,7 @@ def kernel(X, stride_xm, stride_xk,\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n-    if K > 16 or N > 16 or M > 16:\n+    if (K > 16 or N > 16 or M > 16) and (M * N // (num_warps * 32) >= 4):\n         # XXX: skip small sizes because they are not vectorized\n         assert 'ld.global.v4' in ptx\n         assert 'st.global.v4' in ptx"}]