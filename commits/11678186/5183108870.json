[{"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -1010,10 +1010,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n   tt.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n+    // CHECK-SAME: @$3 atom.global.gpu.relaxed.add.f32\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n-    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n+    // CHECK-SAME: @$3 atom.global.gpu.relaxed.add.f32\n+    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32, sem = 1 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n     tt.return\n   }\n }\n@@ -1025,9 +1025,8 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   tt.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n     // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n-    // CHECK: llvm.inline_asm\n-    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n-    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (!tt.ptr<f32>, f32, i1) -> f32\n+    // CHECK-SAME: @$3 atom.global.gpu.relaxed.add.f32\n+    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32, sem = 1: i32} : (!tt.ptr<f32>, f32, i1) -> f32\n     tt.return\n   }\n }"}]