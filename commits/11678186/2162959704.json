[{"filename": "python/triton/code_gen.py", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -22,8 +22,11 @@\n import triton._C.libtriton.triton as _triton\n from .tools.disasm import extract\n \n-current_stream = lambda device: torch.cuda.current_stream(device).cuda_stream\n-\n+def current_cuda_stream(device_idx=0):\n+    # Torch's torch.cuda.current_stream() is slow. We provide this\n+    # function to give the user an opportunity to monkey-patch their\n+    # own faster current stream lookup.\n+    return torch.cuda.current_stream().cuda_stream\n \n def mangle_ty(ty):\n     if ty.is_ptr():\n@@ -947,7 +950,7 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n         cc = str(cc[0]) + '-' + str(cc[1])\n         cache_key = self.fn.cache_key + cc\n         # query current stream\n-        stream = current_stream(device)\n+        stream = current_cuda_stream(device)\n         return _triton.runtime.launch(wargs, self.fn.do_not_specialize, cache_key, self.fn.arg_names,\n                                       device, stream, self.fn.bin_cache, num_warps, num_stages, self.add_to_cache,\n                                       grid)"}]