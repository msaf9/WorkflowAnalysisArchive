[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 12, "changes": 16, "file_content_changes": "@@ -2430,7 +2430,6 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                   ConversionPatternRewriter &rewriter) const override {\n     // D = A * B + C\n     Value A = op.a();\n-    Value C = op.c();\n     Value D = op.getResult();\n \n     // Here we assume the DotOp's operands always comes from shared memory.\n@@ -2802,9 +2801,9 @@ struct MMA16816ConversionHelper {\n   MMA16816ConversionHelper(MmaEncodingAttr mmaLayout, Value thread,\n                            ConversionPatternRewriter &rewriter,\n                            TypeConverter *typeConverter, Location loc)\n-      : mmaLayout(mmaLayout), helper(mmaLayout), rewriter(rewriter),\n-        typeConverter(typeConverter), loc(loc), ctx(mmaLayout.getContext()),\n-        thread(thread) {\n+      : mmaLayout(mmaLayout), thread(thread), helper(mmaLayout),\n+        rewriter(rewriter), typeConverter(typeConverter), loc(loc),\n+        ctx(mmaLayout.getContext()) {\n     wpt = mmaLayout.getWarpsPerCTA();\n \n     Value _32 = i32_val(32);\n@@ -2976,17 +2975,12 @@ struct MMA16816ConversionHelper {\n     helper.deduceMmaType(op);\n \n     auto aTensorTy = a.getType().cast<RankedTensorType>();\n-    auto bTensorTy = b.getType().cast<RankedTensorType>();\n-    auto cTensorTy = c.getType().cast<RankedTensorType>();\n     auto dTensorTy = d.getType().cast<RankedTensorType>();\n \n     auto aShape = aTensorTy.getShape();\n     auto dShape = dTensorTy.getShape();\n \n-    int NK = aShape[1];\n     // shape / shape_per_cta\n-    auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n-    auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n     int numRepM = getNumRepM(aTensorTy, dShape[0]);\n     int numRepN = getNumRepN(aTensorTy, dShape[1]);\n     int numRepK = getNumRepK(aTensorTy, aShape[1]);\n@@ -3051,7 +3045,7 @@ struct MMA16816ConversionHelper {\n private:\n   std::function<void(int, int)>\n   getLoadMatrixFn(Value tensor, Value llTensor, MmaEncodingAttr mmaLayout,\n-                  int wpt, int kOrder, ArrayRef<int> instrShape,\n+                  int wpt, uint32_t kOrder, ArrayRef<int> instrShape,\n                   ArrayRef<int> matShape, Value warpId,\n                   ValueTable &vals) const {\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n@@ -3168,10 +3162,8 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   auto loc = op.getLoc();\n   Value src = op.src();\n   Value dst = op.result();\n-  auto srcTensorTy = src.getType().cast<RankedTensorType>();\n   auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n \n-  auto sharedLayout = srcTensorTy.getEncoding().cast<SharedEncodingAttr>();\n   auto dotOperandLayout =\n       dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n   MmaEncodingAttr mmaLayout ="}]