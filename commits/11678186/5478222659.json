[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -82,7 +82,9 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest\n+          python3 -m pytest -n 8 -m \"not serial\"\n+          # some tests currently don't run in parallel so run them serially separately.\n+          python3 -m pytest -m \"serial\"\n \n       - name: Create artifacts archive\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -29,11 +29,12 @@ def test_op(M, N, dtype, mode):\n     # backward pass\n     elif mode == 'backward':\n         dy = torch.randn_like(tt_y)\n-        # triton backward\n-        tt_y.backward(dy)\n-        tt_dx = x.grad.clone()\n         # torch backward\n-        x.grad.zero_()\n         th_y.backward(dy)\n         th_dx = x.grad.clone()\n+        # triton backward\n+        x.grad.zero_()\n+        tt_y.backward(dy)\n+        tt_dx = x.grad.clone()\n+\n         torch.testing.assert_allclose(th_dx, tt_dx)"}, {"filename": "python/test/unit/runtime/test_subproc.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -3,6 +3,7 @@\n import shutil\n from collections import namedtuple\n \n+import pytest\n import torch\n \n import triton\n@@ -68,6 +69,7 @@ def kernel_dot(Z):\n     )\n \n \n+@pytest.mark.serial\n def test_compile_in_forked_subproc() -> None:\n     reset_tmp_dir()\n     major, minor = torch.cuda.get_device_capability(0)"}]