[{"filename": "python/test/unit/runtime/test_launch.py", "status": "modified", "additions": 21, "deletions": 21, "changes": 42, "file_content_changes": "@@ -2,6 +2,7 @@\n import importlib\n import os\n import sys\n+import tempfile\n import textwrap\n import time\n import tracemalloc\n@@ -12,6 +13,8 @@\n import triton\n import triton.language as tl\n \n+LATENCY_THRESHOLD_US = 35\n+\n \n def test_memory_leak() -> None:\n \n@@ -42,7 +45,7 @@ def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n \n \n def test_kernel_launch_latency() -> None:\n-    def define_empty_kernel(file_path, num_tensor_args):\n+    def define_kernel(num_tensor_args):\n         arg_str = \",\".join([f\"arg{i}: torch.Tensor\" for i in range(num_tensor_args)])\n         arg_str += \", n_elements: int, BLOCK_SIZE: tl.constexpr\"\n         func_str = f\"\"\"\n@@ -55,10 +58,13 @@ def define_empty_kernel(file_path, num_tensor_args):\n         def empty_kernel({arg_str}):\n             pass\n         \"\"\"\n-        with open(file_path, \"w\") as f:\n-            f.write(textwrap.dedent(func_str))\n+        with tempfile.NamedTemporaryFile(mode=\"w+t\", suffix=\".py\", delete=False) as temp_file:\n+            temp_file.write(textwrap.dedent(func_str))\n+            temp_file_path = temp_file.name\n+\n+        return temp_file_path\n \n-    def import_empty_kernel(file_path):\n+    def import_kernel(file_path):\n         directory, filename = os.path.split(file_path)\n         module_name, _ = os.path.splitext(filename)\n         sys.path.insert(0, directory)\n@@ -71,29 +77,23 @@ def empty(*kernel_args: Tuple[torch.Tensor]):\n         first_arg = kernel_args[0]\n         n_elements = first_arg.numel()\n         grid = (triton.cdiv(n_elements, 1024),)\n+        device = torch.cuda.current_device()\n         # Warmup\n-        empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=torch.cuda.current_device())\n+        empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n         torch.cuda.synchronize()\n         # Measure launch overhead at steady state\n         num_runs = 1000\n-        latency = []\n+        start_time = time.time()\n         for i in range(num_runs):\n-            single_start_time = time.time()\n-            empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=torch.cuda.current_device())\n-            single_end_time = time.time()\n-            latency.append((single_end_time - single_start_time) * 1e6)\n-\n-        latency.sort()\n-        mid = len(latency) // 2\n-        median = (latency[mid] + latency[~mid]) / 2\n-        # print(f\"Median latency (usec) = {median}\")\n-        assert median < 35, \"Kernel launch time has increased!\"\n-\n-    # Define a jitted empty_kernel in /tmp and import\n-    file_path = '/tmp/empty_kernel.py'\n+            empty_kernel[grid](*kernel_args, n_elements, BLOCK_SIZE=1024, device=device)\n+        end_time = time.time()\n+        latency_us = (end_time - start_time) / num_runs * 1e6\n+\n+        assert latency_us < LATENCY_THRESHOLD_US, \"Kernel launch time has increased!\"\n+\n     num_tensor_args = 40\n-    define_empty_kernel(file_path, num_tensor_args)\n-    empty_kernel = import_empty_kernel(file_path)\n+    file_path = define_kernel(num_tensor_args)\n+    empty_kernel = import_kernel(file_path)\n \n     # Initialize random tensors for the empty_kernel\n     torch.manual_seed(0)"}]