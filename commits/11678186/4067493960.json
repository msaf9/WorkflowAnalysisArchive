[{"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -40,4 +40,4 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     triton.testing.assert_almost_equal(ref_out, tri_out)\n     triton.testing.assert_almost_equal(ref_dv, tri_dv)\n     triton.testing.assert_almost_equal(ref_dk, tri_dk)\n-    triton.testing.assert_almost_equal(ref_dq, tri_dq)\n\\ No newline at end of file\n+    triton.testing.assert_almost_equal(ref_dq, tri_dq)"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -208,7 +208,7 @@ def forward(ctx, q, k, v, sm_scale):\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n         assert Lq == Lk and Lk == Lv\n         # assert Lk in {16, 32, 64, 128}\n-        assert Lk in {64} #TODO: fix other cases\n+        assert Lk in {64}  # TODO: fix other cases\n         o = torch.empty_like(q)\n         grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)"}]