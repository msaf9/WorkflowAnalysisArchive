[{"filename": "CMakeLists.txt", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -240,9 +240,12 @@ if(TRITON_BUILD_PYTHON_MODULE)\n       ${TRITON_LIBRARIES}\n     )\n   else()\n-    target_link_libraries(triton ${LLVM_LIBRARIES} z stdc++fs\n+    target_link_libraries(triton ${LLVM_LIBRARIES} z\n       ${TRITON_LIBRARIES}\n     )\n+    # TODO: Figure out which target is sufficient to fix errors; triton is\n+    # apparently not enough\n+    link_libraries(stdc++fs)\n   endif()\n \n   target_link_options(triton PRIVATE ${LLVM_LDFLAGS})"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -82,7 +82,7 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n       return failure();\n     ret = sliceEncoding.getParent();\n   }\n-  if (auto view = dyn_cast<triton::ViewOp>(op)) {\n+  if (isa<triton::ViewOp, triton::CatOp>(op)) {\n     return failure();\n   }\n   return success();\n@@ -179,8 +179,6 @@ int simulateBackwardRematerialization(\n       if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n               triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n         continue;\n-      if (auto view = dyn_cast<triton::ViewOp>(opArgI))\n-        continue;\n \n       // We add one expensive conversion for the current operand\n       numCvts += 1;"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -310,7 +310,6 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n-  // llvm::outs() << module << \"\\n\";\n   auto llvmIR = translateLLVMToLLVMIR(llvmContext, module);\n   if (!llvmIR) {\n     llvm::errs() << \"Translate to LLVM IR failed\";"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -56,7 +56,7 @@ def nvsmi(attrs):\n     'a100': {\n         (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n         (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.64, 'float32': 0.57, 'int8': 0.34},\n+        (2048, 2048, 2048): {'float16': 0.59, 'float32': 0.57, 'int8': 0.34},\n         (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n         (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -106,10 +106,8 @@ def allclose(x, y, atol=0, rtol=1e-2):\n         return torch.sum(x ^ y) == 0\n     if x.dtype in [torch.int8, torch.int16, torch.int32, torch.int64]:\n         rtol = 0\n-    diff = abs(x - y)\n-    x_max = torch.max(x)\n-    y_max = torch.max(y)\n-    return torch.max(diff) <= atol + rtol * torch.max(x_max, y_max)\n+        atol = 0\n+    return torch.allclose(x, y, rtol=rtol, atol=atol)\n \n \n def nvsmi(attrs):"}]