[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 18, "deletions": 3, "changes": 21, "file_content_changes": "@@ -52,6 +52,7 @@ jobs:\n           echo \"BACKEND=CUDA\" >> \"${GITHUB_ENV}\"\n           echo \"ENABLE_TMA=0\" >> \"${GITHUB_ENV}\"\n           echo \"ENABLE_MMA_V3=0\" >> \"${GITHUB_ENV}\"\n+          echo \"TRITON_DISABLE_LINE_INFO=1\" >> \"${GITHUB_ENV}\"\n \n       - name: Clear cache\n         run: |\n@@ -91,17 +92,21 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '1' && env.ENABLE_MMA_V3 == '1'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=language/test_line_info.py\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n+          # run test_line_info.py separately with TRITON_DISABLE_LINE_INFO=0\n+          TRITON_DISABLE_LINE_INFO=0 python3 -m pytest language/test_line_info.py\n \n       - name: Run python tests on CUDA with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper --ignore=language/test_line_info.py\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n+          # run test_line_info.py separately with TRITON_DISABLE_LINE_INFO=0\n+          TRITON_DISABLE_LINE_INFO=0 python3 -m pytest language/test_line_info.py\n \n       - name: Create artifacts archive\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n@@ -238,6 +243,7 @@ jobs:\n         env:\n           ARTIFACT_NAME: artifacts A100\n           ARTIFACT_JOB_NAME: Integration-Tests-Nvidia\n+          MAX_NUM_ACTIONS_PAGES: 30\n           GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n         run: |\n           OWNER_REPO=\"${{ github.repository }}\"\n@@ -254,18 +260,27 @@ jobs:\n             USER_ID=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.user.id')\n             echo \"USER_ID: $USER_ID\"\n \n+            run_id_found=false\n             page=1\n             while true; do\n+              if [ \"$page\" -gt $MAX_NUM_ACTIONS_PAGES ]; then\n+                break\n+              fi\n+\n               run_id=$(gh api --method GET \"repos/$OWNER_REPO/actions/runs?page=$page&per_page=100\" | jq --arg branch_name \"$BRANCH_NAME\" --arg run_name \"Integration Tests\" --arg user_id \"$USER_ID\" '.workflow_runs[] | select(.head_branch == $branch_name and .name == $run_name and .actor.id == ($user_id | tonumber))' | jq '.id' | head -1)\n               if [ \"$run_id\" != \"\" ]; then\n                 echo \"First run ID on branch $BRANCH_NAME is: $run_id\"\n                 WORKFLOW_RUN_ID=$run_id\n+                run_id_found=true\n                 break\n               fi\n \n               ((page++))\n             done\n-\n+            if ! $run_id_found; then\n+              echo \"No run_id found for PR ${PR_NUMBER}, moving to the next PR.\"\n+              continue\n+            fi\n             echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n             ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n             echo \"ARTIFACT_URL: $ARTIFACT_URL\""}, {"filename": ".gitignore", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -24,3 +24,6 @@ venv.bak/\n # JetBrains project files\n .idea\n cmake-build-*\n+\n+# Third-party binaries\n+ptxas"}, {"filename": "bin/RegisterTritonDialects.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -7,6 +7,7 @@\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n \n+#include \"triton/Conversion/NVGPUToLLVM/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/Passes.h\"\n #include \"triton/Conversion/TritonToTritonGPU/Passes.h\"\n \n@@ -32,6 +33,7 @@ inline void registerTritonDialects(mlir::DialectRegistry &registry) {\n   mlir::test::registerTestMembarPass();\n   mlir::triton::registerConvertTritonToTritonGPUPass();\n   mlir::triton::registerConvertTritonGPUToLLVMPass();\n+  mlir::triton::registerConvertNVGPUToLLVMPass();\n \n   // TODO: register Triton & TritonGPU passes\n   registry.insert<mlir::triton::TritonDialect, mlir::cf::ControlFlowDialect,"}, {"filename": "docs/meetups/08-22-2023.md", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -0,0 +1,12 @@\n+#### Agenda:\n+\n+##### Announcements:\n+1. Triton conference registration opening soon. Conference on 20th September at the Microsoft Silicon Valley Campus.\n+\n+##### Items:\n+1. H100 updates\n+2. Triton release plan update\n+3. Linalg updates\n+4. Intel GPU Backend status update.\n+2. Intel working on the CPU backend for Triton.\n+4. Open discussion"}, {"filename": "include/triton/Conversion/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n add_subdirectory(TritonToTritonGPU)\n add_subdirectory(TritonGPUToLLVM)\n+add_subdirectory(NVGPUToLLVM)"}, {"filename": "include/triton/Conversion/NVGPUToLLVM/CMakeLists.txt", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+set(LLVM_TARGET_DEFINITIONS Passes.td)\n+mlir_tablegen(Passes.h.inc -gen-pass-decls --name NVGPUToLLVM)\n+add_public_tablegen_target(NVGPUConversionPassIncGen)"}, {"filename": "include/triton/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.h", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -0,0 +1,19 @@\n+#ifndef TRITON_CONVERSION_NVGPU_TO_LLVM_PASS_H\n+#define TRITON_CONVERSION_NVGPU_TO_LLVM_PASS_H\n+\n+#include <memory>\n+\n+namespace mlir {\n+\n+class ModuleOp;\n+template <typename T> class OperationPass;\n+\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>> createConvertNVGPUToLLVMPass();\n+\n+} // namespace triton\n+\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/NVGPUToLLVM/Passes.h", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -0,0 +1,16 @@\n+#ifndef NVGPU_CONVERSION_PASSES_H\n+#define NVGPU_CONVERSION_PASSES_H\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"triton/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.h\"\n+\n+namespace mlir {\n+namespace triton {\n+\n+#define GEN_PASS_REGISTRATION\n+#include \"triton/Conversion/NVGPUToLLVM/Passes.h.inc\"\n+\n+} // namespace triton\n+} // namespace mlir\n+\n+#endif"}, {"filename": "include/triton/Conversion/NVGPUToLLVM/Passes.td", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "file_content_changes": "@@ -0,0 +1,20 @@\n+#ifndef NVGPU_CONVERSION_PASSES\n+#define NVGPU_CONVERSION_PASSES\n+\n+include \"mlir/Pass/PassBase.td\"\n+\n+\n+def ConvertNVGPUToLLVM : Pass<\"convert-nv-gpu-to-llvm\", \"mlir::ModuleOp\"> {\n+    let summary = \"Convert NVGPU to LLVM\";\n+    let description = [{\n+\n+    }];\n+    let constructor = \"mlir::triton::createConvertNVGPUToLLVMPass()\";\n+\n+    let dependentDialects = [\"mlir::arith::ArithDialect\",\n+                             \"mlir::LLVM::LLVMDialect\",\n+                             \"mlir::NVVM::NVVMDialect\",\n+                             \"mlir::triton::nvgpu::NVGPUDialect\"];\n+}\n+\n+#endif"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -151,6 +151,12 @@ struct PTXBuilder {\n   // aggressive optimizations that may lead to incorrect results.\n   Operand *newOperand(StringRef constraint, bool init = false);\n \n+  // Create a new operand that is tied to a previous operand. In this case the\n+  // asm would be permitted to write to an input register. Instead of providing\n+  // constraint code for this operand, the constraint code of the tied operand\n+  // is used.\n+  Operand *newOperand(unsigned operandIndex);\n+\n   // Create a constant integer operand.\n   Operand *newConstantOperand(int64_t v);\n   // Create a constant operand with explicit code specified."}, {"filename": "include/triton/Dialect/NVGPU/IR/NVGPUOps.td", "status": "modified", "additions": 1, "deletions": 124, "changes": 125, "file_content_changes": "@@ -36,38 +36,22 @@ class NVGPU_Op<string mnemonic, list<Trait> traits = []> :\n     LLVM_OpBase<NVGPU_Dialect, mnemonic, traits>;\n \n def NVGPU_WGMMAFenceOp : NVGPU_Op<\"wgmma_fence\", []> {\n-  string llvmBuilder = [{\n-      createWGMMAFence(builder);\n-  }];\n   let assemblyFormat = \"attr-dict\";\n }\n \n \n def NVGPU_WGMMACommitGroupOp : NVGPU_Op<\"wgmma_commit_group\", []> {\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      createWGMMACommitGroup(builder);\n-  }];\n }\n \n-def NVGPU_WGMMAWaitOp : NVGPU_Op<\"wgmma_wait_group\", []> {\n+def NVGPU_WGMMAWaitGroupOp : NVGPU_Op<\"wgmma_wait_group\", []> {\n   let arguments = (ins I32Attr:$pendings);\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      createWGMMAWaitGroup(builder, $pendings);\n-  }];\n }\n \n def NVGPU_MBarrierInitOp : NVGPU_Op<\"mbarrier_init\", [MemoryEffects<[MemWrite]>]> {\n   let arguments = (ins I64Ptr_shared:$mbarrier, I1:$pred, I32Attr:$count);\n   let assemblyFormat = \"$mbarrier `,` $pred attr-dict `:` type($mbarrier)\";\n-  string llvmBuilder = [{\n-      auto *i32Ty = builder.getInt32Ty();\n-      auto *arriveCnt = builder.getInt32($count);\n-        createExternalCall(builder, \"__nv_mbarrier_init\", {builder.CreatePtrToInt($mbarrier, i32Ty),\n-        arriveCnt,\n-        builder.CreateIntCast($pred, i32Ty, false)});\n-  }];\n }\n \n def MBarrier_ArriveTypeAttr : I32EnumAttr<\"MBarriveType\",\n@@ -84,39 +68,21 @@ def MBarrier_ArriveTypeAttr : I32EnumAttr<\"MBarriveType\",\n def NVGPU_MBarrierArriveOp : NVGPU_Op<\"mbarrier_arrive\", []> {\n   let arguments = (ins I64Ptr_shared:$mbarrier, I1:$pred, Optional<I32>:$ctaId, MBarrier_ArriveTypeAttr:$arriveType, DefaultValuedAttr<I32Attr, \"0\">:$txCount);\n   let assemblyFormat = \"$mbarrier `,` $pred (`,` $ctaId^)? attr-dict `:` type($mbarrier)\";\n-  string llvmBuilder = [{\n-      auto *i32Ty = builder.getInt32Ty();\n-      createMBarrierArrive(builder, $arriveType, builder.CreatePtrToInt($mbarrier, i32Ty),\n-        builder.CreateIntCast($pred, i32Ty, false), $ctaId,\n-        $txCount);\n-  }];\n }\n \n def NVGPU_MBarrierWaitOp : NVGPU_Op<\"mbarrier_wait\", []> {\n   let arguments = (ins I64Ptr_shared:$mbarrier, I1:$phase);\n   let assemblyFormat = \"$mbarrier `,` $phase attr-dict `:` type(operands)\";\n-\n-  string llvmBuilder = [{\n-      auto *i32Ty = builder.getInt32Ty();\n-      createExternalCall(builder, \"__nv_mbarrier_wait\", {builder.CreatePtrToInt($mbarrier, i32Ty),\n-        builder.CreateIntCast($phase, i32Ty, false)});\n-  }];\n }\n \n def NVGPU_NamedBarrierArriveOp : NVGPU_Op<\"bar_arrive\", []> {\n   let arguments = (ins I32:$bar, I32:$numThreads);\n   let assemblyFormat = \"$bar `,` $numThreads attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_bar_arrive\", {$bar, $numThreads});\n-  }];\n }\n \n def NVGPU_NamedBarrierWaitOp : NVGPU_Op<\"bar_wait\", []> {\n   let arguments = (ins I32:$bar, I32:$numThreads);\n   let assemblyFormat = \"$bar `,` $numThreads attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_bar_wait\", {$bar, $numThreads});\n-  }];\n }\n \n def WGMMADesc_ModeAttr : I32EnumAttr<\"WGMMADescMode\",\n@@ -134,38 +100,17 @@ def NVGPU_WGMMADescCreateOp : NVGPU_Op<\"wgmma_desc_create\", []> {\n   let arguments = (ins LLVM_AnyPointer:$buffer, I32:$height, WGMMADesc_ModeAttr:$mode);\n   let results = (outs I64:$res);\n   let assemblyFormat = \"$buffer `,` $height attr-dict `:` functional-type(operands, results)\";\n-  string llvmBuilder = [{\n-    $res = createWGMMADesc(builder, builder.CreatePtrToInt($buffer, builder.getInt32Ty()), $mode, $height);\n-  }];\n }\n \n def NVGPU_TMALoadTiledOp : NVGPU_Op<\"tma_load_tiled\", [AttrSizedOperandSegments]> {\n   let arguments = (ins I8Ptr_shared:$dst, I64Ptr_shared:$mbarrier, I8Ptr_global:$tmaDesc, I64:$l2Desc,\n                        I1:$pred, Variadic<I32>:$coords, Optional<I16>:$mcastMask);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-    auto *i32Ty = builder.getInt32Ty();\n-    auto *i64Ty = builder.getInt64Ty();\n-    createTMALoadTiled(builder,\n-      builder.CreatePtrToInt($dst, i32Ty),\n-      builder.CreatePtrToInt($mbarrier, i32Ty),\n-      builder.CreatePtrToInt($tmaDesc, i64Ty),\n-      $l2Desc, $mcastMask, builder.CreateIntCast($pred, builder.getInt32Ty(), false), $coords);\n-  }];\n }\n \n def NVGPU_TMALoadIm2colOp : NVGPU_Op<\"tma_load_im2col\", []> {\n   let arguments = (ins I8Ptr_shared:$dst, I64Ptr_shared:$mbarrier, I8Ptr_global:$tmaDesc, I64:$l2Desc, LLVM_AnyStruct:$im2colOffsets, I1:$pred, Variadic<I32>:$coords, I16Attr:$mcastMask);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-    auto *i32Ty = builder.getInt32Ty();\n-    auto *i64Ty = builder.getInt64Ty();\n-    createTMALoadIm2col(builder,\n-    builder.CreatePtrToInt($dst, i32Ty),\n-    builder.CreatePtrToInt($mbarrier, i32Ty),\n-    builder.CreatePtrToInt($tmaDesc, i64Ty),\n-    $l2Desc, $mcastMask, $im2colOffsets, builder.CreateIntCast($pred, builder.getInt32Ty(), false), $coords);\n-  }];\n }\n \n def WGMMA_LayoutAttr : I32EnumAttr<\"WGMMALayout\",\n@@ -201,30 +146,18 @@ def NVGPU_WGMMAOp : NVGPU_Op<\"wgmma\", []> {\n                    WGMMA_LayoutAttr:$layoutA, WGMMA_LayoutAttr:$layoutB);\n   let results = (outs LLVM_AnyStruct:$res);\n   let assemblyFormat = \"$opA `,` $opB `,` $opC attr-dict `:` functional-type(operands, $res)\";\n-  string llvmBuilder = [{\n-    $res = createWGMMA(builder, $m, $n, $k, $eltTypeC, $eltTypeA, $eltTypeB, $layoutA, $layoutB, $opA, $opB, $opC);\n-  }];\n }\n \n def NVGPU_CGABarrierSyncOp : NVGPU_Op<\"cga_barrier_sync\", []> {\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_cga_barrier_sync\");\n-  }];\n }\n \n def NVGPU_CGABarrierArriveOp : NVGPU_Op<\"cga_barrier_arrive\", []> {\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_cga_barrier_arrive\");\n-  }];\n }\n \n def NVGPU_CGABarrierWaitOp : NVGPU_Op<\"cga_barrier_wait\", []> {\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_cga_barrier_wait\");\n-  }];\n }\n \n def NVGPU_LoadDSmemOp : NVGPU_Op<\"load_dsmem\", [MemoryEffects<[MemRead]>]> {\n@@ -236,9 +169,6 @@ def NVGPU_LoadDSmemOp : NVGPU_Op<\"load_dsmem\", [MemoryEffects<[MemRead]>]> {\n   ];\n   let results = (outs LLVM_LoadableType:$result);\n   let assemblyFormat = \"operands attr-dict `:` functional-type(operands, results)\";\n-  string llvmBuilder = [{\n-      $result = createLoadSharedCluster(builder, $addr, $ctaId, $bitwidth, $vec);\n-  }];\n }\n \n def NVGPU_StoreDSmemOp : NVGPU_Op<\"store_dsmem\", [MemoryEffects<[MemWrite]>]> {\n@@ -248,9 +178,6 @@ def NVGPU_StoreDSmemOp : NVGPU_Op<\"store_dsmem\", [MemoryEffects<[MemWrite]>]> {\n       OpBuilder<(ins \"Value\":$addr, \"Value\":$ctaId, \"Value\":$value, \"Value\":$pred)>,\n   ];\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-      createStoreSharedCluster(builder, $addr, $ctaId, $values, $pred, op.getBitwidth(), op.getVec());\n-  }];\n   let extraClassDeclaration = [{\n       unsigned getBitwidth();\n       unsigned getVec();\n@@ -259,113 +186,63 @@ def NVGPU_StoreDSmemOp : NVGPU_Op<\"store_dsmem\", [MemoryEffects<[MemWrite]>]> {\n \n def NVGPU_FenceAsyncSharedOp : NVGPU_Op<\"fence_async_shared\", []> {\n   let arguments = (ins BoolAttr:$bCluster);\n-  string llvmBuilder = [{\n-    if ($bCluster)\n-      createExternalCall(builder, \"__nv_fence_async_shared_cluster\", {});\n-    else\n-      createExternalCall(builder, \"__nv_fence_async_shared_cta\", {});\n-  }];\n   let assemblyFormat = \"attr-dict\";\n }\n \n def NVGPU_FenceMBarrierInitOp : NVGPU_Op<\"fence_mbarrier_init\", []> {\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_fence_mbarrier_init\", {});\n-  }];\n   let assemblyFormat = \"attr-dict\";\n }\n \n def NVGPU_ClusterArriveOp : NVGPU_Op<\"cluster_arrive\", []> {\n   let arguments = (ins I1Attr:$relaxed);\n \n-  string llvmBuilder = [{\n-    if ($relaxed)\n-      createExternalCall(builder, \"__nv_cluster_arrive_relaxed\", {});\n-    else\n-      createExternalCall(builder, \"__nv_cluster_arrive\", {});\n-  }];\n   let assemblyFormat = \"attr-dict\";\n }\n \n def NVGPU_ClusterWaitOp : NVGPU_Op<\"cluster_wait\", []> {\n-  string llvmBuilder = [{\n-      createExternalCall(builder, \"__nv_cluster_wait\", {});\n-  }];\n   let assemblyFormat = \"attr-dict\";\n }\n \n def NVGPU_TMAStoreTiledOp : NVGPU_Op<\"tma_store_tiled\", [MemoryEffects<[MemWrite]>]> {\n   let arguments = (ins I8Ptr_global:$tmaDesc, I8Ptr_shared:$src, I1:$pred, Variadic<I32>:$coords);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-    auto *i32Ty = builder.getInt32Ty();\n-    auto *i64Ty = builder.getInt64Ty();\n-    createTMAStoreTiled(builder,\n-      builder.CreatePtrToInt($tmaDesc, i64Ty),\n-      builder.CreatePtrToInt($src, i32Ty),\n-      builder.CreateIntCast($pred, i32Ty, false), $coords);\n-  }];\n }\n \n def NVGPU_StoreMatrixOp : NVGPU_Op<\"stmatrix\", [MemoryEffects<[MemWrite]>]> {\n   let arguments = (ins I8Ptr_shared:$addr, Variadic<I32>:$datas);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-    auto *i32Ty = builder.getInt32Ty();\n-    createStoreMatrix(builder,\n-      builder.CreatePtrToInt($addr, i32Ty),\n-      $datas);\n-  }];\n }\n \n def NVGPU_OffsetOfStmatrixV4Op : NVGPU_Op<\"offset_of_stmatrix_v4\", []> {\n   let arguments = (ins I32:$threadId, I32:$rowOfWarp, I32:$elemIdx, I32Attr:$leadingDimOffset, I32Attr:$rowStride, I1Attr:$swizzleEnabled);\n   let results = (outs I32:$offset);\n   let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($offset)\";\n-  string llvmBuilder = [{\n-    $offset = createOffsetOfStmatrixV4(builder, $threadId, $rowOfWarp, $elemIdx, $leadingDimOffset, $rowStride, $swizzleEnabled);\n-  }];\n }\n \n def NVGPU_OffsetOfSts64Op : NVGPU_Op<\"offset_of_sts64\", []> {\n   let arguments = (ins I32:$threadId, I32:$rowOfWarp, I32:$elemIdx, I32Attr:$leadingDimOffset, I32Attr:$rowStride, I1Attr:$swizzleEnabled);\n   let results = (outs I32:$offset);\n   let assemblyFormat = \"operands attr-dict `:` type(operands) `->` type($offset)\";\n-  string llvmBuilder = [{\n-    $offset = createOffsetOfSts64(builder, $threadId, $rowOfWarp, $elemIdx, $leadingDimOffset, $rowStride, $swizzleEnabled);\n-  }];\n }\n \n def NVGPU_Sts64Op : NVGPU_Op<\"sts64\", [MemoryEffects<[MemWrite]>]> {\n   let arguments = (ins I32:$offset, AnyTypeOf<[F32, I32]>:$d0, AnyTypeOf<[F32, I32]>:$d1);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-    createSts64(builder, $offset, $d0, $d1);\n-  }];\n }\n \n def NVGPU_ClusterCTAIdOp : NVGPU_Op<\"cluster_id\", [Pure]> {\n   let results = (outs I32:$result);\n   let assemblyFormat = \"attr-dict\";\n-  string llvmBuilder = [{\n-      $result = createClusterId(builder);\n-      }];\n }\n \n def NVGPU_RegAllocOp : NVGPU_Op<\"reg_alloc\", []> {\n   let arguments = (ins I32Attr: $regCount);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-      createRegAlloc(builder, $regCount);\n-  }];\n }\n \n def NVGPU_RegDeallocOp : NVGPU_Op<\"reg_dealloc\", []> {\n   let arguments = (ins I32Attr: $regCount);\n   let assemblyFormat = \"operands attr-dict `:` type(operands)\";\n-  string llvmBuilder = [{\n-      createRegDealloc(builder, $regCount);\n-  }];\n }\n \n #endif"}, {"filename": "include/triton/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.h", "status": "removed", "additions": 0, "deletions": 41, "changes": 41, "file_content_changes": "@@ -1,41 +0,0 @@\n-/*\n- * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n- *\n- * Permission is hereby granted, free of charge, to any person obtaining\n- * a copy of this software and associated documentation files\n- * (the \"Software\"), to deal in the Software without restriction,\n- * including without limitation the rights to use, copy, modify, merge,\n- * publish, distribute, sublicense, and/or sell copies of the Software,\n- * and to permit persons to whom the Software is furnished to do so,\n- * subject to the following conditions:\n- *\n- * The above copyright notice and this permission notice shall be\n- * included in all copies or substantial portions of the Software.\n- *\n- * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n- * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n- * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n- * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n- */\n-\n-#ifndef TRITON_DIALECT_NVGPU_NVGPUTOLLVMIRTRANSLATION_H\n-#define TRITON_DIALECT_NVGPU_NVGPUTOLLVMIRTRANSLATION_H\n-\n-namespace mlir {\n-\n-class DialectRegistry;\n-class MLIRContext;\n-\n-/// Register the nvgpu dialect and the translation from it to the LLVM IR in the\n-/// given registry;\n-void registerNVGPUDialectTranslation(DialectRegistry &registry);\n-\n-/// Register the nvgpu dialect and the translation from it in the registry\n-/// associated with the given context.\n-void registerNVGPUDialectTranslation(MLIRContext &context);\n-} // namespace mlir\n-\n-#endif // TRITON_DIALECT_NVGPU_NVGPUTOLLVMIRTRANSLATION_H"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -14,7 +14,7 @@ class TritonTypeDef<string name, string _mnemonic>\n }\n \n // Floating-point Type\n-def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E4M3B11FNUZ, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n+def TT_Float : AnyTypeOf<[F8E4M3FNUZ, F8E4M3FN, F8E4M3B11FNUZ, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -146,6 +146,13 @@ Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n                 ArrayRef<unsigned> shape);\n \n+// Returns null if the op is not inside a agent region (warp specialization\n+// mode). Note that there should be at most one agent id attached to the\n+// operation.\n+std::optional<int> getWSAgentId(Operation *op);\n+std::optional<int> getWSRoleId(Operation *op);\n+void setRoleId(Operation *op, int roleId);\n+\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/IR/TritonNvidiaGPUOps.td", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -372,15 +372,15 @@ def TTNG_RegAllocOp : TTNG_Op<\"reg_alloc\", []> {\n \n   let arguments = (ins I32Attr: $regCount);\n \n-  let assemblyFormat = \"$regCount attr-dict `:` type(operands)\";\n+  let assemblyFormat = \"$regCount attr-dict\";\n }\n \n def TTNG_RegDeallocOp : TTNG_Op<\"reg_dealloc\", []> {\n   let summary = \"register deallocation\";\n \n   let arguments = (ins I32Attr: $regCount);\n \n-  let assemblyFormat = \"$regCount attr-dict `:` type(operands)\";\n+  let assemblyFormat = \"$regCount attr-dict\";\n }\n \n #endif"}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -73,6 +73,8 @@ createTritonNvidiaGPUFenceInsertionPass(int computeCapability = 90);\n std::unique_ptr<Pass>\n createTritonGPURewriteTensorPointerPass(int computeCapability = 80);\n \n+std::unique_ptr<Pass> createTritonNvidiaGPUWSFixupMissingAttrs();\n+\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\""}, {"filename": "include/triton/Dialect/TritonNvidiaGPU/Transforms/Passes.td", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -225,4 +225,22 @@ def TritonGPURewriteTensorPointer : Pass</*cli-arg*/\"tritongpu-rewrite-tensor-po\n   ];\n }\n \n+def TritonGPUWSFixupMissingAttrs : Pass<\"triton-nvidia-gpu-ws-fixup-missing-attrs\", \"mlir::ModuleOp\"> {\n+  let summary = \"Fixup missing WS related attributes\";\n+\n+  let description = [{\n+    WS related attributes are attached to some key operations and are used when lowering to llvm.\n+    However these attributes maybe be dropped in the following IR transform. This pass tries to\n+    fixup the missing attributes.\n+  }];\n+\n+  let constructor = \"mlir::createTritonNvidiaGPUWSFixupMissingAttrs()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::triton::nvidia_gpu::TritonNvidiaGPUDialect\",\n+                           \"mlir::scf::SCFDialect\",\n+                           \"mlir::arith::ArithDialect\"];\n+}\n+\n+\n #endif"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 22, "deletions": 8, "changes": 30, "file_content_changes": "@@ -5,6 +5,7 @@\n #include \"../lib/Conversion/TritonGPUToLLVM/Utility.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n #include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n@@ -117,14 +118,27 @@ void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n     return;\n   }\n \n-  if (isa<triton::gpu::AsyncWaitOp>(op) &&\n-      !isa<gpu::BarrierOp>(op->getNextNode())) {\n+  if (isa<triton::gpu::AsyncWaitOp, triton::gpu::AsyncBulkWaitOp>(op) &&\n+      !isa<gpu::BarrierOp>(op->getNextNode()) &&\n+      !(isa<LLVM::InlineAsmOp>(op->getNextNode()) &&\n+        (dyn_cast<LLVM::InlineAsmOp>(op->getNextNode())\n+             .getAsmString()\n+             .find(\"bar.sync\") != std::string::npos))) {\n     // If the current op is an async wait and the next op is not a barrier we\n     // insert a barrier op and sync\n     blockInfo->sync();\n     OpBuilder::InsertionGuard g(*builder);\n     builder->setInsertionPointAfter(op);\n-    builder->create<gpu::BarrierOp>(op->getLoc());\n+    if (auto optionalAgentId = getWSAgentId(op)) {\n+      int agentId = *optionalAgentId, roleId = 0;\n+      if (auto optionalRoleId = getWSRoleId(op))\n+        roleId = *optionalRoleId;\n+      int barId = agentId + roleId + nameBarrierIdBegin;\n+      assert(barId < nameBarrierIdEnd);\n+      barSync(*builder, op, barId, 128);\n+    } else {\n+      builder->create<gpu::BarrierOp>(op->getLoc());\n+    }\n     blockInfo->sync();\n     return;\n   }\n@@ -180,17 +194,17 @@ void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n     // TODO(Keren): Don't expose LLVM Dialect ops here\n     // TODO[shuhaoj]: Change hard code style of numThreads. Hide async_agent\n     // attr. Better way to determine barId (number of agents are limited).\n-    if (op->hasAttr(\"async_agent\")) {\n-      int agentId = getAgentIds(op).front(), roleId = 0;\n-      if (op->hasAttr(\"agent.mutex_role\"))\n-        roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+    if (auto optionalAgentId = getWSAgentId(op)) {\n+      int agentId = *optionalAgentId, roleId = 0;\n+      if (auto optionalRoleId = getWSRoleId(op))\n+        roleId = *optionalRoleId;\n       int barId = agentId + roleId + nameBarrierIdBegin;\n       assert(barId < nameBarrierIdEnd);\n       barSync(*builder, op, barId, 128);\n     } else {\n       builder->create<gpu::BarrierOp>(op->getLoc());\n-      blockInfo->sync();\n     }\n+    blockInfo->sync();\n   }\n   // Update the region info, even if barrier is inserted, we have to maintain\n   // the current op's read/write buffers."}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -174,7 +174,7 @@ unsigned ReduceOpHelper::getScratchSizeInBytes() {\n \n   unsigned bytesPerElem = 0;\n   for (const auto &ty : srcElementTypes) {\n-    bytesPerElem += ty.getIntOrFloatBitWidth() / 8;\n+    bytesPerElem += ceil<unsigned>(ty.getIntOrFloatBitWidth(), 8);\n   }\n   return bytesPerElem * elems;\n }"}, {"filename": "lib/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -3,4 +3,3 @@ add_subdirectory(Analysis)\n add_subdirectory(Conversion)\n add_subdirectory(Dialect)\n add_subdirectory(Target)\n-add_subdirectory(Hopper)"}, {"filename": "lib/Conversion/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n add_subdirectory(TritonToTritonGPU)\n add_subdirectory(TritonGPUToLLVM)\n+add_subdirectory(NVGPUToLLVM)"}, {"filename": "lib/Conversion/NVGPUToLLVM/CMakeLists.txt", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -0,0 +1,27 @@\n+add_mlir_conversion_library(NVGPUToLLVM\n+    NVGPUToLLVMPass.cpp\n+\n+    ADDITIONAL_HEADER_DIRS\n+    ${PROJECT_SOURCE_DIR}/include/triton/Conversion/NVGPUToLLVM\n+    ${PROJECT_BINARY_DIR}/include/triton/Conversion/NVGPUToLLVM\n+\n+    DEPENDS\n+    NVGPUConversionPassIncGen\n+\n+    LINK_COMPONENTS\n+    Core\n+\n+    LINK_LIBS PUBLIC\n+    MLIRIR\n+    MLIRPass\n+    MLIRGPUOps\n+    MLIRGPUToNVVMTransforms\n+    MLIRGPUToROCDLTransforms\n+    MLIRGPUTransforms\n+    TritonAnalysis\n+    TritonIR\n+    TritonGPUIR\n+    TritonGPUTransforms\n+    TritonNvidiaGPUTransforms\n+    NVGPUIR\n+)"}, {"filename": "lib/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.cpp", "status": "added", "additions": 1202, "deletions": 0, "changes": 1202, "file_content_changes": "@@ -0,0 +1,1202 @@\n+#include \"triton/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.h\"\n+\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Pass/Pass.h\"\n+#include \"mlir/Support/LogicalResult.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+\n+#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n+\n+#include \"../lib/Conversion/TritonGPUToLLVM/Utility.h\"\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Conversion/NVGPUToLLVM/Passes.h.inc\"\n+\n+namespace ttn = mlir::triton::nvgpu;\n+using ::mlir::LLVM::getSRegValue;\n+\n+namespace {\n+\n+template <typename SourceOp, typename ConcreteT>\n+class NVGPUOpPatternBase : public mlir::RewritePattern {\n+public:\n+  explicit NVGPUOpPatternBase(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(SourceOp::getOperationName(), 1, context) {}\n+\n+  LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto loc = op->getLoc();\n+    auto sourceOp = llvm::dyn_cast<SourceOp>(op);\n+    if (!sourceOp)\n+      return mlir::failure();\n+    auto ptxAsm = static_cast<const ConcreteT *>(this)->getPtxAsm(sourceOp);\n+    auto hasSideEffects = !isMemoryEffectFree(sourceOp);\n+    PTXBuilder ptxBuilder;\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+    ptxInstr({}, /*onlyAttachMLIRArgs=*/true);\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy,\n+                      /*hasSideEffects*/ hasSideEffects);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class CGABarrierSyncOpPattern\n+    : public NVGPUOpPatternBase<ttn::CGABarrierSyncOp,\n+                                CGABarrierSyncOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::CGABarrierSyncOp, CGABarrierSyncOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::CGABarrierSyncOp op) const {\n+    return \"barrier.cluster.sync.aligned;\";\n+  }\n+};\n+\n+class FenceAsyncSharedOpPattern\n+    : public NVGPUOpPatternBase<ttn::FenceAsyncSharedOp,\n+                                FenceAsyncSharedOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::FenceAsyncSharedOp, FenceAsyncSharedOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::FenceAsyncSharedOp op) const {\n+    auto bCluster = op.getBCluster();\n+    if (bCluster)\n+      return \"fence.proxy.async.shared::cluster;\";\n+    else\n+      return \"fence.proxy.async.shared::cta;\";\n+  }\n+};\n+\n+class WGMMAFenceOpPattern\n+    : public NVGPUOpPatternBase<ttn::WGMMAFenceOp, WGMMAFenceOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::WGMMAFenceOp, WGMMAFenceOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::WGMMAFenceOp op) const {\n+    return \"wgmma.fence.sync.aligned;\";\n+  }\n+};\n+\n+class WGMMACommitGroupOpPattern\n+    : public NVGPUOpPatternBase<ttn::WGMMACommitGroupOp,\n+                                WGMMACommitGroupOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::WGMMACommitGroupOp, WGMMACommitGroupOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::WGMMACommitGroupOp op) const {\n+    return \"wgmma.commit_group.sync.aligned;\";\n+  }\n+};\n+\n+class WGMMAWaitGroupOpPattern\n+    : public NVGPUOpPatternBase<ttn::WGMMAWaitGroupOp,\n+                                WGMMAWaitGroupOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::WGMMAWaitGroupOp, WGMMAWaitGroupOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::WGMMAWaitGroupOp op) const {\n+    auto pendings = op.getPendings();\n+    return \"wgmma.wait_group.sync.aligned \" + std::to_string(pendings) + \";\";\n+  }\n+};\n+\n+class StoreMatrixOpPattern : public mlir::RewritePattern {\n+public:\n+  StoreMatrixOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::StoreMatrixOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto storeMatrixOp = llvm::dyn_cast<ttn::StoreMatrixOp>(op);\n+    if (!storeMatrixOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto addr = storeMatrixOp.getAddr();\n+    auto datas = storeMatrixOp.getDatas();\n+\n+    assert(datas.size() == 1 || datas.size() == 2 ||\n+           datas.size() == 4 && \"Invalid size for StoreMatrixOp\");\n+    PTXBuilder ptxBuilder;\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\n+        \"stmatrix.sync.aligned.m8n8.x\" + std::to_string(datas.size()) +\n+        \".shared.b16\");\n+    auto *addrOpr = ptxBuilder.newAddrOperand(ptrtoint(i32_ty, addr), \"r\");\n+\n+    SmallVector<std::pair<Value, std::string>> args;\n+    for (unsigned i = 0; i < datas.size(); ++i) {\n+      args.push_back({datas[i], \"r\"});\n+    }\n+    auto *operands = ptxBuilder.newListOperand(args);\n+\n+    ptxInstr(addrOpr, operands);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class MBarrierInitOpPattern : public mlir::RewritePattern {\n+public:\n+  MBarrierInitOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::MBarrierInitOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto mBarrierInitOp = llvm::dyn_cast<ttn::MBarrierInitOp>(op);\n+    if (!mBarrierInitOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    Value mbarrier = mBarrierInitOp.getMbarrier();\n+    Value pred = mBarrierInitOp.getPred();\n+    uint32_t count = mBarrierInitOp.getCount();\n+    PTXBuilder ptxBuilder;\n+\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\"mbarrier.init.shared.b64\");\n+    auto *barOpr =\n+        ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+    auto *expectedOpr = ptxBuilder.newConstantOperand(count);\n+\n+    ptxInstr(barOpr, expectedOpr).predicate(pred, \"b\");\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class MBarrierArriveOpPattern : public mlir::RewritePattern {\n+public:\n+  MBarrierArriveOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::MBarrierArriveOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto mbarrierArriveOp = llvm::dyn_cast<ttn::MBarrierArriveOp>(op);\n+    if (!mbarrierArriveOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    Value mbarrier = mbarrierArriveOp.getMbarrier();\n+    Value pred = mbarrierArriveOp.getPred();\n+    Value ctaId = mbarrierArriveOp.getCtaId();\n+    auto arriveType = mbarrierArriveOp.getArriveType();\n+    uint32_t txCount = mbarrierArriveOp.getTxCount();\n+\n+    PTXBuilder ptxBuilder;\n+    if (arriveType == ttn::MBarriveType::normal) {\n+      auto &ptxInstr =\n+          *ptxBuilder.create<PTXInstr>(\"mbarrier.arrive.shared.b64 _,\");\n+      auto *barOpr =\n+          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+\n+      ptxInstr(barOpr).predicate(pred, \"b\");\n+    } else if (arriveType == ttn::MBarriveType::cp_async) {\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\n+          \"cp.async.mbarrier.arrive.noinc.shared.b64\");\n+      auto *barOpr =\n+          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+\n+      ptxInstr(barOpr).predicate(pred, \"b\");\n+    } else if (arriveType == ttn::MBarriveType::expect_tx) {\n+      assert(txCount > 0 && \"txCount should be valid\");\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\n+          \"mbarrier.arrive.expect_tx.shared.b64 _,\");\n+      auto *barOpr =\n+          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+      auto *expectedOpr = ptxBuilder.newConstantOperand(txCount);\n+\n+      ptxInstr(barOpr, expectedOpr).predicate(pred, \"b\");\n+    } else if (arriveType == ttn::MBarriveType::remote) {\n+      assert(ctaId && \"ctaId should have a valid value\");\n+      auto ptxAsm =\n+          \" { .reg .b32 remAddr32;                                       \\n\"\n+          \"  @$2 mapa.shared::cluster.u32  remAddr32, $0, $1;            \\n\"\n+          \"  @$2 mbarrier.arrive.shared::cluster.b64  _, [remAddr32]; }  \\n\";\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+      auto *barOpr =\n+          ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+      auto *ctaIdOpr = ptxBuilder.newOperand(ctaId, \"r\");\n+      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+\n+      ptxInstr({barOpr, ctaIdOpr, predOpr}, /*onlyAttachMLIRArgs=*/true);\n+    } else {\n+      assert(false &&\n+             \"Unsupported mbarrier arrive type\"); // TODO: is this the right way\n+                                                  // to assert in LLVM pass ?\n+    }\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+class MBarrierWaitOpPattern : public mlir::RewritePattern {\n+public:\n+  MBarrierWaitOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::MBarrierWaitOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto mBarrierWaitOp = llvm::dyn_cast<ttn::MBarrierWaitOp>(op);\n+    if (!mBarrierWaitOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    Value mbarrier = mBarrierWaitOp.getMbarrier();\n+    Value phase = mBarrierWaitOp.getPhase();\n+    PTXBuilder ptxBuilder;\n+\n+    auto ptxAsm =\n+        \"{\\n\"\n+        \".reg .pred                P1; \\n\"\n+        \"LAB_WAIT: \\n\"\n+        \"mbarrier.try_wait.parity.shared.b64 P1, [$0], $1, 0x989680; \\n\"\n+        \"@P1                       bra.uni DONE; \\n\"\n+        \"bra.uni                   LAB_WAIT; \\n\"\n+        \"DONE: \\n\"\n+        \"}\";\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+    auto *barOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, mbarrier), \"r\");\n+    auto *phaseOpr = ptxBuilder.newOperand(zext(i32_ty, phase), \"r\");\n+\n+    ptxInstr({barOpr, phaseOpr},\n+             /*onlyAttachMLIRArgs=*/true);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class ClusterArriveOpPattern\n+    : public NVGPUOpPatternBase<ttn::ClusterArriveOp, ClusterArriveOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::ClusterArriveOp, ClusterArriveOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::ClusterArriveOp op) const {\n+    auto relaxed = op.getRelaxed();\n+    if (relaxed)\n+      return \"barrier.cluster.arrive.relaxed.aligned;\";\n+    else\n+      return \"barrier.cluster.arrive.aligned;\";\n+  }\n+};\n+\n+class ClusterWaitOpPattern\n+    : public NVGPUOpPatternBase<ttn::ClusterWaitOp, ClusterWaitOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::ClusterWaitOp, ClusterWaitOpPattern>;\n+  using Base::Base;\n+  std::string getPtxAsm(ttn::ClusterWaitOp op) const {\n+    return \"barrier.cluster.wait.aligned;\";\n+  }\n+};\n+\n+class TMALoadTiledOpPattern : public mlir::RewritePattern {\n+public:\n+  TMALoadTiledOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::TMALoadTiledOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto tmaLoadTiledOp = llvm::dyn_cast<ttn::TMALoadTiledOp>(op);\n+    if (!tmaLoadTiledOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto dst = tmaLoadTiledOp.getDst();\n+    auto mbarrier = tmaLoadTiledOp.getMbarrier();\n+    auto tmaDesc = tmaLoadTiledOp.getTmaDesc();\n+    auto l2Desc = tmaLoadTiledOp.getL2Desc();\n+    auto pred = tmaLoadTiledOp.getPred();\n+    auto coords = tmaLoadTiledOp.getCoords();\n+    auto mcastMask = tmaLoadTiledOp.getMcastMask();\n+\n+    auto dimSize = coords.size();\n+\n+    PTXBuilder ptxBuilder;\n+    if (dimSize == 2) {\n+      if (mcastMask == nullptr) {\n+        auto ptxAsm =\n+            \"@$6 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier:\"\n+            \":complete_tx\"\n+            \"::bytes.L2::cache_hint [$0], [$1, {$2, $3}], [$4], $5;\";\n+        auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+        auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n+        auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+        auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+        auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+        auto *barOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, mbarrier), \"r\");\n+        auto *l2DescOpr = ptxBuilder.newOperand(l2Desc, \"l\");\n+        auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+\n+        ptxInstr({dstOpr, descOpr, c0Opr, c1Opr, barOpr, l2DescOpr, predOpr},\n+                 /*onlyAttachMLIRArgs=*/true);\n+      } else {\n+        auto ptxAsm =\n+            \"@$7 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::\"\n+            \"complete_tx::bytes.multicast::cluster.L2::cache_hint\"\n+            \" [$0], [$1, {$2, $3}], [$4], $5, $6;\";\n+        auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+        auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n+        auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+        auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+        auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+        auto *barOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, mbarrier), \"r\");\n+        auto *maskOpr = ptxBuilder.newOperand(mcastMask, \"h\");\n+        auto *l2DescOpr = ptxBuilder.newOperand(l2Desc, \"l\");\n+        auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+        ptxInstr({dstOpr, descOpr, c0Opr, c1Opr, barOpr, maskOpr, l2DescOpr,\n+                  predOpr},\n+                 /*onlyAttachMLIRArgs=*/true);\n+      }\n+    } else if (dimSize == 4) {\n+      assert(mcastMask == nullptr && \"Does not support multicast\");\n+      auto ptxAsm = \"@$8 \"\n+                    \"cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier:\"\n+                    \":complete_tx\"\n+                    \"::bytes.L2::cache_hint [$0], [$1, {$2, $3, $4, $5}], \"\n+                    \"[$6], $7;\";\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+      auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n+      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+      auto *c2Opr = ptxBuilder.newOperand(coords[2], \"r\");\n+      auto *c3Opr = ptxBuilder.newOperand(coords[3], \"r\");\n+      auto *barOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, mbarrier), \"r\");\n+      auto *l2DescOpr = ptxBuilder.newOperand(l2Desc, \"l\");\n+      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+      ptxInstr({dstOpr, descOpr, c0Opr, c1Opr, c2Opr, c3Opr, barOpr, l2DescOpr,\n+                predOpr},\n+               /*onlyAttachMLIRArgs=*/true);\n+    } else {\n+      assert(false && \"invalid dim size\");\n+    }\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy, /*hasSideEffect*/ true);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class TMAStoreTiledOpPattern : public mlir::RewritePattern {\n+public:\n+  TMAStoreTiledOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::TMAStoreTiledOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto tmaStoreTiledOp = llvm::dyn_cast<ttn::TMAStoreTiledOp>(op);\n+    if (!tmaStoreTiledOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto src = tmaStoreTiledOp.getSrc();\n+    auto tmaDesc = tmaStoreTiledOp.getTmaDesc();\n+    auto pred = tmaStoreTiledOp.getPred();\n+    auto coords = tmaStoreTiledOp.getCoords();\n+\n+    auto dimSize = coords.size();\n+\n+    PTXBuilder ptxBuilder;\n+    if (dimSize == 2) {\n+      auto ptxAsm = \"cp.async.bulk.tensor.2d.global.shared::cta.bulk_group\"\n+                    \"[$0, {$2, $3}], [$1];\";\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+\n+      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+      auto *srcOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, src), \"r\");\n+      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+      ptxInstr({descOpr, srcOpr, c0Opr, c1Opr, predOpr},\n+               /*onlyAttachMLIRArgs=*/true);\n+    } else if (dimSize == 3) {\n+      auto ptxAsm = \"@$5 cp.async.bulk.tensor.3d.global.shared::cta.bulk_group\"\n+                    \"[$0, {$2, $3, $4}], [$1];\";\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+\n+      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+      auto *srcOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, src), \"r\");\n+      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+      auto *c2Opr = ptxBuilder.newOperand(coords[2], \"r\");\n+      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+      ptxInstr({descOpr, srcOpr, c0Opr, c1Opr, c2Opr, predOpr},\n+               /*onlyAttachMLIRArgs=*/true);\n+    } else if (dimSize == 4) {\n+      auto ptxAsm = \"@$6 cp.async.bulk.tensor.4d.global.shared::cta.bulk_group\"\n+                    \"[$0, {$2, $3, $4, $5}], [$1];\";\n+      auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+      auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n+      auto *srcOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, src), \"r\");\n+      auto *c0Opr = ptxBuilder.newOperand(coords[0], \"r\");\n+      auto *c1Opr = ptxBuilder.newOperand(coords[1], \"r\");\n+      auto *c2Opr = ptxBuilder.newOperand(coords[2], \"r\");\n+      auto *c3Opr = ptxBuilder.newOperand(coords[3], \"r\");\n+      auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+      ptxInstr({descOpr, srcOpr, c0Opr, c1Opr, c2Opr, c3Opr, predOpr},\n+               /*onlyAttachMLIRArgs=*/true);\n+    } else {\n+      assert(false && \"invalid dim size\");\n+    }\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy, /*hasSideEffect*/ true);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class LoadDSmemOpPattern : public mlir::RewritePattern {\n+public:\n+  LoadDSmemOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::LoadDSmemOp::getOperationName(), 1, context) {\n+  }\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto loadDSmemOp = llvm::dyn_cast<ttn::LoadDSmemOp>(op);\n+    if (!loadDSmemOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto addr = loadDSmemOp.getAddr();\n+    auto ctaId = loadDSmemOp.getCtaId();\n+    auto bitwidth = loadDSmemOp.getBitwidth();\n+    auto vec = loadDSmemOp.getVec();\n+\n+    assert(\n+        (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n+        \"invalid bitwidth\");\n+    assert((vec == 1 || vec == 2 || vec == 4) && \"invalid vec size\");\n+    PTXBuilder ptxBuilder;\n+\n+    std::string o1 = vec > 1 ? \".v.u\" : \".u\";\n+    std::string vecStr = vec == 1   ? \"$0\"\n+                         : vec == 2 ? \"{$0, $1}\"\n+                                    : \"{$0, $1, $2, $3}\";\n+    unsigned argNum = vec == 1 ? 1 : vec == 2 ? 2 : 4;\n+    auto ptxAsm = \"{\\n\"\n+                  \".reg .u32 remoteAddr;\\n\"\n+                  \"mapa.shared::cluster.u32 remoteAddr, $\" +\n+                  std::to_string(argNum) + \" , $\" + std::to_string(argNum + 1) +\n+                  \" ; \\n\"\n+                  \"ld.shared::cluster\" +\n+                  o1 + std::to_string(bitwidth) + \" \" + vecStr +\n+                  \", [remoteAddr];\\n\"\n+                  \"}\\n\";\n+\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+    std::string c = bitwidth == 16 ? \"=h\" : (bitwidth == 32 ? \"=r\" : \"=l\");\n+    SmallVector<PTXBuilder::Operand *> oprs;\n+    for (unsigned i = 0; i < vec; ++i) {\n+      auto *ret = ptxBuilder.newOperand(c);\n+      oprs.push_back(ret);\n+    }\n+    auto *addrOpr = ptxBuilder.newOperand(addr, \"r\");\n+    auto *ctaIdOpr = ptxBuilder.newOperand(ctaId, \"r\");\n+    oprs.push_back(addrOpr);\n+    oprs.push_back(ctaIdOpr);\n+\n+    Type retTy = IntegerType::get(rewriter.getContext(), bitwidth);\n+    SmallVector<Type> retTys(vec, retTy);\n+    if (vec > 1)\n+      retTy = struct_ty(retTys);\n+\n+    ptxInstr(oprs,\n+             /*onlyAttachMLIRArgs=*/true);\n+\n+    auto res = ptxBuilder.launch(rewriter, loc, retTy);\n+    rewriter.replaceOp(op, {res});\n+    return mlir::success();\n+  }\n+};\n+\n+class WGMMAOpPattern : public mlir::RewritePattern {\n+public:\n+  WGMMAOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::WGMMAOp::getOperationName(), 1, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    using namespace ttn;\n+    auto ctx = rewriter.getContext();\n+    auto wgmmaOp = llvm::dyn_cast<ttn::WGMMAOp>(op);\n+    if (!wgmmaOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto opA = wgmmaOp.getOpA();\n+    auto opB = wgmmaOp.getOpB();\n+    auto opC = wgmmaOp.getOpC();\n+    auto m = wgmmaOp.getM();\n+    auto n = wgmmaOp.getN();\n+    auto k = wgmmaOp.getK();\n+    auto eltTypeC = wgmmaOp.getEltTypeC();\n+    auto eltTypeA = wgmmaOp.getEltTypeA();\n+    auto eltTypeB = wgmmaOp.getEltTypeB();\n+    auto layoutA = wgmmaOp.getLayoutA();\n+    auto layoutB = wgmmaOp.getLayoutB();\n+\n+    // Register checks\n+    auto typeA = opA.getType();\n+    auto typeB = opB.getType();\n+    auto typeC = opC.getType();\n+    auto structTypeA = typeA.dyn_cast<LLVM::LLVMStructType>();\n+    auto structTypeB = typeB.dyn_cast<LLVM::LLVMStructType>();\n+    auto structTypeC = typeC.dyn_cast<LLVM::LLVMStructType>();\n+    assert(!structTypeB && \"Operand B can not be registers\");\n+    assert(structTypeC && \"Operand C must be registers\");\n+\n+    // Element type, MNK shape and transposing support check\n+    // Reference:\n+    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-mma\n+    bool transA = layoutA == WGMMALayout::col;\n+    bool transB = layoutB == WGMMALayout::row;\n+    bool supported = false, needTransArgs = false, floatTypeWGMMA = false;\n+    assert(m % 8 == 0 && n % 8 == 0 && k % 8 == 0);\n+    // Below instructions do support transposing, must pass `trans` arguments\n+    supported |=\n+        (eltTypeA == WGMMAEltType::f16) && (eltTypeB == WGMMAEltType::f16) &&\n+        (eltTypeC == WGMMAEltType::f16 || eltTypeC == WGMMAEltType::f32) &&\n+        (m == 64 && 8 <= n && n <= 256 && k == 16);\n+    supported |= (eltTypeA == WGMMAEltType::bf16) &&\n+                 (eltTypeB == WGMMAEltType::bf16) &&\n+                 (eltTypeC == WGMMAEltType::f32) &&\n+                 (m == 64 && 8 <= n && n <= 256 && k == 16);\n+    needTransArgs = supported;\n+    floatTypeWGMMA = supported;\n+    // Below instructions do not support transposing\n+    if (!supported && !transA && !transB) {\n+      supported |= (eltTypeA == WGMMAEltType::tf32) &&\n+                   (eltTypeB == WGMMAEltType::tf32) &&\n+                   (eltTypeC == WGMMAEltType::f32) &&\n+                   (m == 64 && 8 <= n && n <= 256 && k == 8);\n+      supported |=\n+          (eltTypeA == WGMMAEltType::e4m3 || eltTypeA == WGMMAEltType::e5m2) &&\n+          (eltTypeB == WGMMAEltType::e4m3 || eltTypeB == WGMMAEltType::e5m2) &&\n+          (eltTypeC == WGMMAEltType::f16 || eltTypeC == WGMMAEltType::f32) &&\n+          (m == 64 && 8 <= n && n <= 256 && k == 32);\n+      floatTypeWGMMA = supported;\n+      // Below instructions are integer-based\n+      supported |= (eltTypeA == WGMMAEltType::s8) &&\n+                   (eltTypeB == WGMMAEltType::s8) &&\n+                   (eltTypeC == WGMMAEltType::s32) &&\n+                   (m == 64 && 8 <= n && n <= 224 && k == 32);\n+    }\n+    assert(supported && \"WGMMA type or shape is not supported\");\n+    PTXBuilder ptxBuilder;\n+    SmallVector<PTXBuilder::Operand *> oprs;\n+\n+    // Operands\n+    uint32_t asmOpIdx = 0;\n+\n+    // Operand C\n+    uint32_t numCRegs = structTypeC.getBody().size();\n+\n+    std::string args = \"\";\n+    args += \"{\";\n+    for (uint32_t i = 0; i < numCRegs; ++i) {\n+      args += \"$\" + std::to_string(asmOpIdx++) + (i == numCRegs - 1 ? \"\" : \",\");\n+      // LLVM does not support `+` semantic, we must repeat the arguments for\n+      // both input and outputs\n+      PTXBuilder::Operand *opr;\n+      if (structTypeC.getBody().front().isF32())\n+        opr = ptxBuilder.newOperand(\n+            extract_val(structTypeC.getBody()[i], opC, i), \"=f\");\n+      else\n+        opr = ptxBuilder.newOperand(\n+            extract_val(structTypeC.getBody()[i], opC, i), \"=r\");\n+      oprs.push_back(opr);\n+    }\n+    args += \"}, \";\n+\n+    for (uint32_t i = asmOpIdx - numCRegs; i < asmOpIdx; ++i) {\n+      auto *opr = ptxBuilder.newOperand(i);\n+      oprs.push_back(opr);\n+    }\n+\n+    // Note that LLVM will not skip the indexed repeating placeholders\n+    asmOpIdx += numCRegs;\n+    // Operand A\n+    if (structTypeA) {\n+      uint32_t numARegs = m * k / 128;\n+      assert(numARegs == structTypeA.getBody().size());\n+      args += \"{\";\n+      for (uint32_t i = 0; i < numARegs; ++i) {\n+        args +=\n+            \"$\" + std::to_string(asmOpIdx++) + (i == numARegs - 1 ? \"\" : \",\");\n+        auto *opr = ptxBuilder.newOperand(\n+            extract_val(structTypeA.getBody()[i], opA, i), \"f\");\n+        oprs.push_back(opr);\n+      }\n+      args += \"}, \";\n+    } else {\n+      args += \"$\" + std::to_string(asmOpIdx++) + \", \";\n+      auto *opr = ptxBuilder.newOperand(opA, \"l\");\n+      oprs.push_back(opr);\n+    }\n+\n+    // Operand B (must be `desc`)\n+    args += \"$\" + std::to_string(asmOpIdx++) + \", \";\n+    auto *opr = ptxBuilder.newOperand(opB, \"l\");\n+    oprs.push_back(opr);\n+\n+    // `scale-d` is 1 by default\n+    args += \"1\";\n+\n+    // `imm-scale-a`, and `imm-scale-b` are 1 by default only for float-based\n+    // WGMMA\n+    if (floatTypeWGMMA)\n+      args += \", 1, 1\";\n+\n+    // Push `trans-a` and `trans-b` args if needed (determined as constant)\n+    if (needTransArgs)\n+      args += \", \" + std::to_string(transA) + \", \" + std::to_string(transB);\n+\n+    auto ptxAsm = \"wgmma.mma_async.sync.aligned\"\n+                  \".m\" +\n+                  std::to_string(m) + \"n\" + std::to_string(n) + \"k\" +\n+                  std::to_string(k) + \".\" + stringifyEnum(eltTypeC).str() +\n+                  \".\" + stringifyEnum(eltTypeA).str() + \".\" +\n+                  stringifyEnum(eltTypeB).str() + \" \" + args + \";\";\n+\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+    ptxInstr(oprs,\n+             /*onlyAttachMLIRArgs=*/true);\n+\n+    auto res =\n+        ptxBuilder.launch(rewriter, loc, structTypeC, /*hasSideEffect*/ true);\n+    rewriter.replaceOp(op, {res});\n+    return mlir::success();\n+  }\n+};\n+\n+class FenceMBarrierInitOpPattern\n+    : public NVGPUOpPatternBase<ttn::FenceMBarrierInitOp,\n+                                FenceMBarrierInitOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::FenceMBarrierInitOp, FenceMBarrierInitOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::FenceMBarrierInitOp op) const {\n+    return \"fence.mbarrier_init.release.cluster;\";\n+  }\n+};\n+\n+class NamedBarrierArriveOpPattern : public mlir::RewritePattern {\n+public:\n+  NamedBarrierArriveOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::NamedBarrierArriveOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto namedBarrierArriveOp = llvm::dyn_cast<ttn::NamedBarrierArriveOp>(op);\n+    if (!namedBarrierArriveOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto bar = namedBarrierArriveOp.getBar();\n+    auto numThreads = namedBarrierArriveOp.getNumThreads();\n+    PTXBuilder ptxBuilder;\n+\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\"bar.arrive $0, $1;\");\n+    auto *barOpr = ptxBuilder.newOperand(bar, \"r\");\n+    auto *numThreadsOpr = ptxBuilder.newOperand(numThreads, \"r\");\n+    ptxInstr({barOpr, numThreadsOpr}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class NamedBarrierWaitOpPattern : public mlir::RewritePattern {\n+public:\n+  NamedBarrierWaitOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::NamedBarrierWaitOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto namedBarrierWaitOp = llvm::dyn_cast<ttn::NamedBarrierWaitOp>(op);\n+    if (!namedBarrierWaitOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto bar = namedBarrierWaitOp.getBar();\n+    auto numThreads = namedBarrierWaitOp.getNumThreads();\n+    PTXBuilder ptxBuilder;\n+\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\"bar.sync $0, $1;\");\n+    auto *barOpr = ptxBuilder.newOperand(bar, \"r\");\n+    auto *numThreadsOpr = ptxBuilder.newOperand(numThreads, \"r\");\n+    ptxInstr({barOpr, numThreadsOpr}, /*onlyAttachMLIRArgs=*/true);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class CGABarrierArriveOpPattern\n+    : public NVGPUOpPatternBase<ttn::CGABarrierArriveOp,\n+                                CGABarrierArriveOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::CGABarrierArriveOp, CGABarrierArriveOpPattern>;\n+  using Base::Base;\n+  std::string getPtxAsm(ttn::CGABarrierArriveOp op) const {\n+    return \"barrier.cluster.arrive;\";\n+  }\n+};\n+\n+class CGABarrierWaitOpPattern\n+    : public NVGPUOpPatternBase<ttn::CGABarrierWaitOp,\n+                                CGABarrierWaitOpPattern> {\n+public:\n+  using Base =\n+      NVGPUOpPatternBase<ttn::CGABarrierWaitOp, CGABarrierWaitOpPattern>;\n+  using Base::Base;\n+  std::string getPtxAsm(ttn::CGABarrierWaitOp op) const {\n+    return \"barrier.cluster.wait;\";\n+  }\n+};\n+\n+class StoreDSmemOpPattern : public mlir::RewritePattern {\n+public:\n+  StoreDSmemOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::StoreDSmemOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto storeDSmemOp = llvm::dyn_cast<ttn::StoreDSmemOp>(op);\n+    if (!storeDSmemOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto addr = storeDSmemOp.getAddr();\n+    auto ctaId = storeDSmemOp.getCtaId();\n+    auto values = storeDSmemOp.getValues();\n+    auto pred = storeDSmemOp.getPred();\n+\n+    auto bitwidth = storeDSmemOp.getBitwidth();\n+    auto vec = storeDSmemOp.getVec();\n+    assert(\n+        (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n+        \"invalid bitwidth\");\n+    assert((vec == 1 || vec == 2 || vec == 4) && vec == values.size() &&\n+           \"invalid vec size\");\n+\n+    PTXBuilder ptxBuilder;\n+\n+    std::string ptxAsm = \"{\\n\\t\"\n+                         \".reg .u32 remoteAddr;\\n\\t\"\n+                         \"mapa.shared::cluster.u32 remoteAddr, $0, $1;\\n\\t\"\n+                         \".reg .pred p;\\n\\t\"\n+                         \"mov.pred p, $2;\\n\\t\"\n+                         \"@p st.shared::cluster\";\n+    if (vec > 1)\n+      ptxAsm += \".v\" + std::to_string(vec);\n+    ptxAsm += \".u\" + std::to_string(bitwidth) + \" [remoteAddr], \";\n+    if (vec == 1)\n+      ptxAsm += \"$3\";\n+    else if (vec == 2)\n+      ptxAsm += \"{$3, $4}\";\n+    else if (vec == 4)\n+      ptxAsm += \"{$3, $4, $5, $6}\";\n+    ptxAsm += \";\\n\\t\";\n+    ptxAsm += \"}\\n\";\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+\n+    std::string c = bitwidth == 16 ? \"h\" : (bitwidth == 32 ? \"r\" : \"l\");\n+    SmallVector<PTXBuilder::Operand *> oprs;\n+    auto *addrOpr = ptxBuilder.newOperand(addr, \"r\");\n+    oprs.push_back(addrOpr);\n+    auto *ctaIdOpr = ptxBuilder.newOperand(ctaId, \"r\");\n+    oprs.push_back(ctaIdOpr);\n+    auto *predOpr = ptxBuilder.newOperand(pred, \"b\");\n+    oprs.push_back(predOpr);\n+    for (unsigned i = 0; i < values.size(); i++) {\n+      auto *valueOpr = ptxBuilder.newOperand(values[i], c);\n+      oprs.push_back(valueOpr);\n+    }\n+    ptxInstr(oprs,\n+             /*onlyAttachMLIRArgs=*/true);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy, /*hasSideEffect*/ true);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class Sts64OpPattern : public mlir::RewritePattern {\n+public:\n+  Sts64OpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::Sts64Op::getOperationName(), 1, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto sts64Op = llvm::dyn_cast<ttn::Sts64Op>(op);\n+    if (!sts64Op)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto offset = sts64Op.getOffset();\n+    auto d0 = sts64Op.getD0();\n+    auto d1 = sts64Op.getD1();\n+\n+    PTXBuilder ptxBuilder;\n+\n+    std::string ptxAsm = \"st.shared.v2.b32 [$0], {$1, $2}\";\n+    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n+\n+    SmallVector<PTXBuilder::Operand *> oprs;\n+    auto *addrOpr = ptxBuilder.newOperand(offset, \"r\");\n+    auto *d0Opr = ptxBuilder.newOperand(d0, \"r\");\n+    auto *d1Opr = ptxBuilder.newOperand(d1, \"r\");\n+\n+    ptxInstr({addrOpr, d0Opr, d1Opr},\n+             /*onlyAttachMLIRArgs=*/true);\n+\n+    auto asmReturnTy = void_ty(ctx);\n+    ptxBuilder.launch(rewriter, loc, asmReturnTy);\n+    rewriter.eraseOp(op);\n+    return mlir::success();\n+  }\n+};\n+\n+class RegAllocOpPattern\n+    : public NVGPUOpPatternBase<ttn::RegAllocOp, RegAllocOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::RegAllocOp, RegAllocOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::RegAllocOp op) const {\n+    auto regCount = op.getRegCount();\n+    return \"setmaxnreg.inc.sync.aligned.u32 \" + std::to_string(regCount) + \";\";\n+  }\n+};\n+\n+class RegDeallocOpPattern\n+    : public NVGPUOpPatternBase<ttn::RegDeallocOp, RegDeallocOpPattern> {\n+public:\n+  using Base = NVGPUOpPatternBase<ttn::RegDeallocOp, RegDeallocOpPattern>;\n+  using Base::Base;\n+\n+  std::string getPtxAsm(ttn::RegDeallocOp op) const {\n+    auto regCount = op.getRegCount();\n+    return \"setmaxnreg.dec.sync.aligned.u32 \" + std::to_string(regCount) + \";\";\n+  }\n+};\n+\n+class ClusterCTAIdOpPattern : public mlir::RewritePattern {\n+public:\n+  ClusterCTAIdOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::ClusterCTAIdOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto clusterCTAIdOp = llvm::dyn_cast<ttn::ClusterCTAIdOp>(op);\n+    if (!clusterCTAIdOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+\n+    auto x = getSRegValue(rewriter, loc, \"%cluster_ctaid.x\");\n+    auto y = getSRegValue(rewriter, loc, \"%cluster_ctaid.y\");\n+    auto z = getSRegValue(rewriter, loc, \"%cluster_ctaid.z\");\n+    auto nx = getSRegValue(rewriter, loc, \"%cluster_nctaid.x\");\n+    auto ny = getSRegValue(rewriter, loc, \"%cluster_nctaid.y\");\n+    auto res = add(x, mul(add(y, mul(z, ny)), nx));\n+    rewriter.replaceOp(op, {res});\n+    return mlir::success();\n+  }\n+};\n+\n+class WGMMADescCreateOpPattern : public mlir::RewritePattern {\n+public:\n+  WGMMADescCreateOpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::WGMMADescCreateOp::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto wgmmaDescCreateOp = llvm::dyn_cast<ttn::WGMMADescCreateOp>(op);\n+    if (!wgmmaDescCreateOp)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto buffer = wgmmaDescCreateOp.getBuffer();\n+    auto height = wgmmaDescCreateOp.getHeight();\n+    uint32_t mode = static_cast<uint32_t>(wgmmaDescCreateOp.getMode());\n+\n+    auto smem_nvvm_pointer = ptrtoint(i64_ty, buffer);\n+\n+    Value desc = int_val(64, 0);\n+    uint64_t swizzling = (mode == 1 ? 128 : mode == 2 ? 64 : 32);\n+    Value swizzling_ = int_val(64, swizzling);\n+    Value smem_address_bit = smem_nvvm_pointer;\n+\n+    Value strideDimension =\n+        lshr(shl(swizzling_, int_val(64, 3)), int_val(64, 4));\n+    Value height64 = zext(i64_ty, height);\n+    Value leadingDimension = lshr(mul(height64, swizzling_), int_val(64, 4));\n+\n+    // Value baseOffset = int_val(64, 0);\n+    Value startAddr =\n+        lshr(shl(smem_address_bit, int_val(64, 46)), int_val(64, 50));\n+\n+    Value mode_ = int_val(64, mode);\n+    desc = or_(desc, shl(mode_, int_val(64, 62)));\n+    desc = or_(desc, shl(strideDimension, int_val(64, 32)));\n+    desc = or_(desc, shl(leadingDimension, int_val(64, 16)));\n+    // desc = or_(desc, shl(baseOffset, int_val(64, 49)));\n+    desc = or_(desc, startAddr);\n+\n+    rewriter.replaceOp(op, {desc});\n+    return mlir::success();\n+  }\n+};\n+\n+class OffsetOfSts64OpPattern : public mlir::RewritePattern {\n+public:\n+  OffsetOfSts64OpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::OffsetOfSts64Op::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto offsetOfSts64Op = llvm::dyn_cast<ttn::OffsetOfSts64Op>(op);\n+    if (!offsetOfSts64Op)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto threadId = offsetOfSts64Op.getThreadId();\n+    auto rowOfWarp = offsetOfSts64Op.getRowOfWarp();\n+    auto elemIdx = offsetOfSts64Op.getElemIdx();\n+    auto leadingDimOffset = offsetOfSts64Op.getLeadingDimOffset();\n+    auto rowStride = offsetOfSts64Op.getRowStride();\n+    auto swizzleEnabled = offsetOfSts64Op.getSwizzleEnabled();\n+\n+    if (swizzleEnabled) {\n+      assert((rowStride == 32 || rowStride == 64 || rowStride == 128) &&\n+             \"wrong rowString for swizzleEnabled\");\n+    }\n+\n+    uint32_t perPhase = 0;\n+    uint32_t maxPhase = 0;\n+    if (rowStride == 128) {\n+      perPhase = 1;\n+      maxPhase = 8;\n+    } else if (rowStride == 64) {\n+      perPhase = 2;\n+      maxPhase = 4;\n+    } else if (rowStride == 32) {\n+      perPhase = 4;\n+      maxPhase = 2;\n+    }\n+\n+    auto laneId = and_(threadId, i32_val(0x1f));\n+    auto myRow =\n+        add(mul(and_(lshr(elemIdx, i32_val(1)), i32_val(0x1)), i32_val(8)),\n+            udiv(laneId, i32_val(4)));\n+    auto myCol = add(mul(udiv(elemIdx, i32_val(4)), i32_val(8)),\n+                     mul(urem(laneId, i32_val(4)), i32_val(2)));\n+    myRow = add(myRow, rowOfWarp);\n+    auto phase = urem(udiv(myRow, i32_val(perPhase)), i32_val(maxPhase));\n+    auto lineOffset =\n+        add(mul(urem(myRow, i32_val(perPhase)), i32_val(rowStride)),\n+            mul(myCol, i32_val(4)));\n+    auto colOffset =\n+        add(mul(xor_(udiv(lineOffset, i32_val(16)), phase), i32_val(16)),\n+            urem(lineOffset, i32_val(16)));\n+    auto offset =\n+        add(mul(udiv(myRow, i32_val(perPhase)), i32_val(128)), colOffset);\n+\n+    rewriter.replaceOp(op, {offset});\n+    return mlir::success();\n+  }\n+};\n+\n+class OffsetOfStmatrixV4OpPattern : public mlir::RewritePattern {\n+public:\n+  OffsetOfStmatrixV4OpPattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(ttn::OffsetOfStmatrixV4Op::getOperationName(), 1,\n+                             context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto ctx = rewriter.getContext();\n+    auto offsetOfStmatrixV4Op = llvm::dyn_cast<ttn::OffsetOfStmatrixV4Op>(op);\n+    if (!offsetOfStmatrixV4Op)\n+      return mlir::failure();\n+    auto loc = op->getLoc();\n+    auto threadId = offsetOfStmatrixV4Op.getThreadId();\n+    auto rowOfWarp = offsetOfStmatrixV4Op.getRowOfWarp();\n+    auto elemIdx = offsetOfStmatrixV4Op.getElemIdx();\n+    auto leadingDimOffset = offsetOfStmatrixV4Op.getLeadingDimOffset();\n+    auto rowStride = offsetOfStmatrixV4Op.getRowStride();\n+    auto swizzleEnabled = offsetOfStmatrixV4Op.getSwizzleEnabled();\n+\n+    if (swizzleEnabled) {\n+      uint32_t perPhase = 0;\n+      uint32_t maxPhase = 0;\n+      if (rowStride == 64) {\n+        perPhase = 1;\n+        maxPhase = 8;\n+      } else if (rowStride == 32) {\n+        perPhase = 2;\n+        maxPhase = 4;\n+      } else if (rowStride == 16) {\n+        perPhase = 4;\n+        maxPhase = 2;\n+      }\n+\n+      Value iterOfCol = udiv(elemIdx, i32_val(8));\n+      Value myRow = add(rowOfWarp, and_(threadId, i32_val(0xf)));\n+      Value myCol =\n+          mul(and_(lshr(threadId, i32_val(4)), i32_val(0x1)), i32_val(8));\n+      myCol = add(myCol, mul(iterOfCol, i32_val(16)));\n+\n+      Value offset0 =\n+          mul(udiv(myCol, i32_val(rowStride)), i32_val(leadingDimOffset));\n+      myCol = urem(myCol, i32_val(rowStride));\n+\n+      Value phase = urem(udiv(myRow, i32_val(perPhase)), i32_val(maxPhase));\n+\n+      Value lineOffset =\n+          add(mul(urem(myRow, i32_val(perPhase)), i32_val(rowStride)), myCol);\n+      Value colOffset =\n+          add(mul(xor_(udiv(lineOffset, i32_val(8)), phase), i32_val(8)),\n+              urem(lineOffset, i32_val(8)));\n+      Value offset1 =\n+          add(mul(udiv(myRow, i32_val(perPhase)), i32_val(64)), colOffset);\n+\n+      Value res = add(offset1, offset0);\n+\n+      rewriter.replaceOp(op, {res});\n+    } else {\n+      Value iterOfCol = udiv(elemIdx, i32_val(4));\n+      Value myRow = add(rowOfWarp, and_(threadId, i32_val(0xf)));\n+      Value myCol =\n+          mul(and_(lshr(threadId, i32_val(4)), i32_val(0x1)), i32_val(8));\n+      myCol = add(myCol, mul(iterOfCol, i32_val(16)));\n+\n+      Value offset =\n+          add(mul(myRow, i32_val(rowStride)), mul(myCol, i32_val(2)));\n+      rewriter.replaceOp(op, {offset});\n+    }\n+    return mlir::success();\n+  }\n+};\n+\n+class ConvertNVGPUToLLVM : public ConvertNVGPUToLLVMBase<ConvertNVGPUToLLVM> {\n+\n+public:\n+  explicit ConvertNVGPUToLLVM() {}\n+\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp mod = getOperation();\n+    RewritePatternSet patterns(context);\n+\n+    patterns.add<CGABarrierSyncOpPattern>(context);\n+    patterns.add<FenceAsyncSharedOpPattern>(context);\n+    patterns.add<WGMMAFenceOpPattern>(context);\n+    patterns.add<WGMMACommitGroupOpPattern>(context);\n+    patterns.add<WGMMAWaitGroupOpPattern>(context);\n+    patterns.add<StoreMatrixOpPattern>(context);\n+    patterns.add<OffsetOfStmatrixV4OpPattern>(context);\n+    patterns.add<WGMMADescCreateOpPattern>(context);\n+    patterns.add<MBarrierInitOpPattern>(context);\n+    patterns.add<MBarrierArriveOpPattern>(context);\n+    patterns.add<MBarrierWaitOpPattern>(context);\n+    patterns.add<ClusterArriveOpPattern>(context);\n+    patterns.add<ClusterWaitOpPattern>(context);\n+    patterns.add<TMALoadTiledOpPattern>(context);\n+    patterns.add<TMAStoreTiledOpPattern>(context);\n+    patterns.add<LoadDSmemOpPattern>(context);\n+    patterns.add<ClusterCTAIdOpPattern>(context);\n+    patterns.add<RegAllocOpPattern>(context);\n+    patterns.add<RegDeallocOpPattern>(context);\n+    patterns.add<WGMMAOpPattern>(context);\n+    patterns.add<NamedBarrierWaitOpPattern>(context);\n+    patterns.add<NamedBarrierArriveOpPattern>(context);\n+\n+    patterns.add<FenceMBarrierInitOpPattern>(context);\n+    patterns.add<StoreDSmemOpPattern>(context);\n+    patterns.add<Sts64OpPattern>(context);\n+    patterns.add<OffsetOfSts64OpPattern>(context);\n+    patterns.add<CGABarrierWaitOpPattern>(context);\n+    patterns.add<CGABarrierArriveOpPattern>(context);\n+    if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())\n+      signalPassFailure();\n+  }\n+};\n+\n+} // anonymous namespace\n+\n+namespace mlir {\n+namespace triton {\n+\n+std::unique_ptr<OperationPass<ModuleOp>> createConvertNVGPUToLLVMPass() {\n+  return std::make_unique<::ConvertNVGPUToLLVM>();\n+}\n+\n+} // namespace triton\n+} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -14,7 +14,6 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     TritonGPUToLLVM.cpp\n     GCNAsmFormat.cpp\n     PTXAsmFormat.cpp\n-    TritonGPUToLLVMPass.cpp\n     ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp\n     ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp\n     ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 14, "changes": 30, "file_content_changes": "@@ -1,5 +1,7 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"Utility.h\"\n+\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n@@ -574,7 +576,8 @@ struct ConvertLayoutOpConversion\n     unsigned inVec = 0;\n     unsigned outVec = 0;\n     auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n-    if (getElementTypeOrSelf(op.getType()).isa<mlir::Float8E4M3B11FNUZType>()) {\n+    if (getElementTypeOrSelf(op.getType())\n+            .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>()) {\n       assert(inVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n       assert(outVec % 4 == 0 && \"conversion not supported for FP8E4M3B15\");\n     }\n@@ -589,11 +592,10 @@ struct ConvertLayoutOpConversion\n       if (repId != 0) {\n         // TODO[shuhaoj]: change hard code style of numThreads. Hide async\n         // attr.  Better way to determine barId (number of agents are limited).\n-        if (op->hasAttr(\"async_agent\")) {\n-          int agentId = getAgentIds(op).front(), roleId = 0;\n-          if (op->hasAttr(\"agent.mutex_role\"))\n-            roleId =\n-                op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+        if (auto optionalAgentId = getWSAgentId(op)) {\n+          int agentId = *optionalAgentId, roleId = 0;\n+          if (auto optionalRoleId = getWSRoleId(op))\n+            roleId = *optionalRoleId;\n           int barId = agentId + roleId + nameBarrierIdBegin;\n           assert(barId < nameBarrierIdEnd);\n           auto bar = rewriter.create<LLVM::ConstantOp>(\n@@ -624,10 +626,10 @@ struct ConvertLayoutOpConversion\n \n       // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n       // attr.  Better way to determine barId (number of agents are limited).\n-      if (op->hasAttr(\"async_agent\")) {\n-        int agentId = getAgentIds(op).front(), roleId = 0;\n-        if (op->hasAttr(\"agent.mutex_role\"))\n-          roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+      if (auto optionalAgentId = getWSAgentId(op)) {\n+        int agentId = *optionalAgentId, roleId = 0;\n+        if (auto optionalRoleId = getWSRoleId(op))\n+          roleId = *optionalRoleId;\n         int barId = agentId + roleId + nameBarrierIdBegin;\n         assert(barId < nameBarrierIdEnd);\n         auto bar = rewriter.create<LLVM::ConstantOp>(\n@@ -793,10 +795,10 @@ struct ConvertLayoutOpConversion\n       }\n       // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n       // attr.  Better way to determine barId (number of agents are limited).\n-      if (op->hasAttr(\"async_agent\")) {\n-        int agentId = getAgentIds(op).front(), roleId = 0;\n-        if (op->hasAttr(\"agent.mutex_role\"))\n-          roleId = op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+      if (auto optionalAgentId = getWSAgentId(op)) {\n+        int agentId = *optionalAgentId, roleId = 0;\n+        if (auto optionalRoleId = getWSRoleId(op))\n+          roleId = *optionalRoleId;\n         int barId = agentId + roleId + nameBarrierIdBegin;\n         assert(barId < nameBarrierIdEnd);\n         auto bar = rewriter.create<LLVM::ConstantOp>("}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -503,7 +503,7 @@ std::function<void(int, int)> getLoadMatrixFn(\n   if (tensor.getType()\n           .cast<RankedTensorType>()\n           .getElementType()\n-          .isa<mlir::Float8E4M3B11FNUZType>()) {\n+          .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>()) {\n     bool noTrans = (isA ^ (order[0] == 0));\n     assert(noTrans && \"float8e4b15 must have row-col layout\");\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -125,7 +125,7 @@ struct DotWaitOpConversion\n   matchAndRewrite(triton::nvidia_gpu::DotWaitOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     auto pendings = op.getPendings();\n-    rewriter.create<triton::nvgpu::WGMMAWaitOp>(op.getLoc(), pendings);\n+    rewriter.create<triton::nvgpu::WGMMAWaitGroupOp>(op.getLoc(), pendings);\n \n     // Safe to remove the op since it doesn't have any return value.\n     rewriter.eraseOp(op);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/WGMMA.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -340,7 +340,7 @@ LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n   rewriter.create<triton::nvgpu::WGMMACommitGroupOp>(loc);\n \n   if (sync)\n-    rewriter.create<triton::nvgpu::WGMMAWaitOp>(loc, 0);\n+    rewriter.create<triton::nvgpu::WGMMAWaitGroupOp>(loc, 0);\n \n   SmallVector<Value> results =\n       unpackAccumulator(rewriter, loc, mmaResults, dTensorTy);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -627,17 +627,20 @@ struct FpToFpOpConversion\n     auto F8E4M3B15TyID = TypeID::get<mlir::Float8E4M3B11FNUZType>();\n     auto F8E4M3TyID = TypeID::get<mlir::Float8E4M3FNUZType>();\n     auto F8E5M2TyID = TypeID::get<mlir::Float8E5M2Type>();\n+    auto F8E4M3FNTyID = TypeID::get<mlir::Float8E4M3FNType>();\n     auto F16TyID = TypeID::get<mlir::Float16Type>();\n     auto BF16TyID = TypeID::get<mlir::BFloat16Type>();\n     auto F32TyID = TypeID::get<mlir::Float32Type>();\n     auto F64TyID = TypeID::get<mlir::Float64Type>();\n     static DenseMap<std::pair<TypeID, TypeID>, std::string> srcMap = {\n         // F8 -> F16\n         {{F8E4M3B15TyID, F16TyID}, Fp8E4M3B15_to_Fp16},\n+        {{F8E4M3FNTyID, F16TyID}, Fp8E4M3B15x4_to_Fp16},\n         {{F8E4M3TyID, F16TyID}, Fp8E4M3_to_Fp16},\n         {{F8E5M2TyID, F16TyID}, Fp8E5M2_to_Fp16},\n         // F16 -> F8\n         {{F16TyID, F8E4M3B15TyID}, Fp16_to_Fp8E4M3B15},\n+        {{F16TyID, F8E4M3FNTyID}, Fp16_to_Fp8E4M3B15x4},\n         {{F16TyID, F8E4M3TyID}, Fp16_to_Fp8E4M3},\n         {{F16TyID, F8E5M2TyID}, Fp16_to_Fp8E5M2},\n         // F8 -> BF16\n@@ -1181,7 +1184,11 @@ void populateElementwiseOpToLLVMPatterns(\n   POPULATE_BINARY_OP(arith::ShRSIOp, LLVM::AShrOp)  // >>\n   POPULATE_BINARY_OP(arith::ShRUIOp, LLVM::LShrOp)  // >>\n   POPULATE_BINARY_OP(arith::MinFOp, LLVM::MinNumOp) // fmin\n+  POPULATE_BINARY_OP(arith::MaxFOp, LLVM::MaxNumOp) // fmax\n   POPULATE_BINARY_OP(arith::MinSIOp, LLVM::SMinOp)  // smin\n+  POPULATE_BINARY_OP(arith::MaxSIOp, LLVM::SMaxOp)  // smax\n+  POPULATE_BINARY_OP(arith::MinUIOp, LLVM::UMinOp)  // umin\n+  POPULATE_BINARY_OP(arith::MaxUIOp, LLVM::UMaxOp)  // umax\n #undef POPULATE_BINARY_OP\n \n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PTXAsmFormat.cpp", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -50,6 +50,14 @@ PTXBuilder::Operand *PTXBuilder::newOperand(StringRef constraint, bool init) {\n   return opr;\n }\n \n+PTXBuilder::Operand *PTXBuilder::newOperand(unsigned operandIndex) {\n+  assert(operandIndex < oprCounter && \"operand index out of range\");\n+  auto *opr = newOperand();\n+  opr->idx = oprCounter++;\n+  opr->constraint = std::to_string(operandIndex);\n+  return opr;\n+}\n+\n PTXBuilder::Operand *PTXBuilder::newConstantOperand(const std::string &v) {\n   argArchive.emplace_back(std::make_unique<Operand>());\n   argArchive.back()->repr = [v](int idx) { return v; };"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 12, "changes": 22, "file_content_changes": "@@ -1,6 +1,7 @@\n #include \"ReduceOpToLLVM.h\"\n #include \"Utility.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n \n using namespace mlir;\n@@ -289,7 +290,7 @@ struct ReduceOpConversion\n             triton::ReduceOp op) const {\n     // TODO[shuhaoj]: change hard code style of numThreads. Hide async_agent\n     // attr.\n-    if (op->hasAttr(\"async_agent\")) {\n+    if (getWSAgentId(op)) {\n       barSync(rewriter, op, getAgentIds(op).front(), 128);\n     } else {\n       barrier();\n@@ -320,17 +321,14 @@ struct ReduceOpConversion\n       return NVVM::ReduxKind::OR;\n     if (isa<arith::XOrIOp>(reduceOp))\n       return NVVM::ReduxKind::XOR;\n-    if (auto externalCall =\n-            dyn_cast<triton::PureExternElementwiseOp>(reduceOp)) {\n-      if (externalCall.getSymbol() == \"__nv_min\")\n-        return NVVM::ReduxKind::MIN;\n-      if (externalCall.getSymbol() == \"__nv_umin\")\n-        return NVVM::ReduxKind::UMIN;\n-      if (externalCall.getSymbol() == \"__nv_max\")\n-        return NVVM::ReduxKind::MAX;\n-      if (externalCall.getSymbol() == \"__nv_umax\")\n-        return NVVM::ReduxKind::UMAX;\n-    }\n+    if (isa<arith::MinSIOp>(reduceOp))\n+      return NVVM::ReduxKind::MIN;\n+    if (isa<arith::MinUIOp>(reduceOp))\n+      return NVVM::ReduxKind::UMIN;\n+    if (isa<arith::MaxSIOp>(reduceOp))\n+      return NVVM::ReduxKind::MAX;\n+    if (isa<arith::MaxUIOp>(reduceOp))\n+      return NVVM::ReduxKind::UMAX;\n     return std::nullopt;\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -6,6 +6,7 @@ using namespace mlir;\n using namespace mlir::triton;\n \n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getSRegValue;\n using ::mlir::triton::gpu::getTotalElemsPerThread;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n@@ -403,7 +404,7 @@ struct GetProgramIdOpConversion\n     sreg.append(1, 'x' + op.getAxisAsInt()); // 0 -> 'x', 1 -> 'y', 2 -> 'z'\n \n     Value programId = getSRegValue(rewriter, loc, sreg);\n-    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(op, i32_ty, programId);\n+    rewriter.replaceOp(op, programId);\n     return success();\n   }\n };\n@@ -430,7 +431,7 @@ struct GetNumProgramsOpConversion\n     sreg.append(1, 'x' + op.getAxis()); // 0 -> 'x', 1 -> 'y', 2 -> 'z'\n \n     Value numPrograms = getSRegValue(rewriter, loc, sreg);\n-    rewriter.replaceOpWithNewOp<arith::IndexCastOp>(op, i32_ty, numPrograms);\n+    rewriter.replaceOp(op, numPrograms);\n     return success();\n   }\n };\n@@ -575,7 +576,7 @@ struct ExtractSliceOpConversion\n     // newShape = rank_reduce(shape)\n     // Triton only supports static tensor sizes\n     SmallVector<Value, 4> strideVals;\n-    for (auto i = 0; i < op.static_sizes().size(); ++i) {\n+    for (auto i = 0; i < op.getStaticSizes().size(); ++i) {\n       if (op.getStaticSize(i) == 1) {\n         offsetVals.erase(offsetVals.begin() + i);\n       } else {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 28, "deletions": 42, "changes": 70, "file_content_changes": "@@ -11,6 +11,7 @@\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n #include <set>\n \n@@ -31,6 +32,7 @@ using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n using ::mlir::triton::gpu::TMAMetadataTy;\n+namespace ttng = ::mlir::triton::nvidia_gpu;\n \n typedef DenseMap<Operation *, triton::MakeTensorPtrOp> TensorPtrMapT;\n \n@@ -238,24 +240,24 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return llvmStruct;\n   }\n \n-  Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n-    auto tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n+  // Returns CTA level thread idx\n+  Value getThreadIdInCTA(ConversionPatternRewriter &rewriter,\n+                         Location loc) const {\n+    Value tid = rewriter.create<::mlir::gpu::ThreadIdOp>(\n         loc, ::mlir::gpu::Dimension::x);\n     return rewriter.create<arith::IndexCastOp>(loc, i32_ty, tid);\n   }\n \n-  static Value getSRegValue(OpBuilder &b, Location loc,\n-                            const std::string &sRegStr) {\n-    PTXBuilder builder;\n-    auto &mov = builder.create(\"mov\")->o(\"u32\");\n-    auto *destOpr = builder.newOperand(\"=r\");\n-    auto *sRegOpr = builder.newConstantOperand(sRegStr);\n-    mov(destOpr, sRegOpr);\n-    Value val = builder.launch(b, loc, b.getIntegerType(32), false);\n-\n-    auto cast = b.create<UnrealizedConversionCastOp>(\n-        loc, TypeRange{b.getIntegerType(32)}, ValueRange{val});\n-    return cast.getResult(0);\n+  // Returns CTA level thread idx for not ws mode.\n+  // Returns agent level thread idx for ws mode.\n+  Value getThreadId(ConversionPatternRewriter &rewriter, Location loc) const {\n+    Value tid = getThreadIdInCTA(rewriter, loc);\n+    auto mod = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    if (ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod)) {\n+      Value _128 = rewriter.create<arith::ConstantIntOp>(loc, 128, 32);\n+      tid = rewriter.create<arith::RemSIOp>(loc, tid, _128);\n+    }\n+    return tid;\n   }\n \n   Value getClusterCTAId(ConversionPatternRewriter &rewriter,\n@@ -984,32 +986,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return ret;\n   }\n \n-  SmallVector<Value>\n-  emitBaseIndexForMmaLayoutV2(Location loc, ConversionPatternRewriter &rewriter,\n-                              const MmaEncodingAttr &mmaLayout,\n-                              RankedTensorType type) const {\n-    auto shape = type.getShape();\n-    auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n-    assert(warpsPerCTA.size() == 2);\n-    auto order = triton::gpu::getOrder(mmaLayout);\n-    Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = i32_val(32);\n-    Value laneId = urem(threadId, warpSize);\n-    Value warpId = udiv(threadId, warpSize);\n-\n-    SmallVector<Value> multiDimWarpId =\n-        delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n-    multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n-    multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n-    Value offWarp0 = mul(multiDimWarpId[0], i32_val(16));\n-    Value offWarp1 = mul(multiDimWarpId[1], i32_val(8));\n-\n-    SmallVector<Value> multiDimBase(2);\n-    multiDimBase[0] = add(udiv(laneId, i32_val(4)), offWarp0);\n-    multiDimBase[1] = add(mul(i32_val(2), urem(laneId, i32_val(4))), offWarp1);\n-    return multiDimBase;\n-  }\n-\n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForMmaLayoutV2(const MmaEncodingAttr &mmaLayout,\n                            RankedTensorType type) const {\n@@ -1062,8 +1038,18 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     else\n       warpsN = shape[1] / instrShape[1];\n \n-    SmallVector<Value> multiDimWarpId =\n-        delinearize(rewriter, loc, warpId, _warpsPerCTA, order);\n+    SmallVector<Value> multiDimWarpId(2);\n+    if (mmaLayout.isHopper()) {\n+      // TODO[goostavz]: the tiling order from CTA->warp level is different for\n+      // MMAv2/3. This is a workaround since we don't explicitly have warpGrp\n+      // level in the layout definition, and the tiling order of warpGrp->warp\n+      // must be fixed to meet the HW's needs. We may need to consider to\n+      // explicitly define warpGrpPerCTA for MMAv3 layout.\n+      multiDimWarpId[0] = urem(warpId, warpsPerCTA[0]);\n+      multiDimWarpId[1] = urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]);\n+    } else {\n+      multiDimWarpId = delinearize(rewriter, loc, warpId, _warpsPerCTA, order);\n+    }\n     Value warpId0 = urem(multiDimWarpId[0], i32_val(warpsM));\n     Value warpId1 = urem(multiDimWarpId[1], i32_val(warpsN));\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 22, "deletions": 2, "changes": 24, "file_content_changes": "@@ -49,6 +49,14 @@ namespace ttng = mlir::triton::nvidia_gpu;\n \n namespace {\n \n+// pass ws related named attrs.\n+static void addWSNamedAttrs(Operation *op,\n+                            ArrayRef<mlir::NamedAttribute> attrs) {\n+  for (const NamedAttribute attr : attrs)\n+    if (attr.getName() == \"async_agent\" || attr.getName() == \"agent.mutex_role\")\n+      op->setAttr(attr.getName(), attr.getValue());\n+}\n+\n class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n public:\n   explicit TritonLLVMFunctionConversionTarget(MLIRContext &ctx, bool isROCM)\n@@ -602,7 +610,8 @@ class ConvertTritonGPUToLLVM\n   void decomposeFp8e4b15Convert(ModuleOp mod) const {\n     mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n       OpBuilder builder(cvtOp);\n-      if (!getElementTypeOrSelf(cvtOp).isa<mlir::Float8E4M3B11FNUZType>())\n+      if (!getElementTypeOrSelf(cvtOp)\n+               .isa<mlir::Float8E4M3B11FNUZType, mlir::Float8E4M3FNType>())\n         return;\n       auto shape = cvtOp.getType().cast<RankedTensorType>().getShape();\n       auto argEncoding =\n@@ -617,10 +626,13 @@ class ConvertTritonGPUToLLVM\n       auto newCvtType = RankedTensorType::get(shape, F16Ty, cvtEncoding);\n       auto newArg = builder.create<mlir::triton::FpToFpOp>(\n           cvtOp.getLoc(), newArgType, cvtOp.getOperand());\n+      addWSNamedAttrs(newArg, cvtOp->getAttrs());\n       auto newCvt = builder.create<mlir::triton::gpu::ConvertLayoutOp>(\n           cvtOp.getLoc(), newCvtType, newArg);\n+      addWSNamedAttrs(newCvt, cvtOp->getAttrs());\n       auto newRet = builder.create<mlir::triton::FpToFpOp>(\n           cvtOp.getLoc(), cvtOp.getType(), newCvt.getResult());\n+      addWSNamedAttrs(newRet, cvtOp->getAttrs());\n       cvtOp.replaceAllUsesWith(newRet.getResult());\n       cvtOp.erase();\n     });\n@@ -646,8 +658,10 @@ class ConvertTritonGPUToLLVM\n                 getOrder(srcMma), numWarps, threadsPerWarp, numCTAs));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n+        addWSNamedAttrs(tmp, cvtOp->getAttrs());\n         auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), dstType, tmp);\n+        addWSNamedAttrs(newConvert, cvtOp->getAttrs());\n         cvtOp.replaceAllUsesWith(newConvert.getResult());\n         cvtOp.erase();\n       }\n@@ -674,8 +688,10 @@ class ConvertTritonGPUToLLVM\n                 srcType.getElementType()));\n         auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n+        addWSNamedAttrs(tmp, cvtOp->getAttrs());\n         auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n             cvtOp.getLoc(), dstType, tmp);\n+        addWSNamedAttrs(newConvert, cvtOp->getAttrs());\n         cvtOp.replaceAllUsesWith(newConvert.getResult());\n         cvtOp.erase();\n       }\n@@ -750,6 +766,7 @@ class ConvertTritonGPUToLLVM\n           /*boundaryCheck=*/nullptr, /*padding=*/nullptr,\n           insertSliceAsyncOp.getCache(), insertSliceAsyncOp.getEvict(),\n           insertSliceAsyncOp.getIsVolatile());\n+      addWSNamedAttrs(loadOp, insertSliceAsyncOp->getAttrs());\n \n       // insert_slice\n       auto axis = insertSliceAsyncOp.getAxis();\n@@ -765,6 +782,7 @@ class ConvertTritonGPUToLLVM\n       auto insertSliceOp = builder.create<tensor::InsertSliceOp>(\n           insertSliceAsyncOp.getLoc(), loadOp, insertSliceAsyncOp.getDst(),\n           offsets, sizes, strides);\n+      addWSNamedAttrs(insertSliceOp, insertSliceAsyncOp->getAttrs());\n \n       // Replace\n       insertSliceAsyncOp.replaceAllUsesWith(insertSliceOp.getResult());\n@@ -784,7 +802,9 @@ class ConvertTritonGPUToLLVM\n       } else if (decomposed) {\n         // Wait for all previous async ops\n         OpBuilder builder(asyncWaitOp);\n-        builder.create<triton::gpu::AsyncWaitOp>(asyncWaitOp.getLoc(), 0);\n+        auto newWaitOp =\n+            builder.create<triton::gpu::AsyncWaitOp>(asyncWaitOp.getLoc(), 0);\n+        addWSNamedAttrs(newWaitOp, asyncWaitOp->getAttrs());\n         asyncWaitOp.erase();\n       }\n     });"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -27,6 +27,9 @@ TritonGPUToLLVMTypeConverter::TritonGPUToLLVMTypeConverter(\n   addConversion([&](mlir::Float8E4M3B11FNUZType type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });\n+  addConversion([&](mlir::Float8E4M3FNType type) -> std::optional<Type> {\n+    return IntegerType::get(type.getContext(), 8);\n+  });\n   addConversion([&](mlir::Float8E4M3FNUZType type) -> std::optional<Type> {\n     return IntegerType::get(type.getContext(), 8);\n   });"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -288,6 +288,15 @@ Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                  int i) {\n   return commonShflSync(loc, rewriter, val, i, \"up\", \"0x0\");\n }\n+Value getSRegValue(OpBuilder &b, Location loc, const std::string &sRegStr) {\n+  PTXBuilder builder;\n+  auto &mov = builder.create(\"mov\")->o(\"u32\");\n+  auto *destOpr = builder.newOperand(\"=r\");\n+  auto *sRegOpr = builder.newConstantOperand(sRegStr);\n+  mov(destOpr, sRegOpr);\n+  Value val = builder.launch(b, loc, b.getIntegerType(32), false);\n+  return val;\n+}\n \n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -29,6 +29,7 @@\n #define umin(...) rewriter.create<LLVM::UMinOp>(loc, __VA_ARGS__)\n #define fmin(...) rewriter.create<LLVM::MinNumOp>(loc, __VA_ARGS__)\n #define shl(...) rewriter.create<LLVM::ShlOp>(loc, __VA_ARGS__)\n+#define lshr(...) rewriter.create<LLVM::LShrOp>(loc, __VA_ARGS__)\n #define and_(...) rewriter.create<LLVM::AndOp>(loc, __VA_ARGS__)\n #define xor_(...) rewriter.create<LLVM::XOrOp>(loc, __VA_ARGS__)\n #define or_(...) rewriter.create<LLVM::OrOp>(loc, __VA_ARGS__)\n@@ -323,7 +324,7 @@ Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                int i);\n Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                  int i);\n-\n+Value getSRegValue(OpBuilder &b, Location loc, const std::string &sRegStr);\n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content);\n "}, {"filename": "lib/Dialect/NVGPU/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,2 +1 @@\n add_subdirectory(IR)\n-add_subdirectory(ToLLVMIR)"}, {"filename": "lib/Dialect/NVGPU/ToLLVMIR/CMakeLists.txt", "status": "removed", "additions": 0, "deletions": 17, "changes": 17, "file_content_changes": "@@ -1,17 +0,0 @@\n-add_mlir_translation_library(NVGPUToLLVMIR\n-  NVGPUToLLVMIR.cpp\n-\n-  DEPENDS\n-  NVGPUTableGen\n-\n-  LINK_COMPONENTS\n-  Core\n-\n-  LINK_LIBS PUBLIC\n-  MLIRIR\n-  MLIRLLVMDialect\n-  MLIRNVVMDialect\n-  MLIRSupport\n-  MLIRTargetLLVMIRExport\n-  NVGPUIR\n-  )"}, {"filename": "lib/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.cpp", "status": "removed", "additions": 0, "deletions": 782, "changes": 782, "file_content_changes": "@@ -1,782 +0,0 @@\n-/*\n- * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n- *\n- * Permission is hereby granted, free of charge, to any person obtaining\n- * a copy of this software and associated documentation files\n- * (the \"Software\"), to deal in the Software without restriction,\n- * including without limitation the rights to use, copy, modify, merge,\n- * publish, distribute, sublicense, and/or sell copies of the Software,\n- * and to permit persons to whom the Software is furnished to do so,\n- * subject to the following conditions:\n- *\n- * The above copyright notice and this permission notice shall be\n- * included in all copies or substantial portions of the Software.\n- *\n- * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n- * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n- * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n- * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n- */\n-\n-#include \"triton/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.h\"\n-#include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n-#include \"mlir/IR/Operation.h\"\n-#include \"mlir/Target/LLVMIR/ModuleTranslation.h\"\n-#include \"triton/Dialect/NVGPU/IR/Dialect.h\"\n-\n-#include \"llvm/IR/IRBuilder.h\"\n-#include \"llvm/IR/InlineAsm.h\"\n-#include \"llvm/IR/IntrinsicsNVPTX.h\"\n-\n-using namespace mlir;\n-using namespace mlir::LLVM;\n-\n-namespace {\n-static llvm::FunctionCallee\n-getExternalFuncOP(llvm::Module *module, llvm::StringRef funcName,\n-                  llvm::Type *retTy, ArrayRef<llvm::Type *> argTys = {}) {\n-  return module->getOrInsertFunction(\n-      funcName, llvm::FunctionType::get(retTy, argTys, false),\n-      llvm::AttributeList{});\n-}\n-\n-llvm::Value *createExternalCall(llvm::IRBuilderBase &builder,\n-                                llvm::StringRef funcName,\n-                                ArrayRef<llvm::Value *> args = {},\n-                                ArrayRef<llvm::Type *> tys = {}) {\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = module->getFunction(funcName);\n-\n-  if (func == nullptr) {\n-    llvm::SmallVector<llvm::Type *> argTys;\n-    for (auto *arg : args) {\n-      argTys.push_back(arg->getType());\n-    }\n-\n-    llvm::Type *retTy;\n-    if (tys.empty())\n-      retTy = builder.getVoidTy();\n-    else\n-      retTy = tys[0];\n-\n-    func = dyn_cast<llvm::Function>(\n-        getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  }\n-\n-  return builder.CreateCall(func, args);\n-}\n-\n-void createMBarrierArrive(llvm::IRBuilderBase &builder,\n-                          mlir::triton::nvgpu::MBarriveType arriveType,\n-                          llvm::Value *barrier, llvm::Value *pred,\n-                          llvm::Value *ctaId, uint32_t txCount) {\n-  auto *module = builder.GetInsertBlock()->getModule();\n-\n-  llvm::SmallVector<llvm::Type *> argTys;\n-  argTys.push_back(barrier->getType());\n-  llvm::Type *retTy = builder.getVoidTy();\n-\n-  if (arriveType == mlir::triton::nvgpu::MBarriveType::normal) {\n-    argTys.push_back(pred->getType());\n-    auto *func = dyn_cast<llvm::Function>(\n-        getExternalFuncOP(module, \"__nv_mbarrier_arrive_normal\", retTy, argTys)\n-            .getCallee());\n-    builder.CreateCall(func, {barrier, pred});\n-  } else if (arriveType == mlir::triton::nvgpu::MBarriveType::cp_async) {\n-    argTys.push_back(pred->getType());\n-    auto *func = dyn_cast<llvm::Function>(\n-        getExternalFuncOP(module, \"__nv_mbarrier_arrive_cp_async\", retTy,\n-                          argTys)\n-            .getCallee());\n-    builder.CreateCall(func, {barrier, pred});\n-  } else if (arriveType == mlir::triton::nvgpu::MBarriveType::expect_tx) {\n-    assert(txCount > 0 && \"txCount should be valid\");\n-    argTys.push_back(builder.getInt32Ty());\n-    argTys.push_back(pred->getType());\n-\n-    auto *func = dyn_cast<llvm::Function>(\n-        getExternalFuncOP(module, \"__nv_mbarrier_arrive_expect_tx\", retTy,\n-                          argTys)\n-            .getCallee());\n-    builder.CreateCall(func, {barrier, builder.getInt32(txCount), pred});\n-  } else if (arriveType == mlir::triton::nvgpu::MBarriveType::remote) {\n-    assert(ctaId && \"ctaId should have a valid value\");\n-    argTys.push_back(ctaId->getType());\n-    argTys.push_back(pred->getType());\n-\n-    auto *func = dyn_cast<llvm::Function>(\n-        getExternalFuncOP(module, \"__nv_mbarrier_arrive_remote\", retTy, argTys)\n-            .getCallee());\n-    builder.CreateCall(func, {barrier, ctaId, pred});\n-  }\n-\n-  return;\n-}\n-\n-llvm::Value *createWGMMADesc(llvm::IRBuilderBase &builder, llvm::Value *buffer,\n-                             mlir::triton::nvgpu::WGMMADescMode mode,\n-                             llvm::Value *height) {\n-  llvm::SmallVector<llvm::Type *> argTys;\n-  argTys.push_back(buffer->getType());\n-  argTys.push_back(builder.getInt32Ty());\n-  argTys.push_back(height->getType());\n-  llvm::Type *retTy = builder.getInt64Ty();\n-\n-  llvm::Value *mode_ = builder.getInt32((uint32_t)mode);\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, \"__nv_get_wgmma_desc\", retTy, argTys)\n-          .getCallee());\n-  return builder.CreateCall(func, {buffer, mode_, height});\n-}\n-\n-static std::string getTMALoadFuncName(bool tiled, bool mcast,\n-                                      uint32_t dimSize) {\n-  std::string funcName;\n-  llvm::raw_string_ostream os(funcName);\n-  os << \"__nv_tma_load\";\n-  if (tiled)\n-    os << \"_tiled\";\n-  else\n-    os << \"_im2col\";\n-\n-  if (mcast)\n-    os << \"_mcast\";\n-\n-  os << \"_\" << dimSize << \"d\";\n-\n-  return funcName;\n-}\n-\n-void createTMALoadTiled(llvm::IRBuilderBase &builder, llvm::Value *dst,\n-                        llvm::Value *mbarrier, llvm::Value *tmaDesc,\n-                        llvm::Value *l2Desc, llvm::Value *mcastMask,\n-                        llvm::Value *pred,\n-                        llvm::SmallVector<llvm::Value *> coords) {\n-  assert(coords.size() >= 2 && coords.size() <= 5 && \"invalid coords.size()\");\n-  auto funcName = getTMALoadFuncName(true, mcastMask != 0, coords.size());\n-  llvm::Type *retTy = builder.getVoidTy();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(tmaDesc->getType());\n-  args.push_back(tmaDesc);\n-\n-  argTys.push_back(dst->getType());\n-  args.push_back(dst);\n-\n-  argTys.push_back(mbarrier->getType());\n-  args.push_back(mbarrier);\n-  for (auto *c : coords) {\n-    argTys.push_back(c->getType());\n-    args.push_back(c);\n-  }\n-  argTys.push_back(l2Desc->getType());\n-  args.push_back(l2Desc);\n-\n-  if (mcastMask != nullptr) {\n-    argTys.push_back(builder.getInt16Ty());\n-    args.push_back(mcastMask);\n-  }\n-\n-  argTys.push_back(pred->getType());\n-  args.push_back(pred);\n-\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  builder.CreateCall(func, args);\n-\n-  return;\n-}\n-\n-void createTMALoadIm2col(llvm::IRBuilderBase &builder, llvm::Value *dst,\n-                         llvm::Value *mbarrier, llvm::Value *tmaDesc,\n-                         llvm::Value *l2Desc, uint16_t mcastMask,\n-                         llvm::Value *im2colOffsets, llvm::Value *pred,\n-                         llvm::SmallVector<llvm::Value *> coords) {\n-  assert(coords.size() >= 3 && coords.size() <= 5 &&\n-         \"invalid coords.size() for im2col\");\n-  auto funcName = getTMALoadFuncName(false, mcastMask != 0, coords.size());\n-  llvm::Type *retTy = builder.getVoidTy();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(tmaDesc->getType());\n-  args.push_back(tmaDesc);\n-\n-  argTys.push_back(dst->getType());\n-  args.push_back(dst);\n-\n-  argTys.push_back(mbarrier->getType());\n-  args.push_back(mbarrier);\n-  for (auto *c : coords) {\n-    argTys.push_back(c->getType());\n-    args.push_back(c);\n-  }\n-\n-  {\n-    auto offsetsType = dyn_cast<llvm::StructType>(im2colOffsets->getType());\n-    auto subTypes = offsetsType->elements();\n-    assert((coords.size() - subTypes.size() == 2) && \"wrong imcolOffsets\");\n-    unsigned idx = 0;\n-    for (auto subType : subTypes) {\n-      argTys.push_back(subType);\n-      args.push_back(builder.CreateExtractValue(im2colOffsets, {idx}));\n-      idx++;\n-    }\n-  }\n-\n-  argTys.push_back(l2Desc->getType());\n-  args.push_back(l2Desc);\n-\n-  if (mcastMask != 0) {\n-    argTys.push_back(builder.getInt16Ty());\n-    llvm::Value *mcastMask_ = builder.getInt16(mcastMask);\n-    args.push_back(mcastMask_);\n-  }\n-\n-  argTys.push_back(pred->getType());\n-  args.push_back(pred);\n-\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  builder.CreateCall(func, args);\n-\n-  return;\n-}\n-\n-llvm::Value *createWGMMA(llvm::IRBuilderBase &builder, uint32_t m, uint32_t n,\n-                         uint32_t k, mlir::triton::nvgpu::WGMMAEltType eltTypeC,\n-                         mlir::triton::nvgpu::WGMMAEltType eltTypeA,\n-                         mlir::triton::nvgpu::WGMMAEltType eltTypeB,\n-                         mlir::triton::nvgpu::WGMMALayout layoutA,\n-                         mlir::triton::nvgpu::WGMMALayout layoutB,\n-                         llvm::Value *opA, llvm::Value *opB, llvm::Value *opC) {\n-  // Simplify enum namespace\n-  using namespace mlir::triton::nvgpu;\n-\n-  // Register checks\n-  auto typeA = opA->getType();\n-  auto typeB = opB->getType();\n-  auto typeC = opC->getType();\n-  auto structTypeA = dyn_cast<llvm::StructType>(typeA);\n-  auto structTypeB = dyn_cast<llvm::StructType>(typeB);\n-  auto structTypeC = dyn_cast<llvm::StructType>(typeC);\n-  assert(!structTypeB && \"Operand B can not be registers\");\n-  assert(structTypeC && \"Operand C must be registers\");\n-\n-  // Element type, MNK shape and transposing support check\n-  // Reference:\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions-wgmma-mma\n-  bool transA = layoutA == WGMMALayout::col;\n-  bool transB = layoutB == WGMMALayout::row;\n-  bool supported = false, needTransArgs = false, floatTypeWGMMA = false;\n-  assert(m % 8 == 0 && n % 8 == 0 && k % 8 == 0);\n-  // Below instructions do support transposing, must pass `trans` arguments\n-  supported |=\n-      (eltTypeA == WGMMAEltType::f16) && (eltTypeB == WGMMAEltType::f16) &&\n-      (eltTypeC == WGMMAEltType::f16 || eltTypeC == WGMMAEltType::f32) &&\n-      (m == 64 && 8 <= n && n <= 256 && k == 16);\n-  supported |= (eltTypeA == WGMMAEltType::bf16) &&\n-               (eltTypeB == WGMMAEltType::bf16) &&\n-               (eltTypeC == WGMMAEltType::f32) &&\n-               (m == 64 && 8 <= n && n <= 256 && k == 16);\n-  needTransArgs = supported;\n-  floatTypeWGMMA = supported;\n-  // Below instructions do not support transposing\n-  if (!supported && !transA && !transB) {\n-    supported |= (eltTypeA == WGMMAEltType::tf32) &&\n-                 (eltTypeB == WGMMAEltType::tf32) &&\n-                 (eltTypeC == WGMMAEltType::f32) &&\n-                 (m == 64 && 8 <= n && n <= 256 && k == 8);\n-    supported |=\n-        (eltTypeA == WGMMAEltType::e4m3 || eltTypeA == WGMMAEltType::e5m2) &&\n-        (eltTypeB == WGMMAEltType::e4m3 || eltTypeB == WGMMAEltType::e5m2) &&\n-        (eltTypeC == WGMMAEltType::f16 || eltTypeC == WGMMAEltType::f32) &&\n-        (m == 64 && 8 <= n && n <= 256 && k == 32);\n-    floatTypeWGMMA = supported;\n-    // Below instructions are integer-based\n-    supported |= (eltTypeA == WGMMAEltType::s8) &&\n-                 (eltTypeB == WGMMAEltType::s8) &&\n-                 (eltTypeC == WGMMAEltType::s32) &&\n-                 (m == 64 && 8 <= n && n <= 224 && k == 32);\n-  }\n-  assert(supported && \"WGMMA type or shape is not supported\");\n-\n-  // Build PTX asm\n-  std::string ptxAsm;\n-  std::string constraints;\n-  llvm::raw_string_ostream asmOs(ptxAsm);\n-  llvm::raw_string_ostream conOs(constraints);\n-  llvm::SmallVector<llvm::Type *> argTypes;\n-  llvm::SmallVector<llvm::Value *> args;\n-\n-  // MMA instruction\n-  asmOs << \"wgmma.mma_async.sync.aligned\"\n-        << \".m\" << m << \"n\" << n << \"k\" << k << \".\" << stringifyEnum(eltTypeC)\n-        << \".\" << stringifyEnum(eltTypeA) << \".\" << stringifyEnum(eltTypeB)\n-        << \" \";\n-\n-  // Operands\n-  uint32_t asmOpIdx = 0;\n-\n-  // Operand C\n-  uint32_t numCRegs = structTypeC->getStructNumElements();\n-  asmOs << \"{\";\n-  for (uint32_t i = 0; i < numCRegs; ++i) {\n-    argTypes.push_back(structTypeC->getElementType(i));\n-    args.push_back(builder.CreateExtractValue(opC, {i}));\n-    asmOs << \"$\" << asmOpIdx++ << (i == numCRegs - 1 ? \"\" : \",\");\n-    // LLVM does not support `+` semantic, we must repeat the arguments for both\n-    // input and outputs\n-    if (structTypeC->getElementType(i)->isFloatTy())\n-      conOs << \"=f,\";\n-    else\n-      conOs << \"=r,\";\n-  }\n-  asmOs << \"}, \";\n-  for (uint32_t i = asmOpIdx - numCRegs; i < asmOpIdx; ++i)\n-    conOs << i << \",\";\n-  // Note that LLVM will not skip the indexed repeating placeholders\n-  asmOpIdx += numCRegs;\n-\n-  // Operand A\n-  if (structTypeA) {\n-    uint32_t numARegs = m * k / 128;\n-    assert(numARegs == structTypeA->getNumElements());\n-    asmOs << \"{\";\n-    for (uint32_t i = 0; i < numARegs; ++i) {\n-      argTypes.push_back(structTypeA->getElementType(i));\n-      args.push_back(builder.CreateExtractValue(opA, {i}));\n-      asmOs << \"$\" << asmOpIdx++ << (i == numARegs - 1 ? \"\" : \",\");\n-      conOs << \"f,\";\n-    }\n-    asmOs << \"}, \";\n-  } else {\n-    argTypes.push_back(typeA);\n-    args.push_back(opA);\n-    asmOs << \"$\" << asmOpIdx++ << \", \";\n-    conOs << \"l,\";\n-  }\n-\n-  // Operand B (must be `desc`)\n-  argTypes.push_back(typeB);\n-  args.push_back(opB);\n-  asmOs << \"$\" << asmOpIdx++ << \", \";\n-  conOs << \"l\";\n-\n-  // `scale-d` is 1 by default\n-  asmOs << \"1\";\n-\n-  // `imm-scale-a`, and `imm-scale-b` are 1 by default only for float-based\n-  // WGMMA\n-  if (floatTypeWGMMA)\n-    asmOs << \", 1, 1\";\n-\n-  // Push `trans-a` and `trans-b` args if needed (determined as constant)\n-  if (needTransArgs)\n-    asmOs << \", \" << transA << \", \" << transB;\n-  asmOs << \";\";\n-\n-  // Finally build `llvm::InlineAsm` and call it\n-  auto inlineAsm = llvm::InlineAsm::get(\n-      llvm::FunctionType::get(structTypeC, argTypes, false), ptxAsm,\n-      constraints, true);\n-  return builder.CreateCall(inlineAsm, args);\n-}\n-\n-void createWGMMAFence(llvm::IRBuilderBase &builder) {\n-  std::string asmStr = \"wgmma.fence.sync.aligned;\";\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n-                           asmStr, \"\", /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, {});\n-}\n-\n-void createWGMMACommitGroup(llvm::IRBuilderBase &builder) {\n-  std::string asmStr = \"wgmma.commit_group.sync.aligned;\";\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n-                           asmStr, \"\", /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, {});\n-}\n-\n-void createWGMMAWaitGroup(llvm::IRBuilderBase &builder, uint32_t pendings) {\n-  std::string asmStr = (llvm::Twine(\"wgmma.wait_group.sync.aligned \") +\n-                        llvm::Twine(pendings) + \";\")\n-                           .str();\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n-                           asmStr, \"\", /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, {});\n-}\n-\n-llvm::Value *createLoadSharedCluster(llvm::IRBuilderBase &builder,\n-                                     llvm::Value *addr, llvm::Value *ctaId,\n-                                     unsigned bitwidth, unsigned vec) {\n-  assert(\n-      (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n-      \"invalid bitwidth\");\n-  assert((vec == 1 || vec == 2 || vec == 4) && \"invalid vec size\");\n-\n-  // PTX string\n-  std::string ptxStr;\n-  llvm::raw_string_ostream asmOs(ptxStr);\n-  unsigned addrArgId = vec, ctaIdArgId = vec + 1;\n-  asmOs << \"{\\n\\t\"\n-        << \".reg .u32 remoteAddr;\\n\\t\"\n-        << \"mapa.shared::cluster.u32 remoteAddr, $\" << addrArgId << \", $\"\n-        << ctaIdArgId << \";\\n\\t\";\n-  asmOs << \"ld.shared::cluster\";\n-  if (vec > 1)\n-    asmOs << \".v\" << vec;\n-  asmOs << \".u\" << bitwidth << \" \";\n-  if (vec == 1)\n-    asmOs << \"$0\";\n-  else if (vec == 2)\n-    asmOs << \"{$0, $1}\";\n-  else\n-    asmOs << \"{$0, $1, $2, $3}\";\n-  asmOs << \", [remoteAddr];\\n\\t\"\n-        << \"}\\n\";\n-\n-  // Constraints\n-  std::string constraints;\n-  llvm::raw_string_ostream conOs(constraints);\n-  std::string c = bitwidth == 16 ? \"h\" : (bitwidth == 32 ? \"r\" : \"l\");\n-  for (unsigned i = 0; i < vec; ++i)\n-    conOs << \"=\" << c << \",\";\n-  conOs << \"r,r\";\n-\n-  // Arguments\n-  llvm::SmallVector<llvm::Type *> argTypes;\n-  llvm::SmallVector<llvm::Value *> args;\n-  argTypes.push_back(addr->getType());\n-  args.push_back(addr);\n-  argTypes.push_back(ctaId->getType());\n-  args.push_back(ctaId);\n-\n-  // Return type\n-  llvm::Type *retTy = builder.getIntNTy(bitwidth);\n-  llvm::SmallVector<llvm::Type *> retTys(vec, retTy);\n-  if (vec > 1)\n-    retTy = llvm::StructType::get(builder.getContext(), retTys);\n-\n-  // Call InlineAsm\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(retTy, argTypes, false),\n-                           ptxStr, constraints, /*hasSideEffect*/ false);\n-  return builder.CreateCall(iasm, args);\n-}\n-\n-void createStoreSharedCluster(llvm::IRBuilderBase &builder, llvm::Value *addr,\n-                              llvm::Value *ctaId,\n-                              llvm::SmallVector<llvm::Value *> values,\n-                              llvm::Value *pred, unsigned bitwidth,\n-                              unsigned vec) {\n-  assert(\n-      (bitwidth == 8 || bitwidth == 16 || bitwidth == 32 || bitwidth == 64) &&\n-      \"invalid bitwidth\");\n-  assert((vec == 1 || vec == 2 || vec == 4) && vec == values.size() &&\n-         \"invalid vec size\");\n-\n-  // PTX string\n-  std::string ptxStr;\n-  llvm::raw_string_ostream asmOs(ptxStr);\n-  asmOs << \"{\\n\\t\"\n-        << \".reg .u32 remoteAddr;\\n\\t\"\n-        << \"mapa.shared::cluster.u32 remoteAddr, $0, $1;\\n\\t\"\n-        << \".reg .pred p;\\n\\t\"\n-        << \"mov.pred p, $2;\\n\\t\";\n-  asmOs << \"@p st.shared::cluster\";\n-  if (vec > 1)\n-    asmOs << \".v\" << vec;\n-  asmOs << \".u\" << bitwidth << \" [remoteAddr], \";\n-  if (vec == 1)\n-    asmOs << \"$3\";\n-  else if (vec == 2)\n-    asmOs << \"{$3, $4}\";\n-  else if (vec == 4)\n-    asmOs << \"{$3, $4, $5, $6}\";\n-  asmOs << \";\\n\\t\"\n-        << \"}\\n\";\n-\n-  // Constraints\n-  std::string constraints;\n-  llvm::raw_string_ostream conOs(constraints);\n-  std::string c = bitwidth == 16 ? \"h\" : (bitwidth == 32 ? \"r\" : \"l\");\n-  conOs << \"r,r,b\";\n-  for (unsigned i = 0; i < vec; ++i)\n-    conOs << \",\" << c;\n-\n-  // Arguments\n-  llvm::SmallVector<llvm::Type *> argTypes;\n-  llvm::SmallVector<llvm::Value *> args;\n-  argTypes.push_back(addr->getType());\n-  args.push_back(addr);\n-  argTypes.push_back(ctaId->getType());\n-  args.push_back(ctaId);\n-  argTypes.push_back(pred->getType());\n-  args.push_back(pred);\n-  for (llvm::Value *value : values) {\n-    argTypes.push_back(value->getType());\n-    args.push_back(value);\n-  }\n-\n-  // Call InlineAsm\n-  llvm::InlineAsm *iasm = llvm::InlineAsm::get(\n-      llvm::FunctionType::get(builder.getVoidTy(), argTypes, false), ptxStr,\n-      constraints, /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, args);\n-}\n-\n-static std::string getTMAStoreFuncName(bool tiled, uint32_t dimSize) {\n-  std::string funcName;\n-  llvm::raw_string_ostream os(funcName);\n-  os << \"__nv_tma_store\";\n-  if (tiled)\n-    os << \"_tiled\";\n-  else\n-    os << \"_im2col\";\n-\n-  os << \"_\" << dimSize << \"d\";\n-\n-  return funcName;\n-}\n-\n-void createTMAStoreTiled(llvm::IRBuilderBase &builder, llvm::Value *tmaDesc,\n-                         llvm::Value *src, llvm::Value *pred,\n-                         llvm::SmallVector<llvm::Value *> coords) {\n-  assert(coords.size() >= 2 && coords.size() <= 5 && \"invalid coords.size()\");\n-  auto funcName = getTMAStoreFuncName(true, coords.size());\n-  llvm::Type *retTy = builder.getVoidTy();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(tmaDesc->getType());\n-  args.push_back(tmaDesc);\n-\n-  argTys.push_back(src->getType());\n-  args.push_back(src);\n-\n-  for (auto *c : coords) {\n-    argTys.push_back(c->getType());\n-    args.push_back(c);\n-  }\n-  argTys.push_back(pred->getType());\n-  args.push_back(pred);\n-\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  builder.CreateCall(func, args);\n-\n-  return;\n-}\n-\n-void createStoreMatrix(llvm::IRBuilderBase &builder, llvm::Value *addr,\n-                       llvm::SmallVector<llvm::Value *> datas) {\n-  auto size = datas.size();\n-  assert((size == 1 || size == 2 || size == 4) &&\n-         \"not support size with stmatrix\");\n-\n-  std::string funcName;\n-  llvm::raw_string_ostream os(funcName);\n-  os << \"__nv_stmatrix_x\" << size;\n-\n-  llvm::Type *retTy = builder.getVoidTy();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(addr->getType());\n-  args.push_back(addr);\n-\n-  for (size_t i = 0; i < datas.size(); ++i) {\n-    argTys.push_back(datas[i]->getType());\n-    args.push_back(datas[i]);\n-  }\n-\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  builder.CreateCall(func, args);\n-}\n-\n-llvm::Value *createOffsetOfStmatrixV4(llvm::IRBuilderBase &builder,\n-                                      llvm::Value *threadId,\n-                                      llvm::Value *rowOfWarp,\n-                                      llvm::Value *elemIdx,\n-                                      uint32_t leadingDimOffset,\n-                                      uint32_t rowStride, bool swizzleEnabled) {\n-  if (swizzleEnabled) {\n-    assert((rowStride == 16 || rowStride == 32 || rowStride == 64) &&\n-           \"wrong rowString for swizzleEnabled\");\n-  }\n-  llvm::Type *retTy = builder.getInt32Ty();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(threadId->getType());\n-  args.push_back(threadId);\n-\n-  argTys.push_back(rowOfWarp->getType());\n-  args.push_back(rowOfWarp);\n-\n-  argTys.push_back(elemIdx->getType());\n-  args.push_back(elemIdx);\n-\n-  argTys.push_back(builder.getInt32Ty());\n-  args.push_back(builder.getInt32(leadingDimOffset));\n-\n-  argTys.push_back(builder.getInt32Ty());\n-  args.push_back(builder.getInt32(rowStride));\n-\n-  std::string funcName(\"__nv_offset_of_stmatrix_v4\");\n-  if (!swizzleEnabled)\n-    funcName = \"__nv_offset_of_stmatrix_v4_no_swizzle\";\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  return builder.CreateCall(func, args);\n-}\n-\n-llvm::Value *createOffsetOfSts64(llvm::IRBuilderBase &builder,\n-                                 llvm::Value *threadId, llvm::Value *rowOfWarp,\n-                                 llvm::Value *elemIdx,\n-                                 uint32_t leadingDimOffset, uint32_t rowStride,\n-                                 bool swizzleEnabled) {\n-  if (swizzleEnabled) {\n-    assert((rowStride == 32 || rowStride == 64 || rowStride == 128) &&\n-           \"wrong rowString for swizzleEnabled\");\n-  }\n-  llvm::Type *retTy = builder.getInt32Ty();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-\n-  argTys.push_back(threadId->getType());\n-  args.push_back(threadId);\n-\n-  argTys.push_back(rowOfWarp->getType());\n-  args.push_back(rowOfWarp);\n-\n-  argTys.push_back(elemIdx->getType());\n-  args.push_back(elemIdx);\n-\n-  argTys.push_back(builder.getInt32Ty());\n-  args.push_back(builder.getInt32(leadingDimOffset));\n-\n-  argTys.push_back(builder.getInt32Ty());\n-  args.push_back(builder.getInt32(rowStride));\n-\n-  std::string funcName(\"__nv_offset_of_sts64\");\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  return builder.CreateCall(func, args);\n-}\n-\n-void createSts64(llvm::IRBuilderBase &builder, llvm::Value *offset,\n-                 llvm::Value *d0, llvm::Value *d1) {\n-  std::string funcName(\"__nv_sts64\");\n-\n-  llvm::Type *retTy = builder.getVoidTy();\n-  llvm::SmallVector<llvm::Value *> args;\n-  llvm::SmallVector<llvm::Type *> argTys;\n-  auto i32Ty = builder.getInt32Ty();\n-  argTys.push_back(i32Ty);\n-  args.push_back(offset);\n-\n-  argTys.push_back(i32Ty);\n-  args.push_back(builder.CreateBitCast(d0, i32Ty));\n-\n-  argTys.push_back(i32Ty);\n-  args.push_back(builder.CreateBitCast(d1, i32Ty));\n-\n-  auto *module = builder.GetInsertBlock()->getModule();\n-  auto *func = dyn_cast<llvm::Function>(\n-      getExternalFuncOP(module, funcName, retTy, argTys).getCallee());\n-  builder.CreateCall(func, args);\n-\n-  return;\n-}\n-\n-static llvm::Value *getSRegValue(llvm::IRBuilderBase &builder,\n-                                 llvm::StringRef name) {\n-  std::string ptxStr;\n-  llvm::raw_string_ostream asmOs(ptxStr);\n-  asmOs << \"mov.u32 $0, \" << name << \";\";\n-  std::string constraints = \"=r\";\n-  llvm::InlineAsm *inlineAsm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getInt32Ty(), false),\n-                           ptxStr, constraints, /*hasSideEffect*/ false);\n-  return builder.CreateCall(inlineAsm);\n-}\n-\n-static llvm::Value *createClusterId(llvm::IRBuilderBase &builder) {\n-  llvm::Value *x = getSRegValue(builder, \"%cluster_ctaid.x\");\n-  llvm::Value *y = getSRegValue(builder, \"%cluster_ctaid.y\");\n-  llvm::Value *z = getSRegValue(builder, \"%cluster_ctaid.z\");\n-  llvm::Value *nx = getSRegValue(builder, \"%cluster_nctaid.x\");\n-  llvm::Value *ny = getSRegValue(builder, \"%cluster_nctaid.y\");\n-  llvm::Value *clusterCTAId = builder.CreateAdd(\n-      x, builder.CreateMul(builder.CreateAdd(y, builder.CreateMul(z, ny)), nx));\n-  return clusterCTAId;\n-}\n-void createRegAlloc(llvm::IRBuilderBase &builder, const uint32_t regCount) {\n-  std::string ptxStr;\n-  llvm::raw_string_ostream asmOs(ptxStr);\n-  asmOs << \"setmaxnreg.inc.sync.aligned.u32 \" << regCount << \";\\n\";\n-  // Call InlineAsm\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n-                           ptxStr, \"\", /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, {});\n-}\n-\n-void createRegDealloc(llvm::IRBuilderBase &builder, const uint32_t regCount) {\n-  std::string ptxStr;\n-  llvm::raw_string_ostream asmOs(ptxStr);\n-  asmOs << \"setmaxnreg.dec.sync.aligned.u32 \" << regCount << \";\\n\";\n-  // Call InlineAsm\n-  llvm::InlineAsm *iasm =\n-      llvm::InlineAsm::get(llvm::FunctionType::get(builder.getVoidTy(), {}),\n-                           ptxStr, \"\", /*hasSideEffect*/ true);\n-  builder.CreateCall(iasm, {});\n-}\n-\n-class NVGPUDialectLLVMIRTranslationInterface\n-    : public LLVMTranslationDialectInterface {\n-public:\n-  using LLVMTranslationDialectInterface::LLVMTranslationDialectInterface;\n-\n-  /// Translates the given operation to LLVM IR using the provided IR builder\n-  /// and saving the state in `moduleTranslation`.\n-  LogicalResult\n-  convertOperation(Operation *op, llvm::IRBuilderBase &builder,\n-                   LLVM::ModuleTranslation &moduleTranslation) const final {\n-    Operation &opInst = *op;\n-#include \"triton/Dialect/NVGPU/IR/OpsConversions.inc\"\n-\n-    return failure();\n-  }\n-};\n-} // namespace\n-\n-void mlir::registerNVGPUDialectTranslation(DialectRegistry &registry) {\n-  registry.insert<mlir::triton::nvgpu::NVGPUDialect>();\n-  registry.addExtension(\n-      +[](MLIRContext *ctx, mlir::triton::nvgpu::NVGPUDialect *dialect) {\n-        dialect->addInterfaces<NVGPUDialectLLVMIRTranslationInterface>();\n-      });\n-}\n-\n-void mlir::registerNVGPUDialectTranslation(MLIRContext &context) {\n-  DialectRegistry registry;\n-  registerNVGPUDialectTranslation(registry);\n-  context.appendDialectRegistry(registry);\n-}"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1606,10 +1606,10 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op->getLoc(), newType, extract_slice.getSource());\n     rewriter.replaceOpWithNewOp<triton::gpu::ExtractSliceOp>(\n-        op, resType, newArg.getResult(), extract_slice.offsets(),\n-        extract_slice.sizes(), extract_slice.strides(),\n-        extract_slice.static_offsets(), extract_slice.static_sizes(),\n-        extract_slice.static_strides());\n+        op, resType, newArg.getResult(), extract_slice.getOffsets(),\n+        extract_slice.getSizes(), extract_slice.getStrides(),\n+        extract_slice.getStaticOffsets(), extract_slice.getStaticSizes(),\n+        extract_slice.getStaticStrides());\n     return mlir::success();\n   }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 10, "deletions": 3, "changes": 13, "file_content_changes": "@@ -17,6 +17,7 @@\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n@@ -47,6 +48,12 @@ class TritonGPUReorderInstructionsPass\n     // Sink conversions into loops when they will increase\n     // register pressure\n     DenseMap<Operation *, Operation *> opToMove;\n+    auto moveAfter = [](Operation *lhs, Operation *rhs) {\n+      auto lhsId = getWSRoleId(lhs);\n+      auto rhsId = getWSRoleId(rhs);\n+      if (lhsId == rhsId)\n+        lhs->moveAfter(rhs);\n+    };\n     m.walk([&](triton::gpu::ConvertLayoutOp op) {\n       if (!willIncreaseRegisterPressure(op))\n         return;\n@@ -70,15 +77,15 @@ class TritonGPUReorderInstructionsPass\n       Operation *argOp = op.getOperand().getDefiningOp();\n       if (!argOp)\n         return;\n-      op->moveAfter(argOp);\n+      moveAfter(op, argOp);\n     });\n     // Move transpositions just after their definition\n     opToMove.clear();\n     m.walk([&](triton::TransOp op) {\n       Operation *argOp = op.getOperand().getDefiningOp();\n       if (!argOp)\n         return;\n-      op->moveAfter(argOp);\n+      moveAfter(op, argOp);\n     });\n     // Move `dot` operand so that conversions to opIdx=1 happens after\n     // conversions to opIdx=0\n@@ -104,7 +111,7 @@ class TritonGPUReorderInstructionsPass\n       // after the conversion to OpIdx=0.\n       if (!dom.dominates(op.getOperation(), AOp.getOperation()))\n         return;\n-      op->moveAfter(AOp);\n+      moveAfter(op, AOp);\n     });\n     return;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "@@ -731,4 +731,28 @@ Value linearize(OpBuilder &b, Location loc, ArrayRef<Value> multiDim,\n   return linear;\n }\n \n+std::optional<int> getWSAgentId(Operation *op) {\n+  int prevAgentId = -1;\n+  if (auto attr = op->getAttrOfType<DenseIntElementsAttr>(\"async_agent\")) {\n+    for (auto agentId : attr.getValues<int>()) {\n+      assert(prevAgentId == -1 && \"support at most one agent id\");\n+      prevAgentId = agentId;\n+    }\n+  }\n+  if (prevAgentId == -1)\n+    return std::nullopt;\n+  return prevAgentId;\n+}\n+\n+std::optional<int> getWSRoleId(Operation *op) {\n+  if (!op->hasAttr(\"agent.mutex_role\"))\n+    return std::nullopt;\n+  return op->getAttrOfType<IntegerAttr>(\"agent.mutex_role\").getInt();\n+}\n+\n+void setRoleId(Operation *op, int roleId) {\n+  auto attr = IntegerAttr::get(IntegerType::get(op->getContext(), 32), roleId);\n+  op->setAttr(\"agent.mutex_role\", attr);\n+}\n+\n } // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -6,6 +6,7 @@ add_mlir_dialect_library(TritonNvidiaGPUTransforms\n   WSPipeline.cpp\n   WSMutex.cpp\n   WSMaterialization.cpp\n+  WSFixupMissingAttrs.cpp\n   FenceInsertion.cpp\n   RewriteTensorPointer.cpp\n   Utility.cpp"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/RewriteTensorPointer.cpp", "status": "modified", "additions": 90, "deletions": 3, "changes": 93, "file_content_changes": "@@ -366,6 +366,22 @@ class TritonGPURewriteTensorPointerPass\n       : computeCapability(computeCapability) {}\n \n   static bool needRewrite(Operation *op, const DenseSet<Value> &valueToRemove) {\n+    if (auto ifOp = dyn_cast<scf::IfOp>(op)) {\n+      if (op->getNumResults() == 0)\n+        return false;\n+      Operation *thenYield = ifOp.thenYield().getOperation();\n+      if (!ifOp.getElseRegion().empty()) {\n+        Operation *elseYield = ifOp.elseYield().getOperation();\n+        for (unsigned i = 0; i < thenYield->getNumOperands(); ++i) {\n+          bool thenNeedRewrite = valueToRemove.count(thenYield->getOperand(i));\n+          bool elseNeedRewrite = valueToRemove.count(elseYield->getOperand(i));\n+          assert(!(thenNeedRewrite ^ elseNeedRewrite) &&\n+                 \"For IfOp, operand(i) of thenYield and operand(i) of \"\n+                 \"elseYield should be either all need rewrite or all not\");\n+        }\n+      }\n+      op = thenYield;\n+    }\n     return std::any_of(op->getOperands().begin(), op->getOperands().end(),\n                        [&valueToRemove](Value operand) {\n                          return tt::isTensorPointerType(operand.getType()) &&\n@@ -615,6 +631,77 @@ class TritonGPURewriteTensorPointerPass\n     return nullptr;\n   }\n \n+  Operation *rewriteIfOp(OpBuilder &builder, scf::IfOp op,\n+                         std::stack<Operation *> &eraser,\n+                         DenseSet<Value> &valueToRemove) {\n+    auto thenYieldOp = op.thenYield();\n+    assert(op.getNumResults() == thenYieldOp.getNumOperands());\n+    SmallVector<Value> results = thenYieldOp.getOperands();\n+\n+    // get new result types\n+    SmallVector<Type> newRetTypes;\n+    for (unsigned i = 0; i < results.size(); ++i) {\n+      if (!tt::isTensorPointerType(results[i].getType()) ||\n+          !valueToRemove.count(results[i])) {\n+        newRetTypes.push_back(results[i].getType());\n+        continue;\n+      }\n+      auto makeTensorPtrOp = getMakeTensorPtrOp(results[i]);\n+      assert(rewritedInfo.count(makeTensorPtrOp.getResult()));\n+      auto info = rewritedInfo[makeTensorPtrOp.getResult()];\n+      for (unsigned j = 0; j < info.length(); ++j) {\n+        newRetTypes.push_back(builder.getI64Type());\n+      }\n+    }\n+\n+    // create and clone new IfOp\n+    bool hasElse = !op.getElseRegion().empty();\n+    scf::IfOp newOp = builder.create<scf::IfOp>(op.getLoc(), newRetTypes,\n+                                                op.getCondition(), hasElse);\n+    IRMapping mapping;\n+    for (unsigned i = 0; i < op->getNumOperands(); ++i) {\n+      mapping.map(op->getOperand(i), newOp->getOperand(i));\n+    }\n+    auto rematerialize = [&](Block *block) {\n+      for (Operation &opInIf : block->getOperations()) {\n+        auto newOp = builder.clone(opInIf, mapping);\n+      }\n+    };\n+    builder.setInsertionPointToStart(newOp.thenBlock());\n+    rematerialize(op.thenBlock());\n+    if (hasElse) {\n+      builder.setInsertionPointToStart(newOp.elseBlock());\n+      rematerialize(op.elseBlock());\n+    }\n+\n+    // supported nested ops\n+    for (auto &[k, v] : mapping.getValueMap())\n+      if (valueToRemove.find(k) != valueToRemove.end())\n+        valueToRemove.insert(v);\n+\n+    // update rewritedInfo\n+    unsigned oldResIdx = 0, newResIdx = 0;\n+    while (oldResIdx < results.size()) {\n+      if (!tt::isTensorPointerType(results[oldResIdx].getType()) ||\n+          !valueToRemove.count(results[oldResIdx])) {\n+        oldResIdx++;\n+        newResIdx++;\n+      } else {\n+        auto makeTensorPtrOp = getMakeTensorPtrOp(results[oldResIdx]);\n+        assert(rewritedInfo.count(makeTensorPtrOp.getResult()));\n+        auto info = rewritedInfo[makeTensorPtrOp.getResult()];\n+        for (unsigned j = 0; j < info.length(); ++j) {\n+          info.setOffset(j, newOp->getResult(newResIdx++));\n+        }\n+        rewritedInfo[op.getResult(oldResIdx)] = info;\n+        oldResIdx++;\n+      }\n+    }\n+\n+    eraser.push(op);\n+    return newOp;\n+  }\n+\n   Operation *rewriteOp(Operation *op, std::stack<Operation *> &eraser,\n                        DenseSet<Value> &valueToRemove) {\n     OpBuilder builder(op);\n@@ -630,8 +717,6 @@ class TritonGPURewriteTensorPointerPass\n       return rewriteAdvanceOp(builder, advanceOp, eraser, valueToRemove);\n     } else if (isa<tt::LoadOp>(op) || isa<tt::StoreOp>(op)) {\n       return rewriteLoadStoreOp(builder, op, eraser, valueToRemove);\n-    } else if (auto storeOp = dyn_cast<tt::StoreOp>(op)) {\n-      return rewriteLoadStoreOp(builder, op, eraser, valueToRemove);\n     } else if (op->getDialect()->getNamespace() == \"scf\" ||\n                op->getDialect()->getNamespace() == \"cf\") {\n       if (!needRewrite(op, valueToRemove))\n@@ -641,9 +726,11 @@ class TritonGPURewriteTensorPointerPass\n         return rewriteForOp(builder, forOp, eraser, valueToRemove);\n       } else if (auto yieldOp = dyn_cast<scf::YieldOp>(op)) {\n         return rewriteYieldOp(builder, yieldOp, eraser, valueToRemove);\n+      } else if (auto ifOp = dyn_cast<scf::IfOp>(op)) {\n+        return rewriteIfOp(builder, ifOp, eraser, valueToRemove);\n       } else {\n         llvm_unreachable(\"Currently we only support tensor pointer usages \"\n-                         \"inside a `scf::ForOp`, others such as `scf::IfOp`,\"\n+                         \"inside a `scf::ForOp` or `scf::IfOp`, others such as \"\n                          \"`scf::WhileOp`, `cf::BranchOp` or `cf::CondBranchOp` \"\n                          \"are not supported yet\");\n       }"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/Utility.cpp", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "file_content_changes": "@@ -39,6 +39,10 @@ namespace ttg = triton::gpu;\n \n namespace {\n \n+bool knownSafeToIgnoreRegion(Operation *op) {\n+  return isa<triton::ReduceOp>(op);\n+}\n+\n // Suppose the kernel has following structure:\n // ```\n // scf.for(...) {\n@@ -128,7 +132,8 @@ LogicalResult getDependentPointers(Value ptr, DenseSet<Value> &dependentSet,\n       return failure();\n     return getDependentPointers(ifOp.thenYield()->getOperand(idx), dependentSet,\n                                 processedSet);\n-  } else if (!definingOp->getNumRegions()) {\n+  } else if (!definingOp->getNumRegions() ||\n+             knownSafeToIgnoreRegion(definingOp)) {\n     for (Value operand : definingOp->getOperands())\n       if (failed(getDependentPointers(operand, dependentSet, processedSet)))\n         return failure();\n@@ -417,7 +422,8 @@ LogicalResult getDependentValues(Value val, DenseSet<Value> &depSet,\n         failed(tryInsertAndPropagate(ifOp.elseYield()->getOperand(idx))))\n       return failure();\n     return tryInsertAndPropagate(ifOp.thenYield()->getOperand(idx));\n-  } else if (!definingOp->getNumRegions()) {\n+  } else if (!definingOp->getNumRegions() ||\n+             knownSafeToIgnoreRegion(definingOp)) {\n     for (Value operand : definingOp->getOperands())\n       if (failed(tryInsertAndPropagate(operand)))\n         return failure();\n@@ -489,8 +495,6 @@ bool isWSCandidateLoad(Operation *op) {\n       cvtOp.getResult().getType().cast<RankedTensorType>().getEncoding();\n   if (!encoding || !encoding.dyn_cast<ttg::SharedEncodingAttr>())\n     return false;\n-  if (ttg::getNumCTAs(encoding) > 1)\n-    return false;\n \n   DenseSet<Value> depSet;\n   if (failed(getDependentValues(op->getResult(0), depSet)))"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSFixupMissingAttrs.cpp", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -0,0 +1,69 @@\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Utility.h\"\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonNvidiaGPU/Transforms/Passes.h.inc\"\n+\n+namespace mlir {\n+\n+namespace ttng = triton::nvidia_gpu;\n+\n+namespace {\n+\n+class TritonGPUWSFixupMissingAttrsPass\n+    : public TritonGPUWSFixupMissingAttrsBase<\n+          TritonGPUWSFixupMissingAttrsPass> {\n+public:\n+  TritonGPUWSFixupMissingAttrsPass() = default;\n+\n+  void runOnOperation() override {\n+    ModuleOp mod = getOperation();\n+    if (!ttng::TritonNvidiaGPUDialect::getWSSupportedAttr(mod))\n+      return;\n+    OpBuilder builder(mod);\n+    mod->walk([&](mlir::triton::FuncOp funcOp) {\n+      for (Operation &op : funcOp.getBody().front().getOperations()) {\n+        if (!isa<scf::IfOp>(&op))\n+          continue;\n+        auto agentIds = getAgentIds(&op);\n+        if (agentIds.size() != 1)\n+          continue;\n+        Block *roleIdBlock = nullptr;\n+        op.walk<WalkOrder::PreOrder>([&](Operation *subOp) {\n+          setAgentIds(subOp, agentIds);\n+          // Find the outter most common block that has roleId.\n+          // The below implementation assumes that:\n+          // - all lock/unlock ops are in the same block (denoted as B).\n+          // - there is always one scf.if op in the front of `B` which has\n+          //   role id attached.\n+          // The above assumptions are maintained by WSMutex pass currently.\n+          if (!roleIdBlock && isa<scf::IfOp>(subOp) && getWSRoleId(subOp))\n+            roleIdBlock = subOp->getBlock();\n+        });\n+        if (!roleIdBlock)\n+          continue;\n+        int roleId = 0;\n+        for (Operation &roleOp : roleIdBlock->getOperations()) {\n+          auto optionalRoleId = getWSRoleId(&roleOp);\n+          if (!optionalRoleId) {\n+            setRoleId(&roleOp, roleId);\n+          } else {\n+            roleId = *optionalRoleId;\n+          }\n+          roleOp.walk([&](Operation *subOp) { setRoleId(subOp, roleId); });\n+        }\n+      }\n+    });\n+  }\n+};\n+\n+} // namespace\n+\n+std::unique_ptr<Pass> createTritonNvidiaGPUWSFixupMissingAttrs() {\n+  return std::make_unique<TritonGPUWSFixupMissingAttrsPass>();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMaterialization.cpp", "status": "modified", "additions": 4, "deletions": 28, "changes": 32, "file_content_changes": "@@ -380,8 +380,10 @@ void materializeTokenOperations(Operation *parentOp, int numCTAs) {\n     Value bufferEmptyArray =\n         builder.create<ttng::AllocMBarrierOp>(tokenLoc, mBarriersTy, numCTAs);\n \n-    // Make sure that MBarriers are initialized in all CTAs\n-    if (numCTAs > 1) {\n+    if (numCTAs == 1) {\n+      builder.create<mlir::gpu::BarrierOp>(tokenLoc);\n+    } else {\n+      // Make sure that MBarriers are initialized in all CTAs\n       builder.create<triton::nvidia_gpu::ClusterArriveOp>(tokenLoc, false);\n       builder.create<triton::nvidia_gpu::ClusterWaitOp>(tokenLoc);\n     }\n@@ -706,32 +708,6 @@ struct WSMaterializationPass\n     materializeMutexOperations(mod);\n     tryRegisterRealloc(mod);\n \n-    mod->walk([](Operation *op) {\n-      bool hasTensor = 0;\n-      auto results = op->getResults();\n-      auto operands = op->getOperands();\n-      for (auto i : results) {\n-        if (isa<RankedTensorType>(i.getType())) {\n-          hasTensor = 1;\n-          break;\n-        }\n-      }\n-      if (!hasTensor) {\n-        for (auto i : operands) {\n-          if (isa<RankedTensorType>(i.getType())) {\n-            hasTensor = 1;\n-            break;\n-          }\n-        }\n-      }\n-\n-      if (!hasTensor && !isa<ttng::MBarrierWaitOp>(op) &&\n-          !isa<ttng::ExtractMBarrierOp>(op) &&\n-          !isa<ttng::MBarrierArriveOp>(op)) {\n-        op->removeAttr(\"async_agent\");\n-      }\n-    });\n-\n     // TODO: More flexible way to set num-warps\n     // One dma, one math warp group, set num-warps = 8\n     auto i32_ty = IntegerType::get(mod->getContext(), 32);"}, {"filename": "lib/Dialect/TritonNvidiaGPU/Transforms/WSMutex.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -264,8 +264,9 @@ void mutexSync(ModuleOp &mod, scf::IfOp &ifOp, scf::ForOp &persistentForOp,\n       });\n     for (int i = 0; i < numRoles; ++i) {\n       if (lockLocs[i] == op) {\n+        if (roleId != -1)\n+          op->setAttr(\"agent.mutex_role\", builder.getI32IntegerAttr(roleId));\n         roleId = i;\n-        op->setAttr(\"agent.mutex_role\", builder.getI32IntegerAttr(i));\n         break;\n       }\n     }"}, {"filename": "lib/Hopper/CMakeLists.txt", "status": "removed", "additions": 0, "deletions": 24, "changes": 24, "file_content_changes": "@@ -1,24 +0,0 @@\n-# TODO: re-enable generation of the hopper helper.\n-# For now we just commit a pre-built bc file.\n-#cmake_path(GET LLVM_LIBRARY_DIR PARENT_PATH LLVM_DIR)\n-#set(CUDA_PATH \"/usr/local/cuda\")\n-#if(DEFINED ENV{CUDA_PATH})\n-#        set(CUDA_PATH $ENV{CUDA_PATH})\n-#endif()\n-#\n-#set(outfile \"libhopper_helpers.bc\")\n-#add_custom_target(HopperHelpers ALL\n-#        COMMAND ${LLVM_DIR}/bin/clang -c\n-#        -O3 -emit-llvm\n-#        -fno-unwind-tables -fno-exceptions\n-#        --cuda-gpu-arch=sm_90a\n-#        --cuda-device-only\n-#        -D__CUDACC__ -D__CUDABE__ -D__CUDA_LIBDEVICE__\n-#        -I${CUDA_PATH}/include\n-#        -target nvptx64-nvidia-gpulibs\n-#        ${CMAKE_CURRENT_SOURCE_DIR}/HopperHelpers.c -o ${outfile}\n-#        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/HopperHelpers.c\n-#        BYPRODUCTS ${outfile}\n-#        COMMENT \"Building LLVM bitcode ${outfile}\"\n-#        VERBATIM\n-#)"}, {"filename": "lib/Hopper/HopperHelpers.c", "status": "removed", "additions": 0, "deletions": 503, "changes": 503, "file_content_changes": "@@ -1,503 +0,0 @@\n-/*\n- * Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n- *\n- * Permission is hereby granted, free of charge, to any person obtaining\n- * a copy of this software and associated documentation files\n- * (the \"Software\"), to deal in the Software without restriction,\n- * including without limitation the rights to use, copy, modify, merge,\n- * publish, distribute, sublicense, and/or sell copies of the Software,\n- * and to permit persons to whom the Software is furnished to do so,\n- * subject to the following conditions:\n- *\n- * The above copyright notice and this permission notice shall be\n- * included in all copies or substantial portions of the Software.\n- *\n- * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n- * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n- * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n- * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n- * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n- * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n- * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n- */\n-\n-#include \"device_launch_parameters.h\"\n-#include <builtin_types.h>\n-#include <cuda_device_runtime_api.h>\n-#include <stdarg.h>\n-#include <stdint.h>\n-#include <stdio.h>\n-\n-#ifdef __NVCC__\n-#define __DEVICE__ __device__ inline\n-#else\n-#define __DEVICE__\n-#endif\n-\n-#ifndef BARRIER_RANDOM_DELAY\n-#define BARRIER_RANDOM_DELAY 0\n-#endif\n-\n-#if BARRIER_RANDOM_DELAY\n-__DEVICE__ uint64_t random_avalanche(uint64_t r) {\n-  r ^= r >> 33;\n-  r *= 0xff51afd7ed558ccd;\n-  r ^= r >> 33;\n-  r *= 0xc4ceb9fe1a85ec53;\n-  r ^= r >> 33;\n-  return r;\n-}\n-\n-__DEVICE__ uint64_t random_stateless_uint64() {\n-  uint64_t r = blockIdx.z << 27 ^ blockIdx.y << 22 ^ blockIdx.x;\n-  r = r << 32 ^ threadIdx.z << 20 ^ threadIdx.y << 10 ^ threadIdx.x;\n-\n-  uint64_t timer;\n-  asm volatile(\"mov.u64 %0, %%globaltimer;\\n\\t\" : \"=l\"(timer) : : \"memory\");\n-\n-  r ^= timer;\n-  return random_avalanche(r);\n-}\n-#endif\n-\n-__DEVICE__ void random_stateless_delay() {\n-#if BARRIER_RANDOM_DELAY\n-  uint64_t r = random_stateless_uint64();\n-\n-  // About 2 milliseconds.\n-  uint32_t ns = r >> (64 - 21);\n-  asm volatile(\"nanosleep.u32 %0;\\n\\t\" : : \"r\"(ns));\n-#endif\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) uint64_t\n-__nv_get_wgmma_desc(uint32_t smem_nvvm_pointer, uint32_t mode,\n-                    uint32_t height) {\n-  uint64_t desc = 0;\n-  uint64_t swizzling = (mode == 1 ? 128 : mode == 2 ? 64 : 32);\n-  uint64_t smem_address_bit = smem_nvvm_pointer;\n-\n-  uint64_t stride_dimension = swizzling << 3 >> 4;\n-  uint64_t leading_dimension = height * swizzling >> 4;\n-  // [benzh] from cutlass\n-  uint64_t base_offset = 0; //(smem_address_bit >> 7) % (swizzling >> 4);\n-  uint64_t start_addr = (smem_address_bit << 46) >> 50;\n-\n-  desc |= ((uint64_t)mode) << 62;\n-  desc |= stride_dimension << 32;\n-  desc |= leading_dimension << 16;\n-  desc |= base_offset << 49;\n-  desc |= start_addr;\n-\n-  return desc;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_fence() {\n-  asm volatile(\"wgmma.fence.sync.aligned;\\n\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_commit_group() {\n-  asm volatile(\"wgmma.commit_group.sync.aligned;\\n\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_wgmma_wait_group() {\n-  asm volatile(\"wgmma.wait_group.sync.aligned 0;\\n\");\n-}\n-\n-// GMMA expects data to be in TN format. if A is column major, transa should be\n-// set GMMA expects data to be in TN format. if B is row major, transb should be\n-// set\n-__DEVICE__ __attribute__((__always_inline__)) float32\n-__nv_wgmma_m64n64k16_f32_f16_f16_row_col(const uint64_t desc_a,\n-                                         const uint64_t desc_b, float32 acc) {\n-  const uint32_t scale_d = 1;\n-  asm volatile(\"{\\n\"\n-               \".reg .pred p;\\n\\t\"\n-               \"setp.eq.u32 p, %34, 1;\\n\\t\"\n-               \"wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16\\n\"\n-               \"{%0, %1, %2, %3, %4, %5, %6, %7, \\n\"\n-               \" %8, %9, %10, %11, %12, %13, %14, %15,\\n\"\n-               \" %16, %17, %18, %19, %20, %21, %22, %23,\\n\"\n-               \" %24, %25, %26, %27, %28, %29, %30, %31},\\n\"\n-               \"%32, \\n\"\n-               \"%33, \\n\"\n-               \"p, 1, 1, 0, 0;\\n\"\n-               \"}\"\n-               : \"+f\"(acc.d0), \"+f\"(acc.d1), \"+f\"(acc.d2), \"+f\"(acc.d3),\n-                 \"+f\"(acc.d4), \"+f\"(acc.d5), \"+f\"(acc.d6), \"+f\"(acc.d7),\n-                 \"+f\"(acc.d8), \"+f\"(acc.d9), \"+f\"(acc.d10), \"+f\"(acc.d11),\n-                 \"+f\"(acc.d12), \"+f\"(acc.d13), \"+f\"(acc.d14), \"+f\"(acc.d15),\n-                 \"+f\"(acc.d16), \"+f\"(acc.d17), \"+f\"(acc.d18), \"+f\"(acc.d19),\n-                 \"+f\"(acc.d20), \"+f\"(acc.d21), \"+f\"(acc.d22), \"+f\"(acc.d23),\n-                 \"+f\"(acc.d24), \"+f\"(acc.d25), \"+f\"(acc.d26), \"+f\"(acc.d27),\n-                 \"+f\"(acc.d28), \"+f\"(acc.d29), \"+f\"(acc.d30), \"+f\"(acc.d31)\n-               : \"l\"(desc_a), \"l\"(desc_b), \"r\"(scale_d));\n-\n-  return acc;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_init(uint32_t bar, uint32_t expected, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"{\\n\\t\"\n-                 \"mbarrier.init.shared.b64 [%0], %1;\\n\\t\"\n-                 \"}\"\n-                 :\n-                 : \"r\"(bar), \"r\"(expected));\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_wait(uint32_t bar, uint32_t phase) {\n-  uint32_t large_val = 0x989680;\n-  asm volatile(\"{\\n\\t\"\n-               \".reg .pred                P1; \\n\\t\"\n-               \"LAB_WAIT: \\n\\t\"\n-               \"mbarrier.try_wait.parity.shared.b64 P1, [%0], %1, %2; \\n\\t\"\n-               \"@P1                       bra.uni DONE; \\n\\t\"\n-               \"bra.uni                   LAB_WAIT; \\n\\t\"\n-               \"DONE: \\n\\t\"\n-               \"}\"\n-               :\n-               : \"r\"(bar), \"r\"(phase), \"r\"(large_val));\n-  random_stateless_delay();\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) int\n-__nv_mbarrier_peek(uint32_t bar, uint32_t phase) {\n-  random_stateless_delay();\n-  int ready = 0;\n-  asm volatile(\"{\\n\\t\"\n-               \".reg .pred p;\\n\\t\"\n-               \"mbarrier.try_wait.shared.b64 p, [%1], %2;\\n\\t\"\n-               \"selp.b32 %0, 1, 0, p;\\n\\t\"\n-               \"}\"\n-               : \"=r\"(ready)\n-               : \"r\"(bar), \"l\"((unsigned long long)phase)\n-               : \"memory\");\n-  return ready;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_arrive_normal(uint32_t bar, uint32_t pred) {\n-  random_stateless_delay();\n-  if (pred) {\n-    asm volatile(\"{\\n\\t\"\n-                 \"mbarrier.arrive.shared.b64 _, [%0];\\n\\t\"\n-                 \"}\"\n-                 :\n-                 : \"r\"(bar));\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_arrive_cp_async(uint32_t bar, uint32_t pred) {\n-  random_stateless_delay();\n-  if (pred) {\n-    asm volatile(\"cp.async.mbarrier.arrive.noinc.shared.b64 [%0];\"\n-                 :\n-                 : \"r\"(bar));\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_arrive_expect_tx(uint32_t bar, uint32_t txCount, uint32_t pred) {\n-  random_stateless_delay();\n-  if (pred) {\n-    asm volatile(\"{\\n\\t\"\n-                 \"mbarrier.arrive.expect_tx.shared.b64 _, [%0], %1;\\n\\t\"\n-                 \"}\"\n-                 :\n-                 : \"r\"(bar), \"r\"(txCount));\n-  }\n-}\n-\n-// for warp special empty barrier.\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_mbarrier_arrive_remote(uint32_t bar, uint32_t ctaId, uint32_t pred) {\n-  random_stateless_delay();\n-  if (pred) {\n-    asm volatile(\"{\\n\\t\"\n-                 \".reg .b32 remAddr32;\\n\\t\"\n-                 \"mapa.shared::cluster.u32  remAddr32, %0, %1;\\n\\t\"\n-                 \"mbarrier.arrive.shared::cluster.b64  _, [remAddr32];\\n\\t\"\n-                 \"}\"\n-                 :\n-                 : \"r\"(bar), \"r\"(ctaId));\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_fence_mbarrier_init() {\n-  asm volatile(\"{\\n\\t\"\n-               \"fence.mbarrier_init.release.cluster; \\n\"\n-               \"}\" ::\n-                   : \"memory\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_fence_async_shared_cta() {\n-  asm volatile(\"fence.proxy.async.shared::cta;\\n\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_fence_async_shared_cluster() {\n-  asm volatile(\"fence.proxy.async.shared::cluster;\\n\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_cp_async_bulk(char *gmem_ptr, unsigned smem_ptr, unsigned barrier,\n-                   int bytes, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"cp.async.bulk.shared::cluster.global.mbarrier::complete_tx::\"\n-                 \"bytes [%0], [%1], %2, [%3];\\n\"\n-                 :\n-                 : \"r\"(smem_ptr), \"l\"(gmem_ptr), \"r\"(bytes), \"r\"(barrier)\n-                 : \"memory\");\n-  }\n-}\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_tma_load_tiled_2d(const uint64_t p_tma_desc, uint32_t dst_smem,\n-                       uint32_t barrier, int32_t c0, int32_t c1,\n-                       unsigned long long mem_desc, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\n-        \"cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx\"\n-        \"::bytes.L2::cache_hint [%0], [%1, {%2, %3}], \"\n-        \"[%4], %5;\\n\"\n-        :\n-        : \"r\"(dst_smem), \"l\"(p_tma_desc), \"r\"(c0), \"r\"(c1), \"r\"(barrier),\n-          \"l\"(mem_desc)\n-        : \"memory\");\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_tma_load_tiled_mcast_2d(\n-    const uint64_t p_tma_desc, uint32_t dst_smem, uint32_t barrier, int32_t c0,\n-    int32_t c1, unsigned long long mem_desc, uint16_t mcast, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::\"\n-                 \"complete_tx::bytes.multicast::cluster.L2::cache_hint\"\n-                 \" [%0], [%1, {%2, %3}], [%4], %5, %6;\"\n-                 :\n-                 : \"r\"(dst_smem), \"l\"(p_tma_desc), \"r\"(c0), \"r\"(c1),\n-                   \"r\"(barrier), \"h\"(mcast), \"l\"(mem_desc)\n-                 : \"memory\");\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_tma_load_tiled_4d(const uint64_t p_tma_desc, uint32_t dst_smem,\n-                       uint32_t barrier, int32_t c0, int32_t c1, int32_t c2,\n-                       int32_t c3, unsigned long long mem_desc, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\n-        \"cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx\"\n-        \"::bytes.L2::cache_hint [%0], [%1, {%2, %3, %4, %5}], \"\n-        \"[%6], %7;\\n\"\n-        :\n-        : \"r\"(dst_smem), \"l\"(p_tma_desc), \"r\"(c0), \"r\"(c1), \"r\"(c2), \"r\"(c3),\n-          \"r\"(barrier), \"l\"(mem_desc)\n-        : \"memory\");\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_stmatrix_x1(uint32_t ptr, const uint32_t d0) {\n-  asm volatile(\n-      \"stmatrix.sync.aligned.m8n8.x1.shared.b16 [%0], {%1};\\n\" ::\"r\"(ptr),\n-      \"r\"(d0));\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_stmatrix_x2(uint32_t ptr, const uint32_t d0, const uint32_t d1) {\n-  asm volatile(\n-      \"stmatrix.sync.aligned.m8n8.x2.shared.b16 [%0], {%1, %2};\\n\" ::\"r\"(ptr),\n-      \"r\"(d0), \"r\"(d1));\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_stmatrix_x4(uint32_t ptr, const uint32_t d0, const uint32_t d1,\n-                 const uint32_t d2, const uint32_t d3) {\n-  asm volatile(\n-      \"stmatrix.sync.aligned.m8n8.x4.shared.b16 [%0], {%1, %2, %3, %4};\\n\" ::\n-          \"r\"(ptr),\n-      \"r\"(d0), \"r\"(d1), \"r\"(d2), \"r\"(d3));\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_async_group_commit() {\n-  asm volatile(\"cp.async.bulk.commit_group;\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_async_group_wait0() {\n-  asm volatile(\"cp.async.bulk.wait_group %0;\" : : \"n\"(0) : \"memory\");\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) uint32_t\n-__nv_dsmem_addr(uint32_t buffer_ptr, uint32_t ctaid) {\n-  uint32_t buffer_ptr_;\n-  asm volatile(\"{\\n\\t\"\n-               \"mapa.shared::cluster.u32 %0, %1, %2;\\n\\t\"\n-               \"}\"\n-               : \"=r\"(buffer_ptr_)\n-               : \"r\"(buffer_ptr), \"r\"(ctaid));\n-  return buffer_ptr_;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_bar_arrive(uint32_t bar, uint32_t numThreads) {\n-  random_stateless_delay();\n-  asm volatile(\"bar.arrive %0, %1;\\n\" ::\"r\"(bar), \"r\"(numThreads));\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_bar_wait(uint32_t bar, uint32_t numThreads) {\n-  asm volatile(\"bar.sync %0, %1;\\n\" ::\"r\"(bar), \"r\"(numThreads));\n-  random_stateless_delay();\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_cga_barrier_sync() {\n-  asm volatile(\"barrier.cluster.sync.aligned;\\n\" ::);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_cga_barrier_arrive() {\n-  asm volatile(\"barrier.cluster.arrive;\\n\" ::);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_cga_barrier_wait() {\n-  asm volatile(\"barrier.cluster.wait;\\n\" ::);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_cluster_arrive_relaxed() {\n-  asm volatile(\"barrier.cluster.arrive.relaxed.aligned;\\n\" : :);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_cluster_arrive() {\n-  asm volatile(\"barrier.cluster.arrive.aligned;\\n\" : :);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void __nv_cluster_wait() {\n-  asm volatile(\"barrier.cluster.wait.aligned;\\n\" : :);\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_tma_store_tiled_2d(const uint64_t p_tma_desc, int32_t src_smem, int32_t c0,\n-                        int32_t c1, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"cp.async.bulk.tensor.2d.global.shared::cta.bulk_group\"\n-                 \"[%0, {%2, %3}], [%1];\\n\"\n-                 :\n-                 : \"l\"(p_tma_desc), \"r\"(src_smem), \"r\"(c0), \"r\"(c1)\n-                 : \"memory\");\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_tma_store_tiled_3d(const uint64_t p_tma_desc, uint32_t src_smem,\n-                        int32_t c0, int32_t c1, int32_t c2, uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"cp.async.bulk.tensor.3d.global.shared::cta.bulk_group\"\n-                 \"[%0, {%2, %3, %4}], [%1];\\n\"\n-                 :\n-                 : \"l\"(p_tma_desc), \"r\"(src_smem), \"r\"(c0), \"r\"(c1), \"r\"(c2)\n-                 : \"memory\");\n-  }\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_tma_store_tiled_4d(const uint64_t p_tma_desc, uint32_t src_smem,\n-                        int32_t c0, int32_t c1, int32_t c2, int32_t c3,\n-                        uint32_t pred) {\n-  if (pred) {\n-    asm volatile(\"cp.async.bulk.tensor.4d.global.shared::cta.bulk_group\"\n-                 \"[%0, {%2, %3, %4, %5}], [%1];\\n\"\n-                 :\n-                 : \"l\"(p_tma_desc), \"r\"(src_smem), \"r\"(c0), \"r\"(c1), \"r\"(c2),\n-                   \"r\"(c3)\n-                 : \"memory\");\n-  }\n-}\n-__DEVICE__ __attribute__((__always_inline__)) uint32_t\n-__nv_offset_of_stmatrix_v4(uint32_t threadIdx, uint32_t rowOfWarp,\n-                           uint32_t elemIdx, uint32_t leadingDimOffset,\n-                           uint32_t rowStride) {\n-  uint32_t perPhase = 0;\n-  uint32_t maxPhase = 0;\n-  if (rowStride == 64) {\n-    perPhase = 1;\n-    maxPhase = 8;\n-  } else if (rowStride == 32) {\n-    perPhase = 2;\n-    maxPhase = 4;\n-  } else if (rowStride == 16) {\n-    perPhase = 4;\n-    maxPhase = 2;\n-  }\n-\n-  uint32_t iterOfCol = elemIdx / 8;\n-\n-  uint32_t myRow = rowOfWarp + (threadIdx & 0xf);\n-  uint32_t myCol = ((threadIdx >> 4) & 0x1) * 8;\n-  myCol = myCol + iterOfCol * 16;\n-\n-  uint32_t offset0 = (myCol / rowStride) * leadingDimOffset;\n-  myCol = myCol % rowStride;\n-\n-  uint32_t phase = (myRow / perPhase) % maxPhase;\n-\n-  uint32_t lineOffset = (myRow % perPhase) * rowStride + myCol;\n-  uint32_t colOffset = ((lineOffset / 8) ^ phase) * 8 + lineOffset % 8;\n-  uint32_t offset1 = (myRow / perPhase) * 64 + colOffset;\n-\n-  return offset1 + offset0;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) uint32_t\n-__nv_offset_of_stmatrix_v4_no_swizzle(uint32_t threadIdx, uint32_t rowOfWarp,\n-                                      uint32_t elemIdx, uint32_t rowStride) {\n-  uint32_t iterOfCol = elemIdx / 4;\n-  uint32_t myRow = rowOfWarp + (threadIdx & 0xf);\n-  uint32_t myCol = ((threadIdx >> 4) & 0x1) * 8;\n-\n-  myCol = myCol + iterOfCol * 16;\n-  uint32_t offset = myRow * rowStride + myCol * 2;\n-  return offset;\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) void\n-__nv_sts64(uint32_t ptr, uint32_t d0, uint32_t d1) {\n-  asm volatile(\"st.shared.v2.b32 [%0], {%1, %2};\\n\"\n-               :\n-               : \"r\"(ptr), \"r\"(d0), \"r\"(d1));\n-}\n-\n-__DEVICE__ __attribute__((__always_inline__)) uint32_t\n-__nv_offset_of_sts64(uint32_t threadIdx, uint32_t rowOfWarp, int32_t elemIdx,\n-                     uint32_t rowStride) {\n-  uint32_t perPhase = 0;\n-  uint32_t maxPhase = 0;\n-  if (rowStride == 128) {\n-    perPhase = 1;\n-    maxPhase = 8;\n-  } else if (rowStride == 64) {\n-    perPhase = 2;\n-    maxPhase = 4;\n-  } else if (rowStride == 32) {\n-    perPhase = 4;\n-    maxPhase = 2;\n-  }\n-\n-  uint32_t laneId = threadIdx & 0x1f;\n-\n-  uint32_t myRow = ((elemIdx >> 1) & 0x1) * 8 + laneId / 4;\n-  uint32_t myCol = (elemIdx / 4) * 8 + (laneId % 4) * 2;\n-  myRow += rowOfWarp;\n-\n-  uint32_t phase = (myRow / perPhase) % maxPhase;\n-\n-  uint32_t lineOffset = (myRow % perPhase) * rowStride + myCol * 4;\n-  uint32_t colOffset = ((lineOffset / 16) ^ phase) * 16 + lineOffset % 16;\n-  uint32_t offset = (myRow / perPhase) * 128 + colOffset;\n-\n-  return offset;\n-}"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -23,7 +23,6 @@ add_mlir_translation_library(TritonLLVMIR\n         MLIRSCFToControlFlow\n         MLIRSupport\n         MLIRTargetLLVMIRExport\n-        NVGPUToLLVMIR\n         TritonGPUToLLVM\n         )\n "}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -1,6 +1,8 @@\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n \n-#include \"mlir/Conversion/Passes.h\"\n+#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n+#include \"mlir/Conversion/IndexToLLVM/IndexToLLVM.h\"\n+#include \"mlir/Conversion/SCFToControlFlow/SCFToControlFlow.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/LLVMIR/Transforms/Passes.h\"\n #include \"mlir/ExecutionEngine/ExecutionEngine.h\"\n@@ -15,8 +17,8 @@\n #include \"mlir/Target/LLVMIR/Export.h\"\n #include \"mlir/Target/LLVMIR/LLVMTranslationInterface.h\"\n #include \"mlir/Transforms/Passes.h\"\n+#include \"triton/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n-#include \"triton/Dialect/NVGPU/ToLLVMIR/NVGPUToLLVMIR.h\"\n #include \"triton/Target/LLVMIR/Passes.h\"\n #include \"triton/Target/PTX/TmaMetadata.h\"\n #include \"triton/Tools/Sys/GetEnv.hpp\"\n@@ -278,7 +280,6 @@ translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module,\n   mlir::registerLLVMDialectTranslation(registry);\n   mlir::registerROCDLDialectTranslation(registry);\n   mlir::registerNVVMDialectTranslation(registry);\n-  mlir::registerNVGPUDialectTranslation(registry);\n \n   module->getContext()->appendDialectRegistry(registry);\n \n@@ -351,6 +352,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   pm.addPass(mlir::createConvertIndexToLLVMPass());\n   pm.addPass(\n       createConvertTritonGPUToLLVMPass(computeCapability, &tmaInfos, isROCM));\n+  pm.addPass(createConvertNVGPUToLLVMPass());\n   pm.addPass(mlir::createArithToLLVMConversionPass());\n   pm.addPass(mlir::createCanonicalizerPass());\n   // Simplify the IR"}, {"filename": "python/setup.py", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -68,7 +68,9 @@ def get_pybind11_package_info():\n def get_llvm_package_info():\n     # added statement for Apple Silicon\n     system = platform.system()\n-    arch = 'x86_64'\n+    arch = platform.machine()\n+    if arch == 'aarch64':\n+        arch = 'arm64'\n     if system == \"Darwin\":\n         system_suffix = \"apple-darwin\"\n         arch = platform.machine()\n@@ -84,6 +86,9 @@ def get_llvm_package_info():\n     name = f'llvm+mlir-17.0.0-{arch}-{system_suffix}-{release_suffix}'\n     version = \"llvm-17.0.0-c5dede880d17\"\n     url = f\"https://github.com/ptillet/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n+    # FIXME: remove the following once github.com/ptillet/triton-llvm-releases has arm64 llvm releases\n+    if arch == 'arm64' and 'linux' in system_suffix:\n+        url = f\"https://github.com/acollins3/triton-llvm-releases/releases/download/{version}/{name}.tar.xz\"\n     return Package(\"llvm\", name, url, \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\", \"LLVM_SYSPATH\")\n \n \n@@ -124,7 +129,10 @@ def download_and_copy_ptxas():\n     base_dir = os.path.dirname(__file__)\n     src_path = \"bin/ptxas\"\n     version = \"12.1.105\"\n-    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n+    arch = platform.machine()\n+    if arch == \"x86_64\":\n+        arch = \"64\"\n+    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-{arch}/cuda-nvcc-{version}-0.tar.bz2\"\n     dst_prefix = os.path.join(base_dir, \"triton\")\n     dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n     dst_path = os.path.join(dst_prefix, dst_suffix)"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 46, "deletions": 0, "changes": 46, "file_content_changes": "@@ -23,6 +23,7 @@\n #include \"mlir/Dialect/Index/IR/IndexOps.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Analysis/Allocation.h\"\n+#include \"triton/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.h\"\n #include \"triton/Dialect/NVGPU/IR/Dialect.h\"\n@@ -764,6 +765,12 @@ void init_triton_ir(py::module &&m) {\n              // have a float-like type compatible with float only native ops\n              return self.getBuilder().getType<mlir::Float8E4M3B11FNUZType>();\n            })\n+      .def(\"get_fp8e4b15x4_ty\",\n+           [](TritonOpBuilder &self) -> mlir::Type {\n+             // TODO: upstream FP8E4B15 into MLIR, or find a way to externally\n+             // have a float-like type compatible with float only native ops\n+             return self.getBuilder().getType<mlir::Float8E4M3FNType>();\n+           })\n       .def(\"get_fp8e5_ty\",\n            [](TritonOpBuilder &self) -> mlir::Type {\n              return self.getBuilder().getType<mlir::Float8E5M2Type>();\n@@ -1065,6 +1072,36 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &rhs) -> mlir::Value {\n              return mlir::Value(self.create<mlir::arith::ShRSIOp>(lhs, rhs));\n            })\n+      .def(\"create_minsi\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MinSIOp>(lhs, rhs));\n+           })\n+      .def(\"create_minui\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MinUIOp>(lhs, rhs));\n+           })\n+      .def(\"create_minf\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MinFOp>(lhs, rhs));\n+           })\n+      .def(\"create_maxsi\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MaxSIOp>(lhs, rhs));\n+           })\n+      .def(\"create_maxui\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MaxUIOp>(lhs, rhs));\n+           })\n+      .def(\"create_maxf\",\n+           [](TritonOpBuilder &self, mlir::Value &lhs,\n+              mlir::Value &rhs) -> mlir::Value {\n+             return mlir::Value(self.create<mlir::arith::MaxFOp>(lhs, rhs));\n+           })\n       // AddPtr (similar to GEP)\n       .def(\"create_addptr\",\n            [](TritonOpBuilder &self, mlir::Value &ptr,\n@@ -1628,6 +1665,10 @@ void init_triton_ir(py::module &&m) {\n              self.addPass(mlir::createTritonNvidiaGPUWSMaterializationPass(\n                  computeCapability));\n            })\n+      .def(\"add_tritongpu_ws_fixup_missing_attrs_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonNvidiaGPUWSFixupMissingAttrs());\n+           })\n       .def(\n           \"add_convert_triton_to_tritongpu_pass\",\n           [](mlir::PassManager &self, int numWarps, int threadsPerWarp,\n@@ -1686,6 +1727,10 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::triton::createConvertTritonGPUToLLVMPass());\n            })\n+      .def(\"add_nv_gpu_to_llvm\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::triton::createConvertNVGPUToLLVMPass());\n+           })\n       .def(\"add_scf_to_cfg\", [](mlir::PassManager &self) {\n         self.addPass(mlir::createConvertSCFToCFPass());\n       });\n@@ -1752,6 +1797,7 @@ void init_triton_translation(py::module &m) {\n   });\n   m.def(\"get_num_warps\", [](mlir::ModuleOp mod) {\n     auto shared = mod->getAttrOfType<mlir::IntegerAttr>(\"triton_gpu.num-warps\");\n+    assert(shared);\n     return shared.getInt();\n   });\n "}, {"filename": "python/test/kernel_comparison/kernels.yml", "status": "modified", "additions": 14, "deletions": 12, "changes": 26, "file_content_changes": "@@ -1,31 +1,33 @@\n name_and_extension:\n-  - name: _kernel_0d1d2d34567c89c1011c\n+  - name: _kernel_0d1d2d3de4de5de6c7de8de9c10de11c\n     extension: ptx\n-  - name: _kernel_0d1d2d3d4d5d6d7c8d9c10d11c\n+  - name: _kernel_0d1d2d3de4de5de6de7c8de9c10de11c\n     extension: ptx\n-  - name: _kernel_0d1d2d3d4d5d6d7c8c9d10d11c\n+  - name: _kernel_0d1d2d345de6c789c1011c\n     extension: ptx\n   - name: _kernel_0d1d2d3456c789c1011c\n     extension: ptx\n-  - name: _kernel_0d1d2d345d6d7c8c9d1011c\n+  - name: _kernel_0d1d2d3de4de5de6c7de8c9de10de11c\n     extension: ptx\n   - name: _kernel_0d1d2d34567c8c91011c\n     extension: ptx\n   - name: _kernel_0d1d2d3456c78c91011c\n     extension: ptx\n-  - name: _kernel_0d1d2d345d6c78c9d1011c\n+  - name: _kernel_0d1d2d3de4de5de6de7c8c9de10de11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d34567c89c1011c\n     extension: ptx\n-  - name: _kernel_0d1d2d345d6c789c1011c\n+  - name: _kernel_0d1d2d345de6de7c89c1011c\n     extension: ptx\n-  - name: _kernel_0d1d2d3d4d5d6c7d8d9c10d11c\n+  - name: _kernel_0d1d2d345de6de7c8c9de1011c\n     extension: ptx\n-  - name: _kernel_0d1d2d3d4d5d6c7d8c9d10d11c\n+  - name: kernel_0d1d2de\n     extension: ptx\n-  - name: _kernel_0d1d2d345d6d7c89c1011c\n+  - name: _kernel_0d1d2d345de6c78c9de1011c\n     extension: ptx\n-  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15d16c17d18d19d20c21d22d23d24c2526d27d\n+  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11de12de13de14de15c16de17de18de19c20de21de22de23c2425de26de\n     extension: ptx\n-  - name: _fwd_kernel_0d1d2d34d5d6d7d8d9d10c11d12d13d14c15d16d17d18c19d20d21d22c2324d25d\n+  - name: _fwd_kernel_0d1d2d34d5d6de7de8de9c10de11de12de13c14de15de16de17c18de19de20de21c2223de24de\n     extension: ptx\n-  - name: _bwd_preprocess_0d1d2d3d4d\n+  - name: _bwd_preprocess_0d1d2d\n     extension: ptx"}, {"filename": "python/test/unit/hopper/test_gemm.py", "status": "modified", "additions": 30, "deletions": 60, "changes": 90, "file_content_changes": "@@ -29,7 +29,6 @@\n \n import triton\n import triton.language as tl\n-from .utils import get_proper_err, get_variant_golden\n \n \n @triton.jit\n@@ -94,11 +93,6 @@ def matmul_no_scf_kernel(\n                              ]))\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_gemm_no_scf(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, ENABLE_WS):\n-    if '-'.join(map(str, [USE_TMA_EPILOGUE, ENABLE_WS])) in [\n-        'True-True'\n-    ]:\n-        pytest.skip(\"error, skip\")\n-\n     if (TRANS_A):\n         a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -127,14 +121,12 @@ def test_gemm_no_scf(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE\n     a_f32 = a.to(torch.float32)\n     b_f32 = b.to(torch.float32)\n     golden = torch.matmul(a_f32, b_f32)\n-    golden_variant = get_variant_golden(a_f32, b_f32)\n-    golden_abs_err, golden_rel_err = get_proper_err(golden, golden_variant)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(\n         c,\n         golden,\n-        rtol=max(1e-2, 1.1 * golden_rel_err),\n-        atol=max(1e-3, 1.1 * golden_abs_err),\n+        rtol=1e-2,\n+        atol=1e-3,\n         check_dtype=False)\n \n \n@@ -152,6 +144,7 @@ def matmul_kernel(\n     DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n     A_ORDER_0: tl.constexpr, A_ORDER_1: tl.constexpr,\n     B_ORDER_0: tl.constexpr, B_ORDER_1: tl.constexpr,\n+    W_ORDER_0: tl.constexpr, W_ORDER_1: tl.constexpr,\n     Z_ORDER_0: tl.constexpr, Z_ORDER_1: tl.constexpr\n ):\n     pid = tl.program_id(axis=0)\n@@ -170,8 +163,9 @@ def matmul_kernel(\n                                    offsets=(block_offset_m, 0), block_shape=(BLOCK_M, BLOCK_K), order=(A_ORDER_0, A_ORDER_1))\n     b_tile_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),\n                                    offsets=(0, block_offset_n), block_shape=(BLOCK_K, BLOCK_N), order=(B_ORDER_0, B_ORDER_1))\n+    # for chain-dot, BLOCK_N must always be equal to N, and each program loads the whole W matrix\n     w_tile_ptr = tl.make_block_ptr(base=w_ptr, shape=(N, N), strides=(stride_wm, stride_wn),\n-                                   offsets=(0, block_offset_n), block_shape=(BLOCK_N, BLOCK_N), order=(Z_ORDER_1, Z_ORDER_0))\n+                                   offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_N), order=(W_ORDER_0, W_ORDER_1))\n     z = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n \n     offs_m = block_offset_m + tl.arange(0, BLOCK_M)\n@@ -216,7 +210,7 @@ def matmul_kernel(\n         tl.store(z_ptrs, z, mask=mask)\n \n \n-@pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_C,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n+@pytest.mark.parametrize('BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,M,N,K,TRANS_A,TRANS_B,TRANS_OUTPUT,epilogue,out_dtype,USE_TMA_STORE,NUM_STAGES,ENABLE_WS',\n                          [(128, 128, 64, 4, 1, *shape_w_c, 'none', out_dtype, use_tma_store, 3, enable_ws)\n                           for shape_w_c in [\n                              # badcase from cublas-important-layers\n@@ -228,7 +222,7 @@ def matmul_kernel(\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                               # softmax works for one CTA\n                               for shape_w_c in [\n                              [64, 64, 16, 4, 1, 64, 64, 64],\n@@ -242,10 +236,10 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                               for shape_w_c in [\n                              [64, 64, 16, 4, 1, 128, 128, 64],\n                              *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4, 8] for num_ctas in [1, 2, 4]],\n@@ -267,11 +261,11 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                             if not (epilogue == 'chain-dot' and (shape_w_c[5] is not None or shape_w_c[0] != shape_w_c[1]))\n-                         ] + [(*shape_w_c, trans_a, trans_b, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                             if not (epilogue == 'chain-dot' and (shape_w_c[6] is not None or shape_w_c[1] != shape_w_c[6]))\n+                         ] + [(*shape_w_c, trans_a, trans_b, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                               for shape_w_c in [\n                              [64, 64, 32, 4, 1, 128, 256, 64],\n                              [128, 128, 16, 4, 4, 512, 256, 64],\n@@ -290,34 +284,34 @@ def matmul_kernel(\n                              for use_tma_store in [False, True]\n                              for trans_a in [False, True]\n                              for trans_b in [False, True]\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for num_stages in [3]\n                              for enable_ws in [False, True]\n-                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                         ] + [(64, n, 16, 4, 1, 512, 256, 256, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                               # loop over instr shapes\n                               for n in [16, 32, 64, 128, 256]\n-                              for trans_c in [False, True]\n+                              for trans_output in [False, True]\n                               for out_dtype in ['float16', 'float32']\n                               for use_tma_store in [False, True]\n                               for num_stages in [2, 4, 5, 7]\n                               for enable_ws in [False, True]\n-                              ] + [(*shape_w_c, *shape, False, True, trans_c, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n+                              ] + [(*shape_w_c, *shape, False, True, trans_output, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                                    # irregular shapes\n                                    for shape_w_c in [\n                                        [128, 128, 64, 4, 1],\n                                        [256, 128, 64, 4, 2],\n                                        [128, 128, 128, 4, 2],\n                               ]\n                              for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n-                             for trans_c in [False, True]\n+                             for trans_output in [False, True]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n                              for num_stages in [2, 3, 4]\n                              for enable_ws in [False, True]\n                          ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()\n                     [0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B, TRANS_C, epilogue, out_dtype, USE_TMA_STORE, NUM_STAGES, ENABLE_WS):\n+def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B, TRANS_OUTPUT, epilogue, out_dtype, USE_TMA_STORE, NUM_STAGES, ENABLE_WS):\n     if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B])) in [\n         '16-32-64-4-4-512-256-64-True-False',\n         '16-32-64-4-4-512-256-64-True-True',\n@@ -334,22 +328,7 @@ def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A,\n         '16-32-64-8-2-256-256-256-False',\n         '16-32-64-8-2-256-256-256-True',\n     ]:\n-        pytest.skip('illegal memory access.')\n-\n-    # with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n-    if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K])) in [\n-        '64-64-32-8-1-128-256-64',\n-    ]:\n-        pytest.skip('Tensor-likes are not close!')\n-\n-    if NUM_CTAS > 1 and NUM_WARPS == 8:\n-        pytest.skip('Tensor-likes are not close!')\n-\n-    # with ENABLE_TMA=1 and ENABLE_MMA_V3=1\n-    if ENABLE_WS:\n-        # example:\n-        # [128-128-64-4-1-None-None-None-False-False-False-chain-dot-float16-False-3-True]\n-        pytest.skip('hang!')\n+        pytest.skip('Known legacy issue, ldmatrix can only support x4')\n \n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n@@ -381,27 +360,23 @@ def test_gemm(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A,\n \n     # avoid out of memory\n     if epilogue in ['add-matrix', 'add-rows', 'add-cols']:\n-        if (TRANS_C):\n-            bias = torch.randn((M, N), device='cuda', dtype=torch_out_dtype)\n-        else:\n+        if (TRANS_OUTPUT):\n             bias = torch.randn((N, M), device='cuda', dtype=torch_out_dtype).T\n+        else:\n+            bias = torch.randn((M, N), device='cuda', dtype=torch_out_dtype)\n     else:\n         bias = torch.randn((1, 1), device='cuda', dtype=torch_out_dtype)\n \n-    if epilogue == 'chain-dot':\n-        if (TRANS_C):\n-            w = torch.randn((N, N), device='cuda', dtype=torch.float16).T\n-        else:\n-            w = torch.randn((M, M), device='cuda', dtype=torch.float16)\n-    else:\n-        w = torch.randn((1, 1), device='cuda', dtype=torch.float16).T\n+    # for chain-dot only\n+    w = torch.randn((N, N), device='cuda', dtype=torch.float16).T\n+    w_order = [0, 1]\n \n-    if (TRANS_C):\n-        z = torch.full((M, N), 1., device='cuda', dtype=torch_out_dtype)\n-        z_order = [1, 0]\n-    else:\n+    if (TRANS_OUTPUT):\n         z = torch.full((N, M), 1., device='cuda', dtype=torch_out_dtype).T\n         z_order = [0, 1]\n+    else:\n+        z = torch.full((M, N), 1., device='cuda', dtype=torch_out_dtype)\n+        z_order = [1, 0]\n \n     # torch result\n     a_f32 = a.to(torch.float32)\n@@ -445,17 +420,12 @@ def grid(META):\n                               CHAIN_DOT=epilogue == 'chain-dot',\n                               A_ORDER_0=a_order[0], A_ORDER_1=a_order[1],\n                               B_ORDER_0=b_order[0], B_ORDER_1=b_order[1],\n+                              W_ORDER_0=w_order[0], W_ORDER_1=w_order[1],\n                               Z_ORDER_0=z_order[0], Z_ORDER_1=z_order[1],\n                               num_warps=NUM_WARPS, num_ctas=NUM_CTAS, num_stages=NUM_STAGES,\n                               enable_warp_specialization=ENABLE_WS)\n \n     torch.set_printoptions(profile=\"full\")\n-    # print(\"abs_err: {}, rel_err: {}\".format(golden_abs_err, golden_rel_err))\n-    # print(\"golden: \")\n-    # print(golden)\n-    # print(\"result: \")\n-    # print(z)\n-    # print(\"max_gap: {}\".format(torch.max(torch.abs(z - golden))))\n     golden = torch.nn.functional.normalize(golden)\n     z = torch.nn.functional.normalize(z)\n     assert_close(z, golden,"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_fused-attention.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -297,9 +297,6 @@ def backward(ctx, do):\n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n-    # with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n-    pytest.skip('unspecified launch failure')\n-\n     torch.manual_seed(20)\n     q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n     k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()"}, {"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 116, "deletions": 181, "changes": 297, "file_content_changes": "@@ -26,15 +26,6 @@\n \n import triton\n import triton.language as tl\n-from .utils import get_proper_err, get_variant_golden\n-\n-\n-def isMMAV3OrTMAEnabled():\n-    import os\n-    for k in ('ENABLE_MMA_V3', 'ENABLE_TMA'):\n-        if os.environ.get(k, '0').lower() in ['1', 'on', 'true']:\n-            return True\n-    return False\n \n \n @triton.jit\n@@ -123,20 +114,21 @@ def static_persistent_tma_matmul_kernel(\n         pre_pid_n = pid_n\n \n \n-@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,TRANS_A,TRANS_B', [\n-    [4096, 4096, 64, 64, 64, 16, 4, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 32, 4, 1, False, True],\n-    [4096, 4096, 64, 256, 64, 16, 4, 1, False, True],\n-    [4096, 4096, 64, 128, 128, 16, 4, 1, False, True],\n-    # TODO: fix issue for 8-warp persistent kernel\n-    # [4096, 4096, 64, 128, 128, 16, 8, 1, False, True],\n-    # [4096, 4096, 64, 128, 256, 16, 8, 1, False, True],\n-])\n+@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_WARPS,NUM_CTAS,TRANS_A,TRANS_B,USE_TMA',\n+                         [(*shape, use_tma)\n+                          for shape in [\n+                             [4096, 4096, 64, 64, 64, 16, 4, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 32, 4, 1, False, True],\n+                             [4096, 4096, 64, 256, 64, 16, 4, 1, False, True],\n+                             [4096, 4096, 64, 128, 128, 16, 4, 1, False, True],\n+                             # TODO: fix issue for 8-warp persistent kernel\n+                             # [4096, 4096, 64, 128, 128, 16, 8, 1, False, True],\n+                             # [4096, 4096, 64, 128, 256, 16, 8, 1, False, True],\n+                         ]\n+                             for use_tma in [False, True]\n+                         ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_user_defined_persistent_non_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, TRANS_A, TRANS_B):\n-    # TODO: fix RewriteTensorPtrPass\n-    pytest.skip('RewriteTensorPtrPass issue')\n-\n+def test_user_defined_persistent_non_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, TRANS_A, TRANS_B, USE_TMA):\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -151,26 +143,13 @@ def test_user_defined_persistent_non_warp_specialized_gemm(M, N, K, BLOCK_M, BLO\n     num_SMs = torch.cuda.get_device_properties('cuda').multi_processor_count\n     grid = lambda META: (num_SMs,)\n \n-    def call_vintage():\n-        static_persistent_matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c, M=M, N=N, K=K, stride_am=a.stride(0), stride_ak=a.stride(1), stride_bk=b.stride(0), stride_bn=b.stride(1), stride_cm=c.stride(0), stride_cn=c.stride(1), BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K, NUM_SM=num_SMs, num_warps=NUM_WARPS, num_ctas=NUM_CTAS)\n-        return c\n-\n-    def call_stylish():\n+    if USE_TMA:\n         static_persistent_tma_matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c, M=M, N=N, K=K, stride_am=a.stride(0), stride_ak=a.stride(1), stride_bk=b.stride(0), stride_bn=b.stride(1), stride_cm=c.stride(0), stride_cn=c.stride(1), BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K, NUM_SM=num_SMs, num_warps=NUM_WARPS, num_ctas=NUM_CTAS)\n-        return c\n+    else:\n+        static_persistent_matmul_kernel[grid](a_ptr=a, b_ptr=b, c_ptr=c, M=M, N=N, K=K, stride_am=a.stride(0), stride_ak=a.stride(1), stride_bk=b.stride(0), stride_bn=b.stride(1), stride_cm=c.stride(0), stride_cn=c.stride(1), BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K, NUM_SM=num_SMs, num_warps=NUM_WARPS, num_ctas=NUM_CTAS)\n \n     th_c = torch.matmul(a, b)\n-\n-    # Test using old style of ptr calculation\n-    tt_c = call_vintage()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # Cealr c\n-    c = torch.randn((M, N), device=a.device, dtype=torch.float32)\n-\n-    # Test using make_block_ptr\n-    tt_c = call_stylish()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n+    torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n \n \n @triton.jit\n@@ -252,36 +231,37 @@ def tma_warp_specialized_matmul_kernel(\n     tl.store(c_ptrs, accumulator, mask=mask)\n \n \n-@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B', [\n-    [2048, 2048, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 16, 1, False, True],\n-    [128, 4096, 64, 64, 64, 16, 1, False, True],\n-    [4096, 128, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 32, 1, False, True],\n-    [4096, 4096, 256, 128, 128, 16, 1, False, True],\n-    [4096, 4096, 320, 128, 64, 64, 1, False, True],\n-    [4096, 4096, 320, 64, 128, 64, 1, False, True],\n-    [4096, 4096, 320, 128, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 64, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 64, 1, False, True],\n-    # numCTAs > 1\n-    [2048, 2048, 64, 128, 128, 64, 2, False, True],\n-    [2048, 2048, 64, 128, 128, 64, 2, False, True],\n-    [2048, 2048, 128, 256, 128, 64, 4, False, True],\n-    [4096, 4096, 128, 256, 128, 64, 4, False, True],\n-    [4096, 4096, 256, 128, 256, 64, 4, False, True],\n-    [4096, 4096, 256, 256, 256, 64, 4, False, True],\n-])\n+@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B,USE_TMA',\n+                         [(*shape, use_tma)\n+                          for shape in [\n+                             [2048, 2048, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [128, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 128, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 32, 1, False, True],\n+                             [4096, 4096, 256, 128, 128, 16, 1, False, True],\n+                             [4096, 4096, 320, 128, 64, 64, 1, False, True],\n+                             [4096, 4096, 320, 64, 128, 64, 1, False, True],\n+                             [4096, 4096, 320, 128, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 64, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 1, False, True],\n+                             # numCTAs > 1\n+                             [2048, 2048, 64, 128, 128, 64, 2, False, True],\n+                             [2048, 2048, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 4, False, True],\n+                             [4096, 4096, 256, 256, 256, 64, 4, False, True],\n+                         ]\n+                             for use_tma in [False, True]\n+                         ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B):\n-    pytest.skip('hang')\n-\n+def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B, USE_TMA):\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -296,8 +276,8 @@ def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K\n \n     grid = lambda META: (triton.cdiv(M, BLOCK_M) * triton.cdiv(N, BLOCK_N),)\n \n-    def call_vintage():\n-        warp_specialized_matmul_kernel[grid](\n+    if USE_TMA:\n+        tma_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n             a.stride(0), a.stride(1),\n@@ -307,10 +287,8 @@ def call_vintage():\n             num_warps=4,\n             num_ctas=NUM_CTAS,\n             enable_warp_specialization=True)\n-        return c\n-\n-    def call_stylish():\n-        tma_warp_specialized_matmul_kernel[grid](\n+    else:\n+        warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n             a.stride(0), a.stride(1),\n@@ -320,35 +298,11 @@ def call_stylish():\n             num_warps=4,\n             num_ctas=NUM_CTAS,\n             enable_warp_specialization=True)\n-        return c\n \n     th_c = torch.matmul(a, b)\n+    torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n \n-    # Test using old style of ptr calculation\n-    tt_c = call_vintage()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # Cealr c\n-    c = torch.randn((M, N), device=a.device, dtype=torch.float32)\n \n-    # # Test using make_block_ptr\n-    tt_c = call_stylish()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # # #############################################Performance Evaluation#############################################\n-    # fn = lambda: call_vintage()\n-    # ms = triton.testing.do_bench(fn, warmup=25, rep=100)\n-    # cur_gpu_perf = round(2. * M * N * K / ms * 1e-9, 2)\n-    # print(' '.join(['Performance of', str(M), str(N), str(K), ':', str(ms), 'ms, ', str(cur_gpu_perf), 'TFLOPS']))\n-\n-\n-@triton.autotune(\n-    configs=[\n-        triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=True),\n-        # triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=False),\n-    ],\n-    key=['M', 'N', 'K'],\n-)\n @triton.jit\n def static_persistent_warp_specialized_matmul_kernel(\n     a_ptr, b_ptr, c_ptr,\n@@ -388,13 +342,6 @@ def static_persistent_warp_specialized_matmul_kernel(\n         tl.store(c_ptrs, accumulator)\n \n \n-@triton.autotune(\n-    configs=[\n-        triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=True),\n-        # triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=False),\n-    ],\n-    key=['M', 'N', 'K'],\n-)\n @triton.jit\n def static_persistent_tma_warp_specialized_matmul_kernel(\n     a_ptr, b_ptr, c_ptr,\n@@ -442,29 +389,37 @@ def static_persistent_tma_warp_specialized_matmul_kernel(\n         pre_pid_n = pid_n\n \n \n-@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B', [\n-    [2048, 2048, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 16, 1, False, True],\n-    [128, 4096, 64, 64, 64, 16, 1, False, True],\n-    [4096, 128, 64, 64, 64, 16, 1, False, True],\n-    [4096, 4096, 64, 64, 64, 32, 1, False, True],\n-    [4096, 4096, 256, 128, 128, 16, 1, False, True],\n-    [4096, 4096, 320, 128, 64, 64, 1, False, True],\n-    [4096, 4096, 320, 64, 128, 64, 1, False, True],\n-    [4096, 4096, 320, 128, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 64, 64, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 64, 256, 64, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 16, 1, False, True],\n-    [4096, 4096, 256, 256, 128, 64, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 16, 1, False, True],\n-    [4096, 4096, 256, 128, 256, 64, 1, False, True],\n-])\n+@pytest.mark.parametrize('M,N,K,BLOCK_M,BLOCK_N,BLOCK_K,NUM_CTAS,TRANS_A,TRANS_B,USE_TMA',\n+                         [(*shape, use_tma)\n+                          for shape in [\n+                             [2048, 2048, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [128, 4096, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 128, 64, 64, 64, 16, 1, False, True],\n+                             [4096, 4096, 64, 64, 64, 32, 1, False, True],\n+                             [4096, 4096, 256, 128, 128, 16, 1, False, True],\n+                             [4096, 4096, 320, 128, 64, 64, 1, False, True],\n+                             [4096, 4096, 320, 64, 128, 64, 1, False, True],\n+                             [4096, 4096, 320, 128, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 64, 64, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 64, 256, 64, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 16, 1, False, True],\n+                             [4096, 4096, 256, 256, 128, 64, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 16, 1, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 1, False, True],\n+                             # numCTAs > 1\n+                             [2048, 2048, 64, 128, 128, 64, 2, False, True],\n+                             [2048, 2048, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 4, False, True],\n+                             [4096, 4096, 256, 256, 256, 64, 4, False, True],\n+                         ]\n+                             for use_tma in [False, True]\n+                         ])\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n-def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B):\n-    # TODO: fix RewriteTensorPtrPass\n-    pytest.skip('RewriteTensorPtrPass issue')\n+def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, NUM_CTAS, TRANS_A, TRANS_B, USE_TMA):\n     if (TRANS_A):\n         a = .1 * torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -479,44 +434,29 @@ def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N\n     num_SMs = torch.cuda.get_device_properties('cuda').multi_processor_count\n     grid = lambda META: (num_SMs,)\n \n-    def call_vintage():\n-        static_persistent_warp_specialized_matmul_kernel[grid](\n+    if USE_TMA:\n+        static_persistent_tma_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n-            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs)\n-        return c\n-\n-    def call_stylish():\n-        static_persistent_tma_warp_specialized_matmul_kernel[grid](\n+            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs,\n+            num_warps=4, num_ctas=NUM_CTAS,\n+            enable_warp_specialization=True)\n+    else:\n+        static_persistent_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n-            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs)\n-        return c\n+            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs,\n+            num_warps=4, num_ctas=NUM_CTAS,\n+            enable_warp_specialization=True)\n \n     th_c = torch.matmul(a, b)\n-\n-    # Test using old style of ptr calculation\n-    tt_c = call_vintage()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # Cealr c\n-    c = torch.randn((M, N), device=a.device, dtype=torch.float32)\n-\n-    # Test using make_block_ptr\n-    tt_c = call_stylish()\n-    torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n-\n-    # #############################################Performance Evaluation#############################################\n-    # fn = lambda: call_stylish()\n-    # ms = triton.testing.do_bench(fn, warmup=25, rep=100)\n-    # cur_gpu_perf = round(2. * M * N * K / ms * 1e-9, 2)\n-    # print(' '.join(['Performance of', str(M), str(N), str(K), ':', str(ms), 'ms, ', str(cur_gpu_perf), 'TFLOPS']))\n+    torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n \n \n @triton.jit\n@@ -607,8 +547,6 @@ def static_persistent_matmul_no_scf_kernel(\n                              ]))\n @pytest.mark.skipif(torch.cuda.get_device_capability()[0] < 9, reason=\"Requires compute capability >= 9\")\n def test_static_persistent_matmul_no_scf_kernel(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_TYPE, USE_TMA_EPILOGUE, USE_TMA_LOAD):\n-    if isMMAV3OrTMAEnabled():\n-        pytest.skip(\"known failure\")\n     if (TRANS_A):\n         a = torch.randn((K, M), device='cuda', dtype=torch.float16).T\n     else:\n@@ -641,14 +579,12 @@ def test_static_persistent_matmul_no_scf_kernel(M, N, K, NUM_CTAS, NUM_WARPS, TR\n     a_f32 = a.to(torch.float32)\n     b_f32 = b.to(torch.float32)\n     golden = torch.matmul(a_f32, b_f32)\n-    golden_variant = get_variant_golden(a_f32, b_f32)\n-    golden_abs_err, golden_rel_err = get_proper_err(golden, golden_variant)\n     torch.set_printoptions(profile=\"full\")\n     assert_close(\n         c,\n         golden,\n-        rtol=max(1e-2, 1.1 * golden_rel_err),\n-        atol=max(1e-3, 1.1 * golden_abs_err),\n+        rtol=1e-2,\n+        atol=1e-3,\n         check_dtype=False)\n \n \n@@ -791,20 +727,17 @@ def full_static_persistent_matmul_kernel(\n                              (*shape_w_c, trans_a, trans_b, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              for shape_w_c in [\n                                  [64, 64, 32, 4, 1, 128, 256, 64],\n-                                 # TODO: enable when num_ctas != 1 is supported.\n-                                 # [128, 128, 16, 4, 4, 512, 256, 64],\n-                                 # [128, 256, 32, 4, 8, 256, 256, 192],\n-                                 # [512, 256, 32, 4, 8, 1024, 256, 192],\n+                                 [128, 128, 16, 4, 4, 512, 256, 64],\n+                                 [128, 256, 32, 4, 8, 256, 256, 192],\n+                                 [512, 256, 32, 4, 8, 1024, 256, 192],\n                                  # BLOCK_K >= 128\n                                  [64, 128, 128, 4, 1, 512, 256, 256],\n                                  [128, 128, 128, 4, 1, 256, 256, 192],\n-                                 # TODO: enable when num_ctas != 1 is supported.\n-                                 # [128, 128, 128, 4, 2, 256, 256, 192],\n+                                 [128, 128, 128, 4, 2, 256, 256, 192],\n                                  # small BLOCK_M and BLOCK_K\n                                  [16, 32, 32, 4, 1, 128, 256, 64],\n                                  [32, 32, 16, 4, 1, 256, 256, 192],\n-                                 # TODO: enable when num_ctas != 1 is supported.\n-                                 # [16, 32, 64, 4, 4, 512, 256, 64],\n+                                 [16, 32, 64, 4, 4, 512, 256, 64],\n                              ]\n                              for out_dtype in ['float16', 'float32']\n                              for use_tma_store in [False, True]\n@@ -815,13 +748,13 @@ def full_static_persistent_matmul_kernel(\n                          ] + [(*shape_w_c, trans_a, trans_b, epilogue, out_dtype, use_tma_store, num_stages, enable_ws)\n                               for shape_w_c in [\n                              [64, 64, 16, 4, 1, 128, 128, 64],\n-                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1]],\n+                             *[[256, 64, 16, num_warps, num_ctas, 256, 256, 64] for num_warps in [4] for num_ctas in [1, 2, 4]],\n                              # for chain-dot\n                              [128, 128, 64, 4, 1, None, None, None],\n                              [64, 64, 16, 4, 1, None, None, None],\n                              # small BLOCK_M and BLOCK_K\n                              [16, 16, 64, 4, 1, 128, 128, 64],\n-                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1]],\n+                             *[[16, 32, 64, num_warps, num_ctas, 256, 256, 256] for num_warps in [4] for num_ctas in [1, 2]],\n                              #  # TODO: enable when num_warps != 4 is supported.\n                              #  # repeat\n                              #  # [64, 64, 32, 8, 1, 128, 256, 64],\n@@ -850,9 +783,9 @@ def full_static_persistent_matmul_kernel(\n                              (*shape_w_c, *shape, False, True, 'none', out_dtype, use_tma_store, num_stages, enable_ws)\n                              # irregular shapes\n                              for shape_w_c in [\n-                                 [128, 128, 64, 4, 1]\n-                                 # [256, 128, 64, 4, 2],\n-                                 # [128, 128, 128, 4, 2],\n+                                 [128, 128, 64, 4, 1],\n+                                 [256, 128, 64, 4, 2],\n+                                 [128, 128, 128, 4, 2]\n                              ]\n                              for shape in list(itertools.product([*range(512, 4096, 360)], [*range(512, 4096, 360)], [512, 1024]))\n                              for out_dtype in ['float16', 'float32']\n@@ -864,19 +797,11 @@ def full_static_persistent_matmul_kernel(\n @pytest.mark.skipif(torch.cuda.get_device_capability()\n                     [0] < 9, reason=\"Requires compute capability >= 9\")\n def test_full_static_persistent_matmul_kernel(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B, epilogue, out_dtype, USE_TMA_STORE, NUM_STAGES, ENABLE_WS):\n-    pytest.skip(\"known failure, will fix it later!!!\")\n     if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, epilogue, out_dtype, USE_TMA_STORE, NUM_STAGES, ENABLE_WS])) in [\n         '128-128-128-4-1-256-256-192-none-float32-True-3-True',\n     ]:\n         pytest.skip('out of resource: shared memory, Required: 263168')\n \n-    if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, USE_TMA_STORE, ENABLE_WS])) in ([\n-        '64-16-16-4-1-512-256-256-True-True',\n-    ] + [\n-        f'128-128-64-4-1-{m}-{n}-{k}-True-True' for m in range(512, 4096, 360) for n in range(512, 4096, 360) for k in [512, 1024]\n-    ]):\n-        pytest.skip('known kernel hang problem when tma store is enabled')\n-\n     if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_A, TRANS_B])) in [\n         '16-32-64-4-4-512-256-64-True-False',\n         '16-32-64-4-4-512-256-64-True-True',\n@@ -885,6 +810,16 @@ def test_full_static_persistent_matmul_kernel(BLOCK_M, BLOCK_N, BLOCK_K, NUM_WAR\n     ]:\n         pytest.skip('shapePerCTA[1] < 16 not supported')\n \n+    # with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n+    if '-'.join(map(str, [BLOCK_M, BLOCK_N, BLOCK_K, NUM_WARPS, NUM_CTAS, M, N, K, TRANS_B])) in [\n+        '16-32-64-4-1-256-256-256-False',\n+        '16-32-64-4-2-256-256-256-False',\n+        '16-32-64-4-2-256-256-256-True',\n+        '16-32-64-8-2-256-256-256-False',\n+        '16-32-64-8-2-256-256-256-True',\n+    ]:\n+        pytest.skip('Known legacy issue, ldmatrix can only support x4')\n+\n     if epilogue == 'chain-dot':\n         pytest.skip('known failure: Assertion !region.empty() && unexpected empty region.')\n "}, {"filename": "python/test/unit/hopper/test_tma_store_gemm.py", "status": "modified", "additions": 1, "deletions": 32, "changes": 33, "file_content_changes": "@@ -28,36 +28,6 @@\n import triton.language as tl\n \n \n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)\n-\n-\n @triton.jit\n def matmul_tma_load_store(\n     a_ptr, b_ptr, c_ptr,\n@@ -118,6 +88,5 @@ def test_tma_load_store(M, N, K, NUM_CTAS, NUM_WARPS, TRANS_A, TRANS_B, OUTPUT_F\n                                   num_ctas=NUM_CTAS,\n                                   OUTPUT_F16=OUTPUT_F16)\n     golden = torch.matmul(a, b)\n-    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n     torch.set_printoptions(profile=\"full\")\n-    assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n+    assert_close(c, golden, rtol=1e-2, atol=1e-3, check_dtype=False)"}, {"filename": "python/test/unit/hopper/ttgir_tests/test_tma.py", "status": "modified", "additions": 2, "deletions": 7, "changes": 9, "file_content_changes": "@@ -23,7 +23,6 @@\n \n import pytest\n import torch\n-from test_util import get_proper_err\n from torch.testing import assert_close\n \n import triton\n@@ -63,13 +62,9 @@ def test_tma_wgmma_64_64_16_f16(TTGIR, TRANS_A, TRANS_B):\n \n     golden = torch.matmul(a, b)\n     torch.set_printoptions(profile=\"full\", sci_mode=False)\n-    golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n     assert_close(\n         c,\n         golden,\n-        rtol=max(1e-4,\n-                 1.5 * golden_rel_err),\n-        atol=max(\n-            1e-4,\n-            1.5 * golden_abs_err),\n+        rtol=1e-2,\n+        atol=1e-3,\n         check_dtype=False)"}, {"filename": "python/test/unit/hopper/ttgir_tests/test_util.py", "status": "removed", "additions": 0, "deletions": 52, "changes": 52, "file_content_changes": "@@ -1,52 +0,0 @@\n-# Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n-#\n-# Permission is hereby granted, free of charge, to any person obtaining\n-# a copy of this software and associated documentation files\n-# (the \"Software\"), to deal in the Software without restriction,\n-# including without limitation the rights to use, copy, modify, merge,\n-# publish, distribute, sublicense, and/or sell copies of the Software,\n-# and to permit persons to whom the Software is furnished to do so,\n-# subject to the following conditions:\n-#\n-# The above copyright notice and this permission notice shall be\n-# included in all copies or substantial portions of the Software.\n-#\n-# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n-# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n-# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n-# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n-# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n-# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n-# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n-\n-import torch\n-\n-\n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)"}, {"filename": "python/test/unit/hopper/utils.py", "status": "removed", "additions": 0, "deletions": 32, "changes": 32, "file_content_changes": "@@ -1,32 +0,0 @@\n-import torch\n-\n-\n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K), dtype=a.dtype).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K), dtype=a.dtype).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N), dtype=b.dtype).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N), dtype=b.dtype).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-# It's not easy to get a proper error threshold in different size\n-# Here the gemm calculation is padded to a different size in order to get\n-# a variant version of the golden result. And the error between golden and\n-# golden_variant provide reference on selecting the proper rtol / atol.\n-\n-\n-def get_proper_err(golden, golden_variant):\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    # avoid problems when golden_rel_err is 'inf'\n-    abs_golden = torch.abs(golden) + torch.full_like(golden, torch.finfo(golden.dtype).smallest_normal)\n-    golden_rel_err = torch.max(torch.abs(golden_diff) / abs_golden).item()\n-    return (golden_abs_err, golden_rel_err)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 26, "deletions": 30, "changes": 56, "file_content_changes": "@@ -1384,43 +1384,39 @@ def test_convert_float16_to_float32(in_dtype, device):\n \n \n def serialize_fp8(np_data, in_dtype):\n-    return np_data\n-# def serialize_fp8(np_data, in_dtype):\n-#     if in_dtype == tl.float8e4b15:\n-#         # triton's f8e4b15 format is optimized for software emulation\n-#         # as a result, each pack of 4xfp8 values:\n-#         # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n-#         # is actually internally stored as\n-#         # s0s2b0b2s1s3b1b3\n-#         # we apply the conversion here\n-#         f8x4 = np_data.view(np.uint32)\n-#         s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n-#         b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n-#         signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n-#         bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n-#         # tensor of triton fp8 data\n-#         return (signs | bits).view(np.int8)\n-#     else:\n-#         return np_data\n+    if in_dtype == tl.float8e4b15x4:\n+        # triton's f8e4b15 format is optimized for software emulation\n+        # as a result, each pack of 4xfp8 values:\n+        # s0b0s1b1s2b2s3b3 (for s, b sign and bits respectively)\n+        # is actually internally stored as\n+        # s0s2b0b2s1s3b1b3\n+        # we apply the conversion here\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in range(0, 32, 8)]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in range(0, 32, 8)]\n+        signs = (s[0] >> 0) | (s[1] >> 16) | (s[2] >> 1) | (s[3] >> 17)\n+        bits = (b[0] >> 1) | (b[1] >> 17) | (b[2] >> 8) | (b[3] >> 24)\n+        # tensor of triton fp8 data\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n \n # inverse of `serialize_fp8`\n \n \n def deserialize_fp8(np_data, in_dtype):\n-    return np_data\n-# def deserialize_fp8(np_data, in_dtype):\n-#     if in_dtype == tl.float8e4b15:\n-#         f8x4 = np_data.view(np.uint32)\n-#         s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n-#         b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n-#         signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n-#         bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n-#         return (signs | bits).view(np.int8)\n-#     else:\n-#         return np_data\n+    if in_dtype == tl.float8e4b15x4:\n+        f8x4 = np_data.view(np.uint32)\n+        s = [(f8x4 & (0x80000000 >> i)) << i for i in [0, 16, 1, 17]]\n+        b = [(f8x4 & (0x7f000000 >> i)) << i for i in [1, 17, 8, 24]]\n+        signs = (s[0] >> 0) | (s[1] >> 8) | (s[2] >> 16) | (s[3] >> 24)\n+        bits = (b[0] >> 0) | (b[1] >> 8) | (b[2] >> 16) | (b[3] >> 24)\n+        return (signs | bits).view(np.int8)\n+    else:\n+        return np_data\n \n \n-@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n+@pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4b15x4, tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\""}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 26, "deletions": 28, "changes": 54, "file_content_changes": "@@ -58,14 +58,10 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                # split-k\n-                (64, 64, 16, 2, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # variable input\n                 (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE),\n                 (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n@@ -77,9 +73,6 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE),\n-                # split-k\n-                (64, 64, 16, 8, 4, STAGES, 128, 128, 768, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 8, 4, STAGES, 128, 128, 32, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [4]\n         ],\n         # mixed-precision\n@@ -88,7 +81,6 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n-                (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n             ] for ADTYPE, BDTYPE in [(\"float8e4\", \"float8e5\"),\n                                      (\"float8e4\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n@@ -131,35 +123,44 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n-    a_fp8 = \"float8\" in ADTYPE\n-    b_fp8 = \"float8\" in BDTYPE\n \n     def maybe_upcast(x, dtype, is_float8):\n         if is_float8:\n             return f8_to_f16(x, dtype)\n         return x\n \n-    def init_input(n, m, t, dtype, is_float8):\n-        if t:\n-            return init_input(m, n, False, dtype, is_float8).t()\n-        if is_float8:\n-            return torch.randint(20, 50, (n, m), device=\"cuda\", dtype=torch.int8)\n+    def init_input(m, n, dtype):\n+        if 'float8' in dtype:\n+            ewidth = {'float8e4b15': 4, 'float8e4': 4, 'float8e5': 5}[dtype]\n+            sign = torch.randint(2, size=(m, n), device=\"cuda\", dtype=torch.int8) * 128\n+            val = torch.randint(2**3 - 1, size=(m, n), device=\"cuda\", dtype=torch.int8) << 7 - ewidth\n+            return sign | val\n         if dtype == \"int8\":\n-            return torch.randint(-128, 127, (n, m), device=\"cuda\", dtype=torch.int8)\n+            return torch.randint(-128, 127, (m, n), device=\"cuda\", dtype=torch.int8)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n-        return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n+        exponents = torch.randint(-10, 0, size=(m, n))\n+        ret = (2. ** exponents).to(dtype).to(\"cuda\")\n+        return ret\n \n     # allocate/transpose inputs\n-    a = init_input(M, K, AT, ADTYPE, a_fp8)\n-    b = init_input(K, N, BT, BDTYPE, b_fp8)\n+    a = init_input(M, K, ADTYPE)\n+    b = init_input(K, N, BDTYPE)\n+    a = a if not AT else a.T.contiguous().T\n+    b = b if not BT else b.T.contiguous().T\n     # run test\n-    th_a = maybe_upcast(a, ADTYPE, a_fp8).to(torch.float32)\n+    a_fp8 = \"float8\" in ADTYPE\n+    b_fp8 = \"float8\" in BDTYPE\n+    th_a = maybe_upcast(a, ADTYPE, a_fp8)\n     if AT and a_fp8:\n         th_a = th_a.view(th_a.shape[::-1]).T\n-    th_b = maybe_upcast(b, BDTYPE, b_fp8).to(torch.float32)\n+    th_b = maybe_upcast(b, BDTYPE, b_fp8)\n     if BT and b_fp8:\n         th_b = th_b.view(th_b.shape[::-1]).T\n-    th_c = torch.matmul(th_a, th_b)\n+    if th_a.is_floating_point():\n+        ab_dtype = th_a.dtype if th_a.element_size() > th_b.element_size() else th_b.dtype\n+    else:\n+        ab_dtype = torch.float32\n+    th_c = torch.matmul(th_a.to(ab_dtype), th_b.to(ab_dtype))\n     if ADTYPE == \"int8\" or BDTYPE == \"int8\":\n         th_c = th_c.to(torch.int8)\n     try:\n@@ -168,9 +169,6 @@ def init_input(n, m, t, dtype, is_float8):\n         if b_fp8:\n             b = triton.reinterpret(b, getattr(tl, BDTYPE))\n         tt_c = triton.ops.matmul(a, b)\n-        atol, rtol = 1e-2, 0\n-        if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:\n-            atol, rtol = 3.5e-2, 0\n-        torch.testing.assert_allclose(th_c, tt_c, atol=atol, rtol=rtol)\n+        torch.testing.assert_allclose(th_c, tt_c, atol=0, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1065,6 +1065,7 @@ def str_to_ty(name):\n         \"fp8e4\": language.float8e4,\n         \"fp8e5\": language.float8e5,\n         \"fp8e4b15\": language.float8e4b15,\n+        \"fp8e4b15x4\": language.float8e4b15x4,\n         \"fp16\": language.float16,\n         \"bf16\": language.bfloat16,\n         \"fp32\": language.float32,"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 22, "deletions": 23, "changes": 45, "file_content_changes": "@@ -11,10 +11,9 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-import triton\n-import triton._C.libtriton.triton as _triton\n-from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n-                                   get_num_warps, get_shared_memory_size, ir,\n+from .._C.libtriton.triton import (ClusterInfo, TMAInfos, add_external_libs,\n+                                   compile_ptx_to_cubin, get_env_vars, get_num_warps,\n+                                   get_shared_memory_size, ir,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,\n                                    translate_triton_gpu_to_llvmir)\n from ..common.backend import get_backend, path_to_ptxas\n@@ -101,8 +100,8 @@ def optimize_ttgir(mod, num_stages, num_warps, num_ctas, arch,\n     if arch // 10 >= 9 and enable_warp_specialization and num_warps == 4:\n         pm.add_tritongpu_ws_feasibility_checking_pass(arch)\n         pm.run(mod)\n-        ws_enabled = _triton.ir.is_ws_supported(mod)\n-        pm = _triton.ir.pass_manager(mod.context)\n+        ws_enabled = ir.is_ws_supported(mod)\n+        pm = ir.pass_manager(mod.context)\n         pm.enable_debug()\n     if ws_enabled:\n         pm.add_tritongpu_wsdecomposing_pass(arch)\n@@ -120,11 +119,13 @@ def optimize_ttgir(mod, num_stages, num_warps, num_ctas, arch,\n     pm.add_tritongpu_optimize_dot_operands_pass()\n     pm.add_tritongpu_remove_layout_conversions_pass()\n     pm.add_tritongpu_decompose_conversions_pass()\n+    pm.add_tritongpu_ws_fixup_missing_attrs_pass()\n     pm.add_tritongpu_reorder_instructions_pass()\n     pm.add_cse_pass()\n     pm.add_symbol_dce_pass()\n     if arch // 10 >= 9:\n         pm.add_tritongpu_fence_insertion_pass()\n+    pm.add_tritongpu_ws_fixup_missing_attrs_pass()\n     pm.run(mod)\n     return mod\n \n@@ -424,12 +425,12 @@ def compile(fn, **kwargs):\n     if os.environ.get('OPTIMIZE_EPILOGUE', '') == '1':\n         optimize_epilogue = True\n     #\n-    cluster_info = _triton.ClusterInfo()\n+    cluster_info = ClusterInfo()\n     if \"clusterDims\" in kwargs:\n         cluster_info.clusterDimX = kwargs[\"clusterDims\"][0]\n         cluster_info.clusterDimY = kwargs[\"clusterDims\"][1]\n         cluster_info.clusterDimZ = kwargs[\"clusterDims\"][2]\n-    tma_infos = _triton.TMAInfos()\n+    tma_infos = TMAInfos()\n     # build compilation stages\n     stages = dict()\n     stages[\"ast\"] = (lambda path: fn, None)\n@@ -477,7 +478,7 @@ def compile(fn, **kwargs):\n         first_stage = list(stages.keys()).index(ir_name)\n \n     # create cache manager\n-    fn_cache_manager = get_cache_manager(make_hash(fn, arch, _triton.get_env_vars(), **kwargs))\n+    fn_cache_manager = get_cache_manager(make_hash(fn, arch, get_env_vars(), **kwargs))\n     # determine name and extension type of provided function\n     if isinstance(fn, JITFunction):\n         name, ext = fn.__name__, \"ast\"\n@@ -510,14 +511,13 @@ def compile(fn, **kwargs):\n                     \"constants\": _get_jsonable_constants(constants),\n                     \"debug\": debug,\n                     \"arch\": arch, }\n-        metadata.update(_triton.get_env_vars())\n+        metadata.update(get_env_vars())\n         if ext == \"ptx\":\n             assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n             metadata[\"shared\"] = kwargs[\"shared\"]\n \n     # Add device type to meta information\n     metadata[\"device_type\"] = device_type\n-    metadata[\"cache_key\"] = fn_cache_manager.key\n \n     first_stage = list(stages.keys()).index(ext)\n     asm = dict()\n@@ -556,20 +556,20 @@ def compile(fn, **kwargs):\n             asm[ir_name] = str(next_module)\n         if ir_name == \"llir\" and \"shared\" not in metadata:\n             metadata[\"shared\"] = get_shared_memory_size(module)\n-        if ir_name == \"ttgir\" and enable_warp_specialization:\n-            metadata[\"num_warps\"] = get_num_warps(module)\n+        if ir_name == \"ttgir\":\n+            metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n+            if metadata[\"enable_warp_specialization\"]:\n+                metadata[\"num_warps\"] = get_num_warps(next_module)\n         if ir_name == \"ptx\":\n             metadata[\"name\"] = get_kernel_name(next_module, pattern='// .globl')\n         if ir_name == \"amdgcn\":\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n-        if ir_name == \"ttgir\":\n-            metadata[\"enable_warp_specialization\"] = _triton.ir.is_ws_supported(next_module)\n         if not is_cuda and not is_hip:\n             _device_backend.add_meta_info(ir_name, module, next_module, metadata, asm)\n         module = next_module\n \n-    ids_of_folded_args = tuple([int(k) for k in configs[0].ids_of_folded_args]) if isinstance(fn, triton.runtime.JITFunction) else ()\n+    ids_of_folded_args = tuple([int(k) for k in configs[0].ids_of_folded_args]) if isinstance(fn, JITFunction) else ()\n     if \"clusterDims\" not in metadata:\n         metadata[\"clusterDims\"] = [\n             cluster_info.clusterDimX,\n@@ -584,10 +584,10 @@ def compile(fn, **kwargs):\n             metadata[\"tensormaps_info\"][i].ids_of_folded_args = ids_of_folded_args\n \n     ids_of_tensormaps = get_ids_of_tensormaps(metadata.get(\"tensormaps_info\", None))\n-    if isinstance(fn, triton.runtime.JITFunction) and \"tensormaps_info\" in metadata:\n+    if isinstance(fn, JITFunction) and \"tensormaps_info\" in metadata:\n         fn.tensormaps_info = metadata[\"tensormaps_info\"]\n \n-    ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, triton.runtime.JITFunction) else ()\n+    ids_of_const_exprs = tuple(fn.constexprs) if isinstance(fn, JITFunction) else ()\n     ids = {\"ids_of_tensormaps\": ids_of_tensormaps, \"ids_of_folded_args\": ids_of_folded_args, \"ids_of_const_exprs\": ids_of_const_exprs}\n     # cache manager\n     if is_cuda or is_hip:\n@@ -627,7 +627,6 @@ def __init__(self, fn, so_path, metadata, asm):\n         if \"tensormaps_info\" in metadata:\n             self.tensormaps_info = metadata[\"tensormaps_info\"]\n         self.constants = metadata[\"constants\"]\n-        self.cache_key = metadata[\"cache_key\"]\n         self.device_type = metadata[\"device_type\"]\n         self.device_backend = get_backend(self.device_type) if self.device_type not in [\"cuda\", \"hip\"] else None\n         # initialize asm dict\n@@ -674,22 +673,22 @@ def __getattribute__(self, name):\n         return super().__getattribute__(name)\n \n     # capture args and expand args with cutensormap*\n-    def assemble_tensormap_to_arg(self, args, constants):\n+    def assemble_tensormap_to_arg(self, args):\n         args_with_tma = list(args)\n         if hasattr(self, 'tensormaps_info'):\n             # tuple for hashable\n             args_ptr = tuple([arg.data_ptr() if hasattr(arg, 'data_ptr') else arg for arg in args])\n             for i, e in enumerate(self.tensormaps_info):\n-                args_with_tma.append(CompiledKernel.tensormap_manager[(self.cache_key, e, args_ptr)])\n+                args_with_tma.append(CompiledKernel.tensormap_manager[(e, args_ptr)])\n         return args_with_tma\n \n     def __getitem__(self, grid):\n         self._init_handles()\n \n         def runner(*args, stream=None):\n-            args_expand = self.assemble_tensormap_to_arg(args, self.constants)\n+            args_expand = self.assemble_tensormap_to_arg(args)\n             if stream is None:\n-                if self.device_type in [\"cuda\", \"rocm\"]:\n+                if self.device_type in [\"cuda\", \"hip\"]:\n                     stream = get_cuda_stream()\n                 else:\n                     stream = get_backend(self.device_type).get_stream(None)"}, {"filename": "python/triton/compiler/make_launcher.py", "status": "modified", "additions": 28, "deletions": 2, "changes": 30, "file_content_changes": "@@ -250,6 +250,7 @@ def format_of(ty):\n #include \\\"cuda.h\\\"\n #include <stdbool.h>\n #include <Python.h>\n+#include <dlfcn.h>\n \n static inline void gpuAssert(CUresult code, const char *file, int line)\n {{\n@@ -267,9 +268,30 @@ def format_of(ty):\n \n #define CUDA_CHECK(ans) {{ gpuAssert((ans), __FILE__, __LINE__); }}\n \n+typedef CUresult (*cuLaunchKernelEx_t)(const CUlaunchConfig* config, CUfunction f, void** kernelParams, void** extra);\n+\n+static cuLaunchKernelEx_t getLaunchKernelExHandle() {{\n+  // Open the shared library\n+  void* handle = dlopen(\"libcuda.so\", RTLD_LAZY);\n+  if (!handle) {{\n+    PyErr_SetString(PyExc_RuntimeError, \"Failed to open libcuda.so\");\n+    return NULL;\n+  }}\n+  // Clear any existing error\n+  dlerror();\n+  cuLaunchKernelEx_t cuLaunchKernelExHandle = (cuLaunchKernelEx_t)dlsym(handle, \"cuLaunchKernelEx\");\n+  // Check for errors\n+  const char *dlsym_error = dlerror();\n+  if (dlsym_error) {{\n+    PyErr_SetString(PyExc_RuntimeError, \"Failed to retrieve cuLaunchKernelEx from libcuda.so\");\n+    return NULL;\n+  }}\n+  return cuLaunchKernelExHandle;\n+}}\n+\n static void _launch(int gridX, int gridY, int gridZ, int num_warps, int num_ctas, int clusterDimX, int clusterDimY, int clusterDimZ, int shared_memory, CUstream stream, CUfunction function{', ' + arg_decls if len(arg_decls) > 0 else ''}) {{\n   void *params[] = {{ {', '.join(f\"&arg{i}\" for i in params)} }};\n-  if(gridX*gridY*gridZ > 0){{\n+  if (gridX*gridY*gridZ > 0) {{\n     if (num_ctas == 1) {{\n       CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*num_warps, 1, 1, shared_memory, stream, params, 0));\n     }} else {{\n@@ -291,7 +313,11 @@ def format_of(ty):\n       config.hStream = stream;\n       config.attrs = launchAttr;\n       config.numAttrs = 2;\n-      CUDA_CHECK(cuLaunchKernelEx(&config, function, params, 0));\n+      static cuLaunchKernelEx_t cuLaunchKernelExHandle = NULL;\n+      if (cuLaunchKernelExHandle == NULL) {{\n+        cuLaunchKernelExHandle = getLaunchKernelExHandle();\n+      }}\n+      CUDA_CHECK(cuLaunchKernelExHandle(&config, function, params, 0));\n     }}\n   }}\n }}"}, {"filename": "python/triton/compiler/utils.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -283,7 +283,7 @@ def __getitem__(self, key: tuple):\n         if key in self.tensormaps_device:\n             return int(self.tensormaps_device[key])\n         else:\n-            (cache_key, e, args) = key\n+            (e, args) = key\n             t_tensormap = e.tensormap(args)\n             TENSORMAP_SIZE_IN_BYTES = 128\n             t_tensormap_device = driver.utils.cuMemAlloc(TENSORMAP_SIZE_IN_BYTES)"}, {"filename": "python/triton/hopper_lib/libhopper_helpers.bc", "status": "modified", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -50,6 +50,7 @@\n     float32,\n     float64,\n     float8e4b15,\n+    float8e4b15x4,\n     float8e4,\n     float8e5,\n     function_type,\n@@ -148,6 +149,7 @@\n     \"float32\",\n     \"float64\",\n     \"float8e4b15\",\n+    \"float8e4b15x4\",\n     \"float8e4\",\n     \"float8e5\",\n     \"full\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 25, "deletions": 15, "changes": 40, "file_content_changes": "@@ -76,7 +76,7 @@ def _to_tensor(x, builder):\n class dtype:\n     SINT_TYPES = ['int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['int1', 'uint8', 'uint16', 'uint32', 'uint64']\n-    FP_TYPES = ['fp8e4b15', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n+    FP_TYPES = ['fp8e4b15', 'fp8e4b15x4', 'fp8e4', 'fp8e5', 'fp16', 'bf16', 'fp32', 'fp64']\n     STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n@@ -100,6 +100,10 @@ def __init__(self, name):\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n                 self.exponent_bias = 15\n+            elif name == 'fp8e4b15x4':\n+                self.fp_mantissa_width = 3\n+                self.primitive_bitwidth = 8\n+                self.exponent_bias = 15\n             elif name == 'fp8e4':\n                 self.fp_mantissa_width = 3\n                 self.primitive_bitwidth = 8\n@@ -138,6 +142,9 @@ def is_fp8e4(self):\n     def is_fp8e4b15(self):\n         return self.name == 'fp8e4b15'\n \n+    def is_fp8e4b15x4(self):\n+        return self.name == 'fp8e4b15x4'\n+\n     def is_fp16(self):\n         return self.name == 'fp16'\n \n@@ -241,6 +248,8 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_fp8e4_ty()\n         elif self.name == 'fp8e4b15':\n             return builder.get_fp8e4b15_ty()\n+        elif self.name == 'fp8e4b15x4':\n+            return builder.get_fp8e4b15x4_ty()\n         elif self.name == 'fp16':\n             return builder.get_half_ty()\n         elif self.name == 'bf16':\n@@ -375,6 +384,7 @@ def to_ir(self, builder: ir.builder):\n float8e5 = dtype('fp8e5')\n float8e4 = dtype('fp8e4')\n float8e4b15 = dtype('fp8e4b15')\n+float8e4b15x4 = dtype('fp8e4b15x4')\n float16 = dtype('fp16')\n bfloat16 = dtype('bf16')\n float32 = dtype('fp32')\n@@ -1382,7 +1392,7 @@ def minimum(x, y):\n     :param other: the second input tensor\n     :type other: Block\n     \"\"\"\n-    return where(x < y, x, y)\n+    return math.min(x, y)\n \n \n @jit\n@@ -1395,7 +1405,7 @@ def maximum(x, y):\n     :param other: the second input tensor\n     :type other: Block\n     \"\"\"\n-    return where(x > y, x, y)\n+    return math.max(x, y)\n \n # max and argmax\n \n@@ -1422,11 +1432,6 @@ def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n     return _argmax_combine(value1, index1, value2, index2, False)\n \n \n-@jit\n-def _fast_max(x, y):\n-    return math.max(x, y)\n-\n-\n @jit\n @_add_reduction_docstr(\"maximum\",\n                        return_indices_arg=\"return_indices\",\n@@ -1445,7 +1450,7 @@ def max(input, axis=None, return_indices=False, return_indices_tie_break_left=Tr\n             else:\n                 assert input.dtype.is_integer_type()\n                 input = input.to(int32)\n-        return reduce(input, axis, _fast_max)\n+        return reduce(input, axis, maximum)\n \n \n @jit\n@@ -1479,11 +1484,6 @@ def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n     return _argmin_combine(value1, index1, value2, index2, False)\n \n \n-@jit\n-def _fast_min(x, y):\n-    return math.min(x, y)\n-\n-\n @jit\n @_add_reduction_docstr(\"minimum\",\n                        return_indices_arg=\"return_indices\",\n@@ -1502,7 +1502,7 @@ def min(input, axis=None, return_indices=False, return_indices_tie_break_left=Tr\n             else:\n                 assert input.dtype.is_integer_type()\n                 input = input.to(int32)\n-        return reduce(input, axis, _fast_min)\n+        return reduce(input, axis, minimum)\n \n \n @jit\n@@ -1926,6 +1926,16 @@ def extern_elementwise(lib_name: str, lib_path: str, args: list, arg_type_symbol\n     return dispatch(func, lib_name, lib_path, dispatch_args, arg_type_symbol_dict, ret_shape, is_pure, _builder)\n \n \n+def binary_op_type_legalization(lhs, rhs, builder):\n+    '''\n+        Convert both operands to a single common type\n+        :param lhs: the left operand\n+        :param rhs: the right operand\n+        :param builder: the builder\n+    '''\n+    return semantic.binary_op_type_checking_impl(lhs, rhs, builder)\n+\n+\n def extern(fn):\n     \"\"\"A decorator for external functions.\"\"\"\n     return builtin(fn)"}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 24, "deletions": 16, "changes": 40, "file_content_changes": "@@ -40,26 +40,34 @@ def byte_perm(arg0, arg1, arg2, _builder=None):\n \n @core.extern\n def min(arg0, arg1, _builder=None):\n-    return core.extern_elementwise(\"libdevice\", libdevice_path(), [arg0, arg1, ],\n-                                   {(core.dtype(\"int32\"), core.dtype(\"int32\"),): (\"__nv_min\", core.dtype(\"int32\")),\n-                                    (core.dtype(\"uint32\"), core.dtype(\"uint32\"),): (\"__nv_umin\", core.dtype(\"uint32\")),\n-                                    (core.dtype(\"int64\"), core.dtype(\"int64\"),): (\"__nv_llmin\", core.dtype(\"int64\")),\n-                                    (core.dtype(\"uint64\"), core.dtype(\"uint64\"),): (\"__nv_ullmin\", core.dtype(\"uint64\")),\n-                                    (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fminf\", core.dtype(\"fp32\")),\n-                                    (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fmin\", core.dtype(\"fp64\")),\n-                                    }, is_pure=True, _builder=_builder)\n+    arg0 = core._to_tensor(arg0, _builder)\n+    arg1 = core._to_tensor(arg1, _builder)\n+    arg0, arg1 = core.binary_op_type_legalization(arg0, arg1, _builder)\n+    dtype = arg0.dtype\n+    if dtype.is_floating():\n+        return core.tensor(_builder.create_minf(arg0.handle, arg1.handle), arg0.type)\n+    elif dtype.is_int_signed():\n+        return core.tensor(_builder.create_minsi(arg0.handle, arg1.handle), arg0.type)\n+    elif dtype.is_int_unsigned():\n+        return core.tensor(_builder.create_minui(arg0.handle, arg1.handle), arg0.dtype)\n+    else:\n+        assert False, f\"Unexpected dtype {dtype}\"\n \n \n @core.extern\n def max(arg0, arg1, _builder=None):\n-    return core.extern_elementwise(\"libdevice\", libdevice_path(), [arg0, arg1, ],\n-                                   {(core.dtype(\"int32\"), core.dtype(\"int32\"),): (\"__nv_max\", core.dtype(\"int32\")),\n-                                    (core.dtype(\"uint32\"), core.dtype(\"uint32\"),): (\"__nv_umax\", core.dtype(\"uint32\")),\n-                                    (core.dtype(\"int64\"), core.dtype(\"int64\"),): (\"__nv_llmax\", core.dtype(\"int64\")),\n-                                    (core.dtype(\"uint64\"), core.dtype(\"uint64\"),): (\"__nv_ullmax\", core.dtype(\"uint64\")),\n-                                    (core.dtype(\"fp32\"), core.dtype(\"fp32\"),): (\"__nv_fmaxf\", core.dtype(\"fp32\")),\n-                                    (core.dtype(\"fp64\"), core.dtype(\"fp64\"),): (\"__nv_fmax\", core.dtype(\"fp64\")),\n-                                    }, is_pure=True, _builder=_builder)\n+    arg0 = core._to_tensor(arg0, _builder)\n+    arg1 = core._to_tensor(arg1, _builder)\n+    arg0, arg1 = core.binary_op_type_legalization(arg0, arg1, _builder)\n+    dtype = arg0.dtype\n+    if dtype.is_floating():\n+        return core.tensor(_builder.create_maxf(arg0.handle, arg1.handle), arg0.type)\n+    elif dtype.is_int_signed():\n+        return core.tensor(_builder.create_maxsi(arg0.handle, arg1.handle), arg0.type)\n+    elif dtype.is_int_unsigned():\n+        return core.tensor(_builder.create_maxui(arg0.handle, arg1.handle), arg0.dtype)\n+    else:\n+        assert False, f\"Unexpected dtype {dtype}\"\n \n \n @core.extern"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -55,7 +55,7 @@ def randint(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     using `randint4x` is likely to be faster than calling `randint` 4 times.\n \n     :param seed: The seed for generating random numbers.\n-    :param offsets: The offsets to generate random numbers for.\n+    :param offset: The offsets to generate random numbers for.\n     \"\"\"\n     ret, _, _, _ = randint4x(seed, offset, n_rounds)\n     return ret\n@@ -120,7 +120,7 @@ def rand(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n def rand4x(seed, offsets, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offsets` block,\n-    returns a 4 blocks of random :code:`float32` in :math:`U(0, 1)`.\n+    returns 4 blocks of random :code:`float32` in :math:`U(0, 1)`.\n \n     :param seed: The seed for generating random numbers.\n     :param offsets: The offsets to generate random numbers for.\n@@ -167,7 +167,7 @@ def randn(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n def randn4x(seed, offset, n_rounds: tl.constexpr = N_ROUNDS_DEFAULT):\n     \"\"\"\n     Given a :code:`seed` scalar and an :code:`offset` block,\n-    returns a 4 blocks of random :code:`float32` in :math:`\\\\mathcal{N}(0, 1)`.\n+    returns 4 blocks of random :code:`float32` in :math:`\\\\mathcal{N}(0, 1)`.\n \n     :param seed: The seed for generating random numbers.\n     :param offsets: The offsets to generate random numbers for."}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -192,7 +192,7 @@ def truediv(input: tl.tensor,\n     elif input_scalar_ty.is_int() and other_scalar_ty.is_int():\n         input = cast(input, tl.float32, builder)\n         other = cast(other, tl.float32, builder)\n-    # float / float (cast to highest exponent type)\n+    # float / float (cast to the highest exponent type)\n     elif input_scalar_ty.is_floating() and other_scalar_ty.is_floating():\n         if input_scalar_ty.fp_mantissa_width > other_scalar_ty.fp_mantissa_width:\n             other = cast(other, input_scalar_ty, builder)\n@@ -979,7 +979,7 @@ def _store_block_pointer(ptr, val, mask, boundary_check, cache, eviction, builde\n     if not val.type.is_block():\n         val = broadcast_impl_shape(val, block_shape, builder)\n     assert val.type.is_block(), \"Value argument must be block type or a scalar\"\n-    assert block_shape == val.type.get_block_shapes(), \"Block shape and value shape mismatch\"\n+    assert block_shape == val.type.get_block_shapes(), f\"Block shape({block_shape}) and value shape({val.type.get_block_shapes()}) mismatch\"\n     assert ptr.type.element_ty.element_ty == val.type.element_ty, f\"Block element type({ptr.type.element_ty.element_ty}) and value element type({val.type.element_ty}) mismatch\"\n \n     elt_ty = ptr.type.element_ty.element_ty\n@@ -1364,7 +1364,7 @@ def wrap_tensor(x, scalar_ty):\n \n def _check_dtype(dtypes: List[str]) -> T:\n     \"\"\"\n-    We following libdevice's convention to check accepted data types for math functions.\n+    We're following libdevice's convention to check accepted data types for math functions.\n     It is not a good practice to support all data types as accelerators/GPUs don't support\n     many float16 and bfloat16 math operations.\n     We should let the users know that they are using and invoke explicit cast to convert"}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "modified", "additions": 39, "deletions": 5, "changes": 44, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"cuda.h\"\n+#include <dlfcn.h>\n #define PY_SSIZE_T_CLEAN\n #include <Python.h>\n \n@@ -338,6 +339,36 @@ static cuuint32_t *list_to_cuuint32_array(PyObject *listObj) {\n   return array;\n }\n \n+typedef CUresult (*cuTensorMapEncodeTiled_t)(\n+    CUtensorMap *tensorMap, CUtensorMapDataType tensorDataType,\n+    cuuint32_t tensorRank, void *globalAddress, const cuuint64_t *globalDim,\n+    const cuuint64_t *globalStrides, const cuuint32_t *boxDim,\n+    const cuuint32_t *elementStrides, CUtensorMapInterleave interleave,\n+    CUtensorMapSwizzle swizzle, CUtensorMapL2promotion l2Promotion,\n+    CUtensorMapFloatOOBfill oobFill);\n+\n+static cuTensorMapEncodeTiled_t getCuTensorMapEncodeTiledHandle() {\n+  // Open the shared library\n+  void *handle = dlopen(\"libcuda.so\", RTLD_LAZY);\n+  if (!handle) {\n+    PyErr_SetString(PyExc_RuntimeError, \"Failed to open libcuda.so\");\n+    return NULL;\n+  }\n+  // Clear any existing error\n+  dlerror();\n+  cuTensorMapEncodeTiled_t cuTensorMapEncodeTiledHandle =\n+      (cuTensorMapEncodeTiled_t)dlsym(handle, \"cuTensorMapEncodeTiled\");\n+  // Check for errors\n+  const char *dlsym_error = dlerror();\n+  if (dlsym_error) {\n+    PyErr_SetString(\n+        PyExc_RuntimeError,\n+        \"Failed to retrieve cuTensorMapEncodeTiled from libcuda.so\");\n+    return NULL;\n+  }\n+  return cuTensorMapEncodeTiledHandle;\n+}\n+\n static PyObject *tensorMapEncodeTiled(PyObject *self, PyObject *args) {\n   CUtensorMap *tensorMap = (CUtensorMap *)malloc(sizeof(CUtensorMap));\n   CUtensorMapDataType tensorDataType;\n@@ -364,18 +395,21 @@ static PyObject *tensorMapEncodeTiled(PyObject *self, PyObject *args) {\n   cuuint32_t *boxDim = list_to_cuuint32_array(boxDimObj);\n   cuuint32_t *elementStrides = list_to_cuuint32_array(elementStridesObj);\n \n+  static cuTensorMapEncodeTiled_t cuTensorMapEncodeTiledHandle = NULL;\n+  if (cuTensorMapEncodeTiledHandle == NULL) {\n+    cuTensorMapEncodeTiledHandle = getCuTensorMapEncodeTiledHandle();\n+  }\n   // Call the function\n-  CUDA_CHECK(cuTensorMapEncodeTiled(tensorMap, tensorDataType, tensorRank,\n-                                    globalAddress, globalDim, globalStrides,\n-                                    boxDim, elementStrides, interleave, swizzle,\n-                                    l2Promotion, oobFill));\n+  CUDA_CHECK(cuTensorMapEncodeTiledHandle(\n+      tensorMap, tensorDataType, tensorRank, globalAddress, globalDim,\n+      globalStrides, boxDim, elementStrides, interleave, swizzle, l2Promotion,\n+      oobFill));\n \n   // Clean up\n   free(globalDim);\n   free(globalStrides);\n   free(boxDim);\n   free(elementStrides);\n-\n   // Return the tensor map as a normal pointer\n   return PyLong_FromUnsignedLongLong((unsigned long long)tensorMap);\n }"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 18, "deletions": 16, "changes": 34, "file_content_changes": "@@ -11,7 +11,7 @@\n from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n                     overload)\n \n-import triton\n+from .._C.libtriton.triton import TMAInfos\n from ..common.backend import get_backend, path_to_ptxas\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n@@ -60,7 +60,7 @@ class DependenciesFinder(ast.NodeVisitor):\n \n     def __init__(self, globals, src) -> None:\n         super().__init__()\n-        self.ret = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n+        self.ret = hashlib.sha1(src.encode(\"utf-8\")).hexdigest()\n         self.globals = globals\n \n     def visit_Name(self, node):\n@@ -90,7 +90,7 @@ def visit_Call(self, node):\n             func.hash = finder.ret\n         noinline = str(getattr(func, 'noinline', False))\n         self.ret = (self.ret + func.hash + noinline).encode(\"utf-8\")\n-        self.ret = hashlib.md5(self.ret).hexdigest()\n+        self.ret = hashlib.sha1(self.ret).hexdigest()\n \n # -----------------------------------------------------------------------------\n # JITFunction\n@@ -103,23 +103,29 @@ def version_key():\n     contents = []\n     # frontend\n     with open(__file__, \"rb\") as f:\n-        contents += [hashlib.md5(f.read()).hexdigest()]\n+        contents += [hashlib.sha1(f.read()).hexdigest()]\n     # compiler\n     compiler_path = os.path.join(TRITON_PATH, 'compiler')\n     for lib in pkgutil.iter_modules([compiler_path]):\n         with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n-            contents += [hashlib.md5(f.read()).hexdigest()]\n+            contents += [hashlib.sha1(f.read()).hexdigest()]\n     # backend\n+    libtriton_hash = hashlib.sha1()\n     with open(os.path.join(TRITON_PATH, \"_C/libtriton.so\"), \"rb\") as f:\n-        contents += [hashlib.md5(f.read()).hexdigest()]\n+        while True:\n+            chunk = f.read(1024 ** 2)\n+            if not chunk:\n+                break\n+            libtriton_hash.update(chunk)\n+    contents.append(libtriton_hash.hexdigest())\n     # language\n     language_path = os.path.join(TRITON_PATH, 'language')\n     for lib in pkgutil.iter_modules([language_path]):\n         with open(lib.module_finder.find_spec(lib.name).origin, \"rb\") as f:\n-            contents += [hashlib.md5(f.read()).hexdigest()]\n+            contents += [hashlib.sha1(f.read()).hexdigest()]\n     # ptxas version\n     ptxas = path_to_ptxas()[0]\n-    ptxas_version = hashlib.md5(subprocess.check_output([ptxas, \"--version\"])).hexdigest()\n+    ptxas_version = hashlib.sha1(subprocess.check_output([ptxas, \"--version\"])).hexdigest()\n     return '-'.join(TRITON_VERSION) + '-' + ptxas_version + '-' + '-'.join(contents)\n \n \n@@ -242,6 +248,7 @@ def _type_of(key):\n             \"float8e4\": \"fp8e4\",\n             \"float8e5\": \"fp8e5\",\n             \"float8e4b15\": \"fp8e4b15\",\n+            \"float8e4b15x4\": \"fp8e4b15x4\",\n             \"float16\": \"fp16\",\n             \"bfloat16\": \"bf16\",\n             \"float32\": \"fp32\",\n@@ -400,13 +407,8 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_s\n     if bin is not None:\n       # build dict of constant values\n       args = [{args}]\n-      all_args = {', '.join([f'{arg}' for arg in self.arg_names]) + ', ' if len(self.arg_names) > 0 else ()}\n-      configs = self._get_config(*all_args),\n-      constants = self._make_constants(constexpr_key)\n-      constants.update({{i: None for i, arg in enumerate(all_args) if arg is None}})\n-      constants.update({{i: 1 for i in configs[0].equal_to_1}})\n       # Create tensormaps and append to args\n-      args = bin.assemble_tensormap_to_arg(args, constants)\n+      args = bin.assemble_tensormap_to_arg(args)\n       if not warmup:\n           bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.num_ctas, bin.clusterDims[0], bin.clusterDims[1], bin.clusterDims[2], bin.shared, stream, bin.cu_function, CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, bin, *args)\n       return bin\n@@ -428,7 +430,7 @@ def {self.fn.__name__}({args_signature}grid=None, num_warps=4, num_ctas=1, num_s\n       if not self._call_hook(key, signature, device, constants, num_warps, num_ctas, num_stages, enable_warp_specialization, extern_libs, configs):\n         bin = compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_ctas=num_ctas, num_stages=num_stages, enable_warp_specialization=enable_warp_specialization, extern_libs=extern_libs, configs=configs, debug=self.debug, device_type=device_type)\n         # Create tensormaps and append to args\n-        args = bin.assemble_tensormap_to_arg(args, constants)\n+        args = bin.assemble_tensormap_to_arg(args)\n         if not warmup:\n             bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.num_ctas, bin.clusterDims[0], bin.clusterDims[1], bin.clusterDims[2], bin.shared, stream, bin.cu_function, CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, bin, *args)\n         self.cache[device][key] = bin\n@@ -479,7 +481,7 @@ def __init__(self, fn, version=None, do_not_specialize=None, debug=None, noinlin\n         # index of constexprs\n         self.constexprs = [self.arg_names.index(name) for name, ty in self.__annotations__.items() if 'constexpr' in ty]\n         # tma info\n-        self.tensormaps_info = triton._C.libtriton.triton.TMAInfos()\n+        self.tensormaps_info = TMAInfos()\n         # launcher\n         self.run = self._make_launcher()\n         # re-use docs of wrapped function"}, {"filename": "python/triton/third_party/cuda/bin/ptxas", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/tools/compile.py", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -43,6 +43,7 @@\n     parser.add_argument(\"path\", help=\"Path to Python source containing desired kernel in its scope. File will be executed.\")\n     parser.add_argument(\"--kernel-name\", \"-n\", type=str, default=\"\", help=\"Name of the kernel to compile\", required=True)\n     parser.add_argument(\"--num-warps\", \"-w\", type=int, default=1, help=\"Number of warps to launch the kernel\")\n+    parser.add_argument(\"--num-stages\", \"-ns\", type=int, default=3, help=\"Number of stages meta-parameter for the kernel\")\n     parser.add_argument(\"--out-name\", \"-on\", type=str, default=None, help=\"Out name for the compiled kernel\")\n     parser.add_argument(\"--out-path\", \"-o\", type=Path, default=None, help=\"Out filename\")\n     parser.add_argument(\"--signature\", \"-s\", type=str, help=\"Signature of the kernel\", required=True)\n@@ -96,7 +97,7 @@ def constexpr(s):\n     config = triton.compiler.instance_descriptor(divisible_by_16=divisible_by_16, equal_to_1=equal_to_1)\n     for i in equal_to_1:\n         constexprs.update({i: 1})\n-    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=args.num_warps)\n+    ccinfo = triton.compile(kernel, signature=signature, constants=constexprs, configs=[config], num_warps=args.num_warps, num_stages=args.num_stages)\n     arg_names = []\n     arg_types = []\n     for i in signature.keys():"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 41, "deletions": 38, "changes": 79, "file_content_changes": "@@ -32,33 +32,34 @@ def _fwd_kernel(\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n     stride_oz, stride_oh, stride_om, stride_on,\n-    Z, H, N_CTX,\n+    Z, H, N_CTX, P_SEQ,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n     IS_CAUSAL: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n-    qvk_offset = off_hz * stride_qh\n+    q_offset = off_hz * stride_qh\n+    kv_offset = off_hz * stride_kh\n     Q_block_ptr = tl.make_block_ptr(\n-        base=Q + qvk_offset,\n+        base=Q + q_offset,\n         shape=(N_CTX, BLOCK_DMODEL),\n         strides=(stride_qm, stride_qk),\n         offsets=(start_m * BLOCK_M, 0),\n         block_shape=(BLOCK_M, BLOCK_DMODEL),\n         order=(1, 0)\n     )\n     K_block_ptr = tl.make_block_ptr(\n-        base=K + qvk_offset,\n-        shape=(BLOCK_DMODEL, N_CTX),\n+        base=K + kv_offset,\n+        shape=(BLOCK_DMODEL, N_CTX + P_SEQ),\n         strides=(stride_kk, stride_kn),\n         offsets=(0, 0),\n         block_shape=(BLOCK_DMODEL, BLOCK_N),\n         order=(0, 1)\n     )\n     V_block_ptr = tl.make_block_ptr(\n-        base=V + qvk_offset,\n-        shape=(N_CTX, BLOCK_DMODEL),\n+        base=V + kv_offset,\n+        shape=(N_CTX + P_SEQ, BLOCK_DMODEL),\n         strides=(stride_vk, stride_vn),\n         offsets=(0, 0),\n         block_shape=(BLOCK_N, BLOCK_DMODEL),\n@@ -80,16 +81,16 @@ def _fwd_kernel(\n     q = (q * qk_scale).to(tl.float16)\n     # loop over k, v and update accumulator\n     lo = 0\n-    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n+    hi = P_SEQ + (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX + P_SEQ\n     for start_n in range(lo, hi, BLOCK_N):\n         # -- load k, v --\n         k = tl.load(K_block_ptr)\n         v = tl.load(V_block_ptr)\n         # -- compute qk ---\n-        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float16)\n         if IS_CAUSAL:\n-            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        qk += tl.dot(q, k)\n+            qk = tl.where(P_SEQ + offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n+        qk += tl.dot(q, k, out_dtype=tl.float16)\n         # -- compute scaling constant ---\n         m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n         alpha = tl.math.exp2(m_i - m_i_new)\n@@ -110,7 +111,7 @@ def _fwd_kernel(\n     tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n     # write back O\n     O_block_ptr = tl.make_block_ptr(\n-        base=Out + qvk_offset,\n+        base=Out + q_offset,\n         shape=(N_CTX, BLOCK_DMODEL),\n         strides=(stride_om, stride_on),\n         offsets=(start_m * BLOCK_M, 0),\n@@ -146,8 +147,8 @@ def _bwd_kernel(\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n-    Z, H, N_CTX,\n-    num_block,\n+    Z, H, N_CTX, P_SEQ,\n+    num_block_q, num_block_kv,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n     CAUSAL: tl.constexpr,\n@@ -158,15 +159,15 @@ def _bwd_kernel(\n     qk_scale = sm_scale * 1.44269504\n     # offset pointers for batch/head\n     Q += off_z * stride_qz + off_h * stride_qh\n-    K += off_z * stride_qz + off_h * stride_qh\n-    V += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_kz + off_h * stride_kh\n+    V += off_z * stride_vz + off_h * stride_vh\n     DO += off_z * stride_qz + off_h * stride_qh\n     DQ += off_z * stride_qz + off_h * stride_qh\n-    DK += off_z * stride_qz + off_h * stride_qh\n-    DV += off_z * stride_qz + off_h * stride_qh\n-    for start_n in range(0, num_block):\n+    DK += off_z * stride_kz + off_h * stride_kh\n+    DV += off_z * stride_vz + off_h * stride_vh\n+    for start_n in range(0, num_block_kv):\n         if CAUSAL:\n-            lo = start_n * BLOCK_M\n+            lo = tl.math.max(start_n * BLOCK_M - P_SEQ, 0)\n         else:\n             lo = 0\n         # initialize row/col offsets\n@@ -183,20 +184,20 @@ def _bwd_kernel(\n         # pointer to row-wise quantities in value-like data\n         D_ptrs = D + off_hz * N_CTX\n         l_ptrs = L + off_hz * N_CTX\n-        # initialize dv amd dk\n-        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        # initialize dk amd dv\n         dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n         # k and v stay in SRAM throughout\n         k = tl.load(k_ptrs)\n         v = tl.load(v_ptrs)\n         # loop over rows\n-        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n+        for start_m in range(lo, num_block_q * BLOCK_M, BLOCK_M):\n             offs_m_curr = start_m + offs_m\n             # load q, k, v, do on-chip\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             if CAUSAL:\n-                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+                qk = tl.where(P_SEQ + offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n             else:\n                 qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n             qk += tl.dot(q, tl.trans(k))\n@@ -223,10 +224,10 @@ def _bwd_kernel(\n             q_ptrs += BLOCK_M * stride_qm\n             do_ptrs += BLOCK_M * stride_qm\n         # write-back\n-        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n         dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n-        tl.store(dv_ptrs, dv)\n+        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n         tl.store(dk_ptrs, dk)\n+        tl.store(dv_ptrs, dv)\n \n \n empty = torch.empty(128, device=\"cuda\")\n@@ -242,11 +243,12 @@ def forward(ctx, q, k, v, causal, sm_scale):\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n         BLOCK_M = 128\n-        BLOCK_N = 64\n+        BLOCK_N = 64 if Lk <= 64 else 32\n+        num_stages = 4 if Lk <= 64 else 3\n+        num_warps = 4\n         grid = (triton.cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n-\n-        num_warps = 4 if Lk <= 64 else 8\n+        P_SEQ = 0 if q.shape[-2] == k.shape[-2] else k.shape[-2] - q.shape[-2]\n         _fwd_kernel[grid](\n             q, k, v, sm_scale,\n             L,\n@@ -255,17 +257,18 @@ def forward(ctx, q, k, v, causal, sm_scale):\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n+            q.shape[0], q.shape[1], q.shape[2], P_SEQ,\n             BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_DMODEL=Lk,\n             IS_CAUSAL=causal,\n             num_warps=num_warps,\n-            num_stages=4)\n+            num_stages=num_stages)\n \n         ctx.save_for_backward(q, k, v, o, L)\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n         ctx.causal = causal\n+        ctx.P_SEQ = P_SEQ\n         return o\n \n     @staticmethod\n@@ -290,8 +293,8 @@ def backward(ctx, do):\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n-            q.shape[0], q.shape[1], q.shape[2],\n-            ctx.grid[0],\n+            q.shape[0], q.shape[1], q.shape[2], ctx.P_SEQ,\n+            ctx.grid[0], triton.cdiv(k.shape[2], BLOCK),\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n             CAUSAL=ctx.causal,\n@@ -303,17 +306,17 @@ def backward(ctx, do):\n attention = _attention.apply\n \n \n-@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n+@pytest.mark.parametrize('Z, H, N_CTX, D_HEAD, P_SEQ', [(6, 9, 1024, 64, 128)])\n @pytest.mark.parametrize('causal', [False, True])\n-def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n+def test_op(Z, H, N_CTX, D_HEAD, P_SEQ, causal, dtype=torch.float16):\n     torch.manual_seed(20)\n     q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX + P_SEQ, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n     sm_scale = 0.5\n     dout = torch.randn_like(q)\n     # reference implementation\n-    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+    M = torch.tril(torch.ones((N_CTX, N_CTX + P_SEQ), device=\"cuda\"), diagonal=P_SEQ)\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n     if causal:\n         p[:, :, M == 0] = float(\"-inf\")"}, {"filename": "python/tutorials/09-experimental-tma-matrix-multiplication.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -1,3 +1,10 @@\n+\"\"\"\n+Matrix Multiplication with TMA (Experimental)\n+================================================\n+In this tutorial, you will write a very short high-performance multiplication kernel that achieves\n+performance on parallel with cuBLAS.\n+\"\"\"\n+\n # Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n #\n # Permission is hereby granted, free of charge, to any person obtaining"}, {"filename": "python/tutorials/10-experimental-tmastg-matrix-multiplication.py", "status": "modified", "additions": 12, "deletions": 29, "changes": 41, "file_content_changes": "@@ -1,3 +1,10 @@\n+\"\"\"\n+Matrix Multiplication with TMASTG (Experimental)\n+================================================\n+In this tutorial, you will write a very short high-performance multiplication kernel that achieves\n+performance on parallel with cuBLAS.\n+\"\"\"\n+\n # Copyright (c) 2023 NVIDIA Corporation & Affiliates. All rights reserved.\n #\n # Permission is hereby granted, free of charge, to any person obtaining\n@@ -84,31 +91,6 @@ def matmul_kernel(\n     tl.store(c_block_ptr, accumulator)\n \n \n-def get_variant_golden(a, b):\n-    SIZE_M = a.shape[0]\n-    SIZE_K = a.shape[1]\n-    SIZE_N = b.shape[1]\n-    assert a.shape[1] == b.shape[0]\n-    zero_M_K = torch.zeros((SIZE_M, SIZE_K)).cuda()\n-    zero_3M_K = torch.zeros((3 * SIZE_M, SIZE_K)).cuda()\n-    zero_K_N = torch.zeros((SIZE_K, SIZE_N)).cuda()\n-    zero_3K_N = torch.zeros((3 * SIZE_K, SIZE_N)).cuda()\n-    a_padded = torch.cat((a, zero_M_K, zero_M_K), 0)\n-    a_padded = torch.cat((a_padded, zero_3M_K, zero_3M_K), 1)\n-    b_padded = torch.cat((b, zero_K_N, zero_K_N), 0)\n-    b_padded = torch.cat((b_padded, zero_3K_N, zero_3K_N), 1)\n-    c_padded = torch.matmul(a_padded, b_padded)\n-    return c_padded[:SIZE_M, :SIZE_N]\n-\n-\n-def get_proper_err(a, b, golden):\n-    golden_variant = get_variant_golden(a, b)\n-    golden_diff = golden - golden_variant\n-    golden_abs_err = torch.max(torch.abs(golden_diff)).item()\n-    golden_rel_err = torch.max(torch.abs(golden_diff / golden)).item()\n-    return (golden_abs_err, golden_rel_err)\n-\n-\n def matmul(a, b):\n     # checks constraints\n     assert a.shape[1] == b.shape[0], \"incompatible dimensions\"\n@@ -134,15 +116,16 @@ def grid(META):\n a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n b = torch.randn((512, 512), device='cuda', dtype=torch.float16).T\n c = matmul(a, b)\n+c = torch.nn.functional.normalize(c)\n+\n+golden = torch.nn.functional.normalize(torch.matmul(a, b))\n \n-golden = torch.matmul(a, b)\n-golden_abs_err, golden_rel_err = get_proper_err(a, b, golden)\n torch.set_printoptions(profile=\"full\")\n assert_close(\n     c,\n     golden,\n-    rtol=max(1e-4, 1.5 * golden_rel_err),\n-    atol=max(1e-4, 1.5 * golden_abs_err),\n+    rtol=1e-2,\n+    atol=1e-3,\n     check_dtype=False)\n \n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -1380,3 +1380,19 @@ module attributes {\"triton_gpu.compute-capability\" = 80 : i32, \"triton_gpu.num-c\n     tt.return\n   }\n }\n+\n+// -----\n+#blocked = #triton_gpu.blocked<{sizePerThread = [8, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 2], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n+#slice = #triton_gpu.slice<{dim = 1, parent = #blocked}>\n+module attributes {\"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 2 : i32} {\n+  // CHECK-LABEL: reduce_bools\n+  tt.func public @reduce_bools(%arg: tensor<256x2xi1, #blocked>) {\n+    // CHECK: llvm.mlir.addressof @global_smem\n+    %24 = \"tt.reduce\"(%arg) <{axis = 1 : i32}> ({\n+    ^bb0(%arg4: i1, %arg5: i1):\n+      %48 = arith.ori %arg4, %arg5 : i1\n+      tt.reduce.return %48 : i1\n+    }) : (tensor<256x2xi1, #blocked>) -> tensor<256xi1, #slice>\n+    tt.return\n+  }\n+}"}, {"filename": "test/NVGPU/test_cga.mlir", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_mbarrier() {\n@@ -7,24 +7,24 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.num-ctas\" = 2 :\n     %pred = arith.constant 1 : i1\n     %id0 = arith.constant 0 : i32\n     %id1 = arith.constant 1 : i32\n-    // CHECK: call void @__nv_cga_barrier_sync()\n-    // CHECK: call void @__nv_cga_barrier_arrive()\n-    // CHECK: call void @__nv_cga_barrier_wait()\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n     nvgpu.cga_barrier_sync\n     nvgpu.cga_barrier_arrive\n     nvgpu.cga_barrier_wait\n \n     %ptr = llvm.mlir.null : !llvm.ptr<i32, 3>\n \n-    // CHECK: %[[X:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_ctaid.x;\", \"=r\"()\n-    // CHECK: %[[Y:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_ctaid.y;\", \"=r\"()\n-    // CHECK: %[[Z:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_ctaid.z;\", \"=r\"()\n-    // CHECK: %[[NX:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_nctaid.x;\", \"=r\"()\n-    // CHECK: %[[NY:.+]] = tail call i32 asm \"mov.u32 $0, %cluster_nctaid.y;\", \"=r\"()\n-    // CHECK: %[[A0:.+]] = mul i32 %[[NY]], %[[Z]]\n-    // CHECK: %[[A1:.+]] = add i32 %[[A0]], %[[Y]]\n-    // CHECK: %[[A2:.+]] = mul i32 %[[A1]], %[[NX]]\n-    // CHECK: add i32 %[[A2]], %[[X]]\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.inline_asm\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.add\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.add\n     %v = nvgpu.cluster_id\n     llvm.store %v, %ptr : !llvm.ptr<i32, 3>\n "}, {"filename": "test/NVGPU/test_mbarrier.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -1,18 +1,18 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_mbarrier() {\n     %mbarrier = llvm.mlir.null : !llvm.ptr<i64, 3>\n     %pred = arith.constant 1 : i1\n-    // CHECK: call void @__nv_mbarrier_init\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_init %mbarrier, %pred { count = 32 : i32 } : !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_arrive_cp_async\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_arrive %mbarrier, %pred {arriveType = 1 : i32}: !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_arrive_normal\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_arrive %mbarrier, %pred {arriveType = 0 : i32}: !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_arrive_expect_tx\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_arrive %mbarrier, %pred {arriveType = 2 : i32, txCount = 128 : i32}: !llvm.ptr<i64, 3>\n-    // CHECK: call void @__nv_mbarrier_wait\n+    // CHECK: llvm.inline_asm\n     nvgpu.mbarrier_wait %mbarrier, %pred : !llvm.ptr<i64, 3>, i1\n     tt.return\n   }"}, {"filename": "test/NVGPU/test_tma.mlir", "status": "modified", "additions": 6, "deletions": 24, "changes": 30, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_tma(%im2colOffsets0 : !llvm.struct<(i16, i16)>, %im2colOffsets1 : !llvm.struct<(i16, i16, i16)>) {\n@@ -14,33 +14,15 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2\n     %pred = arith.constant 1 : i1\n     %mask = arith.constant 15 : i16\n \n-    // CHECK: void @__nv_tma_load_tiled_2d\n-    // CHECK: void @__nv_tma_load_tiled_3d\n-    // CHECK: void @__nv_tma_load_tiled_4d\n-    // CHECK: void @__nv_tma_load_tiled_5d\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n     nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 2, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 3, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32\n     nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 4, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3, %c4 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 5, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32, i32\n \n-    // CHECK: void @__nv_tma_load_tiled_mcast_2d\n-    // CHECK: void @__nv_tma_load_tiled_mcast_3d\n-    // CHECK: void @__nv_tma_load_tiled_mcast_4d\n-    // CHECK: void @__nv_tma_load_tiled_mcast_5d\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes.multicast::cluster.L2::cache_hint\n+    // CHECK: llvm.inline_asm {{.*}} cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx::bytes.L2::cache_hint\n     nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 2, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i16\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 3, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i16\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 4, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32, i16\n-    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3, %c4, %mask {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 5, 1>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32, i32, i16\n-\n-    // CHECK: tail call void @__nv_tma_load_im2col_4d\n-    // CHECK: tail call void @__nv_tma_load_im2col_5d\n-    // CHECK: tail call void @__nv_tma_load_im2col_mcast_4d\n-    // CHECK: tail call void @__nv_tma_load_im2col_mcast_5d\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets0, %pred, %c0, %c1, %c2, %c3 {mcastMask = 0 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16)>, i1, i32, i32, i32, i32\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets1, %pred, %c0, %c1, %c2, %c3, %c4 {mcastMask = 0 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16, i16)>, i1, i32, i32, i32, i32, i32\n-\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets0, %pred, %c0, %c1, %c2, %c3 {mcastMask = 1 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16)>, i1, i32, i32, i32, i32\n-    nvgpu.tma_load_im2col %dst, %mbarrier, %tmaDesc, %l2desc, %im2colOffsets1, %pred, %c0, %c1, %c2, %c3, %c4 {mcastMask = 1 : i16}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, !llvm.struct<(i16, i16, i16)>, i1, i32, i32, i32, i32, i32\n+    nvgpu.tma_load_tiled %dst, %mbarrier, %tmaDesc, %l2desc, %pred, %c0, %c1, %c2, %c3 {operand_segment_sizes = array<i32: 1, 1, 1, 1, 1, 4, 0>}: !llvm.ptr<i8, 3>, !llvm.ptr<i64, 3>, !llvm.ptr<i8, 1>, i64, i1, i32, i32, i32, i32\n \n     tt.return\n   }"}, {"filename": "test/NVGPU/test_wgmma.mlir", "status": "modified", "additions": 34, "deletions": 5, "changes": 39, "file_content_changes": "@@ -1,16 +1,45 @@\n-// RUN: triton-translate %s | FileCheck %s\n+// RUN: triton-opt %s -split-input-file --convert-nv-gpu-to-llvm | FileCheck %s\n #SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32,  \"triton_gpu.num-ctas\" = 2 : i32} {\n   tt.func @test_tma(%opC : !llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>) {\n     %buffer = llvm.mlir.null : !llvm.ptr<i64, 3>\n     %height = arith.constant 16 : i32\n-    // CHECK: call i64 @__nv_get_wgmma_desc\n+    // CHECK: llvm.ptrtoint\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.zext\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.or\n     %descA = nvgpu.wgmma_desc_create %buffer, %height {mode = 2 : i32}: (!llvm.ptr<i64, 3>, i32) -> (i64)\n-    // CHECK: call i64 @__nv_get_wgmma_desc\n+    // CHECK: llvm.ptrtoint\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.zext\n+    // CHECK: llvm.mul\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.lshr\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.shl\n+    // CHECK: llvm.or\n+    // CHECK: llvm.or\n     %descB = nvgpu.wgmma_desc_create %buffer, %height {mode = 2 : i32}: (!llvm.ptr<i64, 3>, i32) -> (i64)\n \n-    // CHECK: wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16\n-    %acc0 = nvgpu.wgmma %descA, %descA, %opC {m=64:i32, n=64:i32, k=16:i32, eltTypeC=7:i32, eltTypeA=4:i32, eltTypeB=4:i32, layoutA=0:i32, layoutB=0:i32} : (i64, i64, !llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>) -> (!llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>)\n+    // CHECK-COUNT-32: llvm.extractvalue\n+    // CHECK: llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"wgmma.mma_async.sync.aligned.m64n64k16.f32.f16.f16 {$0,$1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16,$17,$18,$19,$20,$21,$22,$23,$24,$25,$26,$27,$28,$29,$30,$31}, $64, $65, 1, 1, 1, 0, 1;\"\n+    %acc0 = nvgpu.wgmma %descA, %descB, %opC {m=64:i32, n=64:i32, k=16:i32, eltTypeC=7:i32, eltTypeA=4:i32, eltTypeB=4:i32, layoutA=0:i32, layoutB=0:i32} : (i64, i64, !llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>) -> (!llvm.struct<(f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32, f32)>)\n     tt.return\n   }\n } // end module"}, {"filename": "test/TritonGPU/rewrite-tensor-pointer.mlir", "status": "modified", "additions": 58, "deletions": 1, "changes": 59, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -tritongpu-rewrite-tensor-pointer | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-rewrite-tensor-pointer | FileCheck %s\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 2], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n@@ -62,3 +62,60 @@ module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-c\n     tt.return\n   }\n }\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 2], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0], CTAsPerCGA = [1], CTASplitNum = [1], CTAOrder = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0]}>\n+module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @if_for_if(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg4: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg5: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg6: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg7: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<64x1xf32, #blocked>\n+    %c63_i32 = arith.constant 63 : i32\n+    %c-16_i32 = arith.constant -16 : i32\n+    %c132_i32 = arith.constant 132 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %c1_i64 = arith.constant 1 : i64\n+    %c64_i32 = arith.constant 64 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = arith.addi %arg3, %c63_i32 : i32\n+    %2 = arith.divsi %1, %c64_i32 : i32\n+    %3 = arith.muli %0, %c64_i32 : i32\n+    %4 = arith.extsi %arg3 : i32 to i64\n+    %5 = arith.extsi %arg4 : i32 to i64\n+    %6 = arith.extsi %arg5 : i32 to i64\n+    // CHECK-NOT: tt.make_tensor_ptr\n+    %7 = tt.make_tensor_ptr %arg0, [%4, %5], [%6, %c1_i64], [%3, %c0_i32] {order = array<i32: 1, 0>} : <tensor<64x16xf16, #blocked>, 1>\n+    %8 = \"triton_gpu.cmpi\"(%2, %c132_i32) <{predicate = 5 : i64}> : (i32, i32) -> i1\n+    scf.if %8 {\n+      %9 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked1>\n+      %10 = tt.splat %arg7 : (i32) -> tensor<64x1xi32, #blocked>\n+      %11 = tt.splat %arg2 : (!tt.ptr<f32, 1>) -> tensor<64x1x!tt.ptr<f32, 1>, #blocked>\n+      %12 = scf.for %arg8 = %0 to %2 step %c132_i32 iter_args(%arg9 = %7) -> (!tt.ptr<tensor<64x16xf16, #blocked>, 1>)  : i32 {\n+        %13 = \"triton_gpu.cmpi\"(%arg8, %c132_i32) <{predicate = 5 : i64}> : (i32, i32) -> i1\n+        %14 = scf.if %13 -> (!tt.ptr<tensor<64x16xf16, #blocked>, 1>) {\n+          %25 = arith.subi %arg8, %0 : i32\n+          %26 = arith.muli %25, %c64_i32 : i32\n+          // CHECK-NOT: tt.advance\n+          %27 = tt.advance %arg9, [%26, %c-16_i32] : <tensor<64x16xf16, #blocked>, 1>\n+          scf.yield %27 : !tt.ptr<tensor<64x16xf16, #blocked>, 1>\n+        } else {\n+          scf.yield %arg9 : !tt.ptr<tensor<64x16xf16, #blocked>, 1>\n+        }\n+        %15 = arith.muli %arg8, %c64_i32 : i32\n+        %16 = tt.splat %15 : (i32) -> tensor<64xi32, #blocked1>\n+        %17 = arith.addi %9, %16 : tensor<64xi32, #blocked1>\n+        %18 = triton_gpu.convert_layout %17 : (tensor<64xi32, #blocked1>) -> tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+        %19 = tt.expand_dims %18 {axis = 1 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64x1xi32, #blocked2>\n+        %20 = triton_gpu.convert_layout %19 : (tensor<64x1xi32, #blocked2>) -> tensor<64x1xi32, #blocked>\n+        %21 = arith.muli %20, %10 : tensor<64x1xi32, #blocked>\n+        %22 = tt.addptr %11, %21 : tensor<64x1x!tt.ptr<f32, 1>, #blocked>, tensor<64x1xi32, #blocked>\n+        %23 = triton_gpu.convert_layout %22 : (tensor<64x1x!tt.ptr<f32, 1>, #blocked>) -> tensor<64x1x!tt.ptr<f32, 1>, #blocked>\n+        %24 = triton_gpu.convert_layout %cst : (tensor<64x1xf32, #blocked>) -> tensor<64x1xf32, #blocked>\n+        tt.store %23, %24 {cache = 1 : i32, evict = 1 : i32} : tensor<64x1xf32, #blocked>\n+        scf.yield %14 : !tt.ptr<tensor<64x16xf16, #blocked>, 1>\n+      }\n+    }\n+    tt.return\n+  }\n+}"}, {"filename": "test/TritonNvidiaGPU/ws-feasibility-checking.mlir", "status": "modified", "additions": 139, "deletions": 0, "changes": 139, "file_content_changes": "@@ -894,3 +894,142 @@ module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-c\n     tt.return\n   }\n }\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 2], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 4], order = [0, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 3, versionMinor = 0, warpsPerCTA = [4, 1], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [1, 0], instrShape = [16, 64, 16]}>\n+#shared = #triton_gpu.shared<{vec = 8, perPhase = 4, maxPhase = 2, order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1], hasLeadingOffset = true}>\n+#shared1 = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 8, order = [1, 0], CTAsPerCGA = [1, 1], CTASplitNum = [1, 1], CTAOrder = [0, 1], hasLeadingOffset = true}>\n+module attributes {\"triton_gpu.compute-capability\" = 90 : i32, \"triton_gpu.num-ctas\" = 1 : i32, \"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK: \"triton_gpu.enable-warp-specialization\" = 1 : i32\n+  // CHECK-LABEL: @epilogue_with_reduce\n+  tt.func public @epilogue_with_reduce(%arg0: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16, 1> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<f32, 1> {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg6: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg7: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg8: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg9: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}, %arg10: i32 {tt.divisibility = 16 : i32, tt.max_divisibility = 8 : i32}) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #mma>\n+    %c15_i32 = arith.constant 15 : i32\n+    %c63_i32 = arith.constant 63 : i32\n+    %c132_i32 = arith.constant 132 : i32\n+    %c16_i32 = arith.constant 16 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %c1_i64 = arith.constant 1 : i64\n+    %c64_i32 = arith.constant 64 : i32\n+    %c8_i32 = arith.constant 8 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = arith.addi %arg6, %c63_i32 : i32\n+    %2 = arith.divsi %1, %c64_i32 : i32\n+    %3 = arith.addi %arg5, %c63_i32 : i32\n+    %4 = arith.divsi %3, %c64_i32 : i32\n+    %5 = arith.muli %4, %2 : i32\n+    %6 = arith.muli %2, %c8_i32 : i32\n+    %7 = arith.divsi %0, %6 : i32\n+    %8 = arith.muli %7, %c8_i32 : i32\n+    %9 = arith.subi %4, %8 : i32\n+    %10 = \"triton_gpu.cmpi\"(%9, %c8_i32) {predicate = 2 : i64} : (i32, i32) -> i1\n+    %11 = arith.select %10, %9, %c8_i32 : i32\n+    %12 = arith.remsi %0, %6 : i32\n+    %13 = arith.remsi %12, %11 : i32\n+    %14 = arith.addi %8, %13 : i32\n+    %15 = arith.divsi %12, %11 : i32\n+    %16 = arith.muli %14, %c64_i32 : i32\n+    %17 = arith.muli %15, %c64_i32 : i32\n+    %18 = arith.extsi %arg5 : i32 to i64\n+    %19 = arith.extsi %arg7 : i32 to i64\n+    %20 = arith.extsi %arg8 : i32 to i64\n+    %21 = tt.make_tensor_ptr %arg0, [%18, %19], [%20, %c1_i64], [%16, %c0_i32] {order = array<i32: 1, 0>} : <tensor<64x16xf16, #blocked>, 1>\n+    %22 = arith.extsi %arg6 : i32 to i64\n+    %23 = arith.extsi %arg9 : i32 to i64\n+    %24 = tt.make_tensor_ptr %arg1, [%19, %22], [%23, %c1_i64], [%c0_i32, %17] {order = array<i32: 1, 0>} : <tensor<16x64xf16, #blocked1>, 1>\n+    %25 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+    %26 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %27 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+    %28 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %29 = tt.splat %arg10 : (i32) -> tensor<64x1xi32, #blocked2>\n+    %30 = tt.splat %arg4 : (!tt.ptr<f32, 1>) -> tensor<64x1x!tt.ptr<f32, 1>, #blocked2>\n+    %31 = tt.splat %arg5 : (i32) -> tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %32 = tt.splat %arg6 : (i32) -> tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+    %33 = arith.addi %arg7, %c15_i32 : i32\n+    %34 = arith.divsi %33, %c16_i32 : i32\n+    %35 = arith.subi %c0_i32, %34 : i32\n+    %36 = arith.muli %35, %c16_i32 : i32\n+    %37:4 = scf.for %arg11 = %0 to %5 step %c132_i32 iter_args(%arg12 = %21, %arg13 = %24, %arg14 = %14, %arg15 = %15) -> (!tt.ptr<tensor<64x16xf16, #blocked>, 1>, !tt.ptr<tensor<16x64xf16, #blocked1>, 1>, i32, i32)  : i32 {\n+      %38 = arith.divsi %arg11, %6 : i32\n+      %39 = arith.muli %38, %c8_i32 : i32\n+      %40 = arith.subi %4, %39 : i32\n+      %41 = \"triton_gpu.cmpi\"(%40, %c8_i32) {predicate = 2 : i64} : (i32, i32) -> i1\n+      %42 = arith.select %41, %40, %c8_i32 : i32\n+      %43 = arith.remsi %arg11, %6 : i32\n+      %44 = arith.remsi %43, %42 : i32\n+      %45 = arith.addi %39, %44 : i32\n+      %46 = arith.divsi %43, %42 : i32\n+      %47 = arith.muli %45, %c64_i32 : i32\n+      %48 = arith.muli %46, %c64_i32 : i32\n+      %49 = tt.splat %47 : (i32) -> tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %50 = tt.splat %47 : (i32) -> tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %51 = arith.addi %49, %26 : tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %52 = arith.addi %50, %28 : tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %53 = tt.splat %48 : (i32) -> tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+      %54 = tt.splat %48 : (i32) -> tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+      %55 = arith.addi %53, %25 : tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+      %56 = arith.addi %54, %27 : tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+      %57 = tt.expand_dims %51 {axis = 1 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64x1xi32, #blocked2>\n+      %58 = arith.muli %57, %29 : tensor<64x1xi32, #blocked2>\n+      %59 = tt.addptr %30, %58 : tensor<64x1x!tt.ptr<f32, 1>, #blocked2>, tensor<64x1xi32, #blocked2>\n+      %60 = tt.expand_dims %55 {axis = 0 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>) -> tensor<1x64xi32, #blocked2>\n+      %61 = tt.broadcast %59 : (tensor<64x1x!tt.ptr<f32, 1>, #blocked2>) -> tensor<64x64x!tt.ptr<f32, 1>, #blocked2>\n+      %62 = tt.broadcast %60 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n+      %63 = tt.addptr %61, %62 : tensor<64x64x!tt.ptr<f32, 1>, #blocked2>, tensor<64x64xi32, #blocked2>\n+      %64 = \"triton_gpu.cmpi\"(%52, %31) {predicate = 2 : i64} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>, tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64xi1, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %65 = tt.expand_dims %64 {axis = 1 : i32} : (tensor<64xi1, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64x1xi1, #blocked2>\n+      %66 = \"triton_gpu.cmpi\"(%56, %32) {predicate = 2 : i64} : (tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>, tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>) -> tensor<64xi1, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>\n+      %67 = tt.expand_dims %66 {axis = 0 : i32} : (tensor<64xi1, #triton_gpu.slice<{dim = 0, parent = #blocked2}>>) -> tensor<1x64xi1, #blocked2>\n+      %68 = tt.broadcast %65 : (tensor<64x1xi1, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+      %69 = tt.broadcast %67 : (tensor<1x64xi1, #blocked2>) -> tensor<64x64xi1, #blocked2>\n+      %70 = arith.andi %68, %69 : tensor<64x64xi1, #blocked2>\n+      %71 = arith.subi %45, %arg14 : i32\n+      %72 = arith.muli %71, %c64_i32 : i32\n+      %73 = tt.advance %arg12, [%72, %c0_i32] : <tensor<64x16xf16, #blocked>, 1>\n+      %74 = arith.subi %46, %arg15 : i32\n+      %75 = arith.muli %74, %c64_i32 : i32\n+      %76 = tt.advance %arg13, [%c0_i32, %75] : <tensor<16x64xf16, #blocked1>, 1>\n+      %77:3 = scf.for %arg16 = %c0_i32 to %arg7 step %c16_i32 iter_args(%arg17 = %cst, %arg18 = %73, %arg19 = %76) -> (tensor<64x64xf32, #mma>, !tt.ptr<tensor<64x16xf16, #blocked>, 1>, !tt.ptr<tensor<16x64xf16, #blocked1>, 1>)  : i32 {\n+        %91 = tt.load %arg18 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<64x16xf16, #blocked>, 1> -> tensor<64x16xf16, #blocked3>\n+        %92 = tt.load %arg19 {boundaryCheck = array<i32: 0, 1>, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : !tt.ptr<tensor<16x64xf16, #blocked1>, 1> -> tensor<16x64xf16, #blocked4>\n+        %93 = triton_gpu.convert_layout %91 : (tensor<64x16xf16, #blocked3>) -> tensor<64x16xf16, #shared>\n+        %94 = triton_gpu.convert_layout %92 : (tensor<16x64xf16, #blocked4>) -> tensor<16x64xf16, #shared1>\n+        %95 = tt.dot %93, %94, %arg17 {allowTF32 = true} : tensor<64x16xf16, #shared> * tensor<16x64xf16, #shared1> -> tensor<64x64xf32, #mma>\n+        %96 = tt.advance %arg18, [%c0_i32, %c16_i32] : <tensor<64x16xf16, #blocked>, 1>\n+        %97 = tt.advance %arg19, [%c16_i32, %c0_i32] : <tensor<16x64xf16, #blocked1>, 1>\n+        scf.yield %95, %96, %97 : tensor<64x64xf32, #mma>, !tt.ptr<tensor<64x16xf16, #blocked>, 1>, !tt.ptr<tensor<16x64xf16, #blocked1>, 1>\n+      }\n+      %78 = triton_gpu.convert_layout %77#0 : (tensor<64x64xf32, #mma>) -> tensor<64x64xf32, #blocked2>\n+      %79 = triton_gpu.convert_layout %77#0 : (tensor<64x64xf32, #mma>) -> tensor<64x64xf32, #blocked2>\n+      %80 = tt.advance %77#1, [%c0_i32, %36] : <tensor<64x16xf16, #blocked>, 1>\n+      %81 = tt.advance %77#2, [%36, %c0_i32] : <tensor<16x64xf16, #blocked1>, 1>\n+      %82 = \"tt.reduce\"(%78) ({\n+      ^bb0(%arg16: f32, %arg17: f32):\n+        %91 = \"triton_gpu.cmpf\"(%arg16, %arg17) {predicate = 2 : i64} : (f32, f32) -> i1\n+        %92 = arith.select %91, %arg16, %arg17 : f32\n+        tt.reduce.return %92 : f32\n+      }) {axis = 1 : i32} : (tensor<64x64xf32, #blocked2>) -> tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %83 = tt.expand_dims %82 {axis = 1 : i32} : (tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64x1xf32, #blocked2>\n+      %84 = tt.broadcast %83 : (tensor<64x1xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+      %85 = arith.subf %79, %84 : tensor<64x64xf32, #blocked2>\n+      %86 = math.exp %85 : tensor<64x64xf32, #blocked2>\n+      %87 = \"tt.reduce\"(%86) ({\n+      ^bb0(%arg16: f32, %arg17: f32):\n+        %91 = arith.addf %arg16, %arg17 : f32\n+        tt.reduce.return %91 : f32\n+      }) {axis = 1 : i32} : (tensor<64x64xf32, #blocked2>) -> tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %88 = tt.expand_dims %87 {axis = 1 : i32} : (tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64x1xf32, #blocked2>\n+      %89 = tt.broadcast %88 : (tensor<64x1xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n+      %90 = arith.divf %86, %89 : tensor<64x64xf32, #blocked2>\n+      tt.store %63, %90, %70 {cache = 1 : i32, evict = 1 : i32} : tensor<64x64xf32, #blocked2>\n+      scf.yield %80, %81, %45, %46 : !tt.ptr<tensor<64x16xf16, #blocked>, 1>, !tt.ptr<tensor<16x64xf16, #blocked1>, 1>, i32, i32\n+    }\n+    tt.return\n+  }\n+}"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/DumpLayout.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -78,7 +78,8 @@ class IndexEmitter {\n \n private:\n   Value getMockSmemBase() {\n-    Value mockSmemBase = base.getSRegValue(rewriter, loc, \"%mock_smem_base\");\n+    Value mockSmemBase =\n+        mlir::LLVM::getSRegValue(rewriter, loc, \"%mock_smem_base\");\n     auto llPtrTy = LLVM::LLVMPointerType::get(\n         typeConverter.convertType(rewriter.getI8Type()), 3);\n     auto cast = rewriter.create<UnrealizedConversionCastOp>("}]