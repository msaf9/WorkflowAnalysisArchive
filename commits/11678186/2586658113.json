[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -147,7 +147,7 @@ def test_op(Z, H, N_CTX, D_MODEL, dtype=torch.float16):\n except BaseException:\n     HAS_FLASH = False\n \n-BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n+BATCH, N_HEADS, N_CTX, D_HEAD = 4, 64, 2048, 64\n # vary batch size for fixed heads / seq\n batch_bench = triton.testing.Benchmark(\n     x_names=['BATCH'],"}]