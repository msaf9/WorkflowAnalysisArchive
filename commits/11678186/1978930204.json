[{"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "file_content_changes": "@@ -22,6 +22,7 @@ def __init__(self, type_a, type_b):\n def program_id(axis: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_get_program_id(axis), tl.int32)\n \n+\n def num_programs(axis: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_get_num_programs(axis), tl.int32)\n \n@@ -45,6 +46,7 @@ def integer_promote_impl(a_ty: tl.dtype, b_ty: tl.dtype) -> tl.dtype:\n         return b_ty if b_rank >= a_rank else a_ty\n     assert False\n \n+\n def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype, div_or_mod: bool) -> tl.dtype:\n     # 1) if one operand is double, the other is implicitly\n     #    converted to double\n@@ -86,6 +88,7 @@ def check_ptr_type_impl(type_a: tl.dtype, type_b: tl.dtype, allow_ptr_a: bool) -\n         if type_b.is_floating():\n             raise IncompatibleTypeErrorimpl(type_a, type_b)\n \n+\n def binary_op_type_checking_impl(lhs: tl.tensor,\n                                  rhs: tl.tensor,\n                                  builder: ir.builder,\n@@ -105,6 +108,7 @@ def binary_op_type_checking_impl(lhs: tl.tensor,\n         rhs = cast(rhs, ret_sca_ty, builder)\n     return lhs, rhs\n \n+\n def add(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n@@ -126,6 +130,7 @@ def add(input: tl.tensor,\n         return tl.tensor(builder.create_add(input.handle, other.handle), input.dtype)\n     assert False\n \n+\n def sub(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n@@ -143,6 +148,7 @@ def sub(input: tl.tensor,\n         return tl.tensor(builder.create_sub(input.handle, other.handle), input.dtype)\n     assert False\n \n+\n def mul(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n@@ -156,6 +162,7 @@ def mul(input: tl.tensor,\n         return tl.tensor(builder.create_mul(input.handle, other.handle), input.dtype)\n     assert False\n \n+\n def truediv(input: tl.tensor,\n             other: tl.tensor,\n             builder: ir.builder) -> tl.tensor:\n@@ -200,6 +207,7 @@ def floordiv(input: tl.tensor,\n             return tl.tensor(builder.create_udiv(input.handle, other.handle), input.dtype)\n     assert False\n \n+\n def fdiv(input: tl.tensor,\n          other: tl.tensor,\n          ieee_rounding: bool,\n@@ -213,6 +221,7 @@ def fdiv(input: tl.tensor,\n     ret.set_fdiv_ieee_rounding(ieee_rounding)\n     return tl.tensor(ret, input.dtype)\n \n+\n def mod(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n@@ -254,6 +263,7 @@ def bitwise_op_type_checking_impl(input: tl.tensor,\n         other = cast(other, ret_sca_ty, builder)\n     return input, other\n \n+\n def and_(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n@@ -296,6 +306,7 @@ def shl(input: tl.tensor,\n def plus(input: tl.tensor) -> tl.tensor:\n     return input\n \n+\n def minus(input: tl.tensor,\n           builder: ir.builder) -> tl.tensor:\n     input_sca_ty = input.dtype.scalar\n@@ -304,6 +315,7 @@ def minus(input: tl.tensor,\n     _0 = tl.tensor(ir.constant.get_null_value(input_sca_ty.to_ir(builder)), input_sca_ty)\n     return sub(_0, input, builder)\n \n+\n def invert(input: tl.tensor,\n            builder: tl.tensor) -> tl.tensor:\n     input_sca_ty = input.dtype.scalar\n@@ -356,6 +368,7 @@ def greater_equal(input: tl.tensor,\n             return tl.tensor(builder.create_icmpUGE(input.handle, other.handle), _bool_like(input))\n     assert False\n \n+\n def less_than(input: tl.tensor,\n               other: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n@@ -372,6 +385,7 @@ def less_than(input: tl.tensor,\n             return tl.tensor(builder.create_icmpULT(input.handle, other.handle), _bool_like(input))\n     assert False\n \n+\n def less_equal(input: tl.tensor,\n                other: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n@@ -388,6 +402,7 @@ def less_equal(input: tl.tensor,\n             return tl.tensor(builder.create_icmpULE(input.handle, other.handle), _bool_like(input))\n     assert False\n \n+\n def equal(input: tl.tensor,\n           other: tl.tensor,\n           builder: ir.builder) -> tl.tensor:\n@@ -401,6 +416,7 @@ def equal(input: tl.tensor,\n         return tl.tensor(builder.create_icmpEQ(input.handle, other.handle), _bool_like(input))\n     assert False\n \n+\n def not_equal(input: tl.tensor,\n               other: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n@@ -424,6 +440,7 @@ def arange(start: int, end: int, builder: ir.builder) -> tl.tensor:\n     ret_ty = tl.block_type(tl.int32, shape)\n     return tl.tensor(builder.get_range(start, end), ret_ty)\n \n+\n def zeros(shape: List[int], dtype: tl.dtype, builder: ir.builder) -> tl.tensor:\n     _0 = ir.constant.get_null_value(dtype.to_ir(builder))\n     ret_ty = tl.block_type(dtype, shape)\n@@ -445,10 +462,12 @@ def reshape(input: tl.tensor,\n     ret_ty = tl.block_type(input.dtype.scalar, dst_shape)\n     return tl.tensor(builder.create_reshape(input.handle, dst_shape), ret_ty)\n \n+\n def cat(lhs: tl.tensor, rhs: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # TODO: check types\n     return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), lhs.dtype)\n \n+\n def broadcast_impl_shape(input: tl.tensor,\n                          shape: List[int],\n                          builder: ir.builder) -> tl.tensor:\n@@ -463,6 +482,7 @@ def broadcast_impl_shape(input: tl.tensor,\n     ret_ty = tl.block_type(input.dtype.scalar, shape)\n     return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)\n \n+\n def broadcast_impl_value(lhs: tl.tensor,\n                          rhs: tl.tensor,\n                          builder: ir.builder) -> tl.tensor:\n@@ -701,6 +721,7 @@ def load(ptr: tl.tensor,\n                                                 cache, eviction, is_volatile),\n                      dst_ty)\n \n+\n def store(ptr: tl.tensor,\n           val: tl.tensor,\n           mask: Optional[tl.tensor],\n@@ -739,6 +760,7 @@ def atomic_cas(ptr: tl.tensor,\n     # TODO: type checking\n     return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.dtype)\n \n+\n def atom_red_typechecking_impl(ptr: tl.tensor,\n                                val: tl.tensor,\n                                mask: tl.tensor,\n@@ -792,6 +814,7 @@ def atomic_max(ptr: tl.tensor,\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.dtype)\n     return where(pos, pos_ret, neg_ret, builder)\n \n+\n def atomic_min(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n@@ -831,6 +854,7 @@ def atomic_min(ptr: tl.tensor,\n                         i_val.dtype)\n     return where(pos, pos_ret, neg_ret, builder)\n \n+\n def atomic_add(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n@@ -840,27 +864,31 @@ def atomic_add(ptr: tl.tensor,\n     op = ir.ATOMIC_OP.FADD if sca_ty.is_floating() else ir.ATOMIC_OP.ADD\n     return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.dtype)\n \n+\n def atomic_and(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.dtype)\n \n+\n def atomic_or(ptr: tl.tensor,\n               val: tl.tensor,\n               mask: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.dtype)\n \n+\n def atomic_xor(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.dtype)\n \n+\n def atomic_xchg(ptr: tl.tensor,\n                 val: tl.tensor,\n                 mask: tl.tensor,"}]