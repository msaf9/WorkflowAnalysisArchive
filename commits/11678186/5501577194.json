[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 42, "deletions": 23, "changes": 65, "file_content_changes": "@@ -72,11 +72,13 @@ def _fwd_kernel(\n     # causal check on every loop iteration can be expensive\n     # and peeling the last iteration of the loop does not work well with ptxas\n     # so we have a mode to do the causal check in a separate kernel entirely\n-    if MODE == 0:  # entire causal attention\n+    if MODE == 0:  # entire non-causal attention\n+        lo, hi = 0, N_CTX\n+    if MODE == 1:  # entire causal attention\n         lo, hi = 0, (start_m + 1) * BLOCK_M\n-    elif MODE == 1:  # off band-diagonal\n+    if MODE == 2:  # off band-diagonal\n         lo, hi = 0, start_m * BLOCK_M\n-    elif MODE == 2:  # on band-diagonal\n+    if MODE == 3:  # on band-diagonal\n         l_ptrs = L + off_hz * N_CTX + offs_m\n         m_ptrs = M + off_hz * N_CTX + offs_m\n         m_i = tl.load(m_ptrs)\n@@ -98,7 +100,7 @@ def _fwd_kernel(\n         k = tl.load(tl.advance(K_block_ptr, (0, start_n)))\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k)\n-        if MODE != 1:\n+        if MODE == 1 or MODE == 3:\n             qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n         # -- compute m_ij, p, l_ij\n         m_ij = tl.max(qk, 1)\n@@ -165,6 +167,7 @@ def _bwd_kernel(\n     num_block,\n     BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n+    MODE: tl.constexpr,\n ):\n     off_hz = tl.program_id(0)\n     off_z = off_hz // H\n@@ -179,7 +182,10 @@ def _bwd_kernel(\n     DK += off_z * stride_qz + off_h * stride_qh\n     DV += off_z * stride_qz + off_h * stride_qh\n     for start_n in range(0, num_block):\n-        lo = start_n * BLOCK_M\n+        if MODE == 0:\n+            lo = 0\n+        else:\n+            lo = start_n * BLOCK_M\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n         offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n@@ -207,7 +213,11 @@ def _bwd_kernel(\n             q = tl.load(q_ptrs)\n             # recompute p = softmax(qk, dim=-1).T\n             # NOTE: `do` is pre-divided by `l`; no normalization here\n-            qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+            # if MODE == 1:\n+            if MODE == 1:\n+                qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), float(0.), float(\"-inf\"))\n+            else:\n+                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n             qk += tl.dot(q, tl.trans(k))\n             qk *= qk_scale\n             m = tl.load(m_ptrs + offs_m_curr)\n@@ -244,7 +254,7 @@ def _bwd_kernel(\n class _attention(torch.autograd.Function):\n \n     @staticmethod\n-    def forward(ctx, q, k, v, sm_scale):\n+    def forward(ctx, q, k, v, causal, sm_scale):\n         BLOCK = 128\n         # shape constraints\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n@@ -256,7 +266,10 @@ def forward(ctx, q, k, v, sm_scale):\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n \n         num_warps = 4 if Lk <= 64 else 8\n-        modes = [0] if q.shape[2] <= 2048 else [1, 2]\n+        if causal:\n+            modes = [1] if q.shape[2] <= 2048 else [2, 3]\n+        else:\n+            modes = [0]\n         for mode in modes:\n             _fwd_kernel[grid](\n                 q, k, v, sm_scale,\n@@ -276,6 +289,7 @@ def forward(ctx, q, k, v, sm_scale):\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n+        ctx.causal = causal\n         return o\n \n     @staticmethod\n@@ -288,6 +302,10 @@ def backward(ctx, do):\n         dv = torch.empty_like(v)\n         do_scaled = torch.empty_like(do)\n         delta = torch.empty_like(l)\n+        if ctx.causal:\n+            mode = 1\n+        else:\n+            mode = 0\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n@@ -306,17 +324,18 @@ def backward(ctx, do):\n             ctx.grid[0],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            MODE=mode,\n             num_stages=1,\n         )\n-        # print(h.asm[\"ttgir\"])\n-        return dq, dk, dv, None\n+        return dq, dk, dv, None, None\n \n \n attention = _attention.apply\n \n \n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(6, 9, 1024, 64)])\n-def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n+@pytest.mark.parametrize('causal', [False, True])\n+def test_op(Z, H, N_CTX, D_HEAD, causal, dtype=torch.float16):\n     torch.manual_seed(20)\n     q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n     k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0., std=0.5).requires_grad_()\n@@ -326,9 +345,10 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     # reference implementation\n     M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n-    for z in range(Z):\n-        for h in range(H):\n-            p[:, :, M == 0] = float(\"-inf\")\n+    if causal:\n+        for z in range(Z):\n+            for h in range(H):\n+                p[:, :, M == 0] = float(\"-inf\")\n     p = torch.softmax(p.float(), dim=-1).half()\n     # p = torch.exp(p)\n     ref_out = torch.matmul(p, v)\n@@ -337,15 +357,12 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     ref_dk, k.grad = k.grad.clone(), None\n     ref_dq, q.grad = q.grad.clone(), None\n     # triton implementation\n-    tri_out = attention(q, k, v, sm_scale).half()\n-    # print(ref_out)\n-    # print(tri_out)\n+    tri_out = attention(q, k, v, causal, sm_scale).half()\n     tri_out.backward(dout)\n     tri_dv, v.grad = v.grad.clone(), None\n     tri_dk, k.grad = k.grad.clone(), None\n     tri_dq, q.grad = q.grad.clone(), None\n     # compare\n-    print((ref_dv - tri_dv).abs().max())\n     assert torch.allclose(ref_out, tri_out, atol=1e-2, rtol=0)\n     assert torch.allclose(ref_dv, tri_dv, atol=1e-2, rtol=0)\n     assert torch.allclose(ref_dk, tri_dk, atol=1e-2, rtol=0)\n@@ -369,12 +386,12 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n-    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n-) for mode in ['fwd']]\n+    args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode, 'causal': causal}\n+) for mode in ['fwd', 'bwd'] for causal in [False, True]]\n \n \n @triton.testing.perf_report(configs)\n-def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.float16, device=\"cuda\"):\n+def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, causal, mode, provider, dtype=torch.float16, device=\"cuda\"):\n     assert mode in ['fwd', 'bwd']\n     warmup = 25\n     rep = 100\n@@ -383,7 +400,7 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         k = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         v = torch.randn((BATCH, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\", requires_grad=True)\n         sm_scale = 1.3\n-        fn = lambda: attention(q, k, v, sm_scale)\n+        fn = lambda: attention(q, k, v, causal, sm_scale)\n         if mode == 'bwd':\n             o = fn()\n             do = torch.randn_like(o)\n@@ -400,8 +417,10 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, warmup=warmup, rep=rep)\n-    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD * 0.5\n+    flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD\n     total_flops = 2 * flops_per_matmul\n+    if causal:\n+        total_flops *= 0.5\n     if mode == 'bwd':\n         total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n     return total_flops / ms * 1e-9"}]