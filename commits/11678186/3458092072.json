[{"filename": ".github/CODEOWNERS", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -28,6 +28,8 @@ lib/Analysis/Utility.cpp @Jokeren\n # ----------\n # Pipeline pass\n lib/Dialect/TritonGPU/Transforms/Pipeline.cpp @daadaada\n+# Prefetch pass\n+lib/Dialect/TritonGPU/Transforms/Prefetch.cpp @daadaada\n # Coalesce pass\n lib/Dialect/TritonGPU/Transforms/Coalesce.cpp @ptillet\n # Layout simplification pass"}, {"filename": ".gitignore", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -1,12 +1,20 @@\n+# Triton builds\n build/\n \n-__pycache__\n-.pytest_cache\n-\n+# Triton Python module builds\n python/build/\n python/triton.egg-info/\n python/triton/_C/libtriton.pyd\n python/triton/_C/libtriton.so\n \n+# Python caches\n+__pycache__\n+.pytest_cache\n+\n+# VS Code project files\n .vscode\n .vs\n+\n+# JetBrains project files\n+.idea\n+cmake-build-*"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 56, "deletions": 67, "changes": 123, "file_content_changes": "@@ -22,8 +22,8 @@ struct PTXInstrExecution;\n // PTXBuilder helps to manage a PTX asm program consists of one or multiple\n // instructions.\n //\n-// A helper for building a ASM program, the objective of PTXBuilder is to give a\n-// thin encapsulation and make the ASM code for MLIR LLVM Dialect more clear.\n+// A helper for building an ASM program, the objective of PTXBuilder is to give\n+// a thin encapsulation and make the ASM code for MLIR LLVM Dialect more clear.\n // Currently, several factors are introduced to reduce the need for mixing\n // string and C++ if-else code.\n //\n@@ -147,7 +147,7 @@ struct PTXBuilder {\n   Operand *newOperand(StringRef constraint);\n \n   // Create a constant integer operand.\n-  Operand *newConstantOperand(int v);\n+  Operand *newConstantOperand(int64_t v);\n   // Create a constant operand with explicit code specified.\n   Operand *newConstantOperand(const std::string &v);\n \n@@ -172,6 +172,22 @@ struct PTXBuilder {\n     return argArchive.back().get();\n   }\n \n+  // Make the oprands in argArchive follow the provided \\param order.\n+  void reorderArgArchive(ArrayRef<Operand *> order) {\n+    assert(order.size() == argArchive.size());\n+    // The order in argArchive is unnecessary when onlyAttachMLIRArgs=false, but\n+    // it do necessary when onlyAttachMLIRArgs is true for the $0,$1.. are\n+    // determined by PTX code snippet passed from external.\n+    sort(argArchive.begin(), argArchive.end(),\n+         [&](std::unique_ptr<Operand> &a, std::unique_ptr<Operand> &b) {\n+           auto ida = std::find(order.begin(), order.end(), a.get());\n+           auto idb = std::find(order.begin(), order.end(), b.get());\n+           assert(ida != order.end());\n+           assert(idb != order.end());\n+           return ida < idb;\n+         });\n+  }\n+\n   friend struct PTXInstr;\n   friend struct PTXInstrCommon;\n \n@@ -201,10 +217,17 @@ struct PTXInstrCommon {\n   // clang-format on\n \n   // Set operands of this instruction.\n-  PTXInstrExecution &operator()(llvm::ArrayRef<Operand *> oprs);\n+  PTXInstrExecution &operator()(llvm::ArrayRef<Operand *> oprs,\n+                                bool onlyAttachMLIRArgs = false);\n \n protected:\n-  PTXInstrExecution &call(llvm::ArrayRef<Operand *> oprs);\n+  // \"Call\" the instruction with operands.\n+  // \\param oprs The operands of this instruction.\n+  // \\param onlyAttachMLIRArgs Indicate that it simply attach the MLIR Arguments\n+  // to the inline Asm without generating the operand ids(such as $0, $1) in PTX\n+  // code.\n+  PTXInstrExecution &call(llvm::ArrayRef<Operand *> oprs,\n+                          bool onlyAttachMLIRArgs = false);\n \n   PTXBuilder *builder{};\n   llvm::SmallVector<std::string, 4> instrParts;\n@@ -234,70 +257,18 @@ template <class ConcreteT> struct PTXInstrBase : public PTXInstrCommon {\n \n struct PTXInstr : public PTXInstrBase<PTXInstr> {\n   using PTXInstrBase<PTXInstr>::PTXInstrBase;\n-};\n-\n-// A helper for PTX ld/st instruction.\n-// Usage:\n-// PtxIOInstr store(\"st\");\n-// store.predicate(pValue).global().v(32).b(1); // @%0 st.global.v32.b1\n-// store.addAddr(addrValue, \"l\", off);\n-struct PTXIOInstr : public PTXInstrBase<PTXIOInstr> {\n-  using PTXInstrBase<PTXIOInstr>::PTXInstrBase;\n-\n-  // Add \".global\" suffix to instruction\n-  PTXIOInstr &global(bool predicate = true) {\n-    o(\"global\", predicate);\n-    return *this;\n-  }\n-\n-  // Add \".shared\" suffix to instruction\n-  PTXIOInstr &shared(bool predicate = true) {\n-    o(\"shared\", predicate);\n-    return *this;\n-  }\n \n-  // Add \".v\" suffix to instruction\n-  PTXIOInstr &v(int vecWidth, bool predicate = true) {\n-    if (vecWidth > 1) {\n-      o(\"v\" + std::to_string(vecWidth), predicate);\n-    }\n-    return *this;\n-  }\n-\n-  // Add \".b\" suffix to instruction\n-  PTXIOInstr &b(int width) {\n-    o(\"b\" + std::to_string(width));\n-    return *this;\n-  }\n-};\n-\n-struct PTXCpAsyncInstrBase : public PTXInstrBase<PTXCpAsyncInstrBase> {\n-  explicit PTXCpAsyncInstrBase(PTXBuilder *builder)\n-      : PTXInstrBase(builder, \"cp.async\") {}\n-};\n+  // Append a \".global\" to the instruction.\n+  PTXInstr &global();\n \n-struct PTXCpAsyncCommitGroupInstr : public PTXCpAsyncInstrBase {\n-  explicit PTXCpAsyncCommitGroupInstr(PTXBuilder *builder)\n-      : PTXCpAsyncInstrBase(builder) {\n-    o(\"commit_group\");\n-  }\n-};\n+  // Append a \".shared\" to the instruction.\n+  PTXInstr &shared();\n \n-struct PTXCpAsyncWaitGroupInstr : public PTXCpAsyncInstrBase {\n-  explicit PTXCpAsyncWaitGroupInstr(PTXBuilder *builder)\n-      : PTXCpAsyncInstrBase(builder) {\n-    o(\"wait_group\");\n-  }\n-};\n+  // Append a \".v[0-9]+\" to the instruction\n+  PTXInstr &v(int vecWidth, bool predicate = true);\n \n-struct PTXCpAsyncLoadInstr : public PTXCpAsyncInstrBase {\n-  explicit PTXCpAsyncLoadInstr(PTXBuilder *builder,\n-                               triton::CacheModifier modifier)\n-      : PTXCpAsyncInstrBase(builder) {\n-    o(triton::stringifyCacheModifier(modifier).str());\n-    o(\"shared\");\n-    o(\"global\");\n-  }\n+  // Append a\".b[0-9]+\" to the instruction\n+  PTXInstr &b(int width);\n };\n \n // Record the operands and context for \"launching\" a PtxInstr.\n@@ -308,8 +279,10 @@ struct PTXInstrExecution {\n \n   PTXInstrExecution() = default;\n   explicit PTXInstrExecution(PTXInstrCommon *instr,\n-                             llvm::ArrayRef<Operand *> oprs)\n-      : argsInOrder(oprs.begin(), oprs.end()), instr(instr) {}\n+                             llvm::ArrayRef<Operand *> oprs,\n+                             bool onlyAttachMLIRArgs)\n+      : argsInOrder(oprs.begin(), oprs.end()), instr(instr),\n+        onlyAttachMLIRArgs(onlyAttachMLIRArgs) {}\n \n   // Prefix a predicate to the instruction.\n   PTXInstrExecution &predicate(mlir::Value value, StringRef constraint = \"b\") {\n@@ -330,6 +303,22 @@ struct PTXInstrExecution {\n \n   PTXInstrCommon *instr{};\n   Operand *pred{};\n+  bool onlyAttachMLIRArgs{};\n+};\n+\n+//// =============================== Some instruction wrappers\n+///===============================\n+// We add the wrappers to make the usage more intuitive by avoiding mixing the\n+// PTX code with some trivial C++ code.\n+\n+struct PTXCpAsyncLoadInstr : PTXInstrBase<PTXCpAsyncLoadInstr> {\n+  explicit PTXCpAsyncLoadInstr(PTXBuilder *builder,\n+                               triton::CacheModifier modifier)\n+      : PTXInstrBase(builder, \"cp.async\") {\n+    o(triton::stringifyCacheModifier(modifier).str());\n+    o(\"shared\");\n+    o(\"global\");\n+  }\n };\n \n } // namespace triton"}, {"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 10, "deletions": 1, "changes": 11, "file_content_changes": "@@ -4,6 +4,7 @@\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/Dialect/SCF/SCF.h\"\n #include \"mlir/Dialect/StandardOps/IR/Ops.h\"\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/Dialect.h\"\n #include \"mlir/Interfaces/ControlFlowInterfaces.h\"\n@@ -30,7 +31,15 @@ class DialectInferLayoutInterface\n \n   virtual LogicalResult\n   inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n-                            Attribute &resultEncoding) const = 0;\n+                            Attribute &resultEncoding,\n+                            Optional<Location> location) const = 0;\n+\n+  // Note: this function only verify operand encoding but doesn't infer result\n+  // encoding\n+  virtual LogicalResult\n+  inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n+                     Attribute retEncoding,\n+                     Optional<Location> location) const = 0;\n };\n \n } // namespace triton"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -10,6 +10,7 @@ include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n include \"mlir/Interfaces/ControlFlowInterfaces.td\" // BranchOpInterface\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/Interfaces/CastInterfaces.td\" // CastOpInterface\n \n def TensorSizeTrait : NativeOpTrait<\"TensorSizeTrait\">;\n def SameOperandsAndResultEncoding : NativeOpTrait<\"SameOperandsAndResultEncoding\">;\n@@ -72,17 +73,16 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n     // TODO: Add verifier\n }\n \n-def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n-                                   SameOperandsAndResultEncoding,\n-                                   NoSideEffect,\n-                                   /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n+                                     SameOperandsAndResultEncoding,\n+                                     NoSideEffect,\n+                                     DeclareOpInterfaceMethods<CastOpInterface>]> {\n     let summary = \"Floating point casting for custom types\";\n \n     let description = [{\n-        Floating point casting for custom types (F8, BF8).\n+        Floating point casting for custom types (F8).\n \n-        F8 <-> BF8, FP16, FP32\n-        BF8 <-> F8, FP16, FP32\n+        F8 <-> FP16, BF16, FP32, FP64\n     }];\n \n     let arguments = (ins TT_FloatLike:$from);"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -14,9 +14,8 @@ class TritonTypeDef<string name, string _mnemonic>\n \n // Floating-point Type\n def F8 : TritonTypeDef<\"Float8\", \"f8\">;\n-def BF8 : TritonTypeDef<\"BFloat8\", \"bf8\">;\n \n-def TT_Float : AnyTypeOf<[F16, BF16, F32, F64], \"floating-point\">;\n+def TT_Float : AnyTypeOf<[F8, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -25,6 +25,10 @@ namespace gpu {\n \n unsigned getElemsPerThread(Type type);\n \n+SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n+\n+SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n+\n SmallVector<unsigned> getSizePerThread(Attribute layout);\n \n SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 66, "deletions": 2, "changes": 68, "file_content_changes": "@@ -71,6 +71,70 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n     ArrayRefParameter<\"unsigned\", \"order of axes by the rate of changing\">:$order\n   );\n \n+  let builders = [\n+    AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n+                     \"ArrayRef<int64_t>\":$shape,\n+                     \"Type\":$eltTy), [{\n+        auto mmaEnc = dotOpEnc.getParent().dyn_cast<MmaEncodingAttr>();\n+        // Only support row major for now\n+        // TODO(Keren): check why column major code crashes\n+        SmallVector<unsigned> order = {1, 0};\n+\n+        if(!mmaEnc)\n+          return $_get(context, 1, 1, 1, order);\n+\n+        int version = mmaEnc.getVersion();\n+        int opIdx = dotOpEnc.getOpIdx();\n+\n+        // number of rows per phase\n+        int perPhase = 128 / (shape[order[0]] * (eltTy.getIntOrFloatBitWidth() / 8));\n+        perPhase = std::max<int>(perPhase, 1);\n+        \n+        // index of the inner dimension in `order`\n+        unsigned inner = (opIdx == 0) ? 0 : 1;\n+\n+        // ---- begin version 1 ----\n+        // TODO: handle rep (see\n+        // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L209)\n+        if (version == 1) {\n+          int maxPhase = (order[inner] == 1 ? 8 : 4) / perPhase;\n+          return $_get(context, 1, perPhase, maxPhase, order);\n+        } \n+\n+        // ---- begin version 2 ----\n+        if (version == 2) {\n+          std::vector<size_t> matShape = {8, 8,\n+                                          2 * 64 / eltTy.getIntOrFloatBitWidth()};\n+          // for now, disable swizzle when using transposed int8 tensor cores\n+          if (eltTy.isInteger(8) && order[0] == inner)\n+            return $_get(context, 1, 1, 1, order);\n+            \n+          // --- handle A operand ---\n+          if (opIdx == 0) { // compute swizzling for A operand\n+              int vec = (order[0] == 1) ? matShape[2] : matShape[0]; // k : m\n+              int mmaStride = (order[0] == 1) ? matShape[0] : matShape[2];\n+              int maxPhase = mmaStride / perPhase;\n+              return $_get(context, vec, perPhase, maxPhase, order);\n+          } \n+\n+          // --- handle B operand ---\n+          if (opIdx == 1) {\n+              int vec = (order[0] == 1) ? matShape[1] : matShape[2]; // n : k\n+              int mmaStride = (order[0] == 1) ? matShape[2] : matShape[1];\n+              int maxPhase = mmaStride / perPhase;\n+              return $_get(context, vec, perPhase, maxPhase, order);\n+          } \n+            \n+          llvm_unreachable(\"invalid operand index\");\n+        }\n+\n+        // ---- not implemented ----\n+        llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n+\n+\n+    }]>\n+  ];\n+\n   let extraClassDeclaration = extraBaseClassDeclaration;\n }\n \n@@ -326,11 +390,11 @@ def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n   );\n \n   let extraClassDeclaration = extraBaseClassDeclaration # [{\n-    SmallVector<int64_t> paddedShape(ArrayRef<int64_t> shape) const;\n+    template<class T>\n+    SmallVector<T> paddedShape(ArrayRef<T> shape) const;\n   }];\n }\n \n-\n def DotOperandEncodingAttr : DistributedEncoding<\"DotOperandEncoding\"> {\n   let mnemonic = \"dot_op\";\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -37,7 +37,7 @@ def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n // Port Arith_CmpIOp & Arith_CmpFOp & Std_SelectOp to TritonGPU.\n // This is needed because these ops don't\n // handle encodings\n-// e.g., https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Arithmetic/IR/ArithmeticOps.td#L111\n+// e.g., https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Arith/IR/ArithOps.td#L111\n def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect]> {\n   let summary = \"integer comparison operation\";\n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -6,9 +6,10 @@\n namespace mlir {\n std::unique_ptr<Pass> createTritonGPUPipelinePass(int numStages = 2);\n \n-std::unique_ptr<Pass> createTritonGPUCanonicalizeLoopsPass();\n+// TODO(Keren): prefetch pass not working yet\n+std::unique_ptr<Pass> createTritonGPUPrefetchPass();\n \n-std::unique_ptr<Pass> createTritonGPUSwizzlePass();\n+std::unique_ptr<Pass> createTritonGPUCanonicalizeLoopsPass();\n \n std::unique_ptr<Pass> createTritonGPUCoalescePass();\n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 15, "deletions": 13, "changes": 28, "file_content_changes": "@@ -7,7 +7,7 @@ def TritonGPUPipeline : Pass<\"tritongpu-pipeline\", \"mlir::ModuleOp\"> {\n   let summary = \"pipeline\";\n \n   let description = [{\n-    TODO  \n+    Unroll loops to hide global memory -> shared memory latency.\n   }];\n \n   let constructor = \"mlir::createTritonGPUPipelinePass()\";\n@@ -23,6 +23,20 @@ def TritonGPUPipeline : Pass<\"tritongpu-pipeline\", \"mlir::ModuleOp\"> {\n   ];\n }\n \n+def TritonGPUPrefetch : Pass<\"tritongpu-prefetch\", \"mlir::ModuleOp\"> {\n+  let summary = \"prefetch\";\n+\n+  let description = [{\n+    Prefetch operands (a and b) of tt.dot into shared memory to hide shared memory -> register latency.\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUPrefetchPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n+                           \"mlir::scf::SCFDialect\",\n+                           \"mlir::arith::ArithmeticDialect\"];\n+}\n+\n def TritonGPUCoalesce: Pass<\"tritongpu-coalesce\", \"mlir::ModuleOp\"> {\n   let summary = \"coalesce\";\n \n@@ -51,18 +65,6 @@ def TritonGPUCombineOps : Pass<\"tritongpu-combine\", \"mlir::ModuleOp\"> {\n                            \"mlir::triton::TritonDialect\"];\n }\n \n-def TritonGPUSwizzle : Pass<\"tritongpu-swizzle\", \"mlir::ModuleOp\"> {\n-  let summary = \"swizzle\";\n-\n-  let description = [{\n-    Inserts conversions to swizzled layout so as to avoid shared memory bank conflicts.\n-  }];\n-\n-  let constructor = \"mlir::createTritonGPUSwizzlePass()\";\n-\n-  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n-}\n-\n def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::ModuleOp\"> {\n   let summary = \"canonicalize scf.ForOp ops\";\n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 35, "deletions": 28, "changes": 63, "file_content_changes": "@@ -12,6 +12,7 @@\n #include <numeric>\n \n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getOrder;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::getSizePerThread;\n@@ -26,6 +27,26 @@ namespace mlir {\n //===----------------------------------------------------------------------===//\n namespace triton {\n \n+static std::pair<SmallVector<unsigned>, SmallVector<unsigned>>\n+getCvtOrder(const Attribute &srcLayout, const Attribute &dstLayout) {\n+  auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n+  auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n+  auto srcDotLayout = srcLayout.dyn_cast<DotOperandEncodingAttr>();\n+  auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>();\n+  auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();\n+  auto dstDotLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>();\n+  assert(!(srcMmaLayout && dstMmaLayout) &&\n+         \"Unexpected mma -> mma layout conversion\");\n+  // mma or dot layout does not have an order, so the order depends on the\n+  // layout of the other operand.\n+  auto inOrd = (srcMmaLayout || srcDotLayout) ? getOrder(dstLayout)\n+                                              : getOrder(srcLayout);\n+  auto outOrd = (dstMmaLayout || dstDotLayout) ? getOrder(srcLayout)\n+                                               : getOrder(dstLayout);\n+\n+  return {inOrd, outOrd};\n+}\n+\n SmallVector<unsigned>\n getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n                              unsigned &outVec) {\n@@ -35,16 +56,7 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   Attribute dstLayout = dstTy.getEncoding();\n   assert(srcLayout && dstLayout &&\n          \"Unexpect layout in getScratchConfigForCvtLayout()\");\n-  unsigned rank = dstTy.getRank();\n-  SmallVector<unsigned> paddedRepShape(rank);\n-  auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n-  auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n-  auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>();\n-  auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();\n-  assert(!(srcMmaLayout && dstMmaLayout) &&\n-         \"Unexpected mma -> mma layout conversion\");\n-  auto inOrd = srcMmaLayout ? getOrder(dstLayout) : getOrder(srcLayout);\n-  auto outOrd = dstMmaLayout ? getOrder(srcLayout) : getOrder(dstLayout);\n+  auto [inOrd, outOrd] = getCvtOrder(srcLayout, dstLayout);\n   unsigned srcContigPerThread = getSizePerThread(srcLayout)[inOrd[0]];\n   unsigned dstContigPerThread = getSizePerThread(dstLayout)[outOrd[0]];\n   // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n@@ -55,6 +67,8 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   auto srcShapePerCTA = getShapePerCTA(srcLayout);\n   auto dstShapePerCTA = getShapePerCTA(dstLayout);\n \n+  unsigned rank = dstTy.getRank();\n+  SmallVector<unsigned> paddedRepShape(rank);\n   unsigned pad = std::max(inVec, outVec);\n   for (unsigned d = 0; d < rank; ++d) {\n     paddedRepShape[d] =\n@@ -73,22 +87,22 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n \n SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcLayout = srcTy.getEncoding();\n   auto srcShape = srcTy.getShape();\n   auto axis = op.axis();\n \n-  bool fastReduce = axis == srcLayout.getOrder()[0];\n+  bool fastReduce = axis == getOrder(srcLayout)[0];\n \n   SmallVector<unsigned> smemShape;\n   for (auto d : srcShape)\n     smemShape.push_back(d);\n \n   if (fastReduce) {\n-    unsigned sizeInterWarps = srcLayout.getWarpsPerCTA()[axis];\n+    unsigned sizeInterWarps = gpu::getWarpsPerCTA(srcLayout)[axis];\n     smemShape[axis] = sizeInterWarps;\n   } else {\n-    unsigned threadsPerCTAAxis =\n-        srcLayout.getThreadsPerWarp()[axis] * srcLayout.getWarpsPerCTA()[axis];\n+    unsigned threadsPerCTAAxis = gpu::getThreadsPerWarp(srcLayout)[axis] *\n+                                 gpu::getWarpsPerCTA(srcLayout)[axis];\n     smemShape[axis] = threadsPerCTAAxis;\n   }\n \n@@ -143,22 +157,15 @@ class AllocationAnalysis {\n \n   /// Initializes temporary shared memory for a given operation.\n   void getScratchValueSize(Operation *op) {\n-    // TODO(Keren): Add atomic ops\n-    // TODO(Keren): Add convert ops\n     if (auto reduceOp = dyn_cast<triton::ReduceOp>(op)) {\n       // TODO(Keren): Reduce with index is not supported yet.\n       auto value = op->getOperand(0);\n       if (auto tensorType = value.getType().dyn_cast<RankedTensorType>()) {\n-        if (tensorType.getEncoding().isa<BlockedEncodingAttr>()) {\n-          auto smemShape = getScratchConfigForReduce(reduceOp);\n-          unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(),\n-                                           1, std::multiplies{});\n-          auto bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n-          allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n-        } else {\n-          assert(0 && \"ReduceOp with input layout other than blocked layout is \"\n-                      \"not implemented yet\");\n-        }\n+        auto smemShape = getScratchConfigForReduce(reduceOp);\n+        unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                         std::multiplies{});\n+        auto bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n+        allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();\n@@ -167,7 +174,7 @@ class AllocationAnalysis {\n       auto dstEncoding = dstTy.getEncoding();\n       if (srcEncoding.isa<SharedEncodingAttr>() ||\n           dstEncoding.isa<SharedEncodingAttr>()) {\n-        // Only blocked -> blocked conversion requires for scratch allocation\n+        // Conversions from/to shared memory do not need scratch memory.\n         return;\n       }\n       // ConvertLayoutOp with both input/output non-shared_layout"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 44, "deletions": 7, "changes": 51, "file_content_changes": "@@ -45,7 +45,7 @@ PTXBuilder::Operand *PTXBuilder::newConstantOperand(const std::string &v) {\n   return argArchive.back().get();\n }\n \n-PTXBuilder::Operand *PTXBuilder::newConstantOperand(int v) {\n+PTXBuilder::Operand *PTXBuilder::newConstantOperand(int64_t v) {\n   std::stringstream ss;\n   ss << \"0x\" << std::hex << v;\n   return newConstantOperand(ss.str());\n@@ -128,28 +128,43 @@ std::string PTXBuilder::dump() const {\n   return strJoin(lines, \"\\n\\t\");\n }\n \n-PTXInstrExecution &PTXInstrCommon::call(ArrayRef<Operand *> oprs) {\n+PTXInstrExecution &PTXInstrCommon::call(ArrayRef<Operand *> oprs,\n+                                        bool onlyAttachMLIRArgs) {\n+  if (onlyAttachMLIRArgs) {\n+    // Nearly impossible to make the $0,$1 in two PTX code snippets to point to\n+    // the same MLIR values in onlyAttachMLIRArgs mode.\n+    assert(builder->executions.empty() &&\n+           \"builder can only hold a single execution when onlyAttachMIIRArgs \"\n+           \"is true.\");\n+    builder->reorderArgArchive(oprs);\n+  }\n+\n   builder->executions.emplace_back(\n-      std::make_unique<PTXInstrExecution>(this, oprs));\n+      std::make_unique<PTXInstrExecution>(this, oprs, onlyAttachMLIRArgs));\n+\n   return *builder->executions.back();\n }\n \n-PTXInstrExecution &PTXInstrCommon::operator()(ArrayRef<Operand *> oprs) {\n-  return call(oprs);\n+PTXInstrExecution &PTXInstrCommon::operator()(ArrayRef<Operand *> oprs,\n+                                              bool onlyAttachMLIRArgs) {\n+  return call(oprs, onlyAttachMLIRArgs);\n }\n \n std::string PTXInstrExecution::dump() const {\n   std::string osStr;\n   llvm::raw_string_ostream os(osStr);\n+\n+  std::string instrRepr = strJoin(instr->instrParts, \".\");\n+  if (onlyAttachMLIRArgs)\n+    return instrRepr;\n+\n   if (pred) {\n     if (!pred->repr)\n       os << \"@\" << pred->dump() << \" \";\n     else\n       os << pred->repr(pred->idx) << \" \";\n   }\n \n-  std::string instrRepr = strJoin(instr->instrParts, \".\");\n-\n   llvm::SmallVector<std::string, 4> argReprs;\n   for (auto *arg : argsInOrder) {\n     argReprs.push_back(arg->dump());\n@@ -174,5 +189,27 @@ PTXInstrExecution::getArgList() const {\n   return args;\n }\n \n+PTXInstr &PTXInstr::global() {\n+  o(\"global\");\n+  return *this;\n+}\n+\n+PTXInstr &PTXInstr::shared() {\n+  o(\"shared\");\n+  return *this;\n+}\n+\n+PTXInstr &PTXInstr::v(int vecWidth, bool predicate) {\n+  if (vecWidth > 1) {\n+    o(\"v\" + std::to_string(vecWidth), predicate);\n+  }\n+  return *this;\n+}\n+\n+PTXInstr &PTXInstr::b(int width) {\n+  o(\"b\" + std::to_string(width));\n+  return *this;\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 822, "deletions": 236, "changes": 1058, "file_content_changes": "@@ -64,9 +64,8 @@ Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n                                            rewriter.getF32FloatAttr(v));\n }\n \n-// Create a index type constant.\n+// Create an index type constant.\n static Value createIndexConstant(OpBuilder &builder, Location loc,\n-\n                                  TypeConverter *converter, int64_t value) {\n   Type ty = converter->convertType(builder.getIndexType());\n   return builder.create<LLVM::ConstantOp>(loc, ty,\n@@ -127,6 +126,7 @@ void llPrintf(StringRef msg, ValueRange args,\n #define i32_ty rewriter.getIntegerType(32)\n #define ui32_ty rewriter.getIntegerType(32, false)\n #define f16_ty rewriter.getF16Type()\n+#define bf16_ty rewriter.getBF16Type()\n #define i8_ty rewriter.getIntegerType(8)\n #define f32_ty rewriter.getF32Type()\n #define f64_ty rewriter.getF64Type()\n@@ -339,7 +339,8 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   }\n \n   Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n-  for (auto v : llvm::enumerate(resultVals)) {\n+  for (const auto &v : llvm::enumerate(resultVals)) {\n+    assert(v.value() && \"can not insert null values\");\n     llvmStruct = insert_val(structType, llvmStruct, v.value(),\n                             rewriter.getI64ArrayAttr(v.index()));\n   }\n@@ -388,9 +389,9 @@ static Value storeShared(ConversionPatternRewriter &rewriter, Location loc,\n   const char *c = bits == 64 ? \"l\" : (bits == 16 ? \"h\" : \"r\");\n \n   PTXBuilder builder;\n-  auto &st = builder.create<PTXIOInstr>(\"st\")->shared().b(bits);\n   auto *ptrOpr = builder.newAddrOperand(ptr, \"r\");\n   auto *valOpr = builder.newOperand(val, c);\n+  auto &st = builder.create<>(\"st\")->shared().b(bits);\n   st(ptrOpr, valOpr).predicate(pred, \"b\");\n   return builder.launch(rewriter, loc, void_ty(ctx));\n }\n@@ -512,6 +513,10 @@ class ConvertTritonGPUOpToLLVMPattern\n         rewriter.getIntegerAttr(rewriter.getIndexType(), value));\n   }\n \n+  // -----------------------------------------------------------------------\n+  // Utilities\n+  // -----------------------------------------------------------------------\n+\n   // Convert an \\param index to a multi-dim coordinate given \\param shape and\n   // \\param order.\n   SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n@@ -574,6 +579,10 @@ class ConvertTritonGPUOpToLLVMPattern\n     return ret;\n   }\n \n+  // -----------------------------------------------------------------------\n+  // Blocked layout indices\n+  // -----------------------------------------------------------------------\n+\n   // Get an index-base for each dimension for a \\param blocked_layout.\n   SmallVector<Value>\n   emitBaseIndexForBlockedLayout(Location loc,\n@@ -617,52 +626,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimBase;\n   }\n \n-  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n-                                              ConversionPatternRewriter &b,\n-                                              const Attribute &layout,\n-                                              ArrayRef<int64_t> shape) const {\n-    if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n-      return emitIndicesForBlockedLayout(loc, b, blocked, shape);\n-    } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n-      return emitIndicesForSliceLayout(loc, b, slice, shape);\n-    } else {\n-      assert(0 && \"emitIndices for layouts other than blocked & slice not \"\n-                  \"implemented yet\");\n-      return {};\n-    }\n-  }\n-\n-  SmallVector<SmallVector<Value>>\n-  emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n-                            const SliceEncodingAttr &sliceLayout,\n-                            ArrayRef<int64_t> shape) const {\n-    auto parent = sliceLayout.getParent();\n-    unsigned dim = sliceLayout.getDim();\n-    size_t rank = shape.size();\n-    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n-      auto paddedIndices = emitIndicesForBlockedLayout(\n-          loc, rewriter, blockedParent, sliceLayout.paddedShape(shape));\n-      unsigned numIndices = paddedIndices.size();\n-      SmallVector<SmallVector<Value>> resultIndices(numIndices);\n-      for (unsigned i = 0; i < numIndices; ++i)\n-        for (unsigned d = 0; d < rank + 1; ++d)\n-          if (d != dim)\n-            resultIndices[i].push_back(paddedIndices[i][d]);\n-\n-      return resultIndices;\n-\n-    } else if (auto sliceParent = parent.dyn_cast<SliceEncodingAttr>()) {\n-      assert(0 && \"emitIndicesForSliceLayout with parent of sliceLayout\"\n-                  \"is not implemented yet\");\n-      return {};\n-\n-    } else {\n-      assert(0 && \"emitIndicesForSliceLayout with parent other than blocked & \"\n-                  \"slice not implemented yet\");\n-      return {};\n-    }\n-  }\n-\n   SmallVector<SmallVector<unsigned>>\n   emitOffsetForBlockedLayout(const BlockedEncodingAttr &blockedLayout,\n                              ArrayRef<int64_t> shape) const {\n@@ -714,23 +677,109 @@ class ConvertTritonGPUOpToLLVMPattern\n     return reorderedOffset;\n   }\n \n+  // -----------------------------------------------------------------------\n+  // Mma layout indices\n+  // -----------------------------------------------------------------------\n+\n+  SmallVector<Value>\n+  emitBaseIndexForMmaLayoutV1(Location loc, ConversionPatternRewriter &rewriter,\n+                              const MmaEncodingAttr &mmaLayout,\n+                              ArrayRef<int64_t> shape) const {\n+    llvm_unreachable(\"emitIndicesForMmaLayoutV1 not implemented\");\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForMmaLayoutV1(const MmaEncodingAttr &mmaLayout,\n+                           ArrayRef<int64_t> shape) const {\n+    llvm_unreachable(\"emitOffsetForMmaLayoutV1 not implemented\");\n+  }\n+\n+  SmallVector<Value>\n+  emitBaseIndexForMmaLayoutV2(Location loc, ConversionPatternRewriter &rewriter,\n+                              const MmaEncodingAttr &mmaLayout,\n+                              ArrayRef<int64_t> shape) const {\n+    auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+    assert(_warpsPerCTA.size() == 2);\n+    SmallVector<Value> warpsPerCTA = {idx_val(_warpsPerCTA[0]),\n+                                      idx_val(_warpsPerCTA[1])};\n+    Value threadId = getThreadId(rewriter, loc);\n+    Value warpSize = idx_val(32);\n+    Value laneId = urem(threadId, warpSize);\n+    Value warpId = udiv(threadId, warpSize);\n+    Value warpId0 = urem(warpId, warpsPerCTA[0]);\n+    Value warpId1 = urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]);\n+    Value offWarp0 = mul(warpId0, idx_val(16));\n+    Value offWarp1 = mul(warpId1, idx_val(8));\n+\n+    SmallVector<Value> multiDimBase(2);\n+    multiDimBase[0] = add(udiv(laneId, idx_val(4)), offWarp0);\n+    multiDimBase[1] = add(mul(idx_val(2), urem(laneId, idx_val(4))), offWarp1);\n+    return multiDimBase;\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForMmaLayoutV2(const MmaEncodingAttr &mmaLayout,\n+                           ArrayRef<int64_t> shape) const {\n+    SmallVector<SmallVector<unsigned>> ret;\n+    for (unsigned i = 0; i < shape[0]; i += getShapePerCTA(mmaLayout)[0]) {\n+      for (unsigned j = 0; j < shape[1]; j += getShapePerCTA(mmaLayout)[1]) {\n+        ret.push_back({i, j});\n+        ret.push_back({i, j + 1});\n+      }\n+      for (unsigned j = 0; j < shape[1]; j += getShapePerCTA(mmaLayout)[1]) {\n+        ret.push_back({i + 8, j});\n+        ret.push_back({i + 8, j + 1});\n+      }\n+    }\n+    return ret;\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Get offsets / indices for any layout\n+  // -----------------------------------------------------------------------\n+\n+  SmallVector<Value> emitBaseIndexForLayout(Location loc,\n+                                            ConversionPatternRewriter &rewriter,\n+                                            const Attribute &layout,\n+                                            ArrayRef<int64_t> shape) const {\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+      return emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      if (mmaLayout.getVersion() == 1)\n+        return emitBaseIndexForMmaLayoutV1(loc, rewriter, mmaLayout, shape);\n+      if (mmaLayout.getVersion() == 2)\n+        return emitBaseIndexForMmaLayoutV2(loc, rewriter, mmaLayout, shape);\n+    }\n+    llvm_unreachable(\"unsupported emitBaseIndexForLayout\");\n+  }\n+\n+  SmallVector<SmallVector<unsigned>>\n+  emitOffsetForLayout(const Attribute &layout, ArrayRef<int64_t> shape) const {\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>())\n+      return emitOffsetForBlockedLayout(blockedLayout, shape);\n+    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      if (mmaLayout.getVersion() == 1)\n+        return emitOffsetForMmaLayoutV1(mmaLayout, shape);\n+      if (mmaLayout.getVersion() == 2)\n+        return emitOffsetForMmaLayoutV2(mmaLayout, shape);\n+    }\n+    llvm_unreachable(\"unsupported emitOffsetForLayout\");\n+  }\n+\n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n-  // TODO: [goostavz] Double confirm the redundant indices calculations will\n-  //       be eliminated in the consequent MLIR/LLVM optimization. We might\n-  //       implement a indiceCache if necessary.\n-  SmallVector<SmallVector<Value>>\n-  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &rewriter,\n-                              const BlockedEncodingAttr &blockedLayout,\n-                              ArrayRef<int64_t> shape) const {\n-    // step 1, delinearize threadId to get the base index\n-    auto multiDimBase =\n-        emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n \n-    // step 2, get offset of each element\n-    SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForBlockedLayout(blockedLayout, shape);\n+  // TODO: [phil] redundant indices commputation do not appear to hurt\n+  // performance much, but they could still significantly slow down\n+  // computations.\n+  SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const Attribute &layout, ArrayRef<int64_t> shape) const {\n \n+    // step 1, delinearize threadId to get the base index\n+    auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, shape);\n+    // step 2, get offset of each element\n+    auto offset = emitOffsetForLayout(layout, shape);\n     // step 3, add offset to base, and reorder the sequence of indices to\n     // guarantee that elems in the same sizePerThread are adjacent in order\n     unsigned rank = shape.size();\n@@ -744,6 +793,49 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDimIdx;\n   }\n \n+  SmallVector<SmallVector<Value>>\n+  emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &rewriter,\n+                            const SliceEncodingAttr &sliceLayout,\n+                            ArrayRef<int64_t> shape) const {\n+    auto parent = sliceLayout.getParent();\n+    unsigned dim = sliceLayout.getDim();\n+    size_t rank = shape.size();\n+    auto paddedIndices =\n+        emitIndices(loc, rewriter, parent, sliceLayout.paddedShape(shape));\n+    unsigned numIndices = paddedIndices.size();\n+    SmallVector<SmallVector<Value>> resultIndices(numIndices);\n+    for (unsigned i = 0; i < numIndices; ++i)\n+      for (unsigned d = 0; d < rank + 1; ++d)\n+        if (d != dim)\n+          resultIndices[i].push_back(paddedIndices[i][d]);\n+\n+    return resultIndices;\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Emit indices\n+  // -----------------------------------------------------------------------\n+  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n+                                              ConversionPatternRewriter &b,\n+                                              const Attribute &layout,\n+                                              ArrayRef<int64_t> shape) const {\n+    if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n+      return emitIndicesForDistributedLayout(loc, b, blocked, shape);\n+    } else if (auto mma = layout.dyn_cast<MmaEncodingAttr>()) {\n+      return emitIndicesForDistributedLayout(loc, b, mma, shape);\n+    } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n+      return emitIndicesForSliceLayout(loc, b, slice, shape);\n+    } else {\n+      assert(0 && \"emitIndices for layouts other than blocked & slice not \"\n+                  \"implemented yet\");\n+      return {};\n+    }\n+  }\n+\n+  // -----------------------------------------------------------------------\n+  // Shared memory utilities\n+  // -----------------------------------------------------------------------\n+\n   template <typename T>\n   Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n                             T value) const {\n@@ -953,7 +1045,8 @@ struct LoadOpConversion\n \n     // Determine the vectorization size\n     Type valueTy = op.getResult().getType();\n-    Type valueElemTy = getElementTypeOrSelf(valueTy);\n+    Type valueElemTy =\n+        typeConverter->convertType(getElementTypeOrSelf(valueTy));\n     unsigned vec = getVectorSize(ptr);\n     unsigned numElems = getElemsPerThread(ptr.getType());\n     if (llMask)\n@@ -1005,7 +1098,6 @@ struct LoadOpConversion\n       const bool hasL2EvictPolicy = false;\n \n       PTXBuilder ptxBuilder;\n-      auto &ld = *ptxBuilder.create<PTXIOInstr>(\"ld\");\n \n       Value pred = mask ? maskElems[vecStart] : int_val(1, 1);\n \n@@ -1025,16 +1117,18 @@ struct LoadOpConversion\n           ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n \n       // Define the instruction opcode\n-      ld.o(\"volatile\", op.isVolatile())\n-          .global()\n-          .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n-          .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n-          .o(\"L1::evict_first\",\n-             op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n-          .o(\"L1::evict_last\", op.evict() == triton::EvictionPolicy::EVICT_LAST)\n-          .o(\"L1::cache_hint\", hasL2EvictPolicy)\n-          .v(nWords)\n-          .b(width);\n+      auto &ld = ptxBuilder.create<>(\"ld\")\n+                     ->o(\"volatile\", op.isVolatile())\n+                     .global()\n+                     .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n+                     .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n+                     .o(\"L1::evict_first\",\n+                        op.evict() == triton::EvictionPolicy::EVICT_FIRST)\n+                     .o(\"L1::evict_last\",\n+                        op.evict() == triton::EvictionPolicy::EVICT_LAST)\n+                     .o(\"L1::cache_hint\", hasL2EvictPolicy)\n+                     .v(nWords)\n+                     .b(width);\n \n       PTXBuilder::Operand *evictOpr{};\n \n@@ -1049,8 +1143,8 @@ struct LoadOpConversion\n \n       if (other) {\n         for (size_t ii = 0; ii < nWords; ++ii) {\n-          PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n-          mov.o(\"u\" + std::to_string(width));\n+          PTXInstr &mov =\n+              ptxBuilder.create<>(\"mov\")->o(\"u\" + std::to_string(width));\n \n           size_t size = width / valueElemNbits;\n \n@@ -1083,7 +1177,8 @@ struct LoadOpConversion\n                        : retTys[0];\n \n       // TODO: if (has_l2_evict_policy)\n-      // auto asmDialectAttr = LLVM::AsmDialectAttr::get(rewriter.getContext(),\n+      // auto asmDialectAttr =\n+      // LLVM::AsmDialectAttr::get(rewriter.getContext(),\n       //                                                 LLVM::AsmDialect::AD_ATT);\n       Value ret = ptxBuilder.launch(rewriter, loc, retTy);\n \n@@ -1146,7 +1241,8 @@ struct StoreOpConversion\n     MLIRContext *ctx = rewriter.getContext();\n \n     auto valueTy = value.getType();\n-    Type valueElemTy = getElementTypeOrSelf(valueTy);\n+    Type valueElemTy =\n+        typeConverter->convertType(getElementTypeOrSelf(valueTy));\n \n     unsigned vec = getVectorSize(ptr);\n     unsigned numElems = getElemsPerThread(ptr.getType());\n@@ -1222,7 +1318,7 @@ struct StoreOpConversion\n           ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n \n       auto &ptxStoreInstr =\n-          ptxBuilder.create<PTXIOInstr>(\"st\")->global().v(nWords).b(width);\n+          ptxBuilder.create<>(\"st\")->global().v(nWords).b(width);\n       ptxStoreInstr(asmAddr, asmArgList).predicate(maskVal, \"b\");\n \n       Type boolTy = getTypeConverter()->convertType(rewriter.getIntegerType(1));\n@@ -1567,19 +1663,19 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   unsigned axis = adaptor.axis();\n \n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n-  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcLayout = srcTy.getEncoding();\n   auto srcShape = srcTy.getShape();\n   auto srcRank = srcTy.getRank();\n \n-  auto threadsPerWarp = srcLayout.getThreadsPerWarp();\n-  auto warpsPerCTA = srcLayout.getWarpsPerCTA();\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n \n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n   auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n   Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n   smemBase = bitcast(smemBase, elemPtrTy);\n \n-  auto order = srcLayout.getOrder();\n+  auto order = getOrder(srcLayout);\n   unsigned sizeIntraWarps = threadsPerWarp[axis];\n   unsigned sizeInterWarps = warpsPerCTA[axis];\n \n@@ -1588,7 +1684,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto srcValues = getElementsFromStruct(loc, adaptor.operand(), rewriter);\n \n   SmallVector<SmallVector<unsigned>> offset =\n-      emitOffsetForBlockedLayout(srcLayout, srcShape);\n+      emitOffsetForLayout(srcLayout, srcShape);\n \n   std::map<SmallVector<unsigned>, Value> accs;\n   std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n@@ -1651,7 +1747,8 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   // each thread needs to process:\n   //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n   unsigned elems = product<unsigned>(smemShape);\n-  unsigned numThreads = product<unsigned>(srcLayout.getWarpsPerCTA()) * 32;\n+  unsigned numThreads =\n+      product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) * 32;\n   unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n   Value readOffset = threadId;\n   for (unsigned round = 0; round < elemsPerThread; ++round) {\n@@ -1733,8 +1830,8 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   LogicalResult\n   matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    // We cannot directly\n-    //   rewriter.replaceOp(op, adaptor.src());\n+    // We cannot directly run\n+    //   `rewriter.replaceOp(op, adaptor.src())`\n     // due to MLIR's restrictions\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n@@ -2095,6 +2192,322 @@ struct ExtractSliceOpConversion\n   }\n };\n \n+struct FpToFpOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::FpToFpOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  static SmallVector<Value>\n+  convertFp8x4ToFp16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    Value fp8x4Vec = undef(fp8x4VecTy);\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n+    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm = \"{                                      \\n\"\n+                   \".reg .b32 a<2>, b<2>;                  \\n\"\n+                   \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n+                   \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n+                   \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"shr.b32  b0, b0, 1;                    \\n\"\n+                   \"shr.b32  b1, b1, 1;                    \\n\"\n+                   \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n+                   \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n+                   \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o0 = builder.newOperand(\"=r\");\n+    auto *o1 = builder.newOperand(\"=r\");\n+    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n+    call({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    auto fp16x2x2StructTy =\n+        struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n+    auto fp16x2x2Struct =\n+        builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n+    auto fp16x2Vec0 =\n+        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n+    auto fp16x2Vec1 =\n+        extract_val(fp16x2VecTy, fp16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    return {extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n+            extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n+            extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n+            extract_element(f16_ty, fp16x2Vec1, i32_val(1))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    Value fp16x2Vec0 = undef(fp16x2VecTy);\n+    Value fp16x2Vec1 = undef(fp16x2VecTy);\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n+    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n+    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm = \"{                                      \\n\"\n+                   \".reg .b32 a<2>, b<2>;                  \\n\"\n+                   \"shl.b32 a0, $1, 1;                     \\n\"\n+                   \"shl.b32 a1, $2, 1;                     \\n\"\n+                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+                   \"add.u32 a0, a0, 0x00800080;            \\n\"\n+                   \"add.u32 a1, a1, 0x00800080;            \\n\"\n+                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n+                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n+                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+                   \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n+    call({o, i0, i1}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    Value fp8x4Vec = undef(fp8x4VecTy);\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n+    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm = \"{                                          \\n\"\n+                   \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>;  \\n\"\n+                   \"prmt.b32 a0, 0, $2, 0x5040;                \\n\"\n+                   \"prmt.b32 a1, 0, $2, 0x7060;                \\n\"\n+                   \"and.b32 sign0, a0, 0x80008000;             \\n\"\n+                   \"and.b32 sign1, a1, 0x80008000;             \\n\"\n+                   \"and.b32 nosign0, a0, 0x7fff7fff;           \\n\"\n+                   \"and.b32 nosign1, a1, 0x7fff7fff;           \\n\"\n+                   \"shr.b32 nosign0, nosign0, 4;               \\n\"\n+                   \"shr.b32 nosign1, nosign1, 4;               \\n\"\n+                   \"add.u32 nosign0, nosign0, 0x38003800;      \\n\"\n+                   \"add.u32 nosign1, nosign1, 0x38003800;      \\n\"\n+                   \"or.b32 $0, sign0, nosign0;                 \\n\"\n+                   \"or.b32 $1, sign1, nosign1;                 \\n\"\n+                   \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o0 = builder.newOperand(\"=r\");\n+    auto *o1 = builder.newOperand(\"=r\");\n+    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n+    call({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto bf16x2VecTy = vec_ty(bf16_ty, 2);\n+    auto bf16x2x2StructTy =\n+        struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n+    auto bf16x2x2Struct =\n+        builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n+    auto bf16x2Vec0 =\n+        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({0}));\n+    auto bf16x2Vec1 =\n+        extract_val(bf16x2VecTy, bf16x2x2Struct, rewriter.getI32ArrayAttr({1}));\n+    return {extract_element(bf16_ty, bf16x2Vec0, i32_val(0)),\n+            extract_element(bf16_ty, bf16x2Vec0, i32_val(1)),\n+            extract_element(bf16_ty, bf16x2Vec1, i32_val(0)),\n+            extract_element(bf16_ty, bf16x2Vec1, i32_val(1))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertBf16x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto ctx = rewriter.getContext();\n+    auto bf16x2VecTy = vec_ty(bf16_ty, 2);\n+    Value bf16x2Vec0 = undef(bf16x2VecTy);\n+    Value bf16x2Vec1 = undef(bf16x2VecTy);\n+    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v0, i32_val(0));\n+    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v1, i32_val(1));\n+    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v2, i32_val(0));\n+    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v3, i32_val(1));\n+    bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);\n+    bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm = \"{                                            \\n\"\n+                   \".reg .u32 sign, sign<2>, nosign, nosign<2>;  \\n\"\n+                   \".reg .u32 fp8_min, fp8_max, rn_, zero;       \\n\"\n+                   \"mov.u32 fp8_min, 0x38003800;                 \\n\"\n+                   \"mov.u32 fp8_max, 0x3ff03ff0;                 \\n\"\n+                   \"mov.u32 rn_, 0x80008;                        \\n\"\n+                   \"mov.u32 zero, 0;                             \\n\"\n+                   \"and.b32 sign0, $1, 0x80008000;               \\n\"\n+                   \"and.b32 sign1, $2, 0x80008000;               \\n\"\n+                   \"prmt.b32 sign, sign0, sign1, 0x7531;         \\n\"\n+                   \"and.b32 nosign0, $1, 0x7fff7fff;             \\n\"\n+                   \"and.b32 nosign1, $2, 0x7fff7fff;             \\n\"\n+                   \".reg .u32 nosign_0_<2>, nosign_1_<2>;        \\n\"\n+                   \"and.b32 nosign_0_0, nosign0, 0xffff0000;     \\n\"\n+                   \"max.u32 nosign_0_0, nosign_0_0, 0x38000000;  \\n\"\n+                   \"min.u32 nosign_0_0, nosign_0_0, 0x3ff00000;  \\n\"\n+                   \"and.b32 nosign_0_1, nosign0, 0x0000ffff;     \\n\"\n+                   \"max.u32 nosign_0_1, nosign_0_1, 0x3800;      \\n\"\n+                   \"min.u32 nosign_0_1, nosign_0_1, 0x3ff0;      \\n\"\n+                   \"or.b32 nosign0, nosign_0_0, nosign_0_1;      \\n\"\n+                   \"and.b32 nosign_1_0, nosign1, 0xffff0000;     \\n\"\n+                   \"max.u32 nosign_1_0, nosign_1_0, 0x38000000;  \\n\"\n+                   \"min.u32 nosign_1_0, nosign_1_0, 0x3ff00000;  \\n\"\n+                   \"and.b32 nosign_1_1, nosign1, 0x0000ffff;     \\n\"\n+                   \"max.u32 nosign_1_1, nosign_1_1, 0x3800;      \\n\"\n+                   \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0;      \\n\"\n+                   \"or.b32 nosign1, nosign_1_0, nosign_1_1;      \\n\"\n+                   \"add.u32 nosign0, nosign0, rn_;               \\n\"\n+                   \"add.u32 nosign1, nosign1, rn_;               \\n\"\n+                   \"sub.u32 nosign0, nosign0, 0x38003800;        \\n\"\n+                   \"sub.u32 nosign1, nosign1, 0x38003800;        \\n\"\n+                   \"shr.u32 nosign0, nosign0, 4;                 \\n\"\n+                   \"shr.u32 nosign1, nosign1, 4;                 \\n\"\n+                   \"prmt.b32 nosign, nosign0, nosign1, 0x6420;   \\n\"\n+                   \"or.b32 $0, nosign, sign;                     \\n\"\n+                   \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n+    call({o, i0, i1}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8x4ToFp32x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp32x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp8x4ToFp64x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n+            rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])};\n+  }\n+\n+  static SmallVector<Value>\n+  convertFp64x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n+                       const Value &v0, const Value &v1, const Value &v2,\n+                       const Value &v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  LogicalResult\n+  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto srcTensorType = op.from().getType().cast<mlir::RankedTensorType>();\n+    auto dstTensorType = op.result().getType().cast<mlir::RankedTensorType>();\n+    auto srcEltType = srcTensorType.getElementType();\n+    auto dstEltType = dstTensorType.getElementType();\n+    assert(srcEltType.isa<triton::Float8Type>() ||\n+           dstEltType.isa<triton::Float8Type>());\n+    auto convertedDstTensorType =\n+        this->getTypeConverter()->convertType(dstTensorType);\n+    auto convertedDstEleType =\n+        this->getTypeConverter()->convertType(dstEltType);\n+\n+    // Select convertor\n+    std::function<SmallVector<Value>(Location, ConversionPatternRewriter &,\n+                                     const Value &, const Value &,\n+                                     const Value &, const Value &)>\n+        convertor;\n+    if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF16()) {\n+      convertor = convertFp8x4ToFp16x4;\n+    } else if (srcEltType.isF16() && dstEltType.isa<triton::Float8Type>()) {\n+      convertor = convertFp16x4ToFp8x4;\n+    } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isBF16()) {\n+      convertor = convertFp8x4ToBf16x4;\n+    } else if (srcEltType.isBF16() && dstEltType.isa<triton::Float8Type>()) {\n+      convertor = convertBf16x4ToFp8x4;\n+    } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF32()) {\n+      convertor = convertFp8x4ToFp32x4;\n+    } else if (srcEltType.isF32() && dstEltType.isa<triton::Float8Type>()) {\n+      convertor = convertFp32x4ToFp8x4;\n+    } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF64()) {\n+      convertor = convertFp8x4ToFp64x4;\n+    } else if (srcEltType.isF64() && dstEltType.isa<triton::Float8Type>()) {\n+      convertor = convertFp64x4ToFp8x4;\n+    } else {\n+      assert(false && \"unsupported type casting\");\n+    }\n+\n+    // Vectorized casting\n+    auto loc = op->getLoc();\n+    auto elems = getElemsPerThread(dstTensorType);\n+    assert(elems % 4 == 0 &&\n+           \"FP8 casting only support tensors with 4-aligned sizes\");\n+    auto elements = getElementsFromStruct(loc, adaptor.from(), rewriter);\n+    SmallVector<Value> resultVals;\n+    for (size_t i = 0; i < elems; i += 4) {\n+      auto converted = convertor(loc, rewriter, elements[i], elements[i + 1],\n+                                 elements[i + 2], elements[i + 3]);\n+      resultVals.append(converted);\n+    }\n+    assert(resultVals.size() == elems);\n+    auto result = getStructFromElements(loc, resultVals, rewriter,\n+                                        convertedDstTensorType);\n+    rewriter.replaceOp(op, result);\n+    return success();\n+  }\n+};\n+\n // A CRTP style of base class.\n template <typename SourceOp, typename DestOp, typename ConcreteT>\n class ElementwiseOpConversionBase\n@@ -2294,10 +2707,87 @@ struct ConvertLayoutOpConversion\n       return lowerDistributedToDistributed(op, adaptor, rewriter);\n     }\n     // TODO: to be implemented\n+    llvm_unreachable(\"unsupported layout conversion\");\n     return failure();\n   }\n \n private:\n+  SmallVector<Value> getMultiDimOffset(Attribute layout, Location loc,\n+                                       ConversionPatternRewriter &rewriter,\n+                                       unsigned elemId, ArrayRef<int64_t> shape,\n+                                       ArrayRef<unsigned> multiDimCTAInRepId,\n+                                       ArrayRef<unsigned> shapePerCTA) const {\n+    unsigned rank = shape.size();\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+      auto multiDimOffsetFirstElem =\n+          emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n+      SmallVector<Value> multiDimOffset(rank);\n+      SmallVector<unsigned> multiDimElemId =\n+          getMultiDimIndex<unsigned>(elemId, blockedLayout.getSizePerThread());\n+      for (unsigned d = 0; d < rank; ++d) {\n+        multiDimOffset[d] = add(multiDimOffsetFirstElem[d],\n+                                idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                                        multiDimElemId[d]));\n+      }\n+      return multiDimOffset;\n+    }\n+    if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+      unsigned dim = sliceLayout.getDim();\n+      auto multiDimOffsetParent =\n+          getMultiDimOffset(sliceLayout.getParent(), loc, rewriter, elemId,\n+                            sliceLayout.paddedShape(shape),\n+                            sliceLayout.paddedShape(multiDimCTAInRepId),\n+                            sliceLayout.paddedShape(shapePerCTA));\n+      SmallVector<Value> multiDimOffset(rank);\n+      for (unsigned d = 0; d < rank + 1; ++d) {\n+        if (d == dim)\n+          continue;\n+        unsigned slicedD = d < dim ? d : (d - 1);\n+        multiDimOffset[slicedD] = multiDimOffsetParent[d];\n+      }\n+      return multiDimOffset;\n+    }\n+    if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+      SmallVector<Value> mmaColIdx(2);\n+      SmallVector<Value> mmaRowIdx(2);\n+      Value threadId = getThreadId(rewriter, loc);\n+      Value warpSize = idx_val(32);\n+      Value laneId = urem(threadId, warpSize);\n+      Value warpId = udiv(threadId, warpSize);\n+      // auto multiDimWarpId =\n+      //     delinearize(rewriter, loc, warpId, mmaLayout.getWarpsPerCTA());\n+      // TODO: double confirm if its document bug or DotConversion's Bug\n+      SmallVector<Value> multiDimWarpId(2);\n+      multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n+      multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n+      Value four = idx_val(4);\n+      Value mmaGrpId = udiv(laneId, four);\n+      Value mmaGrpIdP8 = add(mmaGrpId, idx_val(8));\n+      Value mmaThreadIdInGrp = urem(laneId, four);\n+      Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, idx_val(2));\n+      Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, idx_val(1));\n+      Value colWarpOffset = mul(multiDimWarpId[0], idx_val(16));\n+      mmaColIdx[0] = add(mmaGrpId, colWarpOffset);\n+      mmaColIdx[1] = add(mmaGrpIdP8, colWarpOffset);\n+      Value rowWarpOffset = mul(multiDimWarpId[1], idx_val(8));\n+      mmaRowIdx[0] = add(mmaThreadIdInGrpM2, rowWarpOffset);\n+      mmaRowIdx[1] = add(mmaThreadIdInGrpM2P1, rowWarpOffset);\n+\n+      assert(rank == 2);\n+      assert(mmaLayout.getVersion() == 2 &&\n+             \"mmaLayout ver1 not implemented yet\");\n+      SmallVector<Value> multiDimOffset(rank);\n+      multiDimOffset[0] = elemId < 2 ? mmaColIdx[0] : mmaColIdx[1];\n+      multiDimOffset[1] = elemId % 2 == 0 ? mmaRowIdx[0] : mmaRowIdx[1];\n+      multiDimOffset[0] = add(multiDimOffset[0],\n+                              idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n+      multiDimOffset[1] = add(multiDimOffset[1],\n+                              idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+      return multiDimOffset;\n+    }\n+    llvm_unreachable(\"unexpected layout in getMultiDimOffset\");\n+  }\n+\n   // shared memory rd/st for blocked or mma layout with data padding\n   void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n                       bool stNotRd, RankedTensorType type,\n@@ -2308,7 +2798,7 @@ struct ConvertLayoutOpConversion\n                       Value smemBase) const;\n \n   // blocked/mma -> blocked/mma.\n-  // Data padding in shared memory to avoid bank confict.\n+  // Data padding in shared memory to avoid bank conflict.\n   LogicalResult\n   lowerDistributedToDistributed(triton::gpu::ConvertLayoutOp op,\n                                 OpAdaptor adaptor,\n@@ -2325,6 +2815,19 @@ struct ConvertLayoutOpConversion\n   LogicalResult\n   lowerSharedToDotOperand(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n                           ConversionPatternRewriter &rewriter) const;\n+\n+  // shared -> dot_operand if the result layout is mma\n+  Value lowerSharedToDotOperandMMA(\n+      triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+      ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n+      const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const;\n+\n+  // shared -> dot_operand if the result layout is blocked\n+  Value lowerSharedToDotOperandBlocked(\n+      triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+      ConversionPatternRewriter &rewriter,\n+      const BlockedEncodingAttr &blockedLayout,\n+      const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const;\n };\n \n void ConvertLayoutOpConversion::processReplica(\n@@ -2352,47 +2855,6 @@ void ConvertLayoutOpConversion::processReplica(\n     elemTy = IntegerType::get(elemTy.getContext(), 8);\n   auto llvmElemTy = getTypeConverter()->convertType(elemTy);\n \n-  SmallVector<Value> multiDimOffsetFirstElem;\n-  SmallVector<Value> mmaColIdx(2);\n-  SmallVector<Value> mmaRowIdx(2);\n-  if (blockedLayout) {\n-    multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n-        loc, rewriter, blockedLayout, type.getShape());\n-  } else if (sliceLayout) {\n-    auto parent = sliceLayout.getParent();\n-    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n-      SmallVector<int64_t> paddedShape =\n-          sliceLayout.paddedShape(type.getShape());\n-      multiDimOffsetFirstElem = emitBaseIndexForBlockedLayout(\n-          loc, rewriter, blockedParent, paddedShape);\n-    } else {\n-      assert(0 && \"SliceEncodingAttr with parent other than \"\n-                  \"BlockedEncodingAttr not implemented\");\n-    }\n-  } else if (mmaLayout) {\n-    Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = idx_val(32);\n-    Value laneId = urem(threadId, warpSize);\n-    Value warpId = udiv(threadId, warpSize);\n-    // auto multiDimWarpId =\n-    //     delinearize(rewriter, loc, warpId, mmaLayout.getWarpsPerCTA());\n-    // TODO: double confirm if its document bug or DotConversion's Bug\n-    SmallVector<Value> multiDimWarpId(2);\n-    multiDimWarpId[0] = urem(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-    multiDimWarpId[1] = udiv(warpId, idx_val(mmaLayout.getWarpsPerCTA()[0]));\n-    Value four = idx_val(4);\n-    Value mmaGrpId = udiv(laneId, four);\n-    Value mmaGrpIdP8 = add(mmaGrpId, idx_val(8));\n-    Value mmaThreadIdInGrp = urem(laneId, four);\n-    Value mmaThreadIdInGrpM2 = mul(mmaThreadIdInGrp, idx_val(2));\n-    Value mmaThreadIdInGrpM2P1 = add(mmaThreadIdInGrpM2, idx_val(1));\n-    Value colWarpOffset = mul(multiDimWarpId[0], idx_val(16));\n-    mmaColIdx[0] = add(mmaGrpId, colWarpOffset);\n-    mmaColIdx[1] = add(mmaGrpIdP8, colWarpOffset);\n-    Value rowWarpOffset = mul(multiDimWarpId[1], idx_val(8));\n-    mmaRowIdx[0] = add(mmaThreadIdInGrpM2, rowWarpOffset);\n-    mmaRowIdx[1] = add(mmaThreadIdInGrpM2P1, rowWarpOffset);\n-  }\n   for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n     auto multiDimCTAInRepId = getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n     SmallVector<unsigned> multiDimCTAId(rank);\n@@ -2406,48 +2868,9 @@ void ConvertLayoutOpConversion::processReplica(\n     //       consider of caching the index calculation result in case\n     //       of performance issue observed.\n     for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n-      SmallVector<Value> multiDimOffset(rank);\n-      if (blockedLayout) {\n-        SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n-            elemId, blockedLayout.getSizePerThread());\n-        for (unsigned d = 0; d < rank; ++d) {\n-          multiDimOffset[d] =\n-              add(multiDimOffsetFirstElem[d],\n-                  idx_val(multiDimCTAInRepId[d] * shapePerCTA[d] +\n-                          multiDimElemId[d]));\n-        }\n-      } else if (sliceLayout) {\n-        unsigned dim = sliceLayout.getDim();\n-        auto parent = sliceLayout.getParent();\n-        if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n-          SmallVector<unsigned> multiDimElemId = getMultiDimIndex<unsigned>(\n-              elemId, blockedParent.getSizePerThread());\n-          for (unsigned d = 0; d < rank + 1; ++d) {\n-            if (d == dim)\n-              continue;\n-            unsigned slicedD = d < dim ? d : (d - 1);\n-            multiDimOffset[slicedD] =\n-                add(multiDimOffsetFirstElem[d],\n-                    idx_val(multiDimCTAInRepId[slicedD] * shapePerCTA[slicedD] +\n-                            multiDimElemId[d]));\n-          }\n-        } else {\n-          assert(0 && \"SliceEncodingAttr with parent other than \"\n-                      \"BlockedEncodingAttr not implemented\");\n-        }\n-      } else if (mmaLayout) {\n-        assert(rank == 2);\n-        assert(mmaLayout.getVersion() == 2 &&\n-               \"mmaLayout ver1 not implemented yet\");\n-        multiDimOffset[0] = elemId < 2 ? mmaColIdx[0] : mmaColIdx[1];\n-        multiDimOffset[1] = elemId % 2 == 0 ? mmaRowIdx[0] : mmaRowIdx[1];\n-        multiDimOffset[0] = add(\n-            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n-        multiDimOffset[1] = add(\n-            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n-      } else {\n-        assert(0 && \"unexpected layout in processReplica\");\n-      }\n+      SmallVector<Value> multiDimOffset =\n+          getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n+                            multiDimCTAInRepId, shapePerCTA);\n       Value offset =\n           linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n                     reorder<unsigned>(paddedRepShape, outOrd));\n@@ -3010,6 +3433,7 @@ class MMA16816SmemLoader {\n       Value i8Elems[4][4];\n       Type elemTy = type::i8Ty(ctx);\n       Type elemPtrTy = ptr_ty(elemTy);\n+      Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n       if (kOrder == 1) {\n         for (int i = 0; i < 2; ++i)\n           for (int j = 0; j < 4; ++j)\n@@ -3024,7 +3448,7 @@ class MMA16816SmemLoader {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i32_ty);\n+          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n         }\n       } else { // k first\n         for (int j = 0; j < 4; ++j)\n@@ -3040,7 +3464,7 @@ class MMA16816SmemLoader {\n           for (int e = 0; e < 4; ++e)\n             i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n                                           i8Elems[m][e], i32_val(e));\n-          i32Elems[m] = bitcast(i8v4Elems[m], i32_ty);\n+          i32Elems[m] = bitcast(i8v4Elems[m], i8x4Ty);\n         }\n       }\n \n@@ -3251,7 +3675,7 @@ struct DotOpMmaV1ConversionHelper {\n     int NK = shape[1];\n     unsigned numM = rep[0] * shape[0] / (spw[0] * wpt[0]);\n \n-    // NOTE We cound't get the vec from the shared layout.\n+    // NOTE: We couldn't get the vec from the shared layout.\n     // int vecA = sharedLayout.getVec();\n     // TODO[Superjomn]: Consider the case when vecA > 4\n     bool vecGt4 = false;\n@@ -3269,7 +3693,7 @@ struct DotOpMmaV1ConversionHelper {\n     SmallVector<int> fpw({2, 2, 1});\n     SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n     SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n-    // NOTE We cound't get the vec from the shared layout.\n+    // NOTE: We couldn't get the vec from the shared layout.\n     // int vecB = sharedLayout.getVec();\n     // TODO[Superjomn]: Consider the case when vecA > 4\n     bool vecGt4 = false;\n@@ -3296,10 +3720,10 @@ struct DotOpMmaV1ConversionHelper {\n   // Compute the offset of the matrix to load.\n   // Returns offsetAM, offsetAK, offsetBN, offsetBK.\n   // NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n-  // the same time in the usage in convert_layout[shared->dot_op], we leave the\n-  // noexist info to be 0 and only use the desired argument from the composed\n-  // result. In this way we want to retain the original code structure in\n-  // convert_mma884 method for easier debugging.\n+  // the same time in the usage in convert_layout[shared->dot_op], we leave\n+  // the noexist info to be 0 and only use the desired argument from the\n+  // composed result. In this way we want to retain the original code\n+  // structure in convert_mma884 method for easier debugging.\n   std::tuple<Value, Value, Value, Value>\n   computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n                  ArrayRef<int> spw, ArrayRef<int> rep,\n@@ -3373,7 +3797,7 @@ struct DotOpMmaV2ConversionHelper {\n     return Type{};\n   }\n \n-  // The type of a matrix that loaded by either a ldmatrix or composed lds.\n+  // The type of matrix that loaded by either a ldmatrix or composed lds.\n   Type getMatType() const {\n     Type fp32Ty = type::f32Ty(ctx);\n     Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n@@ -3569,7 +3993,7 @@ struct DotOpMmaV2ConversionHelper {\n        \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n   };\n \n-  // vector length per ldmatrix (16*8/elelment_size_in_bits)\n+  // vector length per ldmatrix (16*8/element_size_in_bits)\n   inline static const std::map<TensorCoreType, uint8_t> mmaInstrVec = {\n       {TensorCoreType::FP32_FP16_FP16_FP32, 8},\n       {TensorCoreType::FP32_BF16_BF16_FP32, 8},\n@@ -3709,7 +4133,7 @@ struct MMA16816ConversionHelper {\n       // load from smem\n       loadFn = getLoadMatrixFn(\n           tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n-          1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShpae*/,\n+          1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n           {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n     } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n       // load from registers, used in gemm fuse\n@@ -3725,8 +4149,7 @@ struct MMA16816ConversionHelper {\n         loadFn(2 * m, 2 * k);\n \n     // step2. Format the values to LLVM::Struct to passing to mma codegen.\n-    Value result = composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n-    return result;\n+    return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n   }\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n@@ -3741,7 +4164,7 @@ struct MMA16816ConversionHelper {\n \n     auto loadFn = getLoadMatrixFn(\n         tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n-        0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n+        0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n     for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n@@ -4005,31 +4428,14 @@ struct DotOpFMAConversionHelper {\n   }\n };\n \n-LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n+Value ConvertLayoutOpConversion::lowerSharedToDotOperandMMA(\n     triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n-    ConversionPatternRewriter &rewriter) const {\n+    ConversionPatternRewriter &rewriter, const MmaEncodingAttr &mmaLayout,\n+    const DotOperandEncodingAttr &dotOperandLayout, bool isOuter) const {\n   auto loc = op.getLoc();\n   Value src = op.src();\n   Value dst = op.result();\n   auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n-\n-  auto dotOperandLayout =\n-      dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-\n-  MmaEncodingAttr mmaLayout =\n-      dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>();\n-  assert(mmaLayout);\n-\n-  bool isOuter{};\n-  {\n-    int K{};\n-    if (dotOperandLayout.getOpIdx() == 0) // $a\n-      K = dstTensorTy.getShape()[1];\n-    else // $b\n-      K = dstTensorTy.getShape()[0];\n-    isOuter = K == 1;\n-  }\n-\n   // TODO[Superjomn]: the allowTF32 is not available in ConvertLayoutOp for it\n   // is an attribute of DotOp.\n   bool allowTF32 = false;\n@@ -4080,6 +4486,41 @@ LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n   } else {\n     assert(false && \"Unsupported mma layout found\");\n   }\n+  return res;\n+}\n+\n+LogicalResult ConvertLayoutOpConversion::lowerSharedToDotOperand(\n+    triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+    ConversionPatternRewriter &rewriter) const {\n+  auto loc = op.getLoc();\n+  Value src = op.src();\n+  Value dst = op.result();\n+  auto dstTensorTy = dst.getType().cast<RankedTensorType>();\n+  auto srcTensorTy = src.getType().cast<RankedTensorType>();\n+  auto dotOperandLayout =\n+      dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+  auto sharedLayout = srcTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+  bool isOuter{};\n+  int K{};\n+  if (dotOperandLayout.getOpIdx() == 0) // $a\n+    K = dstTensorTy.getShape()[sharedLayout.getOrder()[0]];\n+  else // $b\n+    K = dstTensorTy.getShape()[sharedLayout.getOrder()[1]];\n+  isOuter = K == 1;\n+\n+  Value res;\n+  if (auto mmaLayout =\n+          dotOperandLayout.getParent().dyn_cast_or_null<MmaEncodingAttr>()) {\n+    res = lowerSharedToDotOperandMMA(op, adaptor, rewriter, mmaLayout,\n+                                     dotOperandLayout, isOuter);\n+  } else if (auto blockedLayout =\n+                 dotOperandLayout.getParent()\n+                     .dyn_cast_or_null<BlockedEncodingAttr>()) {\n+    assert(false && \"Blocked layout is not supported yet\");\n+  } else {\n+    assert(false && \"Unsupported dot operand layout found\");\n+  }\n \n   rewriter.replaceOp(op, res);\n   return success();\n@@ -4103,23 +4544,13 @@ DotOpConversion::convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n   auto ATensorTy = A.getType().cast<RankedTensorType>();\n   auto BTensorTy = B.getType().cast<RankedTensorType>();\n \n-  Value loadedA, loadedB, loadedC;\n-  // We support two kinds of operand layouts: 1. both $a, $b are dot_operand\n-  // layout, 2. both of them are shared layout.\n-  if (ATensorTy.getEncoding().isa<DotOperandEncodingAttr>()) {\n-    assert(BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n-           \"Both $a and %b should be DotOperand layout.\");\n-    loadedA = adaptor.a();\n-    loadedB = adaptor.b();\n-  } else {\n-    SharedMemoryObject smemA =\n-        getSharedMemoryObjectFromStruct(loc, adaptor.a(), rewriter);\n-    SharedMemoryObject smemB =\n-        getSharedMemoryObjectFromStruct(loc, adaptor.b(), rewriter);\n-    loadedA = mmaHelper.loadA(op.a(), smemA);\n-    loadedB = mmaHelper.loadB(op.b(), smemB);\n-  }\n+  assert(ATensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+         BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+         \"Both $a and %b should be DotOperand layout.\");\n \n+  Value loadedA, loadedB, loadedC;\n+  loadedA = adaptor.a();\n+  loadedB = adaptor.b();\n   loadedC = mmaHelper.loadC(op.c(), adaptor.c());\n \n   return mmaHelper.convertDot(A, B, C, op.d(), loadedA, loadedB, loadedC, op,\n@@ -4905,14 +5336,15 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n     addConversion([&](RankedTensorType type) -> llvm::Optional<Type> {\n       return convertTritonTensorType(type);\n     });\n-    // internally store bfloat16 as int16\n-    addConversion([&](BFloat16Type type) -> llvm::Optional<Type> {\n-      return IntegerType::get(type.getContext(), 16);\n+    // Internally store float8 as int8\n+    addConversion([&](triton::Float8Type type) -> llvm::Optional<Type> {\n+      return IntegerType::get(type.getContext(), 8);\n     });\n   }\n \n   Type convertTritonPointerType(triton::PointerType type) {\n-    return LLVM::LLVMPointerType::get(type.getPointeeType(),\n+    // Recursively translate pointee type\n+    return LLVM::LLVMPointerType::get(convertType(type.getPointeeType()),\n                                       type.getAddressSpace());\n   }\n \n@@ -5005,7 +5437,7 @@ struct AsyncWaitOpConversion\n   matchAndRewrite(triton::gpu::AsyncWaitOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     PTXBuilder ptxBuilder;\n-    auto &asyncWaitOp = *ptxBuilder.create<PTXCpAsyncWaitGroupInstr>();\n+    auto &asyncWaitOp = *ptxBuilder.create<>(\"cp.async.wait_group\");\n     auto num = op->getAttrOfType<IntegerAttr>(\"num\").getInt();\n     asyncWaitOp(ptxBuilder.newConstantOperand(num));\n \n@@ -5080,8 +5512,8 @@ struct InsertSliceAsyncOpConversion\n         srcStrides.emplace_back(smemObj.strides[i]);\n       }\n     }\n-    // Compute the offset based on the original dimensions of the shared memory\n-    // object\n+    // Compute the offset based on the original dimensions of the shared\n+    // memory object\n     auto dstOffset = dot(rewriter, loc, offsetVals, smemObj.strides);\n     auto dstPtrTy =\n         ptr_ty(getTypeConverter()->convertType(resTy.getElementType()), 3);\n@@ -5228,7 +5660,7 @@ struct InsertSliceAsyncOpConversion\n     }\n \n     PTXBuilder ptxBuilder;\n-    ptxBuilder.create<PTXCpAsyncCommitGroupInstr>()->operator()();\n+    ptxBuilder.create<>(\"cp.async.commit_group\")->operator()();\n     ptxBuilder.launch(rewriter, loc, void_ty(getContext()));\n     rewriter.replaceOp(op, llDst);\n     return success();\n@@ -5319,6 +5751,125 @@ struct FDivOpConversion\n   }\n };\n \n+/// ====================== atomic_rmw codegen begin ==========================\n+struct AtomicRMWOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>,\n+      public LoadStoreConversionBase {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::AtomicRMWOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  AtomicRMWOpConversion(LLVMTypeConverter &converter,\n+                        AxisInfoAnalysis &axisAnalysisPass,\n+                        PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::AtomicRMWOp>(converter,\n+                                                             benefit),\n+        LoadStoreConversionBase(axisAnalysisPass) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::AtomicRMWOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+\n+    auto loc = op.getLoc();\n+    MLIRContext *ctx = rewriter.getContext();\n+\n+    auto atomicRmwAttr = op.atomic_rmw_op();\n+    Value ptr = op.ptr();\n+    Value val = op.val();\n+\n+    Value llPtr = adaptor.ptr();\n+    Value llVal = adaptor.val();\n+\n+    auto valElements = getElementsFromStruct(loc, llVal, rewriter);\n+    auto ptrElements = getElementsFromStruct(loc, llPtr, rewriter);\n+\n+    // TODO[dongdongl]: Support mask and scalar\n+\n+    auto valueTy = op.getResult().getType().dyn_cast<RankedTensorType>();\n+    if (!valueTy)\n+      return failure();\n+    Type valueElemTy =\n+        getTypeConverter()->convertType(valueTy.getElementType());\n+\n+    auto valTy = val.getType().cast<RankedTensorType>();\n+    const size_t valueElemNbits = valueElemTy.getIntOrFloatBitWidth();\n+    auto vec = getVectorSize(ptr);\n+    vec = std::min<unsigned>(vec, valTy.getElementType().isF16() ? 2 : 1);\n+\n+    auto vecTy = vec_ty(valueElemTy, vec);\n+    auto elemsPerThread = getElemsPerThread(val.getType());\n+    SmallVector<Value> resultVals(elemsPerThread);\n+    for (size_t i = 0; i < elemsPerThread; i += vec) {\n+      Value rmvVal = undef(vecTy);\n+      for (int ii = 0; ii < vec; ++ii) {\n+        Value iiVal = createIndexAttrConstant(\n+            rewriter, loc, getTypeConverter()->getIndexType(), ii);\n+        rmvVal = insert_element(vecTy, rmvVal, valElements[i], iiVal);\n+      }\n+      Value rmwPtr = bitcast(ptrElements[i], ptr_ty(valTy.getElementType()));\n+      std::string sTy;\n+      PTXBuilder ptxBuilder;\n+\n+      auto *dstOpr = ptxBuilder.newOperand(\"=r\");\n+      auto *ptrOpr = ptxBuilder.newAddrOperand(rmwPtr, \"r\");\n+      auto *valOpr = ptxBuilder.newOperand(rmvVal, \"r\");\n+\n+      auto &atom = ptxBuilder.create<>(\"atom\")->global().o(\"gpu\");\n+      auto rmwOp = stringifyRMWOp(atomicRmwAttr).str();\n+      auto sBits = std::to_string(valueElemNbits);\n+      switch (atomicRmwAttr) {\n+      case RMWOp::AND:\n+        sTy = \"b\" + sBits;\n+        break;\n+      case RMWOp::OR:\n+        sTy = \"b\" + sBits;\n+        break;\n+      case RMWOp::XOR:\n+        sTy = \"b\" + sBits;\n+        break;\n+      case RMWOp::ADD:\n+        sTy = \"s\" + sBits;\n+        break;\n+      case RMWOp::FADD:\n+        rmwOp = \"add\";\n+        rmwOp += (valueElemNbits == 16 ? \".noftz\" : \"\");\n+        sTy = \"f\" + sBits;\n+        sTy += (vec == 2 && valueElemNbits == 16) ? \"x2\" : \"\";\n+        break;\n+      case RMWOp::MAX:\n+        sTy = \"s\" + sBits;\n+        break;\n+      case RMWOp::MIN:\n+        sTy = \"s\" + sBits;\n+        break;\n+      case RMWOp::UMAX:\n+        rmwOp = \"max\";\n+        sTy = \"u\" + sBits;\n+        break;\n+      case RMWOp::UMIN:\n+        rmwOp = \"min\";\n+        sTy = \"u\" + sBits;\n+        break;\n+      default:\n+        return failure();\n+      }\n+      atom.o(rmwOp).o(sTy);\n+\n+      atom(dstOpr, ptrOpr, valOpr);\n+      auto ret = ptxBuilder.launch(rewriter, loc, valueElemTy, false);\n+      for (int ii = 0; ii < vec; ++ii) {\n+        resultVals[i * vec + ii] =\n+            vec == 1 ? ret : extract_element(vecTy, ret, idx_val(ii));\n+      }\n+    }\n+    Type structTy = getTypeConverter()->convertType(valueTy);\n+    Value resultStruct =\n+        getStructFromElements(loc, resultVals, rewriter, structTy);\n+    rewriter.replaceOp(op, {resultStruct});\n+    return success();\n+  }\n+};\n+/// ====================== atomic_rmw codegen end ==========================\n+\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -5382,6 +5933,8 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   POPULATE_UNARY_OP(triton::PtrToIntOp, LLVM::PtrToIntOp)\n #undef POPULATE_UNARY_OP\n \n+  patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n+\n   patterns.add<FDivOpConversion>(typeConverter, benefit);\n \n   patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n@@ -5390,7 +5943,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   patterns.add<ReduceOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n                                           benefit);\n-\n+  patterns.add<AtomicRMWOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<ExtractSliceOpConversion>(typeConverter, allocation, smem,\n                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n@@ -5410,6 +5963,35 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n \n class ConvertTritonGPUToLLVM\n     : public ConvertTritonGPUToLLVMBase<ConvertTritonGPUToLLVM> {\n+\n+private:\n+  void decomposeBlockedToDotOperand(ModuleOp mod) {\n+    // replace `blocked -> dot_op` with `blocked -> shared -> dot_op`\n+    // because the codegen doesn't handle `blocked -> dot_op` directly\n+    mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n+      OpBuilder builder(cvtOp);\n+      auto srcType = cvtOp.getOperand().getType().cast<RankedTensorType>();\n+      auto dstType = cvtOp.getType().cast<RankedTensorType>();\n+      auto srcBlocked =\n+          srcType.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n+      auto dstDotOp =\n+          dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+      if (srcBlocked && dstDotOp) {\n+        auto tmpType = RankedTensorType::get(\n+            dstType.getShape(), dstType.getElementType(),\n+            triton::gpu::SharedEncodingAttr::get(mod.getContext(), dstDotOp,\n+                                                 srcType.getShape(),\n+                                                 srcType.getElementType()));\n+        auto tmp = builder.create<triton::gpu::ConvertLayoutOp>(\n+            cvtOp.getLoc(), tmpType, cvtOp.getOperand());\n+        auto newConvert = builder.create<triton::gpu::ConvertLayoutOp>(\n+            cvtOp.getLoc(), dstType, tmp);\n+        cvtOp.replaceAllUsesWith(newConvert.getResult());\n+        cvtOp.erase();\n+      }\n+    });\n+  }\n+\n public:\n   ConvertTritonGPUToLLVM() = default;\n \n@@ -5426,15 +6008,19 @@ class ConvertTritonGPUToLLVM\n \n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    // step 1: Allocate shared memories and insert barriers\n-    // setp 2: Convert SCF to CFG\n-    // step 3: Convert FuncOp to LLVMFuncOp via partial conversion\n-    // step 4: Convert the rest of ops via partial conversion\n+    // step 1: Decompose unoptimized layout conversions to use shared memory\n+    // step 2: Allocate shared memories and insert barriers\n+    // step 3: Convert SCF to CFG\n+    // step 4: Convert FuncOp to LLVMFuncOp via partial conversion\n+    // step 5: Convert the rest of ops via partial conversion\n     // The reason for putting step 1 before step 2 is that the membar analysis\n     // currently only supports SCF but not CFG.\n-    // The reason for a seperation between 1/4 is that, step 3 is out of\n+    // The reason for a separation between 1/4 is that, step 3 is out of\n     // the scope of Dialect Conversion, thus we need to make sure the smem\n     // is not revised during the conversion of step 4.\n+\n+    decomposeBlockedToDotOperand(mod);\n+\n     Allocation allocation(mod);\n     MembarAnalysis membar(&allocation);\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "file_content_changes": "@@ -221,6 +221,7 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n   matchAndRewrite(triton::DotOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = getTypeConverter()->convertType(op.getType());\n+    Attribute dEncoding = retType.cast<RankedTensorType>().getEncoding();\n     // a & b must be of smem layout\n     auto aType = adaptor.a().getType().cast<RankedTensorType>();\n     auto bType = adaptor.b().getType().cast<RankedTensorType>();\n@@ -230,17 +231,16 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n       return failure();\n     Value a = adaptor.a();\n     Value b = adaptor.b();\n-    SmallVector<unsigned, 2> order{1, 0};\n-    if (!aEncoding.isa<triton::gpu::SharedEncodingAttr>()) {\n+    if (!aEncoding.isa<triton::gpu::DotOperandEncodingAttr>()) {\n       Attribute encoding =\n-          triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, order);\n+          triton::gpu::DotOperandEncodingAttr::get(getContext(), 0, dEncoding);\n       auto dstType = RankedTensorType::get(aType.getShape(),\n                                            aType.getElementType(), encoding);\n       a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), dstType, a);\n     }\n-    if (!bEncoding.isa<triton::gpu::SharedEncodingAttr>()) {\n+    if (!bEncoding.isa<triton::gpu::DotOperandEncodingAttr>()) {\n       Attribute encoding =\n-          triton::gpu::SharedEncodingAttr::get(getContext(), 1, 1, 1, order);\n+          triton::gpu::DotOperandEncodingAttr::get(getContext(), 1, dEncoding);\n       auto dstType = RankedTensorType::get(bType.getShape(),\n                                            bType.getElementType(), encoding);\n       b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), dstType, b);\n@@ -371,6 +371,7 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n   patterns.add< // TODO: view should have custom pattern that views the layout\n       TritonGenericPattern<triton::ViewOp>,\n       TritonGenericPattern<triton::BitcastOp>,\n+      TritonGenericPattern<triton::FpToFpOp>,\n       TritonGenericPattern<triton::IntToPtrOp>,\n       TritonGenericPattern<triton::PtrToIntOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 41, "deletions": 6, "changes": 47, "file_content_changes": "@@ -124,6 +124,29 @@ void printStoreOp(OpAsmPrinter &printer, StoreOp storeOp) {\n namespace mlir {\n namespace triton {\n \n+//-- FpToFpOp --\n+bool FpToFpOp::areCastCompatible(::mlir::TypeRange inputs,\n+                               ::mlir::TypeRange outputs) {\n+  if (inputs.size() != 1 || outputs.size() != 1)\n+    return false;\n+  auto srcEltType = inputs.front();\n+  auto dstEltType = outputs.front();\n+  auto srcTensorType = srcEltType.dyn_cast<mlir::RankedTensorType>();\n+  auto dstTensorType = dstEltType.dyn_cast<mlir::RankedTensorType>();\n+  if (srcTensorType && dstTensorType) {\n+    srcEltType = srcTensorType.getElementType();\n+    dstEltType = dstTensorType.getElementType();\n+  }\n+  // Check whether fp8 <=> fp16, bf16, f32, f64\n+  // Make `srcEltType` always the fp8 side\n+  if (dstEltType.dyn_cast<mlir::triton::Float8Type>())\n+    std::swap(srcEltType, dstEltType);\n+  if (!srcEltType.dyn_cast<mlir::triton::Float8Type>())\n+    return false;\n+  return dstEltType.isF16() || dstEltType.isBF16() ||\n+         dstEltType.isF32() || dstEltType.isF64();\n+}\n+\n //-- StoreOp --\n void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                     ::mlir::Value ptr, ::mlir::Value value) {\n@@ -191,6 +214,20 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n   // type is the same as the accumulator\n   auto accTy = operands[2].getType().cast<RankedTensorType>();\n   inferredReturnTypes.push_back(accTy);\n+\n+  // verify encodings\n+  auto aEnc = operands[0].getType().cast<RankedTensorType>().getEncoding();\n+  auto bEnc = operands[1].getType().cast<RankedTensorType>().getEncoding();\n+  auto retEnc = accTy.getEncoding();\n+  if (aEnc) {\n+    assert(bEnc);\n+    Dialect &dialect = aEnc.getDialect();\n+    auto interface = dyn_cast<DialectInferLayoutInterface>(&dialect);\n+    if (interface->inferDotOpEncoding(aEnc, 0, retEnc, location).failed())\n+      return mlir::failure();\n+    if (interface->inferDotOpEncoding(bEnc, 1, retEnc, location).failed())\n+      return mlir::failure();\n+  }\n   return mlir::success();\n }\n \n@@ -244,7 +281,7 @@ OpFoldResult SplatOp::fold(ArrayRef<Attribute> operands) {\n \n //-- ExpandDimsOp --\n mlir::LogicalResult mlir::triton::ExpandDimsOp::inferReturnTypes(\n-    MLIRContext *context, Optional<Location> location, ValueRange operands,\n+    MLIRContext *context, Optional<Location> loc, ValueRange operands,\n     DictionaryAttr attributes, RegionRange regions,\n     SmallVectorImpl<Type> &inferredReturnTypes) {\n   // infer shape\n@@ -260,11 +297,9 @@ mlir::LogicalResult mlir::triton::ExpandDimsOp::inferReturnTypes(\n     Dialect &dialect = argEncoding.getDialect();\n     auto inferLayoutInterface = dyn_cast<DialectInferLayoutInterface>(&dialect);\n     if (inferLayoutInterface\n-            ->inferExpandDimsOpEncoding(argEncoding, axis, retEncoding)\n-            .failed()) {\n-      llvm::report_fatal_error(\"failed to infer layout for ExpandDimsOp\");\n-      return mlir::failure();\n-    }\n+            ->inferExpandDimsOpEncoding(argEncoding, axis, retEncoding, loc)\n+            .failed())\n+      return emitOptionalError(loc, \"failed to infer layout for ExpandDimsOp\");\n   }\n   // create type\n   auto argEltTy = argTy.getElementType();"}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -48,7 +48,8 @@ mlir::LogicalResult mlir::OpTrait::impl::verifyTensorSize(Operation *op) {\n                << \" has more than that\";\n       if ((numElements & (numElements - 1)) != 0)\n         return op->emitError(\"Number of elements must be power-of-two, but \")\n-               << *op << \" doesn't follow the rule\";\n+               << *op << \" doesn't follow the rule (\" << numElements << \")\"\n+               << \" elements\";\n     }\n   }\n   for (auto opType : op->getResultTypes()) {\n@@ -62,7 +63,8 @@ mlir::LogicalResult mlir::OpTrait::impl::verifyTensorSize(Operation *op) {\n                << \" has more than that\";\n       if ((numElements & (numElements - 1)) != 0)\n         return op->emitError(\"Number of elements must be power-of-two, but \")\n-               << *op << \" doesn't follow the rule\";\n+               << *op << \" doesn't follow the rule (\" << numElements << \")\"\n+               << \" elements\";\n     }\n   }\n   return success();"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 151, "deletions": 64, "changes": 215, "file_content_changes": "@@ -42,13 +42,11 @@ static Type getPointeeType(Type type) {\n \n namespace gpu {\n \n-// TODO: Inheritation of layout attributes\n-unsigned getElemsPerThread(Type type) {\n-  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n-    return 1;\n-  auto tensorType = type.cast<RankedTensorType>();\n-  auto layout = tensorType.getEncoding();\n-  auto shape = tensorType.getShape();\n+// TODO: Inheritance of layout attributes\n+// so that all distributed layouts implement\n+// these utilities\n+\n+unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return blockedLayout.getElemsPerThread(shape);\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n@@ -57,12 +55,51 @@ unsigned getElemsPerThread(Type type) {\n     return mmaLayout.getElemsPerThread(shape);\n   } else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n     return sharedLayout.getElemsPerThread(shape);\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    return dotLayout.getElemsPerThread(shape);\n   } else {\n     assert(0 && \"getElemsPerThread not implemented\");\n     return 0;\n   }\n }\n \n+unsigned getElemsPerThread(Type type) {\n+  if (type.isIntOrIndexOrFloat() ||\n+      type.isa<triton::Float8Type>() ||\n+      type.isa<triton::PointerType>())\n+    return 1;\n+  auto tensorType = type.cast<RankedTensorType>();\n+  return getElemsPerThread(tensorType.getEncoding(), tensorType.getShape());\n+}\n+\n+SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return SmallVector<unsigned>(blockedLayout.getThreadsPerWarp().begin(),\n+                                 blockedLayout.getThreadsPerWarp().end());\n+  }\n+  if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    if (mmaLayout.getVersion() == 1)\n+      return SmallVector<unsigned>{4, 8};\n+    if (mmaLayout.getVersion() == 2)\n+      return SmallVector<unsigned>{8, 4};\n+  }\n+  assert(0 && \"getThreadsPerWarp not implemented\");\n+  return {};\n+}\n+\n+SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return SmallVector<unsigned>(blockedLayout.getWarpsPerCTA().begin(),\n+                                 blockedLayout.getWarpsPerCTA().end());\n+  }\n+  if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    return SmallVector<unsigned>(mmaLayout.getWarpsPerCTA().begin(),\n+                                 mmaLayout.getWarpsPerCTA().end());\n+  }\n+  assert(0 && \"getWarpsPerCTA not implemented\");\n+  return {};\n+}\n+\n SmallVector<unsigned> getSizePerThread(Attribute layout) {\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     return SmallVector<unsigned>(blockedLayout.getSizePerThread().begin(),\n@@ -73,6 +110,27 @@ SmallVector<unsigned> getSizePerThread(Attribute layout) {\n     assert(mmaLayout.getVersion() == 2 &&\n            \"mmaLayout version = 1 is not implemented yet\");\n     return SmallVector<unsigned>{2, 2};\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    auto parentLayout = dotLayout.getParent();\n+    assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n+    if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert(parentMmaLayout.getVersion() == 2 &&\n+             \"mmaLayout version = 1 is not implemented yet\");\n+      auto parentShapePerCTA = getShapePerCTA(parentLayout);\n+      auto opIdx = dotLayout.getOpIdx();\n+      if (opIdx == 0) {\n+        return {2, 4};\n+      } else if (opIdx == 1) {\n+        return {4, 1};\n+      } else {\n+        assert(0 && \"DotOperandEncodingAttr opIdx must be 0 or 1\");\n+        return {};\n+      }\n+    } else {\n+      assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n+                  \"supported yet\");\n+      return {};\n+    }\n   } else {\n     assert(0 && \"getSizePerThread not implemented\");\n     return {};\n@@ -104,17 +162,11 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     unsigned dim = sliceLayout.getDim();\n     auto parent = sliceLayout.getParent();\n-    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n-      for (unsigned d = 0, n = blockedParent.getOrder().size(); d < n; ++d) {\n-        if (d == dim)\n-          continue;\n-        shape.push_back(blockedParent.getSizePerThread()[d] *\n-                        blockedParent.getThreadsPerWarp()[d] *\n-                        blockedParent.getWarpsPerCTA()[d]);\n-      }\n-    } else {\n-      assert(0 && \"SliceEncodingAttr with parent other than \"\n-                  \"BlockedEncodingAttr not implemented\");\n+    for (unsigned d = 0, n = getOrder(parent).size(); d < n; ++d) {\n+      if (d == dim)\n+        continue;\n+      shape.push_back(getSizePerThread(parent)[d] *\n+                      getThreadsPerWarp(parent)[d] * getWarpsPerCTA(parent)[d]);\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.getVersion() == 2)\n@@ -124,6 +176,25 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n       return {16 * mmaLayout.getWarpsPerCTA()[0],\n               16 * mmaLayout.getWarpsPerCTA()[1]};\n     assert(0 && \"Unexpected MMA layout version found\");\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    auto parentLayout = dotLayout.getParent();\n+    assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n+    if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert(parentMmaLayout.getVersion() == 2 &&\n+             \"mmaLayout version = 1 is not implemented yet\");\n+      auto parentShapePerCTA = getShapePerCTA(parentLayout);\n+      auto opIdx = dotLayout.getOpIdx();\n+      if (opIdx == 0) {\n+        return {parentShapePerCTA[0], 16};\n+      } else if (opIdx == 1) {\n+        return {16, parentShapePerCTA[1]};\n+      } else {\n+        assert(0 && \"DotOperandEncodingAttr opIdx must be 0 or 1\");\n+      }\n+    } else {\n+      assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n+                  \"supported yet\");\n+    }\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n@@ -136,6 +207,8 @@ SmallVector<unsigned> getOrder(const Attribute &layout) {\n                                  blockedLayout.getOrder().end());\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     return SmallVector<unsigned>{1, 0};\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    return SmallVector<unsigned>{1, 0};\n   } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     SmallVector<unsigned> parentOrder = getOrder(sliceLayout.getParent());\n     unsigned dim = sliceLayout.getDim();\n@@ -243,11 +316,11 @@ unsigned BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   return product<unsigned>(elemsPerThread);\n }\n \n-SmallVector<int64_t>\n-SliceEncodingAttr::paddedShape(ArrayRef<int64_t> shape) const {\n+template <class T>\n+SmallVector<T> SliceEncodingAttr::paddedShape(ArrayRef<T> shape) const {\n   size_t rank = shape.size();\n   unsigned dim = getDim();\n-  SmallVector<int64_t> retShape(rank + 1);\n+  SmallVector<T> retShape(rank + 1);\n   for (unsigned d = 0; d < rank + 1; ++d) {\n     if (d < dim)\n       retShape[d] = shape[d];\n@@ -258,18 +331,15 @@ SliceEncodingAttr::paddedShape(ArrayRef<int64_t> shape) const {\n   }\n   return retShape;\n }\n+template SmallVector<unsigned>\n+SliceEncodingAttr::paddedShape<unsigned>(ArrayRef<unsigned> shape) const;\n+template SmallVector<int64_t>\n+SliceEncodingAttr::paddedShape<int64_t>(ArrayRef<int64_t> shape) const;\n \n unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n   auto parent = getParent();\n-  if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n-    assert(rank == blockedParent.getSizePerThread().size() - 1 &&\n-           \"unexpected rank in SliceEncodingAttr::getElemsPerThread\");\n-    return blockedParent.getElemsPerThread(paddedShape(shape));\n-  } else {\n-    assert(0 && \"getElemsPerThread not implemented\");\n-    return 0;\n-  }\n+  return ::getElemsPerThread(parent, paddedShape(shape));\n }\n \n unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n@@ -300,6 +370,12 @@ unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   return 0;\n }\n \n+unsigned\n+DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  assert(0 && \"DotOPerandEncodingAttr::getElemsPerThread not implemented\");\n+  return 0;\n+}\n+\n //===----------------------------------------------------------------------===//\n // Blocked Encoding\n //===----------------------------------------------------------------------===//\n@@ -471,6 +547,30 @@ void SharedEncodingAttr::print(AsmPrinter &printer) const {\n           << \"}>\";\n }\n \n+//===----------------------------------------------------------------------===//\n+// DotOperand Encoding\n+//===----------------------------------------------------------------------===//\n+Attribute DotOperandEncodingAttr::parse(AsmParser &parser, Type type) {\n+  if (parser.parseLess().failed())\n+    return {};\n+  NamedAttrList attrs;\n+  if (parser.parseOptionalAttrDict(attrs).failed())\n+    return {};\n+  if (parser.parseGreater().failed())\n+    return {};\n+  unsigned opIdx = attrs.get(\"opIdx\").cast<IntegerAttr>().getInt();\n+  Attribute parent = attrs.get(\"parent\");\n+\n+  return parser.getChecked<DotOperandEncodingAttr>(parser.getContext(), opIdx,\n+                                                   parent);\n+}\n+\n+void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n+  printer << \"<{\"\n+          << \"opIdx = \" << getOpIdx() << \", \"\n+          << \"parent = \" << getParent() << \"}>\";\n+}\n+\n //===----------------------------------------------------------------------===//\n // InsertSliceAsyncOp\n //===----------------------------------------------------------------------===//\n@@ -530,30 +630,6 @@ void printInsertSliceAsyncOp(OpAsmPrinter &printer,\n   printer.printStrippedAttrOrType(insertSliceAsyncOp.result().getType());\n }\n \n-//===----------------------------------------------------------------------===//\n-// DotOperand Encoding\n-//===----------------------------------------------------------------------===//\n-Attribute DotOperandEncodingAttr::parse(AsmParser &parser, Type type) {\n-  if (parser.parseLess().failed())\n-    return {};\n-  NamedAttrList attrs;\n-  if (parser.parseOptionalAttrDict(attrs).failed())\n-    return {};\n-  if (parser.parseGreater().failed())\n-    return {};\n-  unsigned opIdx = attrs.get(\"opIdx\").cast<IntegerAttr>().getInt();\n-  Attribute parent = attrs.get(\"parent\");\n-\n-  return parser.getChecked<DotOperandEncodingAttr>(parser.getContext(), opIdx,\n-                                                   parent);\n-}\n-\n-void DotOperandEncodingAttr::print(mlir::AsmPrinter &printer) const {\n-  printer << \"<{\"\n-          << \"opIdx = \" << getOpIdx() << \", \"\n-          << \"parent = \" << getParent() << \"}>\";\n-}\n-\n //===----------------------------------------------------------------------===//\n // ASM Interface (i.e.: alias)\n //===----------------------------------------------------------------------===//\n@@ -594,21 +670,32 @@ struct TritonGPUInferLayoutInterface\n \n   LogicalResult\n   inferExpandDimsOpEncoding(Attribute operandEncoding, unsigned axis,\n-                            Attribute &resultEncoding) const override {\n+                            Attribute &resultEncoding,\n+                            Optional<Location> location) const override {\n     auto sliceEncoding = operandEncoding.dyn_cast<SliceEncodingAttr>();\n-    if (!sliceEncoding) {\n-      llvm::report_fatal_error(\n-          \"ExpandDimsOp operand encoding must be SliceEncodingAttr\");\n-      return failure();\n-    }\n-    if (sliceEncoding.getDim() != axis) {\n-      llvm::report_fatal_error(\n-          \"Incompatible slice dimension for ExpandDimsOp operand\");\n-      return failure();\n-    }\n+    if (!sliceEncoding)\n+      return emitOptionalError(\n+          location, \"ExpandDimsOp operand encoding must be SliceEncodingAttr\");\n+    if (sliceEncoding.getDim() != axis)\n+      return emitOptionalError(\n+          location, \"Incompatible slice dimension for ExpandDimsOp operand\");\n     resultEncoding = sliceEncoding.getParent();\n     return success();\n   }\n+\n+  LogicalResult inferDotOpEncoding(Attribute operandEncoding, unsigned opIdx,\n+                                   Attribute retEncoding,\n+                                   Optional<Location> location) const override {\n+    if (auto dotOpEnc = operandEncoding.dyn_cast<DotOperandEncodingAttr>()) {\n+      if (opIdx != dotOpEnc.getOpIdx())\n+        return emitOptionalError(location, \"Wrong opIdx\");\n+      if (retEncoding != dotOpEnc.getParent())\n+        return emitOptionalError(location, \"Incompatible parent encoding\");\n+    } else\n+      return emitOptionalError(\n+          location, \"Dot's a/b's encoding should be of DotOperandEncodingAttr\");\n+    return success();\n+  }\n };\n \n void TritonGPUDialect::initialize() {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -7,7 +7,7 @@ add_mlir_dialect_library(TritonGPUTransforms\n   CanonicalizeLoops.cpp\n   Combine.cpp\n   Pipeline.cpp\n-  Swizzle.cpp\n+  Prefetch.cpp\n   TritonGPUConversion.cpp\n \n   DEPENDS"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -32,7 +32,10 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n     PointerType ptrType = origType.getElementType().cast<PointerType>();\n-    unsigned numBits = ptrType.getPointeeType().getIntOrFloatBitWidth();\n+    auto pointeeType = ptrType.getPointeeType();\n+    unsigned numBits =\n+        pointeeType.isa<triton::Float8Type>() ?\n+        8 : pointeeType.getIntOrFloatBitWidth();\n     unsigned maxMultiple = info.getDivisibility(order[0]);\n     unsigned maxContig = info.getContiguity(order[0]);\n     unsigned alignment = std::min(maxMultiple, maxContig);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 65, "deletions": 31, "changes": 96, "file_content_changes": "@@ -12,21 +12,13 @@\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"mlir/Transforms/RegionUtils.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n \n #include <memory>\n \n using namespace mlir;\n-\n-static bool isSharedLayout(Value v) {\n-  if (auto tensorType = v.getType().dyn_cast<RankedTensorType>()) {\n-    Attribute encoding = tensorType.getEncoding();\n-    return encoding.isa<triton::gpu::SharedEncodingAttr>();\n-  }\n-  return false;\n-}\n-\n namespace {\n #include \"TritonGPUCombine.inc\"\n \n@@ -37,7 +29,7 @@ namespace {\n // convert(blocked, dot_operand) ->\n // convert(blocked, mma) + convert(mma,  dot_operand)\n // if this value is itself the result of a dot operation\n-// this is a hueiristics to accomodate some pattern seen in fused attention\n+// this is a heuristic to accomodate some pattern seen in fused attention\n // kernels.\n // TODO: replace this by something more generic, i.e. layout-aware CSE\n class DecomposeDotOperand : public mlir::RewritePattern {\n@@ -59,9 +51,8 @@ class DecomposeDotOperand : public mlir::RewritePattern {\n         dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>()) {\n       auto tmpType =\n           RankedTensorType::get(dstType.getShape(), dstType.getElementType(),\n-                                dstType.getEncoding()\n-                                    .cast<triton::gpu::DotOperandEncodingAttr>()\n-                                    .getParent());\n+                                triton::gpu::SharedEncodingAttr::get(\n+                                    op->getContext(), 1, 1, 1, {1, 0}));\n       auto tmp = rewriter.create<triton::gpu::ConvertLayoutOp>(\n           convert.getLoc(), tmpType, convert.getOperand());\n       auto newConvert = rewriter.create<triton::gpu::ConvertLayoutOp>(\n@@ -87,11 +78,12 @@ class SimplifyConversion : public mlir::RewritePattern {\n     if (!llvm::isa<triton::gpu::ConvertLayoutOp>(op))\n       return mlir::failure();\n     auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n+    auto srcType = convert.getOperand().getType().cast<RankedTensorType>();\n     auto dstType = convert.getType().cast<RankedTensorType>();\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accomodate fused attention\n-    if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n-      return mlir::failure();\n+    // if (dstType.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n+    //   return mlir::failure();\n     // convert to the same layout -- we can delete\n     if (op->getResultTypes() == op->getOperandTypes()) {\n       rewriter.replaceOp(op, op->getOperands());\n@@ -122,8 +114,8 @@ class SimplifyConversion : public mlir::RewritePattern {\n       rewriter.replaceOpWithNewOp<triton::gpu::InsertSliceAsyncOp>(\n           op, newType, insert_slice.src(), newArg.getResult(),\n           insert_slice.index(), insert_slice.mask(), insert_slice.other(),\n-          insert_slice.cache(), insert_slice.evict(),\n-          insert_slice.isVolatile(), insert_slice.axis());\n+          insert_slice.cache(), insert_slice.evict(), insert_slice.isVolatile(),\n+          insert_slice.axis());\n       return mlir::success();\n     }\n     // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n@@ -133,7 +125,10 @@ class SimplifyConversion : public mlir::RewritePattern {\n       auto newType = RankedTensorType::get(\n           origType.getShape(), origType.getElementType(),\n           op->getResult(0).getType().cast<RankedTensorType>().getEncoding());\n-      auto resType = op->getResult(0).getType().cast<RankedTensorType>();\n+      auto origResType = op->getResult(0).getType().cast<RankedTensorType>();\n+      auto resType = RankedTensorType::get(\n+          origResType.getShape(), origResType.getElementType(),\n+          extract_slice.getType().cast<RankedTensorType>().getEncoding());\n       // Ensure that the new extract_slice op is placed in the same place as the\n       // old extract_slice op. Otherwise, the new extract_slice op may be placed\n       // after the async_wait op, which is not allowed.\n@@ -148,8 +143,21 @@ class SimplifyConversion : public mlir::RewritePattern {\n           extract_slice.static_strides());\n       return mlir::success();\n     }\n+\n     // cvt(type2, x)\n     if (llvm::isa<triton::gpu::ConvertLayoutOp>(arg)) {\n+      auto argType = arg->getOperand(0).getType().cast<RankedTensorType>();\n+      if (arg->getOperand(0).getDefiningOp() &&\n+          !argType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n+          srcType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n+          !dstType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n+\n+        return mlir::failure();\n+      }\n+      auto srcShared =\n+          srcType.getEncoding().dyn_cast<triton::gpu::SharedEncodingAttr>();\n+      if (srcShared && srcShared.getVec() > 1)\n+        return mlir::failure();\n       rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n           op, op->getResultTypes().front(), arg->getOperand(0));\n       return mlir::success();\n@@ -253,8 +261,8 @@ class RematerializeBackward : public mlir::RewritePattern {\n     if (!op)\n       return mlir::failure();\n     // we don't want to rematerialize any conversion to/from shared\n-    if (isSharedLayout(cvt->getResults()[0]) ||\n-        isSharedLayout(cvt->getOperand(0)))\n+    if (isSharedEncoding(cvt->getResults()[0]) ||\n+        isSharedEncoding(cvt->getOperand(0)))\n       return mlir::failure();\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accomodate fused attention\n@@ -325,7 +333,6 @@ class RematerializeBackward : public mlir::RewritePattern {\n     for (Operation *op : tmp)\n       sortedValues.push_back(op->getResult(0));\n \n-    // llvm::outs() << \"----\\n\";\n     BlockAndValueMapping mapping;\n     for (Value currOperand : sortedValues) {\n       // unpack information\n@@ -346,7 +353,6 @@ class RematerializeBackward : public mlir::RewritePattern {\n         newOperand->moveAfter(currOperation);\n       mapping.map(currOperand, newOperand);\n     }\n-    //  llvm::outs() << cvt->getParentOfType<mlir::FuncOp>() << \"\\n\";\n     rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n     return mlir::success();\n   }\n@@ -356,8 +362,6 @@ class RematerializeBackward : public mlir::RewritePattern {\n //\n // -----------------------------------------------------------------------------\n \n-// int test = 0;\n-\n class MoveConvertOutOfLoop : public mlir::RewritePattern {\n public:\n   MoveConvertOutOfLoop(mlir::MLIRContext *context)\n@@ -435,9 +439,25 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n       auto users = iterArg.value().getUsers();\n       // check first condition\n       SetVector<Type> cvtTargetTypes;\n-      for (auto user : users)\n-        if (isa<triton::gpu::ConvertLayoutOp>(user))\n-          cvtTargetTypes.insert(user->getResults()[0].getType());\n+      for (auto user : users) {\n+        if (isa<triton::gpu::ConvertLayoutOp>(user)) {\n+          auto newType =\n+              user->getResults()[0].getType().cast<RankedTensorType>();\n+          auto oldType = user->getOperand(0).getType().cast<RankedTensorType>();\n+          if (oldType.getEncoding().isa<triton::gpu::SharedEncodingAttr>() &&\n+              newType.getEncoding()\n+                  .isa<triton::gpu::DotOperandEncodingAttr>()) {\n+            continue;\n+          }\n+          if (newType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n+            if (newType.getEncoding()\n+                    .cast<triton::gpu::SharedEncodingAttr>()\n+                    .getVec() == 1)\n+              continue;\n+          }\n+          cvtTargetTypes.insert(newType);\n+        }\n+      }\n       if (cvtTargetTypes.size() != 1)\n         continue;\n       // TODO: check second condition\n@@ -446,6 +466,7 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n           continue;\n       }\n       // check\n+      // llvm::outs() << \"replacing \" << iterArg.index() << \"\\n\";\n       for (auto op : iterArg.value().getUsers()) {\n         auto cvt = dyn_cast<triton::gpu::ConvertLayoutOp>(op);\n         if (!cvt)\n@@ -597,10 +618,23 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldAcc = dotOp.getOperand(2);\n     auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         oldAcc.getLoc(), newRetType, oldAcc);\n-    // convert output\n+    Value a = dotOp.a();\n+    Value b = dotOp.b();\n+    auto oldAType = a.getType().cast<RankedTensorType>();\n+    auto oldBType = b.getType().cast<RankedTensorType>();\n+    auto newAType = RankedTensorType::get(\n+        oldAType.getShape(), oldAType.getElementType(),\n+        triton::gpu::DotOperandEncodingAttr::get(oldAType.getContext(), 0,\n+                                                 newRetType.getEncoding()));\n+    auto newBType = RankedTensorType::get(\n+        oldBType.getShape(), oldBType.getElementType(),\n+        triton::gpu::DotOperandEncodingAttr::get(oldBType.getContext(), 1,\n+                                                 newRetType.getEncoding()));\n+    a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n+    b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n     auto newDot = rewriter.create<triton::DotOp>(\n-        dotOp.getLoc(), newRetType, dotOp.getOperand(0), dotOp.getOperand(1),\n-        newAcc, dotOp.allowTF32(), dotOp.transA(), dotOp.transB());\n+        dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.allowTF32(),\n+        dotOp.transA(), dotOp.transB());\n \n     rewriter.replaceOpWithNewOp<triton::gpu::ConvertLayoutOp>(\n         op, oldRetType, newDot.getResult());\n@@ -623,7 +657,7 @@ class TritonGPUCombineOpsPass\n     mlir::RewritePatternSet patterns(context);\n \n     patterns.add<SimplifyConversion>(context);\n-    patterns.add<DecomposeDotOperand>(context);\n+    // patterns.add<DecomposeDotOperand>(context);\n     patterns.add<RematerializeBackward>(context);\n     patterns.add<RematerializeForward>(context);\n     patterns.add<MoveConvertOutOfLoop>(context);"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 64, "deletions": 24, "changes": 88, "file_content_changes": "@@ -1,3 +1,4 @@\n+#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/BlockAndValueMapping.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n@@ -11,6 +12,7 @@\n //===----------------------------------------------------------------------===//\n \n using namespace mlir;\n+namespace ttg = triton::gpu;\n \n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n@@ -24,6 +26,7 @@ static Type getI1SameShape(Value v) {\n }\n \n namespace {\n+\n class LoopPipeliner {\n   /// cache forOp we are working on\n   scf::ForOp forOp;\n@@ -37,6 +40,8 @@ class LoopPipeliner {\n   DenseMap<Value, Value> loadsMapping;\n   /// load => buffer\n   DenseMap<Value, Value> loadsBuffer;\n+  /// load => buffer type (with shared layout after swizzling)\n+  DenseMap<Value, RankedTensorType> loadsBufferType;\n   /// load => buffer at stage N\n   DenseMap<Value, SmallVector<Value>> loadStageBuffer;\n   /// load => after extract\n@@ -67,8 +72,7 @@ class LoopPipeliner {\n   Value lookupOrDefault(Value origin, int stage);\n \n   /// returns a empty buffer of size <numStages, ...>\n-  triton::gpu::AllocTensorOp allocateEmptyBuffer(Operation *op,\n-                                                 OpBuilder &builder);\n+  ttg::AllocTensorOp allocateEmptyBuffer(Operation *op, OpBuilder &builder);\n \n public:\n   LoopPipeliner(scf::ForOp forOp, int numStages)\n@@ -128,21 +132,14 @@ void LoopPipeliner::collectDeps(Value v, int stages, DenseSet<Value> &deps) {\n   }\n }\n \n-triton::gpu::AllocTensorOp\n-LoopPipeliner::allocateEmptyBuffer(Operation *op, OpBuilder &builder) {\n+ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n+                                                      OpBuilder &builder) {\n   // allocate a buffer for each pipelined tensor\n   // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n   Value convertLayout = loadsMapping[op->getResult(0)];\n   if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>()) {\n-    SmallVector<int64_t> shape(tensorType.getShape().begin(),\n-                               tensorType.getShape().end());\n-    shape.insert(shape.begin(), numStages);\n-    Type elementType = tensorType.getElementType();\n-    // The encoding of the buffer is similar to the original tensor\n-    Attribute encoding = tensorType.getEncoding();\n-    auto bufferType = RankedTensorType::get(shape, elementType, encoding);\n-    return builder.create<triton::gpu::AllocTensorOp>(convertLayout.getLoc(),\n-                                                      bufferType);\n+    return builder.create<ttg::AllocTensorOp>(\n+        convertLayout.getLoc(), loadsBufferType[op->getResult(0)]);\n   }\n   llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n }\n@@ -186,19 +183,27 @@ LogicalResult LoopPipeliner::initialize() {\n       }\n     }\n \n-    // For now, we only pipeline loads that have one covert_layout (to smem) use\n+    // We only pipeline loads that have one covert_layout (to dot_op) use\n     // TODO: lift this constraint in the future\n     if (isCandiate && loadOp.getResult().hasOneUse()) {\n       isCandiate = false;\n       Operation *use = *loadOp.getResult().getUsers().begin();\n-      if (auto convertLayout =\n-              llvm::dyn_cast<triton::gpu::ConvertLayoutOp>(use)) {\n+      if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use)) {\n         if (auto tensorType = convertLayout.getResult()\n                                   .getType()\n                                   .dyn_cast<RankedTensorType>()) {\n-          if (tensorType.getEncoding().isa<triton::gpu::SharedEncodingAttr>()) {\n+          if (auto dotOpEnc = tensorType.getEncoding()\n+                                  .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n             isCandiate = true;\n             loadsMapping[loadOp] = convertLayout;\n+            auto ty = loadOp.getType().cast<RankedTensorType>();\n+            SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n+                                             ty.getShape().end());\n+            bufferShape.insert(bufferShape.begin(), numStages);\n+            auto sharedEnc = ttg::SharedEncodingAttr::get(\n+                ty.getContext(), dotOpEnc, ty.getShape(), ty.getElementType());\n+            loadsBufferType[loadOp] = RankedTensorType::get(\n+                bufferShape, ty.getElementType(), sharedEnc);\n           }\n         }\n       }\n@@ -238,6 +243,9 @@ void LoopPipeliner::emitPrologue() {\n     setValueMapping(arg, operand.get(), 0);\n   }\n \n+  // helper to construct int attribute\n+  auto intAttr = [&](int64_t val) { return builder.getI64IntegerAttr(val); };\n+\n   // prologue from [0, numStage-1)\n   Value iv = forOp.getLowerBound();\n   pipelineIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n@@ -330,14 +338,15 @@ void LoopPipeliner::emitPrologue() {\n         builder.create<arith::ConstantIntOp>(iv.getLoc(), 1, 32));\n   } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n-  auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n-\n   // async.wait & extract_slice\n-  builder.create<triton::gpu::AsyncWaitOp>(loads[0].getLoc(),\n-                                           loads.size() * (numStages - 2));\n+  builder.create<ttg::AsyncWaitOp>(loads[0].getLoc(),\n+                                   loads.size() * (numStages - 2));\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (Value loadOp : loads) {\n     auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n+    sliceType =\n+        RankedTensorType::get(sliceType.getShape(), sliceType.getElementType(),\n+                              loadsBufferType[loadOp].getEncoding());\n     Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{intAttr(0), intAttr(0), intAttr(0)},\n@@ -366,6 +375,7 @@ void LoopPipeliner::emitEpilogue() {\n \n scf::ForOp LoopPipeliner::createNewForOp() {\n   OpBuilder builder(forOp);\n+  auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n \n   // order of new args:\n   //   (original args),\n@@ -477,8 +487,6 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   extractSliceIndex = builder.create<arith::IndexCastOp>(\n       extractSliceIndex.getLoc(), builder.getIndexType(), extractSliceIndex);\n \n-  auto intAttr = [&](int64_t v) { return builder.getI64IntegerAttr(v); };\n-\n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n     // update loading mask\n@@ -508,6 +516,9 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n       nextBuffers.push_back(insertAsyncOp);\n       auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n+      sliceType = RankedTensorType::get(sliceType.getShape(),\n+                                        sliceType.getElementType(),\n+                                        loadsBufferType[loadOp].getEncoding());\n       nextOp = builder.create<tensor::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, intAttr(0), intAttr(0)},\n@@ -534,8 +545,37 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     }\n   }\n \n+  {\n+    OpBuilder::InsertionGuard guard(builder);\n+    for (Operation &op : *newForOp.getBody()) {\n+      if (auto dotOp = llvm::dyn_cast<triton::DotOp>(&op)) {\n+        builder.setInsertionPoint(&op);\n+        auto dotType = dotOp.getType().cast<RankedTensorType>();\n+        Value a = dotOp.a();\n+        Value b = dotOp.b();\n+        auto layoutCast = [&](Value dotOperand, int opIdx) -> Value {\n+          auto tensorType = dotOperand.getType().cast<RankedTensorType>();\n+          if (!tensorType.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n+            auto newEncoding = ttg::DotOperandEncodingAttr::get(\n+                tensorType.getContext(), opIdx, dotType.getEncoding());\n+            auto newType =\n+                RankedTensorType::get(tensorType.getShape(),\n+                                      tensorType.getElementType(), newEncoding);\n+            return builder.create<ttg::ConvertLayoutOp>(dotOperand.getLoc(),\n+                                                        newType, dotOperand);\n+          }\n+          return dotOperand;\n+        };\n+        a = layoutCast(a, 0);\n+        b = layoutCast(b, 1);\n+        dotOp->setOperand(0, a);\n+        dotOp->setOperand(1, b);\n+      }\n+    }\n+  }\n+\n   // async.wait & extract_slice\n-  Operation *asyncWait = builder.create<triton::gpu::AsyncWaitOp>(\n+  Operation *asyncWait = builder.create<ttg::AsyncWaitOp>(\n       loads[0].getLoc(), loads.size() * (numStages - 2));\n   for (auto it = extractSlices.rbegin(); it != extractSlices.rend(); ++it) {\n     // move extract_slice after asyncWait"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "added", "additions": 304, "deletions": 0, "changes": 304, "file_content_changes": "@@ -0,0 +1,304 @@\n+//===----------------------------------------------------------------------===//\n+//\n+// This pass tries to prefetch operands (a and b) of tt.dot.\n+// Those ConvertLayoutOps will be lowered to shared memory loads.\n+//\n+// For example:\n+// %a: tensor<128x32xf16, #enc>\n+// scf.for %iv = ... iter_args(%a_arg = %a, ...) {\n+//   %d = tt.dot %a_arg, %b, %c\n+//   ...\n+//   scf.yield %a_next, ...\n+// }\n+//\n+// will be translated to\n+//\n+// %a: tensor<128x32xf16, #enc>\n+// %a_tmp = tensor.extract_slice %a[0, 0] [128, 16]\n+// %a_prefetch = triton_gpu.convert_layout %a_tmp\n+// scf.for %iv = ... iter_args(%a_buf = %a, ..., %a_prefetch_arg = %a_prefetch)\n+// {\n+//   %x = tt.dot %a_arg, %b, %c\n+//   %a_tmp_rem = tensor.extract_slice %a_buf[0, 16] [128, 16]\n+//   %a_prefetch_next = triton_gpu.convert_layout %a_tmp_rem\n+//   ...\n+//   scf.yield %next_a, ..., %a_prefetch_next\n+// }\n+//===----------------------------------------------------------------------===//\n+\n+#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+\n+using namespace mlir;\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+namespace {\n+\n+class Prefetcher {\n+  /// cache the ForOp we are working on\n+  scf::ForOp forOp;\n+  /// cache the YieldOp of this ForOp\n+  scf::YieldOp yieldOp;\n+  ///\n+  // TODO: add a hook to infer prefetchWidth\n+  unsigned prefetchWidth = 16;\n+\n+  /// dots to be prefetched\n+  SetVector<Value> dots;\n+  /// dot => dot operand\n+  DenseMap<Value, Value> dot2aLoopArg;\n+  DenseMap<Value, Value> dot2aHeaderDef;\n+  DenseMap<Value, Value> dot2bLoopArg;\n+  DenseMap<Value, Value> dot2bHeaderDef;\n+  DenseMap<Value, Value> dot2aYield;\n+  DenseMap<Value, Value> dot2bYield;\n+  /// operand => defining\n+  DenseMap<Value, Value> operand2headPrefetch;\n+\n+  LogicalResult isForOpOperand(Value v);\n+\n+  Value generatePrefetch(Value v, unsigned opIdx, bool isPrefetch,\n+                         Attribute dotEncoding, OpBuilder &builder,\n+                         llvm::Optional<int64_t> offsetK = llvm::None,\n+                         llvm::Optional<int64_t> shapeK = llvm::None);\n+\n+public:\n+  Prefetcher() = delete;\n+\n+  Prefetcher(scf::ForOp forOp) : forOp(forOp) {\n+    yieldOp = cast<scf::YieldOp>(forOp.getBody()->getTerminator());\n+  }\n+\n+  LogicalResult initialize();\n+\n+  void emitPrologue();\n+\n+  scf::ForOp createNewForOp();\n+};\n+\n+Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrefetch,\n+                                   Attribute dotEncoding, OpBuilder &builder,\n+                                   llvm::Optional<int64_t> offsetK,\n+                                   llvm::Optional<int64_t> shapeK) {\n+  // opIdx: 0 => a, 1 => b\n+  auto type = v.getType().cast<RankedTensorType>();\n+  SmallVector<int64_t> shape{type.getShape().begin(), type.getShape().end()};\n+  SmallVector<int64_t> offset{0, 0};\n+  Type elementType = type.getElementType();\n+\n+  auto intAttr = [&](int64_t val) { return builder.getI64IntegerAttr(val); };\n+\n+  // k => (prefetchWidth, k - prefetchWidth)\n+  int64_t kIdx = opIdx == 0 ? 1 : 0;\n+\n+  offset[kIdx] = isPrefetch ? 0 : prefetchWidth;\n+  shape[kIdx] = isPrefetch ? prefetchWidth : (shape[kIdx] - prefetchWidth);\n+\n+  if (shapeK)\n+    shape[kIdx] = *shapeK;\n+  if (offsetK)\n+    offset[kIdx] = *offsetK;\n+\n+  Value newSmem = builder.create<tensor::ExtractSliceOp>(\n+      v.getLoc(),\n+      // TODO: encoding?\n+      RankedTensorType::get(shape, elementType, type.getEncoding()), v,\n+      SmallVector<OpFoldResult>{intAttr(offset[0]), intAttr(offset[1])},\n+      SmallVector<OpFoldResult>{intAttr(shape[0]), intAttr(shape[1])},\n+      SmallVector<OpFoldResult>{intAttr(1), intAttr(1)});\n+\n+  auto dotOperandEnc = triton::gpu::DotOperandEncodingAttr::get(\n+      builder.getContext(), opIdx, dotEncoding);\n+  Value prefetchSlice = builder.create<triton::gpu::ConvertLayoutOp>(\n+      v.getLoc(), RankedTensorType::get(shape, elementType, dotOperandEnc),\n+      newSmem);\n+\n+  return prefetchSlice;\n+}\n+\n+LogicalResult Prefetcher::initialize() {\n+  Block *loop = forOp.getBody();\n+\n+  SmallVector<triton::DotOp> dotsInFor;\n+  for (Operation &op : *loop)\n+    if (auto dotOp = dyn_cast<triton::DotOp>(op))\n+      dotsInFor.push_back(dotOp);\n+\n+  if (dotsInFor.empty())\n+    return failure();\n+\n+  // returns source of cvt\n+  auto getPrefetchSrc = [](Value v) -> Value {\n+    // TODO: Check if the layout of src is SharedEncodingAttr\n+    if (auto cvt = v.getDefiningOp<triton::gpu::ConvertLayoutOp>())\n+      return cvt.src();\n+    return Value();\n+  };\n+\n+  auto getIncomingOp = [this](Value v) -> Value {\n+    if (auto arg = v.dyn_cast<BlockArgument>())\n+      if (arg.getOwner()->getParentOp() == forOp.getOperation())\n+        return forOp.getOpOperandForRegionIterArg(arg).get();\n+    return Value();\n+  };\n+\n+  auto getYieldOp = [this](Value v) -> Value {\n+    auto arg = v.cast<BlockArgument>();\n+    unsigned yieldIdx = arg.getArgNumber() - forOp.getNumInductionVars();\n+    return yieldOp.getOperand(yieldIdx);\n+  };\n+\n+  for (triton::DotOp dot : dotsInFor) {\n+    Value aSmem = getPrefetchSrc(dot.a());\n+    Value bSmem = getPrefetchSrc(dot.b());\n+    if (aSmem && bSmem) {\n+      Value aHeaderDef = getIncomingOp(aSmem);\n+      Value bHeaderDef = getIncomingOp(bSmem);\n+      // Only prefetch loop arg\n+      if (aHeaderDef && bHeaderDef) {\n+        dots.insert(dot);\n+        dot2aHeaderDef[dot] = aHeaderDef;\n+        dot2bHeaderDef[dot] = bHeaderDef;\n+        dot2aLoopArg[dot] = aSmem;\n+        dot2bLoopArg[dot] = bSmem;\n+        dot2aYield[dot] = getYieldOp(aSmem);\n+        dot2bYield[dot] = getYieldOp(bSmem);\n+      }\n+    }\n+  }\n+\n+  return success();\n+}\n+\n+void Prefetcher::emitPrologue() {\n+  OpBuilder builder(forOp);\n+\n+  for (Value dot : dots) {\n+    Attribute dotEncoding =\n+        dot.getType().cast<RankedTensorType>().getEncoding();\n+    Value aPrefetched =\n+        generatePrefetch(dot2aHeaderDef[dot], 0, true, dotEncoding, builder);\n+    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().a()] = aPrefetched;\n+    Value bPrefetched =\n+        generatePrefetch(dot2bHeaderDef[dot], 1, true, dotEncoding, builder);\n+    operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().b()] = bPrefetched;\n+  }\n+}\n+\n+scf::ForOp Prefetcher::createNewForOp() {\n+  OpBuilder builder(forOp);\n+\n+  SmallVector<Value> loopArgs;\n+  for (auto v : forOp.getIterOperands())\n+    loopArgs.push_back(v);\n+  for (Value dot : dots) {\n+    loopArgs.push_back(\n+        operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().a()]);\n+    loopArgs.push_back(\n+        operand2headPrefetch[dot.getDefiningOp<triton::DotOp>().b()]);\n+  }\n+\n+  auto newForOp = builder.create<scf::ForOp>(\n+      forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n+      forOp.getStep(), loopArgs);\n+\n+  auto largestPow2 = [](int64_t n) -> int64_t {\n+    while ((n & (n - 1)) != 0)\n+      n = n & (n - 1);\n+    return n;\n+  };\n+\n+  builder.setInsertionPointToStart(newForOp.getBody());\n+  BlockAndValueMapping mapping;\n+  for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n+    mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+\n+  for (Operation &op : forOp.getBody()->without_terminator()) {\n+    Operation *newOp = nullptr;\n+    auto dot = dyn_cast<triton::DotOp>(&op);\n+    if (dots.contains(dot)) {\n+      Attribute dotEncoding =\n+          dot.getType().cast<RankedTensorType>().getEncoding();\n+      // prefetched dot\n+      Operation *firstDot = builder.clone(*dot, mapping);\n+      if (Value a = operand2headPrefetch.lookup(dot.a()))\n+        firstDot->setOperand(\n+            0, newForOp.getRegionIterArgForOpOperand(*a.use_begin()));\n+      if (Value b = operand2headPrefetch.lookup(dot.b()))\n+        firstDot->setOperand(\n+            1, newForOp.getRegionIterArgForOpOperand(*b.use_begin()));\n+\n+      // remaining part\n+      int64_t kOff = prefetchWidth;\n+      int64_t kRem = dot.a().getType().cast<RankedTensorType>().getShape()[1] -\n+                     prefetchWidth;\n+      Operation *prevDot = firstDot;\n+      while (kRem != 0) {\n+        int64_t kShape = largestPow2(kRem);\n+        Value aRem =\n+            generatePrefetch(mapping.lookup(dot2aLoopArg[dot]), 0, false,\n+                             dotEncoding, builder, kOff, kShape);\n+        Value bRem =\n+            generatePrefetch(mapping.lookup(dot2bLoopArg[dot]), 1, false,\n+                             dotEncoding, builder, kOff, kShape);\n+        newOp = builder.clone(*dot, mapping);\n+        newOp->setOperand(0, aRem);\n+        newOp->setOperand(1, bRem);\n+        newOp->setOperand(2, prevDot->getResult(0));\n+        prevDot = newOp;\n+        kOff += kShape;\n+        kRem -= kShape;\n+      }\n+    } else {\n+      newOp = builder.clone(op, mapping);\n+    }\n+    // update mapping of results\n+    for (unsigned dstIdx : llvm::seq(unsigned(0), op.getNumResults()))\n+      mapping.map(op.getResult(dstIdx), newOp->getResult(dstIdx));\n+  }\n+\n+  // prefetch next iteration\n+  SmallVector<Value> yieldValues;\n+  for (Value v : forOp.getBody()->getTerminator()->getOperands())\n+    yieldValues.push_back(mapping.lookup(v));\n+  for (Value dot : dots) {\n+    Attribute dotEncoding =\n+        dot.getType().cast<RankedTensorType>().getEncoding();\n+    yieldValues.push_back(generatePrefetch(mapping.lookup(dot2aYield[dot]), 0,\n+                                           true, dotEncoding, builder));\n+    yieldValues.push_back(generatePrefetch(mapping.lookup(dot2bYield[dot]), 1,\n+                                           true, dotEncoding, builder));\n+  }\n+  // Update ops of yield\n+  builder.create<scf::YieldOp>(yieldOp.getLoc(), yieldValues);\n+  return newForOp;\n+}\n+\n+struct PrefetchPass : public TritonGPUPrefetchBase<PrefetchPass> {\n+  void runOnOperation() override {\n+    getOperation()->walk([&](scf::ForOp forOp) {\n+      Prefetcher prefetcher(forOp);\n+\n+      if (prefetcher.initialize().failed())\n+        return;\n+\n+      prefetcher.emitPrologue();\n+\n+      scf::ForOp newForOp = prefetcher.createNewForOp();\n+\n+      // replace the original loop\n+      for (unsigned i = 0; i < forOp->getNumResults(); ++i)\n+        forOp->getResult(i).replaceAllUsesWith(newForOp->getResult(i));\n+      forOp->erase();\n+    });\n+  }\n+};\n+\n+} // anonymous namespace\n+\n+std::unique_ptr<Pass> mlir::createTritonGPUPrefetchPass() {\n+  return std::make_unique<PrefetchPass>();\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Swizzle.cpp", "status": "removed", "additions": 0, "deletions": 102, "changes": 102, "file_content_changes": "@@ -1,102 +0,0 @@\n-#include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n-\n-using namespace mlir;\n-using namespace mlir::triton;\n-\n-#define GEN_PASS_CLASSES\n-#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n-\n-namespace {\n-\n-struct SwizzlePass : public TritonGPUSwizzleBase<SwizzlePass> {\n-  SwizzlePass() = default;\n-\n-  struct SwizzleInfo {\n-    int vec;\n-    int perPhase;\n-    int maxPhase;\n-  };\n-\n-  SwizzleInfo getSwizzleMMA(int opIdx, triton::gpu::MmaEncodingAttr retEncoding,\n-                            RankedTensorType ty) {\n-    SwizzleInfo noSwizzling = {1, 1, 1};\n-    int version = retEncoding.getVersion();\n-    auto tyEncoding = ty.getEncoding().cast<triton::gpu::SharedEncodingAttr>();\n-    auto order = tyEncoding.getOrder();\n-    // number of rows per phase\n-    int perPhase = 128 / (ty.getShape()[order[0]] *\n-                          (ty.getElementType().getIntOrFloatBitWidth() / 8));\n-    perPhase = std::max<int>(perPhase, 1);\n-    // index of the inner dimension in `order`\n-    size_t inner = (opIdx == 0) ? 0 : 1;\n-    if (version == 1) {\n-      int maxPhase = (order[inner] == 1 ? 8 : 4) / perPhase;\n-      // TODO: handle rep (see\n-      // https://github.com/openai/triton/blob/master/lib/codegen/analysis/layout.cc#L209)\n-      int vec = 1;\n-      return SwizzleInfo{vec, perPhase, maxPhase};\n-    } else if (version == 2) {\n-      auto eltTy = ty.getElementType();\n-      std::vector<size_t> mat_shape = {8, 8,\n-                                       2 * 64 / eltTy.getIntOrFloatBitWidth()};\n-      // for now, disable swizzle when using transposed int8 tensor cores\n-      bool is_int8_mma = ty.getElementType().isInteger(8);\n-      if (is_int8_mma && order[0] == inner)\n-        return noSwizzling;\n-      // compute swizzling for A operand\n-      if (opIdx == 0) {\n-        int vec = order[0] == 1 ? mat_shape[2] : mat_shape[0]; // k : m\n-        int mmaStride = order[0] == 1 ? mat_shape[0] : mat_shape[2];\n-        int maxPhase = mmaStride / perPhase;\n-        return SwizzleInfo{vec, perPhase, maxPhase};\n-      }\n-      // compute swizzling for B operand\n-      else if (opIdx == 1) {\n-        int vec = order[0] == 1 ? mat_shape[1] : mat_shape[2]; // n : k\n-        int mmaStride = order[0] == 1 ? mat_shape[2] : mat_shape[1];\n-        int maxPhase = mmaStride / perPhase;\n-        return SwizzleInfo{vec, perPhase, maxPhase};\n-      } else {\n-        llvm_unreachable(\"invalid operand index\");\n-      }\n-    } else\n-      llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n-  }\n-\n-  void runOnOperation() override {\n-    Operation *op = getOperation();\n-    op->walk([&](triton::DotOp dotOp) -> void {\n-      OpBuilder builder(dotOp);\n-      auto _retEncoding =\n-          dotOp.getResult().getType().cast<RankedTensorType>().getEncoding();\n-      auto retEncoding = _retEncoding.dyn_cast<triton::gpu::MmaEncodingAttr>();\n-      if (!retEncoding)\n-        return;\n-      for (int opIdx : {0, 1}) {\n-        Value op = dotOp.getOperand(opIdx);\n-        auto ty = op.getType().template cast<RankedTensorType>();\n-        // compute new swizzled encoding\n-        SwizzleInfo swizzle = getSwizzleMMA(opIdx, retEncoding, ty);\n-        auto newEncoding = triton::gpu::SharedEncodingAttr::get(\n-            &getContext(), swizzle.vec, swizzle.perPhase, swizzle.maxPhase,\n-            ty.getEncoding()\n-                .cast<triton::gpu::SharedEncodingAttr>()\n-                .getOrder());\n-        // create conversion\n-        auto newType = RankedTensorType::get(ty.getShape(), ty.getElementType(),\n-                                             newEncoding);\n-        Operation *newOp = builder.create<triton::gpu::ConvertLayoutOp>(\n-            op.getLoc(), newType, op);\n-        // bind new op to dot operand\n-        dotOp->replaceUsesOfWith(op, newOp->getResult(0));\n-      }\n-    });\n-  }\n-};\n-} // anonymous namespace\n-\n-std::unique_ptr<Pass> mlir::createTritonGPUSwizzlePass() {\n-  return std::make_unique<SwizzlePass>();\n-}\n\\ No newline at end of file"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -95,8 +95,8 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n         dotOp.a().getType().cast<RankedTensorType>().getEncoding();\n     Attribute bEncoding =\n         dotOp.b().getType().cast<RankedTensorType>().getEncoding();\n-    if (aEncoding && aEncoding.isa<triton::gpu::SharedEncodingAttr>() &&\n-        bEncoding && bEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+    if (aEncoding && aEncoding.isa<triton::gpu::DotOperandEncodingAttr>() &&\n+        bEncoding && bEncoding.isa<triton::gpu::DotOperandEncodingAttr>())\n       return true;\n     return false;\n   });"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -140,7 +140,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n   pm.addPass(createConvertTritonGPUToLLVMPass());\n-  // Conanicalize to eliminate the remaining UnrealizedConversionCastOp\n+  // Canonicalize to eliminate the remaining UnrealizedConversionCastOp\n   pm.addPass(mlir::createCanonicalizerPass());\n   pm.addPass(mlir::createCSEPass()); // Simplify the IR to improve readability.\n   pm.addPass(mlir::createSymbolDCEPass());"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 12, "deletions": 11, "changes": 23, "file_content_changes": "@@ -493,10 +493,6 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self) -> mlir::Type {\n              return self.getType<mlir::triton::Float8Type>();\n            })\n-      .def(\"get_bf8_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::triton::BFloat8Type>();\n-           })\n       .def(\n           \"get_half_ty\",\n           [](mlir::OpBuilder &self) -> mlir::Type { return self.getF16Type(); })\n@@ -616,14 +612,20 @@ void init_triton_ir(py::module &&m) {\n            })\n \n       // Cast instructions\n+      // Conversions for custom FP types (FP8)\n+      .def(\"create_fp_to_fp\",\n+           [](mlir::OpBuilder &self, mlir::Value &src,\n+              mlir::Type &dstType) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::FpToFpOp>(loc, dstType, src);\n+           })\n+      // Conversions for standard LLVM builtin types\n       .def(\"create_bitcast\",\n            [](mlir::OpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::BitcastOp>(loc, dstType, src);\n            })\n-      // .def(\"create_cast\", &ir::builder::create_cast)\n-      // .def(\"create_ptr_to_int\", &ir::builder::create_ptr_to_int)\n       .def(\"create_si_to_fp\",\n            [](mlir::OpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n@@ -697,7 +699,6 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::arith::IndexCastOp>(loc, input,\n                                                           self.getI32Type());\n            })\n-\n       .def(\"create_fmul\",\n            [](mlir::OpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {\n@@ -1255,13 +1256,13 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::PassManager &self, int numStages) {\n              self.addPass(mlir::createTritonGPUPipelinePass(numStages));\n            })\n-      .def(\"add_triton_gpu_combine_pass\",\n+      .def(\"add_tritongpu_prefetch_pass\",\n            [](mlir::PassManager &self) {\n-             self.addPass(mlir::createTritonGPUCombineOpsPass());\n+             self.addPass(mlir::createTritonGPUPrefetchPass());\n            })\n-      .def(\"add_triton_gpu_swizzle_pass\",\n+      .def(\"add_triton_gpu_combine_pass\",\n            [](mlir::PassManager &self) {\n-             self.addPass(mlir::createTritonGPUSwizzlePass());\n+             self.addPass(mlir::createTritonGPUCombineOpsPass());\n            })\n       .def(\"add_triton_gpu_to_llvm\",\n            [](mlir::PassManager &self) {"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 80, "deletions": 80, "changes": 160, "file_content_changes": "@@ -780,88 +780,88 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     assert (to_numpy(src).view('uint8') == to_numpy(dst).view('uint8')).all()\n \n \n-# @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n-# def test_f8_xf16_roundtrip(dtype):\n-#     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n-#     check_type_supported(dtype)\n+@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n+def test_f8_xf16_roundtrip(dtype):\n+    \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n+    check_type_supported(dtype)\n \n-#     @triton.jit\n-#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n-#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-#         mask = offsets < n_elements\n-#         input = tl.load(input_ptr + offsets, mask=mask)\n-#         output = input\n-#         tl.store(output_ptr + offsets, output, mask=mask)\n-\n-#     f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n-#     f8 = triton.reinterpret(f8_tensor, tl.float8)\n-#     n_elements = f8_tensor.numel()\n-#     xf16 = torch.empty_like(f8_tensor, dtype=dtype)\n-#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-#     copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n-\n-#     f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n-#     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n-#     copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n-\n-#     assert torch.all(f8_tensor == f8_output_tensor)\n-\n-\n-# def test_f16_to_f8_rounding():\n-#     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n-#     error is the minimum over all float8.\n-#     Or the same explanation a bit mathier:\n-#     for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n-#     @triton.jit\n-#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n-#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-#         mask = offsets < n_elements\n-#         input = tl.load(input_ptr + offsets, mask=mask)\n-#         output = input\n-#         tl.store(output_ptr + offsets, output, mask=mask)\n-\n-#     # torch.view with a dtype isn't supported in triton's torch yet so use numpy's view\n-#     f16_input_np = (\n-#         np.array(\n-#             range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=np.int16,\n-#         )\n-#         .view(np.float16)\n-#     )\n-#     f16_input = torch.tensor(f16_input_np, dtype=torch.float16, device='cuda')\n-#     n_elements = f16_input.numel()\n-#     f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n-#     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n-#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-#     copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n-\n-#     f16_output = torch.empty_like(f16_input, dtype=torch.float16)\n-#     copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n-\n-#     abs_error = torch.abs(f16_input - f16_output)\n-\n-#     all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n-#     all_f8_vals = triton.reinterpret(all_f8_vals_tensor, tl.float8)\n-#     all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=torch.float16)\n-#     copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n-\n-#     all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n-#         torch.isfinite(all_f8_vals_in_f16)\n-#     ]\n+    @triton.jit\n+    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        input = tl.load(input_ptr + offsets, mask=mask)\n+        output = input\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+    f8 = triton.reinterpret(f8_tensor, tl.float8)\n+    n_elements = f8_tensor.numel()\n+    xf16 = torch.empty_like(f8_tensor, dtype=dtype)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n+\n+    f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n+    f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n+    copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n+\n+    assert torch.all(f8_tensor == f8_output_tensor)\n+\n+\n+def test_f16_to_f8_rounding():\n+    \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n+    error is the minimum over all float8.\n+    Or the same explanation a bit mathier:\n+    for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n+    @triton.jit\n+    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        input = tl.load(input_ptr + offsets, mask=mask)\n+        output = input\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+    # torch.view with a dtype isn't supported in triton's torch yet so use numpy's view\n+    f16_input_np = (\n+        np.array(\n+            range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=np.int16,\n+        )\n+        .view(np.float16)\n+    )\n+    f16_input = torch.tensor(f16_input_np, dtype=torch.float16, device='cuda')\n+    n_elements = f16_input.numel()\n+    f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n+    f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n+\n+    f16_output = torch.empty_like(f16_input, dtype=torch.float16)\n+    copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n+\n+    abs_error = torch.abs(f16_input - f16_output)\n+\n+    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n+    all_f8_vals = triton.reinterpret(all_f8_vals_tensor, tl.float8)\n+    all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=torch.float16)\n+    copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n+\n+    all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n+        torch.isfinite(all_f8_vals_in_f16)\n+    ]\n \n-#     min_error = torch.min(\n-#         torch.abs(\n-#             f16_input.reshape((-1, 1))\n-#             - all_finite_f8_vals_in_f16.reshape((1, -1))\n-#         ),\n-#         dim=1,\n-#     )[0]\n-#     # 1.9375 is float8 max\n-#     mismatch = torch.logical_and(\n-#         abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.abs(f16_input) < 1.9375)\n-#     )\n-#     assert torch.all(\n-#         torch.logical_not(mismatch)\n-#     ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n+    min_error = torch.min(\n+        torch.abs(\n+            f16_input.reshape((-1, 1))\n+            - all_finite_f8_vals_in_f16.reshape((1, -1))\n+        ),\n+        dim=1,\n+    )[0]\n+    # 1.9375 is float8 max\n+    mismatch = torch.logical_and(\n+        abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.abs(f16_input) < 1.9375)\n+    )\n+    assert torch.all(\n+        torch.logical_not(mismatch)\n+    ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n \n \n # # ---------------"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 62, "deletions": 60, "changes": 122, "file_content_changes": "@@ -171,63 +171,65 @@ def test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLO\n     assert_close(c, golden, rtol=max(1e-4, 1.5 * golden_rel_err), atol=max(1e-4, 1.5 * golden_abs_err), check_dtype=False)\n \n \n-@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n-    [32, 32, 16, 4, 32, 32, 16],\n-    [32, 16, 16, 4, 32, 32, 16],\n-    [128, 8, 8, 4, 32, 32, 16],\n-    [127, 41, 43, 4, 32, 32, 16],\n-])\n-def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n-    @triton.jit\n-    def matmul_kernel(\n-        a_ptr, b_ptr, c_ptr,\n-        M, N, K,\n-        stride_am, stride_ak,\n-        stride_bk, stride_bn,\n-        stride_cm, stride_cn,\n-        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n-    ):\n-        pid = tl.program_id(axis=0)\n-        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n-        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n-        pid_m = pid // num_pid_n\n-        pid_n = pid % num_pid_n\n-\n-        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-        offs_k = tl.arange(0, BLOCK_SIZE_K)\n-        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n-        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n-\n-        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-        for k in range(0, K, BLOCK_SIZE_K):\n-            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n-            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n-            a = tl.load(a_ptrs, a_mask)\n-            b = tl.load(b_ptrs, b_mask)\n-            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n-            accumulator += tl.dot(a, b, allow_tf32=False)\n-            a_ptrs += BLOCK_SIZE_K * stride_ak\n-            b_ptrs += BLOCK_SIZE_K * stride_bk\n-            offs_k += BLOCK_SIZE_K\n-\n-        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n-        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n-        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n-        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n-        tl.store(c_ptrs, accumulator, c_mask)\n-\n-    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n-    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n-    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n-\n-    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n-    matmul_kernel[grid](a, b, c,\n-                        M, N, K,\n-                        stride_am=a.stride(0), stride_ak=a.stride(1),\n-                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n-                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n-                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n-\n-    golden = torch.matmul(a, b)\n-    torch.testing.assert_close(c, golden)\n+# XXX(Keren): Temporarily disable this test until we have shared -> dot conversion implemented\n+#@pytest.mark.parametrize('M,N,K,num_warps,block_M,block_N,block_K', [\n+#    [32, 32, 16, 4, 32, 32, 16],\n+#    [32, 16, 16, 4, 32, 32, 16],\n+#    [128, 8, 8, 4, 32, 32, 16],\n+#    [127, 41, 43, 4, 32, 32, 16],\n+#])\n+#def test_gemm_fmadot(M, N, K, num_warps, block_M, block_N, block_K):\n+#    @triton.jit\n+#    def matmul_kernel(\n+#        a_ptr, b_ptr, c_ptr,\n+#        M, N, K,\n+#        stride_am, stride_ak,\n+#        stride_bk, stride_bn,\n+#        stride_cm, stride_cn,\n+#        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n+#    ):\n+#        pid = tl.program_id(axis=0)\n+#        # num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n+#        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n+#        pid_m = pid // num_pid_n\n+#        pid_n = pid % num_pid_n\n+#\n+#        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+#        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+#        offs_k = tl.arange(0, BLOCK_SIZE_K)\n+#        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n+#        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n+#\n+#        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+#        for k in range(0, K, BLOCK_SIZE_K):\n+#            a_mask = (offs_am[:, None] < M) & (offs_k[None, :] < K)\n+#            b_mask = (offs_k[:, None] < K) & (offs_bn[None, :] < N)\n+#            a = tl.load(a_ptrs, a_mask)\n+#            b = tl.load(b_ptrs, b_mask)\n+#            # NOTE the allow_tf32 should be false to force the dot op to do fmadot lowering\n+#            accumulator += tl.dot(a, b, allow_tf32=False)\n+#            a_ptrs += BLOCK_SIZE_K * stride_ak\n+#            b_ptrs += BLOCK_SIZE_K * stride_bk\n+#            offs_k += BLOCK_SIZE_K\n+#\n+#        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+#        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n+#        c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn\n+#        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n+#        tl.store(c_ptrs, accumulator, c_mask)\n+#\n+#    a = torch.randn((M, K), device='cuda', dtype=torch.float32)\n+#    b = torch.randn((K, N), device='cuda', dtype=torch.float32)\n+#    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n+#\n+#    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),)\n+#    matmul_kernel[grid](a, b, c,\n+#                        M, N, K,\n+#                        stride_am=a.stride(0), stride_ak=a.stride(1),\n+#                        stride_bk=b.stride(0), stride_bn=b.stride(1),\n+#                        stride_cm=c.stride(0), stride_cn=c.stride(1),\n+#                        BLOCK_SIZE_M=block_M, BLOCK_SIZE_N=block_N, BLOCK_SIZE_K=block_K)\n+#\n+#    golden = torch.matmul(a, b)\n+#    torch.testing.assert_close(c, golden)\n+#"}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -117,8 +117,7 @@ def test_reduce2d(op, dtype, shape, axis):\n     z = torch.empty(reduced_shape, device=x.device, dtype=reduced_dtype)\n \n     kernel = patch_kernel(reduce2d_kernel, {'OP': op})\n-    grid = (1,)\n-    kernel[grid](x_ptr=x, z_ptr=z, axis=axis, block_m=shape[0], block_n=shape[1])\n+    kernel[(1,)](x_ptr=x, z_ptr=z, axis=axis, block_m=shape[0], block_n=shape[1])\n \n     if op == 'sum':\n         golden_z = torch.sum(x, dim=axis, keepdim=False, dtype=reduced_dtype)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -876,13 +876,15 @@ def ttir_to_ttgir(mod, num_warps, num_stages):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n     pm.enable_debug()\n+    # Convert blocked layout to mma layout for dot ops so that pipeline\n+    # can get shared memory swizzled correctly.\n+    pm.add_triton_gpu_combine_pass()\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n     pm.add_coalesce_pass()\n     pm.add_triton_gpu_combine_pass()\n     pm.add_licm_pass()\n-    pm.add_triton_gpu_swizzle_pass()\n     pm.add_triton_gpu_combine_pass()\n     pm.add_cse_pass()\n     pm.run(mod)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -48,6 +48,8 @@ class dtype:\n     SINT_TYPES = ['int1', 'int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['uint8', 'uint16', 'uint32', 'uint64']\n     FP_TYPES = ['fp8', 'fp16', 'bf16', 'fp32', 'fp64']\n+    CUSTOMIZED_FP_TYPES = ['fp8']\n+    STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n     class SIGNEDNESS(Enum):\n@@ -129,6 +131,12 @@ def is_uint64(self):\n     def is_floating(self):\n         return self.name in dtype.FP_TYPES\n \n+    def is_customized_floating(self):\n+        return self.name in dtype.CUSTOMIZED_FP_TYPES\n+\n+    def is_standard_floating(self):\n+        return self.name in dtype.STANDARD_FP_TYPES\n+\n     def is_int_signed(self):\n         return self.name in dtype.SINT_TYPES\n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 30, "deletions": 29, "changes": 59, "file_content_changes": "@@ -613,39 +613,45 @@ def cast(input: tl.tensor,\n         dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n     if src_ty == dst_ty:\n         return input\n+\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n-    # fp8 <=> bf16/fp16\n-    if (src_sca_ty.is_bf16() or src_sca_ty.is_fp16()) and dst_sca_ty.is_fp8():\n-        return tl.tensor(builder.create_fp_trunc(input.handle, dst_ty.to_ir(builder)),\n-                         dst_ty)\n-    if src_sca_ty.is_fp8() and (dst_sca_ty.is_bf16() or dst_sca_ty.is_fp16()):\n-        return tl.tensor(builder.create_fp_ext(input.handle, dst_ty.to_ir(builder)),\n+\n+    # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n+    if (src_sca_ty.is_customized_floating() and dst_sca_ty.is_floating()) or \\\n+       (src_sca_ty.is_floating() and dst_sca_ty.is_customized_floating()):\n+        return tl.tensor(builder.create_fp_to_fp(input.handle, dst_ty.to_ir(builder)),\n                          dst_ty)\n-    # bf16 <=> (not fp32)\n-    if (src_sca_ty.is_bf16() and not dst_sca_ty.is_fp32()) or \\\n-       (dst_sca_ty.is_bf16() and not src_sca_ty.is_fp32()):\n+\n+    # Casting types of the same bit width: fp16 <=> bf16\n+    if (src_sca_ty.is_fp16() and dst_sca_ty.is_bf16()) or \\\n+       (src_sca_ty.is_bf16() and dst_sca_ty.is_fp16()):\n         return cast(cast(input, tl.float32, builder), dst_sca_ty, builder)\n \n-    # FP Truncation\n+    # Standard floating types' casting: truncation\n+    #   fp64 => fp32, fp16, bf16\n+    #   fp32 => fp16, bf16\n     truncate_fp = src_sca_ty.is_floating() and \\\n         dst_sca_ty.is_floating() and \\\n-        src_sca_ty.fp_mantissa_width > dst_sca_ty.fp_mantissa_width\n+        src_sca_ty.primitive_bitwidth > dst_sca_ty.primitive_bitwidth\n     if truncate_fp:\n         return tl.tensor(builder.create_fp_trunc(input.handle,\n                                                  dst_ty.to_ir(builder)),\n                          dst_ty)\n \n-    # FP Extension\n+    # Standard floating types' casting: extension\n+    #   fp32 => fp64\n+    #   fp16 => fp32, fp64\n+    #   bf16 => fp32, fp64\n     ext_fp = src_sca_ty.is_floating() and \\\n         dst_sca_ty.is_floating() and \\\n-        src_sca_ty.fp_mantissa_width < dst_sca_ty.fp_mantissa_width\n+        src_sca_ty.primitive_bitwidth < dst_sca_ty.primitive_bitwidth\n     if ext_fp:\n         return tl.tensor(builder.create_fp_ext(input.handle,\n                                                dst_ty.to_ir(builder)),\n                          dst_ty)\n \n-    # Int cast\n+    # Casting between integer types\n     if src_sca_ty.is_int() and dst_sca_ty.is_int() and \\\n        (src_sca_ty.int_bitwidth != dst_sca_ty.int_bitwidth or src_sca_ty.int_signedness != dst_sca_ty.int_signedness):\n         sign_extend = src_sca_ty.is_int_signed() and not src_sca_ty.is_bool()\n@@ -658,8 +664,8 @@ def cast(input: tl.tensor,\n                                                      dst_ty.to_ir(builder), sign_extend),\n                              dst_ty)\n \n-    # Float to Int\n-    if src_sca_ty.is_floating() and dst_sca_ty.is_int():\n+    # Casting standard floating types to integer types\n+    if src_sca_ty.is_standard_floating() and dst_sca_ty.is_int():\n         if dst_sca_ty.is_bool():\n             ty = input.dtype.to_ir(builder)\n             _0 = tl.tensor(builder.get_null_value(ty), input.dtype)\n@@ -673,8 +679,8 @@ def cast(input: tl.tensor,\n                                                      dst_ty.to_ir(builder)),\n                              dst_ty)\n \n-    # int => float\n-    if src_sca_ty.is_int() and dst_sca_ty.is_floating():\n+    # Casting integer types to standard floating types\n+    if src_sca_ty.is_int() and dst_sca_ty.is_standard_floating():\n         if src_sca_ty.is_bool() or not src_sca_ty.is_int_signed():\n             return tl.tensor(builder.create_ui_to_fp(input.handle,\n                                                      dst_ty.to_ir(builder)),\n@@ -684,7 +690,7 @@ def cast(input: tl.tensor,\n                                                      dst_ty.to_ir(builder)),\n                              dst_ty)\n \n-    # ptr => int\n+    # Casting pointer types to integer types\n     if src_sca_ty.is_ptr() and dst_sca_ty.is_int():\n         bitwidth = dst_sca_ty.int_bitwidth\n         if bitwidth == 64:\n@@ -695,19 +701,14 @@ def cast(input: tl.tensor,\n                              tl.tensor(builder.get_int64(0), tl.int64),\n                              builder)\n \n-    if not src_sca_ty.is_ptr() and dst_sca_ty.is_ptr():\n+    # Casting integer types to pointer types\n+    if src_sca_ty.is_int() and dst_sca_ty.is_ptr():\n         return tl.tensor(builder.create_int_to_ptr(input.handle, dst_ty.to_ir(builder)), dst_ty)\n-    # Ptr . Ptr\n+\n+    # Casting pointer types to pointer types\n     if src_sca_ty.is_ptr() and dst_sca_ty.is_ptr():\n         return tl.tensor(builder.create_bitcast(input.handle, dst_ty.to_ir(builder)), dst_ty)\n-    # * . Bool\n-    if dst_sca_ty.is_bool():\n-        if src_sca_ty.is_ptr():\n-            input = cast(input, tl.int64, builder)\n-        other = builder.get_int64(0)\n-        if src_ty.is_bool():\n-            other = builder.create_splat(other, src_ty.get_block_shapes())\n-        return tl.tensor(builder.create_icmpNE(input.handle, other), dst_ty)\n+\n     assert False, f'cannot cast {input} to {dst_ty}'\n \n # ===----------------------------------------------------------------------===//"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -176,6 +176,9 @@ def _type_of(key):\n                 triton.language.uint32: 'u32',\n                 triton.language.uint64: 'u64',\n                 triton.language.float8: 'fp8',\n+                triton.language.float16: 'fp16',\n+                triton.language.bfloat16: 'bf16',\n+                triton.language.float32: 'fp32',\n             }[key]\n             return f'*{ty}'\n         if key is None:"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 50, "deletions": 49, "changes": 99, "file_content_changes": "@@ -2,11 +2,14 @@\n \n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#A = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#B = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n+#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n // CHECK-LABEL: matmul_loop\n+// There shouldn't be any aliasing with the dot op encoding.\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n@@ -19,12 +22,10 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-    // CHECK: %4 -> %4\n-    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n-    // CHECK-NEXT: %6 -> %6 \n-    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n-    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n+    %c = tt.dot %a, %b, %prev_c {transA = false, transB = false, allowTF32 = true} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n     %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n@@ -36,18 +37,18 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n // CHECK-LABEL: alloc\n func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n   // CHECK: %0 -> %0\n-  %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A>\n+  %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: convert\n func @convert(%A : !tt.ptr<f16>) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: %0 -> %0\n-  %cst1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A>\n+  %cst1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n@@ -57,134 +58,134 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: %cst_0 -> %cst_0\n-  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   // CHECK: %2 -> %cst_0\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: extract_slice\n func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n   // CHECK-NEXT: %0 -> %cst\n-  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A> to tensor<16x16xf16, #A>\n+  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: if_cat\n func @if_cat(%i1 : i1) {\n   // CHECK: %cst -> %cst\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: %cst_0 -> %cst_0\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: %0 -> %1,%1\n-  %cst2 = scf.if %i1 -> tensor<32x16xf16, #A> {\n+  %cst2 = scf.if %i1 -> tensor<32x16xf16, #A_SHARED> {\n     // CHECK: %1 -> %1\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n-    scf.yield %a : tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %a : tensor<32x16xf16, #A_SHARED>\n   } else {\n     // CHECK: %1 -> %1\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n-    scf.yield %b : tensor<32x16xf16, #A>\n+    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %b : tensor<32x16xf16, #A_SHARED>\n   }\n   return\n }\n \n // CHECK-LABEL: if_alias\n func @if_alias(%i1 : i1) {\n   // CHECK: %cst -> %cst\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: %0 -> %cst,%cst_0\n-  %cst2 = scf.if %i1 -> tensor<16x16xf16, #A> {\n-    scf.yield %cst0 : tensor<16x16xf16, #A>\n+  %cst2 = scf.if %i1 -> tensor<16x16xf16, #A_SHARED> {\n+    scf.yield %cst0 : tensor<16x16xf16, #A_SHARED>\n   } else {\n-    scf.yield %cst1 : tensor<16x16xf16, #A>\n+    scf.yield %cst1 : tensor<16x16xf16, #A_SHARED>\n   }\n   return\n }\n \n // CHECK-LABEL: for\n func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_1 -> %cst_1\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %arg6 -> %cst\n   // CHECK-NEXT: %arg7 -> %cst_0\n   // CHECK-NEXT: %arg8 -> %cst_1\n   // CHECK-NEXT: %0#0 -> %cst,%cst_0\n   // CHECK-NEXT: %0#1 -> %cst,%cst_0\n   // CHECK-NEXT: %0#2 -> %cst,%cst_0\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n-    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n }\n \n // CHECK-LABEL: for_if\n func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_1 -> %cst_1\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %arg7 -> %cst\n   // CHECK-NEXT: %arg8 -> %cst_0\n   // CHECK-NEXT: %arg9 -> %cst_1\n   // CHECK-NEXT: %0#0 -> %cst,%cst_0\n   // CHECK-NEXT: %0#1 -> %cst,%cst_0\n   // CHECK-NEXT: %0#2 -> %cst,%cst_0\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.if %i1 {\n       %index = arith.constant 8 : index\n       // CHECK-NEXT: %1 -> %cst,%cst_0\n-      %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A> to tensor<32xf16, #A>\n+      %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n       scf.yield\n     }\n-    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n }\n \n // CHECK-LABEL: for_if_for\n func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_1 -> %cst_1\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %arg7 -> %cst\n   // CHECK-NEXT: %arg8 -> %cst_0\n   // CHECK-NEXT: %arg9 -> %cst_1\n   // CHECK-NEXT: %0#0 -> %cst\n   // CHECK-NEXT: %0#1 -> %cst_0\n   // CHECK-NEXT: %0#2 -> %cst_2,%cst_2\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     // CHECK-NEXT: %arg11 -> %cst_1,%cst_2,%cst_2\n     // CHECK-NEXT: %1 -> %cst_2,%cst_2\n-    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A>) {\n+    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A_SHARED>) {\n       // CHECK-NEXT: %2 -> %cst_2,%cst_2\n-      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A> {\n+      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A_SHARED> {\n         // CHECK-NEXT: %cst_2 -> %cst_2\n-        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-        scf.yield %cst0 : tensor<128x32xf16, #A>\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n       } else {\n         // CHECK-NEXT: %cst_2 -> %cst_2\n-        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-        scf.yield %cst0 : tensor<128x32xf16, #A>\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n       }\n-      scf.yield %c_shared_next_next : tensor<128x32xf16, #A>\n+      scf.yield %c_shared_next_next : tensor<128x32xf16, #A_SHARED>\n     }\n-    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n }"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 95, "deletions": 93, "changes": 188, "file_content_changes": "@@ -3,9 +3,11 @@\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#A = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#B = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n+#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n // CHECK-LABEL: matmul_loop\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n@@ -23,20 +25,20 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-    // CHECK: offset = 0, size = 8192\n-    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+    // CHECK: offset = 0, size = 4608\n+    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n-    // CHECK-NEXT: offset = 8192, size = 8192\n-    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n+    // CHECK-NEXT: offset = 0, size = 4224\n+    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n \n-    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n     %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return\n-  // CHECK-NEXT: size = 16384\n+  // CHECK-NEXT: size = 4608\n }\n \n // Shared memory is available after a tensor's liveness range ends\n@@ -51,21 +53,21 @@ func @reusable(%A : !tt.ptr<f16>) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #AL>\n   %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  // CHECK-NEXT: offset = 0, size = 8192\n-  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+  // CHECK-NEXT: offset = 0, size = 4608\n+  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n   %a2_ = tt.load %b_ptr, %cst3, %cst4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #AL>\n-  // CHECK-NEXT: offset = 8192, size = 8192\n-  %a2 = triton_gpu.convert_layout %a2_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #A>\n+  // CHECK-NEXT: offset = 0, size = 1152\n+  %a2 = triton_gpu.convert_layout %a2_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #B_DOT>\n   %a3_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  // CHECK-NEXT: offset = 16384, size = 8192\n-  %a3 = triton_gpu.convert_layout %a3_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n-  %c = tt.dot %a1, %a2, %c_init {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+  // CHECK-NEXT: offset = 0, size = 4608\n+  %a3 = triton_gpu.convert_layout %a3_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n+  %c = tt.dot %a1, %a2, %c_init {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n   %a4_ = tt.load %b_ptr, %cst3, %cst4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #AL>\n-  // CHECK-NEXT: offset = 0, size = 8192\n-  %a4 = triton_gpu.convert_layout %a4_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #A>\n-  %c1 = tt.dot %a3, %a4, %c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+  // CHECK-NEXT: offset = 0, size = 1152\n+  %a4 = triton_gpu.convert_layout %a4_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #B_DOT>\n+  %c1 = tt.dot %a3, %a4, %c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n   return\n-  // CHECK-NEXT: size = 24576\n+  // CHECK-NEXT: size = 4608\n }\n \n // A tensor's shared memory offset is larger than it needs to accommodate further tensors\n@@ -75,33 +77,33 @@ func @reusable(%A : !tt.ptr<f16>) {\n // CHECK-LABEL: preallocate\n func @preallocate(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 512\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 512\n-  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 2048, size = 1024\n-  %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 3072, size = 1024\n-  %b = tt.cat %cst0, %cst2 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %b = tt.cat %cst0, %cst2 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 0, size = 1024\n-  %c = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %c = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 1024\n-  %cst4 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A>\n+  %cst4 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 6144, size = 2048\n-  %e = tt.cat %a, %cst4 {axis = 0} : (tensor<32x16xf16, #A>, tensor<32x16xf16, #A>) -> tensor<64x16xf16, #A>\n+  %e = tt.cat %a, %cst4 {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 2048\n-  %d = tt.cat %b, %cst4 {axis = 0} : (tensor<32x16xf16, #A>, tensor<32x16xf16, #A>) -> tensor<64x16xf16, #A>\n+  %d = tt.cat %b, %cst4 {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 10240, size = 2048\n-  %f = tt.cat %c, %cst4 {axis = 0} : (tensor<32x16xf16, #A>, tensor<32x16xf16, #A>) -> tensor<64x16xf16, #A>\n+  %f = tt.cat %c, %cst4 {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 0, size = 2048\n-  %cst5 = arith.constant dense<0.000000e+00> : tensor<64x16xf16, #A>\n+  %cst5 = arith.constant dense<0.000000e+00> : tensor<64x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 2048, size = 4096\n-  %g = tt.cat %e, %cst5 {axis = 0} : (tensor<64x16xf16, #A>, tensor<64x16xf16, #A>) -> tensor<128x16xf16, #A>\n+  %g = tt.cat %e, %cst5 {axis = 0} : (tensor<64x16xf16, #A_SHARED>, tensor<64x16xf16, #A_SHARED>) -> tensor<128x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 2048, size = 4096\n-  %h = tt.cat %d, %cst5 {axis = 0} : (tensor<64x16xf16, #A>, tensor<64x16xf16, #A>) -> tensor<128x16xf16, #A>\n+  %h = tt.cat %d, %cst5 {axis = 0} : (tensor<64x16xf16, #A_SHARED>, tensor<64x16xf16, #A_SHARED>) -> tensor<128x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 2048, size = 4096\n-  %i = tt.cat %f, %cst5 {axis = 0} : (tensor<64x16xf16, #A>, tensor<64x16xf16, #A>) -> tensor<128x16xf16, #A>\n+  %i = tt.cat %f, %cst5 {axis = 0} : (tensor<64x16xf16, #A_SHARED>, tensor<64x16xf16, #A_SHARED>) -> tensor<128x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 12288\n }\n@@ -110,13 +112,13 @@ func @preallocate(%A : !tt.ptr<f16>) {\n // CHECK-LABEL: unused\n func @unused(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 1024\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 0, size = 512\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 1024\n-  %a = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n   // CHECK: size = 2048\n }\n@@ -125,38 +127,38 @@ func @unused(%A : !tt.ptr<f16>) {\n // CHECK-LABEL: longlive\n func @longlive(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 512\n-  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 1024\n-  %a = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 512\n-  %cst4 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst4 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 1024\n-  %b = tt.cat %cst3, %cst4 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %b = tt.cat %cst3, %cst4 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 512\n-  %cst5 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst5 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 512\n-  %cst6 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst6 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 1024\n-  %c = tt.cat %cst3, %cst4 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %c = tt.cat %cst3, %cst4 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 1024\n-  %d = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %d = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 2560\n }\n \n // CHECK-LABEL: alloc\n func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n   // CHECK-NEXT: offset = 0, size = 512\n-  %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A>\n+  %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 512\n }\n@@ -176,19 +178,19 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: offset = 0, size = 512\n-  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 512\n }\n \n // CHECK-LABEL: extract_slice\n func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n-  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1,1,1] : tensor<1x16x16xf16, #A> to tensor<16x16xf16, #A>\n+  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1,1,1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 512\n }\n@@ -198,21 +200,21 @@ func @extract_slice(%A : !tt.ptr<f16>) {\n // CHECK-LABEL: if\n func @if(%i1 : i1) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     // CHECK-NEXT: offset = 1024, size = 1024\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     // CHECK-NEXT: offset = 1024, size = 1024\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: offset = 0, size = 512\n-  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 1024\n-  %a = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 2048\n }\n@@ -222,24 +224,24 @@ func @if(%i1 : i1) {\n // CHECK-LABEL: if_else\n func @if_else(%i1 : i1) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     // CHECK-NEXT: offset = 1024, size = 1024\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     // CHECK-NEXT: offset = 1024, size = 1024\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   } else {\n     // CHECK-NEXT: offset = 1024, size = 512\n-    %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+    %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n     // CHECK-NEXT: offset = 1536, size = 512\n-    %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+    %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n     // CHECK-NEXT: offset = 2048, size = 1024\n-    %a = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: offset = 1024, size = 1024\n-  %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 3072\n }\n@@ -249,13 +251,13 @@ func @if_else(%i1 : i1) {\n // CHECK-LABEL: for\n func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 8192\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 16384, size = 8192\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n-    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n   // CHECK-NEXT: size = 24576\n@@ -264,18 +266,18 @@ func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.p\n // CHECK-LABEL: for_if_slice\n func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 16384, size = 8192\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.if %i1 {\n       %index = arith.constant 8 : index\n-      %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A> to tensor<32xf16, #A>\n+      %cst0 = tensor.extract_slice %a_shared[%index, 0][1, 32][1, 1] : tensor<128x32xf16, #A_SHARED> to tensor<32xf16, #A_SHARED>\n       scf.yield\n     }\n-    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n   // CHECK-NEXT: size = 24576\n@@ -286,28 +288,28 @@ func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %\n // CHECK-LABEL: for_if_for\n func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 16384, size = 8192\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n-    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A>) {\n-      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A> {\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A_SHARED>) {\n+      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A_SHARED> {\n         // CHECK-NEXT: offset = 24576, size = 8192\n-        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-        scf.yield %cst0 : tensor<128x32xf16, #A>\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n       } else {\n         // CHECK-NEXT: offset = 32768, size = 8192\n-        %cst1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-        scf.yield %cst1 : tensor<128x32xf16, #A>\n+        %cst1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst1 : tensor<128x32xf16, #A_SHARED>\n       }\n-      scf.yield %c_shared_next_next : tensor<128x32xf16, #A>\n+      scf.yield %c_shared_next_next : tensor<128x32xf16, #A_SHARED>\n     }\n-    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: offset = 0, size = 8192\n-  %cst2 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %cst2 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 40960\n }"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 75, "deletions": 73, "changes": 148, "file_content_changes": "@@ -3,11 +3,14 @@\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#A = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#B = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n+#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n // CHECK-LABEL: matmul_loop\n+// There shouldn't be any membar with the dot op encoding.\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n@@ -23,11 +26,10 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n-    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n-    // CHECK: Membar 13\n-    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n+    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n     %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n@@ -42,9 +44,9 @@ func @raw_single_block(%A : !tt.ptr<f16>) {\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n   // CHECK: Membar 5\n-  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A>) -> tensor<128x32xf16, #A>\n+  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #A_SHARED>\n   return\n }\n \n@@ -54,56 +56,56 @@ func @war_single_block(%A : !tt.ptr<f16>) {\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n   // CHECK: Membar 5\n-  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A>) -> tensor<128x32xf16, #AL>\n+  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL>\n   // a2's liveness range ends here, and a3 and a2 have the same address range.\n   // So it makes sense to have a WAR dependency between a2 and a3.\n   // CHECK-NEXT: Membar 7\n-  %a3 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+  %a3 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: scratch\n func @scratch() {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: Membar 1\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: Membar 3\n-  %aa = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A>) -> tensor<32x16xf16, #AL>\n+  %aa = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   %b = tt.reduce %aa {redOp = 1 : i32, axis = 0 : i32} : tensor<32x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n   return\n }\n \n // CHECK-LABEL: async_wait\n func @async_wait() {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: Membar 1\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   triton_gpu.async_wait {num = 4 : i32}\n   // CHECK-NEXT: Membar 4\n-  %a_ = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A>) -> tensor<32x16xf16, #AL>\n+  %a_ = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: alloc\n func @alloc() {\n-  %cst0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A>\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %cst0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n+  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK: Membar 2\n-  %b = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A>) -> tensor<32x16xf16, #AL>\n+  %b = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: extract_slice\n func @extract_slice() {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : index\n-  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A> to tensor<16x16xf16, #A>\n+  %cst1 = tensor.extract_slice %cst0[%index, 0, 0][1, 16, 16][1, 1, 1] : tensor<1x16x16xf16, #A_SHARED> to tensor<16x16xf16, #A_SHARED>\n   // CHECK: Membar 3\n-  %cst2 = triton_gpu.convert_layout %cst1 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  %cst2 = triton_gpu.convert_layout %cst1 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   // CHECK-NEXT: Membar 5\n-  %cst3 = triton_gpu.convert_layout %cst2 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A>\n+  %cst3 = triton_gpu.convert_layout %cst2 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n@@ -112,119 +114,119 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n-  %tensor = triton_gpu.alloc_tensor : tensor<1x16x16xf16, #A>\n+  %tensor = triton_gpu.alloc_tensor : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n-  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A>, tensor<1x16x16xf16, #A>) -> tensor<2x16x16xf16, #A>\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n+  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n   // CHECK: Membar 7\n-  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A>, tensor<2x16x16xf16, #A>) -> tensor<4x16x16xf16, #A>\n+  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n   return\n }\n \n // If branch inserted a barrier for %cst0 and %cst1, but else didn't, then the barrier should be inserted in the parent region\n // CHECK-LABEL: multi_blocks\n func @multi_blocks(%i1 : i1) {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   } else {\n-    %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-    %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+    %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+    %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n     // CHECK-NEXT: Membar 7\n-    %b = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %b = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n   // CHECK-NEXT: Membar 10\n-  %c = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %c = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n }\n \n // Both branches inserted a barrier for %cst0 and %cst1, then the barrier doesn't need to be inserted in the parent region\n // CHECK-LABEL: multi_blocks_join_barrier\n func @multi_blocks_join_barrier(%i1 : i1) {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   } else {\n     // CHECK-NEXT: Membar 5\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   return\n }\n \n // Read yielded tensor requires a barrier\n // CHECK-LABEL: multi_blocks_yield\n func @multi_blocks_yield(%i1 : i1) {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %a = scf.if %i1 -> (tensor<32x16xf16, #A>) {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n     // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n-    scf.yield %a : tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %a : tensor<32x16xf16, #A_SHARED>\n   } else {\n     // CHECK-NEXT: Membar 5\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n-    scf.yield %b : tensor<32x16xf16, #A>\n+    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %b : tensor<32x16xf16, #A_SHARED>\n   }\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   // CHECK-NEXT: Membar 9\n-  %b = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A>, tensor<32x16xf16, #A>) -> tensor<64x16xf16, #A>\n+  %b = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n   return\n }\n \n // Conservatively add a barrier as if the branch (%i1) is never taken\n // CHECK-LABEL: multi_blocks_noelse\n func @multi_blocks_noelse(%i1 : i1) {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   return\n }\n \n // Conservatively add a barrier as if the branch (%i2) is never taken\n // CHECK-LABEL: multi_blocks_nested_scf\n func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     scf.if %i2 {\n       // CHECK: Membar 2\n-      %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+      %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n       scf.yield\n     }\n     scf.yield\n   } else {\n     // CHECK-NEXT: Membar 6\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n   // CHECK-NEXT: Membar 9\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: for\n func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     // CHECK-NEXT: Membar 3\n-    %cst0 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) -> tensor<256x32xf16, #A>\n-    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    %cst0 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n }\n@@ -233,18 +235,18 @@ func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.p\n // they are reassociated with aliases (c_shared) and thus require a barrier.\n // CHECK-LABEL: for_alias\n func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: Membar 2\n-  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) -> tensor<256x32xf16, #A>\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n-    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) -> tensor<256x32xf16, #A>\n+  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n     // CHECK-NEXT: Membar 6\n-    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) -> tensor<256x32xf16, #A>\n-    scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: Membar 9\n-  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A>, tensor<256x32xf16, #A>) -> tensor<512x32xf16, #A>\n+  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -6,8 +6,8 @@ func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n   %0 = tt.int_to_ptr %scalar_i64 : i64 -> !tt.ptr<f32>\n   // CHECK: !tt.ptr<f32> -> i64\n   %1 = tt.ptr_to_int %scalar_ptr : !tt.ptr<f32> -> i64\n-  // CHECK: f32 -> f16\n-  %2 = tt.fp_to_fp %scalar_f32 : f32 -> f16\n+  // CHECK: f32 to f16\n+  %2 = arith.truncf %scalar_f32 : f32 to f16\n \n   // 0D tensor -> 0D tensor\n   %tensor_ptr_0d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<!tt.ptr<f32>>\n@@ -18,8 +18,8 @@ func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n   %3 = tt.int_to_ptr %tensor_i64_0d : tensor<i64> -> tensor<!tt.ptr<f32>>\n   // CHECK: tensor<!tt.ptr<f32>> -> tensor<i64>\n   %4 = tt.ptr_to_int %tensor_ptr_0d : tensor<!tt.ptr<f32>> -> tensor<i64>\n-  // CHECK: tensor<f32> -> tensor<f16>\n-  %5 = tt.fp_to_fp %tensor_f32_0d : tensor<f32> -> tensor<f16>\n+  // CHECK: tensor<f32> to tensor<f16>\n+  %5 = arith.truncf %tensor_f32_0d : tensor<f32> to tensor<f16>\n \n   // 1D tensor -> 1D tensor\n   %tensor_ptr_1d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>>\n@@ -30,8 +30,8 @@ func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n   %6 = tt.int_to_ptr %tensor_i64_1d : tensor<16xi64> -> tensor<16x!tt.ptr<f32>>\n   // CHECK: tensor<16x!tt.ptr<f32>> -> tensor<16xi64>\n   %7 = tt.ptr_to_int %tensor_ptr_1d : tensor<16x!tt.ptr<f32>> -> tensor<16xi64>\n-  // CHECK: tensor<16xf32> -> tensor<16xf16>\n-  %8 = tt.fp_to_fp %tensor_f32_1d : tensor<16xf32> -> tensor<16xf16>\n+  // CHECK: tensor<16xf32> to tensor<16xf16>\n+  %8 = arith.truncf %tensor_f32_1d : tensor<16xf32> to tensor<16xf16>\n   return\n }\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 19, "deletions": 2, "changes": 21, "file_content_changes": "@@ -669,22 +669,26 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n #mma0 = #triton_gpu.mma<{version=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_dot\n   func @convert_dot(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) {\n     %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #shared0>\n     %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #shared0>\n-    %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma0>\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4\n+    %AA_DOT = triton_gpu.convert_layout %AA : (tensor<16x16xf16, #shared0>) -> tensor<16x16xf16, #dot_operand_a>\n+    %BB_DOT = triton_gpu.convert_layout %BB : (tensor<16x16xf16, #shared0>) -> tensor<16x16xf16, #dot_operand_b>\n+    %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma0>\n \n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n-    %D = tt.dot %AA, %BB, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #shared0> * tensor<16x16xf16, #shared0> -> tensor<16x16xf32, #mma0>\n+    %D = tt.dot %AA_DOT, %BB_DOT, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n \n     return\n   }\n@@ -813,6 +817,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n }\n \n // -----\n+\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n #shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n #mma = #triton_gpu.mma<{version = 2, warpsPerCTA = [2, 2]}>\n@@ -833,3 +838,15 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     return\n   }\n }\n+\n+// -----\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: atomic_add_f32\n+  func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n+    // CHECK: llvm.inline_asm\n+    // CHECK-SAME: atom.global.gpu.add.f32\n+    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n+    return\n+  }\n+}"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 12, "deletions": 7, "changes": 19, "file_content_changes": "@@ -4,9 +4,9 @@\n // matmul: 128x32 @ 32x128 -> 128x128\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#A = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#B = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#A = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n+#B = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n // CHECK: func @matmul_loop\n // CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n@@ -30,7 +30,9 @@\n // CHECK: %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n // CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n-// CHECK:   tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n+// CHECK:   %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n+// CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n+// CHECK:   tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n@@ -87,15 +89,17 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n // CHECK:   %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n // CHECK:   %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n-// CHECK:     tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n+// CHECK:     %[[arg_a0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_a0]]\n+// CHECK:     %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n+// CHECK:     tt.dot %[[arg_a0_dot_op]], %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index\n // CHECK:     %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:     %[[NEXT_A:.*]] = tensor.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n-// CHECK:     %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_A:.*]] = tensor.extract_slice %[[NEXT_A_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n+// CHECK:   %[[NEXT_B:.*]] = tensor.extract_slice %[[NEXT_B_BUFFER]][%[[EXTRACT_IDX]], 0, 0]\n // CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n // CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n // CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n@@ -141,7 +145,8 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n // CHECK: triton_gpu.async_wait {num = 1 : i32}\n // CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n // CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_1]]\n-// CHECK:   tt.dot {{.*}}, %[[arg_b0]], {{.*}}\n+// CHECK:   %[[arg_b0_dot_op:.*]] = triton_gpu.convert_layout %[[arg_b0]]\n+// CHECK:   tt.dot {{.*}}, %[[arg_b0_dot_op]], {{.*}}\n // CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_INT:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n // CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.index_cast %[[EXTRACT_INT]] : i32 to index"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu -tritongpu-pipeline=num-stages=3 -tritongpu-combine -test-print-allocation 2>&1 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu -tritongpu-combine -tritongpu-pipeline=num-stages=3 -tritongpu-combine -test-print-allocation 2>&1 | FileCheck %s\n \n // CHECK: offset = 0, size = 49152\n // CHECK: offset = 49152, size = 49152"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "added", "additions": 65, "deletions": 0, "changes": 65, "file_content_changes": "@@ -0,0 +1,65 @@\n+// RUN: triton-opt %s -split-input-file -tritongpu-prefetch | FileCheck %s\n+\n+// 4 warps\n+// matmul: 128x32 @ 32x128 -> 128x128\n+#AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#A = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#B = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#A_OP = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n+#B_OP = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n+\n+\n+// CHECK: func @matmul_loop\n+// CHECK-DAG: %[[A0_PREFETCH_SMEM:.*]] = tensor.extract_slice %[[A0:.*]][0, 0] [128, 16]\n+// CHECK-DAG: %[[A0_PREFETCH:.*]] = triton_gpu.convert_layout %[[A0_PREFETCH_SMEM]]\n+// CHECK-DAG: %[[B0_PREFETCH_SMEM:.*]] = tensor.extract_slice %[[B0:.*]][0, 0] [16, 128]\n+// CHECK-DAG: %[[B0_PREFETCH:.*]] = triton_gpu.convert_layout %[[B0_PREFETCH_SMEM]]\n+// CHECK:     scf.for {{.*}} iter_args({{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, %[[a0_prefetch:.*]] = %[[A0_PREFETCH]], %[[b0_prefetch:.*]] = %[[B0_PREFETCH]]\n+// CHECK:       %[[D_FIRST:.*]] = tt.dot %[[a0_prefetch]], %[[b0_prefetch:.*]], {{.*}}\n+// CHECK-DAG:   %[[A_REM_SMEM:.*]] = tensor.extract_slice %[[arg_a0]][0, 16] [128, 16]\n+// CHECK-DAG:   %[[A_REM:.*]] = triton_gpu.convert_layout %[[A_REM_SMEM]]\n+// CHECK-DAG:   %[[B_REM_SMEM:.*]] = tensor.extract_slice %[[arg_b0]][16, 0] [16, 128]\n+// CHECK-DAG:   %[[B_REM:.*]] = triton_gpu.convert_layout %[[B_REM_SMEM]]\n+// CHECK:       tt.dot %[[A_REM]], %[[B_REM]], %[[D_FIRST:.*]]\n+// CHECK-DAG:   %[[NEXT_A_PREFETCH_SMEM:.*]] = tensor.extract_slice {{.*}}[0, 0] [128, 16]\n+// CHECK-DAG:   %[[NEXT_A_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_A_PREFETCH_SMEM]]\n+// CHECK-DAG:   %[[NEXT_B_PREFETCH_SMEM:.*]] = tensor.extract_slice {{.*}}[0, 0] [16, 128]\n+// CHECK-DAG:   %[[NEXT_B_PREFETCH:.*]] = triton_gpu.convert_layout %[[NEXT_B_PREFETCH_SMEM]]\n+// CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_PREFETCH]], %[[NEXT_B_PREFETCH]]\n+func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n+  %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+\n+  %a_mask = arith.constant dense<true> : tensor<128x32xi1, #AL>\n+  %a_other = arith.constant dense<0.00e+00> : tensor<128x32xf16, #AL>\n+  %b_mask = arith.constant dense<true> : tensor<32x128xi1, #BL>\n+  %b_other = arith.constant dense<0.00e+00> : tensor<32x128xf16, #BL>\n+  %c_init = arith.constant dense<0.00e+00> : tensor<128x128xf32, #C>\n+\n+  %a_off = arith.constant dense<4> : tensor<128x32xi32, #AL>\n+  %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n+\n+  %a_ = tt.load %a_ptr_init, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+  %a_init = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+  %b_ = tt.load %b_ptr_init, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n+  %b_init = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n+\n+  scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %a = %a_init, %b = %b_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x32xf16, #A>, tensor<32x128xf16, #B>, tensor<128x128xf32, #C>) {\n+    %a_op = triton_gpu.convert_layout %a : (tensor<128x32xf16, #A>) -> tensor<128x32xf16, #A_OP>\n+    %b_op = triton_gpu.convert_layout %b : (tensor<32x128xf16, #B>) -> tensor<32x128xf16, #B_OP>\n+    %c = tt.dot %a_op, %b_op, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_OP> * tensor<32x128xf16, #B_OP> -> tensor<128x128xf32, #C>\n+\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ = tt.load %next_a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+    %next_a = triton_gpu.convert_layout %next_a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+    %next_b_ = tt.load %next_b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n+    %next_b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n+\n+    scf.yield %next_a_ptr, %next_b_ptr, %next_a, %next_b, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x32xf16, #A>, tensor<32x128xf16, #B>, tensor<128x128xf32, #C>\n+  }\n+  return\n+}\n+"}, {"filename": "test/TritonGPU/swizzle.mlir", "status": "removed", "additions": 0, "deletions": 71, "changes": 71, "file_content_changes": "@@ -1,71 +0,0 @@\n-// RUN: triton-opt %s -split-input-file -tritongpu-swizzle | FileCheck %s\n-\n-#shared = #triton_gpu.shared<{vec=1, perPhase=1, maxPhase=1 ,order = [1, 0]}>\n-#mma1w = #triton_gpu.mma<{version=2, warpsPerCTA=[1, 1]}>\n-#mma2w = #triton_gpu.mma<{version=2, warpsPerCTA=[1, 2]}>\n-#mma4w = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 2]}>\n-#mma8w = #triton_gpu.mma<{version=2, warpsPerCTA=[2, 4]}>\n-\n-// CHECK: [[shared_v8p1m8:#.*]] = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 8, order = [1, 0]}>\n-// CHECK: [[shared_v8p2m4:#.*]] = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-// CHECK: [[shared_v8p4m2:#.*]] = #triton_gpu.shared<{vec = 8, perPhase = 4, maxPhase = 2, order = [1, 0]}>\n-\n-#shared2 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#shared3 = #triton_gpu.shared<{vec = 8, perPhase = 4, maxPhase = 2, order = [1, 0]}>\n-\n-\n-module attributes {\"triton_gpu.num-warps\" = 8 : i32} {\n-  // CHECK-LABEL: swizzle_mma_f16_128x256x64_w8\n-  func @swizzle_mma_f16_128x256x64_w8(%A: tensor<128x64xf16, #shared>, %B: tensor<64x256xf16, #shared>) {\n-    %cst0 = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma8w>\n-    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x64xf16, {{.*}}>) -> tensor<128x64xf16, [[shared_v8p1m8]]>\n-    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<64x256xf16, {{.*}}>) -> tensor<64x256xf16, [[shared_v8p1m8]]>\n-    %D = tt.dot %A, %B, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<128x64xf16, #shared> * tensor<64x256xf16, #shared> -> tensor<128x256xf32, #mma8w>\n-    return\n-  }\n-}\n-\n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK-LABEL: swizzle_mma_f16_128x128x64_w4\n-  func @swizzle_mma_f16_128x128x64_w4(%A: tensor<128x64xf16, #shared>, %B: tensor<64x128xf16, #shared>) {\n-    %cst0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #mma4w>\n-    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x64xf16, {{.*}}>) -> tensor<128x64xf16, [[shared_v8p1m8]]>\n-    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<64x128xf16, {{.*}}>) -> tensor<64x128xf16, [[shared_v8p1m8]]>\n-    %D = tt.dot %A, %B, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<128x64xf16, #shared> * tensor<64x128xf16, #shared> -> tensor<128x128xf32, #mma4w>\n-    return\n-  }\n-}\n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK-LABEL: swizzle_mma_f16_128x128x32_w4\n-  func @swizzle_mma_f16_128x128x32_w4(%A: tensor<128x32xf16, #shared>, %B: tensor<32x128xf16, #shared>) {\n-    %cst0 = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #mma4w>\n-    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<128x32xf16, {{.*}}>) -> tensor<128x32xf16, [[shared_v8p2m4]]>\n-    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x128xf16, {{.*}}>) -> tensor<32x128xf16, [[shared_v8p1m8]]>\n-    %D = tt.dot %A, %B, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #shared> * tensor<32x128xf16, #shared> -> tensor<128x128xf32, #mma4w>\n-    return\n-  }\n-}\n-\n-module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n-  // CHECK-LABEL: swizzle_mma_f16_32x32x32_w2\n-  func @swizzle_mma_f16_32x32x32_w2(%A: tensor<32x32xf16, #shared>, %B: tensor<32x32xf16, #shared>) {\n-    %cst0 = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma2w>\n-    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x32xf16, {{.*}}>) -> tensor<32x32xf16, [[shared_v8p2m4]]>\n-    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<32x32xf16, {{.*}}>) -> tensor<32x32xf16, [[shared_v8p2m4]]>\n-    %D = tt.dot %A, %B, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<32x32xf16, #shared> * tensor<32x32xf16, #shared> -> tensor<32x32xf32, #mma2w>\n-    return\n-  }\n-}\n-\n-module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK-LABEL: swizzle_mma_f16_16x16x16_w1\n-  func @swizzle_mma_f16_16x16x16_w1(%A: tensor<16x16xf16, #shared>, %B: tensor<16x16xf16, #shared>) {\n-    %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma1w>\n-    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<16x16xf16, {{.*}}>) -> tensor<16x16xf16, [[shared_v8p4m2]]>\n-    // CHECK: {{.*}} = triton_gpu.convert_layout {{.*}} : (tensor<16x16xf16, {{.*}}>) -> tensor<16x16xf16, [[shared_v8p4m2]]>\n-    %D = tt.dot %A, %B, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #shared> * tensor<16x16xf16, #shared> -> tensor<16x16xf32, #mma1w>\n-    return\n-  }\n-}"}, {"filename": "unittest/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -26,3 +26,4 @@ endfunction()\n \n add_subdirectory(Analysis)\n add_subdirectory(Conversion)\n+add_subdirectory(Dialect)"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PtxAsmFormatTest.cpp", "status": "modified", "additions": 22, "deletions": 1, "changes": 23, "file_content_changes": "@@ -76,7 +76,7 @@ TEST_F(PtxAsmFormatTest, complexInstruction) {\n \n   auto &ld =\n       builder\n-          .create<PTXIOInstr>(\"ld\") //\n+          .create<>(\"ld\") //\n           ->o(\"volatile\", isVolatile)\n           .global()\n           .o(\"ca\", cache == CacheModifier::CA)\n@@ -121,5 +121,26 @@ TEST_F(PtxAsmFormatTest, MultiLinePTX) {\n   EXPECT_EQ(values[1], v[2]); // $1 -> v[2]\n }\n \n+TEST_F(PtxAsmFormatTest, onlyAttachMLIRArgs) {\n+  PTXBuilder builder;\n+  const char *ptxCode =\n+      \".param .b64 param0;\\n\" // prepare param0 (format string)\n+      \"st.param.b64 [param0], %0;\\n\"\n+      \"st.param.b64 [param0], %1;\\n\"\n+      \"st.param.b64 [param0], %2;\\n\";\n+\n+  auto &ptxSnippet = *builder.create(ptxCode);\n+  auto *opr0 = builder.newOperand(v[0], \"r\");\n+  auto *opr1 = builder.newOperand(v[1], \"r\");\n+  auto *opr2 = builder.newOperand(v[2], \"r\");\n+  ptxSnippet({opr1, opr2, opr0}, true);\n+\n+  EXPECT_EQ(builder.dump(), ptxCode);\n+  ASSERT_EQ(builder.getAllMLIRArgs()[0], v[1]);\n+  ASSERT_EQ(builder.getAllMLIRArgs()[1], v[2]);\n+  ASSERT_EQ(builder.getAllMLIRArgs()[2], v[0]);\n+  ASSERT_EQ(builder.getAllMLIRArgs().size(), 3);\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "unittest/Dialect/CMakeLists.txt", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+add_subdirectory(TritonGPU)"}, {"filename": "unittest/Dialect/TritonGPU/CMakeLists.txt", "status": "added", "additions": 6, "deletions": 0, "changes": 6, "file_content_changes": "@@ -0,0 +1,6 @@\n+\n+add_triton_ut(\n+\tNAME TestSwizzling\n+\tSRCS SwizzleTest.cpp\n+\tLIBS TritonGPUIR  ${dialect_libs} ${conversion_libs}\n+)\n\\ No newline at end of file"}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "added", "additions": 52, "deletions": 0, "changes": 52, "file_content_changes": "@@ -0,0 +1,52 @@\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include <gtest/gtest.h>\n+\n+using namespace mlir;\n+using mlir::triton::gpu::SharedEncodingAttr;\n+\n+struct swizzleParams {\n+  int vec;\n+  int perPhase;\n+  int maxPhase;\n+};\n+\n+struct ParamT {\n+  std::array<int64_t, 2> shape;\n+  int opIdx;\n+  int typeWidth;\n+  swizzleParams refSwizzle;\n+};\n+\n+class SwizzleDotOperandTestFixture : public ::testing::TestWithParam<ParamT> {\n+protected:\n+  ParamType param;\n+};\n+\n+TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n+  auto params = GetParam();\n+  // init context\n+  MLIRContext ctx;\n+  ctx.loadDialect<triton::gpu::TritonGPUDialect>();\n+  // create encoding\n+  auto parent = triton::gpu::MmaEncodingAttr::get(&ctx, 2, {1, 1});\n+  auto encoding =\n+      triton::gpu::DotOperandEncodingAttr::get(&ctx, params.opIdx, parent);\n+\n+  // create element type\n+  Type eltType = IntegerType::get(&ctx, params.typeWidth);\n+  auto layout = SharedEncodingAttr::get(&ctx, encoding, params.shape, eltType);\n+\n+  ASSERT_EQ(layout.getVec(), params.refSwizzle.vec);\n+  ASSERT_EQ(layout.getPerPhase(), params.refSwizzle.perPhase);\n+  ASSERT_EQ(layout.getMaxPhase(), params.refSwizzle.maxPhase);\n+}\n+\n+INSTANTIATE_TEST_SUITE_P(TestDotOperands, SwizzleDotOperandTestFixture,\n+                         ::testing::Values(ParamT{{128, 64}, 0, 16, {8, 1, 8}},\n+                                           ParamT{{64, 256}, 1, 16, {8, 1, 8}},\n+                                           ParamT{{128, 32}, 0, 16, {8, 2, 4}},\n+                                           ParamT{{32, 128}, 1, 16, {8, 1, 8}},\n+                                           ParamT{{32, 32}, 0, 16, {8, 2, 4}},\n+                                           ParamT{{32, 32}, 1, 16, {8, 2, 4}},\n+                                           ParamT{{16, 16}, 0, 16, {8, 4, 2}},\n+                                           ParamT{{16, 16}, 1, 16, {8, 4, 2}}));\n\\ No newline at end of file"}]