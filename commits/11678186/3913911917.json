[{"filename": "include/triton/Dialect/Triton/IR/Dialect.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -26,8 +26,8 @@ class DialectInferLayoutInterface\n   DialectInferLayoutInterface(Dialect *dialect) : Base(dialect) {}\n \n   virtual LogicalResult\n-  inferTransOpEncoding(Attribute operandEncoding, \n-                      Attribute &resultEncoding) const = 0;\n+  inferTransOpEncoding(Attribute operandEncoding,\n+                       Attribute &resultEncoding) const = 0;\n \n   virtual LogicalResult\n   inferReduceOpEncoding(Attribute operandEncoding, unsigned axis,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -363,7 +363,6 @@ static Value shflSync(Location loc, ConversionPatternRewriter &rewriter,\n   return builder.launch(rewriter, loc, val.getType(), false);\n }\n \n-\n } // namespace LLVM\n } // namespace mlir\n "}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 2, "deletions": 5, "changes": 7, "file_content_changes": "@@ -221,10 +221,8 @@ mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n   Attribute retEncoding;\n   if (argEncoding) {\n     Dialect &dialect = argEncoding.getDialect();\n-    auto inferLayoutInterface =\n-        dyn_cast<DialectInferLayoutInterface>(&dialect);\n-    if (inferLayoutInterface\n-            ->inferTransOpEncoding(argEncoding, retEncoding)\n+    auto inferLayoutInterface = dyn_cast<DialectInferLayoutInterface>(&dialect);\n+    if (inferLayoutInterface->inferTransOpEncoding(argEncoding, retEncoding)\n             .failed()) {\n       llvm::report_fatal_error(\"failed to infer layout for ReduceOp\");\n       return mlir::failure();\n@@ -233,7 +231,6 @@ mlir::LogicalResult mlir::triton::TransOp::inferReturnTypes(\n   inferredReturnTypes.push_back(\n       RankedTensorType::get(retShape, retEltTy, retEncoding));\n   return mlir::success();\n-\n }\n \n //-- DotOp --"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 12, "deletions": 14, "changes": 26, "file_content_changes": "@@ -161,11 +161,10 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n       threads.push_back(blockedLayout.getThreadsPerWarp()[d] *\n                         blockedLayout.getWarpsPerCTA()[d]);\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    if(mmaLayout.getVersionMajor() == 2){\n-      threads = {8*mmaLayout.getWarpsPerCTA()[0], \n-                 4*mmaLayout.getWarpsPerCTA()[1]};\n-    }\n-    else\n+    if (mmaLayout.getVersionMajor() == 2) {\n+      threads = {8 * mmaLayout.getWarpsPerCTA()[0],\n+                 4 * mmaLayout.getWarpsPerCTA()[1]};\n+    } else\n       assert(0 && \"Unimplemented usage of MmaEncodingAttr\");\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n@@ -742,19 +741,18 @@ struct TritonGPUInferLayoutInterface\n     return success();\n   }\n \n-  LogicalResult\n-  inferTransOpEncoding(Attribute operandEncoding, Attribute &resultEncoding) const {\n-    SharedEncodingAttr sharedEncoding = operandEncoding.dyn_cast<SharedEncodingAttr>();\n-    if(!sharedEncoding)\n+  LogicalResult inferTransOpEncoding(Attribute operandEncoding,\n+                                     Attribute &resultEncoding) const {\n+    SharedEncodingAttr sharedEncoding =\n+        operandEncoding.dyn_cast<SharedEncodingAttr>();\n+    if (!sharedEncoding)\n       return failure();\n     SmallVector<unsigned> retOrder(sharedEncoding.getOrder().begin(),\n                                    sharedEncoding.getOrder().end());\n     std::reverse(retOrder.begin(), retOrder.end());\n-    resultEncoding = SharedEncodingAttr::get(getDialect()->getContext(),\n-                                              sharedEncoding.getVec(),\n-                                              sharedEncoding.getPerPhase(),\n-                                              sharedEncoding.getMaxPhase(),\n-                                              retOrder);\n+    resultEncoding = SharedEncodingAttr::get(\n+        getDialect()->getContext(), sharedEncoding.getVec(),\n+        sharedEncoding.getPerPhase(), sharedEncoding.getMaxPhase(), retOrder);\n     return mlir::success();\n   }\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/DecomposeConversions.cpp", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "file_content_changes": "@@ -27,22 +27,24 @@ class TritonGPUDecomposeConversionsPass\n public:\n   TritonGPUDecomposeConversionsPass() = default;\n \n-  void runOnOperation() override { \n+  void runOnOperation() override {\n     MLIRContext *context = &getContext();\n     ModuleOp mod = getOperation();\n     mod.walk([&](triton::gpu::ConvertLayoutOp cvtOp) -> void {\n       OpBuilder builder(cvtOp);\n       auto srcType = cvtOp.getOperand().getType().cast<RankedTensorType>();\n       auto dstType = cvtOp.getType().cast<RankedTensorType>();\n       auto srcEncoding = srcType.getEncoding();\n-      if(srcEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+      if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>())\n         return;\n       auto dstDotOp =\n           dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-      if(!dstDotOp)\n+      if (!dstDotOp)\n         return;\n-      if (auto srcMmaEncoding = srcEncoding.dyn_cast<triton::gpu::MmaEncodingAttr>()) {\n-        if(srcMmaEncoding.getWarpsPerCTA()[1] == 1 && dstDotOp.getParent()==srcMmaEncoding)\n+      if (auto srcMmaEncoding =\n+              srcEncoding.dyn_cast<triton::gpu::MmaEncodingAttr>()) {\n+        if (srcMmaEncoding.getWarpsPerCTA()[1] == 1 &&\n+            dstDotOp.getParent() == srcMmaEncoding)\n           return;\n       }\n       auto tmpType = RankedTensorType::get(\n@@ -60,7 +62,6 @@ class TritonGPUDecomposeConversionsPass\n   }\n };\n \n-std::unique_ptr<Pass>\n-mlir::createTritonGPUDecomposeConversionsPass() {\n+std::unique_ptr<Pass> mlir::createTritonGPUDecomposeConversionsPass() {\n   return std::make_unique<TritonGPUDecomposeConversionsPass>();\n }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 27, "deletions": 24, "changes": 51, "file_content_changes": "@@ -21,20 +21,22 @@\n \n using namespace mlir;\n \n-static inline bool willIncreaseRegisterPressure(triton::gpu::ConvertLayoutOp op) {\n+static inline bool\n+willIncreaseRegisterPressure(triton::gpu::ConvertLayoutOp op) {\n   auto srcType = op.getOperand().getType().cast<RankedTensorType>();\n   auto dstType = op.getResult().getType().cast<RankedTensorType>();\n   auto srcEncoding = srcType.getEncoding();\n   auto dstEncoding = dstType.getEncoding();\n-  if(srcEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+  if (srcEncoding.isa<triton::gpu::SharedEncodingAttr>())\n     return true;\n-  if(dstEncoding.isa<triton::gpu::DotOperandEncodingAttr>())\n+  if (dstEncoding.isa<triton::gpu::DotOperandEncodingAttr>())\n     return true;\n   return false;\n }\n \n class TritonGPUReorderInstructionsPass\n-    : public TritonGPUReorderInstructionsBase<TritonGPUReorderInstructionsPass> {\n+    : public TritonGPUReorderInstructionsBase<\n+          TritonGPUReorderInstructionsPass> {\n public:\n   TritonGPUReorderInstructionsPass() = default;\n \n@@ -43,47 +45,49 @@ class TritonGPUReorderInstructionsPass\n     ModuleOp m = getOperation();\n     // Sink conversions into loops when they will increase\n     // register pressure\n-    DenseMap<Operation*, Operation *> opToMove;\n-    m.walk([&](triton::gpu::ConvertLayoutOp op){\n-      if(!willIncreaseRegisterPressure(op))\n+    DenseMap<Operation *, Operation *> opToMove;\n+    m.walk([&](triton::gpu::ConvertLayoutOp op) {\n+      if (!willIncreaseRegisterPressure(op))\n         return;\n       auto user_begin = op->user_begin();\n       auto user_end = op->user_end();\n-      if(std::distance(user_begin, user_end) != 1)\n+      if (std::distance(user_begin, user_end) != 1)\n         return;\n       opToMove.insert({op, *user_begin});\n     });\n-    for(auto &kv: opToMove)\n+    for (auto &kv : opToMove)\n       kv.first->moveBefore(kv.second);\n     // Move convert(load) immediately after dependent load\n-    m.walk([&](triton::gpu::ConvertLayoutOp op){\n+    m.walk([&](triton::gpu::ConvertLayoutOp op) {\n       auto dstType = op.getResult().getType().cast<RankedTensorType>();\n       auto dstEncoding = dstType.getEncoding();\n-      if(!dstEncoding.isa<triton::gpu::SharedEncodingAttr>())\n+      if (!dstEncoding.isa<triton::gpu::SharedEncodingAttr>())\n         return;\n-      Operation* argOp = op.getOperand().getDefiningOp();\n-      if(!argOp)\n+      Operation *argOp = op.getOperand().getDefiningOp();\n+      if (!argOp)\n         return;\n       op->moveAfter(argOp);\n     });\n     // Move transpositions just after their definition\n     opToMove.clear();\n-    m.walk([&](triton::TransOp op){\n-      Operation* argOp =op.getOperand().getDefiningOp();\n-      if(!argOp)\n+    m.walk([&](triton::TransOp op) {\n+      Operation *argOp = op.getOperand().getDefiningOp();\n+      if (!argOp)\n         return;\n       op->moveAfter(argOp);\n     });\n-    // Move `dot` operand so that conversions to opIdx=0 happens before conversions to opIdx=1\n-    m.walk([&](triton::gpu::ConvertLayoutOp op){\n+    // Move `dot` operand so that conversions to opIdx=0 happens before\n+    // conversions to opIdx=1\n+    m.walk([&](triton::gpu::ConvertLayoutOp op) {\n       auto dstType = op.getResult().getType().cast<RankedTensorType>();\n-      auto dstEncoding = dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-      if(!dstEncoding)\n+      auto dstEncoding =\n+          dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n+      if (!dstEncoding)\n         return;\n       int opIdx = dstEncoding.getOpIdx();\n-      if(opIdx != 1)\n+      if (opIdx != 1)\n         return;\n-      if(op->getUsers().empty())\n+      if (op->getUsers().empty())\n         return;\n       auto user_begin = op->user_begin();\n       op->moveBefore(*user_begin);\n@@ -92,7 +96,6 @@ class TritonGPUReorderInstructionsPass\n   }\n };\n \n-std::unique_ptr<Pass>\n-mlir::createTritonGPUReorderInstructionsPass() {\n+std::unique_ptr<Pass> mlir::createTritonGPUReorderInstructionsPass() {\n   return std::make_unique<TritonGPUReorderInstructionsPass>();\n }"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1353,8 +1353,7 @@ void init_triton_ir(py::module &&m) {\n            })\n       .def(\"add_tritongpu_decompose_conversions_pass\",\n            [](mlir::PassManager &self) {\n-             self.addPass(\n-                 mlir::createTritonGPUDecomposeConversionsPass());\n+             self.addPass(mlir::createTritonGPUDecomposeConversionsPass());\n            })\n       .def(\"add_triton_gpu_to_llvm\",\n            [](mlir::PassManager &self) {"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -194,6 +194,7 @@ def _bwd_kernel(\n \n empty = torch.empty(128, device=\"cuda\")\n \n+\n class _attention(torch.autograd.Function):\n \n     @staticmethod"}]