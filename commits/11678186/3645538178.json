[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -88,8 +88,10 @@ jobs:\n       - name: Run python tests on V100\n         if: ${{matrix.runner[0] == 'self-hosted' && matrix.runner[1] == 'V100'}}\n         run: |\n+          # TODO[Superjomn]: Remove the forloop-unroll setting after pipeline pass works\n           cd python/tests\n-          pytest test_gemm.py::test_gemm_no_scf_for_mmav1\n+          export TRITON_STATIC_LOOP_UNROLLING=1\n+          pytest test_gemm.py::test_gemm_for_mmav1\n \n       - name: Run CXX unittests\n         run: |"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "file_content_changes": "@@ -103,15 +103,12 @@ def TT_AddPtrOp : TT_Op<\"addptr\",\n                      SameOperandsAndResultShape,\n                      SameOperandsAndResultEncoding,\n                       TypesMatchWith<\"result type matches ptr type\",\n-                                     \"result\", \"ptr\", \"$_self\">,\n-                      TypesMatchWith<\"result shape matches offset shape\",\n-                                     \"result\", \"offset\",\n-                                     \"getI32SameShape($_self)\">]> {\n-    let arguments = (ins TT_PtrLike:$ptr, TT_I32Like:$offset);\n+                                     \"result\", \"ptr\", \"$_self\">]> {\n+    let arguments = (ins TT_PtrLike:$ptr, TT_IntLike:$offset);\n \n     let results = (outs TT_PtrLike:$result);\n \n-    let assemblyFormat = \"$ptr `,` $offset attr-dict `:` type($result)\";\n+    let assemblyFormat = \"$ptr `,` $offset attr-dict `:` type($result) `,` type($offset)\";\n }\n \n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 13, "deletions": 1, "changes": 14, "file_content_changes": "@@ -32,6 +32,12 @@ def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n   let arguments = (ins I32Attr:$num);\n \n   let assemblyFormat = \"attr-dict\";\n+\n+  let extraClassDeclaration = [{\n+    static bool isSupported(int computeCapability) {\n+      return computeCapability >= 80;\n+    }\n+  }];\n }\n \n // Port Arith_CmpIOp & Arith_CmpFOp & Std_SelectOp to TritonGPU.\n@@ -152,7 +158,13 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n   //}];\n \n   let extraClassDeclaration = [{\n-      static DenseSet<unsigned> getEligibleLoadByteWidth(int computeCapability);\n+    static DenseSet<unsigned> getEligibleLoadByteWidth(int computeCapability) {\n+      DenseSet<unsigned> validLoadBytes;\n+      if (computeCapability >= 80) {\n+        validLoadBytes = {4, 8, 16};\n+      }\n+      return validLoadBytes;\n+    }\n   }];\n \n   // The custom parser could be replaced with oilist in LLVM-16"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotHelpers.h", "status": "modified", "additions": 10, "deletions": 1, "changes": 11, "file_content_changes": "@@ -72,6 +72,11 @@ struct DotOpMmaV1ConversionHelper {\n \n     bool isARow = order[0] != 0;\n     bool isAVec4 = !isARow && shape[order[0]] <= 16; // fp16*4 = 16bytes\n+    // TODO[Superjomn]: Support the case when isAVec4=false later\n+    // Currently, we only support ld.v2, for the mma layout varies with\n+    // different ld vector width.\n+    isAVec4 = true;\n+\n     int packSize0 = (isARow || isAVec4) ? 1 : 2;\n \n     SmallVector<int> fpw({2, 2, 1});\n@@ -98,6 +103,11 @@ struct DotOpMmaV1ConversionHelper {\n     auto order = getOrder();\n     bool isBRow = order[0] != 0;\n     bool isBVec4 = isBRow && shape[order[0]] <= 16;\n+    // TODO[Superjomn]: Support the case when isBVec4=false later\n+    // Currently, we only support ld.v2, for the mma layout varies with\n+    // different ld vector width.\n+    isBVec4 = true;\n+\n     int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n     SmallVector<int> fpw({2, 2, 1});\n     SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n@@ -1455,7 +1465,6 @@ Value DotOpMmaV1ConversionHelper::loadB(\n   SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n                               sharedLayout.getOrder().end());\n \n-\n   Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n   bool isBRow = order[0] != 0;\n   bool isBVec4 = isBRow && shape[order[0]] <= 16;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 70, "deletions": 15, "changes": 85, "file_content_changes": "@@ -1707,6 +1707,43 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n \n /// ====================== reduce codegen end ==========================\n \n+/// ====================== cat codegen begin  ==========================\n+\n+struct CatOpConversion : public ConvertTritonGPUOpToLLVMPattern<CatOp> {\n+  using OpAdaptor = typename CatOp::Adaptor;\n+\n+  explicit CatOpConversion(LLVMTypeConverter &typeConverter,\n+                           PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<CatOp>(typeConverter, benefit) {}\n+\n+  LogicalResult\n+  matchAndRewrite(CatOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    Location loc = op->getLoc();\n+    auto resultTy = op.getType().template cast<RankedTensorType>();\n+    unsigned elems = getElemsPerThread(resultTy);\n+    Type elemTy =\n+        this->getTypeConverter()->convertType(resultTy.getElementType());\n+    SmallVector<Type> types(elems, elemTy);\n+    // unpack input values\n+    auto lhsVals = getElementsFromStruct(loc, adaptor.lhs(), rewriter);\n+    auto rhsVals = getElementsFromStruct(loc, adaptor.rhs(), rewriter);\n+    // concatenate (and potentially reorder) values\n+    SmallVector<Value> retVals;\n+    for(Value v: lhsVals)\n+      retVals.push_back(v);\n+    for(Value v: rhsVals)\n+      retVals.push_back(v);\n+    // pack and replace\n+    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+    Value ret = getStructFromElements(loc, retVals, rewriter, structTy);\n+    rewriter.replaceOp(op, ret);\n+    return success();\n+  }\n+};\n+\n+/// ====================== cat codegen end    ==========================\n+\n template <typename SourceOp>\n struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   using OpAdaptor = typename SourceOp::Adaptor;\n@@ -3536,14 +3573,18 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n   auto has = helper.extractLoadedOperand(loadedA, NK, rewriter);\n   auto hbs = helper.extractLoadedOperand(loadedB, NK, rewriter);\n \n-  // initialize accumulators\n+  // Initialize accumulators with external values, the acc holds the accumulator\n+  // value that is shared between the MMA instructions inside a DotOp, we can\n+  // call the order of the values the accumulator-internal order.\n   SmallVector<Value> acc = getElementsFromStruct(loc, loadedC, rewriter);\n   size_t resSize = acc.size();\n+\n+  // The resVals holds the final result of the DotOp.\n+  // NOTE The current order of resVals is different from acc, we call it the\n+  // accumulator-external order. and\n   SmallVector<Value> resVals(resSize);\n \n-  auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n-    auto ha = has.at({m, k});\n-    auto hb = hbs.at({n, k});\n+  auto getIdx = [&](int m, int n) {\n     std::vector<size_t> idx{{\n         (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n         (m * 2 + 0) + (n * 4 + 1) * numM,\n@@ -3554,8 +3595,29 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n         (m * 2 + 1) + (n * 4 + 2) * numM, // row3\n         (m * 2 + 1) + (n * 4 + 3) * numM,\n     }};\n+    return idx;\n+  };\n+\n+  { // convert the acc's value from accumuator-external order to\n+    // accumulator-internal order.\n+    SmallVector<Value> accInit(acc.size());\n+\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      for (unsigned n = 0; n < numN / 2; ++n) {\n+        auto idx = getIdx(m, n);\n+        for (unsigned i = 0; i < 8; ++i)\n+          accInit[idx[i]] = acc[(m * numN / 2 + n) * 8 + i];\n+      }\n+\n+    acc = accInit;\n+  }\n+\n+  auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n+    auto ha = has.at({m, k});\n+    auto hb = hbs.at({n, k});\n \n     PTXBuilder builder;\n+    auto idx = getIdx(m, n);\n \n     auto *resOprs = builder.newListOperand(8, \"=f\");\n     auto *AOprs = builder.newListOperand({\n@@ -3587,8 +3649,6 @@ DotOpConversion::convertMMA884(triton::DotOp op, DotOpAdaptor adaptor,\n     for (unsigned i = 0; i < 8; i++) {\n       Value elem = extract_val(f32_ty, res, getIntAttr(i));\n       acc[idx[i]] = elem;\n-      // TODO[goostavz]: double confirm this when m/n/k = [32, 32, x] has been\n-      // verified before MMA\n       resVals[(m * numN / 2 + n) * 8 + i] = elem;\n     }\n   };\n@@ -4555,6 +4615,7 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                                            benefit);\n   patterns.add<DotOpConversion>(typeConverter, allocation, smem, benefit);\n   patterns.add<TransOpConversion>(typeConverter, benefit);\n+  patterns.add<CatOpConversion>(typeConverter, benefit);\n   patterns.add<PrintfOpConversion>(typeConverter, benefit);\n }\n \n@@ -4666,8 +4727,7 @@ class ConvertTritonGPUToLLVM\n       // capability does not support async copy, then we do decompose\n       if (triton::gpu::InsertSliceAsyncOp::getEligibleLoadByteWidth(\n               computeCapability)\n-              .contains(byteWidth) &&\n-          computeCapability >= 80)\n+              .contains(byteWidth))\n         return;\n \n       // load\n@@ -4701,13 +4761,8 @@ class ConvertTritonGPUToLLVM\n \n     // async wait is supported in Ampere and later\n     mod.walk([&](triton::gpu::AsyncWaitOp asyncWaitOp) -> void {\n-      if (computeCapability < 80) {\n-        asyncWaitOp.erase();\n-      } else if (decomposed) {\n-        OpBuilder builder(asyncWaitOp);\n-        // Wait for all previous async ops\n-        auto newAsyncWaitOp = builder.create<triton::gpu::AsyncWaitOp>(\n-            asyncWaitOp.getLoc(), builder.getI64IntegerAttr(0));\n+      if (!triton::gpu::AsyncWaitOp::isSupported(computeCapability) ||\n+          decomposed) {\n         asyncWaitOp.erase();\n       }\n     });"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "file_content_changes": "@@ -251,6 +251,22 @@ struct TritonDotPattern : public OpConversionPattern<triton::DotOp> {\n   }\n };\n \n+struct TritonCatPattern : public OpConversionPattern<triton::CatOp> {\n+\n+  using OpConversionPattern<triton::CatOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::CatOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    // For now, this behaves like generic, but this will evolve when\n+    // we add support for `can_reorder=False`\n+    Type retType = this->getTypeConverter()->convertType(op.getType());\n+    rewriter.replaceOpWithNewOp<triton::CatOp>(op, retType, adaptor.getOperands());\n+    return success();\n+  }\n+\n+};\n+\n struct TritonTransPattern : public OpConversionPattern<triton::TransOp> {\n \n   using OpConversionPattern<triton::TransOp>::OpConversionPattern;\n@@ -433,7 +449,9 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n       TritonGenericPattern<triton::IntToPtrOp>,\n       TritonGenericPattern<triton::PtrToIntOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n-      TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n+      TritonGenericPattern<triton::AddPtrOp>, \n+      TritonCatPattern,\n+      TritonReducePattern,\n       TritonTransPattern, TritonExpandDimsPattern, TritonMakeRangePattern,\n       TritonDotPattern, TritonLoadPattern, TritonStorePattern,\n       TritonExtElemwisePattern, TritonPrintfPattern, TritonAtomicRMWPattern>("}, {"filename": "lib/Dialect/Triton/IR/Traits.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -19,7 +19,7 @@ mlir::OpTrait::impl::verifySameOperandsAndResultEncoding(Operation *op) {\n   for (auto resultType : op->getResultTypes())\n     if (failed(verifySameEncoding(resultType, type)))\n       return op->emitOpError()\n-             << \"requires the same shape for all operands and results\";\n+             << \"requires the same encoding for all operands and results\";\n   return verifySameOperandsEncoding(op);\n }\n "}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -196,7 +196,7 @@ class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {\n     patterns.add<CombineDotAddFRevPattern>(context);\n     // %}\n     patterns.add<CombineSelectMaskedLoadPattern>(context);\n-    patterns.add<CombineAddPtrPattern>(context);\n+    // patterns.add<CombineAddPtrPattern>(context);\n     patterns.add<CombineBroadcastConstantPattern>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.td", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -29,13 +29,14 @@ def CombineDotAddFRevPattern : Pat<\n         (TT_DotOp $a, $b, $d, $allowTF32),\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n \n-\n+// TODO: this fails for addptr(addptr(ptr, i32), i64)\n+// Commented out until fixed\n // addptr(addptr(%ptr, %idx0), %idx1) => addptr(%ptr, AddI(%idx0, %idx1))\n //   Note: leave (sub %c0, %c0) canceling to ArithmeticDialect\n //         (ref: ArithmeticCanonicalization.td)\n-def CombineAddPtrPattern : Pat<\n-        (TT_AddPtrOp (TT_AddPtrOp $ptr, $idx0), $idx1),\n-        (TT_AddPtrOp $ptr, (Arith_AddIOp $idx0, $idx1))>;\n+// def CombineAddPtrPattern : Pat<\n+//         (TT_AddPtrOp (TT_AddPtrOp $ptr, $idx0), $idx1),\n+//         (TT_AddPtrOp $ptr, (Arith_AddIOp $idx0, $idx1))>;\n \n // broadcast(cst) => cst\n def getConstantValue : NativeCodeCall<\"getConstantValue($_builder, $0, $1)\">;"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 0, "deletions": 9, "changes": 9, "file_content_changes": "@@ -659,15 +659,6 @@ void printInsertSliceAsyncOp(OpAsmPrinter &printer,\n   printer.printStrippedAttrOrType(insertSliceAsyncOp.result().getType());\n }\n \n-DenseSet<unsigned>\n-InsertSliceAsyncOp::getEligibleLoadByteWidth(int computeCapability) {\n-  DenseSet<unsigned> validLoadBytes;\n-  if (computeCapability >= 80) {\n-    validLoadBytes = {4, 8, 16};\n-  }\n-  return validLoadBytes;\n-}\n-\n //===----------------------------------------------------------------------===//\n // ASM Interface (i.e.: alias)\n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -134,7 +134,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnChange=*/true,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n-  pm.addPass(createConvertTritonGPUToLLVMPass());\n+  pm.addPass(createConvertTritonGPUToLLVMPass(computeCapability));\n   // Canonicalize to eliminate the remaining UnrealizedConversionCastOp\n   pm.addPass(mlir::createCanonicalizerPass());\n   pm.addPass(mlir::createCSEPass()); // Simplify the IR to improve readability."}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -1382,10 +1382,11 @@ void init_triton_translation(py::module &m) {\n         llvm::SMDiagnostic error;\n         std::unique_ptr<llvm::Module> module =\n             llvm::parseIR(buffer->getMemBufferRef(), error, context);\n-        if (!module)\n+        if (!module) {\n           llvm::report_fatal_error(\n               \"failed to parse IR: \" + error.getMessage() +\n               \"lineno: \" + std::to_string(error.getLineNo()));\n+        }\n \n         // translate module to PTX\n         auto ptxCode ="}, {"filename": "python/tests/test_backend.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -64,12 +64,12 @@ def test_convert2d(dtype, shape, src_layout, dst_layout, device='cuda'):\n     %7 = tt.broadcast %6 : (tensor<1x128xi32, #src>) -> tensor<128x128xi32, #src>\n     %8 = tt.broadcast %5 : (tensor<128x1xi32, #src>) -> tensor<128x128xi32, #src>\n     %9 = arith.addi %8, %7 : tensor<128x128xi32, #src>\n-    %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>\n+    %10 = tt.addptr %2, %9 : tensor<128x128x!tt.ptr<f16>, #src>, tensor<128x128xi32, #src>\n     %11 = tt.load %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf16, #src>\n     %3 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<128x128x!tt.ptr<f16>, #dst>\n     %12 = triton_gpu.convert_layout %9 : (tensor<128x128xi32, #src>) -> tensor<128x128xi32, #dst>\n     %13 = triton_gpu.convert_layout %11 : (tensor<128x128xf16, #src>) -> tensor<128x128xf16, #dst>\n-    %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>\n+    %14 = tt.addptr %3, %12 : tensor<128x128x!tt.ptr<f16>, #dst>, tensor<128x128xi32, #dst>\n     tt.store %14, %13 : tensor<128x128xf16, #dst>\n     return\n   }"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 13, "deletions": 14, "changes": 27, "file_content_changes": "@@ -295,18 +295,17 @@ def matmul_kernel(\n \n \n # NOTE this is useful only on Volta GPU.\n-@pytest.mark.parametrize('SHAPE,NUM_WARPS,TRANS_A,TRANS_B', [\n-    (shape, num_warps, trans_a, trans_b)\n-    for shape in [\n-        [16, 16, 16],\n-        [16, 16, 32],\n-        [32, 16, 16],\n-        [32, 32, 32],\n-        [128, 16, 16],\n-    ]\n-    for num_warps in [1]\n-    for trans_a in [False]\n-    for trans_b in [False]\n+@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n+    # Non-forloop\n+    [16, 16, 16, 1, 16, 16, 16, False, False],\n+    [16, 16, 32, 1, 16, 16, 32, False, False],\n+    [32, 16, 32, 1, 32, 16, 32, False, False],\n+    [32, 32, 32, 1, 32, 32, 32, False, False],\n+    [128, 32, 32, 1, 128, 32, 32, False, False],\n+\n+    # split-K\n+    [16, 16, 32, 1, 16, 16, 16, False, False],\n+    [64, 64, 128, 1, 64, 64, 32, False, False],\n ])\n-def test_gemm_no_scf_for_mmav1(SHAPE, NUM_WARPS, TRANS_A, TRANS_B):\n-    test_gemm_no_scf(SHAPE, NUM_WARPS, TRANS_A, TRANS_B)\n+def test_gemm_for_mmav1(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n+    test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B)"}, {"filename": "python/tests/test_random.py", "status": "added", "additions": 198, "deletions": 0, "changes": 198, "file_content_changes": "@@ -0,0 +1,198 @@\n+import numpy as np\n+import pytest\n+import scipy.stats\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+#####################################\n+# Reference Philox Implementation\n+#####################################\n+\n+\n+class PhiloxConfig:\n+    def __init__(self, PHILOX_ROUND_A, PHILOX_ROUND_B, PHILOX_KEY_A, PHILOX_KEY_B, DTYPE):\n+        self.PHILOX_ROUND_A = np.array(PHILOX_ROUND_A, dtype=DTYPE)\n+        self.PHILOX_ROUND_B = np.array(PHILOX_ROUND_B, dtype=DTYPE)\n+        self.PHILOX_KEY_A = np.array(PHILOX_KEY_A, dtype=DTYPE)\n+        self.PHILOX_KEY_B = np.array(PHILOX_KEY_B, dtype=DTYPE)\n+        self.DTYPE = DTYPE\n+\n+\n+# This is better for GPU\n+PHILOX_32 = PhiloxConfig(\n+    PHILOX_KEY_A=0x9E3779B9,\n+    PHILOX_KEY_B=0xBB67AE85,\n+    PHILOX_ROUND_A=0xD2511F53,\n+    PHILOX_ROUND_B=0xCD9E8D57,\n+    DTYPE=np.uint32,\n+)\n+\n+# This is what numpy implements\n+PHILOX_64 = PhiloxConfig(\n+    PHILOX_KEY_A=0x9E3779B97F4A7C15,\n+    PHILOX_KEY_B=0xBB67AE8584CAA73B,\n+    PHILOX_ROUND_A=0xD2E7470EE14C6C93,\n+    PHILOX_ROUND_B=0xCA5A826395121157,\n+    DTYPE=np.uint64,\n+)\n+\n+\n+class CustomPhilox4x:\n+    def __init__(self, seed, config):\n+        self._config = config\n+        seed = self._into_pieces(seed)\n+        self._key = np.array(seed[:2], dtype=self._dtype)\n+        self._counter = np.array((0, 0) + seed[2:], dtype=self._dtype)\n+\n+    @property\n+    def _dtype(self):\n+        return self._config.DTYPE\n+\n+    def _into_pieces(self, n, pad=4):\n+        res = []\n+        while len(res) < pad:\n+            res.append(np.array(n, dtype=self._dtype))\n+            n >>= (np.dtype(self._dtype).itemsize * 8)\n+        assert n == 0\n+        return tuple(res)\n+\n+    def _multiply_low_high(self, a, b):\n+        low = a * b\n+        high = int(a) * int(b)\n+        high = np.array(high >> (np.dtype(self._dtype).itemsize * 8), dtype=self._dtype)\n+        return low, high\n+\n+    def _single_round(self, counter, key):\n+        lo0, hi0 = self._multiply_low_high(self._config.PHILOX_ROUND_A, counter[0])\n+        lo1, hi1 = self._multiply_low_high(self._config.PHILOX_ROUND_B, counter[2])\n+        ret0 = hi1 ^ counter[1] ^ key[0]\n+        ret1 = lo1\n+        ret2 = hi0 ^ counter[3] ^ key[1]\n+        ret3 = lo0\n+        return np.array([ret0, ret1, ret2, ret3], dtype=self._dtype)\n+\n+    def _raise_key(self, key):\n+        pk = [self._config.PHILOX_KEY_A, self._config.PHILOX_KEY_B]\n+        return key + np.array(pk, dtype=self._dtype)\n+\n+    def random_raw(self):\n+        counter = self._counter\n+        key = self._key\n+        for _ in range(10):\n+            counter = self._single_round(counter, key)\n+            key = self._raise_key(key)\n+        self.advance(1)\n+        return counter\n+\n+    def advance(self, n_steps):\n+        self._counter[0] += n_steps\n+        assert self._counter[0] < 2**32, \"FIXME: doesn't work for large offsets\"\n+\n+\n+class CustomPhilox(CustomPhilox4x):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.buffer = []\n+\n+    def random_raw(self):\n+        if len(self.buffer) == 0:\n+            self.buffer = list(super().random_raw())[::-1]\n+        return int(self.buffer.pop())\n+\n+\n+#####################################\n+# Unit Tests\n+#####################################\n+\n+BLOCK = 1024\n+\n+# test generation of random uint32\n+\n+\n+@pytest.mark.parametrize('size, seed',\n+                         [(size, seed) for size in ['10', '4,53', '10000']\n+                          for seed in [0, 42, 124, 54, 0xffffffff, 0xdeadbeefcafeb0ba]]\n+                         )\n+def test_randint(size, seed, device='cuda'):\n+    size = list(map(int, size.split(',')))\n+\n+    @triton.jit\n+    def kernel(X, N, seed):\n+        offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n+        rand = tl.randint(seed, offset)\n+        tl.store(X + offset, rand, mask=offset < N)\n+    # triton result\n+    x = torch.empty(size, dtype=torch.int32, device=device)\n+    N = x.numel()\n+    grid = (triton.cdiv(N, BLOCK),)\n+    kernel[grid](x, N, seed)\n+    out_tri = x.cpu().numpy().astype(np.uint32).flatten().tolist()\n+    # reference result\n+    gen = CustomPhilox4x(seed, config=PHILOX_32)\n+    out_ref = [gen.random_raw()[0] for _ in out_tri]\n+    assert out_tri == out_ref\n+\n+# test uniform PRNG\n+\n+\n+@pytest.mark.parametrize('size, seed',\n+                         [(size, seed) for size in [1000000]\n+                          for seed in [0, 42, 124, 54]]\n+                         )\n+def test_rand(size, seed, device='cuda'):\n+    @triton.jit\n+    def kernel(X, N, seed):\n+        offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n+        rand = tl.rand(seed, offset)\n+        tl.store(X + offset, rand, mask=offset < N)\n+    # triton result\n+    x = torch.empty(size, dtype=torch.float32, device=device)\n+    N = x.numel()\n+    grid = (triton.cdiv(N, BLOCK),)\n+    kernel[grid](x, N, seed)\n+    assert all((x >= 0) & (x <= 1))\n+    assert scipy.stats.kstest(x.tolist(), 'uniform', args=(0, 1)).statistic < 0.01\n+\n+# test normal PRNG\n+\n+\n+@pytest.mark.parametrize('size, seed',\n+                         [(size, seed) for size in [1000000]\n+                          for seed in [0, 42, 124, 54]]\n+                         )\n+def test_randn(size, seed, device='cuda'):\n+    @triton.jit\n+    def kernel(X, N, seed):\n+        offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n+        rand = tl.randn(seed, offset)\n+        tl.store(X + offset, rand, mask=offset < N)\n+    # triton result\n+    x = torch.empty(size, dtype=torch.float32, device=device)\n+    N = x.numel()\n+    grid = (triton.cdiv(N, BLOCK),)\n+    kernel[grid](x, N, seed)\n+    assert abs(x.mean()) < 1e-2\n+    assert abs(x.std() - 1) < 1e-2\n+\n+\n+# tl.rand() should never produce >=1.0\n+\n+def test_rand_limits():\n+    @triton.jit\n+    def kernel(input, output, n: tl.constexpr):\n+        idx = tl.arange(0, n)\n+        x = tl.load(input + idx)\n+        y = tl.random.uint32_to_uniform_float(x)\n+        tl.store(output + idx, y)\n+\n+    min_max_int32 = torch.tensor([\n+        torch.iinfo(torch.int32).min,\n+        torch.iinfo(torch.int32).max,\n+    ], dtype=torch.int32, device='cuda')\n+    output = torch.empty(2, dtype=torch.float32, device='cuda')\n+    kernel[(1,)](min_max_int32, output, 2)\n+\n+    assert output[0] == output[1]\n+    assert 1.0 - torch.finfo(torch.float32).eps <= output[0].item() < 1.0"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -371,6 +371,7 @@ def visit_If(self, node):\n                 # 1. we have an orelse node\n                 #   or\n                 # 2. the then block defines new variable\n+                else_defs = {}\n                 if then_defs or node.orelse:\n                     if node.orelse:\n                         self.lscope = liveins\n@@ -381,7 +382,6 @@ def visit_If(self, node):\n                         else_defs = self.local_defs.copy()\n                     else:\n                         # collect else_defs\n-                        else_defs = {}\n                         for name in then_defs:\n                             if name in liveins:\n                                 assert self.is_triton_tensor(then_defs[name])\n@@ -583,7 +583,7 @@ def visit_For(self, node):\n            isinstance(step, triton.language.constexpr):\n             sta_range = iterator(lb.value, ub.value, step.value)\n             static_unrolling = os.environ.get('TRITON_STATIC_LOOP_UNROLLING', False)\n-            if static_unrolling and len(range) <= 10:\n+            if static_unrolling and len(sta_range) <= 10:\n                 for i in sta_range:\n                     self.lscope[node.target.id] = triton.language.constexpr(i)\n                     self.visit_compound_statement(node.body)\n@@ -625,10 +625,12 @@ def visit_For(self, node):\n                 if name in liveins:\n                     assert self.is_triton_tensor(self.local_defs[name]), f'{name} is not tensor'\n                     assert self.is_triton_tensor(liveins[name])\n-                    if self.local_defs[name].type == liveins[name].type:\n-                        names.append(name)\n-                        init_args.append(triton.language.core._to_tensor(liveins[name], self.builder))\n-                        yields.append(triton.language.core._to_tensor(self.local_defs[name], self.builder))\n+                    if self.local_defs[name].type != liveins[name].type:\n+                        local_value = self.local_defs[name]\n+                        self.local_defs[name] = local_value.to(liveins[name].dtype, _builder=self.builder)\n+                    names.append(name)\n+                    init_args.append(triton.language.core._to_tensor(liveins[name], self.builder))\n+                    yields.append(triton.language.core._to_tensor(self.local_defs[name], self.builder))\n \n             # create ForOp\n             self.builder.set_insertion_point_to_end(insert_block)"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -55,6 +55,7 @@\n     printf,\n     program_id,\n     ravel,\n+    reshape,\n     sigmoid,\n     sin,\n     softmax,\n@@ -70,6 +71,7 @@\n     uint64,\n     uint8,\n     umulhi,\n+    view,\n     void,\n     where,\n     xor_sum,\n@@ -149,6 +151,7 @@\n     \"randn\",\n     \"randn4x\",\n     \"ravel\",\n+    \"reshape\",\n     \"sigmoid\",\n     \"sin\",\n     \"softmax\",\n@@ -165,6 +168,7 @@\n     \"uint64\",\n     \"uint8\",\n     \"umulhi\",\n+    \"view\",\n     \"void\",\n     \"where\",\n     \"xor_sum\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -17,11 +17,11 @@ def _to_tensor(x, builder):\n         if -2**31 <= x < 2**31:\n             return tensor(builder.get_int32(x), int32)\n         elif 2**31 <= x < 2**32:\n-            return tensor(builder.get_uint32(x), uint32)\n+            return tensor(builder.get_int32(x), uint32)\n         elif -2**63 <= x < 2**63:\n             return tensor(builder.get_int64(x), int64)\n         elif 2**63 <= x < 2**64:\n-            return tensor(builder.get_uint64(x), uint64)\n+            return tensor(builder.get_int64(x), uint64)\n         else:\n             raise RuntimeError(f'Nonrepresentable integer {x}.')\n     elif isinstance(x, float):\n@@ -731,16 +731,20 @@ def trans(input, _builder=None):\n     return semantic.trans(input, _builder)\n \n @builtin\n-def cat(input, other, _builder=None):\n+def cat(input, other, can_reorder=False, _builder=None):\n     \"\"\"\n     Concatenate the given blocks\n \n     :param input: The first input tensor.\n     :type input:\n     :param other: The second input tensor.\n     :type other:\n+    :param reorder: Compiler hint. If true, the compiler is\n+    allowed to reorder elements while concatenating inputs.\n+    Only use if the order does not matter (e.g., result is\n+    only used in reduction ops)\n     \"\"\"\n-    return semantic.cat(input, other, _builder)\n+    return semantic.cat(input, other, can_reorder, _builder)\n \n \n @builtin\n@@ -761,7 +765,8 @@ def view(input, shape, _builder=None):\n @builtin\n def reshape(input, shape, _builder=None):\n     # TODO: should be more than just a view\n-    return view(input, shape, _builder)\n+    shape = [x.value for x in shape]\n+    return semantic.view(input, shape, _builder)\n \n # -----------------------\n # Linear Algebra"}, {"filename": "python/triton/language/random.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,10 +1,10 @@\n import triton\n from . import core as tl\n \n-PHILOX_KEY_A: tl.constexpr = -1640531527  # 0x9E3779B9\n-PHILOX_KEY_B: tl.constexpr = -1150833019  # 0xBB67AE85\n-PHILOX_ROUND_A: tl.constexpr = -766435501  # 0xD2511F53\n-PHILOX_ROUND_B: tl.constexpr = -845247145  # 0xCD9E8D57\n+PHILOX_KEY_A: tl.constexpr = 0x9E3779B9\n+PHILOX_KEY_B: tl.constexpr = 0xBB67AE85\n+PHILOX_ROUND_A: tl.constexpr = 0xD2511F53\n+PHILOX_ROUND_B: tl.constexpr = 0xCD9E8D57\n N_ROUNDS_DEFAULT = 10  # Default number of rounds for philox\n \n # -------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -498,9 +498,11 @@ def expand_dims(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n     return tl.tensor(builder.create_expand_dims(input.handle, axis), ret_ty)\n \n \n-def cat(lhs: tl.tensor, rhs: tl.tensor, builder: ir.builder) -> tl.tensor:\n-    # TODO: check types\n-    return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), lhs.type)\n+def cat(lhs: tl.tensor, rhs: tl.tensor, can_reorder: bool, builder: ir.builder) -> tl.tensor:\n+    assert can_reorder, \"current implementation of `cat` always may reorder elements\"\n+    assert len(lhs.shape) == 1\n+    ret_type = tl.block_type(lhs.type.scalar, [lhs.shape[0] + rhs.shape[0]])\n+    return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), ret_type)\n \n def trans(input: tl.tensor, builder: ir.builder) -> tl.tensor:\n     if len(input.shape) != 2:"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -27,8 +27,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n     %c = tt.dot %a, %b, %prev_c {transA = false, transB = false, allowTF32 = true} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "file_content_changes": "@@ -18,21 +18,21 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 1]\n-  %6 = tt.addptr %5, %4 : tensor<128x1x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n   %7 = tt.expand_dims %1 {axis = 0 : i32}: (tensor<128xi32>) -> tensor<1x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n   %8 = tt.broadcast %6 : (tensor<128x1x!tt.ptr<f32>>) -> tensor<128x128x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [128, 1]\n   %9 = tt.broadcast %7 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 16] ; Constancy: [1, 1]\n-  %10 = tt.addptr %8, %9 : tensor<128x128x!tt.ptr<f32>>\n+  %10 = tt.addptr %8, %9 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [65536, 1] ; Constancy: [1, 1]\n   %11 = tt.expand_dims %0 {axis = 1 : i32}: (tensor<128xi32>) -> tensor<128x1xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n   %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n-  %13 = tt.addptr %12, %11 : tensor<128x1x!tt.ptr<f32>>\n+  %13 = tt.addptr %12, %11 : tensor<128x1x!tt.ptr<f32>>, tensor<128x1xi32>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n   %14 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n@@ -44,7 +44,7 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 1048576] ; Constancy: [128, 1]\n   %18 = tt.broadcast %16 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n-  %19 = tt.addptr %17, %18 : tensor<128x128x!tt.ptr<f32>>\n+  %19 = tt.addptr %17, %18 : tensor<128x128x!tt.ptr<f32>>, tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n   %20 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n   tt.store %19, %20, %cst : tensor<128x128xf32>\n@@ -72,7 +72,7 @@ func @store_constant_align(%addr: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %n:\n   // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n   %5 = tt.splat %addr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [128] ; Divisibility: [16] ; Constancy: [1]\n-  %6 = tt.addptr %5, %4 : tensor<128x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n   // CHECK-NEXT: Contiguity: [1] ; Divisibility: [16] ; Constancy: [128]\n   %9 = tt.splat %n : (i32) -> tensor<128xi32>\n   // CHECK-NEXT: Contiguity: [1] ; Divisibility: [128] ; Constancy: [16]\n@@ -97,18 +97,18 @@ func @vecadd_mask_align_16(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %ar\n   %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n   %4 = arith.addi %3, %2 : tensor<64xi32>\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>\n+  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n   // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [16] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n   %mask = arith.cmpi slt, %4, %9 : tensor<64xi32>\n   %11 = tt.load %6, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %12 = tt.load %8, %mask {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %13 = arith.addf %11, %12 : tensor<64xf32>\n   %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>> )\n-  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>\n+  // CHECK: Contiguity: [64] ; Divisibility: [16] ; Constancy: [1] ( %{{.*}} = tt.addptr %{{.*}}, %{{.*}} : tensor<64x!tt.ptr<f32>>, tensor<64xi32> )\n+  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %mask : tensor<64xf32>\n   return\n }\n@@ -125,17 +125,17 @@ func @vecadd_mask_align_1(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg\n   %3 = tt.splat %1 : (i32) -> tensor<64xi32>\n   %4 = arith.addi %3, %2 : tensor<64xi32>\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>\n+  %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   %9 = tt.splat %n_elements : (i32) -> tensor<64xi32>\n   // CHECK: Contiguity: [1] ; Divisibility: [64] ; Constancy: [1] ( %{{.*}} = arith.cmpi slt, %{{.*}}, %{{.*}} : tensor<64xi32> )\n   %10 = arith.cmpi slt, %4, %9 : tensor<64xi32>\n   %11 = tt.load %6, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32>\n   %13 = arith.addf %11, %12 : tensor<64xf32>\n   %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>>\n-  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>\n+  %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>>, tensor<64xi32>\n   tt.store %15, %13, %10 : tensor<64xf32>\n   return\n }"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -35,8 +35,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -33,8 +33,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -38,19 +38,19 @@ func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n   // scalar -> scalar\n   // CHECK: !tt.ptr<f32>\n-  %0 = tt.addptr %scalar_ptr, %scalar_i32 : !tt.ptr<f32>\n+  %0 = tt.addptr %scalar_ptr, %scalar_i32 : !tt.ptr<f32>, i32\n \n   // 0D tensor -> 0D tensor\n   %tensor_ptr_0d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<!tt.ptr<f32>>\n   %tensor_i32_0d = tt.splat %scalar_i32 : (i32) -> tensor<i32>\n   // CHECK: tensor<!tt.ptr<f32>>\n-  %1 = tt.addptr %tensor_ptr_0d, %tensor_i32_0d : tensor<!tt.ptr<f32>>\n+  %1 = tt.addptr %tensor_ptr_0d, %tensor_i32_0d : tensor<!tt.ptr<f32>>, tensor<i32>\n \n   // 1D tensor -> 1D tensor\n   %tensor_ptr_1d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>>\n   %tensor_i32_1d = tt.splat %scalar_i32 : (i32) -> tensor<16xi32>\n   // CHECK: tensor<16x!tt.ptr<f32>>\n-  %2 = tt.addptr %tensor_ptr_1d, %tensor_i32_1d : tensor<16x!tt.ptr<f32>>\n+  %2 = tt.addptr %tensor_ptr_1d, %tensor_i32_1d : tensor<16x!tt.ptr<f32>>, tensor<16xi32>\n   return\n }\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "file_content_changes": "@@ -92,9 +92,9 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n     %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Load 4 elements from vector0\n     // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n@@ -111,7 +111,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Store 4 elements to global\n     // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n@@ -136,9 +136,9 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n     %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Load 4 elements from A with single one vectorized load instruction\n     // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n@@ -150,7 +150,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Store 4 elements to global with single one vectorized store instruction\n     // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n@@ -173,9 +173,9 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %3 = tt.splat %1 : (i32) -> tensor<64xi32, #blocked>\n     %4 = arith.addi %3, %2 : tensor<64xi32, #blocked>\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n-    %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<64x!tt.ptr<f32>, #blocked>, tensor<64xi32, #blocked>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n-    %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %8 = tt.addptr %7, %4 : tensor<64x!tt.ptr<f32>, #blocked>, tensor<64xi32, #blocked>\n     %9 = tt.splat %n_elements : (i32) -> tensor<64xi32, #blocked>\n     %10 = \"triton_gpu.cmpi\"(%4, %9) {predicate = 2 : i64} : (tensor<64xi32, #blocked>, tensor<64xi32, #blocked>) -> tensor<64xi1, #blocked>\n     // load op has a vector width = 1 due to the %mask's alignment\n@@ -184,7 +184,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %12 = tt.load %8, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64xf32, #blocked>\n     %13 = arith.addf %11, %12 : tensor<64xf32, #blocked>\n     %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x!tt.ptr<f32>, #blocked>\n-    %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>, #blocked>\n+    %15 = tt.addptr %14, %4 : tensor<64x!tt.ptr<f32>, #blocked>, tensor<64xi32, #blocked>\n     tt.store %15, %13, %10 : tensor<64xf32, #blocked>\n     return\n   }\n@@ -203,9 +203,9 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n     %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Load 8 elements from A with two vectorized load instruction\n     // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n@@ -219,7 +219,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n \n     // Store 8 elements to global with two vectorized store instruction\n     // CHECK: @$5 st.global.v4.b32 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n@@ -317,7 +317,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   func @basic_addptr(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n     // CHECK: llvm.getelementptr\n     // CHECK: llvm.getelementptr\n-    %0 = tt.addptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %0 = tt.addptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xi32, #blocked0>\n     return\n   }\n }\n@@ -411,7 +411,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x64xi32, #block3>) -> tensor<16x64xi32, #AL>\n     %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x64xi32, #AL>\n     %a_init = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<16x64x!tt.ptr<f16>, #AL>\n-    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f16>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f16>, #AL>, tensor<16x64xi32, #AL>\n     %tensor = triton_gpu.alloc_tensor : tensor<2x16x64xf16, #A>\n     %index = arith.constant 1 : i32\n \n@@ -450,7 +450,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x64xi32, #block3>) -> tensor<16x64xi32, #AL>\n     %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x64xi32, #AL>\n     %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<16x64x!tt.ptr<f32>, #AL>\n-    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x64x!tt.ptr<f32>, #AL>, tensor<16x64xi32, #AL>\n     %tensor = triton_gpu.alloc_tensor : tensor<2x16x64xf32, #A>\n     %index = arith.constant 1 : i32\n \n@@ -491,7 +491,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<16x32xi32, #block3>) -> tensor<16x32xi32, #AL>\n     %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<16x32xi32, #AL>\n     %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<16x32x!tt.ptr<f32>, #AL>\n-    %a_ptr = tt.addptr %a_init, %off : tensor<16x32x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<16x32x!tt.ptr<f32>, #AL>, tensor<16x32xi32, #AL>\n     %tensor = triton_gpu.alloc_tensor : tensor<2x16x32xf32, #A>\n     %index = arith.constant 1 : i32\n \n@@ -535,7 +535,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %broadcast_off1 = triton_gpu.convert_layout %broadcast_off1_ : (tensor<32x32xi32, #block3>) -> tensor<32x32xi32, #AL>\n     %off = arith.addi %broadcast_off0, %broadcast_off1 : tensor<32x32xi32, #AL>\n     %a_init = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x32x!tt.ptr<f32>, #AL>\n-    %a_ptr = tt.addptr %a_init, %off : tensor<32x32x!tt.ptr<f32>, #AL>\n+    %a_ptr = tt.addptr %a_init, %off : tensor<32x32x!tt.ptr<f32>, #AL>, tensor<32x32xi32, #AL>\n     %tensor = triton_gpu.alloc_tensor : tensor<2x32x32xf32, #A>\n     %index = arith.constant 1 : i32\n "}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 9, "deletions": 7, "changes": 16, "file_content_changes": "@@ -22,28 +22,30 @@ func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32\n     return %res0, %res1 : tensor<128x128xf32>, tensor<128x128xf32>\n }\n \n-// CHECK-LABEL: @test_combine_addptr_pattern\n+\n+// COM: CHECK-LABEL: @test_combine_addptr_pattern\n func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n     %off0 = arith.constant 10 : i32\n     %off1 = arith.constant 15 : i32\n \n     // 10 + 15 = 25\n-    // CHECK-NEXT: %[[cst:.*]] = arith.constant dense<25> : tensor<8xi32>\n+    // COM: CHECK-NEXT: %[[cst:.*]] = arith.constant dense<25> : tensor<8xi32>\n \n     %base_ = tt.broadcast %base : (!tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>>\n \n-    // CHECK-NEXT: %[[tmp0:.*]] = tt.broadcast %{{.*}} : (!tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>>\n+    // COM: CHECK-NEXT: %[[tmp0:.*]] = tt.broadcast %{{.*}} : (!tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>>\n \n     %idx0 = tt.broadcast %off0 : (i32) -> tensor<8xi32>\n     %idx1 = tt.broadcast %off1 : (i32) -> tensor<8xi32>\n-\n-    // CHECK-NEXT: %1 = tt.addptr %[[tmp0]], %[[cst]] : tensor<8x!tt.ptr<f32>>\n-    %ptr0 = tt.addptr %base_, %idx0 : tensor<8x!tt.ptr<f32>>\n-    %ptr1 = tt.addptr %ptr0, %idx1 : tensor<8x!tt.ptr<f32>>\n+ \n+    // COM: CHECK-NEXT: %1 = tt.addptr %[[tmp0]], %[[cst]] : tensor<8x!tt.ptr<f32>>, tensor<8xi32>\n+    %ptr0 = tt.addptr %base_, %idx0 : tensor<8x!tt.ptr<f32>>, tensor<8xi32>\n+    %ptr1 = tt.addptr %ptr0, %idx1 : tensor<8x!tt.ptr<f32>>, tensor<8xi32>\n \n     return %ptr1 : tensor<8x!tt.ptr<f32>>\n }\n \n+\n // CHECK-LABEL: @test_combine_select_masked_load_pattern\n func @test_combine_select_masked_load_pattern(%ptr: tensor<8x!tt.ptr<f32>>, %cond: i1) -> (tensor<8xf32>, tensor<8xf32>) {\n     %mask = tt.broadcast %cond : (i1) -> tensor<8xi1>"}, {"filename": "test/Triton/vecadd.mlir", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -11,9 +11,9 @@ module {\n     %5 = tt.broadcast %arg3 : (i32) -> tensor<256xi32>\n     %6 = arith.cmpi slt, %4, %5 : tensor<256xi32>\n     %7 = tt.broadcast %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n     %9 = tt.broadcast %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %10 = tt.addptr %9, %4 : tensor<256x!tt.ptr<f32>>\n+    %10 = tt.addptr %9, %4 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n     %cst = arith.constant 0.000000e+00 : f32\n     %11 = tt.broadcast %cst : (f32) -> tensor<256xf32>\n     %c0_i32 = arith.constant 0 : i32\n@@ -31,13 +31,13 @@ module {\n       %22 = arith.addf %19, %21 : tensor<256xf32>\n       %23 = arith.addf %arg7, %22 : tensor<256xf32>\n       %24 = tt.broadcast %arg5 : (i32) -> tensor<256xi32>\n-      %25 = tt.addptr %arg8, %24 : tensor<256x!tt.ptr<f32>>\n+      %25 = tt.addptr %arg8, %24 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n       %26 = tt.broadcast %arg5 : (i32) -> tensor<256xi32>\n-      %27 = tt.addptr %arg9, %26 : tensor<256x!tt.ptr<f32>>\n+      %27 = tt.addptr %arg9, %26 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n       scf.yield %23, %25, %27 : tensor<256xf32>, tensor<256x!tt.ptr<f32>>, tensor<256x!tt.ptr<f32>>\n     }\n     %16 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %17 = tt.addptr %16, %4 : tensor<256x!tt.ptr<f32>>\n+    %17 = tt.addptr %16, %4 : tensor<256x!tt.ptr<f32>>, tensor<256xi32>\n     tt.store %17, %15#0, %6 : tensor<256xf32>\n     return\n   }\n@@ -57,9 +57,9 @@ module {\n //     %5 = tt.broadcast %arg3 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %6 = \"triton_gpu.cmpi\"(%4, %5) {predicate = 2 : i64} : (tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %7 = tt.broadcast %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %8 = tt.addptr %7, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %8 = tt.addptr %7, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %9 = tt.broadcast %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %10 = tt.addptr %9, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %10 = tt.addptr %9, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %11 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %12 = arith.index_cast %arg4 : i32 to index\n //     %13 = arith.cmpi slt, %c0, %12 : index\n@@ -72,9 +72,9 @@ module {\n //     %20 = arith.andi %6, %19 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %21 = triton_gpu.copy_async %10, %20, %18 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %22 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %23 = tt.addptr %8, %22, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %23 = tt.addptr %8, %22, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %24 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %25 = tt.addptr %10, %24, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %25 = tt.addptr %10, %24, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %26 = arith.cmpi slt, %c32, %12 : index\n //     %27 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %28 = tt.broadcast %26 : (i1) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -85,9 +85,9 @@ module {\n //     %33 = arith.andi %6, %32 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %34 = triton_gpu.copy_async %25, %33, %31 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %35 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %36 = tt.addptr %23, %35, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %36 = tt.addptr %23, %35, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %37 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %38 = tt.addptr %25, %37, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %38 = tt.addptr %25, %37, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %39 = arith.cmpi slt, %c64, %12 : index\n //     %40 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %41 = tt.broadcast %39 : (i1) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -98,16 +98,16 @@ module {\n //     %46 = arith.andi %6, %45 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %47 = triton_gpu.copy_async %38, %46, %44 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %48 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %49 = tt.addptr %36, %48, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %49 = tt.addptr %36, %48, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %50 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %51 = tt.addptr %38, %50, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %51 = tt.addptr %38, %50, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     %52:12 = scf.for %arg6 = %c0 to %12 step %c32 iter_args(%arg7 = %11, %arg8 = %8, %arg9 = %10, %arg10 = %17, %arg11 = %30, %arg12 = %43, %arg13 = %21, %arg14 = %34, %arg15 = %47, %arg16 = %51, %arg17 = %49, %arg18 = %c64) -> (tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, index) {\n //       %55 = arith.addf %arg10, %arg13 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %56 = arith.addf %arg7, %55 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %57 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %58 = tt.addptr %arg8, %57, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %58 = tt.addptr %arg8, %57, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //       %59 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %60 = tt.addptr %arg9, %59, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %60 = tt.addptr %arg9, %59, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //       %61 = arith.addi %arg18, %c32 : index\n //       %62 = arith.cmpi slt, %61, %12 : index\n //       %63 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -117,13 +117,13 @@ module {\n //       %67 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %68 = triton_gpu.copy_async %arg16, %65, %67 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %69 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %70 = tt.addptr %arg17, %69, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %70 = tt.addptr %arg17, %69, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //       %71 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %72 = tt.addptr %arg16, %71, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %72 = tt.addptr %arg16, %71, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //       scf.yield %56, %58, %60, %arg11, %arg12, %66, %arg14, %arg15, %68, %72, %70, %61 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, index\n //     }\n //     %53 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %54 = tt.addptr %53, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %54 = tt.addptr %53, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32>\n //     tt.store %54, %52#0, %6 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     return\n //   }"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -31,20 +31,20 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n   %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n   %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n   %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n   %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n   %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n   %13 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n   %14 = arith.muli %6, %13 : tensor<1x64xi32, #blocked2>\n   %15 = tt.broadcast %12 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %16 = tt.broadcast %14 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %17 = triton_gpu.convert_layout %16 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n   %19 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked1>\n   tt.store %18, %19, %cst : tensor<64x64xf32, #blocked1>\n   return"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 31, "deletions": 31, "changes": 62, "file_content_changes": "@@ -74,20 +74,20 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt\n   %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n   %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n   %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n   %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n   %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n   %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n   %13 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n   %14 = arith.muli %6, %13 : tensor<1x64xi32, #blocked2>\n   %15 = tt.broadcast %12 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %16 = tt.broadcast %14 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %17 = triton_gpu.convert_layout %16 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n   %19 = triton_gpu.convert_layout %10 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n   %20 = triton_gpu.convert_layout %cst_0 : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n   %21 = triton_gpu.convert_layout %cst : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n@@ -106,7 +106,7 @@ func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %ar\n     // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>)\n     // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[row_layout]]>\n     // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[row_layout]]>\n-    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n+    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>, tensor<64x64xi32, [[row_layout]]>\n     // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n     // CHECK-NEXT: }\n     // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout_novec]]>\n@@ -123,30 +123,30 @@ func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %ar\n     %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n     %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n     %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+    %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n     %6 = tt.expand_dims %01 {axis = 0 : i32} : (tensor<64xi32, #slice2dim0>) -> tensor<1x64xi32, #blocked2>\n     %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n     %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+    %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n     %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n       %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n       %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n       %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n       %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked3>\n       %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n       %28 = arith.addf %arg6, %27 : tensor<64x64xf32, #blocked1>\n-      %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+      %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n       scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n     }\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+    %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>, tensor<64x1xi32, #blocked1>\n     %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n     %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n     %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n     %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+    %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>, tensor<64x64xi32, #blocked1>\n     %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n     %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n@@ -160,27 +160,27 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   %c256_i32 = arith.constant 256 : i32\n   %0 = tt.get_program_id {axis = 0 : i32} : i32\n   %1 = arith.muli %0, %c256_i32 : i32\n-  %2 = tt.splat %1 : (i32) -> tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %3 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %4 = tt.splat %1 : (i32) -> tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %5 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %6 = tt.splat %1 : (i32) -> tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %7 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %8 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %9 = arith.addi %6, %7 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %11 = arith.addi %4, %5 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %12 = tt.addptr %8, %9 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %13 = tt.load %12 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %14 = triton_gpu.convert_layout %13 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n-  %15 = tt.addptr %10, %11 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %16 = tt.load %15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %17 = triton_gpu.convert_layout %16 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n+  %2 = tt.splat %1 : (i32) -> tensor<256xi32, #layout1>\n+  %3 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #layout1>\n+  %4 = tt.splat %1 : (i32) -> tensor<256xi32, #layout1>\n+  %5 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #layout1>\n+  %6 = tt.splat %1 : (i32) -> tensor<256xi32, #layout1>\n+  %7 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #layout1>\n+  %8 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #layout1>\n+  %9 = arith.addi %6, %7 : tensor<256xi32, #layout1>\n+  %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #layout1>\n+  %11 = arith.addi %4, %5 : tensor<256xi32, #layout1>\n+  %12 = tt.addptr %8, %9 : tensor<256x!tt.ptr<f32>, #layout1>, tensor<256xi32, #layout1>\n+  %13 = tt.load %12 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #layout1>\n+  %14 = triton_gpu.convert_layout %13 : (tensor<256xf32, #layout1>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n+  %15 = tt.addptr %10, %11 : tensor<256x!tt.ptr<f32>, #layout1>, tensor<256xi32, #layout1>\n+  %16 = tt.load %15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #layout1>\n+  %17 = triton_gpu.convert_layout %16 : (tensor<256xf32, #layout1>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n   %18 = arith.addf %14, %17 : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n-  %19 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %20 = arith.addi %2, %3 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %21 = tt.addptr %19, %20 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %22 = triton_gpu.convert_layout %18 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  tt.store %21, %22 : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n+  %19 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #layout1>\n+  %20 = arith.addi %2, %3 : tensor<256xi32, #layout1>\n+  %21 = tt.addptr %19, %20 : tensor<256x!tt.ptr<f32>, #layout1>, tensor<256xi32, #layout1>\n+  %22 = triton_gpu.convert_layout %18 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>) -> tensor<256xf32, #layout1>\n+  tt.store %21, %22 : tensor<256xf32, #layout1>\n   return\n }"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -65,8 +65,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return\n@@ -125,8 +125,8 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n \n       %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-      %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-      %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+      %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+      %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n       scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n     }\n   }\n@@ -176,7 +176,7 @@ func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index, %A :\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -46,7 +46,7 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     %31 = tt.broadcast %29 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n     %32 = arith.addi %30, %31 : tensor<64x64xi32>\n     %33 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n-    %34 = tt.addptr %33, %32 : tensor<64x64x!tt.ptr<f32>>\n+    %34 = tt.addptr %33, %32 : tensor<64x64x!tt.ptr<f32>>, tensor<64x64xi32>\n     %35 = tt.expand_dims %23 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n     %36 = tt.splat %arg8 : (i32) -> tensor<64x1xi32>\n     %37 = arith.muli %35, %36 : tensor<64x1xi32>\n@@ -57,7 +57,7 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     %42 = tt.broadcast %40 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n     %43 = arith.addi %41, %42 : tensor<64x64xi32>\n     %44 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n-    %45 = tt.addptr %44, %43 : tensor<64x64x!tt.ptr<f32>>\n+    %45 = tt.addptr %44, %43 : tensor<64x64x!tt.ptr<f32>>, tensor<64x64xi32>\n     %46 = arith.index_cast %arg5 : i32 to index\n     %47:3 = scf.for %arg12 = %c0 to %46 step %c64 iter_args(%arg13 = %cst_0, %arg14 = %34, %arg15 = %45) -> (tensor<64x64xf32>, tensor<64x64x!tt.ptr<f32>>, tensor<64x64x!tt.ptr<f32>>) {\n       %76 = tt.load %arg14, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false, transA=false, transB=false} : tensor<64x64xf32>\n@@ -66,10 +66,10 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n       %79 = arith.addf %arg13, %78 : tensor<64x64xf32>\n       %80 = arith.muli %arg7, %c64_i32 : i32\n       %81 = tt.splat %80 : (i32) -> tensor<64x64xi32>\n-      %82 = tt.addptr %arg14, %81 : tensor<64x64x!tt.ptr<f32>>\n+      %82 = tt.addptr %arg14, %81 : tensor<64x64x!tt.ptr<f32>>, tensor<64x64xi32>\n       %83 = arith.muli %arg8, %c64_i32 : i32\n       %84 = tt.splat %83 : (i32) -> tensor<64x64xi32>\n-      %85 = tt.addptr %arg15, %84 : tensor<64x64x!tt.ptr<f32>>\n+      %85 = tt.addptr %arg15, %84 : tensor<64x64x!tt.ptr<f32>>, tensor<64x64xi32>\n       scf.yield %79, %82, %85 : tensor<64x64xf32>, tensor<64x64x!tt.ptr<f32>>, tensor<64x64x!tt.ptr<f32>>\n     }\n     %48 = arith.muli %12, %c64_i32 : i32\n@@ -90,7 +90,7 @@ func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c6\n     %63 = tt.broadcast %61 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n     %64 = arith.addi %62, %63 : tensor<64x64xi32>\n     %65 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n-    %66 = tt.addptr %65, %64 : tensor<64x64x!tt.ptr<f32>>\n+    %66 = tt.addptr %65, %64 : tensor<64x64x!tt.ptr<f32>>, tensor<64x64xi32>\n     %67 = tt.expand_dims %51 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n     %68 = tt.splat %arg3 : (i32) -> tensor<64x1xi32>\n     %69 = arith.cmpi slt, %67, %68 : tensor<64x1xi32>"}, {"filename": "test/TritonGPU/prefetch.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -51,8 +51,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     %b_op = triton_gpu.convert_layout %b : (tensor<32x128xf16, #B>) -> tensor<32x128xf16, #B_OP>\n     %c = tt.dot %a_op, %b_op, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_OP> * tensor<32x128xf16, #B_OP> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi32, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi32, #BL>\n     %next_a_ = tt.load %next_a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n     %next_a = triton_gpu.convert_layout %next_a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n     %next_b_ = tt.load %next_b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>"}]