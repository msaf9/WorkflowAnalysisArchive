[{"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -34,6 +34,7 @@ Shape Manipulation Ops\n     :nosignatures:\n \n     broadcast_to\n+    expand_dims\n     reshape\n     ravel\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -107,14 +107,19 @@ struct ConvertLayoutOpConversion\n       unsigned dim = sliceLayout.getDim();\n       auto parentEncoding = sliceLayout.getParent();\n       auto parentSizePerThread = getSizePerThread(parentEncoding);\n-      unsigned stride = 1;\n-      if (getOrder(parentEncoding)[0] == dim)\n-        stride = parentSizePerThread[dim];\n       auto parentShape = sliceLayout.paddedShape(shape);\n       auto parentTy = RankedTensorType::get(parentShape, type.getElementType(),\n                                             parentEncoding);\n+      auto offsets = emitOffsetForLayout(layout, type);\n+      auto parentOffset = emitOffsetForLayout(parentEncoding, parentTy);\n+      SmallVector<int> idxs;\n+      for (SmallVector<unsigned> off : offsets) {\n+        off.insert(off.begin() + dim, 0);\n+        auto it = std::find(parentOffset.begin(), parentOffset.end(), off);\n+        idxs.push_back(std::distance(parentOffset.begin(), it));\n+      }\n       auto multiDimOffsetParent = getMultiDimOffset(\n-          parentEncoding, loc, rewriter, elemId * stride, parentTy,\n+          parentEncoding, loc, rewriter, idxs[elemId], parentTy,\n           sliceLayout.paddedShape(multiDimCTAInRepId),\n           sliceLayout.paddedShape(shapePerCTA));\n       SmallVector<Value> multiDimOffset(rank);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 8, "changes": 9, "file_content_changes": "@@ -298,14 +298,7 @@ struct StoreOpConversion\n       vec = std::min(vec, maskAlign);\n     }\n \n-    // numElements = 1 for scalar\n-    auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n-    auto numElems = tensorTy ? tensorTy.getNumElements() : 1;\n-    Value mask = int_val(1, 1);\n-    auto tid = tid_val();\n-    mask = and_(mask,\n-                icmp_slt(mul(tid, i32_val(elemsPerThread)), i32_val(numElems)));\n-\n+    Value mask = getMask(valueTy, rewriter, loc);\n     const size_t dtsize =\n         std::max<int>(1, valueElemTy.getIntOrFloatBitWidth() / 8);\n     const size_t valueElemNBits = dtsize * 8;"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 40, "deletions": 0, "changes": 40, "file_content_changes": "@@ -421,6 +421,46 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   // -----------------------------------------------------------------------\n   // Utilities\n   // -----------------------------------------------------------------------\n+  Value getMask(Type valueTy, ConversionPatternRewriter &rewriter,\n+                Location loc) const {\n+    auto tensorTy = valueTy.dyn_cast<RankedTensorType>();\n+    Value mask = int_val(1, 1);\n+    auto tid = tid_val();\n+    if (tensorTy) {\n+      auto layout = tensorTy.getEncoding();\n+      auto shape = tensorTy.getShape();\n+      unsigned rank = shape.size();\n+      auto sizePerThread = triton::gpu::getSizePerThread(layout);\n+      auto threadsPerWarp = triton::gpu::getThreadsPerWarp(layout);\n+      auto warpsPerCTA = triton::gpu::getWarpsPerCTA(layout);\n+      auto order = triton::gpu::getOrder(layout);\n+      auto shapePerCTA = triton::gpu::getShapePerCTA(layout, shape);\n+      Value warpSize = i32_val(32);\n+      Value laneId = urem(tid, warpSize);\n+      Value warpId = udiv(tid, warpSize);\n+      SmallVector<Value> multiDimWarpId =\n+          delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+      SmallVector<Value> multiDimThreadId =\n+          delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+      for (unsigned dim = 0; dim < rank; ++dim) {\n+        // if there is no data replication across threads on this dimension\n+        if (shape[dim] >= shapePerCTA[dim])\n+          continue;\n+        // Otherwise, we need to mask threads that will replicate data on this\n+        // dimension. Calculate the thread index on this dimension for the CTA\n+        Value threadDim =\n+            add(mul(multiDimWarpId[dim], i32_val(threadsPerWarp[dim])),\n+                multiDimThreadId[dim]);\n+        mask = and_(mask, icmp_slt(mul(threadDim, i32_val(sizePerThread[dim])),\n+                                   i32_val(shape[dim])));\n+      }\n+    } else {\n+      // If the tensor is not ranked, then it is a scalar and only thread 0 can\n+      // write\n+      mask = and_(mask, icmp_slt(tid, i32_val(1)));\n+    }\n+    return mask;\n+  }\n \n   // Convert an \\param index to a multi-dim coordinate given \\param shape and\n   // \\param order."}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -579,7 +579,12 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     // we replace the use new load use with a convert layout\n     size_t i = std::distance(loads.begin(), it);\n     auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n-    auto cvtDstEnc = cvtDstTy.getEncoding().cast<ttg::DotOperandEncodingAttr>();\n+    auto cvtDstEnc =\n+        cvtDstTy.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n+    if (!cvtDstEnc) {\n+      builder.clone(op, mapping);\n+      continue;\n+    }\n     auto newDstTy = RankedTensorType::get(\n         cvtDstTy.getShape(), cvtDstTy.getElementType(),\n         ttg::DotOperandEncodingAttr::get("}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -368,7 +368,7 @@ class RematerializeForward : public mlir::RewritePattern {\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n           !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp>(op)) {\n+          !isa<triton::StoreOp>(op) && !isa<triton::ReduceOp>(op)) {\n         return failure();\n       }\n       // don't rematerialize if it adds an extra conversion that can't\n@@ -378,9 +378,10 @@ class RematerializeForward : public mlir::RewritePattern {\n         SetVector<Operation *> processed;\n         SetVector<Attribute> layout;\n         llvm::MapVector<Value, Attribute> toConvert;\n-        if (argOp && (argOp != cvt) && cvtSlices.count(argOp) == 0 &&\n-            simulateBackwardRematerialization(argOp, processed, layout,\n-                                              toConvert, srcEncoding) > 0) {\n+        int numAddedConvs = simulateBackwardRematerialization(\n+            argOp, processed, layout, toConvert, srcEncoding);\n+        if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n+            cvtSlices.count(argOp) == 0 && numAddedConvs > 0) {\n           return failure();\n         }\n       }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 9, "deletions": 28, "changes": 37, "file_content_changes": "@@ -89,11 +89,11 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n }\n \n bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n-  // Case 1a: A size 1 tensor is not expensive since all threads will load the\n+  // Case 1: A size 1 tensor is not expensive since all threads will load the\n   // same\n   if (isSingleValue(op->getOperand(0)))\n     return false;\n-  // Case 1b: Tensor of pointers has more threads than elements\n+  // Case 2: Tensor of pointers has more threads than elements\n   // we can presume a high hit-rate that makes it cheap to load\n   auto ptrType = op->getOperand(0).getType().cast<RankedTensorType>();\n   IntegerAttr numWarps =\n@@ -104,28 +104,6 @@ bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n     if (ptrType.getNumElements() < numWarps.getInt() * 32)\n       return false;\n   }\n-  // auto ptr = op->getOperand(0);\n-  //// Case 2: We assume that `evict_last` loads/stores have high hit rate\n-  // if (auto load = dyn_cast<triton::LoadOp>(op))\n-  //   if (load.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-  //     return false;\n-  // if (auto store = dyn_cast<triton::StoreOp>(op))\n-  //   if (store.getEvict() == triton::EvictionPolicy::EVICT_LAST)\n-  //     return false;\n-  // if (auto tensorTy = ptr.getType().dyn_cast<RankedTensorType>()) {\n-  //   auto encoding = tensorTy.getEncoding();\n-  //   // Case 3: Different type conversion is expensive (e.g., mma <->\n-  //   block) if (encoding.getTypeID() != targetEncoding.getTypeID())\n-  //     return true;\n-  //   auto sizePerThread = triton::gpu::getSizePerThread(encoding);\n-  //   auto targetSizePerThread =\n-  //   triton::gpu::getSizePerThread(targetEncoding); auto order =\n-  //   triton::gpu::getOrder(encoding); auto targetOrder =\n-  //   triton::gpu::getOrder(targetEncoding);\n-  //   // Case 4: The targeEncoding may expose more vectorization\n-  //   opportunities return sizePerThread[order[0]] >=\n-  //   targetSizePerThread[targetOrder[0]];\n-  // }\n   return true;\n }\n \n@@ -144,6 +122,12 @@ bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   return false;\n }\n \n+bool canFoldConversion(Operation *op) {\n+  return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n+             triton::MakeRangeOp, triton::SplatOp, triton::ViewOp,\n+             triton::CatOp>(*op);\n+}\n+\n int simulateBackwardRematerialization(\n     Operation *initOp, SetVector<Operation *> &processed,\n     SetVector<Attribute> &layout, llvm::MapVector<Value, Attribute> &toConvert,\n@@ -189,10 +173,7 @@ int simulateBackwardRematerialization(\n         continue;\n       // If the conversion can be folded into opArgI then\n       // we don't count this conversion as expensive\n-      if (isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-              triton::MakeRangeOp, triton::SplatOp>(*opArgI))\n-        continue;\n-      if (isa<triton::ViewOp, triton::CatOp>(opArgI))\n+      if (canFoldConversion(opArgI))\n         continue;\n \n       // We add one expensive conversion for the current operand"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 127, "deletions": 0, "changes": 127, "file_content_changes": "@@ -457,6 +457,86 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n     assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n \n \n+# ----------------\n+# test expand_dims\n+# ----------------\n+def test_expand_dims():\n+    @triton.jit\n+    def expand_dims_kernel(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, 0)\n+        tl.static_assert(t.shape == [1, N])\n+\n+        t = tl.expand_dims(offset1, 1)\n+        tl.static_assert(t.shape == [N, 1])\n+\n+        t = tl.expand_dims(offset1, -1)\n+        tl.static_assert(t.shape == [N, 1])\n+\n+        t = tl.expand_dims(offset1, -2)\n+        tl.static_assert(t.shape == [1, N])\n+\n+        t = tl.expand_dims(offset1, (0, -1))\n+        tl.static_assert(t.shape == [1, N, 1])\n+\n+        t = tl.expand_dims(offset1, (0, 1, 3))\n+        tl.static_assert(t.shape == [1, 1, N, 1])\n+\n+        t = tl.expand_dims(offset1, (-4, 2, -1))\n+        tl.static_assert(t.shape == [1, N, 1, 1])\n+\n+        t = tl.expand_dims(offset1, (3, 1, 2))\n+        tl.static_assert(t.shape == [N, 1, 1, 1])\n+\n+    N = 32\n+    dummy_tensor = torch.empty((), device=\"cuda\")\n+    expand_dims_kernel[(1,)](dummy_tensor, N)\n+\n+\n+def test_expand_dims_error_cases():\n+    @triton.jit\n+    def dim_out_of_range1(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, -2)\n+        t = tl.expand_dims(offset1, -3)\n+\n+    @triton.jit\n+    def dim_out_of_range2(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, 1)\n+        t = tl.expand_dims(offset1, 2)\n+\n+    @triton.jit\n+    def duplicate_dim1(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, (0, 0))\n+\n+    @triton.jit\n+    def duplicate_dim2(dummy, N: tl.constexpr):\n+        offset1 = tl.arange(0, N)\n+\n+        t = tl.expand_dims(offset1, (0, -3))\n+\n+    N = 32\n+    dummy_tensor = torch.empty((), device=\"cuda\")\n+\n+    with pytest.raises(triton.CompilationError, match=\"invalid axis -3\"):\n+        dim_out_of_range1[(1,)](dummy_tensor, N)\n+\n+    with pytest.raises(triton.CompilationError, match=\"invalid axis 2\"):\n+        dim_out_of_range2[(1,)](dummy_tensor, N)\n+\n+    with pytest.raises(triton.CompilationError, match=r\"duplicate axes, normalized axes = \\[0, 0\\]\"):\n+        duplicate_dim1[(1,)](dummy_tensor, N)\n+\n+    with pytest.raises(triton.CompilationError, match=r\"duplicate axes, normalized axes = \\[0, 0\\]\"):\n+        duplicate_dim2[(1,)](dummy_tensor, N)\n+\n+\n # ---------------\n # test where\n # ---------------\n@@ -1420,6 +1500,53 @@ def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n+layouts = [\n+    BlockedLayout([1, 4], [1, 32], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [1, 32], [2, 2], [1, 0]),\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1])\n+]\n+\n+\n+@pytest.mark.parametrize(\"M\", [32, 64, 128, 256])\n+@pytest.mark.parametrize(\"src_layout\", layouts)\n+def test_store_op(M, src_layout, device='cuda'):\n+    ir = f\"\"\"\n+    #src = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n+        tt.func public @kernel(%arg0: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<f32> {{tt.divisibility = 16 : i32}}) {{\n+            %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<{M}x!tt.ptr<f32>, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %2 = tt.addptr %1, %0 : tensor<{M}x!tt.ptr<f32>, #triton_gpu.slice<{{dim = 1, parent = #src}}>>, tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %3 = tt.load %2 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}xf32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %4 = tt.expand_dims %3 {{axis = 1 : i32}} : (tensor<{M}xf32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xf32, #src>\n+            %5 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+            %6 = tt.expand_dims %5 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+            %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<{M}x1x!tt.ptr<f32>, #src>\n+            %8 = tt.addptr %7, %6 : tensor<{M}x1x!tt.ptr<f32>, #src>, tensor<{M}x1xi32, #src>\n+            tt.store %8, %4 : tensor<{M}x1xf32, #src>\n+            tt.return\n+        }}\n+    }}\n+    \"\"\"\n+\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        store_kernel = triton.compile(f.name)\n+\n+    rs = RandomState(17)\n+    x = rs.randint(0, 4, (M, 1)).astype('float32')\n+    y = np.zeros((M, 1), dtype='float32')\n+    x_tri = torch.tensor(x, device=device)\n+    y_tri = torch.tensor(y, device=device)\n+\n+    pgm = store_kernel[(1, 1, 1)](x_tri, y_tri)\n+    y_ref = x\n+\n+    np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n @triton.jit\n def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n     delta = mean_2 - mean_1"}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -4,10 +4,6 @@\n # ---------------------------------------\n # Note: import order is significant here.\n \n-# TODO: torch needs to be imported first\n-# or pybind11 shows `munmap_chunk(): invalid pointer`\n-import torch  # noqa: F401\n-\n # submodules\n from .runtime import (\n     autotune,"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -11,8 +11,6 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-import torch\n-\n import triton\n import triton._C.libtriton.triton as _triton\n from ..runtime import driver\n@@ -324,6 +322,10 @@ def _is_cuda(arch):\n \n \n def get_architecture_descriptor(capability):\n+    try:\n+        import torch\n+    except ImportError:\n+        raise ImportError(\"Triton requires PyTorch to be installed\")\n     if capability is None:\n         if torch.version.hip is None:\n             device = triton.runtime.jit.get_current_device()"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -39,6 +39,7 @@\n     dot,\n     dtype,\n     exp,\n+    expand_dims,\n     full,\n     fdiv,\n     float16,\n@@ -130,6 +131,7 @@\n     \"dot\",\n     \"dtype\",\n     \"exp\",\n+    \"expand_dims\",\n     \"extra\",\n     \"fdiv\",\n     \"float16\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 39, "deletions": 4, "changes": 43, "file_content_changes": "@@ -3,7 +3,7 @@\n from contextlib import contextmanager\n from enum import Enum\n from functools import wraps\n-from typing import Callable, List, TypeVar\n+from typing import Callable, List, Sequence, TypeVar\n \n import triton\n from . import semantic\n@@ -883,6 +883,41 @@ def reshape(input, shape, _builder=None):\n     shape = _shape_check_impl(shape)\n     return semantic.reshape(input, shape, _builder)\n \n+\n+def _wrap_axis(axis, ndim):\n+    if not (-ndim <= axis < ndim):\n+        raise ValueError(f\"invalid axis {axis}. Expected {-ndim} <= axis < {ndim}\")\n+\n+    return axis if axis >= 0 else axis + ndim\n+\n+\n+@builtin\n+def expand_dims(input, axis, _builder=None):\n+    \"\"\"\n+    Expand the shape of a tensor, by inserting new length-1 dimensions.\n+\n+    Axis indices are with respect to the resulting tensor, so\n+    ``result.shape[axis]`` will be 1 for each axis.\n+\n+    :param input: The input tensor.\n+    :type input: tl.tensor\n+    :param axis: The indices to add new axes\n+    :type axis: int | Sequence[int]\n+\n+    \"\"\"\n+    axis = _constexpr_to_value(axis)\n+    axes = list(axis) if isinstance(axis, Sequence) else [axis]\n+    new_ndim = len(input.shape) + len(axes)\n+    axes = [_wrap_axis(_constexpr_to_value(d), new_ndim) for d in axes]\n+\n+    if len(set(axes)) != len(axes):\n+        raise ValueError(f\"expand_dims recieved duplicate axes, normalized axes = {axes}\")\n+\n+    ret = input\n+    for a in sorted(axes):\n+        ret = semantic.expand_dims(ret, a, _builder)\n+    return ret\n+\n # -----------------------\n # Linear Algebra\n # -----------------------\n@@ -1281,9 +1316,9 @@ def _argreduce(input, axis, combine_fn, _builder=None, _generator=None):\n \n     if len(input.shape) > 1:\n         # Broadcast index across the non-reduced axes\n-        expand_dims_index = [constexpr(None)] * len(input.shape)\n-        expand_dims_index[axis] = slice(None)\n-        index = index.__getitem__(expand_dims_index, _builder=_builder)\n+        axes_to_expand = [constexpr(d) for d in range(len(input.shape))]\n+        del axes_to_expand[axis]\n+        index = expand_dims(index, axes_to_expand, _builder=_builder)\n         index = broadcast_to(index, input.shape, _builder=_builder)\n \n     rvalue, rindices = reduce((input, index), axis, combine_fn,"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -3,8 +3,6 @@\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n-import torch\n-\n import triton\n from . import core as tl\n from triton._C.libtriton.triton import ir\n@@ -1183,6 +1181,10 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n+    try:\n+        import torch\n+    except ImportError:\n+        raise ImportError(\"Triton requires PyTorch to be installed\")\n     if torch.version.hip is None:\n         device = triton.runtime.jit.get_current_device()\n         capability = triton.runtime.jit.get_device_capability(device)"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1038,7 +1038,6 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: store_f32\n   tt.func @store_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xf32, #blocked0>) {\n-    // CHECK: llvm.icmp \"slt\"\n     // CHECK: llvm.inline_asm\n     // CHECK-SAME: @$2 st.global.b32\n     // CHECK: llvm.inline_asm"}]