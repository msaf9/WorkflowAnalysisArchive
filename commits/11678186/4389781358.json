[{"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 33, "deletions": 81, "changes": 114, "file_content_changes": "@@ -195,15 +195,13 @@ struct FpToFpOpConversion\n                            const Value &v3) {\n     auto *ptxAsm = \"{                                      \\n\"\n                    \".reg .b32 a<2>, b<2>;                  \\n\"\n-                   \"shl.b32 a0, $1, 1;                     \\n\"\n-                   \"shl.b32 a1, $2, 1;                     \\n\"\n-                   \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n-                   \"add.u32 a0, a0, 0x00800080;            \\n\"\n-                   \"add.u32 a1, a1, 0x00800080;            \\n\"\n-                   \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n-                   \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n-                   \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+                   \"shl.b32  b0, $1, 1;                    \\n\"\n+                   \"shl.b32  b1, $2, 1;                    \\n\"\n+                   \"add.u32   b0, b0, 128;                  \\n\"\n+                   \"add.u32   b1, b1, 128;                  \\n\"\n+                   \"lop3.b32  b0, b0, 0x80008000, $1, 0xf8;\\n\"\n+                   \"lop3.b32  b1, b1, 0x80008000, $2, 0xf8;\\n\"\n+                   \"prmt.b32  $0, b0, b1, 0x7531;          \\n\"\n                    \"}\";\n     return convertFp16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   }\n@@ -223,64 +221,25 @@ struct FpToFpOpConversion\n   /* ------------------ */\n \n   static SmallVector<Value>\n-  convertFp32x4ToFp8x4(Location loc, ConversionPatternRewriter &rewriter,\n-                       const char *ptxAsm, const Value &v0, const Value &v1,\n-                       const Value &v2, const Value &v3) {\n-    PTXBuilder builder;\n-    auto &ptxOp = *builder.create(ptxAsm);\n-\n-    auto *o = builder.newOperand(\"=r\");\n-    auto *i0 = builder.newOperand(v0, \"r\");\n-    auto *i1 = builder.newOperand(v1, \"r\");\n-    auto *i2 = builder.newOperand(v2, \"r\");\n-    auto *i3 = builder.newOperand(v3, \"r\");\n-    ptxOp({o, i0, i1, i2, i3}, /*onlyAttachMLIRArgs=*/true);\n-\n-    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n-    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n-    return {extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n-            extract_element(i8_ty, fp8x4Vec, i32_val(3))};\n-  }\n-\n-  static SmallVector<Value>\n-  convertFp32x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+  convertFp32x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    const char *ptxAsm = \"{                    \\n\"\n-                         \".reg .f16 v<4>;              \\n\"\n-                         \".reg .u32 b<4>;              \\n\"\n-                         \"cvt.rn.f16.f32 v0, $1;       \\n\"\n-                         \"cvt.rn.f16.f32 v1, $2;       \\n\"\n-                         \"cvt.rn.f16.f32 v2, $3;       \\n\"\n-                         \"cvt.rn.f16.f32 v3, $4;       \\n\"\n-                         \"mov.b32 b0, {v0, v1};        \\n\"\n-                         \"mov.b32 b1, {v2, v3};        \\n\"\n-                         \"prmt.b32 $0, b0, b1, 0x7531; \\n\"\n-                         \"}\";\n-    return convertFp32x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E4M3x4(loc, rewriter, c0, c1, c2, c3);\n   }\n \n   static SmallVector<Value>\n-  convertFp32x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n+  convertFp32x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    const char *ptxAsm = \"{                                      \\n\"\n-                         \".reg .b32 b<4>;                        \\n\"\n-                         \"shl.b32  b0, $1, 4;                    \\n\"\n-                         \"shl.b32  b1, $2, 4;                    \\n\"\n-                         \"shl.b32  b2, $3, 4;                    \\n\"\n-                         \"shl.b32  b3, $4, 4;                    \\n\"\n-                         \"lop3.b32 b0, b0, 0x80000000, $1, 0xf8; \\n\"\n-                         \"lop3.b32 b1, b1, 0x80000000, $2, 0xf8; \\n\"\n-                         \"lop3.b32 b2, b2, 0x80000000, $3, 0xf8; \\n\"\n-                         \"lop3.b32 b3, b3, 0x80000000, $4, 0xf8; \\n\"\n-                         \"prmt.b32 b0, b0, b1, 0x6273;           \\n\"\n-                         \"prmt.b32 b2, b2, b3, 0x6273;           \\n\"\n-                         \"prmt.b32 %0, b0, b2, 0x5410;           \\n\"\n-                         \"}\";\n-    return convertFp32x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8E5M2x4(loc, rewriter, c0, c1, c2, c3);\n   }\n \n   /* ------------------ */\n@@ -360,20 +319,12 @@ struct FpToFpOpConversion\n     return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   };\n \n-  static SmallVector<Value>\n-  convertBf16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n-                           const Value &v0, const Value &v1, const Value &v2,\n-                           const Value &v3) {\n-    auto newV0 =\n-        rewriter.create<LLVM::FPExtOp>(loc, f32_ty, bitcast(v0, f16_ty));\n-    auto newV1 =\n-        rewriter.create<LLVM::FPExtOp>(loc, f32_ty, bitcast(v1, f16_ty));\n-    auto newV2 =\n-        rewriter.create<LLVM::FPExtOp>(loc, f32_ty, bitcast(v2, f16_ty));\n-    auto newV3 =\n-        rewriter.create<LLVM::FPExtOp>(loc, f32_ty, bitcast(v3, f16_ty));\n-    return convertFp32x4ToFp8E5M2x4(loc, rewriter, newV0, newV1, newV2, newV3);\n-  }\n+  // TODO:\n+  // static SmallVector<Value>\n+  // convertBf16x4ToFp8E5M2x4(Location loc, ConversionPatternRewriter &rewriter,\n+  //                          const Value &v0, const Value &v1, const Value &v2,\n+  //                          const Value &v3) {\n+  // }\n \n   /* ------------------ */\n   // FP8 -> FP32\n@@ -384,10 +335,10 @@ struct FpToFpOpConversion\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n     auto fp16Values = convertFp8E4M3x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n-    return {rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n-            rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])};\n+    return {convertFp16ToFp32(loc, rewriter, fp16Values[0]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[1]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[2]),\n+            convertFp16ToFp32(loc, rewriter, fp16Values[3])};\n   }\n \n   static SmallVector<Value>\n@@ -511,13 +462,14 @@ struct FpToFpOpConversion\n         {{F8E5M2TyID, BF16TyID}, convertFp8E5M2x4ToBf16x4},\n         // BF16 -> F8\n         {{BF16TyID, F8E4M3TyID}, convertBf16x4ToFp8E4M3x4},\n-        {{BF16TyID, F8E5M2TyID}, convertBf16x4ToFp8E5M2x4},\n+        // TODO:\n+        // {{BF16TyID, F8E5M2TyID}, convertBf16x4ToFp8E5M2x4},\n         // F8 -> F32\n         {{F8E4M3TyID, F32TyID}, convertFp8E4M3x4ToFp32x4},\n         {{F8E5M2TyID, F32TyID}, convertFp8E5M2x4ToFp32x4},\n         // F32 -> F8\n-        {{F32TyID, F8E4M3TyID}, convertFp8E4M3x4ToFp32x4},\n-        {{F32TyID, F8E5M2TyID}, convertFp32x4ToFp8E4M3x4},\n+        {{F32TyID, F8E4M3TyID}, convertFp32x4ToFp8E4M3x4},\n+        {{F32TyID, F8E5M2TyID}, convertFp32x4ToFp8E5M2x4},\n     };\n \n     auto convertor ="}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -310,7 +310,6 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n-  // llvm::outs() << module << \"\\n\";\n   auto llvmIR = translateLLVMToLLVMIR(llvmContext, module);\n   if (!llvmIR) {\n     llvm::errs() << \"Translate to LLVM IR failed\";"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -838,7 +838,7 @@ def kernel(in_out_ptr):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16])\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.float32])\n def test_f8_xf16_roundtrip(in_dtype, out_dtype):\n     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n     check_type_supported(out_dtype)\n@@ -852,12 +852,16 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n     f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+    # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n+    all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n+    f8_tensor[all_exp_ones] = 0\n     f8 = triton.reinterpret(f8_tensor, in_dtype)\n     n_elements = f8_tensor.numel()\n     xf16 = torch.empty_like(f8_tensor, dtype=out_dtype)\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n \n+\n     f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n     f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n     copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)"}]