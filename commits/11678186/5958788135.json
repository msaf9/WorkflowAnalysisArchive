[{"filename": "README.md", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -28,13 +28,14 @@ Tentative Agenda for the conference (subject to change):\n |12:30 PM|**Lunch**||\n |1:40 PM |Triton for MTIA|Roman Levenstein et al, (Meta)|\n |2:00 PM |Using Triton IR for high-performance fusions in XLA|George Karpenkov (Google)|\n-|2:20 PM |Triton for All: Triton as a device-independent language|Ian Bearman (Microsoft)|\t\t\n+|2:20 PM |Triton for All: Triton as a device-independent language|Ian Bearman (Microsoft)|\n |2:40 PM|**Break**||\n |3:00 PM|PyTorch 2.0 and TorchInductor|Jason Ansel, Horace He (Meta)|\n |3:20 PM|Pallas: A JAX Kernel Language|Sharad Vikram (Google)|\n |3:40 PM|Writing Grouped GEMMs in Triton|Vinod Grover (Nvidia)|\n |4:00 PM|**Reception**||\n \n+\n # Triton\n \n This is the development repository of Triton, a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs."}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "file_content_changes": "@@ -102,6 +102,7 @@ warpsPerTileV3(tt::DotOp dotOp, const ArrayRef<int64_t> shape, int numWarps,\n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n   mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n+  mutable unsigned mmaV3InstrN{};\n \n   static bool bwdFilter(Operation *op) {\n     return op->getNumOperands() == 1 &&\n@@ -144,6 +145,29 @@ class BlockedToMMA : public mlir::RewritePattern {\n     }\n   }\n \n+  unsigned getMmaV3InstrN(tt::DotOp dotOp, unsigned currN) const {\n+    if (mmaV3InstrN > 0)\n+      return mmaV3InstrN;\n+\n+    mmaV3InstrN = currN;\n+    SetVector<Operation *> slices;\n+    mlir::getForwardSlice(dotOp.getResult(), &slices);\n+    auto iter =\n+        llvm::find_if(slices, [](Operation *op) { return isa<tt::DotOp>(op); });\n+\n+    if (iter != slices.end()) {\n+      tt::DotOp nextDotOp = dyn_cast<tt::DotOp>(*iter);\n+      auto type = nextDotOp.getResult().getType().cast<RankedTensorType>();\n+      auto AType = nextDotOp.getOperand(0).getType().cast<RankedTensorType>();\n+      auto shapePerCTA = ttg::getShapePerCTA(type);\n+      auto instrShape = mmaVersionToInstrShape(3, shapePerCTA, AType);\n+\n+      if (instrShape[1] < currN)\n+        mmaV3InstrN = instrShape[1];\n+    }\n+    return mmaV3InstrN;\n+  }\n+\n   static Value getMMAv3Operand(Value v, mlir::PatternRewriter &rewriter,\n                                int opIdx) {\n     auto cvtOp = dyn_cast_or_null<ttg::ConvertLayoutOp>(v.getDefiningOp());\n@@ -201,6 +225,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n \n     auto instrShape =\n         mmaVersionToInstrShape(versionMajor, retShapePerCTA, AType);\n+    if (versionMajor == 3)\n+      instrShape[1] = getMmaV3InstrN(dotOp, instrShape[1]);\n \n     // operands\n     Value a = dotOp.getA();"}]