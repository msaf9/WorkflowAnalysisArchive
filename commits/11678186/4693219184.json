[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 14, "deletions": 12, "changes": 26, "file_content_changes": "@@ -13,7 +13,7 @@ concurrency:\n   cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}\n \n env:\n-  TRITON_USE_ASSERT_ENABLED_LLVM: 'TRUE'\n+  TRITON_USE_ASSERT_ENABLED_LLVM: \"TRUE\"\n \n jobs:\n   Runner-Preparation:\n@@ -71,25 +71,27 @@ jobs:\n         if: ${{ env.BACKEND != 'ROCM'}}\n         run: |\n           cd python\n+          python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n-          python3 -m pip install -vvv -e '.[tests]'\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Install Triton on ROCM\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n           cd python\n+          python3 -m pip install --upgrade pip\n           python3 -m pip install cmake==3.24\n           python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n-          python3 -m pip install -vvv -e '.[tests]'\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n       - name: Run lit tests\n         if: ${{ env.BACKEND != 'ROCM'}}\n         run: |\n           python3 -m pip install lit\n           cd python\n-          LIT_TEST_DIR=\"build/$(ls build)/test\"\n+          LIT_TEST_DIR=\"build/$(ls build | grep -i temp)/test\"\n           if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n-            echo \"Not found '${LIT_TEST_DIR}'.  Did you change an installation method?\" ; exit -1\n+            echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n           fi\n           lit -v \"${LIT_TEST_DIR}\"\n \n@@ -99,19 +101,19 @@ jobs:\n           cd python/test/unit\n           python3 -m pytest\n \n-      - name: Run python tests on ROCM\n-        if: ${{ env.BACKEND == 'ROCM'}}\n-        run: |\n-          cd python/test/unit/language\n-          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n-\n       - name: Run CXX unittests\n         if: ${{ env.BACKEND != 'ROCM'}}\n         run: |\n           cd python\n-          cd \"build/$(ls build)\"\n+          cd \"build/$(ls build | grep -i temp)\"\n           ctest\n \n+      - name: Run python tests on ROCM\n+        if: ${{ env.BACKEND == 'ROCM'}}\n+        run: |\n+          cd python/test/unit/language\n+          python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n+\n       - name: Regression tests\n         if: ${{ contains(matrix.runner, 'A100') }}\n         run: |"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 22, "deletions": 6, "changes": 28, "file_content_changes": "@@ -12,13 +12,26 @@ namespace mlir {\n \n class ReduceOpHelper {\n public:\n-  explicit ReduceOpHelper(triton::ReduceOp op) : op(op) {\n-    srcTy = op.getOperand().getType().cast<RankedTensorType>();\n+  explicit ReduceOpHelper(triton::ReduceOp rop)\n+      : op(rop.getOperation()), axis(rop.getAxis()) {\n+    auto firstTy = rop.getOperands()[0].getType().cast<RankedTensorType>();\n+    srcShape = firstTy.getShape();\n+    srcEncoding = firstTy.getEncoding();\n+    srcElementTypes = rop.getElementTypes();\n+\n+    for (const auto &t : rop.getInputTypes()) {\n+      if (t.getShape() != srcShape) {\n+        rop.emitError() << \"shape mismatch\";\n+      }\n+      if (t.getEncoding() != srcEncoding) {\n+        rop.emitError() << \"encoding mismatch\";\n+      }\n+    }\n   }\n \n-  ArrayRef<int64_t> getSrcShape() { return srcTy.getShape(); }\n+  ArrayRef<int64_t> getSrcShape() { return srcShape; }\n \n-  Attribute getSrcLayout() { return srcTy.getEncoding(); }\n+  Attribute getSrcLayout() { return srcEncoding; }\n \n   bool isFastReduction();\n \n@@ -37,8 +50,11 @@ class ReduceOpHelper {\n   bool isSupportedLayout();\n \n private:\n-  triton::ReduceOp op;\n-  RankedTensorType srcTy{};\n+  Operation *op;\n+  ArrayRef<int64_t> srcShape;\n+  Attribute srcEncoding;\n+  SmallVector<Type> srcElementTypes;\n+  int axis;\n };\n \n bool isSharedEncoding(Value value);"}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 0, "deletions": 24, "changes": 24, "file_content_changes": "@@ -34,30 +34,6 @@ def TT_PaddingOptionAttr : I32EnumAttr<\n     let cppNamespace = \"::mlir::triton\";\n }\n \n-// reduction\n-def TT_RedOpAttr : I32EnumAttr<\n-    /*name*/\"RedOp\", /*summary*/\"\",\n-    /*case*/\n-    [\n-        I32EnumAttrCase</*sym*/\"ADD\", 1, /*str*/\"add\">,\n-        I32EnumAttrCase<\"FADD\", 2, \"fadd\">,\n-        I32EnumAttrCase<\"MIN\", 3, \"min\">,\n-        I32EnumAttrCase<\"MAX\", 4, \"max\">,\n-        I32EnumAttrCase<\"UMIN\", 5, \"umin\">,\n-        I32EnumAttrCase<\"UMAX\", 6, \"umax\">,\n-        I32EnumAttrCase<\"ARGMIN\", 7, \"argmin\">,\n-        I32EnumAttrCase<\"ARGMAX\", 8, \"argmax\">,\n-        I32EnumAttrCase<\"ARGUMIN\", 9, \"argumin\">,\n-        I32EnumAttrCase<\"ARGUMAX\", 10, \"argumax\">,\n-        I32EnumAttrCase<\"FMIN\", 11, \"fmin\">,\n-        I32EnumAttrCase<\"FMAX\", 12, \"fmax\">,\n-        I32EnumAttrCase<\"ARGFMIN\", 13, \"argfmin\">,\n-        I32EnumAttrCase<\"ARGFMAX\", 14, \"argfmax\">,\n-        I32EnumAttrCase<\"XOR\", 15, \"xor\">\n-    ]> {\n-    let cppNamespace = \"::mlir::triton\";\n-}\n-\n // atomic\n def TT_AtomicRMWAttr : I32EnumAttr<\n     \"RMWOp\", \"\","}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 23, "deletions": 15, "changes": 38, "file_content_changes": "@@ -388,27 +388,35 @@ def TT_DotOp : TT_Op<\"dot\", [Pure,\n //\n // Reduce Op\n //\n-def TT_ReduceOp : TT_Op<\"reduce\", [Pure,\n-                                   DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n-    let summary = \"reduce\";\n-\n-    let arguments = (ins TT_RedOpAttr:$redOp, TT_Tensor:$operand, I32Attr:$axis);\n-\n-    let results = (outs TT_Type:$result);\n-\n+def TT_ReduceOp: TT_Op<\"reduce\",\n+                       [Pure,\n+                        SameOperandsEncoding,\n+                        SingleBlock,\n+                        DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n+    let summary = \"Reduction using generic combination algorithm\";\n+    let arguments = (ins Variadic<TT_Tensor>:$operands, I32Attr:$axis);\n+    let results = (outs Variadic<TT_Type>:$result);\n+    let regions = (region SizedRegion<1>:$combineOp);\n     let builders = [\n-        OpBuilder<(ins \"triton::RedOp\":$redOp, \"Value\":$operand, \"int\":$axis)>,\n+        OpBuilder<(ins \"ValueRange\":$operands, \"int\":$axis)>,\n     ];\n-\n-    let assemblyFormat = \"$operand attr-dict `:` type($operand) `->` type($result)\";\n-\n+    let hasVerifier = 1;\n+    let hasRegionVerifier = 1;\n     let extraClassDeclaration = [{\n-        // This member function is marked static because we need to call it before the ReduceOp\n-        // is constructed, see the implementation of create_reduce in triton.cc.\n-        static bool withIndex(mlir::triton::RedOp redOp);\n+      llvm::SmallVector<RankedTensorType> getInputTypes();\n+      llvm::SmallVector<Type> getElementTypes();\n+      unsigned getNumOperands();\n     }];\n }\n \n+def TT_ReduceReturnOp: TT_Op<\"reduce.return\",\n+                             [HasParent<\"ReduceOp\">, Pure, Terminator, ReturnLike]> {\n+    let summary = \"terminator for reduce operator\";\n+    let arguments = (ins Variadic<AnyType>:$result);\n+    let assemblyFormat = \"$result attr-dict `:` type($result)\";\n+}\n+\n+\n //\n // External Elementwise op\n //"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -103,7 +103,7 @@ def TTG_SelectOp : TTG_Op<\"select\", [Pure, Elementwise,\n                        TT_Tensor:$true_value,\n                        TT_Tensor:$false_value);\n \n-  let results = (outs TT_Tensor:$result);\n+  let results = (outs TT_Type:$result);\n }\n \n "}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -74,7 +74,10 @@ void MembarAnalysis::visitTerminator(Operation *op,\n     return;\n   }\n   // Otherwise, it could be a return op\n-  assert(isa<triton::ReturnOp>(op) && \"Unknown terminator\");\n+  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ReturnOp>(op)) {\n+    return;\n+  }\n+  llvm_unreachable(\"Unknown terminator encountered in membar analysis\");\n }\n \n void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 12, "deletions": 25, "changes": 37, "file_content_changes": "@@ -10,49 +10,38 @@\n namespace mlir {\n \n bool ReduceOpHelper::isFastReduction() {\n-  auto srcLayout = srcTy.getEncoding();\n-  auto axis = op.getAxis();\n-  return axis == triton::gpu::getOrder(srcLayout)[0];\n+  return axis == triton::gpu::getOrder(getSrcLayout())[0];\n }\n \n unsigned ReduceOpHelper::getInterWarpSize() {\n-  auto srcLayout = srcTy.getEncoding();\n-  auto srcShape = srcTy.getShape();\n-  auto axis = op.getAxis();\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   unsigned sizeIntraWarps = getIntraWarpSize();\n   return std::min(srcReduceDimSize / sizeIntraWarps,\n-                  triton::gpu::getWarpsPerCTA(srcLayout)[axis]);\n+                  triton::gpu::getWarpsPerCTA(getSrcLayout())[axis]);\n }\n \n unsigned ReduceOpHelper::getIntraWarpSize() {\n-  auto srcLayout = srcTy.getEncoding();\n-  auto srcShape = srcTy.getShape();\n-  auto axis = op.getAxis();\n   auto srcReduceDimSize = static_cast<unsigned>(srcShape[axis]);\n   return std::min(srcReduceDimSize,\n-                  triton::gpu::getThreadsPerWarp(srcLayout)[axis]);\n+                  triton::gpu::getThreadsPerWarp(getSrcLayout())[axis]);\n }\n \n unsigned ReduceOpHelper::getThreadsReductionAxis() {\n-  auto srcLayout = srcTy.getEncoding();\n-  auto axis = op.getAxis();\n+  auto srcLayout = getSrcLayout();\n   return triton::gpu::getThreadsPerWarp(srcLayout)[axis] *\n          triton::gpu::getWarpsPerCTA(srcLayout)[axis];\n }\n \n SmallVector<unsigned> ReduceOpHelper::getScratchConfigBasic() {\n-  auto axis = op.getAxis();\n   auto smemShape = convertType<unsigned>(getSrcShape());\n   smemShape[axis] = std::min(smemShape[axis], getThreadsReductionAxis());\n   return smemShape;\n }\n \n SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n-  auto axis = op.getAxis();\n   SmallVector<SmallVector<unsigned>> smemShapes(3);\n \n-  auto argLayout = srcTy.getEncoding();\n+  auto argLayout = getSrcLayout();\n   auto argLayoutMma = argLayout.dyn_cast<triton::gpu::MmaEncodingAttr>();\n   if (argLayoutMma && argLayoutMma.getVersionMajor() == 2 &&\n       triton::gpu::getWarpsPerCTA(argLayout)[axis] == 1)\n@@ -64,7 +53,7 @@ SmallVector<SmallVector<unsigned>> ReduceOpHelper::getScratchConfigsFast() {\n \n   /// FIXME(Qingyi): This size is actually larger than required.\n   /// shared memory block1:\n-  auto mod = op.getOperation()->getParentOfType<ModuleOp>();\n+  auto mod = op->getParentOfType<ModuleOp>();\n   unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n   smemShapes[1].push_back(numWarps * 32);\n \n@@ -82,17 +71,15 @@ unsigned ReduceOpHelper::getScratchSizeInBytes() {\n     elems = product<unsigned>(smemShape);\n   }\n \n-  auto tensorType = op.getOperand().getType().cast<RankedTensorType>();\n-  unsigned bytes = elems * tensorType.getElementTypeBitWidth() / 8;\n-\n-  if (triton::ReduceOp::withIndex(op.getRedOp()))\n-    bytes += elems * sizeof(int32_t);\n-\n-  return bytes;\n+  unsigned bytesPerElem = 0;\n+  for (const auto &ty : srcElementTypes) {\n+    bytesPerElem += ty.getIntOrFloatBitWidth() / 8;\n+  }\n+  return bytesPerElem * elems;\n }\n \n bool ReduceOpHelper::isSupportedLayout() {\n-  auto srcLayout = srcTy.getEncoding();\n+  auto srcLayout = getSrcLayout();\n   if (srcLayout.isa<triton::gpu::BlockedEncodingAttr>()) {\n     return true;\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "file_content_changes": "@@ -1073,12 +1073,14 @@ void populateElementwiseOpToLLVMPatterns(\n   POPULATE_BINARY_OP(arith::RemFOp, LLVM::FRemOp) // %\n   POPULATE_BINARY_OP(arith::RemSIOp, LLVM::SRemOp)\n   POPULATE_BINARY_OP(arith::RemUIOp, LLVM::URemOp)\n-  POPULATE_BINARY_OP(arith::AndIOp, LLVM::AndOp)   // &\n-  POPULATE_BINARY_OP(arith::OrIOp, LLVM::OrOp)     // |\n-  POPULATE_BINARY_OP(arith::XOrIOp, LLVM::XOrOp)   // ^\n-  POPULATE_BINARY_OP(arith::ShLIOp, LLVM::ShlOp)   // <<\n-  POPULATE_BINARY_OP(arith::ShRSIOp, LLVM::AShrOp) // >>\n-  POPULATE_BINARY_OP(arith::ShRUIOp, LLVM::LShrOp) // >>\n+  POPULATE_BINARY_OP(arith::AndIOp, LLVM::AndOp)    // &\n+  POPULATE_BINARY_OP(arith::OrIOp, LLVM::OrOp)      // |\n+  POPULATE_BINARY_OP(arith::XOrIOp, LLVM::XOrOp)    // ^\n+  POPULATE_BINARY_OP(arith::ShLIOp, LLVM::ShlOp)    // <<\n+  POPULATE_BINARY_OP(arith::ShRSIOp, LLVM::AShrOp)  // >>\n+  POPULATE_BINARY_OP(arith::ShRUIOp, LLVM::LShrOp)  // >>\n+  POPULATE_BINARY_OP(arith::MinFOp, LLVM::MinNumOp) // fmin\n+  POPULATE_BINARY_OP(arith::MinSIOp, LLVM::SMinOp)  // smin\n #undef POPULATE_BINARY_OP\n \n #define POPULATE_UNARY_OP(SRC_OP, DST_OP)                                      \\"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 210, "deletions": 272, "changes": 482, "file_content_changes": "@@ -23,112 +23,59 @@ struct ReduceOpConversion\n   }\n \n private:\n-  void accumulate(ConversionPatternRewriter &rewriter, Location loc,\n-                  RedOp redOp, Value &acc, Value cur, bool isFirst) const {\n+  void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n+                  llvm::SmallVectorImpl<Value> &acc, ValueRange cur,\n+                  bool isFirst) const {\n     if (isFirst) {\n-      acc = cur;\n+      acc.resize(cur.size());\n+      for (unsigned i = 0; i < cur.size(); ++i) {\n+        acc[i] = cur[i];\n+      }\n       return;\n     }\n-    switch (redOp) {\n-    case RedOp::ADD:\n-      acc = add(acc, cur);\n-      break;\n-    case RedOp::FADD:\n-      acc = fadd(acc.getType(), acc, cur);\n-      break;\n-    case RedOp::MIN:\n-      acc = smin(acc, cur);\n-      break;\n-    case RedOp::MAX:\n-      acc = smax(acc, cur);\n-      break;\n-    case RedOp::UMIN:\n-      acc = umin(acc, cur);\n-      break;\n-    case RedOp::UMAX:\n-      acc = umax(acc, cur);\n-      break;\n-    case RedOp::FMIN:\n-      acc = fmin(acc, cur);\n-      break;\n-    case RedOp::FMAX:\n-      acc = fmax(acc, cur);\n-      break;\n-    case RedOp::XOR:\n-      acc = xor_(acc, cur);\n-      break;\n-    case RedOp::ARGMIN:\n-    case RedOp::ARGMAX:\n-    case RedOp::ARGUMIN:\n-    case RedOp::ARGUMAX:\n-    case RedOp::ARGFMIN:\n-    case RedOp::ARGFMAX:\n-      llvm::report_fatal_error(\n-          \"This accumulate implementation is not for argmin / argmax\");\n-    default:\n-      llvm::report_fatal_error(\"Unsupported reduce op\");\n+\n+    // Create a new copy of the reduce block, and inline it\n+    Block *currentBlock = rewriter.getBlock();\n+    Region &parent = *currentBlock->getParent();\n+    rewriter.cloneRegionBefore(combineOp, &parent.front());\n+    auto &newReduce = parent.front();\n+    auto returnOp = dyn_cast<triton::ReduceReturnOp>(newReduce.getTerminator());\n+\n+    llvm::SmallVector<Value> combineArgs(2 * acc.size());\n+    for (unsigned i = 0; i < acc.size(); ++i) {\n+      combineArgs[i] = acc[i];\n+      combineArgs[acc.size() + i] = cur[i];\n     }\n-  }\n \n-  void accumulateWithIndex(ConversionPatternRewriter &rewriter, Location loc,\n-                           RedOp redOp, Value &acc, Value &accIndex, Value cur,\n-                           Value curIndex, bool isFirst) const {\n-    if (isFirst) {\n-      acc = cur;\n-      accIndex = curIndex;\n-      return;\n+    rewriter.inlineBlockBefore(&newReduce, &*rewriter.getInsertionPoint(),\n+                               combineArgs);\n+\n+    auto results = returnOp.getResult();\n+    for (unsigned i = 0; i < acc.size(); ++i) {\n+      acc[i] = results[i];\n     }\n-    switch (redOp) {\n-    case RedOp::ARGMIN:\n-      accIndex = select(\n-          icmp_slt(acc, cur), accIndex,\n-          select(icmp_sgt(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = smin(acc, cur);\n-      break;\n-    case RedOp::ARGMAX:\n-      accIndex = select(\n-          icmp_sgt(acc, cur), accIndex,\n-          select(icmp_slt(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = smax(acc, cur);\n-      break;\n-    case RedOp::ARGUMIN:\n-      accIndex = select(\n-          icmp_ult(acc, cur), accIndex,\n-          select(icmp_ugt(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = umin(acc, cur);\n-      break;\n-    case RedOp::ARGUMAX:\n-      accIndex = select(\n-          icmp_ugt(acc, cur), accIndex,\n-          select(icmp_ult(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = umax(acc, cur);\n-      break;\n-    case RedOp::ARGFMIN:\n-      accIndex = select(\n-          fcmp_olt(acc, cur), accIndex,\n-          select(fcmp_ogt(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = fmin(acc, cur);\n-      break;\n-    case RedOp::ARGFMAX:\n-      accIndex = select(\n-          fcmp_ogt(acc, cur), accIndex,\n-          select(fcmp_olt(acc, cur), curIndex, smin(accIndex, curIndex)));\n-      acc = fmax(acc, cur);\n-      break;\n-    case RedOp::ADD:\n-    case RedOp::FADD:\n-    case RedOp::MIN:\n-    case RedOp::MAX:\n-    case RedOp::UMIN:\n-    case RedOp::UMAX:\n-    case RedOp::FMIN:\n-    case RedOp::FMAX:\n-    case RedOp::XOR:\n-      llvm::report_fatal_error(\n-          \"This accumulate implementation is only for argmin / argmax\");\n-    default:\n-      llvm::report_fatal_error(\"Unsupported reduce op\");\n+\n+    // Delete the terminator, which is no longer used\n+    rewriter.eraseOp(returnOp);\n+  }\n+\n+  SmallVector<SmallVector<Value>>\n+  unpackInputs(Location loc, triton::ReduceOp op, OpAdaptor adaptor,\n+               ConversionPatternRewriter &rewriter) const {\n+    auto types = op.getInputTypes();\n+    auto operands = adaptor.getOperands();\n+    unsigned srcElems = getElemsPerThread(types[0]);\n+    SmallVector<SmallVector<Value>> srcValues(srcElems);\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      auto values = getTypeConverter()->unpackLLElements(loc, operands[i],\n+                                                         rewriter, types[i]);\n+\n+      assert(values.size() == srcValues.size());\n+      for (unsigned j = 0; j < srcValues.size(); ++j) {\n+        srcValues[j].push_back(values[j]);\n+      }\n     }\n+    return srcValues;\n   }\n \n   // Calculates the write index in the shared memory where we would be writing\n@@ -177,63 +124,64 @@ struct ReduceOpConversion\n   matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n                        ConversionPatternRewriter &rewriter) const {\n     ReduceOpHelper helper(op);\n-    Location loc = op->getLoc();\n+    Location loc = op.getLoc();\n     unsigned axis = op.getAxis();\n-    // Specifies whether the reduce operation returns an index\n-    // rather than a value, e.g. argmax, argmin, .. etc\n-    bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n-    auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding();\n+    auto srcTys = op.getInputTypes();\n+    auto srcLayout = helper.getSrcLayout();\n     if (!helper.isSupportedLayout()) {\n       assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n     }\n     // The order of the axes for the the threads within the warp\n     auto srcOrd = triton::gpu::getOrder(srcLayout);\n     auto sizePerThread = triton::gpu::getSizePerThread(srcLayout);\n-    auto srcShape = srcTy.getShape();\n+    auto srcShape = helper.getSrcShape();\n \n-    auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+    SmallVector<Type> elemPtrTys(srcTys.size());\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      auto ty = srcTys[i].getElementType();\n+      auto llvmElemTy = getTypeConverter()->convertType(ty);\n+      elemPtrTys[i] = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+    }\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n     auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n \n-    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-    smemBase = bitcast(smemBase, elemPtrTy);\n-\n     auto smemShape = helper.getScratchConfigBasic();\n     unsigned elems = product<unsigned>(smemShape);\n-    Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(elems));\n-    indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n \n-    unsigned srcElems = getElemsPerThread(srcTy);\n+    SmallVector<Value> smemBases(op.getNumOperands());\n+    smemBases[0] = bitcast(\n+        getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys[0]);\n+    for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n+      smemBases[i] =\n+          bitcast(gep(elemPtrTys[i - 1], smemBases[i - 1], i32_val(elems)),\n+                  elemPtrTys[i]);\n+    }\n+\n+    unsigned srcElems = getElemsPerThread(srcTys[0]);\n     // Emits indices of the original tensor that each thread\n     // would own\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n-    auto srcValues = getTypeConverter()->unpackLLElements(\n-        loc, adaptor.getOperand(), rewriter, srcTy);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTys[0]);\n+    auto srcValues = unpackInputs(loc, op, adaptor, rewriter);\n+\n     // Emits offsets (the offset from the base index)\n     // of the original tensor that each thread would own\n+    // NOTE: Assumes offsets don't actually depend on type\n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForLayout(srcLayout, srcTy);\n+        emitOffsetForLayout(srcLayout, srcTys[0]);\n+\n     // Keep track of accumulations and their indices\n-    std::map<SmallVector<unsigned>, Value> accs;\n-    std::map<SmallVector<unsigned>, Value> accIndices;\n+    std::map<SmallVector<unsigned>, SmallVector<Value>> accs;\n     std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n \n+    Region *combineOp = &op.getCombineOp();\n+\n     // reduce within threads\n     for (unsigned i = 0; i < srcElems; ++i) {\n       SmallVector<unsigned> key = offset[i];\n       key[axis] = 0;\n       bool isFirst = accs.find(key) == accs.end();\n-      if (!withIndex) {\n-        accumulate(rewriter, loc, op.getRedOp(), accs[key], srcValues[i],\n-                   isFirst);\n-      } else {\n-        Value curIndex = srcIndices[i][axis];\n-        accumulateWithIndex(rewriter, loc, op.getRedOp(), accs[key],\n-                            accIndices[key], srcValues[i], curIndex, isFirst);\n-      }\n+      accumulate(rewriter, *combineOp, accs[key], srcValues[i], isFirst);\n       if (isFirst)\n         indices[key] = srcIndices[i];\n     }\n@@ -250,24 +198,20 @@ struct ReduceOpConversion\n     // reduce across threads\n     for (auto it : accs) {\n       const SmallVector<unsigned> &key = it.first;\n-      Value acc = it.second;\n-      Value accIndex;\n-      if (withIndex)\n-        accIndex = accIndices[key];\n+      auto &acc = it.second;\n       // get the writeIdx at which to write in smem\n       SmallVector<Value> writeIdx;\n       getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n                          axis);\n+\n       // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n-      // Get element pointers for the value and index\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      Value indexWritePtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n-      // Store the within-thread accumulated value at writePtr\n-      store(acc, writePtr);\n-      // Store the index of within-thread accumulation at indexWritePtr\n-      if (withIndex)\n-        store(accIndex, indexWritePtr);\n+      SmallVector<Value> writePtrs(op.getNumOperands());\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        // Store the within-thread accumulated value into shared memory\n+        writePtrs[i] = gep(elemPtrTys[i], smemBases[i], writeOffset);\n+        store(acc[i], writePtrs[i]);\n+      }\n \n       SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n       // Perform parallel reduction with sequential addressing\n@@ -286,61 +230,62 @@ struct ReduceOpConversion\n         Value readOffset = select(\n             readMask, linearize(rewriter, loc, readIdx, smemShape, srcOrd),\n             ints[0]);\n-        // The readPtr is readOffset away from writePtr\n-        Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n+        SmallVector<Value> readPtrs(op.getNumOperands());\n+        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+          // The readPtr is readOffset away from writePtr\n+          readPtrs[i] = gep(elemPtrTys[i], writePtrs[i], readOffset);\n+        }\n+\n+        barrier();\n+        // Combine accumulator value from another thread\n+        SmallVector<Value> cur(op.getNumOperands());\n+        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+          cur[i] = load(readPtrs[i]);\n+        }\n+        accumulate(rewriter, *combineOp, acc, cur, false);\n+\n         barrier();\n-        // If we do not care about the index, i.e. this is not an argmax,\n-        // argmin, .. etc\n-        if (!withIndex) {\n-          // The value at the readPtr, whereas acc is the value at writePtr\n-          Value cur = load(readPtr);\n-          accumulate(rewriter, loc, op.getRedOp(), acc, cur, false);\n-          barrier();\n-          // Update writePtr value\n-          store(acc, writePtr);\n-        } else {\n-          Value cur = load(readPtr);\n-          Value indexReadPtr = gep(indexPtrTy, indexWritePtr, readOffset);\n-          Value curIndex = load(indexReadPtr);\n-          accumulateWithIndex(rewriter, loc, op.getRedOp(), acc, accIndex, cur,\n-                              curIndex, false);\n-          barrier();\n-          store(acc, writePtr);\n-          store(accIndex, indexWritePtr);\n+        // Publish our new accumulator value to shared memory\n+        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+          store(acc[i], writePtrs[i]);\n         }\n       }\n     }\n \n     barrier();\n \n     // set output values\n-    if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n-      // nd-tensor where n >= 1\n-      auto resultLayout = resultTy.getEncoding();\n-      auto resultShape = resultTy.getShape();\n-\n-      unsigned resultElems = getElemsPerThread(resultTy);\n-      auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n-      assert(resultIndices.size() == resultElems);\n-\n-      SmallVector<Value> resultVals(resultElems);\n-      for (unsigned i = 0; i < resultElems; ++i) {\n-        SmallVector<Value> readIdx = resultIndices[i];\n-        readIdx.insert(readIdx.begin() + axis, ints[0]);\n-        Value readOffset = linearize(rewriter, loc, readIdx, smemShape, srcOrd);\n-        Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n-        Value indexReadPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n-        resultVals[i] = withIndex ? load(indexReadPtr) : load(readPtr);\n+    SmallVector<Value> results(op.getNumOperands());\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      if (auto resultTy =\n+              op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n+        // nd-tensor where n >= 1\n+\n+        auto resultLayout = resultTy.getEncoding();\n+\n+        unsigned resultElems = getElemsPerThread(resultTy);\n+        auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n+        assert(resultIndices.size() == resultElems);\n+\n+        SmallVector<Value> resultVals(resultElems);\n+        for (unsigned j = 0; j < resultElems; ++j) {\n+          SmallVector<Value> readIdx = resultIndices[j];\n+          readIdx.insert(readIdx.begin() + axis, ints[0]);\n+          Value readOffset =\n+              linearize(rewriter, loc, readIdx, smemShape, srcOrd);\n+          Value readPtr = gep(elemPtrTys[i], smemBases[i], readOffset);\n+          resultVals[j] = load(readPtr);\n+        }\n+        results[i] = getTypeConverter()->packLLElements(loc, resultVals,\n+                                                        rewriter, resultTy);\n+      } else {\n+        // 0d-tensor -> scalar\n+        results[i] = load(smemBases[i]);\n       }\n-      Value ret = getTypeConverter()->packLLElements(loc, resultVals, rewriter,\n-                                                     resultTy);\n-      rewriter.replaceOp(op, ret);\n-    } else {\n-      // 0d-tensor -> scalar\n-      Value resultVal = withIndex ? load(indexSmemBase) : load(smemBase);\n-      rewriter.replaceOp(op, resultVal);\n     }\n \n+    auto parentBlock = op.getOperation()->getBlock();\n+    rewriter.replaceOp(op, results);\n     return success();\n   }\n \n@@ -351,60 +296,59 @@ struct ReduceOpConversion\n     ReduceOpHelper helper(op);\n     Location loc = op->getLoc();\n     unsigned axis = adaptor.getAxis();\n-    bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n-    auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n-    auto srcLayout = srcTy.getEncoding();\n+    auto srcTys = op.getInputTypes();\n+    auto srcLayout = helper.getSrcLayout();\n     if (!helper.isSupportedLayout()) {\n       assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n     }\n-    auto srcShape = srcTy.getShape();\n-    auto order = getOrder(srcLayout);\n-\n-    auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n-    auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n+    auto srcOrd = triton::gpu::getOrder(srcLayout);\n+    auto srcShape = helper.getSrcShape();\n \n-    auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n+    SmallVector<Type> elemPtrTys(srcTys.size());\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      auto ty = srcTys[i].getElementType();\n+      auto llvmElemTy = getTypeConverter()->convertType(ty);\n+      elemPtrTys[i] = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+    }\n     auto llvmIndexTy = getTypeConverter()->getIndexType();\n-    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n     auto indexPtrTy = LLVM::LLVMPointerType::get(llvmIndexTy, 3);\n-    Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n-    smemBase = bitcast(smemBase, elemPtrTy);\n \n     auto smemShapes = helper.getScratchConfigsFast();\n     unsigned elems = product<unsigned>(smemShapes[0]);\n     unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));\n-    Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(maxElems));\n-    indexSmemBase = bitcast(indexSmemBase, indexPtrTy);\n+\n+    SmallVector<Value> smemBases(op.getNumOperands());\n+    smemBases[0] = bitcast(\n+        getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys[0]);\n+    for (unsigned i = 1; i < op.getNumOperands(); ++i) {\n+      smemBases[i] =\n+          bitcast(gep(elemPtrTys[i - 1], smemBases[i - 1], i32_val(maxElems)),\n+                  elemPtrTys[i]);\n+    }\n \n     unsigned sizeIntraWarps = helper.getIntraWarpSize();\n     unsigned sizeInterWarps = helper.getInterWarpSize();\n \n-    unsigned srcElems = getElemsPerThread(srcTy);\n-    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTy);\n-    auto srcValues = getTypeConverter()->unpackLLElements(\n-        loc, adaptor.getOperand(), rewriter, srcTy);\n+    unsigned srcElems = getElemsPerThread(srcTys[0]);\n+    auto srcIndices = emitIndices(loc, rewriter, srcLayout, srcTys[0]);\n+    auto srcValues = unpackInputs(loc, op, adaptor, rewriter);\n \n+    std::map<SmallVector<unsigned>, SmallVector<Value>> accs;\n+    std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n+\n+    // Assumes offsets don't actually depend on type\n     SmallVector<SmallVector<unsigned>> offset =\n-        emitOffsetForLayout(srcLayout, srcTy);\n+        emitOffsetForLayout(srcLayout, srcTys[0]);\n \n-    std::map<SmallVector<unsigned>, Value> accs;\n-    std::map<SmallVector<unsigned>, Value> accIndices;\n-    std::map<SmallVector<unsigned>, SmallVector<Value>> indices;\n+    auto *combineOp = &op.getCombineOp();\n \n     // reduce within threads\n     for (unsigned i = 0; i < srcElems; ++i) {\n       SmallVector<unsigned> key = offset[i];\n       key[axis] = 0;\n       bool isFirst = accs.find(key) == accs.end();\n-      if (!withIndex) {\n-        accumulate(rewriter, loc, op.getRedOp(), accs[key], srcValues[i],\n-                   isFirst);\n-      } else {\n-        Value curIndex = srcIndices[i][axis];\n-        accumulateWithIndex(rewriter, loc, op.getRedOp(), accs[key],\n-                            accIndices[key], srcValues[i], curIndex, isFirst);\n-      }\n+      accumulate(rewriter, *combineOp, accs[key], srcValues[i], isFirst);\n       if (isFirst)\n         indices[key] = srcIndices[i];\n     }\n@@ -414,6 +358,9 @@ struct ReduceOpConversion\n     Value warpId = udiv(threadId, warpSize);\n     Value laneId = urem(threadId, warpSize);\n \n+    auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n+    auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n+    auto order = getOrder(srcLayout);\n     SmallVector<Value> multiDimLaneId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n     SmallVector<Value> multiDimWarpId =\n@@ -427,32 +374,24 @@ struct ReduceOpConversion\n \n     for (auto it : accs) {\n       const SmallVector<unsigned> &key = it.first;\n-      Value acc = it.second;\n-      Value accIndex;\n-      if (withIndex)\n-        accIndex = accIndices[key];\n+      SmallVector<Value> acc = it.second;\n \n       // Reduce within warps\n       for (unsigned N = sizeIntraWarps / 2; N > 0; N >>= 1) {\n-        Value shfl = shflSync(loc, rewriter, acc, N);\n-        if (!withIndex) {\n-          accumulate(rewriter, loc, op.getRedOp(), acc, shfl, false);\n-        } else {\n-          Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n-          accumulateWithIndex(rewriter, loc, op.getRedOp(), acc, accIndex, shfl,\n-                              shflIndex, false);\n+        SmallVector<Value> shfl(op.getNumOperands());\n+        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+          shfl[i] = shflSync(loc, rewriter, acc[i], N);\n         }\n+        accumulate(rewriter, *combineOp, acc, shfl, false);\n       }\n \n       SmallVector<Value> writeIdx = indices[key];\n       writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n       Value writeOffset =\n           linearize(rewriter, loc, writeIdx, smemShapes[0], order);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, laneZero);\n-      if (withIndex) {\n-        Value indexWritePtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n-        storeShared(rewriter, loc, indexWritePtr, accIndex, laneZero);\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        Value writePtr = gep(elemPtrTys[i], smemBases[i], writeOffset);\n+        storeShared(rewriter, loc, writePtr, acc[i], laneZero);\n       }\n     }\n \n@@ -469,39 +408,36 @@ struct ReduceOpConversion\n     unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n     Value readOffset = threadId;\n     for (unsigned round = 0; round < elemsPerThread; ++round) {\n-      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       // FIXME(Qingyi): need predicate icmp_slt(threadId,\n       // i32_val(sizeInerWarps))\n-      Value acc = load(readPtr);\n-      Value accIndex;\n-      if (withIndex) {\n-        Value readIndexPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n-        accIndex = load(readIndexPtr);\n+      SmallVector<Value> acc(op.getNumOperands());\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        Value readPtr = gep(elemPtrTys[i], smemBases[i], readOffset);\n+        acc[i] = load(readPtr);\n       }\n \n       for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n-        Value shfl = shflSync(loc, rewriter, acc, N);\n-        if (!withIndex) {\n-          accumulate(rewriter, loc, op.getRedOp(), acc, shfl, false);\n-        } else {\n-          Value shflIndex = shflSync(loc, rewriter, accIndex, N);\n-          accumulateWithIndex(rewriter, loc, op.getRedOp(), acc, accIndex, shfl,\n-                              shflIndex, false);\n+        SmallVector<Value> shfl(op.getNumOperands());\n+        for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+          shfl[i] = shflSync(loc, rewriter, acc[i], N);\n         }\n+        accumulate(rewriter, *combineOp, acc, shfl, false);\n       }\n \n       // only the first thread in each sizeInterWarps is writing\n       Value writeOffset = readOffset;\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+      SmallVector<Value> writePtrs(op.getNumOperands());\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        writePtrs[i] = gep(elemPtrTys[i], smemBases[i], writeOffset);\n+      }\n       Value threadIsNeeded = icmp_slt(threadId, i32_val(elems));\n       Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));\n       Value laneIdModSizeInterWarpsIsZero =\n           icmp_eq(laneIdModSizeInterWarps, zero);\n       Value pred = and_(threadIsNeeded, laneIdModSizeInterWarpsIsZero);\n-      storeShared(rewriter, loc, writePtr, acc, pred);\n-      if (withIndex) {\n-        Value writeIndexPtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n-        storeShared(rewriter, loc, writeIndexPtr, accIndex, pred);\n+\n+      for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+        storeShared(rewriter, loc, writePtrs[i], acc[i], pred);\n       }\n \n       if (round != elemsPerThread - 1) {\n@@ -515,32 +451,34 @@ struct ReduceOpConversion\n     barrier();\n \n     // set output values\n-    if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n-      // nd-tensor where n >= 1\n-      auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n-      auto resultShape = resultTy.getShape();\n-      unsigned resultElems = getElemsPerThread(resultTy);\n-      auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n-      assert(resultIndices.size() == resultElems);\n-\n-      SmallVector<Value> resultVals(resultElems);\n-      for (size_t i = 0; i < resultElems; ++i) {\n-        SmallVector<Value> readIdx = resultIndices[i];\n-        readIdx.insert(readIdx.begin() + axis, i32_val(0));\n-        Value readOffset =\n-            linearize(rewriter, loc, readIdx, smemShapes[0], order);\n-        Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n-        Value indexReadPtr = gep(indexPtrTy, indexSmemBase, readOffset);\n-        resultVals[i] = withIndex ? load(indexReadPtr) : load(readPtr);\n+    SmallVector<Value> results(op.getNumOperands());\n+    for (unsigned i = 0; i < op.getNumOperands(); ++i) {\n+      if (auto resultTy =\n+              op.getResult()[i].getType().dyn_cast<RankedTensorType>()) {\n+        // nd-tensor where n >= 1\n+        auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n+        unsigned resultElems = getElemsPerThread(resultTy);\n+        auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultTy);\n+        assert(resultIndices.size() == resultElems);\n+\n+        SmallVector<Value> resultVals(resultElems);\n+        for (size_t j = 0; j < resultElems; ++j) {\n+          SmallVector<Value> readIdx = resultIndices[j];\n+          readIdx.insert(readIdx.begin() + axis, i32_val(0));\n+          Value readOffset =\n+              linearize(rewriter, loc, readIdx, smemShapes[0], order);\n+          Value readPtr = gep(elemPtrTys[i], smemBases[i], readOffset);\n+          resultVals[j] = load(readPtr);\n+        }\n+\n+        results[i] = getTypeConverter()->packLLElements(loc, resultVals,\n+                                                        rewriter, resultTy);\n+      } else {\n+        // 0d-tensor -> scalar\n+        results[i] = load(smemBases[i]);\n       }\n-      Value ret = getTypeConverter()->packLLElements(loc, resultVals, rewriter,\n-                                                     resultTy);\n-      rewriter.replaceOp(op, ret);\n-    } else {\n-      // 0d-tensor -> scalar\n-      Value resultVal = withIndex ? load(indexSmemBase) : load(smemBase);\n-      rewriter.replaceOp(op, resultVal);\n     }\n+    rewriter.replaceOp(op, results);\n \n     return success();\n   }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 19, "deletions": 4, "changes": 23, "file_content_changes": "@@ -46,15 +46,30 @@ Type TritonGPUToLLVMTypeConverter::convertTritonPointerType(\n Value TritonGPUToLLVMTypeConverter::packLLElements(\n     Location loc, ValueRange resultVals, ConversionPatternRewriter &rewriter,\n     Type type) {\n-  auto structType = this->convertType(type);\n-  if (!structType.isa<LLVM::LLVMStructType>()) {\n+  auto structType = this->convertType(type).dyn_cast<LLVM::LLVMStructType>();\n+  if (!structType) {\n+    assert(resultVals.size() == 1);\n     return *resultVals.begin();\n   }\n \n+  auto elementTypes = structType.getBody();\n+  if (elementTypes.size() != resultVals.size()) {\n+    emitError(loc) << \" size mismatch when packing elements for LLVM struct\"\n+                   << \" expected \" << elementTypes.size() << \" but got \"\n+                   << resultVals.size();\n+  }\n   Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n-  // llvm::outs() << structType << \"\\n\";\n   for (const auto &v : llvm::enumerate(resultVals)) {\n-    assert(v.value() && \"can not insert null values\");\n+    if (!v.value()) {\n+      emitError(loc)\n+          << \"cannot insert null values into struct, but tried to insert\"\n+          << v.value();\n+    }\n+    if (v.value().getType() != elementTypes[v.index()]) {\n+      emitError(loc) << \"invalid element type in packLLEElements. Expected \"\n+                     << elementTypes[v.index()] << \" but got \"\n+                     << v.value().getType();\n+    }\n     llvmStruct = insert_val(structType, llvmStruct, v.value(), v.index());\n   }\n   return llvmStruct;"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 36, "deletions": 15, "changes": 51, "file_content_changes": "@@ -68,13 +68,15 @@ class ArithConstantPattern : public OpConversionPattern<arith::ConstantOp> {\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = getTypeConverter()->convertType(op.getType());\n     auto value = adaptor.getValue().dyn_cast<DenseElementsAttr>();\n-    assert(value);\n-    if (value.getElementType().isInteger(1) && value.isSplat())\n-      // Workaround until https://reviews.llvm.org/D133743 is included.\n-      value = DenseElementsAttr::get(retType, value.getSplatValue<bool>());\n-    else\n-      // This is a hack. We just want to add encoding\n-      value = value.reshape(retType);\n+    if (dyn_cast<RankedTensorType>(retType)) {\n+      assert(value);\n+      if (value.getElementType().isInteger(1) && value.isSplat())\n+        // Workaround until https://reviews.llvm.org/D133743 is included.\n+        value = DenseElementsAttr::get(retType, value.getSplatValue<bool>());\n+      else\n+        // This is a hack. We just want to add encoding\n+        value = value.reshape(retType);\n+    }\n     addNamedAttrs(\n         rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, retType, value),\n         adaptor.getAttributes());\n@@ -469,10 +471,28 @@ struct TritonReducePattern : public OpConversionPattern<triton::ReduceOp> {\n   LogicalResult\n   matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    addNamedAttrs(\n-        rewriter.replaceOpWithNewOp<triton::ReduceOp>(\n-            op, adaptor.getRedOp(), adaptor.getOperand(), adaptor.getAxis()),\n-        adaptor.getAttributes());\n+    auto newReduce = rewriter.create<triton::ReduceOp>(\n+        op.getLoc(), adaptor.getOperands(), adaptor.getAxis());\n+    addNamedAttrs(newReduce, adaptor.getAttributes());\n+\n+    auto &newCombineOp = newReduce.getCombineOp();\n+    rewriter.inlineRegionBefore(op.getCombineOp(), newCombineOp,\n+                                newCombineOp.end());\n+    rewriter.replaceOp(op, newReduce.getResult());\n+    return success();\n+  }\n+};\n+\n+struct TritonReduceReturnPattern\n+    : public OpConversionPattern<triton::ReduceReturnOp> {\n+  using OpConversionPattern<triton::ReduceReturnOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ReduceReturnOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::ReduceReturnOp>(\n+                      op, adaptor.getResult()),\n+                  adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -517,10 +537,11 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n           TritonGenericPattern<triton::PtrToIntOp>,\n           TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n           TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n-          TritonReducePattern, TritonTransPattern, TritonExpandDimsPattern,\n-          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n-          TritonStorePattern, TritonExtElemwisePattern, TritonPrintPattern,\n-          TritonAssertPattern, TritonAtomicRMWPattern>(typeConverter, context);\n+          TritonReducePattern, TritonReduceReturnPattern, TritonTransPattern,\n+          TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n+          TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern,\n+          TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern>(\n+          typeConverter, context);\n }\n \n //"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 109, "deletions": 21, "changes": 130, "file_content_changes": "@@ -310,21 +310,10 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n }\n \n //-- ReduceOp --\n-mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n-    MLIRContext *context, std::optional<Location> location, ValueRange operands,\n-    DictionaryAttr attributes, RegionRange regions,\n-    SmallVectorImpl<Type> &inferredReturnTypes) {\n-  // infer shape\n-  Value arg = operands[0];\n-  auto argTy = arg.getType().cast<RankedTensorType>();\n-  auto argEltTy = argTy.getElementType();\n-  auto i32Ty = IntegerType::get(argEltTy.getContext(), 32);\n-  auto redOp =\n-      attributes.get(\"redOp\").cast<mlir::triton::RedOpAttr>().getValue();\n-  bool withIndex = mlir::triton::ReduceOp::withIndex(redOp);\n-  auto retEltTy = withIndex ? i32Ty : argEltTy;\n+static mlir::LogicalResult\n+inferReduceReturnShape(const RankedTensorType &argTy, const Type &retEltTy,\n+                       int axis, SmallVectorImpl<Type> &inferredReturnTypes) {\n   auto retShape = argTy.getShape().vec();\n-  int axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n   retShape.erase(retShape.begin() + axis);\n   if (retShape.empty()) {\n     // 0d-tensor -> scalar\n@@ -352,15 +341,114 @@ mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n   return mlir::success();\n }\n \n-bool mlir::triton::ReduceOp::withIndex(mlir::triton::RedOp redOp) {\n-  return redOp == mlir::triton::RedOp::ARGMIN ||\n-         redOp == mlir::triton::RedOp::ARGMAX ||\n-         redOp == mlir::triton::RedOp::ARGUMIN ||\n-         redOp == mlir::triton::RedOp::ARGUMAX ||\n-         redOp == mlir::triton::RedOp::ARGFMIN ||\n-         redOp == mlir::triton::RedOp::ARGFMAX;\n+void ReduceOp::build(mlir::OpBuilder &builder, mlir::OperationState &state,\n+                     mlir::ValueRange operands, int axis) {\n+  SmallVector<Type> inferredReturnTypes;\n+  for (unsigned i = 0; i < operands.size(); ++i) {\n+    auto argTy = operands[i].getType().cast<RankedTensorType>();\n+    auto retEltTy = argTy.getElementType();\n+    (void)inferReduceReturnShape(argTy, retEltTy, axis, inferredReturnTypes);\n+  }\n+\n+  ReduceOp::build(builder, state, inferredReturnTypes, operands, axis);\n }\n \n+mlir::LogicalResult mlir::triton::ReduceOp::inferReturnTypes(\n+    MLIRContext *context, std::optional<Location> location, ValueRange operands,\n+    DictionaryAttr attributes, RegionRange regions,\n+    SmallVectorImpl<Type> &inferredReturnTypes) {\n+  for (auto arg : operands) {\n+    auto argTy = arg.getType().cast<RankedTensorType>();\n+    auto retEltTy = argTy.getElementType();\n+    int axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n+    if (inferReduceReturnShape(argTy, retEltTy, axis, inferredReturnTypes)\n+            .failed()) {\n+      return failure();\n+    }\n+  }\n+  return success();\n+}\n+\n+mlir::LogicalResult mlir::triton::ReduceOp::verify() {\n+  if (this->getOperands().size() < 1) {\n+    return this->emitOpError() << \"must have at least 1 operand\";\n+  }\n+  for (const auto &operand : this->getOperands()) {\n+    if (!dyn_cast<RankedTensorType>(operand.getType())) {\n+      return this->emitOpError() << \"operands must be RankedTensorType\";\n+    }\n+  }\n+  return success();\n+}\n+\n+mlir::LogicalResult mlir::triton::ReduceOp::verifyRegions() {\n+  auto argElementTypes = this->getElementTypes();\n+  const auto &operands = this->getOperands();\n+  const auto numArgs = 2 * operands.size();\n+  auto &block = *this->getBody();\n+  if (block.getNumArguments() != numArgs) {\n+    return this->emitOpError() << \"nested block must take \" << numArgs\n+                               << \" arguments, but given block with \"\n+                               << block.getNumArguments() << \" arguments\";\n+  }\n+  unsigned i = 0;\n+  const auto &blockArgTypes = block.getArgumentTypes();\n+  for (unsigned i = 0; i < numArgs; ++i) {\n+    const auto &blockArgTy = blockArgTypes[i];\n+    const auto &argElemTy = argElementTypes[i % operands.size()];\n+    if (blockArgTy != argElemTy) {\n+      return this->emitOpError()\n+             << \"type mismatch on combine operation. Expected argument \" << i\n+             << \" to have type \" << argElemTy << \" but got \" << blockArgTy;\n+    }\n+  }\n+\n+  auto terminator =\n+      dyn_cast<mlir::triton::ReduceReturnOp>(block.getTerminator());\n+  if (!terminator) {\n+    return this->emitOpError()\n+           << \"combine operation must be terminated \"\n+           << \"with a ReduceReturnOp but got \" << block.getTerminator();\n+  }\n+  const auto &combineResults = terminator->getOperands();\n+  if (combineResults.size() != operands.size()) {\n+    return this->emitOpError()\n+           << \"expected combine operation to return \" << operands.size()\n+           << \" values but got \" << combineResults.size();\n+  }\n+  for (unsigned i = 0; i < combineResults.size(); ++i) {\n+    const auto &resultTy = combineResults[i].getType();\n+    const auto &argElemTy = argElementTypes[i];\n+    if (resultTy != argElemTy) {\n+      return this->emitOpError()\n+             << \"type mismatch on combine operation. Expected argument \" << i\n+             << \" to have type \" << argElemTy << \" but got \" << resultTy;\n+    }\n+  }\n+  return mlir::success();\n+}\n+\n+llvm::SmallVector<mlir::RankedTensorType> ReduceOp::getInputTypes() {\n+  llvm::SmallVector<RankedTensorType> srcTys;\n+  srcTys.reserve(this->getNumOperands());\n+  for (const auto &ty : this->getOperands().getTypes()) {\n+    srcTys.push_back(ty.cast<RankedTensorType>());\n+  }\n+  return srcTys;\n+}\n+\n+llvm::SmallVector<Type> ReduceOp::getElementTypes() {\n+  llvm::SmallVector<Type> srcElemTys;\n+  srcElemTys.reserve(this->getNumOperands());\n+  for (const auto &op : this->getOperands()) {\n+    srcElemTys.push_back(\n+        op.getType().cast<RankedTensorType>().getElementType());\n+  }\n+  return srcElemTys;\n+}\n+\n+unsigned ReduceOp::getNumOperands() { return this->getOperands().size(); }\n+\n //-- SplatOp --\n OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n   auto value = adaptor.getSrc();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 47, "deletions": 17, "changes": 64, "file_content_changes": "@@ -101,29 +101,59 @@ class SimplifyReduceCvt : public mlir::RewritePattern {\n     auto convert = llvm::cast<triton::gpu::ConvertLayoutOp>(op);\n     triton::ReduceOp reduce;\n     for (auto &use : convert.getResult().getUses()) {\n-      auto owner = use.getOwner();\n-      if (llvm::isa_and_nonnull<triton::ReduceOp>(owner)) {\n-        reduce = llvm::cast<triton::ReduceOp>(owner);\n-        break;\n+      auto owner = llvm::dyn_cast<triton::ReduceOp>(use.getOwner());\n+      if (!owner) {\n+        continue;\n+      }\n+\n+      // TODO: This only moves conversions from the first argument which is\n+      // fine for argmin/argmax but may not be optimal generally\n+      if (convert.getResult() != owner.getOperands()[0]) {\n+        continue;\n       }\n+      reduce = owner;\n+      break;\n     }\n     if (!reduce)\n       return mlir::failure();\n+\n+    SmallVector<Value> newOperands = reduce.getOperands();\n+\n+    newOperands[0] = convert.getOperand();\n+    auto newEncoding =\n+        newOperands[0].getType().cast<RankedTensorType>().getEncoding();\n+\n     // this may generate unsupported conversions in the LLVM codegen\n-    if (convert.getOperand()\n-            .getType()\n-            .cast<RankedTensorType>()\n-            .getEncoding()\n-            .isa<triton::gpu::MmaEncodingAttr>())\n-      return mlir::failure();\n+    if (newEncoding.isa<triton::gpu::MmaEncodingAttr>()) {\n+      return failure();\n+    }\n+\n+    for (unsigned i = 1; i < newOperands.size(); ++i) {\n+      auto oldTy = newOperands[i].getType().cast<RankedTensorType>();\n+      RankedTensorType newTy =\n+          RankedTensorType::Builder(oldTy).setEncoding(newEncoding);\n+\n+      newOperands[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+          op->getLoc(), newTy, newOperands[i]);\n+    }\n+\n     auto newReduce = rewriter.create<triton::ReduceOp>(\n-        op->getLoc(), reduce.getRedOp(), convert.getOperand(),\n-        reduce.getAxis());\n-    Value newRet = newReduce.getResult();\n-    if (newRet.getType() != reduce.getResult().getType())\n-      newRet = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-          op->getLoc(), reduce.getResult().getType(), newRet);\n-    rewriter.replaceAllUsesWith(reduce, newRet);\n+        op->getLoc(), newOperands, reduce.getAxis());\n+    auto &newCombineOp = newReduce.getCombineOp();\n+    rewriter.inlineRegionBefore(reduce.getCombineOp(), newCombineOp,\n+                                newCombineOp.end());\n+\n+    SmallVector<Value> newRet = newReduce.getResult();\n+    auto oldTypes = reduce.getResult().getType();\n+    for (unsigned i = 0; i < reduce.getNumOperands(); ++i) {\n+      // it's still beneficial to move the conversion\n+      // to after the reduce if necessary since it will be\n+      // done on a rank-reduced tensor hence cheaper\n+      if (newRet[i].getType() != oldTypes[i])\n+        newRet[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+            op->getLoc(), oldTypes[i], newRet[i]);\n+    }\n+    rewriter.replaceAllUsesWith(reduce.getResult(), newRet);\n \n     return success();\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/TritonGPUConversion.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -79,6 +79,8 @@ TritonGPUConversionTarget::TritonGPUConversionTarget(\n   // Some ops from SCF are illegal\n   addIllegalOp<scf::ExecuteRegionOp, scf::ParallelOp, scf::ReduceOp,\n                scf::ReduceReturnOp>();\n+  // We have custom versions of some arith operators\n+  addIllegalOp<arith::CmpIOp, arith::CmpFOp>();\n \n   addDynamicallyLegalDialect<arith::ArithDialect, math::MathDialect,\n                              triton::TritonDialect, cf::ControlFlowDialect,"}, {"filename": "python/MANIFEST.in", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -1,2 +1,3 @@\n graft src\n graft triton/third_party\n+graft triton/runtime/backends/"}, {"filename": "python/setup.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -227,7 +227,6 @@ def build_extension(self, ext):\n         \"triton/ops\",\n         \"triton/ops/blocksparse\",\n         \"triton/runtime\",\n-        \"triton/runtime/driver\",\n         \"triton/tools\",\n     ],\n     install_requires=["}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 13, "deletions": 31, "changes": 44, "file_content_changes": "@@ -95,23 +95,6 @@ void init_triton_ir(py::module &&m) {\n       .value(\"EVICT_LAST\", mlir::triton::EvictionPolicy::EVICT_LAST)\n       .export_values();\n \n-  py::enum_<mlir::triton::RedOp>(m, \"REDUCE_OP\")\n-      .value(\"ADD\", mlir::triton::RedOp::ADD)\n-      .value(\"FADD\", mlir::triton::RedOp::FADD)\n-      .value(\"MIN\", mlir::triton::RedOp::MIN)\n-      .value(\"MAX\", mlir::triton::RedOp::MAX)\n-      .value(\"UMIN\", mlir::triton::RedOp::UMIN)\n-      .value(\"UMAX\", mlir::triton::RedOp::UMAX)\n-      .value(\"ARGMIN\", mlir::triton::RedOp::ARGMIN)\n-      .value(\"ARGMAX\", mlir::triton::RedOp::ARGMAX)\n-      .value(\"ARGUMIN\", mlir::triton::RedOp::ARGUMIN)\n-      .value(\"ARGUMAX\", mlir::triton::RedOp::ARGUMAX)\n-      .value(\"FMIN\", mlir::triton::RedOp::FMIN)\n-      .value(\"FMAX\", mlir::triton::RedOp::FMAX)\n-      .value(\"ARGFMIN\", mlir::triton::RedOp::ARGFMIN)\n-      .value(\"ARGFMAX\", mlir::triton::RedOp::ARGFMAX)\n-      .value(\"XOR\", mlir::triton::RedOp::XOR);\n-\n   py::enum_<mlir::triton::RMWOp>(m, \"ATOMIC_OP\")\n       .value(\"ADD\", mlir::triton::RMWOp::ADD)\n       .value(\"FADD\", mlir::triton::RMWOp::FADD)\n@@ -1349,21 +1332,20 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::math::AbsIOp>(loc, val);\n            })\n       .def(\"create_reduce\",\n-           [](mlir::OpBuilder &self, mlir::Value &operand,\n-              mlir::triton::RedOp redOp, int axis) -> mlir::Value {\n-             auto loc = self.getUnknownLoc();\n-             auto inputTensorType =\n-                 operand.getType().dyn_cast<mlir::RankedTensorType>();\n-             std::vector<int64_t> shape = inputTensorType.getShape();\n-             shape.erase(shape.begin() + axis);\n-             bool withIndex = mlir::triton::ReduceOp::withIndex(redOp);\n-             mlir::Type resType = withIndex ? self.getI32Type()\n-                                            : inputTensorType.getElementType();\n-             if (!shape.empty()) {\n-               resType = mlir::RankedTensorType::get(shape, resType);\n+           [](mlir::OpBuilder &self, std::vector<mlir::Value> operands,\n+              int axis) -> mlir::OpState {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::ReduceOp>(loc, operands, axis);\n+           })\n+      .def(\"create_reduce_ret\",\n+           [](mlir::OpBuilder &self, py::args args) -> mlir::OpState {\n+             auto loc = self.getUnknownLoc();\n+             llvm::SmallVector<mlir::Value> return_values;\n+             for (const auto &arg : args) {\n+               return_values.push_back(py::cast<mlir::Value>(arg));\n              }\n-             return self.create<mlir::triton::ReduceOp>(loc, resType, redOp,\n-                                                        operand, axis);\n+             return self.create<mlir::triton::ReduceReturnOp>(loc,\n+                                                              return_values);\n            })\n       .def(\"create_ptr_to_int\",\n            [](mlir::OpBuilder &self, mlir::Value &val,"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 27, "deletions": 19, "changes": 46, "file_content_changes": "@@ -16,6 +16,11 @@\n #######################\n \n \n+def print_perf(cur_ms, cur_util, ref_util):\n+    # print on the same line cur_ms, cur_util and ref_util with 3 decimal places\n+    print(f'{cur_ms:.3f} ms \\t cur: {cur_util:.3f} \\t ref: {ref_util:.3f} \\t dif={cur_util - ref_util:.3f}', end='\\t')\n+\n+\n def nvsmi(attrs):\n     attrs = ','.join(attrs)\n     cmd = ['nvidia-smi', '-i', '0', '--query-gpu=' + attrs, '--format=csv,noheader,nounits']\n@@ -55,21 +60,21 @@ def nvsmi(attrs):\n     # A100 in the CI server is slow-ish for some reason.\n     # On some other servers, we are getting about 90% peak for 8kx8x8k float16\n     'a100': {\n-        (512, 512, 512): {'float16': 0.08, 'float32': 0.13, 'int8': 0.05},\n-        (1024, 1024, 1024): {'float16': 0.33, 'float32': 0.35, 'int8': 0.169},\n-        (2048, 2048, 2048): {'float16': 0.62, 'float32': 0.57, 'int8': 0.34},\n-        (4096, 4096, 4096): {'float16': 0.81, 'float32': 0.75, 'int8': 0.46},\n-        (8192, 8192, 8192): {'float16': 0.77, 'float32': 0.85, 'int8': 0.51},\n+        (512, 512, 512): {'float16': 0.084, 'float32': 0.13, 'int8': 0.05},\n+        (1024, 1024, 1024): {'float16': 0.332, 'float32': 0.35, 'int8': 0.169},\n+        (2048, 2048, 2048): {'float16': 0.641, 'float32': 0.57, 'int8': 0.34},\n+        (4096, 4096, 4096): {'float16': 0.785, 'float32': 0.75, 'int8': 0.46},\n+        (8192, 8192, 8192): {'float16': 0.805, 'float32': 0.85, 'int8': 0.51},\n         # tall-skinny\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n-        (16, 4096, 4096): {'float16': 0.0363, 'float32': 0.0457, 'int8': 0.0259},\n+        (16, 4096, 4096): {'float16': 0.044, 'float32': 0.0457, 'int8': 0.0259},\n         (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n-        (64, 1024, 1024): {'float16': 0.0271, 'float32': 0.0509, 'int8': 0.0169},\n-        (64, 4096, 4096): {'float16': 0.16, 'float32': 0.162, 'int8': 0.097},\n-        (64, 8192, 8192): {'float16': 0.30, 'float32': 0.257, 'int8': 0.174},\n-        (1024, 64, 1024): {'float16': 0.037, 'float32': 0.0458, 'int8': 0.017},\n+        (64, 1024, 1024): {'float16': 0.030, 'float32': 0.0509, 'int8': 0.0169},\n+        (64, 4096, 4096): {'float16': 0.163, 'float32': 0.162, 'int8': 0.097},\n+        (64, 8192, 8192): {'float16': 0.285, 'float32': 0.257, 'int8': 0.174},\n+        (1024, 64, 1024): {'float16': 0.033, 'float32': 0.0458, 'int8': 0.017},\n         (4096, 64, 4096): {'float16': 0.16, 'float32': 0.177, 'int8': 0.102},\n-        (8192, 64, 8192): {'float16': 0.25, 'float32': 0.230, 'int8': 0.177},\n+        (8192, 64, 8192): {'float16': 0.254, 'float32': 0.230, 'int8': 0.177},\n     }\n }\n \n@@ -94,9 +99,10 @@ def test_matmul(M, N, K, dtype_str):\n         a = torch.randn((M, K), dtype=dtype, device='cuda')\n         b = torch.randn((K, N), dtype=dtype, device='cuda')\n     fn = lambda: triton.ops.matmul(a, b)\n-    ms = triton.testing.do_bench(fn, warmup=100, rep=300)\n+    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=300)\n     cur_gpu_perf = 2. * M * N * K / ms * 1e-9\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n \n@@ -129,12 +135,12 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n         1024 * 65536: 0.939,\n     },\n     'a100': {\n-        1024 * 16: 0.008,\n-        1024 * 64: 0.034,\n+        1024 * 16: 0.010,\n+        1024 * 64: 0.040,\n         1024 * 256: 0.132,\n-        1024 * 1024: 0.352,\n-        1024 * 4096: 0.580,\n-        1024 * 16384: 0.782,\n+        1024 * 1024: 0.353,\n+        1024 * 4096: 0.605,\n+        1024 * 16384: 0.758,\n         1024 * 65536: 0.850,\n     }\n }\n@@ -150,9 +156,10 @@ def test_elementwise(N):\n     y = torch.randn_like(z)\n     grid = lambda args: (triton.cdiv(N, args['BLOCK_SIZE']), )\n     fn = lambda: _add[grid](x, y, z, N, BLOCK_SIZE=1024)\n-    ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n+    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n     cur_gpu_perf = 3. * N * z.element_size() / ms * 1e-6\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)\n \n #######################\n@@ -189,7 +196,7 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n         o = fn()\n         do = torch.randn_like(o)\n         fn = lambda: o.backward(do, retain_graph=True)\n-    ms = triton.testing.do_bench(fn, warmup=100, rep=500)\n+    ms = triton.testing.do_bench(fn, return_mode=\"min\", warmup=100, rep=500)\n     # compute flops\n     flops_per_matmul = 2. * Z * H * N_CTX * N_CTX * D_HEAD * 0.5\n     total_flops = 2 * flops_per_matmul\n@@ -201,4 +208,5 @@ def test_flash_attention(Z, H, N_CTX, D_HEAD, mode, dtype_str):\n     max_gpu_perf = get_max_tensorcore_tflops(dtype, clock_rate=cur_sm_clock * 1e3)\n     cur_gpu_util = cur_gpu_perf / max_gpu_perf\n     ref_gpu_util = flash_attention_data[DEVICE_NAME][(Z, H, N_CTX, D_HEAD, mode, dtype_str)]\n+    print_perf(ms, cur_gpu_util, ref_gpu_util)\n     triton.testing.assert_close(cur_gpu_util, ref_gpu_util, atol=0.01, rtol=0.05)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -1295,10 +1295,15 @@ def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n         %12 = tt.addptr %11, {store_range} : tensor<{rdims_2d}x!tt.ptr<f32>, #blocked>, tensor<{rdims_2d}xi32, #blocked>\n         %13 = tt.load %10 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xf32, #blocked>\n         %14 = triton_gpu.convert_layout %13 : (tensor<{M}x{N}xf32, #blocked>) -> tensor<{M}x{N}xf32, #src>\n-        %15 = tt.reduce %14 {{axis = {axis} : i32, redOp = 12 : i32}} : tensor<{M}x{N}xf32, #src> -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n-        %16 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n-        %17 = tt.expand_dims %16 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xf32, #blocked>\n-        tt.store %12, %17 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xf32, #blocked>\n+        %15 = \"tt.reduce\"(%14) ({{\n+        ^bb0(%arg3: f32, %arg4: f32):\n+          %16 = \"triton_gpu.cmpf\"(%arg3, %arg4) {{predicate = 2 : i64}} : (f32, f32) -> i1\n+          %17 = arith.select %16, %arg3, %arg4 : f32\n+          tt.reduce.return %17 : f32\n+        }}) {{axis = {axis} : i32}} : (tensor<{M}x{N}xf32, #src>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>\n+        %18 = triton_gpu.convert_layout %15 : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #src}}>>) -> tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>\n+        %19 = tt.expand_dims %18 {{axis = {axis} : i32}} : (tensor<{rdims_1d}xf32, #triton_gpu.slice<{{dim = {axis}, parent = #blocked}}>>) -> tensor<{rdims_2d}xf32, #blocked>\n+        tt.store %12, %19 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{rdims_2d}xf32, #blocked>\n         tt.return\n     }}\n     }}"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 44, "deletions": 37, "changes": 81, "file_content_changes": "@@ -1,4 +1,5 @@\n import ast\n+import inspect\n import re\n import sys\n import warnings\n@@ -755,6 +756,43 @@ def visit_Assert(self, node) -> Any:\n         # Convert assert to triton's device_assert which happens on the device\n         return language.core.device_assert(test, msg, _builder=self.builder)\n \n+    def call_JitFunction(self, fn: JITFunction, args, kwargs):\n+        args = inspect.getcallargs(fn.fn, *args, **kwargs)\n+        args = [args[name] for name in fn.arg_names]\n+        args = [arg if _is_triton_tensor(arg)\n+                else constexpr(arg) for arg in args]\n+        # generate function def\n+        attributes = dict()\n+        constexprs = [i for i, arg in enumerate(args) if _is_constexpr(arg)]\n+        constants = {i: args[i] for i in constexprs}\n+        # generate call\n+        args = [None if i in constexprs else arg for i, arg in enumerate(args)]\n+        arg_vals = [arg.handle for arg in args if arg is not None]\n+        arg_types = [arg.type for arg in args if arg is not None]\n+        fn_name = mangle_fn(fn.__name__, arg_types, constants)\n+        # generate function def if necessary\n+        if not self.module.has_function(fn_name):\n+            prototype = language.function_type([], arg_types)\n+            gscope = sys.modules[fn.fn.__module__].__dict__\n+            generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=self.debug)\n+            generator.visit(fn.parse())\n+            callee_ret_type = generator.last_ret_type\n+            self.function_ret_types[fn_name] = callee_ret_type\n+        else:\n+            callee_ret_type = self.function_ret_types[fn_name]\n+        symbol = self.module.get_function(fn_name)\n+        call_op = self.builder.call(symbol, arg_vals)\n+        if call_op.get_num_results() == 0 or callee_ret_type is None:\n+            return None\n+        elif call_op.get_num_results() == 1:\n+            return tensor(call_op.get_result(0), callee_ret_type)\n+        else:\n+            # should return a tuple of tl.tensor\n+            results = []\n+            for i in range(call_op.get_num_results()):\n+                results.append(tensor(call_op.get_result(i), callee_ret_type[i]))\n+            return tuple(results)\n+\n     def visit_Call(self, node):\n         fn = _unwrap_if_constexpr(self.visit(node.func))\n \n@@ -768,44 +806,13 @@ def visit_Call(self, node):\n             if not self.debug:\n                 return\n         if isinstance(fn, JITFunction):\n-            from inspect import getcallargs\n-            args = getcallargs(fn.fn, *args, **kws)\n-            args = [args[name] for name in fn.arg_names]\n-            args = [arg if _is_triton_tensor(arg)\n-                    else constexpr(arg) for arg in args]\n-            # generate function def\n-            attributes = dict()\n-            constexprs = [i for i, arg in enumerate(args) if _is_constexpr(arg)]\n-            constants = {i: args[i] for i in constexprs}\n-            # generate call\n-            args = [None if i in constexprs else arg for i, arg in enumerate(args)]\n-            arg_vals = [arg.handle for arg in args if arg is not None]\n-            arg_types = [arg.type for arg in args if arg is not None]\n-            fn_name = mangle_fn(fn.__name__, arg_types, constants)\n-            # generate function def if necessary\n-            if not self.module.has_function(fn_name):\n-                prototype = language.function_type([], arg_types)\n-                gscope = sys.modules[fn.fn.__module__].__dict__\n-                generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types, debug=self.debug)\n-                generator.visit(fn.parse())\n-                callee_ret_type = generator.last_ret_type\n-                self.function_ret_types[fn_name] = callee_ret_type\n-            else:\n-                callee_ret_type = self.function_ret_types[fn_name]\n-            symbol = self.module.get_function(fn_name)\n-            call_op = self.builder.call(symbol, arg_vals)\n-            if call_op.get_num_results() == 0 or callee_ret_type is None:\n-                return None\n-            elif call_op.get_num_results() == 1:\n-                return tensor(call_op.get_result(0), callee_ret_type)\n-            else:\n-                # should return a tuple of tl.tensor\n-                results = []\n-                for i in range(call_op.get_num_results()):\n-                    results.append(tensor(call_op.get_result(i), callee_ret_type[i]))\n-                return tuple(results)\n+            return self.call_JitFunction(fn, args, kws)\n         if (hasattr(fn, '__self__') and _is_triton_tensor(fn.__self__)) or language.core.is_builtin(fn):\n-            return fn(*args, _builder=self.builder, **kws)\n+            extra_kwargs = dict(_builder=self.builder)\n+            sig = inspect.signature(fn)\n+            if '_generator' in sig.parameters:\n+                extra_kwargs['_generator'] = self\n+            return fn(*args, **extra_kwargs, **kws)\n         if fn in self.builtin_namespace.values():\n             args = map(_unwrap_if_constexpr, args)\n         return fn(*args, **kws)"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 9, "deletions": 14, "changes": 23, "file_content_changes": "@@ -15,10 +15,10 @@\n \n import triton\n import triton._C.libtriton.triton as _triton\n+from ..runtime import driver\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n from ..runtime.cache import get_cache_manager\n-from ..runtime.driver import get_cuda_utils, get_hip_utils\n from ..tools.disasm import extract\n from .code_generator import ast_to_ttir\n from .make_launcher import make_stub\n@@ -519,24 +519,19 @@ def __init__(self, fn, so_path, metadata, asm):\n         self.metadata = metadata\n         self.cu_module = None\n         self.cu_function = None\n-        self.is_hip = \"amdgcn\" in asm\n \n     def _init_handles(self):\n         if self.cu_module is not None:\n             return\n         device = triton.runtime.jit.get_current_device()\n-        if self.is_hip:\n-            hip_utils = get_hip_utils()\n-            max_shared = hip_utils.get_device_properties(device)[\"max_shared_mem\"]\n-            if self.shared > max_shared:\n-                raise OutOfResources(self.shared, max_shared, \"shared memory\")\n-            mod, func, n_regs, n_spills = hip_utils.load_binary(self.metadata[\"name\"], self.asm[\"hsaco_path\"], self.shared, device)\n-        else:\n-            cuda_utils = get_cuda_utils()\n-            max_shared = cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n-            if self.shared > max_shared:\n-                raise OutOfResources(self.shared, max_shared, \"shared memory\")\n-            mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n+        bin_path = {\n+            driver.HIP: \"hsaco_path\",\n+            driver.CUDA: \"cubin\"\n+        }[driver.backend]\n+        max_shared = driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n+        if self.shared > max_shared:\n+            raise OutOfResources(self.shared, max_shared, \"shared memory\")\n+        mod, func, n_regs, n_spills = driver.utils.load_binary(self.metadata[\"name\"], self.asm[bin_path], self.shared, device)\n \n         self.n_spills = n_spills\n         self.n_regs = n_regs"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 143, "deletions": 21, "changes": 164, "file_content_changes": "@@ -1,5 +1,6 @@\n from __future__ import annotations\n \n+from contextlib import contextmanager\n from enum import Enum\n from functools import wraps\n from typing import Callable, List, TypeVar\n@@ -1190,46 +1191,167 @@ def _decorator(func: T) -> T:\n     return _decorator\n \n \n+@contextmanager\n+def _insertion_guard(builder):\n+    ip = builder.get_insertion_point()\n+    yield\n+    builder.restore_insertion_point(ip)\n+\n+\n @builtin\n-@_add_reduction_docstr(\"maximum\")\n-def max(input, axis, _builder=None):\n+def reduction(input, axis, combine_fn, _builder=None, _generator=None):\n+    \"\"\"Applies the combine_fn to all elements in :code:`input` tensors along the provided :code:`axis`\n+\n+    :param input: the input tensor, or tuple of tensors\n+    :param axis: the dimension along which the reduction should be done\n+    :param combine_fn: a function to combine two groups of scalar tensors (must be marked with @triton.jit)\n+\n+    \"\"\"\n+    if isinstance(input, tensor):\n+        return reduction((input,), axis, combine_fn,\n+                         _builder=_builder, _generator=_generator)[0]\n+\n+    def make_combine_region(reduce_op):\n+        in_scalar_tys = [t.type.scalar for t in input]\n+        prototype = function_type(in_scalar_tys, in_scalar_tys * 2)\n+\n+        region = reduce_op.get_region(0)\n+        with _insertion_guard(_builder):\n+            param_types = [ty.to_ir(_builder) for ty in prototype.param_types]\n+            block = _builder.create_block_with_parent(region, param_types)\n+            args = [tensor(block.arg(i), ty)\n+                    for i, ty in enumerate(prototype.param_types)]\n+            results = _generator.call_JitFunction(combine_fn, args, kwargs={})\n+            if isinstance(results, tensor):\n+                handles = [results.handle]\n+            else:\n+                handles = [r.handle for r in results]\n+            _builder.create_reduce_ret(*handles)\n+\n     axis = _constexpr_to_value(axis)\n-    return semantic.max(input, axis, _builder)\n+    return semantic.reduction(input, axis, make_combine_region, _builder)\n \n \n @builtin\n-@_add_reduction_docstr(\"maximum index\")\n-def argmax(input, axis, _builder=None):\n-    axis = _constexpr_to_value(axis)\n-    return semantic.argmax(input, axis, _builder)\n+def _promote_reduction_input(t, _builder=None):\n+    scalar_ty = t.type.scalar\n+    # input is extended to 32-bits if necessary\n+    # this increases numerical accuracy and can be done pretty much for free\n+    # on GPUs\n+    if scalar_ty.is_int() and scalar_ty.int_bitwidth < 32:\n+        return t.to(int32, _builder=_builder)\n+\n+    # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n+    if scalar_ty is bfloat16:\n+        return t.to(float32, _builder=_builder)\n+\n+    return t\n \n \n @builtin\n-@_add_reduction_docstr(\"minimum\")\n-def min(input, axis, _builder=None):\n+def _argreduce(input, axis, combine_fn, _builder=None, _generator=None):\n     axis = _constexpr_to_value(axis)\n-    return semantic.min(input, axis, _builder)\n+    n = input.shape[axis]\n+    index = arange(0, n, _builder=_builder)\n \n+    if len(input.shape) > 1:\n+        # Broadcast index across the non-reduced axes\n+        expand_dims_index = [None] * len(input.shape)\n+        expand_dims_index[axis] = slice(None)\n+        index = index.__getitem__(expand_dims_index, _builder=_builder)\n+        index = broadcast_to(index, input.shape, _builder=_builder)\n \n-@builtin\n+    rvalue, rindices = reduction((input, index), axis, combine_fn,\n+                                 _builder=_builder, _generator=_generator)\n+    return rindices\n+\n+\n+@triton.jit\n+def _max_combine(a, b):\n+    return maximum(a, b)\n+\n+\n+@triton.jit\n+@_add_reduction_docstr(\"maximum\")\n+def max(input, axis):\n+    input = _promote_reduction_input(input)\n+    return reduction(input, axis, _max_combine)\n+\n+\n+@triton.jit\n+def _argmax_combine(value1, index1, value2, index2):\n+    gt = value1 > value2\n+    lt = value1 < value2\n+    index_min = minimum(index1, index2)\n+    index_ret = where(gt, index1, where(lt, index2, index_min))\n+    value_ret = maximum(value1, value2)\n+    return value_ret, index_ret\n+\n+\n+@triton.jit\n+@_add_reduction_docstr(\"maximum index\")\n+def argmax(input, axis):\n+    input = _promote_reduction_input(input)\n+    return _argreduce(input, axis, _argmax_combine)\n+\n+\n+@triton.jit\n+def _min_combine(a, b):\n+    # TODO: minimum/maximum doesn't get lowered to fmin/fmax...\n+    return minimum(a, b)\n+\n+\n+@triton.jit\n+@_add_reduction_docstr(\"minimum\")\n+def min(input, axis):\n+    input = _promote_reduction_input(input)\n+    return reduction(input, axis, _min_combine)\n+\n+\n+@triton.jit\n+def _argmin_combine(value1, index1, value2, index2):\n+    lt = value1 < value2\n+    gt = value1 > value2\n+    index_min = minimum(index1, index2)\n+    index_ret = where(lt, index1, where(gt, index2, index_min))\n+    value_ret = minimum(value1, value2)\n+    return value_ret, index_ret\n+\n+\n+@triton.jit\n @_add_reduction_docstr(\"minimum index\")\n-def argmin(input, axis, _builder=None):\n-    axis = _constexpr_to_value(axis)\n-    return semantic.argmin(input, axis, _builder)\n+def argmin(input, axis):\n+    input = _promote_reduction_input(input)\n+    return _argreduce(input, axis, _argmin_combine)\n \n \n-@builtin\n+@triton.jit\n+def _sum_combine(a, b):\n+    return a + b\n+\n+\n+@triton.jit\n @_add_reduction_docstr(\"sum\")\n-def sum(input, axis, _builder=None):\n-    axis = _constexpr_to_value(axis)\n-    return semantic.sum(input, axis, _builder)\n+def sum(input, axis):\n+    input = _promote_reduction_input(input)\n+    return reduction(input, axis, _sum_combine)\n+\n+\n+@triton.jit\n+def _xor_combine(a, b):\n+    return a ^ b\n \n \n @builtin\n @_add_reduction_docstr(\"xor sum\")\n-def xor_sum(input, axis, _builder=None):\n-    axis = _constexpr_to_value(axis)\n-    return semantic.xor_sum(input, axis, _builder)\n+def xor_sum(input, axis, _builder=None, _generator=None):\n+    scalar_ty = input.type.scalar\n+    if not scalar_ty.is_int():\n+        raise ValueError(\"xor_sum only supported for integers\")\n+\n+    input = _promote_reduction_input(input, _builder=_builder)\n+    return reduction(input, axis, _xor_combine,\n+                     _builder=_builder, _generator=_generator)\n \n \n # -----------------------"}, {"filename": "python/triton/language/math.py", "status": "modified", "additions": 2, "deletions": 7, "changes": 9, "file_content_changes": "@@ -1,14 +1,9 @@\n import os\n \n-import torch\n-\n+from ..runtime import driver\n from . import core, extern\n \n-if torch.version.hip is not None:\n-    LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"cuda2gcn.bc\")\n-else:\n-    LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\", \"cuda\", \"lib\", \"libdevice.10.bc\")\n-LIBDEVICE_PATH = os.getenv(\"TRITON_LIBDEVICE_PATH\", LOCAL_PATH)\n+LIBDEVICE_PATH = os.getenv(\"TRITON_LIBDEVICE_PATH\", driver.libdevice_path)\n \n \n @extern.extern"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 27, "deletions": 82, "changes": 109, "file_content_changes": "@@ -1,7 +1,7 @@\n from __future__ import annotations  # remove after python 3.11\n \n from functools import wraps\n-from typing import List, Optional, Tuple, TypeVar\n+from typing import List, Optional, Sequence, Tuple, TypeVar\n \n from . import core as tl\n from triton._C.libtriton.triton import ir\n@@ -1228,91 +1228,36 @@ def where(condition: tl.tensor,\n     return tl.tensor(builder.create_select(condition.handle, x.handle, y.handle), ret_ty)\n \n # ===----------------------------------------------------------------------===//\n-#                               Reductions\n+#                               Reduction\n # ===----------------------------------------------------------------------===\n \n \n-def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n-                FLOAT_OP: ir.REDUCE_OP, INT_OP: ir.REDUCE_OP) -> tl.tensor:\n-    scalar_ty = input.type.scalar\n-    out_scalar_ty = scalar_ty\n-    # input is extended to 32-bits if necessary\n-    # this increases numerical accuracy and can be done pretty much for free\n-    # on GPUs\n-    if scalar_ty.is_int() and scalar_ty.int_bitwidth <= 32:\n-        input = cast(input, tl.int32, builder)\n-        out_scalar_ty = tl.int32\n-\n-    # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n-    if scalar_ty is tl.bfloat16:\n-        input = cast(input, tl.float32, builder)\n-        out_scalar_ty = tl.float32\n-\n-    # choose the right unsigned operation\n-    if scalar_ty.is_int_unsigned():\n-        int_op_to_unit = {\n-            ir.REDUCE_OP.MIN: ir.REDUCE_OP.UMIN,\n-            ir.REDUCE_OP.MAX: ir.REDUCE_OP.UMAX,\n-            ir.REDUCE_OP.ARGMIN: ir.REDUCE_OP.ARGUMIN,\n-            ir.REDUCE_OP.ARGMAX: ir.REDUCE_OP.ARGUMAX,\n-        }\n-        if INT_OP in int_op_to_unit:\n-            INT_OP = int_op_to_unit[INT_OP]\n-\n-    # If we are doing an argmin or argmax we want to use an int32 output type\n-    if FLOAT_OP is ir.REDUCE_OP.ARGFMAX or INT_OP is ir.REDUCE_OP.ARGMAX:\n-        out_scalar_ty = tl.int32\n-    elif FLOAT_OP is ir.REDUCE_OP.ARGFMIN or INT_OP is ir.REDUCE_OP.ARGMIN:\n-        out_scalar_ty = tl.int32\n-\n-    # get result type\n-    shape = input.type.shape\n-\n-    rank = len(shape)\n-    assert 0 <= axis < rank, f\"axis (v={axis}) is out of range, should be within [0, {rank})\"\n-\n-    ret_shape = []\n-    for i, s in enumerate(shape):\n-        if i != axis:\n-            ret_shape.append(s)\n-    if ret_shape:\n-        res_ty = tl.block_type(out_scalar_ty, ret_shape)\n-    else:\n-        # 0d-tensor -> scalar\n-        res_ty = out_scalar_ty\n-\n-    if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_reduce(input.handle, FLOAT_OP, axis), res_ty)\n-    elif scalar_ty.is_int():\n-        return tl.tensor(builder.create_reduce(input.handle, INT_OP, axis), res_ty)\n-    assert False\n-\n-\n-def min(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    return reduce_impl(input, axis, builder, \"min\", ir.REDUCE_OP.FMIN, ir.REDUCE_OP.MIN)\n-\n+def reduction(\n+    inputs: Sequence[tl.tensor], axis: int, region_builder_fn, builder: ir.builder\n+) -> Tuple[tl.tensor, ...]:\n+    # get result shape\n+    shape = inputs[0].type.shape\n+    print(shape, axis)\n+    ret_shape = [s for i, s in enumerate(shape) if i != axis]\n+    for t in inputs:\n+        assert t.type.shape == shape\n \n-def argmin(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    return reduce_impl(input, axis, builder, \"argmin\", ir.REDUCE_OP.ARGFMIN, ir.REDUCE_OP.ARGMIN)\n-\n-\n-def max(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    return reduce_impl(input, axis, builder, \"max\", ir.REDUCE_OP.FMAX, ir.REDUCE_OP.MAX)\n-\n-\n-def argmax(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    return reduce_impl(input, axis, builder, \"argmax\", ir.REDUCE_OP.ARGFMAX, ir.REDUCE_OP.ARGMAX)\n-\n-\n-def sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.FADD, ir.REDUCE_OP.ADD)\n-\n-\n-def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    scalar_ty = input.type.scalar\n-    if not scalar_ty.is_int():\n-        raise ValueError(\"xor_sum only supported for integers\")\n-    return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.XOR, ir.REDUCE_OP.XOR)\n+    def wrap_tensor(x, scalar_ty):\n+        if ret_shape:\n+            res_ty = tl.block_type(scalar_ty, ret_shape)\n+        else:\n+            # 0d-tensor -> scalar\n+            res_ty = scalar_ty\n+        return tl.tensor(x, res_ty)\n+\n+    reduce_op = builder.create_reduce([t.handle for t in inputs], axis)\n+    region_builder_fn(reduce_op)\n+    reduce_op.verify()\n+\n+    return tuple(\n+        wrap_tensor(reduce_op.get_result(i), inputs[i].type.scalar)\n+        for i in range(len(inputs))\n+    )\n \n \n # ===----------------------------------------------------------------------==="}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 5, "deletions": 10, "changes": 15, "file_content_changes": "@@ -4,24 +4,22 @@\n \n import triton\n import triton._C.libtriton.triton as _triton\n-from triton.runtime.driver.cuda import get_cuda_utils\n+from triton.runtime import driver\n from triton.testing import get_dram_gbps, get_max_simd_tflops, get_max_tensorcore_tflops\n \n \n def get_tensorcore_tflops(backend, device, num_ctas, num_warps, dtype):\n     ''' return compute throughput in TOPS '''\n     total_warps = num_ctas * min(num_warps, 4)\n-    cuda_utils = get_cuda_utils()\n-    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n+    num_subcores = driver.utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n     tflops = min(num_subcores, total_warps) / num_subcores * get_max_tensorcore_tflops(dtype, backend, device)\n     return tflops\n \n \n def get_simd_tflops(backend, device, num_ctas, num_warps, dtype):\n     ''' return compute throughput in TOPS '''\n     total_warps = num_ctas * min(num_warps, 4)\n-    cuda_utils = get_cuda_utils()\n-    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n+    num_subcores = driver.utils.get_device_properties(device)[\"multiprocessor_count\"] * 4  # on recent GPUs\n     tflops = min(num_subcores, total_warps) / num_subcores * get_max_simd_tflops(dtype, backend, device)\n     return tflops\n \n@@ -62,8 +60,7 @@ def estimate_matmul_time(\n     compute_ms = total_ops / tput\n \n     # time to load data\n-    cuda_utils = get_cuda_utils()\n-    num_sm = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"]\n+    num_sm = driver.utils.get_device_properties(device)[\"multiprocessor_count\"]\n     active_cta_ratio = min(1, num_ctas / num_sm)\n     active_cta_ratio_bw1 = min(1, num_ctas / 32)  # 32 active ctas are enough to saturate\n     active_cta_ratio_bw2 = max(min(1, (num_ctas - 32) / (108 - 32)), 0)  # 32-108, remaining 5%\n@@ -114,9 +111,7 @@ def early_config_prune(configs, named_args):\n         BLOCK_M, BLOCK_N, BLOCK_K, num_stages = \\\n             kw['BLOCK_M'], kw['BLOCK_N'], kw['BLOCK_K'], config.num_stages\n \n-        # TODO: move to `cuda_utils` submodule\n-        cuda_utils = get_cuda_utils()\n-        max_shared_memory = cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n+        max_shared_memory = driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n         required_shared_memory = (BLOCK_M + BLOCK_N) * BLOCK_K * num_stages * dtsize\n         if required_shared_memory <= max_shared_memory:\n             pruned_configs.append(config)"}, {"filename": "python/triton/runtime/__init__.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,6 +1,6 @@\n-from . import driver\n from .autotuner import (Autotuner, Config, Heuristics, OutOfResources, autotune,\n                         heuristics)\n+from .driver import driver\n from .jit import (JITFunction, KernelInterface, MockTensor, TensorWrapper, reinterpret,\n                   version_key)\n "}, {"filename": "python/triton/runtime/backends/cuda.c", "status": "added", "additions": 124, "deletions": 0, "changes": 124, "file_content_changes": "@@ -0,0 +1,124 @@\n+#include \"cuda.h\"\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+\n+static inline void gpuAssert(CUresult code, const char *file, int line) {\n+  if (code != CUDA_SUCCESS) {\n+    const char *prefix = \"Triton Error [CUDA]: \";\n+    const char *str;\n+    cuGetErrorString(code, &str);\n+    char err[1024] = {0};\n+    strcat(err, prefix);\n+    strcat(err, str);\n+    PyErr_SetString(PyExc_RuntimeError, err);\n+  }\n+}\n+\n+#define CUDA_CHECK(ans)                                                        \\\n+  {                                                                            \\\n+    gpuAssert((ans), __FILE__, __LINE__);                                      \\\n+    if (PyErr_Occurred())                                                      \\\n+      return NULL;                                                             \\\n+  }\n+\n+static PyObject *getDeviceProperties(PyObject *self, PyObject *args) {\n+  int device_id;\n+  if (!PyArg_ParseTuple(args, \"i\", &device_id))\n+    return NULL;\n+  // Get device handle\n+  CUdevice device;\n+  cuDeviceGet(&device, device_id);\n+\n+  // create a struct to hold device properties\n+  int max_shared_mem;\n+  int multiprocessor_count;\n+  int sm_clock_rate;\n+  int mem_clock_rate;\n+  int mem_bus_width;\n+  CUDA_CHECK(cuDeviceGetAttribute(\n+      &max_shared_mem, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN,\n+      device));\n+  CUDA_CHECK(cuDeviceGetAttribute(\n+      &multiprocessor_count, CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, device));\n+  CUDA_CHECK(cuDeviceGetAttribute(&sm_clock_rate,\n+                                  CU_DEVICE_ATTRIBUTE_CLOCK_RATE, device));\n+  CUDA_CHECK(cuDeviceGetAttribute(\n+      &mem_clock_rate, CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE, device));\n+  CUDA_CHECK(cuDeviceGetAttribute(\n+      &mem_bus_width, CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH, device));\n+\n+  return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\",\n+                       max_shared_mem, \"multiprocessor_count\",\n+                       multiprocessor_count, \"sm_clock_rate\", sm_clock_rate,\n+                       \"mem_clock_rate\", mem_clock_rate, \"mem_bus_width\",\n+                       mem_bus_width);\n+}\n+\n+static PyObject *loadBinary(PyObject *self, PyObject *args) {\n+  const char *name;\n+  const char *data;\n+  Py_ssize_t data_size;\n+  int shared;\n+  int device;\n+  if (!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared,\n+                        &device)) {\n+    return NULL;\n+  }\n+  CUfunction fun;\n+  CUmodule mod;\n+  int32_t n_regs = 0;\n+  int32_t n_spills = 0;\n+  // create driver handles\n+  CUDA_CHECK(cuModuleLoadData(&mod, data));\n+  CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n+  // get allocated registers and spilled registers from the function\n+  CUDA_CHECK(cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS, fun));\n+  CUDA_CHECK(\n+      cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun));\n+  n_spills /= 4;\n+  // set dynamic shared memory if necessary\n+  int shared_optin;\n+  CUDA_CHECK(cuDeviceGetAttribute(\n+      &shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN,\n+      device));\n+  if (shared > 49152 && shared_optin > 49152) {\n+    CUDA_CHECK(cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED));\n+    int shared_total, shared_static;\n+    CUDA_CHECK(cuDeviceGetAttribute(\n+        &shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR,\n+        device));\n+    CUDA_CHECK(cuFuncGetAttribute(&shared_static,\n+                                  CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n+    CUDA_CHECK(\n+        cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n+                           shared_optin - shared_static));\n+  }\n+\n+  if (PyErr_Occurred()) {\n+    return NULL;\n+  }\n+  return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs,\n+                       n_spills);\n+}\n+\n+static PyMethodDef ModuleMethods[] = {\n+    {\"load_binary\", loadBinary, METH_VARARGS,\n+     \"Load provided cubin into CUDA driver\"},\n+    {\"get_device_properties\", getDeviceProperties, METH_VARARGS,\n+     \"Get the properties for a given device\"},\n+    {NULL, NULL, 0, NULL} // sentinel\n+};\n+\n+static struct PyModuleDef ModuleDef = {PyModuleDef_HEAD_INIT, \"cuda_utils\",\n+                                       NULL, // documentation\n+                                       -1,   // size\n+                                       ModuleMethods};\n+\n+PyMODINIT_FUNC PyInit_cuda_utils(void) {\n+  PyObject *m = PyModule_Create(&ModuleDef);\n+  if (m == NULL) {\n+    return NULL;\n+  }\n+  PyModule_AddFunctions(m, ModuleMethods);\n+  return m;\n+}"}, {"filename": "python/triton/runtime/backends/hip.c", "status": "added", "additions": 120, "deletions": 0, "changes": 120, "file_content_changes": "@@ -0,0 +1,120 @@\n+#define __HIP_PLATFORM_AMD__\n+#include <hip/hip_runtime.h>\n+#define PY_SSIZE_T_CLEAN\n+#include <Python.h>\n+#include <stdio.h>\n+#include <stdlib.h>\n+\n+static inline void gpuAssert(hipError_t code, const char *file, int line) {\n+  {\n+    if (code != HIP_SUCCESS) {\n+      {\n+        const char *prefix = \"Triton Error [HIP]: \";\n+        const char *str = hipGetErrorString(code);\n+        char err[1024] = {0};\n+        snprintf(err, 1024, \"%s Code: %d, Messsage: %s\", prefix, code, str);\n+        PyErr_SetString(PyExc_RuntimeError, err);\n+      }\n+    }\n+  }\n+}\n+\n+#define HIP_CHECK(ans)                                                         \\\n+  {                                                                            \\\n+    gpuAssert((ans), __FILE__, __LINE__);                                      \\\n+    if (PyErr_Occurred())                                                      \\\n+      return NULL;                                                             \\\n+  }\n+\n+static PyObject *getDeviceProperties(PyObject *self, PyObject *args) {\n+  int device_id;\n+  if (!PyArg_ParseTuple(args, \"i\", &device_id))\n+    return NULL;\n+\n+  hipDeviceProp_t props;\n+  HIP_CHECK(hipGetDeviceProperties(&props, device_id));\n+\n+  // create a struct to hold device properties\n+  return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\",\n+                       props.sharedMemPerBlock, \"multiprocessor_count\",\n+                       props.multiProcessorCount, \"sm_clock_rate\",\n+                       props.clockRate, \"mem_clock_rate\", props.memoryClockRate,\n+                       \"mem_bus_width\", props.memoryBusWidth);\n+}\n+\n+static PyObject *loadBinary(PyObject *self, PyObject *args) {\n+  const char *name;\n+  const char *data;\n+  Py_ssize_t data_size;\n+  int shared;\n+  int device;\n+  if (!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared,\n+                        &device)) {\n+    return NULL;\n+  }\n+\n+  // Open HSACO file\n+  FILE *hsaco_file;\n+  if ((hsaco_file = fopen(data, \"rb\")) == NULL) {\n+    return NULL;\n+  }\n+\n+  // Read HSCAO file into Buffer\n+  fseek(hsaco_file, 0L, SEEK_END);\n+  size_t hsaco_file_size = ftell(hsaco_file);\n+  unsigned char *hsaco =\n+      (unsigned char *)malloc(hsaco_file_size * sizeof(unsigned char));\n+  rewind(hsaco_file);\n+  fread(hsaco, sizeof(unsigned char), hsaco_file_size, hsaco_file);\n+  fclose(hsaco_file);\n+\n+  // set HIP options\n+  hipJitOption opt[] = {hipJitOptionErrorLogBufferSizeBytes,\n+                        hipJitOptionErrorLogBuffer,\n+                        hipJitOptionInfoLogBufferSizeBytes,\n+                        hipJitOptionInfoLogBuffer, hipJitOptionLogVerbose};\n+  const unsigned int errbufsize = 8192;\n+  const unsigned int logbufsize = 8192;\n+  char _err[errbufsize];\n+  char _log[logbufsize];\n+  void *optval[] = {(void *)(uintptr_t)errbufsize, (void *)_err,\n+                    (void *)(uintptr_t)logbufsize, (void *)_log, (void *)1};\n+\n+  // launch HIP Binary\n+  hipModule_t mod;\n+  hipFunction_t fun;\n+  hipModuleLoadDataEx(&mod, hsaco, 5, opt, optval);\n+  hipModuleGetFunction(&fun, mod, name);\n+  free(hsaco);\n+\n+  // get allocated registers and spilled registers from the function\n+  int n_regs = 0;\n+  int n_spills = 0;\n+  if (PyErr_Occurred()) {\n+    return NULL;\n+  }\n+  return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs,\n+                       n_spills);\n+}\n+\n+static PyMethodDef ModuleMethods[] = {\n+    {\"load_binary\", loadBinary, METH_VARARGS,\n+     \"Load provided hsaco into HIP driver\"},\n+    {\"get_device_properties\", getDeviceProperties, METH_VARARGS,\n+     \"Get the properties for a given device\"},\n+    {NULL, NULL, 0, NULL} // sentinel\n+};\n+\n+static struct PyModuleDef ModuleDef = {PyModuleDef_HEAD_INIT, \"hip_utils\",\n+                                       NULL, // documentation\n+                                       -1,   // size\n+                                       ModuleMethods};\n+\n+PyMODINIT_FUNC PyInit_hip_utils(void) {\n+  PyObject *m = PyModule_Create(&ModuleDef);\n+  if (m == NULL) {\n+    return NULL;\n+  }\n+  PyModule_AddFunctions(m, ModuleMethods);\n+  return m;\n+}"}, {"filename": "python/triton/runtime/driver.py", "status": "added", "additions": 149, "deletions": 0, "changes": 149, "file_content_changes": "@@ -0,0 +1,149 @@\n+import abc\n+import hashlib\n+import os\n+import tempfile\n+from pathlib import Path\n+\n+from ..common.build import _build\n+from .cache import get_cache_manager\n+\n+\n+class DriverBase(metaclass=abc.ABCMeta):\n+\n+    CUDA = 0\n+    HIP = 1\n+\n+    @staticmethod\n+    def third_party_dir():\n+        return os.path.join(os.path.dirname(os.path.abspath(__file__)), \"..\", \"third_party\")\n+\n+    def __init__(self) -> None:\n+        pass\n+# -----------------------------\n+# CUDA\n+# -----------------------------\n+\n+\n+class CudaUtils(object):\n+\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(CudaUtils, cls).__new__(cls)\n+        return cls.instance\n+\n+    def __init__(self):\n+        dirname = os.path.dirname(os.path.realpath(__file__))\n+        src = Path(os.path.join(dirname, \"backends\", \"cuda.c\")).read_text()\n+        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n+        cache = get_cache_manager(key)\n+        fname = \"cuda_utils.so\"\n+        cache_path = cache.get_file(fname)\n+        if cache_path is None:\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                src_path = os.path.join(tmpdir, \"main.c\")\n+                with open(src_path, \"w\") as f:\n+                    f.write(src)\n+                so = _build(\"cuda_utils\", src_path, tmpdir)\n+                with open(so, \"rb\") as f:\n+                    cache_path = cache.put(f.read(), fname, binary=True)\n+        import importlib.util\n+        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache_path)\n+        mod = importlib.util.module_from_spec(spec)\n+        spec.loader.exec_module(mod)\n+        self.load_binary = mod.load_binary\n+        self.get_device_properties = mod.get_device_properties\n+\n+\n+class CudaDriver(DriverBase):\n+\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(CudaDriver, cls).__new__(cls)\n+        return cls.instance\n+\n+    def get_libdevice_path(self):\n+        return os.path.join(self.third_party_dir(), \"cuda\", \"lib\", \"libdevice.10.bc\")\n+\n+    def __init__(self):\n+        self.utils = CudaUtils()\n+        self.backend = self.CUDA\n+        self.libdevice_path = self.get_libdevice_path()\n+\n+# -----------------------------\n+# HIP\n+# -----------------------------\n+\n+\n+class HIPUtils(object):\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(HIPUtils, cls).__new__(cls)\n+        return cls.instance\n+\n+    def __init__(self):\n+        dirname = os.path.dirname(os.path.realpath(__file__))\n+        src = Path(os.path.join(dirname, \"backends\", \"hip.c\")).read_text()\n+        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n+        cache = get_cache_manager(key)\n+        fname = \"hip_utils.so\"\n+        cache_path = cache.get_file(fname)\n+        if cache_path is None:\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                src_path = os.path.join(tmpdir, \"main.c\")\n+                with open(src_path, \"w\") as f:\n+                    f.write(src)\n+                so = _build(\"hip_utils\", src_path, tmpdir)\n+                with open(so, \"rb\") as f:\n+                    cache_path = cache.put(f.read(), fname, binary=True)\n+        import importlib.util\n+        spec = importlib.util.spec_from_file_location(\"hip_utils\", cache_path)\n+        mod = importlib.util.module_from_spec(spec)\n+        spec.loader.exec_module(mod)\n+        self.load_binary = mod.load_binary\n+        self.get_device_properties = mod.get_device_properties\n+\n+\n+class HIPDriver(DriverBase):\n+\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(HIPDriver, cls).__new__(cls)\n+        return cls.instance\n+\n+    def get_libdevice_path(self):\n+        return os.path.join(self.third_party_dir(), \"third_party\", \"rocm\", \"lib\", \"libdevice.10.bc\")\n+\n+    def __init__(self):\n+        self.utils = HIPUtils()\n+        self.backend = self.HIP\n+        self.libdevice_path = self.get_libdevice_path()\n+\n+\n+class UnsupportedDriver(DriverBase):\n+\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(UnsupportedDriver, cls).__new__(cls)\n+        return cls.instance\n+\n+    def __init__(self):\n+        self.utils = None\n+        self.backend = None\n+        self.libdevice_path = ''\n+\n+# -----------------------------\n+# Driver\n+# -----------------------------\n+\n+\n+def create_driver():\n+    import torch\n+    if torch.version.hip is not None:\n+        return HIPDriver()\n+    elif torch.cuda.is_available():\n+        return CudaDriver()\n+    else:\n+        return UnsupportedDriver()\n+\n+\n+driver = create_driver()"}, {"filename": "python/triton/runtime/driver/__init__.py", "status": "removed", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -1,4 +0,0 @@\n-from .cuda import get_cuda_utils\n-from .hip import get_hip_utils\n-\n-__all__ = ['get_cuda_utils', 'get_hip_utils']"}, {"filename": "python/triton/runtime/driver/cuda.py", "status": "removed", "additions": 0, "deletions": 159, "changes": 159, "file_content_changes": "@@ -1,159 +0,0 @@\n-import hashlib\n-import os\n-import tempfile\n-\n-from ...common.build import _build\n-from ..cache import get_cache_manager\n-\n-\n-def get_cuda_utils():\n-    global _cuda_utils\n-    if _cuda_utils is None:\n-        _cuda_utils = CudaUtils()\n-    return _cuda_utils\n-\n-\n-_cuda_utils = None\n-\n-\n-class CudaUtils(object):\n-\n-    def __new__(cls):\n-        if not hasattr(cls, 'instance'):\n-            cls.instance = super(CudaUtils, cls).__new__(cls)\n-        return cls.instance\n-\n-    @staticmethod\n-    def _generate_src():\n-        return \"\"\"\n-        #include <cuda.h>\n-\n-        #include \\\"cuda.h\\\"\n-        #define PY_SSIZE_T_CLEAN\n-        #include <Python.h>\n-\n-        static inline void gpuAssert(CUresult code, const char *file, int line)\n-        {\n-           if (code != CUDA_SUCCESS)\n-           {\n-              const char* prefix = \"Triton Error [CUDA]: \";\n-              const char* str;\n-              cuGetErrorString(code, &str);\n-              char err[1024] = {0};\n-              strcat(err, prefix);\n-              strcat(err, str);\n-              PyErr_SetString(PyExc_RuntimeError, err);\n-           }\n-        }\n-\n-        #define CUDA_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); if(PyErr_Occurred()) return NULL; }\n-\n-        static PyObject* getDeviceProperties(PyObject* self, PyObject* args){\n-            int device_id;\n-            if(!PyArg_ParseTuple(args, \"i\", &device_id))\n-                return NULL;\n-            // Get device handle\n-            CUdevice device;\n-            cuDeviceGet(&device, device_id);\n-\n-            // create a struct to hold device properties\n-            int max_shared_mem;\n-            int multiprocessor_count;\n-            int sm_clock_rate;\n-            int mem_clock_rate;\n-            int mem_bus_width;\n-            CUDA_CHECK(cuDeviceGetAttribute(&max_shared_mem, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device));\n-            CUDA_CHECK(cuDeviceGetAttribute(&multiprocessor_count, CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, device));\n-            CUDA_CHECK(cuDeviceGetAttribute(&sm_clock_rate, CU_DEVICE_ATTRIBUTE_CLOCK_RATE, device));\n-            CUDA_CHECK(cuDeviceGetAttribute(&mem_clock_rate, CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE, device));\n-            CUDA_CHECK(cuDeviceGetAttribute(&mem_bus_width, CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH, device));\n-\n-\n-            return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\", max_shared_mem,\n-                                       \"multiprocessor_count\", multiprocessor_count,\n-                                       \"sm_clock_rate\", sm_clock_rate,\n-                                       \"mem_clock_rate\", mem_clock_rate,\n-                                       \"mem_bus_width\", mem_bus_width);\n-        }\n-\n-        static PyObject* loadBinary(PyObject* self, PyObject* args) {\n-            const char* name;\n-            const char* data;\n-            Py_ssize_t data_size;\n-            int shared;\n-            int device;\n-            if(!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared, &device)) {\n-                return NULL;\n-            }\n-            CUfunction fun;\n-            CUmodule mod;\n-            int32_t n_regs = 0;\n-            int32_t n_spills = 0;\n-            // create driver handles\n-            CUDA_CHECK(cuModuleLoadData(&mod, data));\n-            CUDA_CHECK(cuModuleGetFunction(&fun, mod, name));\n-            // get allocated registers and spilled registers from the function\n-            CUDA_CHECK(cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS, fun));\n-            CUDA_CHECK(cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun));\n-            n_spills /= 4;\n-            // set dynamic shared memory if necessary\n-            int shared_optin;\n-            CUDA_CHECK(cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device));\n-            if (shared > 49152 && shared_optin > 49152) {\n-              CUDA_CHECK(cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED));\n-              int shared_total, shared_static;\n-              CUDA_CHECK(cuDeviceGetAttribute(&shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, device));\n-              CUDA_CHECK(cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun));\n-              CUDA_CHECK(cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static));\n-            }\n-\n-            if(PyErr_Occurred()) {\n-              return NULL;\n-            }\n-            return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs, n_spills);\n-        }\n-\n-        static PyMethodDef ModuleMethods[] = {\n-          {\"load_binary\", loadBinary, METH_VARARGS, \"Load provided cubin into CUDA driver\"},\n-          {\"get_device_properties\", getDeviceProperties, METH_VARARGS, \"Get the properties for a given device\"},\n-          {NULL, NULL, 0, NULL} // sentinel\n-        };\n-\n-        static struct PyModuleDef ModuleDef = {\n-          PyModuleDef_HEAD_INIT,\n-          \\\"cuda_utils\\\",\n-          NULL, //documentation\n-          -1, //size\n-          ModuleMethods\n-        };\n-\n-        PyMODINIT_FUNC PyInit_cuda_utils(void) {\n-          PyObject *m = PyModule_Create(&ModuleDef);\n-          if(m == NULL) {\n-            return NULL;\n-          }\n-          PyModule_AddFunctions(m, ModuleMethods);\n-          return m;\n-        }\n-        \"\"\"\n-\n-    def __init__(self):\n-        src = self._generate_src()\n-        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n-        cache = get_cache_manager(key)\n-        fname = \"cuda_utils.so\"\n-        cache_path = cache.get_file(fname)\n-        if cache_path is None:\n-            with tempfile.TemporaryDirectory() as tmpdir:\n-                src_path = os.path.join(tmpdir, \"main.c\")\n-                with open(src_path, \"w\") as f:\n-                    f.write(src)\n-                so = _build(\"cuda_utils\", src_path, tmpdir)\n-                with open(so, \"rb\") as f:\n-                    cache_path = cache.put(f.read(), fname, binary=True)\n-        import importlib.util\n-        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache_path)\n-        mod = importlib.util.module_from_spec(spec)\n-        spec.loader.exec_module(mod)\n-        self.load_binary = mod.load_binary\n-        self.get_device_properties = mod.get_device_properties"}, {"filename": "python/triton/runtime/driver/hip.py", "status": "removed", "additions": 0, "deletions": 158, "changes": 158, "file_content_changes": "@@ -1,158 +0,0 @@\n-import hashlib\n-import os\n-import tempfile\n-\n-from ...common.build import _build\n-from ..cache import get_cache_manager\n-\n-\n-def get_hip_utils():\n-    global _hip_utils\n-    if _hip_utils is None:\n-        _hip_utils = HIPUtils()\n-    return _hip_utils\n-\n-\n-_hip_utils = None\n-\n-\n-class HIPUtils(object):\n-    def __new__(cls):\n-        if not hasattr(cls, 'instance'):\n-            cls.instance = super(HIPUtils, cls).__new__(cls)\n-        return cls.instance\n-\n-    def _generate_src(self):\n-        return \"\"\"\n-        #define __HIP_PLATFORM_AMD__\n-        #include <hip/hip_runtime.h>\n-        #define PY_SSIZE_T_CLEAN\n-        #include <Python.h>\n-        #include <stdio.h>\n-        #include <stdlib.h>\n-        static inline void gpuAssert(hipError_t code, const char *file, int line)\n-        {{\n-          if (code != HIP_SUCCESS)\n-          {{\n-             const char* prefix = \"Triton Error [HIP]: \";\n-             const char* str = hipGetErrorString(code);\n-             char err[1024] = {0};\n-             snprintf(err, 1024, \"%s Code: %d, Messsage: %s\", prefix, code, str );\n-             PyErr_SetString(PyExc_RuntimeError, err);\n-          }}\n-        }}\n-\n-        #define HIP_CHECK(ans) { gpuAssert((ans), __FILE__, __LINE__); if (PyErr_Occurred()) return NULL; }\n-\n-        static PyObject* getDeviceProperties(PyObject* self, PyObject* args){\n-            int device_id;\n-            if (!PyArg_ParseTuple(args, \"i\", &device_id))\n-                return NULL;\n-\n-            hipDeviceProp_t props;\n-            HIP_CHECK(hipGetDeviceProperties(&props, device_id));\n-\n-            // create a struct to hold device properties\n-            return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\", props.sharedMemPerBlock,\n-                                       \"multiprocessor_count\", props.multiProcessorCount,\n-                                       \"sm_clock_rate\", props.clockRate,\n-                                       \"mem_clock_rate\", props.memoryClockRate,\n-                                       \"mem_bus_width\", props.memoryBusWidth);\n-        }\n-\n-        static PyObject* loadBinary(PyObject* self, PyObject* args) {\n-            const char* name;\n-            const char* data;\n-            Py_ssize_t data_size;\n-            int shared;\n-            int device;\n-            if (!PyArg_ParseTuple(args, \"ss#ii\", &name, &data, &data_size, &shared, &device)) {\n-                return NULL;\n-            }\n-\n-            // Open HSACO file\n-            FILE* hsaco_file;\n-            if ((hsaco_file = fopen(data, \"rb\")) == NULL) {\n-                return NULL;\n-            }\n-\n-            // Read HSCAO file into Buffer\n-            fseek(hsaco_file, 0L, SEEK_END);\n-            size_t hsaco_file_size = ftell(hsaco_file);\n-            unsigned char* hsaco = (unsigned char*) malloc(hsaco_file_size * sizeof(unsigned char));\n-            rewind(hsaco_file);\n-            fread(hsaco, sizeof(unsigned char), hsaco_file_size, hsaco_file);\n-            fclose(hsaco_file);\n-\n-            // set HIP options\n-            hipJitOption opt[] = {hipJitOptionErrorLogBufferSizeBytes, hipJitOptionErrorLogBuffer,\n-                                  hipJitOptionInfoLogBufferSizeBytes, hipJitOptionInfoLogBuffer,\n-                                  hipJitOptionLogVerbose};\n-            const unsigned int errbufsize = 8192;\n-            const unsigned int logbufsize = 8192;\n-            char _err[errbufsize];\n-            char _log[logbufsize];\n-            void *optval[] = {(void *)(uintptr_t)errbufsize,\n-                              (void *)_err, (void *)(uintptr_t)logbufsize,\n-                              (void *)_log, (void *)1};\n-\n-            // launch HIP Binary\n-            hipModule_t mod;\n-            hipFunction_t fun;\n-            hipModuleLoadDataEx(&mod, hsaco, 5, opt, optval);\n-            hipModuleGetFunction(&fun, mod, name);\n-            free(hsaco);\n-\n-            // get allocated registers and spilled registers from the function\n-            int n_regs = 0;\n-            int n_spills = 0;\n-            if (PyErr_Occurred()) {\n-              return NULL;\n-            }\n-            return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs, n_spills);\n-        }\n-\n-        static PyMethodDef ModuleMethods[] = {\n-          {\"load_binary\", loadBinary, METH_VARARGS, \"Load provided hsaco into HIP driver\"},\n-          {\"get_device_properties\", getDeviceProperties, METH_VARARGS, \"Get the properties for a given device\"},\n-          {NULL, NULL, 0, NULL} // sentinel\n-        };\n-\n-        static struct PyModuleDef ModuleDef = {\n-          PyModuleDef_HEAD_INIT,\n-          \"hip_utils\",\n-          NULL, //documentation\n-          -1, //size\n-          ModuleMethods\n-        };\n-\n-        PyMODINIT_FUNC PyInit_hip_utils(void) {\n-          PyObject *m = PyModule_Create(&ModuleDef);\n-          if (m == NULL) {\n-            return NULL;\n-          }\n-          PyModule_AddFunctions(m, ModuleMethods);\n-          return m;\n-        }\n-        \"\"\"\n-\n-    def __init__(self):\n-        src = self._generate_src()\n-        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n-        cache = get_cache_manager(key)\n-        fname = \"hip_utils.so\"\n-        cache_path = cache.get_file(fname)\n-        if cache_path is None:\n-            with tempfile.TemporaryDirectory() as tmpdir:\n-                src_path = os.path.join(tmpdir, \"main.c\")\n-                with open(src_path, \"w\") as f:\n-                    f.write(src)\n-                so = _build(\"hip_utils\", src_path, tmpdir)\n-                with open(so, \"rb\") as f:\n-                    cache_path = cache.put(f.read(), fname, binary=True)\n-        import importlib.util\n-        spec = importlib.util.spec_from_file_location(\"hip_utils\", cache_path)\n-        mod = importlib.util.module_from_spec(spec)\n-        spec.loader.exec_module(mod)\n-        self.load_binary = mod.load_binary\n-        self.get_device_properties = mod.get_device_properties"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 12, "deletions": 10, "changes": 22, "file_content_changes": "@@ -5,7 +5,6 @@\n from contextlib import contextmanager\n \n import triton._C.libtriton.triton as _triton\n-from .runtime.driver.cuda import get_cuda_utils\n \n \n def nvsmi(attrs):\n@@ -275,28 +274,30 @@ def perf_report(benchmarks):\n def get_dram_gbps(backend=None, device=None):\n     ''' return DRAM bandwidth in GB/s '''\n     import torch\n+\n+    from .runtime import driver\n     if not backend:\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n-    cuda_utils = get_cuda_utils()\n-    mem_clock_khz = cuda_utils.get_device_properties(device)[\"mem_clock_rate\"]  # in kHz\n-    bus_width = cuda_utils.get_device_properties(device)[\"mem_bus_width\"]\n+    mem_clock_khz = driver.utils.get_device_properties(device)[\"mem_clock_rate\"]  # in kHz\n+    bus_width = driver.utils.get_device_properties(device)[\"mem_bus_width\"]\n     bw_gbps = mem_clock_khz * bus_width * 2 / 1e6 / 8  # In GB/s\n     return bw_gbps\n \n \n def get_max_tensorcore_tflops(dtype, backend=None, device=None, clock_rate=None):\n     import torch\n+\n+    from .runtime import driver\n     if not backend:\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n \n-    cuda_utils = get_cuda_utils()\n-    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n+    num_subcores = driver.utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n     if not clock_rate:\n-        clock_rate = cuda_utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n+        clock_rate = driver.utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n     capability = torch.cuda.get_device_capability(device)\n     if capability[0] < 8:\n         assert dtype == torch.float16\n@@ -390,14 +391,15 @@ def set_gpu_clock(ref_sm_clock=1350, ref_mem_clock=1215):\n \n def get_max_simd_tflops(dtype, backend=None, device=None):\n     import torch\n+\n+    from .runtime import driver\n     if not backend:\n         backend = _triton.runtime.backend.CUDA\n     if not device:\n         device = torch.cuda.current_device()\n \n-    cuda_utils = get_cuda_utils()\n-    num_subcores = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n-    clock_rate = cuda_utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n+    num_subcores = driver.utils.get_device_properties(device)[\"multiprocessor_count\"] * 4\n+    clock_rate = driver.utils.get_device_properties(device)[\"sm_clock_rate\"]  # in kHz\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         if dtype == torch.float32:"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -288,12 +288,14 @@ def _output_stubs(self) -> str:\n         #   arg_type_symbol_dict = {[arg_type]: {(symbol, ret_type)}}\n         #   return extern.dispatch(\"libdevice\", <path>, <args>, <arg_type_symbol_dict>, _builder)\n         import_str = \"from . import core, extern\\n\"\n+        import_str += \"from ..runtime import driver\\n\"\n         import_str += \"import os\\n\"\n-        header_str = \"LOCAL_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), \\\"..\\\", \\\"third_party\\\", \\\"cuda\\\", \\\"lib\\\", \\\"libdevice.10.bc\\\")\\n\"\n-        header_str += \"LIBDEVICE_PATH = os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", LOCAL_PATH)\\n\"\n+\n+        header_str = \"\"\n+        header_str += \"LIBDEVICE_PATH = os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", driver.libdevice_path)\\n\"\n         func_str = \"\"\n         for symbols in self._symbol_groups.values():\n-            func_str += \"@language.extern\\n\"\n+            func_str += \"@extern.extern\\n\"\n             func_name_str = f\"def {symbols[0].op_name}(\"\n             for arg_name in symbols[0].arg_names:\n                 func_name_str += f\"{arg_name}, \""}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -217,7 +217,11 @@ tt.func @alloc(%A : !tt.ptr<f16>) {\n tt.func @scratch() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: scratch offset = 0, size = 512\n-  %b = tt.reduce %cst0 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n+  %b = \"tt.reduce\" (%cst0) ({\n+  ^bb0(%arg0: f16, %arg1: f16):\n+    %add = arith.addf %arg0, %arg1 : f16\n+    tt.reduce.return %add : f16\n+  }) {axis = 0 : i32} : (tensor<16x16xf16, #AL>) -> tensor<16xf16, #sliceAd0>\n   tt.return\n   // CHECK-NEXT: size = 512\n }"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -79,7 +79,11 @@ tt.func @scratch() {\n   // CHECK: gpu.barrier\n   // CHECK-NEXT: triton_gpu.convert_layout\n   %1 = triton_gpu.convert_layout %0 : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n-  %2 = tt.reduce %1 {redOp = 1 : i32, axis = 0 : i32} : tensor<32x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n+  %2 = \"tt.reduce\" (%1) ({\n+  ^bb0(%arg1: f16, %arg2: f16):\n+    %add = arith.addf %arg1, %arg2 : f16\n+    tt.reduce.return %add : f16\n+  }) {axis = 0 : i32} : (tensor<32x16xf16, #AL>) -> tensor<16xf16, #sliceAd0>\n   tt.return\n }\n "}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 36, "deletions": 12, "changes": 48, "file_content_changes": "@@ -79,18 +79,42 @@ tt.func @load_store_ops_scalar(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n tt.func @reduce_ops_infer(%ptr: !tt.ptr<f32>, %v : tensor<1x2x4xf32>) {\n   // Test if reduce ops infer types correctly\n \n-  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<2x4xf32>\n-  %a = tt.reduce %v {redOp = 1 : i32, axis = 0 : i32} : tensor<1x2x4xf32> -> tensor<2x4xf32>\n-  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<1x4xf32>\n-  %b = tt.reduce %v {redOp = 1 : i32, axis = 1 : i32} : tensor<1x2x4xf32> -> tensor<1x4xf32>\n-  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<1x2xf32>\n-  %c = tt.reduce %v {redOp = 1 : i32, axis = 2 : i32} : tensor<1x2x4xf32> -> tensor<1x2xf32>\n-  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<1xf32>\n-  %e = tt.reduce %b {redOp = 1 : i32, axis = 1 : i32} : tensor<1x4xf32> -> tensor<1xf32>\n-  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> tensor<4xf32>\n-  %f = tt.reduce %a {redOp = 1 : i32, axis = 0 : i32} : tensor<2x4xf32> -> tensor<4xf32>\n-  // CHECK: %{{.*}} = tt.reduce %{{.*}} -> f32\n-  %g = tt.reduce %f {redOp = 1 : i32, axis = 0 : i32} : tensor<4xf32> -> f32\n+  // CHECK: }) {axis = 0 : i32} : (tensor<1x2x4xf32>) -> tensor<2x4xf32>\n+  %a = \"tt.reduce\" (%v) ({\n+  ^bb0(%arg0: f32, %arg1: f32):\n+    %add = arith.addf %arg0, %arg1 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 0 : i32}  : (tensor<1x2x4xf32>) -> tensor<2x4xf32>\n+  // CHECK: }) {axis = 1 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x4xf32>\n+  %b = \"tt.reduce\" (%v) ({\n+  ^bb0(%arg0: f32, %arg1: f32):\n+    %add = arith.addf %arg0, %arg1 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 1 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x4xf32>\n+  // CHECK: }) {axis = 2 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x2xf32>\n+  %c = \"tt.reduce\" (%v) ({\n+  ^bb0(%arg0: f32, %arg1: f32):\n+    %add = arith.addf %arg0, %arg1 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 2 : i32}  : (tensor<1x2x4xf32>) -> tensor<1x2xf32>\n+  // CHECK: }) {axis = 1 : i32}  : (tensor<1x4xf32>) -> tensor<1xf32>\n+  %e = \"tt.reduce\" (%b) ({\n+  ^bb0(%arg0: f32, %arg1: f32):\n+    %add = arith.addf %arg0, %arg1 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 1 : i32}  : (tensor<1x4xf32>) -> tensor<1xf32>\n+  // CHECK: }) {axis = 0 : i32}  : (tensor<2x4xf32>) -> tensor<4xf32>\n+  %f = \"tt.reduce\" (%a) ({\n+  ^bb0(%arg0: f32, %arg1: f32):\n+    %add = arith.addf %arg0, %arg1 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 0 : i32}  : (tensor<2x4xf32>) -> tensor<4xf32>\n+  // CHECK: }) {axis = 0 : i32}  : (tensor<4xf32>) -> f32\n+  %g = \"tt.reduce\" (%f) ({\n+  ^bb0(%arg0: f32, %arg1: f32):\n+    %add = arith.addf %arg0, %arg1 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 0 : i32}  : (tensor<4xf32>) -> f32\n \n   // Avoid optimizations for c, e, and g\n   %ptr1x2 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<1x2x!tt.ptr<f32>>"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 24, "deletions": 8, "changes": 32, "file_content_changes": "@@ -40,14 +40,30 @@ tt.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n   %c0 = arith.constant dense<1.00e+00> : tensor<4x4xf32>\n   %c1 = arith.constant dense<2.00e+00> : tensor<8x2xf32>\n   %c2 = arith.constant dense<3.00e+00> : tensor<16x16xf32>\n-  // CHECK: tensor<4x4xf32, #[[blocked0]]> -> tensor<4xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked0]]}>>\n-  %c0_ = tt.reduce %c0 {redOp = 1 : i32, axis = 0 : i32} : tensor<4x4xf32> -> tensor<4xf32>\n-  // CHECK: tensor<8x2xf32, #[[blocked1]]> -> tensor<2xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked1]]}>\n-  %c1_ = tt.reduce %c1 {redOp = 1 : i32, axis = 0 : i32} : tensor<8x2xf32> -> tensor<2xf32>\n-  // CHECK: tensor<8x2xf32, #[[blocked1]]> -> tensor<8xf32, #triton_gpu.slice<{dim = 1, parent = #[[blocked1]]}>>\n-  %c2_ = tt.reduce %c1 {redOp = 1 : i32, axis = 1 : i32} : tensor<8x2xf32> -> tensor<8xf32>\n-  // CHECK: tensor<16x16xf32, #[[blocked2]]> -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked2]]}>>\n-  %c3_ = tt.reduce %c2 {redOp = 1 : i32, axis = 0 : i32} : tensor<16x16xf32> -> tensor<16xf32>\n+  // CHECK: (tensor<4x4xf32, #[[blocked0]]>) -> tensor<4xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked0]]}>>\n+  %c0_ = \"tt.reduce\" (%c0) ({\n+  ^bb0(%arg1: f32, %arg2: f32):\n+    %add = arith.addf %arg1, %arg2 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 0 : i32} : (tensor<4x4xf32>) -> tensor<4xf32>\n+  // CHECK: (tensor<8x2xf32, #[[blocked1]]>) -> tensor<2xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked1]]}>\n+  %c1_ = \"tt.reduce\" (%c1) ({\n+  ^bb0(%arg3: f32, %arg4: f32):\n+    %add = arith.addf %arg3, %arg4 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 0 : i32} : (tensor<8x2xf32>) -> tensor<2xf32>\n+  // CHECK: (tensor<8x2xf32, #[[blocked1]]>) -> tensor<8xf32, #triton_gpu.slice<{dim = 1, parent = #[[blocked1]]}>>\n+  %c2_ = \"tt.reduce\" (%c1) ({\n+  ^bb0(%arg5: f32, %arg6: f32):\n+    %add = arith.addf %arg5, %arg6 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 1 : i32} : (tensor<8x2xf32>) -> tensor<8xf32>\n+  // CHECK: (tensor<16x16xf32, #[[blocked2]]>) -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #[[blocked2]]}>>\n+  %c3_ = \"tt.reduce\" (%c2) ({\n+  ^bb0(%arg7: f32, %arg8: f32):\n+    %add = arith.addf %arg7, %arg8 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 0 : i32} : (tensor<16x16xf32>) -> tensor<16xf32>\n \n   tt.return\n }"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 28, "deletions": 6, "changes": 34, "file_content_changes": "@@ -787,7 +787,11 @@ tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !\n   %27 = \"triton_gpu.cmpf\"(%cst_2, %26) {predicate = 4 : i64} : (tensor<16x16xf32, #blocked2>, tensor<16x16xf32, #blocked2>) -> tensor<16x16xi1, #blocked2>\n   %28 = arith.andi %22, %27 : tensor<16x16xi1, #blocked2>\n   %29 = \"triton_gpu.select\"(%28, %26, %cst_2) : (tensor<16x16xi1, #blocked2>, tensor<16x16xf32, #blocked2>, tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>\n-  %30 = tt.reduce %29 {axis = 1 : i32, redOp = 12 : i32} : tensor<16x16xf32, #blocked2> -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+  %30 = \"tt.reduce\" (%29) ({\n+  ^bb0(%arg4: f32, %arg5: f32):\n+    %max = arith.maxf %arg4, %arg5 : f32\n+    tt.reduce.return %max : f32\n+  }) {axis = 1 : i32} : (tensor<16x16xf32, #blocked2>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n   %31 = triton_gpu.convert_layout %30 : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16xf32, #blocked0>\n   %32 = triton_gpu.convert_layout %31 : (tensor<16xf32, #blocked0>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n   %33 = tt.expand_dims %32 {axis = 1 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xf32, #blocked1>\n@@ -803,7 +807,11 @@ tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !\n   %43 = math.exp %42 : tensor<16x16xf32, #blocked2>\n   %44 = arith.addf %36, %43 : tensor<16x16xf32, #blocked2>\n   %45 = \"triton_gpu.select\"(%22, %44, %36) : (tensor<16x16xi1, #blocked2>, tensor<16x16xf32, #blocked2>, tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>\n-  %46 = tt.reduce %45 {axis = 1 : i32, redOp = 2 : i32} : tensor<16x16xf32, #blocked2> -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+  %46 = \"tt.reduce\" (%45) ({\n+  ^bb0(%arg4: f32, %arg5: f32):\n+    %add = arith.addf %arg4, %arg5 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 1 : i32} : (tensor<16x16xf32, #blocked2>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n   %47 = triton_gpu.convert_layout %46 : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16xf32, #blocked0>\n   %48 = triton_gpu.convert_layout %47 : (tensor<16xf32, #blocked0>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n   %49 = tt.expand_dims %48 {axis = 1 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<16x1xf32, #blocked1>\n@@ -907,7 +915,11 @@ tt.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt\n     %74 = \"triton_gpu.select\"(%54, %73, %arg7) : (tensor<64x64xi1, #blocked2>, tensor<64x64xf32, #blocked2>, tensor<64x64xf32, #blocked2>) -> tensor<64x64xf32, #blocked2>\n     scf.yield %74 : tensor<64x64xf32, #blocked2>\n   }\n-  %26 = tt.reduce %25 {axis = 1 : i32, redOp = 2 : i32} : tensor<64x64xf32, #blocked2> -> tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+  %26 = \"tt.reduce\" (%25) ({\n+  ^bb0(%arg8: f32, %arg9: f32):\n+    %add = arith.addf %arg8, %arg9 : f32\n+    tt.reduce.return %add : f32\n+  }) {axis = 1 : i32} : (tensor<64x64xf32, #blocked2>) -> tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n   %27 = triton_gpu.convert_layout %26 : (tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<64xf32, #blocked0>\n   %28 = triton_gpu.convert_layout %27 : (tensor<64xf32, #blocked0>) -> tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n   %29 = tt.expand_dims %28 {axis = 1 : i32} : (tensor<64xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<64x1xf32, #blocked1>\n@@ -1016,7 +1028,11 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n     %1 = triton_gpu.convert_layout %0 : (tensor<2xi32, #blocked1>) -> tensor<2xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n     %2 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<2xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x2xi32, #blocked>\n     %3 = \"triton_gpu.cmpi\"(%2, %cst_0) {predicate = 2 : i64} : (tensor<1x2xi32, #blocked>, tensor<1x2xi32, #blocked>) -> tensor<1x2xi1, #blocked>\n-    %4 = tt.reduce %cst {axis = 1 : i32, redOp = 1 : i32} : tensor<1x2xi32, #blocked> -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %4 = \"tt.reduce\" (%cst) ({\n+    ^bb0(%arg3: i32, %arg4: i32):\n+      %add = arith.addi %arg3, %arg4 : i32\n+      tt.reduce.return %add : i32\n+    }) {axis = 1 : i32} : (tensor<1x2xi32, #blocked>) -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n     %5 = triton_gpu.convert_layout %4 : (tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<1xi32, #blocked1>\n     %6 = triton_gpu.convert_layout %5 : (tensor<1xi32, #blocked1>) -> tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n     %7 = tt.expand_dims %6 {axis = 1 : i32} : (tensor<1xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<1x1xi32, #blocked2>\n@@ -1037,7 +1053,8 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // Check if the SimplifyReduceCvt handles convert_layout lifted from the for loop.\n // CHECK-LABEL: reduce_cvt2\n-// CHECK: tt.reduce\n+// Match the reduction\n+// CHECK: }) {axis = 1 : i32} : (tensor<1x256xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n // CHECK-NEXT: triton_gpu.convert_layout\n // CHECK-NOT: triton_gpu.convert_layout\n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>\n@@ -1092,7 +1109,12 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n       %59 = \"triton_gpu.select\"(%52, %58, %arg6) : (tensor<1x256xi1, #blocked>, tensor<1x256xf32, #blocked>, tensor<1x256xf32, #blocked>) -> tensor<1x256xf32, #blocked>\n       scf.yield %59 : tensor<1x256xf32, #blocked>\n     }\n-    %16 = tt.reduce %15 {axis = 1 : i32, redOp = 2 : i32} : tensor<1x256xf32, #blocked> -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %16 = \"tt.reduce\" (%15) ({\n+    ^bb0(%arg7: f32, %arg8: f32):\n+      %add = arith.addf %arg7, %arg8 : f32\n+      tt.reduce.return %add : f32\n+\n+    }) {axis = 1 : i32} : (tensor<1x256xf32, #blocked>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n     %17 = triton_gpu.convert_layout %16 : (tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<1xf32, #blocked1>\n     %18 = triton_gpu.convert_layout %17 : (tensor<1xf32, #blocked1>) -> tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n     %19 = tt.expand_dims %18 {axis = 1 : i32} : (tensor<1xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<1x1xf32, #blocked2>"}]