[{"filename": "lib/Conversion/NVGPUToLLVM/NVGPUToLLVMPass.cpp", "status": "modified", "additions": 18, "deletions": 54, "changes": 72, "file_content_changes": "@@ -195,7 +195,7 @@ class StoreMatrixOpPattern : public mlir::RewritePattern {\n     auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\n         \"stmatrix.sync.aligned.m8n8.x\" + std::to_string(datas.size()) +\n         \".shared.b16\");\n-    auto *addrOpr = ptxBuilder.newAddrOperand(addr, \"r\", 0);\n+    auto *addrOpr = ptxBuilder.newAddrOperand(ptrtoint(i32_ty, addr), \"r\");\n \n     SmallVector<std::pair<Value, std::string>> args;\n     for (unsigned i = 0; i < datas.size(); ++i) {\n@@ -212,38 +212,6 @@ class StoreMatrixOpPattern : public mlir::RewritePattern {\n   }\n };\n \n-class CvtPackOpPattern : public mlir::RewritePattern {\n-public:\n-  CvtPackOpPattern(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(mlir::triton::nvgpu::CvtPackOp::getOperationName(),\n-                             1, context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto ctx = rewriter.getContext();\n-    auto cvtPackOp = llvm::dyn_cast<mlir::triton::nvgpu::CvtPackOp>(op);\n-    if (!cvtPackOp)\n-      return mlir::failure();\n-    auto loc = op->getLoc();\n-    auto d0 = cvtPackOp.getD0();\n-    auto d1 = cvtPackOp.getD1();\n-\n-    PTXBuilder ptxBuilder;\n-    auto &ptxInstr = *ptxBuilder.create<PTXInstr>(\"cvt.pack.sat.u16.s32\");\n-    auto *ret = ptxBuilder.newOperand(\"=r\");\n-    auto *d0Opr = ptxBuilder.newOperand(d0, \"r\");\n-    auto *d1Opr = ptxBuilder.newOperand(d1, \"r\");\n-\n-    ptxInstr(ret, d0Opr, d1Opr);\n-\n-    auto asmReturnTy = rewriter.getIntegerType(32);\n-    auto res = ptxBuilder.launch(rewriter, loc, asmReturnTy);\n-    rewriter.replaceOp(op, {res});\n-    return mlir::success();\n-  }\n-};\n-\n class OffsetOfStmatrixV4OpPattern : public mlir::RewritePattern {\n public:\n   OffsetOfStmatrixV4OpPattern(mlir::MLIRContext *context)\n@@ -489,24 +457,22 @@ class MBarrierWaitOpPattern : public mlir::RewritePattern {\n     auto loc = op->getLoc();\n     Value mbarrier = mBarrierWaitOp.getMbarrier();\n     Value phase = mBarrierWaitOp.getPhase();\n-    Value largeVal = i32_val(0x989680);\n     PTXBuilder ptxBuilder;\n \n-    auto ptxAsm = \"{\\n\"\n-                  \".reg .pred                P1; \\n\"\n-                  \"LAB_WAIT: \\n\"\n-                  \"mbarrier.try_wait.parity.shared.b64 P1, [$0], $1, $2; \\n\"\n-                  \"@P1                       bra.uni DONE; \\n\"\n-                  \"bra.uni                   LAB_WAIT; \\n\"\n-                  \"DONE: \\n\"\n-                  \"}\";\n+    auto ptxAsm =\n+        \"{\\n\"\n+        \".reg .pred                P1; \\n\"\n+        \"LAB_WAIT: \\n\"\n+        \"mbarrier.try_wait.parity.shared.b64 P1, [$0], $1, 0x989680; \\n\"\n+        \"@P1                       bra.uni DONE; \\n\"\n+        \"bra.uni                   LAB_WAIT; \\n\"\n+        \"DONE: \\n\"\n+        \"}\";\n     auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n-    auto *barOpr =\n-        ptxBuilder.newAddrOperand(ptrtoint(i32_ty, mbarrier), \"r\", 0);\n+    auto *barOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, mbarrier), \"r\");\n     auto *phaseOpr = ptxBuilder.newOperand(zext(i32_ty, phase), \"r\");\n-    auto *largeValOpr = ptxBuilder.newOperand(largeVal, \"r\");\n \n-    ptxInstr({barOpr, phaseOpr, largeValOpr},\n+    ptxInstr({barOpr, phaseOpr},\n              /*onlyAttachMLIRArgs=*/true);\n \n     auto asmReturnTy = void_ty(ctx);\n@@ -613,8 +579,7 @@ class TMALoadTiledOpPattern : public mlir::RewritePattern {\n         auto ptxAsm =\n             \"@$6 cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier:\"\n             \":complete_tx\"\n-            \"::bytes.L2::cache_hint [$0], [$1, {$2, $3}], \"\n-            \"[$4], $5;\";\n+            \"::bytes.L2::cache_hint [$0], [$1, {$2, $3}], [$4], $5;\";\n         auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n         auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n         auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n@@ -646,11 +611,11 @@ class TMALoadTiledOpPattern : public mlir::RewritePattern {\n       }\n     } else if (dimSize == 4) {\n       assert(mcastMask == nullptr && \"Does not support multicast\");\n-      auto ptxAsm =\n-          \"@$8 \"\n-          \"cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier::complete_tx\"\n-          \"::bytes.L2::cache_hint [$0], [$1, {$2, $3, $4, $5}], \"\n-          \"[$6], $7;\";\n+      auto ptxAsm = \"@$8 \"\n+                    \"cp.async.bulk.tensor.4d.shared::cluster.global.mbarrier:\"\n+                    \":complete_tx\"\n+                    \"::bytes.L2::cache_hint [$0], [$1, {$2, $3, $4, $5}], \"\n+                    \"[$6], $7;\";\n       auto &ptxInstr = *ptxBuilder.create<PTXInstr>(ptxAsm);\n       auto *dstOpr = ptxBuilder.newOperand(ptrtoint(i32_ty, dst), \"r\");\n       auto *descOpr = ptxBuilder.newOperand(ptrtoint(i64_ty, tmaDesc), \"l\");\n@@ -1416,7 +1381,6 @@ class ConvertNVGPUToLLVM : public ConvertNVGPUToLLVMBase<ConvertNVGPUToLLVM> {\n     patterns.add<WGMMACommitGroupOpPattern>(context);\n     patterns.add<WGMMAWaitGroupOpPattern>(context);\n     patterns.add<StoreMatrixOpPattern>(context);\n-    patterns.add<CvtPackOpPattern>(context);\n     patterns.add<OffsetOfStmatrixV4OpPattern>(context);\n     patterns.add<WGMMADescCreateOpPattern>(context);\n     patterns.add<MBarrierInitOpPattern>(context);"}]