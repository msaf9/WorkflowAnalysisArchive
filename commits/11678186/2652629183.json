[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 54, "deletions": 49, "changes": 103, "file_content_changes": "@@ -13,41 +13,41 @@\n @triton.jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,\n-    TMP, L, M, #NOTE: TMP is a scratchpad buffer to workaround a compiler bug\n-    Out, \n+    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to workaround a compiler bug\n+    Out,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n     stride_oz, stride_oh, stride_om, stride_on,\n-    Z, H, N_CTX, \n-    BLOCK_M: tl.constexpr, BLOCK_DMODEL : tl.constexpr, \n+    Z, H, N_CTX,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n ):\n     start_m = tl.program_id(0)\n     off_hz = tl.program_id(1)\n     # initialize offsets\n-    offs_m = start_m*BLOCK_M + tl.arange(0, BLOCK_M) \n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n-    off_q = off_hz*stride_qh + offs_m[:, None]*stride_qm + offs_d[None, :]*stride_qk\n-    off_k = off_hz*stride_qh + offs_n[:, None]*stride_kn + offs_d[None, :]*stride_kk\n-    off_v = off_hz*stride_qh + offs_n[:, None]*stride_qm + offs_d[None, :]*stride_qk\n+    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n+    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n     k_ptrs = K + off_k\n     v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    t_ptrs = TMP + off_hz*N_CTX + offs_m\n+    t_ptrs = TMP + off_hz * N_CTX + offs_m\n     m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n     l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n     # load q: it will stay in SRAM throughout\n     q = tl.load(q_ptrs)\n     # loop over k, v and update accumulator\n-    for start_n in range(0, (start_m + 1)*BLOCK_M, BLOCK_N):\n+    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n         start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(k_ptrs + start_n*stride_kn)\n+        k = tl.load(k_ptrs + start_n * stride_kn)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n         qk += tl.dot(q, k, trans_b=True)\n         qk *= sm_scale\n@@ -68,97 +68,99 @@ def _fwd_kernel(\n         # scale acc\n         acc_scale = l_i / l_i_new * alpha\n         tl.store(t_ptrs, acc_scale)\n-        acc_scale = tl.load(t_ptrs) # BUG: have to store and immediately load\n+        acc_scale = tl.load(t_ptrs)  # BUG: have to store and immediately load\n         acc = acc * acc_scale[:, None]\n         # update acc\n-        v = tl.load(v_ptrs + start_n*stride_vk) \n+        v = tl.load(v_ptrs + start_n * stride_vk)\n         p = p.to(tl.float16)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n         l_i = l_i_new\n         m_i = m_i_new\n     # rematerialize offsets to save registers\n     start_m = tl.program_id(0)\n-    offs_m = start_m*BLOCK_M + tl.arange(0, BLOCK_M) \n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     # write back l and m\n-    l_ptrs = L   + off_hz*N_CTX + offs_m\n-    m_ptrs = M   + off_hz*N_CTX + offs_m\n+    l_ptrs = L + off_hz * N_CTX + offs_m\n+    m_ptrs = M + off_hz * N_CTX + offs_m\n     tl.store(l_ptrs, l_i)\n     tl.store(m_ptrs, m_i)\n     # initialize pointers to output\n     offs_n = tl.arange(0, BLOCK_DMODEL)\n-    off_o = off_hz * stride_oh + offs_m[:, None]*stride_om + offs_n[None, :]*stride_on\n+    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n     out_ptrs = Out + off_o\n     tl.store(out_ptrs, acc)\n \n+\n @triton.jit\n def _bwd_preprocess(\n     O, DO, L,\n     NewDO, Delta,\n     BLOCK_M: tl.constexpr, D_HEAD: tl.constexpr,\n ):\n-    off_m = tl.program_id(0)*BLOCK_M + tl.arange(0, BLOCK_M)\n+    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n     off_n = tl.arange(0, D_HEAD)\n     # load\n-    o  = tl.load(O  + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n+    o = tl.load(O + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n     do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :]).to(tl.float32)\n-    l  = tl.load(L + off_m).to(tl.float32)\n+    l = tl.load(L + off_m).to(tl.float32)\n     # compute\n     do = do / l[:, None]\n     delta = tl.sum(o * do, axis=1)\n     # write-back\n     tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n     tl.store(Delta + off_m, delta)\n \n+\n @triton.jit\n def _bwd_kernel(\n     Q, K, V, sm_scale, O, DO,\n     DQ, DK, DV,\n-    L, M, \n+    L, M,\n     D,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n     stride_vz, stride_vh, stride_vk, stride_vn,\n     Z, H, N_CTX,\n     num_block,\n-    BLOCK_M: tl.constexpr, BLOCK_DMODEL : tl.constexpr, \n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n ):\n     off_hz = tl.program_id(0)\n     off_z = off_hz // H\n     off_h = off_hz % H\n     # offset pointers for batch/head\n-    Q += off_z*stride_qz + off_h*stride_qh\n-    K += off_z*stride_qz + off_h*stride_qh\n-    V += off_z*stride_qz + off_h*stride_qh\n-    DO += off_z*stride_qz + off_h*stride_qh\n-    DQ += off_z*stride_qz + off_h*stride_qh\n-    DK += off_z*stride_qz + off_h*stride_qh\n-    DV += off_z*stride_qz + off_h*stride_qh\n+    Q += off_z * stride_qz + off_h * stride_qh\n+    K += off_z * stride_qz + off_h * stride_qh\n+    V += off_z * stride_qz + off_h * stride_qh\n+    DO += off_z * stride_qz + off_h * stride_qh\n+    DQ += off_z * stride_qz + off_h * stride_qh\n+    DK += off_z * stride_qz + off_h * stride_qh\n+    DV += off_z * stride_qz + off_h * stride_qh\n     for start_n in range(0, num_block):\n-        lo = start_n*BLOCK_M\n+        lo = start_n * BLOCK_M\n         # initialize row/col offsets\n         offs_qm = lo + tl.arange(0, BLOCK_M)\n-        offs_n = start_n*BLOCK_M + tl.arange(0, BLOCK_M) \n+        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n         offs_m = tl.arange(0, BLOCK_N)\n         offs_k = tl.arange(0, BLOCK_DMODEL)\n         # initialize pointers to value-like data\n-        q_ptrs  = Q  + (offs_qm[:, None]*stride_qm + offs_k[None, :]*stride_qk)\n-        k_ptrs  = K  + (offs_n[:, None]*stride_kn + offs_k[None, :]*stride_kk)\n-        v_ptrs  = V  + (offs_n[:, None]*stride_qm + offs_k[None, :]*stride_qk)\n-        do_ptrs = DO + (offs_qm[:, None]*stride_qm + offs_k[None, :]*stride_qk)\n-        dq_ptrs = DQ + (offs_qm[:, None]*stride_qm + offs_k[None, :]*stride_qk)\n+        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n+        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n         # pointer to row-wise quantities in value-like data\n-        D_ptrs = D + off_hz*N_CTX\n-        m_ptrs = M + off_hz*N_CTX \n+        D_ptrs = D + off_hz * N_CTX\n+        m_ptrs = M + off_hz * N_CTX\n         # initialize dv amd dk\n         dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n         dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n         # k and v stay in SRAM throughout\n         k = tl.load(k_ptrs)\n         v = tl.load(v_ptrs)\n         # loop over rows\n-        for start_m in range(lo, num_block*BLOCK_M, BLOCK_M):\n+        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n             offs_m_curr = start_m + offs_m\n             # load q, k, v, do on-chip\n             q = tl.load(q_ptrs)\n@@ -167,12 +169,12 @@ def _bwd_kernel(\n             qk = tl.dot(q, k, trans_b=True)\n             qk = tl.where(offs_m_curr[:, None] >= (offs_n[None, :]), qk, float(\"-inf\"))\n             m = tl.load(m_ptrs + offs_m_curr)\n-            p = tl.exp(qk*sm_scale - m[:, None])\n+            p = tl.exp(qk * sm_scale - m[:, None])\n             # compute dv\n             do = tl.load(do_ptrs)\n             dv += tl.dot(p.to(tl.float16), do, trans_a=True)\n             # compute dp = dot(v, do)\n-            Di  = tl.load(D_ptrs + offs_m_curr)\n+            Di = tl.load(D_ptrs + offs_m_curr)\n             dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n             dp += tl.dot(do, v, trans_b=True)\n             # compute ds = p * (dp - delta[:, None])\n@@ -184,15 +186,16 @@ def _bwd_kernel(\n             dq += tl.dot(ds.to(tl.float16), k)\n             tl.store(dq_ptrs, dq, eviction_policy=\"evict_last\")\n             # # increment pointers\n-            dq_ptrs += BLOCK_M*stride_qm\n-            q_ptrs  += BLOCK_M*stride_qm\n-            do_ptrs += BLOCK_M*stride_qm\n+            dq_ptrs += BLOCK_M * stride_qm\n+            q_ptrs += BLOCK_M * stride_qm\n+            do_ptrs += BLOCK_M * stride_qm\n         # write-back\n-        dv_ptrs = DV + (offs_n[:, None]*stride_qm + offs_k[None, :]*stride_qk)\n-        dk_ptrs = DK + (offs_n[:, None]*stride_kn + offs_k[None, :]*stride_kk)\n+        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n+        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n         tl.store(dv_ptrs, dv)\n         tl.store(dk_ptrs, dk)\n \n+\n class _attention(torch.autograd.Function):\n \n     @staticmethod\n@@ -235,7 +238,7 @@ def backward(ctx, do):\n         dv = torch.empty_like(v)\n         do_scaled = torch.empty_like(do)\n         delta = torch.empty_like(l)\n-        _bwd_preprocess[(ctx.grid[0]*ctx.grid[1], )](\n+        _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n             BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n@@ -257,6 +260,7 @@ def backward(ctx, do):\n         )\n         return dq, dk, dv, None\n \n+\n attention = _attention.apply\n \n \n@@ -274,7 +278,7 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n     for z in range(Z):\n         for h in range(H):\n-            p[:,:,M==0] = float(\"-inf\")\n+            p[:, :, M == 0] = float(\"-inf\")\n     p = torch.softmax(p.float(), dim=-1).half()\n     ref_out = torch.matmul(p, v)\n     ref_out.backward(dout)\n@@ -293,6 +297,7 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     triton.testing.assert_almost_equal(ref_dk, tri_dk)\n     triton.testing.assert_almost_equal(ref_dq, tri_dq)\n \n+\n try:\n     from flash_attn.flash_attn_interface import flash_attn_func\n     HAS_FLASH = True\n@@ -301,7 +306,7 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n \n BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n # vary seq length for fixed head and batch=4\n-configs = [ triton.testing.Benchmark(\n+configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n     x_vals=[2**i for i in range(10, 16)],\n     line_arg='provider',"}]