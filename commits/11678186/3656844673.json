[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -89,8 +89,8 @@ jobs:\n         if: ${{matrix.runner[0] == 'self-hosted' && matrix.runner[1] == 'V100'}}\n         run: |\n           cd python/tests\n-          pytest -v -k \"not test_where_broadcast and not test_dot\" test_core.py\n-          pytest test_gemm.py::test_gemm_for_mmav1\n+          pytest -k \"not test_where_broadcast and not test_dot\" test_core.py\n+          pytest test_gemm.py\n           pytest test_backend.py\n \n       - name: Run CXX unittests"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -617,13 +617,15 @@ SmallVector<unsigned, 2> warpsPerTileV1(triton::DotOp dotOp,\n   bool changed = false;\n   do {\n     changed = false;\n+    int pre = ret[0];\n     if (ret[0] * ret[1] < numWarps) {\n       ret[0] = std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n-      changed = true;\n+      changed = pre != ret[0];\n     }\n     if (ret[0] * ret[1] < numWarps) {\n+      pre = ret[1];\n       ret[1] = std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n-      changed = true;\n+      changed = pre != ret[1];\n     }\n   } while (changed);\n   return ret;"}, {"filename": "python/tests/test_gemm.py", "status": "modified", "additions": 1, "deletions": 48, "changes": 49, "file_content_changes": "@@ -317,52 +317,5 @@ def valid_on_Volta(num_warps, trans_a, trans_b, is_int8=False, is_tf32=False):\n     capability = torch.cuda.get_device_capability()\n     is_on_Volta = capability[0] < 8\n     # TODO[Superjomn]: Remove the constraints below when features are ready\n-    is_feature_ready = num_warps == 1 and not (trans_a or trans_b)\n+    is_feature_ready = not (trans_a or trans_b)\n     return is_on_Volta and is_feature_ready\n-\n-\n-# NOTE this is useful only on Volta GPU.\n-@pytest.mark.parametrize('SIZE_M,SIZE_N,SIZE_K,NUM_WARPS,BLOCK_SIZE_M,BLOCK_SIZE_N,BLOCK_SIZE_K,TRANS_A,TRANS_B', [\n-    # Non-forloop\n-    [16, 16, 16, 1, 16, 16, 16, False, False],\n-    [16, 16, 32, 1, 16, 16, 32, False, False],\n-    [32, 16, 32, 1, 32, 16, 32, False, False],\n-    [32, 32, 32, 1, 32, 32, 32, False, False],\n-    [128, 32, 32, 1, 128, 32, 32, False, False],\n-    # # split-K\n-    [16, 16, 32, 1, 16, 16, 16, False, False],\n-    [64, 64, 128, 1, 64, 64, 32, False, False],\n-    # numWarps > 1\n-    [32, 32, 64, 2, 32, 32, 32, False, False],\n-    [64, 32, 64, 4, 64, 32, 64, False, False],\n-    [128, 64, 128, 4, 128, 64, 128, False, False],\n-    # [16, 16, 16, 16, 16, 16, 16, False, False],  # wpt overflow issue, hang on Volta\n-    # K-Forloop\n-    # [16, 16, 64, 4, 8, 8, 8, False, False],  # Wrap threads\n-    [32, 32, 64, 4, 32, 32, 32, False, False],  # Single shared encoding\n-    # [16, 16, 128, 4, 16, 16, 16, False, False],  # Single shared encoding and small k, hang on Volta\n-    [64, 32, 128, 4, 64, 32, 64, False, False],\n-    [128, 16, 128, 4, 128, 16, 32, False, False],\n-    # [32, 16, 128, 4, 32, 16, 32, False, False], # hang on Volta\n-    [32, 64, 128, 4, 32, 64, 32, False, False],\n-    [32, 128, 256, 4, 32, 128, 64, False, False],\n-    [64, 128, 64, 4, 64, 128, 32, False, False],\n-    [64, 64, 128, 4, 64, 64, 32, False, False],\n-    [128, 128, 64, 4, 128, 128, 32, False, False],\n-    [128, 128, 128, 4, 128, 128, 32, False, False],\n-    [128, 128, 256, 4, 128, 128, 64, False, False],\n-    [128, 256, 128, 4, 128, 256, 32, False, False],\n-    [256, 128, 64, 4, 256, 128, 16, False, False],\n-    [128, 64, 128, 4, 128, 64, 32, False, False],\n-    # [16, 16, 64, 4, 16, 16, 16, False, False], # hang on Volta\n-    [32, 32, 64, 4, 32, 32, 32, False, False],\n-    # trans\n-    # [128, 64, 128, 4, 128, 64, 32, True, False],\n-    # [128, 64, 128, 4, 128, 64, 32, False, True],\n-])\n-def test_gemm_for_mmav1(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B):\n-    capability = torch.cuda.get_device_capability()\n-    if capability[0] >= 8:\n-        pytest.skip(\"Only test test_gemm_for_mmav1 on Volta.\")\n-\n-    test_gemm(SIZE_M, SIZE_N, SIZE_K, NUM_WARPS, BLOCK_SIZE_M, BLOCK_SIZE_N, BLOCK_SIZE_K, TRANS_A, TRANS_B)"}]