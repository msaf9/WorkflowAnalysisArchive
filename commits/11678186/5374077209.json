[{"filename": ".hypothesis/unicode_data/13.0.0/charmap.json.gz", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/test/unit/language/conftest.py", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+# content of conftest.py\n+\n+import pytest\n+\n+\n+def pytest_addoption(parser):\n+    parser.addoption(\n+        \"--device\", action=\"store\", default='cuda'\n+    )\n+\n+\n+@pytest.fixture\n+def device(request):\n+    return request.config.getoption(\"--device\")"}, {"filename": "python/test/unit/language/test_annotations.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -7,13 +7,13 @@\n import triton.language as tl\n \n \n-def test_annotations():\n+def test_annotations(device):\n \n     @triton.jit\n     def _kernel(X: torch.Tensor, N: int, BLOCK_SIZE: tl.constexpr):\n         pass\n \n-    x = torch.empty(1, device='cuda')\n+    x = torch.empty(1, device=device)\n     _kernel[(1,)](x, x.shape[0], 32)\n     try:\n         _kernel[(1,)](x.shape[0], x.shape[0], 32)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 200, "deletions": 192, "changes": 392, "file_content_changes": "@@ -101,13 +101,19 @@ def patch_kernel(template, to_replace):\n     return kernel\n \n \n-def check_type_supported(dtype):\n+def check_cuda_only(device):\n+    if device not in ['cuda']:\n+        pytest.skip(\"Only for cuda\")\n+\n+\n+def check_type_supported(dtype, device):\n     '''\n     skip test if dtype is not supported on the current device\n     '''\n-    cc = torch.cuda.get_device_capability()\n-    if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n-        pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+    if device in ['cuda']:\n+        cc = torch.cuda.get_device_capability()\n+        if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n+            pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n class MmaLayout:\n@@ -142,20 +148,20 @@ def __str__(self):\n \n \n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n-def test_empty_kernel(dtype_x, device='cuda'):\n+def test_empty_kernel(dtype_x, device):\n     SIZE = 128\n \n     @triton.jit\n     def kernel(X, SIZE: tl.constexpr):\n         pass\n-    check_type_supported(dtype_x)\n+    check_type_supported(dtype_x, device)\n     x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device, dst_type=dtype_x)\n     kernel[(1, )](x, SIZE=SIZE, num_warps=4)\n \n \n # generic test functions\n def _test_unary(dtype_x, expr, numpy_expr=None, device='cuda'):\n-    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_x, device)  # early return if dtype_x is not supported\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -213,8 +219,8 @@ def _binary_op_dtype_override(a: str, b: str) -> Optional[np.dtype]:\n \n \n def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real', mode_y='real', device='cuda', y_low=None, y_high=None):\n-    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n-    check_type_supported(dtype_y)\n+    check_type_supported(dtype_x, device)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_y, device)\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -284,7 +290,7 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n     for dtype_x in dtypes_with_bfloat16\n     for dtype_y in dtypes_with_bfloat16\n ])\n-def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_bin_op(dtype_x, dtype_y, op, device):\n     expr = f' x {op} y'\n     if op == '%' and dtype_x in int_dtypes + uint_dtypes and dtype_y in int_dtypes + uint_dtypes:\n         # LLVM has 'numpy.fmod', not 'numpy.remainder', semantics on integer remainders.\n@@ -317,7 +323,7 @@ def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n                          [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n                          )\n-def test_floordiv(dtype_x, dtype_y, device='cuda'):\n+def test_floordiv(dtype_x, dtype_y, device):\n     # Triton has IEEE, not numpy/torch, semantics for %, and those carry\n     # through to //, so we have to use a nonstandard expression to get a\n     # reference result for //.\n@@ -326,7 +332,7 @@ def test_floordiv(dtype_x, dtype_y, device='cuda'):\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n \n-def test_unsigned_name_mangling(device='cuda'):\n+def test_unsigned_name_mangling(device):\n     # Test that uint32 and int32 are mangled differently by the compiler\n     SIZE = 128\n     # define the kernel / launch-grid\n@@ -372,7 +378,7 @@ def kernel(O1, O2, X, Y, SIZE: tl.constexpr):\n     for dtype_x in dtypes + dtypes_with_bfloat16\n     for dtype_y in dtypes + dtypes_with_bfloat16\n ])\n-def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_bitwise_op(dtype_x, dtype_y, op, device):\n     expr = f'x {op} y'\n     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n@@ -395,7 +401,7 @@ def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n     for dtype_x in int_dtypes + uint_dtypes\n     for dtype_y in int_dtypes + uint_dtypes\n ])\n-def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_shift_op(dtype_x, dtype_y, op, device):\n     expr = f'x {op} y'\n     bw = max(_bitwidth(dtype_x), _bitwidth(dtype_y))\n     if dtype_x.startswith('int'):\n@@ -428,7 +434,7 @@ def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n                                                     ('nan', 'nan')]\n \n                           ])\n-def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n+def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device):\n     expr = f'x {op} y'\n     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n@@ -443,7 +449,7 @@ def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n # test broadcast\n # ---------------\n @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16)\n-def test_broadcast(dtype):\n+def test_broadcast(dtype, device):\n     @triton.jit\n     def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.constexpr):\n         offset1 = tl.arange(0, M)\n@@ -460,9 +466,9 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n     y = numpy_random(N, dtype_str=dtype, rs=rs)\n     _, y_broadcasted_np = np.broadcast_arrays(x, y)\n \n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n-    y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device='cuda', dst_type=dtype)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    y_tri = to_triton(y, device=device, dst_type=dtype)\n+    y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device=device, dst_type=dtype)\n \n     broadcast_kernel[(1,)](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)\n     assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n@@ -472,8 +478,8 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n # ------------------\n \n \n-def test_invalid_slice():\n-    dst = torch.empty(128, device='cuda')\n+def test_invalid_slice(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -486,7 +492,7 @@ def _kernel(dst):\n # ----------------\n # test expand_dims\n # ----------------\n-def test_expand_dims():\n+def test_expand_dims(device):\n     @triton.jit\n     def expand_dims_kernel(dummy, N: tl.constexpr):\n         offset1 = tl.arange(0, N)\n@@ -516,11 +522,11 @@ def expand_dims_kernel(dummy, N: tl.constexpr):\n         tl.static_assert(t.shape == [N, 1, 1, 1])\n \n     N = 32\n-    dummy_tensor = torch.empty((), device=\"cuda\")\n+    dummy_tensor = torch.empty((), device=device)\n     expand_dims_kernel[(1,)](dummy_tensor, N)\n \n \n-def test_expand_dims_error_cases():\n+def test_expand_dims_error_cases(device):\n     @triton.jit\n     def dim_out_of_range1(dummy, N: tl.constexpr):\n         offset1 = tl.arange(0, N)\n@@ -548,7 +554,7 @@ def duplicate_dim2(dummy, N: tl.constexpr):\n         t = tl.expand_dims(offset1, (0, -3))\n \n     N = 32\n-    dummy_tensor = torch.empty((), device=\"cuda\")\n+    dummy_tensor = torch.empty((), device=device)\n \n     with pytest.raises(triton.CompilationError, match=\"invalid axis -3\"):\n         dim_out_of_range1[(1,)](dummy_tensor, N)\n@@ -566,8 +572,8 @@ def duplicate_dim2(dummy, N: tl.constexpr):\n # ----------------------------\n # test invalid program id axis\n # ----------------------------\n-def test_invalid_pid_axis():\n-    dst = torch.empty(128, device='cuda')\n+def test_invalid_pid_axis(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -581,12 +587,12 @@ def _kernel(dst):\n # test where\n # ---------------\n @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16 + [\"*int32\"])\n-def test_where(dtype):\n+def test_where(dtype, device):\n     select_ptrs = False\n     if dtype == \"*int32\":\n         dtype = \"int64\"\n         select_ptrs = True\n-    check_type_supported(dtype)\n+    check_type_supported(dtype, device)\n \n     @triton.jit\n     def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n@@ -616,10 +622,10 @@ def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n     y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n     z = np.where(cond, x, y)\n \n-    cond_tri = to_triton(cond, device='cuda')\n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n-    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n+    cond_tri = to_triton(cond, device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    y_tri = to_triton(y, device=device, dst_type=dtype)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device=device, dst_type=dtype)\n \n     grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n     where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs, TEST_SCALAR_POINTERS=False)\n@@ -630,7 +636,7 @@ def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n         assert (z == to_numpy(z_tri)).all()\n \n \n-def test_where_broadcast():\n+def test_where_broadcast(device):\n     @triton.jit\n     def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n         xoffsets = tl.arange(0, BLOCK_SIZE)[:, None]\n@@ -656,9 +662,9 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n     x = numpy_random((SIZE, SIZE), dtype_str=dtype, rs=rs)\n     mask = numpy_random(SIZE, 'bool', rs=rs)\n     z = np.where(mask, x, 0)\n-    cond_tri = to_triton(mask, device=\"cuda\")\n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device='cuda', dst_type=dtype)\n+    cond_tri = to_triton(mask, device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device=device, dst_type=dtype)\n     where_kernel[(1,)](cond_tri, x_tri, z_tri, SIZE)\n     assert (z == to_numpy(z_tri)).all()\n     where_scalar_condition[(1,)](x_tri, z_tri, SIZE)\n@@ -675,7 +681,7 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n ] + [\n     (dtype_x, ' ~x') for dtype_x in int_dtypes\n ])\n-def test_unary_op(dtype_x, expr, device='cuda'):\n+def test_unary_op(dtype_x, expr, device):\n     _test_unary(dtype_x, expr, device=device)\n \n # ----------------\n@@ -684,7 +690,7 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n \n \n @pytest.mark.parametrize(\"dtype_x, expr\", [(dtype_x, expr) for dtype_x in [\"float32\", \"float64\"] for expr in ['exp', 'log', 'cos', 'sin']])\n-def test_math_op(dtype_x, expr, device='cuda'):\n+def test_math_op(dtype_x, expr, device):\n     _test_unary(dtype_x, f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n # ----------------\n@@ -696,12 +702,12 @@ def test_math_op(dtype_x, expr, device='cuda'):\n     (dtype_x)\n     for dtype_x in dtypes_with_bfloat16\n ])\n-def test_abs(dtype_x, device='cuda'):\n+def test_abs(dtype_x, device):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n-def test_abs_fp8(in_dtype):\n+def test_abs_fp8(in_dtype, device):\n \n     @triton.jit\n     def abs_kernel(Z, X, SIZE: tl.constexpr):\n@@ -710,7 +716,7 @@ def abs_kernel(Z, X, SIZE: tl.constexpr):\n         z = tl.abs(x)\n         tl.store(Z + off, z)\n \n-    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device=device)\n     # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n     all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n     f8_tensor[all_exp_ones] = 0\n@@ -749,7 +755,7 @@ def make_ptr_str(name, shape):\n               ':, :, None']\n     for d in ['int32', 'uint32', 'uint16']\n ])\n-def test_index1d(expr, dtype_str, device='cuda'):\n+def test_index1d(expr, dtype_str, device):\n     rank_x = expr.count(':')\n     rank_y = expr.count(',') + 1\n     shape_x = [32 for _ in range(rank_x)]\n@@ -784,7 +790,7 @@ def generate_kernel(shape_x, shape_z):\n     z_ref = eval(expr) + y\n     # triton result\n     z_tri = to_triton(np.empty_like(z_ref), device=device)\n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     kernel_match[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n     # compare\n     assert (z_ref == to_numpy(z_tri)).all()\n@@ -813,9 +819,7 @@ def tuples_fn(a, b):\n         a * b\n \n \n-def test_tuples():\n-    device = 'cuda'\n-\n+def test_tuples(device):\n     @triton.jit\n     def with_fn(X, Y, A, B, C):\n         x = tl.load(X)\n@@ -906,9 +910,7 @@ def noinline_multi_values_fn(x, y, Z):\n \n \n @pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\", \"multi_values\"])\n-def test_noinline(mode):\n-    device = 'cuda'\n-\n+def test_noinline(mode, device):\n     @triton.jit\n     def kernel(X, Y, Z):\n         x = tl.load(X)\n@@ -945,7 +947,9 @@ def kernel(X, Y, Z):\n     ]\n     for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']\n     for sem in [None, 'acquire', 'release', 'acq_rel', 'relaxed']]))\n-def test_atomic_rmw(op, dtype_x_str, mode, sem, device='cuda'):\n+def test_atomic_rmw(op, dtype_x_str, mode, sem, device):\n+    check_cuda_only(device)\n+\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         if dtype_x_str == 'float16':\n@@ -995,7 +999,7 @@ def kernel(X, Z):\n     assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n-def test_atomic_rmw_predicate(device=\"cuda\"):\n+def test_atomic_rmw_predicate(device):\n     @triton.jit\n     def kernel(X):\n         val = tl.program_id(0)\n@@ -1008,7 +1012,7 @@ def kernel(X):\n \n @pytest.mark.parametrize(\"shape, axis\",\n                          [(shape, axis) for shape in [(2, 2), (2, 8), (8, 2), (8, 8), (32, 32)] for axis in [0, 1]])\n-def test_tensor_atomic_rmw(shape, axis, device=\"cuda\"):\n+def test_tensor_atomic_rmw(shape, axis, device):\n     shape0, shape1 = shape\n     # triton kernel\n \n@@ -1034,7 +1038,7 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n \n-def test_tensor_atomic_rmw_block(device=\"cuda\"):\n+def test_tensor_atomic_rmw_block(device):\n     shape = (8, 8)\n \n     @triton.jit\n@@ -1051,13 +1055,13 @@ def kernel(X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"sem\", [None, 'acquire', 'release', 'acq_rel', 'relaxed'])\n-def test_atomic_cas(sem):\n+def test_atomic_cas(sem, device):\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n     def change_value(Lock):\n         tl.atomic_cas(Lock, 0, 1)\n \n-    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    Lock = torch.zeros((1,), device=device, dtype=torch.int32)\n     change_value[(1,)](Lock)\n \n     assert (Lock[0] == 1)\n@@ -1074,8 +1078,8 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n         # release lock\n         tl.atomic_xchg(Lock, 0)\n \n-    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n-    data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+    Lock = torch.zeros((1,), device=device, dtype=torch.int32)\n+    data = torch.zeros((128,), device=device, dtype=torch.float32)\n     ref = torch.full((128,), 64.0)\n     h = serialized_add[(64,)](data, Lock, SEM=sem)\n     sem_str = \"acq_rel\" if sem is None else sem\n@@ -1102,10 +1106,10 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n ] + [\n     (f'int{x}', f'uint{x}', True) for x in [8, 16, 32, 64]\n ])\n-def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n+def test_cast(dtype_x, dtype_z, bitcast, device):\n     # bfloat16 on cc < 80 will not be tested\n-    check_type_supported(dtype_x)\n-    check_type_supported(dtype_z)\n+    check_type_supported(dtype_x, device)\n+    check_type_supported(dtype_z, device)\n \n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     x0 = 43 if dtype_x in int_dtypes else 43.5\n@@ -1115,7 +1119,7 @@ def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n         x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n     else:\n         x = np.array([x0], dtype=getattr(np, dtype_x))\n-        x_tri = to_triton(x)\n+        x_tri = to_triton(x, device=device)\n \n     # triton kernel\n     @triton.jit\n@@ -1147,8 +1151,8 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"dtype_str, num_warps\", [(dtype_str, num_warps) for dtype_str in int_dtypes + float_dtypes for num_warps in [4, 8]])\n-def test_cat(dtype_str, num_warps):\n-    check_type_supported(dtype_str)\n+def test_cat(dtype_str, num_warps, device):\n+    check_type_supported(dtype_str, device)\n \n     @triton.jit\n     def kernel(X, Y, Z, N: tl.constexpr):\n@@ -1158,19 +1162,19 @@ def kernel(X, Y, Z, N: tl.constexpr):\n         z = tl.cat(x, y, can_reorder=True)\n         tl.store(Z + tl.arange(0, 2 * N), z)\n \n-    x = torch.arange(0, 128, device='cuda').to(getattr(torch, dtype_str))\n-    y = torch.arange(-128, 0, device='cuda').to(getattr(torch, dtype_str))\n+    x = torch.arange(0, 128, device=device).to(getattr(torch, dtype_str))\n+    y = torch.arange(-128, 0, device=device).to(getattr(torch, dtype_str))\n     z_ref = torch.cat([x, y], dim=0).sum()\n-    z = torch.zeros((256,), dtype=getattr(torch, dtype_str), device='cuda')\n+    z = torch.zeros((256,), dtype=getattr(torch, dtype_str), device=device)\n     kernel[(1, )](x, y, z, N=128, num_warps=num_warps)\n     assert z.sum() == z_ref\n     # check if there's no duplicate value in z\n     assert z.unique().size(0) == z.size(0)\n \n \n @pytest.mark.parametrize(\"dtype_str\", list(torch_dtypes))\n-def test_store_constant(dtype_str):\n-    check_type_supported(dtype_str)\n+def test_store_constant(dtype_str, device):\n+    check_type_supported(dtype_str, device)\n \n     \"\"\"Tests that boolean True is stored as 1\"\"\"\n     @triton.jit\n@@ -1183,14 +1187,14 @@ def kernel(output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     triton_dtype_str = 'uint8' if dtype_str == 'bool' else dtype_str\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.zeros([BLOCK_SIZE], dtype=tl.{triton_dtype_str}) + 1'})\n     block_size = 128\n-    ref = torch.ones([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n-    output = torch.zeros([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n+    ref = torch.ones([block_size], dtype=getattr(torch, dtype_str), device=device)\n+    output = torch.zeros([block_size], dtype=getattr(torch, dtype_str), device=device)\n     kernel[(1,)](output, block_size, BLOCK_SIZE=block_size)\n \n     assert torch.all(output == ref)\n \n \n-def test_load_store_same_ptr():\n+def test_load_store_same_ptr(device):\n     @triton.jit()\n     def kernel(in_out_ptr):\n         pid = tl.program_id(axis=0)\n@@ -1199,7 +1203,7 @@ def kernel(in_out_ptr):\n         tl.store(in_out_ptr + pid, out)\n \n     for _ in range(1000):\n-        x = torch.ones((65536,), device=\"cuda\", dtype=torch.float32)\n+        x = torch.ones((65536,), device=device, dtype=torch.float32)\n         kernel[(65536,)](x, num_warps=32)\n         assert torch.all(x == 2)\n \n@@ -1235,9 +1239,9 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [torch.float16, torch.bfloat16])\n-def test_convert_float16_to_float32(in_dtype):\n+def test_convert_float16_to_float32(in_dtype, device):\n     \"\"\"Tests that check convert_float_to_float32 function\"\"\"\n-    check_type_supported(in_dtype)\n+    check_type_supported(in_dtype, device)\n \n     f16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16).view(in_dtype)\n     f32_output = convert_float_to_float32(f16_input)\n@@ -1271,14 +1275,14 @@ def serialize_fp8(np_data, in_dtype):\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4b15, tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n-def test_fp8_fpN_roundtrip(in_dtype, out_dtype):\n+def test_fp8_fpN_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"\n     For all possible float8 values (ref_fp8 = range(0, 256)), test that:\n         - conversion tri_fp16 = convert(input=ref_fp8, out=out_dtype) matches the reference\n         - conversion tri_fp8 = convert(input=tri_fp16, out=out_dtype) matches the original\n     this is only possible if both conversions are correct\n     \"\"\"\n-    check_type_supported(out_dtype)\n+    check_type_supported(out_dtype, device)\n     from contextlib import nullcontext as does_not_raise\n     expectation = does_not_raise()\n     err_msg = None\n@@ -1318,7 +1322,6 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     if err_msg is not None:\n         assert err_msg in str(e)\n \n-\n # ---------------\n # test reduce\n # ---------------\n@@ -1344,8 +1347,8 @@ def get_reduced_dtype(dtype_str, op):\n                                      'sum']\n                           for dtype in dtypes_with_bfloat16\n                           for shape in [32, 64, 128, 512]])\n-def test_reduce1d(op, dtype_str, shape, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_reduce1d(op, dtype_str, shape, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1417,7 +1420,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n reduce2d_shapes = [(2, 32), (4, 32), (4, 128)]\n # TODO: fix and uncomment\n # , (32, 64), (64, 128)]\n-if 'V100' in torch.cuda.get_device_name(0):\n+if torch.cuda.is_available() and 'V100' in torch.cuda.get_device_name(0):\n     reduce2d_shapes += [(128, 256) and (32, 1024)]\n \n \n@@ -1433,8 +1436,8 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n-def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_reduce2d(op, dtype_str, shape, axis, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1455,7 +1458,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     z_dtype_str = get_reduced_dtype(dtype_str, op)\n@@ -1501,7 +1504,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n @pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n-def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n+def test_reduce_layouts(M, N, src_layout, axis, device):\n     rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n     rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n     store_range = \"%7\" if axis == 0 else \"%1\"\n@@ -1573,7 +1576,7 @@ def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n \n @pytest.mark.parametrize(\"M\", [32, 64, 128, 256])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_store_op(M, src_layout, device='cuda'):\n+def test_store_op(M, src_layout, device):\n     ir = f\"\"\"\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n@@ -1622,7 +1625,7 @@ def test_store_op(M, src_layout, device='cuda'):\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n @pytest.mark.parametrize(\"src_dim\", [0, 1])\n @pytest.mark.parametrize(\"dst_dim\", [0, 1])\n-def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device='cuda'):\n+def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):\n     ir = f\"\"\"\n     #dst = {dst_layout}\n     #src = {src_layout}\n@@ -1679,7 +1682,7 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n \n @pytest.mark.parametrize(\"M, N\", [[128, 128], [256, 128], [256, 256], [128, 256]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_chain_reduce(M, N, src_layout, device='cuda'):\n+def test_chain_reduce(M, N, src_layout, device):\n     ir = f\"\"\"\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n@@ -1731,7 +1734,7 @@ def test_chain_reduce(M, N, src_layout, device='cuda'):\n     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n-def test_generic_reduction(device='cuda'):\n+def test_generic_reduction(device):\n \n     @triton.jit\n     def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n@@ -1767,8 +1770,8 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n                           for dtype in ['float16', 'float32']\n                              for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n-def test_permute(dtype_str, shape, perm, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_permute(dtype_str, shape, perm, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1838,7 +1841,9 @@ def kernel(X, stride_xm, stride_xn,\n                                                       ('float16', 'float16'),\n                                                       ('float16', 'float32'),\n                                                       ('float32', 'float32')]])\n-def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device='cuda'):\n+def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device):\n+    check_cuda_only(device)\n+\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n@@ -1997,9 +2002,9 @@ def kernel(X, stride_xm, stride_xk,\n \n \n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n-def test_full(dtype_str):\n+def test_full(dtype_str, device):\n     dtype = getattr(torch, dtype_str)\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     @triton.jit\n     def kernel_static(out):\n@@ -2014,9 +2019,9 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n         tl.store(out_ptr, a)\n \n     kernel_static_patched = patch_kernel(kernel_static, {'GENERATE_TEST_HERE': f\"tl.full((128,), 2, tl.{dtype_str})\"})\n-    out_static = torch.zeros((128), dtype=dtype, device=\"cuda\")\n+    out_static = torch.zeros((128), dtype=dtype, device=device)\n     kernel_static_patched[(1,)](out_static)\n-    out_dynamic = torch.zeros((128), dtype=dtype, device=\"cuda\")\n+    out_dynamic = torch.zeros((128), dtype=dtype, device=device)\n     kernel_dynamic[(1,)](out_dynamic, 2, getattr(triton.language, dtype_str))\n     assert torch.all(out_static == 2)\n     assert torch.all(out_dynamic == 2)\n@@ -2028,20 +2033,20 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n                           ('float(\"nan\")', \"f32\"), ('float(\"-nan\")', \"f32\"),\n                           (0., \"f32\"),\n                           (5, \"i32\"), (2**40, \"i64\"),])\n-def test_constexpr(literal, dtype_str):\n+def test_constexpr(literal, dtype_str, device):\n     @triton.jit\n     def kernel(out_ptr):\n         val = GENERATE_TEST_HERE\n         tl.store(out_ptr.to(tl.pointer_type(val.dtype)), val)\n \n     kernel_patched = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{literal}\"})\n-    out = torch.zeros((1,), dtype=torch.float32, device=\"cuda\")\n+    out = torch.zeros((1,), dtype=torch.float32, device=device)\n     h = kernel_patched[(1,)](out)\n     assert re.search(r\"arith.constant .* : \" + dtype_str, h.asm[\"ttir\"]) is not None\n \n # TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n # @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n-# def test_dot_without_load(dtype_str):\n+# def test_dot_without_load(dtype_str, device=device):\n #     @triton.jit\n #     def _kernel(out):\n #         a = GENERATE_TEST_HERE\n@@ -2051,10 +2056,10 @@ def kernel(out_ptr):\n #         tl.store(out_ptr, c)\n \n #     kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n-#     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n-#     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+#     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+#     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n #     out_ref = torch.matmul(a, b)\n-#     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n+#     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=device)\n #     kernel[(1,)](out)\n #     assert torch.all(out == out_ref)\n \n@@ -2064,7 +2069,7 @@ def kernel(out_ptr):\n \n \n @pytest.mark.parametrize(\"start\", [0, 1, 7, 16])\n-def test_arange(start, device='cuda'):\n+def test_arange(start, device):\n     BLOCK = 128\n     z_tri = torch.empty(BLOCK, dtype=torch.int32, device=device)\n \n@@ -2084,9 +2089,9 @@ def _kernel(z, BLOCK: tl.constexpr,\n \n \n @pytest.mark.parametrize(\"dtype_str, size, size_diff\", [(dtype_str, size, size_diff) for dtype_str in torch_dtypes for size in [128, 512] for size_diff in [0, 1, 2, 3, 4]])\n-def test_masked_load(dtype_str, size, size_diff, device='cuda'):\n+def test_masked_load(dtype_str, size, size_diff, device):\n     dtype = getattr(torch, dtype_str)\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     input_size = size - size_diff\n     output_size = size\n@@ -2119,8 +2124,8 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n-def test_masked_load_shared_memory(dtype, device='cuda'):\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+def test_masked_load_shared_memory(dtype, device):\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     M = 32\n     N = 32\n@@ -2168,9 +2173,9 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n \n \n @pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])\n-def test_load_cache_modifier(cache):\n-    src = torch.empty(128, device='cuda')\n-    dst = torch.empty(128, device='cuda')\n+def test_load_cache_modifier(cache, device):\n+    src = torch.empty(128, device=device)\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst, src, CACHE: tl.constexpr):\n@@ -2192,9 +2197,9 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"N\", [16, 10, 11, 1024])\n-def test_vectorization(N):\n-    src = torch.empty(1024, device='cuda')\n-    dst = torch.empty(1024, device='cuda')\n+def test_vectorization(N, device):\n+    src = torch.empty(1024, device=device)\n+    dst = torch.empty(1024, device=device)\n \n     @triton.jit\n     def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n@@ -2211,10 +2216,10 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"has_hints\", [False, True])\n-def test_vectorization_hints(has_hints):\n-    src = torch.empty(1024, device='cuda')\n-    dst = torch.empty(1024, device='cuda')\n-    off = torch.zeros(1, device='cuda', dtype=torch.int32)\n+def test_vectorization_hints(has_hints, device):\n+    src = torch.empty(1024, device=device)\n+    dst = torch.empty(1024, device=device)\n+    off = torch.zeros(1, device=device, dtype=torch.int32)\n \n     @triton.jit\n     def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n@@ -2289,10 +2294,10 @@ def _impl(value=10):\n     return value\n \n \n-def test_default():\n+def test_default(device):\n     value = 5\n-    ret0 = torch.zeros(1, dtype=torch.int32, device='cuda')\n-    ret1 = torch.zeros(1, dtype=torch.int32, device='cuda')\n+    ret0 = torch.zeros(1, dtype=torch.int32, device=device)\n+    ret1 = torch.zeros(1, dtype=torch.int32, device=device)\n \n     @triton.jit\n     def _kernel(ret0, ret1, value):\n@@ -2308,7 +2313,7 @@ def _kernel(ret0, ret1, value):\n # ----------------\n \n \n-def test_noop(device='cuda'):\n+def test_noop(device):\n     @triton.jit\n     def kernel(x):\n         pass\n@@ -2335,7 +2340,7 @@ def kernel(x):\n     (2**31, 'i64'), (2**32 - 1, 'i64'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n     (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n ])\n-def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n+def test_value_specialization(value: int, value_type: str, device) -> None:\n     spec_type = None\n \n     def cache_hook(*args, **kwargs):\n@@ -2347,7 +2352,7 @@ def cache_hook(*args, **kwargs):\n     def kernel(VALUE, X):\n         pass\n \n-    x = torch.tensor([3.14159], device='cuda')\n+    x = torch.tensor([3.14159], device=device)\n     pgm = kernel[(1, )](value, x)\n \n     JITFunction.cache_hook = None\n@@ -2362,13 +2367,13 @@ def kernel(VALUE, X):\n     \"value, overflow\",\n     [(2**64 - 1, False), (2**64, True), (-2**63, False), (-2**63 - 1, True)]\n )\n-def test_value_specialization_overflow(value: int, overflow: bool, device='cuda') -> None:\n+def test_value_specialization_overflow(value: int, overflow: bool, device) -> None:\n \n     @triton.jit\n     def kernel(VALUE, X):\n         pass\n \n-    x = torch.tensor([3.14159], device='cuda')\n+    x = torch.tensor([3.14159], device=device)\n \n     if overflow:\n         with pytest.raises(OverflowError):\n@@ -2384,7 +2389,7 @@ def kernel(VALUE, X):\n @pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>', '<<', '>>', '&', '^', '|'])\n @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n-def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n+def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr, device):\n \n     @triton.jit\n     def kernel(Z, X, Y):\n@@ -2405,34 +2410,34 @@ def kernel(Z, X, Y):\n         y = numpy_random((1,), dtype_str=\"float32\")\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n     z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n-    x_tri = to_triton(x)\n-    y_tri = to_triton(y)\n-    z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    z_tri = to_triton(np.empty((1,), dtype=z.dtype), device=device)\n     kernel[(1,)](z_tri, x_tri, y_tri)\n     np.testing.assert_allclose(z, to_numpy(z_tri))\n \n \n-def test_constexpr_shape():\n+def test_constexpr_shape(device):\n \n     @triton.jit\n     def kernel(X):\n         off = tl.arange(0, 128 + 128)\n         tl.store(X + off, off)\n \n-    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32), device=device)\n     kernel[(1,)](x_tri)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))\n \n \n-def test_constexpr_scalar_shape():\n+def test_constexpr_scalar_shape(device):\n \n     @triton.jit\n     def kernel(X, s):\n         off = tl.arange(0, 256)\n         val = off % (256 // s)\n         tl.store(X + off, val)\n \n-    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32), device=device)\n     kernel[(1,)](x_tri, 32)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256) % 8)\n \n@@ -2466,7 +2471,7 @@ def vecmul_kernel(ptr, n_elements, rep, type: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"type\", [\"inline\", \"noinline\"])\n-def test_call(type):\n+def test_call(type, device):\n \n     @triton.jit\n     def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n@@ -2475,7 +2480,7 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n \n     size = 1024\n     rand_val = numpy_random((size,), dtype_str=\"float32\")\n-    rand_val_tri = to_triton(rand_val, device='cuda')\n+    rand_val_tri = to_triton(rand_val, device=device)\n     err_msg = \"\"\n     try:\n         kernel[(size // 128,)](rand_val_tri, size, 3, 5, type)\n@@ -2493,8 +2498,8 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n-def test_if(if_type):\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+def test_if(if_type, device):\n \n     @triton.jit\n     def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr, StaticVaue: tl.constexpr):\n@@ -2518,16 +2523,17 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n             else:\n                 tl.store(Ret, tl.load(XFalse))\n \n-    cond = torch.ones(1, dtype=torch.int32, device='cuda')\n-    x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n-    x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n-    ret = torch.empty(1, dtype=torch.float32, device='cuda')\n+    cond = torch.ones(1, dtype=torch.int32, device=device)\n+    x_true = torch.tensor([3.14], dtype=torch.float32, device=device)\n+    x_false = torch.tensor([1.51], dtype=torch.float32, device=device)\n+    ret = torch.empty(1, dtype=torch.float32, device=device)\n+\n     kernel[(1,)](cond, x_true, x_false, ret, if_type, True, 1)\n     assert torch.equal(ret, x_true)\n \n \n-def test_num_warps_pow2():\n-    dst = torch.empty(128, device='cuda')\n+def test_num_warps_pow2(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -2551,7 +2557,7 @@ def _kernel(dst):\n                           ('float32', 'math.pow', tl.math.libdevice_path()),\n                           ('float64', 'math.pow_dtype', tl.math.libdevice_path()),\n                           ('float64', 'math.norm4d', '')])\n-def test_math_tensor(dtype_str, expr, lib_path):\n+def test_math_tensor(dtype_str, expr, lib_path, device):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -2592,9 +2598,9 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{expr}(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))\n \n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     # triton result\n-    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     if expr == 'math.ffs':\n@@ -2607,7 +2613,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n                          [('float32', 'math.pow', ''),\n                           ('float64', 'math.pow_dtype', ''),\n                           ('float64', 'math.pow', tl.math.libdevice_path())])\n-def test_math_scalar(dtype_str, expr, lib_path):\n+def test_math_scalar(dtype_str, expr, lib_path, device):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -2632,8 +2638,8 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         y_ref[:] = np.power(x, 0.5)\n \n     # triton result\n-    x_tri = to_triton(x)[0].item()\n-    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    x_tri = to_triton(x, device=device)[0].item()\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n@@ -2646,7 +2652,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"lo, hi, iv\", [(2**35, 2**35 + 20, 1), (2**35, 2**35 + 20, 2), (2**35, 2**35 + 20, 3),\n                                         (15, -16, -1), (15, -16, -2), (15, -16, -3),\n                                         (-18, -22, -1), (22, 18, -1)])\n-def test_for_iv(lo, hi, iv):\n+def test_for_iv(lo, hi, iv, device):\n \n     @triton.jit\n     def kernel(Out, lo, hi, iv: tl.constexpr):\n@@ -2658,12 +2664,12 @@ def kernel(Out, lo, hi, iv: tl.constexpr):\n \n     lo = 2**35\n     hi = 2**35 + 20\n-    out = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int64), device=device)\n     kernel[(1,)](out, lo, hi, iv)\n     assert out[0] == sum(range(lo, hi, iv))\n \n \n-def test_if_else():\n+def test_if_else(device):\n \n     @triton.jit\n     def kernel(Cond, TrueVal, FalseVal, Out):\n@@ -2673,10 +2679,10 @@ def kernel(Cond, TrueVal, FalseVal, Out):\n             val = tl.load(FalseVal)\n         tl.store(Out, val)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    true_val = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    false_val = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n-    cond = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    true_val = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    false_val = to_triton(np.full((1,), 2, dtype=np.int32), device=device)\n+    cond = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     # True\n     cond[0] = True\n     kernel[(1,)](cond, true_val, false_val, out)\n@@ -2688,7 +2694,7 @@ def kernel(Cond, TrueVal, FalseVal, Out):\n \n \n @pytest.mark.parametrize(\"mode\", [\"dynamic\", \"static\"])\n-def test_if_return(mode):\n+def test_if_return(mode, device):\n \n     @triton.jit\n     def kernel(ExitEarly, Out, cond: tl.constexpr, mode: tl.constexpr):\n@@ -2702,8 +2708,8 @@ def kernel(ExitEarly, Out, cond: tl.constexpr, mode: tl.constexpr):\n                 return\n         tl.store(Out, 1)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    exit_early = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    exit_early = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     # exit early path taken\n     exit_early[0] = 1\n     kernel[(1,)](exit_early, out, True, mode)\n@@ -2748,7 +2754,7 @@ def add_fn_static_cond(x, cond: tl.constexpr):\n @pytest.mark.parametrize(\"call_type\", [\"attribute\", \"attribute_jit\",\n                                        \"jit\", \"jit_if\", \"jit_ifexp\", \"jit_expr\",\n                                        \"jit_static_cond\", \"jit_noinline\", \"jit_extern\"])\n-def test_if_call(call_type):\n+def test_if_call(call_type, device):\n     @triton.jit\n     def kernel(Out, call_type: tl.constexpr):\n         pid = tl.program_id(0)\n@@ -2807,15 +2813,15 @@ def kernel(Out, call_type: tl.constexpr):\n \n         tl.store(Out, o)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     kernel[(1,)](out, call_type)\n     assert to_numpy(out)[0] == 1\n \n \n @pytest.mark.parametrize(\"_cond1\", [True, False])\n @pytest.mark.parametrize(\"_cond2\", [True, False])\n @pytest.mark.parametrize(\"_cond3\", [True, False])\n-def test_nested_if_else_return(_cond1, _cond2, _cond3):\n+def test_nested_if_else_return(_cond1, _cond2, _cond3, device):\n \n     @triton.jit\n     def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n@@ -2832,13 +2838,13 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n                 val = tl.load(Val3)\n         tl.store(Out, val)\n \n-    out = to_triton(np.full((1,), -1, dtype=np.int32), device='cuda')\n-    cond1 = to_triton(np.full((1,), _cond1, dtype=np.int32), device='cuda')\n-    cond2 = to_triton(np.full((1,), _cond2, dtype=np.int32), device='cuda')\n-    cond3 = to_triton(np.full((1,), _cond3, dtype=np.int32), device='cuda')\n-    val1 = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    val2 = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n-    val3 = to_triton(np.full((1,), 3, dtype=np.int32), device='cuda')\n+    out = to_triton(np.full((1,), -1, dtype=np.int32), device=device)\n+    cond1 = to_triton(np.full((1,), _cond1, dtype=np.int32), device=device)\n+    cond2 = to_triton(np.full((1,), _cond2, dtype=np.int32), device=device)\n+    cond3 = to_triton(np.full((1,), _cond3, dtype=np.int32), device=device)\n+    val1 = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    val2 = to_triton(np.full((1,), 2, dtype=np.int32), device=device)\n+    val3 = to_triton(np.full((1,), 3, dtype=np.int32), device=device)\n     kernel[(1,)](cond1, cond2, cond3, val1, val2, val3, out)\n     targets = {\n         (True, True, True): val1[0],\n@@ -2853,7 +2859,7 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n     assert out[0] == targets[(_cond1, _cond2, _cond3)]\n \n \n-def test_while():\n+def test_while(device):\n \n     @triton.jit\n     def kernel(InitI, Bound, CutOff, OutI, OutJ):\n@@ -2866,16 +2872,16 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n         tl.store(OutI, curr_i)\n         tl.store(OutJ, j)\n \n-    out_i = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    out_j = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    init_i = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    bound = to_triton(np.full((1,), 10, dtype=np.int32), device='cuda')\n-    cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device='cuda')\n+    out_i = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    out_j = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    init_i = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    bound = to_triton(np.full((1,), 10, dtype=np.int32), device=device)\n+    cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device=device)\n     kernel[(1,)](init_i, bound, cut_off, out_i, out_j)\n     assert out_i[0] == init_i[0] + 1\n     assert out_j[0] == cut_off[0] + 1\n \n-# def test_for_if():\n+# def test_for_if(device):\n \n #     @triton.jit\n #     def kernel(bound, cutoff, M, N):\n@@ -2889,8 +2895,8 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n #         tl.store(M, m)\n #         tl.store(N, n)\n \n-#     m = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-#     n = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+#     m = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+#     n = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n #     kernel[(1,)](10, 7, m, n)\n #     print(m[0])\n #     print(n[0])\n@@ -2900,7 +2906,8 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n # -----------------------\n \n \n-def test_globaltimer():\n+def test_globaltimer(device):\n+    check_cuda_only(device)\n \n     @triton.jit\n     def kernel(Out1, Out2):\n@@ -2911,21 +2918,22 @@ def kernel(Out1, Out2):\n         end = tl.extra.cuda.globaltimer()\n         tl.store(Out2, end - start)\n \n-    out1 = to_triton(np.zeros((128,), dtype=np.int64), device='cuda')\n-    out2 = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    out1 = to_triton(np.zeros((128,), dtype=np.int64), device=device)\n+    out2 = to_triton(np.zeros((1,), dtype=np.int64), device=device)\n     h = kernel[(1,)](out1, out2)\n     assert out2[0] > 0\n     # 2 inlined globaltimers + one extra in the wrapper extern function\n     assert h.asm[\"ptx\"].count(\"%globaltimer\") == 3\n \n \n-def test_smid():\n+def test_smid(device):\n+    check_cuda_only(device)\n \n     @triton.jit\n     def kernel(Out):\n         tl.store(Out + tl.program_id(0), tl.extra.cuda.smid())\n \n-    out = to_triton(np.zeros((1024,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1024,), dtype=np.int32), device=device)\n     h = kernel[(out.shape[0],)](out)\n     assert out.sort()[0].unique().shape[0] > 0\n     assert h.asm[\"ptx\"].count(\"%smid\") == 2\n@@ -2963,7 +2971,7 @@ def kernel(Out):\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"interm_layout\", intermediate_layouts)\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n-def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='cuda'):\n+def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n@@ -3015,7 +3023,7 @@ def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='\n }\n \"\"\"\n \n-    x = to_triton(numpy_random(shape, dtype_str=dtype))\n+    x = to_triton(numpy_random(shape, dtype_str=dtype), device=device)\n     z = torch.empty_like(x)\n \n     # write the IR to a temporary file using mkstemp\n@@ -3029,23 +3037,23 @@ def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='\n     assert torch.equal(z, x)\n \n \n-def test_load_scalar_with_mask():\n+def test_load_scalar_with_mask(device):\n     @triton.jit\n     def kernel(Input, Index, Out, N: int):\n         index = tl.load(Index)\n         scalar = tl.load(Input + index, mask=index < N, other=0)\n         tl.store(Out, scalar, mask=index < N)\n-    Index = torch.tensor([0], dtype=torch.int32, device='cuda')\n-    Input = torch.tensor([0], dtype=torch.int32, device='cuda')\n-    Out = torch.empty_like(Index, device='cuda')\n+    Index = torch.tensor([0], dtype=torch.int32, device=device)\n+    Input = torch.tensor([0], dtype=torch.int32, device=device)\n+    Out = torch.empty_like(Index, device=device)\n     kernel[(1,)](Input, Index, Out, Index.numel())\n     assert Out.data[0] == 0\n \n \n # This test is used to test our own PTX codegen for float16 and int16 conversions\n # maybe delete it later after ptxas has been fixed\n @pytest.mark.parametrize(\"dtype_str\", ['float16', 'int16'])\n-def test_ptx_cast(dtype_str):\n+def test_ptx_cast(dtype_str, device):\n     @triton.jit\n     def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n         xoffset = tl.program_id(0) * XBLOCK\n@@ -3075,7 +3083,7 @@ def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.co\n         triton_dtype = tl.float32\n \n     s0 = 4\n-    buf11 = -torch.ones((6 * s0, 197, 197), device='cuda', dtype=torch_dtype)\n-    buf14 = -torch.ones((s0, 6, 197, 197), device='cuda', dtype=torch_dtype)\n+    buf11 = -torch.ones((6 * s0, 197, 197), device=device, dtype=torch_dtype)\n+    buf14 = -torch.ones((s0, 6, 197, 197), device=device, dtype=torch_dtype)\n     kernel[(4728,)](buf11, buf14, 1182 * s0, 197, triton_dtype, 1, 256, num_warps=2)\n     assert buf14.to(torch.float32).mean() == -2.0"}, {"filename": "python/test/unit/language/test_random.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -115,7 +115,7 @@ def random_raw(self):\n                          [(size, seed) for size in ['10', '4,53', '10000']\n                           for seed in [0, 42, 124, 54, 0xffffffff, 0xdeadbeefcafeb0ba]]\n                          )\n-def test_randint(size, seed, device='cuda'):\n+def test_randint(size, seed, device):\n     size = list(map(int, size.split(',')))\n \n     @triton.jit\n@@ -141,7 +141,7 @@ def kernel(X, N, seed):\n                          [(size, seed) for size in [1000000]\n                           for seed in [0, 42, 124, 54]]\n                          )\n-def test_rand(size, seed, device='cuda'):\n+def test_rand(size, seed, device):\n     @triton.jit\n     def kernel(X, N, seed):\n         offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n@@ -162,7 +162,7 @@ def kernel(X, N, seed):\n                          [(size, seed) for size in [1000000]\n                           for seed in [0, 42, 124, 54]]\n                          )\n-def test_randn(size, seed, device='cuda'):\n+def test_randn(size, seed, device):\n     @triton.jit\n     def kernel(X, N, seed):\n         offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n@@ -179,7 +179,7 @@ def kernel(X, N, seed):\n \n # tl.rand() should never produce >=1.0\n \n-def test_rand_limits():\n+def test_rand_limits(device):\n     @triton.jit\n     def kernel(input, output, n: tl.constexpr):\n         idx = tl.arange(0, n)\n@@ -190,8 +190,8 @@ def kernel(input, output, n: tl.constexpr):\n     min_max_int32 = torch.tensor([\n         torch.iinfo(torch.int32).min,\n         torch.iinfo(torch.int32).max,\n-    ], dtype=torch.int32, device='cuda')\n-    output = torch.empty(2, dtype=torch.float32, device='cuda')\n+    ], dtype=torch.int32, device=device)\n+    output = torch.empty(2, dtype=torch.float32, device=device)\n     kernel[(1,)](min_max_int32, output, 2)\n \n     assert output[0] == output[1]"}, {"filename": "python/triton/common/backend.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -85,10 +85,10 @@ def register_backend(device_type: str, backend_cls: type):\n \n def get_backend(device_type: str):\n     if device_type not in _backends:\n-        device_backend_package_name = f\"triton.third_party.{device_type}\"\n-        if importlib.util.find_spec(device_backend_package_name):\n+        device_backend_package_name = f\"...third_party.{device_type}\"\n+        if importlib.util.find_spec(device_backend_package_name, package=__spec__.name):\n             try:\n-                importlib.import_module(device_backend_package_name)\n+                importlib.import_module(device_backend_package_name, package=__spec__.name)\n             except Exception:\n                 return None\n         else:"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -11,7 +11,6 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-# import triton\n from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n                                    get_shared_memory_size, ir,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,"}, {"filename": "python/triton/interpreter/memory_map.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2,7 +2,7 @@\n \n import dataclasses\n \n-from triton.interpreter import torch_wrapper\n+from . import torch_wrapper\n \n torch = torch_wrapper.torch\n "}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-# import triton\n from ..language import core as lcore\n from . import torch_wrapper\n from .core import ExecutionContext"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -6,7 +6,6 @@\n from typing import Callable, List, Sequence, TypeVar\n \n from .._C.libtriton.triton import ir\n-# import triton\n from ..runtime.jit import jit\n from . import semantic\n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -4,8 +4,8 @@\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n+from .._C.libtriton.triton import ir\n from . import core as tl\n-from triton._C.libtriton.triton import ir\n \n T = TypeVar('T')\n "}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -3,9 +3,6 @@\n from ... import cdiv, heuristics, jit\n from ... import language as tl\n \n-# import triton\n-# import language as tl\n-\n # ********************************************************\n # --------------------------------------------------------\n # Sparse = Dense x Dense (SDD)"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,7 +1,5 @@\n import torch\n \n-# import triton\n-# import language as tl\n from ... import jit\n from ... import language as tl\n from ... import next_power_of_2"}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,7 +1,5 @@\n import torch\n \n-# import triton\n-# import language as tl\n from .. import heuristics, jit\n from .. import language as tl\n from .. import next_power_of_2"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -10,9 +10,6 @@\n from .. import cdiv, jit\n from .. import language as tl\n \n-# import triton\n-# import language as tl\n-\n \n @jit\n def _fwd_kernel("}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -4,9 +4,6 @@\n from .. import language as tl\n from .matmul_perf_model import early_config_prune, estimate_matmul_time\n \n-# import triton\n-# import language as tl\n-\n \n def init_to_zero(name):\n     return lambda nargs: nargs[name].zero_()"}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -2,7 +2,6 @@\n \n import torch\n \n-# import triton\n from .. import cdiv\n from .._C.libtriton.triton import runtime\n from ..runtime import driver"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -11,8 +11,6 @@\n from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n                     overload)\n \n-# import triton\n-# from .. import compile, CompiledKernel\n from ..common.backend import get_backend\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n@@ -71,7 +69,7 @@ def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n         while isinstance(lhs, ast.Attribute):\n             lhs = self.visit(lhs.value)\n-        if lhs is None or getattr(lhs, \"__name__\", \"\") == \"triton\":\n+        if lhs is None or (getattr(lhs, \"__name__\", \"\") == \"triton\" or getattr(lhs, \"__name__\", \"\").endswith(\".triton\")):\n             return None\n         return getattr(lhs, node.attr)\n \n@@ -81,7 +79,7 @@ def visit_Call(self, node):\n             return\n         if inspect.isbuiltin(func):\n             return\n-        if func.__module__ and func.__module__.startswith('triton.'):\n+        if func.__module__ and (func.__module__.startswith('triton.') or '.triton.' in func.__module__):\n             return\n         assert isinstance(func, JITFunction), f\"Function \\\"{func.__name__}\\\" is being called from a Triton function but is not a Triton function itself. Decorate it with @triton.jit to fix this\"\n         if func.hash is None:"}]