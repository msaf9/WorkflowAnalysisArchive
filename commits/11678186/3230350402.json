[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -1191,13 +1191,13 @@ def _kernel(z, BLOCK: tl.constexpr,\n # ---------------\n \n \n-@pytest.mark.parametrize(\"dtype_str\", torch_dtypes)\n-def test_masked_load(dtype_str, device='cuda'):\n+@pytest.mark.parametrize(\"dtype_str, size, size_diff\", [(dtype_str, size, size_diff) for dtype_str in torch_dtypes for size in [128, 512] for size_diff in [1, 2, 3, 4]])\n+def test_masked_load(dtype_str, size, size_diff, device='cuda'):\n     dtype = getattr(torch, dtype_str)\n     check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n \n-    input_size = 126\n-    output_size = 128\n+    input_size = size - size_diff\n+    output_size = size\n     if dtype_str == 'bool':\n         input = torch.randint(0, 2, (input_size,), dtype=dtype, device=device)\n     elif dtype_str in int_dtypes or dtype_str in uint_dtypes:\n@@ -1218,7 +1218,7 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n     _kernel[(1,)](input, output, input_size, output_size)\n \n     reference_out = input\n-    reference_out = torch.cat((reference_out, torch.ones((2,), dtype=dtype, device=device)))\n+    reference_out = torch.cat((reference_out, torch.ones((size_diff,), dtype=dtype, device=device)))\n     triton.testing.allclose(output, reference_out)\n \n "}]