[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 51, "deletions": 47, "changes": 98, "file_content_changes": "@@ -15,7 +15,7 @@\n @triton.jit\n def _fwd_kernel(\n     Q, K, V, sm_scale,\n-    L, M,\n+    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to work around a compiler bug\n     Out,\n     stride_qz, stride_qh, stride_qm, stride_qk,\n     stride_kz, stride_kh, stride_kn, stride_kk,\n@@ -32,55 +32,58 @@ def _fwd_kernel(\n     offs_n = tl.arange(0, BLOCK_N)\n     offs_d = tl.arange(0, BLOCK_DMODEL)\n     off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n-    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :] * stride_kk\n     off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n     # Initialize pointers to Q, K, V\n     q_ptrs = Q + off_q\n     k_ptrs = K + off_k\n     v_ptrs = V + off_v\n     # initialize pointer to m and l\n-    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n     acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n     # load q: it will stay in SRAM throughout\n     q = tl.load(q_ptrs)\n     # loop over k, v and update accumulator\n     for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n+        # start_n = tl.multiple_of(start_n, BLOCK_N)\n         # -- compute qk ----\n-        k = tl.load(k_ptrs)\n+        k = tl.load(k_ptrs + start_n * stride_kn)\n         qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-        qk += tl.dot(q, k)\n+        qk += tl.dot(q, tl.trans(k))\n         qk *= sm_scale\n-        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk, float(\"-inf\"))\n-        # compute new m\n-        m = tl.maximum(tl.max(qk, 1), m_prev)\n-        # correct old l\n-        l_prev *= tl.exp(m_prev - m)\n-        # attention weights\n-        p = tl.exp(qk - m[:, None])\n-        l = tl.sum(p, 1) + l_prev\n-        l_rcp = 1. / l\n-        # rescale operands of matmuls\n-        p *= l_rcp\n-        acc *= (l_prev * l_rcp)[:, None]\n+        qk += tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), 0, float(\"-inf\"))\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        p = tl.exp(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        alpha = tl.exp(m_i - m_i_new)\n+        beta = tl.exp(m_ij - m_i_new)\n+        l_i_new = alpha * l_i + beta * l_ij\n+        # -- update output accumulator --\n+        # scale p\n+        p_scale = beta / l_i_new\n+        p = p * p_scale[:, None]\n+        # scale acc\n+        acc_scale = l_i / l_i_new * alpha\n+        acc = acc * acc_scale[:, None]\n         # update acc\n+        v = tl.load(v_ptrs + start_n * stride_vk)\n         p = p.to(tl.float16)\n-        v = tl.load(v_ptrs)\n         acc += tl.dot(p, v)\n         # update m_i and l_i\n-        l_prev = l\n-        m_prev = m\n-        # update pointers\n-        k_ptrs += BLOCK_N * stride_kn\n-        v_ptrs += BLOCK_N * stride_vk\n+        l_i = l_i_new\n+        m_i = m_i_new\n     # rematerialize offsets to save registers\n     start_m = tl.program_id(0)\n     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n     # write back l and m\n     l_ptrs = L + off_hz * N_CTX + offs_m\n     m_ptrs = M + off_hz * N_CTX + offs_m\n-    tl.store(l_ptrs, l_prev)\n-    tl.store(m_ptrs, m_prev)\n+    tl.store(l_ptrs, l_i)\n+    tl.store(m_ptrs, m_i)\n     # initialize pointers to output\n     offs_n = tl.arange(0, BLOCK_DMODEL)\n     off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n@@ -206,33 +209,34 @@ def forward(ctx, q, k, v, sm_scale):\n         assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n         grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1], 1)\n+        tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         num_warps = 4 if Lk <= 64 else 8\n \n-        h = _fwd_kernel[grid](\n+        _fwd_kernel[grid](\n             q, k, v, sm_scale,\n-            L, m,\n+            tmp, L, m,\n             o,\n             q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n             k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n             q.shape[0], q.shape[1], q.shape[2],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=Lk, num_warps=num_warps,\n-            num_stages=1,\n+            BLOCK_DMODEL=Lk, num_warps=4,\n+            num_stages=2,\n         )\n \n         ctx.save_for_backward(q, k, v, o, L, m)\n-        ctx.BLOCK = BLOCK\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n         ctx.BLOCK_DMODEL = Lk\n         return o\n \n     @staticmethod\n     def backward(ctx, do):\n+        BLOCK = 128\n         q, k, v, o, l, m = ctx.saved_tensors\n         do = do.contiguous()\n         dq = torch.zeros_like(q, dtype=torch.float32)\n@@ -243,9 +247,8 @@ def backward(ctx, do):\n         _bwd_preprocess[(ctx.grid[0] * ctx.grid[1], )](\n             o, do, l,\n             do_scaled, delta,\n-            BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n+            BLOCK_M=BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n-\n         _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n@@ -257,7 +260,7 @@ def backward(ctx, do):\n             v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n             q.shape[0], q.shape[1], q.shape[2],\n             ctx.grid[0],\n-            BLOCK_M=ctx.BLOCK, BLOCK_N=ctx.BLOCK,\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n             BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n             num_stages=1,\n         )\n@@ -270,9 +273,9 @@ def backward(ctx, do):\n @pytest.mark.parametrize('Z, H, N_CTX, D_HEAD', [(4, 48, 1024, 64)])\n def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     torch.manual_seed(20)\n-    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.1).requires_grad_()\n-    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.1).requires_grad_()\n-    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.1).requires_grad_()\n+    q = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.1, std=0.2).requires_grad_()\n+    k = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.4, std=0.2).requires_grad_()\n+    v = torch.empty((Z, H, N_CTX, D_HEAD), dtype=dtype, device=\"cuda\").normal_(mean=0.3, std=0.2).requires_grad_()\n     sm_scale = 0.2\n     dout = torch.randn_like(q)\n     # reference implementation\n@@ -303,19 +306,25 @@ def test_op(Z, H, N_CTX, D_HEAD, dtype=torch.float16):\n     triton.testing.assert_almost_equal(ref_dq, tri_dq)\n \n \n+try:\n+    from flash_attn.flash_attn_interface import flash_attn_func\n+    HAS_FLASH = True\n+except BaseException:\n+    HAS_FLASH = False\n+\n BATCH, N_HEADS, N_CTX, D_HEAD = 4, 48, 4096, 64\n # vary seq length for fixed head and batch=4\n configs = [triton.testing.Benchmark(\n     x_names=['N_CTX'],\n-    x_vals=[2**i for i in range(10, 14)],\n+    x_vals=[2**i for i in range(10, 15)],\n     line_arg='provider',\n-    line_vals=['triton'],\n-    line_names=['Triton'],\n+    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n     styles=[('red', '-'), ('blue', '-')],\n     ylabel='ms',\n     plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}-{mode}',\n     args={'H': N_HEADS, 'BATCH': BATCH, 'D_HEAD': D_HEAD, 'dtype': torch.float16, 'mode': mode}\n-) for mode in ['bwd']]\n+) for mode in ['fwd']]\n \n \n @triton.testing.perf_report(configs)\n@@ -334,10 +343,6 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n             do = torch.randn_like(o)\n             fn = lambda: o.backward(do, retain_graph=True)\n         ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n-        flops_per_matmul = 2. * BATCH * H * N_CTX * N_CTX * D_HEAD * 0.5\n-        total_flops = 2 * flops_per_matmul\n-        # print(total_flops/ms*1e-9)\n-        print(ms)\n         return ms\n     if provider == \"flash\":\n         lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n@@ -352,5 +357,4 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n         return ms\n \n-\n-bench_flash_attention.run(save_path='.', print_data=True)\n+# bench_flash_attention.run(save_path='.', print_data=True)"}]