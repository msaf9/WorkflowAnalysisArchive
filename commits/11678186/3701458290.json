[{"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -3685,22 +3685,24 @@ DotOpConversion::convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n   int nSizePerThread =\n       order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n \n-\n   auto has = helper.getValueTableFromStruct(llA, K, M, mShapePerCTA,\n                                             mSizePerThread, rewriter, loc);\n   auto hbs = helper.getValueTableFromStruct(llB, K, N, nShapePerCTA,\n                                             nSizePerThread, rewriter, loc);\n \n   SmallVector<Value> ret = cc;\n+  bool isCRow = order[0] == 1;\n   for (unsigned k = 0; k < K; k++) {\n-    int z = 0;\n     for (unsigned m = 0; m < M; m += mShapePerCTA)\n       for (unsigned n = 0; n < N; n += nShapePerCTA)\n         for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n           for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+            int mIdx = m / mShapePerCTA + mm;\n+            int nIdx = n / nShapePerCTA + nn;\n+            int z = isCRow ? mIdx * N / nShapePerCTA + nIdx\n+                           : nIdx * M / mShapePerCTA + mIdx;\n             ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n                                                       hbs[{n + nn, k}], ret[z]);\n-            ++z;\n           }\n   }\n "}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -25,9 +25,8 @@ def get_build_type():\n     elif check_env_flag(\"REL_WITH_DEB_INFO\"):\n         return \"RelWithDebInfo\"\n     else:\n-        return \"Debug\"\n         # TODO: change to release when stable enough\n-        #return \"Release\"\n+        return \"RelWithDebInfo\"\n \n \n # --- third party packages -----"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -392,7 +392,7 @@ def visit_If(self, node):\n                             if then_defs[then_name].type == else_defs[else_name].type:\n                                 names.append(then_name)\n                                 ret_types.append(then_defs[then_name].type)\n-                \n+\n                 # defined in else block but not in then block\n                 # to find in parent scope and yield them\n                 for else_name in else_defs:\n@@ -1511,7 +1511,7 @@ def __init__(self, so_path, metadata, asm):\n         self.metadata = metadata\n         self.cu_module = None\n         self.cu_function = None\n-    \n+\n     def _init_handles(self):\n         if self.cu_module is not None:\n             return\n@@ -1521,14 +1521,15 @@ def _init_handles(self):\n         mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n         self.cu_module = mod\n         self.cu_function = func\n-    \n+\n     def __getattribute__(self, name):\n         if name == 'c_wrapper':\n             self._init_handles()\n         return super().__getattribute__(name)\n \n     def __getitem__(self, grid):\n         self._init_handles()\n+\n         def runner(*args, stream=None):\n             if stream is None:\n                 stream = torch.cuda.current_stream().cuda_stream"}]