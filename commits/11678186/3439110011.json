[{"filename": ".gitignore", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -1,12 +1,20 @@\n+# Triton builds\n build/\n \n-__pycache__\n-.pytest_cache\n-\n+# Triton Python module builds\n python/build/\n python/triton.egg-info/\n python/triton/_C/libtriton.pyd\n python/triton/_C/libtriton.so\n \n+# Python caches\n+__pycache__\n+.pytest_cache\n+\n+# VS Code project files\n .vscode\n .vs\n+\n+# JetBrains project files\n+.idea\n+cmake-build-*"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 19, "deletions": 3, "changes": 22, "file_content_changes": "@@ -22,8 +22,8 @@ struct PTXInstrExecution;\n // PTXBuilder helps to manage a PTX asm program consists of one or multiple\n // instructions.\n //\n-// A helper for building a ASM program, the objective of PTXBuilder is to give a\n-// thin encapsulation and make the ASM code for MLIR LLVM Dialect more clear.\n+// A helper for building an ASM program, the objective of PTXBuilder is to give\n+// a thin encapsulation and make the ASM code for MLIR LLVM Dialect more clear.\n // Currently, several factors are introduced to reduce the need for mixing\n // string and C++ if-else code.\n //\n@@ -147,7 +147,7 @@ struct PTXBuilder {\n   Operand *newOperand(StringRef constraint);\n \n   // Create a constant integer operand.\n-  Operand *newConstantOperand(int v);\n+  Operand *newConstantOperand(int64_t v);\n   // Create a constant operand with explicit code specified.\n   Operand *newConstantOperand(const std::string &v);\n \n@@ -172,6 +172,22 @@ struct PTXBuilder {\n     return argArchive.back().get();\n   }\n \n+  // Make the oprands in argArchive follow the provided \\param order.\n+  void reorderArgArchive(ArrayRef<Operand *> order) {\n+    assert(order.size() == argArchive.size());\n+    // The order in argArchive is unnecessary when onlyAttachMLIRArgs=false, but\n+    // it do necessary when onlyAttachMLIRArgs is true for the $0,$1.. are\n+    // determined by PTX code snippet passed from external.\n+    sort(argArchive.begin(), argArchive.end(),\n+         [&](std::unique_ptr<Operand> &a, std::unique_ptr<Operand> &b) {\n+           auto ida = std::find(order.begin(), order.end(), a.get());\n+           auto idb = std::find(order.begin(), order.end(), b.get());\n+           assert(ida != order.end());\n+           assert(idb != order.end());\n+           return ida < idb;\n+         });\n+  }\n+\n   friend struct PTXInstr;\n   friend struct PTXInstrCommon;\n "}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "file_content_changes": "@@ -10,6 +10,7 @@ include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n include \"mlir/Interfaces/ControlFlowInterfaces.td\" // BranchOpInterface\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n+include \"mlir/Interfaces/CastInterfaces.td\" // CastOpInterface\n \n def TensorSizeTrait : NativeOpTrait<\"TensorSizeTrait\">;\n def SameOperandsAndResultEncoding : NativeOpTrait<\"SameOperandsAndResultEncoding\">;\n@@ -72,17 +73,16 @@ def TT_BitcastOp : TT_Op<\"bitcast\", [SameOperandsAndResultShape,\n     // TODO: Add verifier\n }\n \n-def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n-                                   SameOperandsAndResultEncoding,\n-                                   NoSideEffect,\n-                                   /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+def TT_FpToFpOp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape,\n+                                     SameOperandsAndResultEncoding,\n+                                     NoSideEffect,\n+                                     DeclareOpInterfaceMethods<CastOpInterface>]> {\n     let summary = \"Floating point casting for custom types\";\n \n     let description = [{\n-        Floating point casting for custom types (F8, BF8).\n+        Floating point casting for custom types (F8).\n \n-        F8 <-> BF8, FP16, FP32\n-        BF8 <-> F8, FP16, FP32\n+        F8 <-> FP16, BF16, FP32, FP64\n     }];\n \n     let arguments = (ins TT_FloatLike:$from);"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -14,9 +14,8 @@ class TritonTypeDef<string name, string _mnemonic>\n \n // Floating-point Type\n def F8 : TritonTypeDef<\"Float8\", \"f8\">;\n-def BF8 : TritonTypeDef<\"BFloat8\", \"bf8\">;\n \n-def TT_Float : AnyTypeOf<[F16, BF16, F32, F64], \"floating-point\">;\n+def TT_Float : AnyTypeOf<[F8, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "file_content_changes": "@@ -45,7 +45,7 @@ PTXBuilder::Operand *PTXBuilder::newConstantOperand(const std::string &v) {\n   return argArchive.back().get();\n }\n \n-PTXBuilder::Operand *PTXBuilder::newConstantOperand(int v) {\n+PTXBuilder::Operand *PTXBuilder::newConstantOperand(int64_t v) {\n   std::stringstream ss;\n   ss << \"0x\" << std::hex << v;\n   return newConstantOperand(ss.str());\n@@ -130,8 +130,18 @@ std::string PTXBuilder::dump() const {\n \n PTXInstrExecution &PTXInstrCommon::call(ArrayRef<Operand *> oprs,\n                                         bool onlyAttachMLIRArgs) {\n+  if (onlyAttachMLIRArgs) {\n+    // Nearly impossible to make the $0,$1 in two PTX code snippets to point to\n+    // the same MLIR values in onlyAttachMLIRArgs mode.\n+    assert(builder->executions.empty() &&\n+           \"builder can only hold a single execution when onlyAttachMIIRArgs \"\n+           \"is true.\");\n+    builder->reorderArgArchive(oprs);\n+  }\n+\n   builder->executions.emplace_back(\n       std::make_unique<PTXInstrExecution>(this, oprs, onlyAttachMLIRArgs));\n+\n   return *builder->executions.back();\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 356, "deletions": 24, "changes": 380, "file_content_changes": "@@ -64,9 +64,8 @@ Value createConstantF32(Location loc, PatternRewriter &rewriter, float v) {\n                                            rewriter.getF32FloatAttr(v));\n }\n \n-// Create a index type constant.\n+// Create an index type constant.\n static Value createIndexConstant(OpBuilder &builder, Location loc,\n-\n                                  TypeConverter *converter, int64_t value) {\n   Type ty = converter->convertType(builder.getIndexType());\n   return builder.create<LLVM::ConstantOp>(loc, ty,\n@@ -127,6 +126,7 @@ void llPrintf(StringRef msg, ValueRange args,\n #define i32_ty rewriter.getIntegerType(32)\n #define ui32_ty rewriter.getIntegerType(32, false)\n #define f16_ty rewriter.getF16Type()\n+#define bf16_ty rewriter.getBF16Type()\n #define i8_ty rewriter.getIntegerType(8)\n #define f32_ty rewriter.getF32Type()\n #define f64_ty rewriter.getF64Type()\n@@ -339,7 +339,8 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   }\n \n   Value llvmStruct = rewriter.create<LLVM::UndefOp>(loc, structType);\n-  for (auto v : llvm::enumerate(resultVals)) {\n+  for (const auto& v : llvm::enumerate(resultVals)) {\n+    assert(v.value() && \"can not insert null values\");\n     llvmStruct = insert_val(structType, llvmStruct, v.value(),\n                             rewriter.getI64ArrayAttr(v.index()));\n   }\n@@ -748,13 +749,15 @@ class ConvertTritonGPUOpToLLVMPattern\n \n   // Emit indices calculation within each ConversionPattern, and returns a\n   // [elemsPerThread X rank] index matrix.\n-  // TODO: [phil] redundant indices commputation do not appear to hurt\n+\n+// TODO: [phil] redundant indices commputation do not appear to hurt\n   // performance much, but they could still significantly slow down\n   // computations.\n   SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n       Location loc, ConversionPatternRewriter &rewriter,\n       const Attribute &layout, ArrayRef<int64_t> shape) const {\n \n+\n     // step 1, delinearize threadId to get the base index\n     auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, shape);\n     // step 2, get offset of each element\n@@ -1043,7 +1046,8 @@ struct LoadOpConversion\n \n     // Determine the vectorization size\n     Type valueTy = op.getResult().getType();\n-    Type valueElemTy = getElementTypeOrSelf(valueTy);\n+    Type valueElemTy = typeConverter->convertType(\n+        getElementTypeOrSelf(valueTy));\n     unsigned vec = getVectorSize(ptr);\n     unsigned numElems = getElemsPerThread(ptr.getType());\n     if (llMask)\n@@ -1238,7 +1242,8 @@ struct StoreOpConversion\n     MLIRContext *ctx = rewriter.getContext();\n \n     auto valueTy = value.getType();\n-    Type valueElemTy = getElementTypeOrSelf(valueTy);\n+    Type valueElemTy = typeConverter->convertType(\n+        getElementTypeOrSelf(valueTy));\n \n     unsigned vec = getVectorSize(ptr);\n     unsigned numElems = getElemsPerThread(ptr.getType());\n@@ -1826,8 +1831,8 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   LogicalResult\n   matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    // We cannot directly\n-    //   rewriter.replaceOp(op, adaptor.src());\n+    // We cannot directly run\n+    //   `rewriter.replaceOp(op, adaptor.src())`\n     // due to MLIR's restrictions\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().template cast<RankedTensorType>();\n@@ -2188,6 +2193,330 @@ struct ExtractSliceOpConversion\n   }\n };\n \n+struct FpToFpOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::FpToFpOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::FpToFpOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  static SmallVector<Value> convertFp8x4ToFp16x4(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const Value& v0, const Value& v1, const Value& v2, const Value& v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    Value fp8x4Vec = undef(fp8x4VecTy);\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n+    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm =\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\"\n+        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\"\n+        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\"\n+        \"lop3.b32 b0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+        \"lop3.b32 b1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+        \"shr.b32  b0, b0, 1;                    \\n\"\n+        \"shr.b32  b1, b1, 1;                    \\n\"\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\"\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\"\n+        \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o0 = builder.newOperand(\"=r\");\n+    auto *o1 = builder.newOperand(\"=r\");\n+    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n+    call({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    auto fp16x2x2StructTy =\n+        struct_ty(SmallVector<Type>{fp16x2VecTy, fp16x2VecTy});\n+    auto fp16x2x2Struct =\n+        builder.launch(rewriter, loc, fp16x2x2StructTy, false);\n+    auto fp16x2Vec0 = extract_val(fp16x2VecTy, fp16x2x2Struct,\n+                                  rewriter.getI32ArrayAttr({0}));\n+    auto fp16x2Vec1 = extract_val(fp16x2VecTy, fp16x2x2Struct,\n+                                  rewriter.getI32ArrayAttr({1}));\n+    return {\n+        extract_element(f16_ty, fp16x2Vec0, i32_val(0)),\n+        extract_element(f16_ty, fp16x2Vec0, i32_val(1)),\n+        extract_element(f16_ty, fp16x2Vec1, i32_val(0)),\n+        extract_element(f16_ty, fp16x2Vec1, i32_val(1))\n+    };\n+  }\n+\n+  static SmallVector<Value> convertFp16x4ToFp8x4(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const Value& v0, const Value& v1, const Value& v2, const Value& v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp16x2VecTy = vec_ty(f16_ty, 2);\n+    Value fp16x2Vec0 = undef(fp16x2VecTy);\n+    Value fp16x2Vec1 = undef(fp16x2VecTy);\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v0, i32_val(0));\n+    fp16x2Vec0 = insert_element(fp16x2VecTy, fp16x2Vec0, v1, i32_val(1));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v2, i32_val(0));\n+    fp16x2Vec1 = insert_element(fp16x2VecTy, fp16x2Vec1, v3, i32_val(1));\n+    fp16x2Vec0 = bitcast(fp16x2Vec0, i32_ty);\n+    fp16x2Vec1 = bitcast(fp16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm =\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\"\n+        \"shl.b32 a0, $1, 1;                     \\n\"\n+        \"shl.b32 a1, $2, 1;                     \\n\"\n+        \"lop3.b32 a0, a0, 0x7fff7fff, 0, 0xc0;  \\n\"\n+        \"lop3.b32 a1, a1, 0x7fff7fff, 0, 0xc0;  \\n\"\n+        \"add.u32 a0, a0, 0x00800080;            \\n\"\n+        \"add.u32 a1, a1, 0x00800080;            \\n\"\n+        \"lop3.b32 b0, $1, 0x80008000, a0, 0xea; \\n\"\n+        \"lop3.b32 b1, $2, 0x80008000, a1, 0xea; \\n\"\n+        \"prmt.b32 $0, b0, b1, 0x7531;           \\n\"\n+        \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(fp16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(fp16x2Vec1, \"r\");\n+    call({o, i0, i1}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {\n+        extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+        extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+        extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+        extract_element(i8_ty, fp8x4Vec, i32_val(3))\n+    };\n+  }\n+\n+  static SmallVector<Value> convertFp8x4ToBf16x4(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const Value& v0, const Value& v1, const Value& v2, const Value& v3) {\n+    auto ctx = rewriter.getContext();\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    Value fp8x4Vec = undef(fp8x4VecTy);\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v0, i32_val(0));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v1, i32_val(1));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v2, i32_val(2));\n+    fp8x4Vec = insert_element(fp8x4VecTy, fp8x4Vec, v3, i32_val(3));\n+    fp8x4Vec = bitcast(fp8x4Vec, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm =\n+        \"{                                          \\n\"\n+        \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>;  \\n\"\n+        \"prmt.b32 a0, 0, $2, 0x5040;                \\n\"\n+        \"prmt.b32 a1, 0, $2, 0x7060;                \\n\"\n+        \"and.b32 sign0, a0, 0x80008000;             \\n\"\n+        \"and.b32 sign1, a1, 0x80008000;             \\n\"\n+        \"and.b32 nosign0, a0, 0x7fff7fff;           \\n\"\n+        \"and.b32 nosign1, a1, 0x7fff7fff;           \\n\"\n+        \"shr.b32 nosign0, nosign0, 4;               \\n\"\n+        \"shr.b32 nosign1, nosign1, 4;               \\n\"\n+        \"add.u32 nosign0, nosign0, 0x38003800;      \\n\"\n+        \"add.u32 nosign1, nosign1, 0x38003800;      \\n\"\n+        \"or.b32 $0, sign0, nosign0;                 \\n\"\n+        \"or.b32 $1, sign1, nosign1;                 \\n\"\n+        \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o0 = builder.newOperand(\"=r\");\n+    auto *o1 = builder.newOperand(\"=r\");\n+    auto *i = builder.newOperand(fp8x4Vec, \"r\");\n+    call({o0, o1, i}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto bf16x2VecTy = vec_ty(bf16_ty, 2);\n+    auto bf16x2x2StructTy =\n+        struct_ty(SmallVector<Type>{bf16x2VecTy, bf16x2VecTy});\n+    auto bf16x2x2Struct =\n+        builder.launch(rewriter, loc, bf16x2x2StructTy, false);\n+    auto bf16x2Vec0 = extract_val(bf16x2VecTy, bf16x2x2Struct,\n+                                  rewriter.getI32ArrayAttr({0}));\n+    auto bf16x2Vec1 = extract_val(bf16x2VecTy, bf16x2x2Struct,\n+                                  rewriter.getI32ArrayAttr({1}));\n+    return {\n+        extract_element(bf16_ty, bf16x2Vec0, i32_val(0)),\n+        extract_element(bf16_ty, bf16x2Vec0, i32_val(1)),\n+        extract_element(bf16_ty, bf16x2Vec1, i32_val(0)),\n+        extract_element(bf16_ty, bf16x2Vec1, i32_val(1))\n+    };\n+  }\n+\n+  static SmallVector<Value> convertBf16x4ToFp8x4(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const Value& v0, const Value& v1, const Value& v2, const Value& v3) {\n+    auto ctx = rewriter.getContext();\n+    auto bf16x2VecTy = vec_ty(bf16_ty, 2);\n+    Value bf16x2Vec0 = undef(bf16x2VecTy);\n+    Value bf16x2Vec1 = undef(bf16x2VecTy);\n+    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v0, i32_val(0));\n+    bf16x2Vec0 = insert_element(bf16x2VecTy, bf16x2Vec0, v1, i32_val(1));\n+    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v2, i32_val(0));\n+    bf16x2Vec1 = insert_element(bf16x2VecTy, bf16x2Vec1, v3, i32_val(1));\n+    bf16x2Vec0 = bitcast(bf16x2Vec0, i32_ty);\n+    bf16x2Vec1 = bitcast(bf16x2Vec1, i32_ty);\n+\n+    PTXBuilder builder;\n+    auto *ptxAsm =\n+        \"{                                            \\n\"\n+        \".reg .u32 sign, sign<2>, nosign, nosign<2>;  \\n\"\n+        \".reg .u32 fp8_min, fp8_max, rn_, zero;       \\n\"\n+        \"mov.u32 fp8_min, 0x38003800;                 \\n\"\n+        \"mov.u32 fp8_max, 0x3ff03ff0;                 \\n\"\n+        \"mov.u32 rn_, 0x80008;                        \\n\"\n+        \"mov.u32 zero, 0;                             \\n\"\n+        \"and.b32 sign0, $1, 0x80008000;               \\n\"\n+        \"and.b32 sign1, $2, 0x80008000;               \\n\"\n+        \"prmt.b32 sign, sign0, sign1, 0x7531;         \\n\"\n+        \"and.b32 nosign0, $1, 0x7fff7fff;             \\n\"\n+        \"and.b32 nosign1, $2, 0x7fff7fff;             \\n\"\n+        \".reg .u32 nosign_0_<2>, nosign_1_<2>;        \\n\"\n+        \"and.b32 nosign_0_0, nosign0, 0xffff0000;     \\n\"\n+        \"max.u32 nosign_0_0, nosign_0_0, 0x38000000;  \\n\"\n+        \"min.u32 nosign_0_0, nosign_0_0, 0x3ff00000;  \\n\"\n+        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;     \\n\"\n+        \"max.u32 nosign_0_1, nosign_0_1, 0x3800;      \\n\"\n+        \"min.u32 nosign_0_1, nosign_0_1, 0x3ff0;      \\n\"\n+        \"or.b32 nosign0, nosign_0_0, nosign_0_1;      \\n\"\n+        \"and.b32 nosign_1_0, nosign1, 0xffff0000;     \\n\"\n+        \"max.u32 nosign_1_0, nosign_1_0, 0x38000000;  \\n\"\n+        \"min.u32 nosign_1_0, nosign_1_0, 0x3ff00000;  \\n\"\n+        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;     \\n\"\n+        \"max.u32 nosign_1_1, nosign_1_1, 0x3800;      \\n\"\n+        \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0;      \\n\"\n+        \"or.b32 nosign1, nosign_1_0, nosign_1_1;      \\n\"\n+        \"add.u32 nosign0, nosign0, rn_;               \\n\"\n+        \"add.u32 nosign1, nosign1, rn_;               \\n\"\n+        \"sub.u32 nosign0, nosign0, 0x38003800;        \\n\"\n+        \"sub.u32 nosign1, nosign1, 0x38003800;        \\n\"\n+        \"shr.u32 nosign0, nosign0, 4;                 \\n\"\n+        \"shr.u32 nosign1, nosign1, 4;                 \\n\"\n+        \"prmt.b32 nosign, nosign0, nosign1, 0x6420;   \\n\"\n+        \"or.b32 $0, nosign, sign;                     \\n\"\n+        \"}\";\n+    auto &call = *builder.create(ptxAsm);\n+\n+    auto *o = builder.newOperand(\"=r\");\n+    auto *i0 = builder.newOperand(bf16x2Vec0, \"r\");\n+    auto *i1 = builder.newOperand(bf16x2Vec1, \"r\");\n+    call({o, i0, i1}, /* onlyAttachMLIRArgs */ true);\n+\n+    auto fp8x4VecTy = vec_ty(i8_ty, 4);\n+    auto fp8x4Vec = builder.launch(rewriter, loc, fp8x4VecTy, false);\n+    return {\n+        extract_element(i8_ty, fp8x4Vec, i32_val(0)),\n+        extract_element(i8_ty, fp8x4Vec, i32_val(1)),\n+        extract_element(i8_ty, fp8x4Vec, i32_val(2)),\n+        extract_element(i8_ty, fp8x4Vec, i32_val(3))\n+    };\n+  }\n+\n+  static SmallVector<Value> convertFp8x4ToFp32x4(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const Value& v0, const Value& v1, const Value& v2, const Value& v3) {\n+    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {\n+        rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[0]),\n+        rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[1]),\n+        rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[2]),\n+        rewriter.create<LLVM::FPExtOp>(loc, f32_ty, fp16Values[3])\n+    };\n+  }\n+\n+  static SmallVector<Value> convertFp32x4ToFp8x4(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const Value& v0, const Value& v1, const Value& v2, const Value& v3) {\n+      auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+      auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+      auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+      auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+      return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  static SmallVector<Value> convertFp8x4ToFp64x4(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const Value& v0, const Value& v1, const Value& v2, const Value& v3) {\n+    auto fp16Values = convertFp8x4ToFp16x4(loc, rewriter, v0, v1, v2, v3);\n+    return {\n+        rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[0]),\n+        rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[1]),\n+        rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[2]),\n+        rewriter.create<LLVM::FPExtOp>(loc, f64_ty, fp16Values[3])\n+    };\n+  }\n+\n+  static SmallVector<Value> convertFp64x4ToFp8x4(\n+      Location loc, ConversionPatternRewriter &rewriter,\n+      const Value& v0, const Value& v1, const Value& v2, const Value& v3) {\n+    auto c0 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v0);\n+    auto c1 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v1);\n+    auto c2 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v2);\n+    auto c3 = rewriter.create<LLVM::FPTruncOp>(loc, f16_ty, v3);\n+    return convertFp16x4ToFp8x4(loc, rewriter, c0, c1, c2, c3);\n+  }\n+\n+  LogicalResult\n+  matchAndRewrite(triton::FpToFpOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto srcTensorType = op.from().getType().cast<mlir::RankedTensorType>();\n+    auto dstTensorType = op.result().getType().cast<mlir::RankedTensorType>();\n+    auto srcEltType = srcTensorType.getElementType();\n+    auto dstEltType = dstTensorType.getElementType();\n+    assert(srcEltType.isa<triton::Float8Type>() ||\n+           dstEltType.isa<triton::Float8Type>());\n+    auto convertedDstTensorType =\n+        this->getTypeConverter()->convertType(dstTensorType);\n+    auto convertedDstEleType =\n+        this->getTypeConverter()->convertType(dstEltType);\n+\n+    // Select convertor\n+    std::function<SmallVector<Value>(Location, ConversionPatternRewriter&,\n+                                     const Value&, const Value&,\n+                                     const Value&, const Value&)> convertor;\n+    if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF16()) {\n+      convertor = convertFp8x4ToFp16x4;\n+    } else if (srcEltType.isF16() && dstEltType.isa<triton::Float8Type>()) {\n+      convertor = convertFp16x4ToFp8x4;\n+    } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isBF16()) {\n+      convertor = convertFp8x4ToBf16x4;\n+    } else if (srcEltType.isBF16() && dstEltType.isa<triton::Float8Type>()) {\n+      convertor = convertBf16x4ToFp8x4;\n+    } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF32()) {\n+      convertor = convertFp8x4ToFp32x4;\n+    } else if (srcEltType.isF32() && dstEltType.isa<triton::Float8Type>()) {\n+      convertor = convertFp32x4ToFp8x4;\n+    } else if (srcEltType.isa<triton::Float8Type>() && dstEltType.isF64()) {\n+      convertor = convertFp8x4ToFp64x4;\n+    } else if (srcEltType.isF64() && dstEltType.isa<triton::Float8Type>()) {\n+      convertor = convertFp64x4ToFp8x4;\n+    } else {\n+      assert(false && \"unsupported type casting\");\n+    }\n+\n+    // Vectorized casting\n+    auto loc = op->getLoc();\n+    auto elems = getElemsPerThread(dstTensorType);\n+    assert(elems % 4 == 0 &&\n+           \"FP8 casting only support tensors with 4-aligned sizes\");\n+    auto elements = getElementsFromStruct(loc, adaptor.from(), rewriter);\n+    SmallVector<Value> resultVals;\n+    for (size_t i = 0; i < elems; i += 4) {\n+      auto converted = convertor(loc, rewriter,\n+                                 elements[i], elements[i + 1],\n+                                 elements[i + 2], elements[i + 3]);\n+      resultVals.append(converted);\n+    }\n+    assert(resultVals.size() == elems);\n+    auto result = getStructFromElements(loc, resultVals, rewriter,\n+                                        convertedDstTensorType);\n+    rewriter.replaceOp(op, result);\n+    return success();\n+  }\n+};\n+\n // A CRTP style of base class.\n template <typename SourceOp, typename DestOp, typename ConcreteT>\n class ElementwiseOpConversionBase\n@@ -2477,7 +2806,7 @@ struct ConvertLayoutOpConversion\n                       Value smemBase) const;\n \n   // blocked/mma -> blocked/mma.\n-  // Data padding in shared memory to avoid bank confict.\n+  // Data padding in shared memory to avoid bank conflict.\n   LogicalResult\n   lowerDistributedToDistributed(triton::gpu::ConvertLayoutOp op,\n                                 OpAdaptor adaptor,\n@@ -3353,7 +3682,7 @@ struct DotOpMmaV1ConversionHelper {\n     int NK = shape[1];\n     unsigned numM = rep[0] * shape[0] / (spw[0] * wpt[0]);\n \n-    // NOTE We cound't get the vec from the shared layout.\n+    // NOTE: We couldn't get the vec from the shared layout.\n     // int vecA = sharedLayout.getVec();\n     // TODO[Superjomn]: Consider the case when vecA > 4\n     bool vecGt4 = false;\n@@ -3371,7 +3700,7 @@ struct DotOpMmaV1ConversionHelper {\n     SmallVector<int> fpw({2, 2, 1});\n     SmallVector<int> rep({0, 2 * packSize1, 1});       // pad M with 0\n     SmallVector<int> spw({0, fpw[1] * 4 * rep[1], 1}); // pad M with 0\n-    // NOTE We cound't get the vec from the shared layout.\n+    // NOTE: We couldn't get the vec from the shared layout.\n     // int vecB = sharedLayout.getVec();\n     // TODO[Superjomn]: Consider the case when vecA > 4\n     bool vecGt4 = false;\n@@ -3475,7 +3804,7 @@ struct DotOpMmaV2ConversionHelper {\n     return Type{};\n   }\n \n-  // The type of a matrix that loaded by either a ldmatrix or composed lds.\n+  // The type of matrix that loaded by either a ldmatrix or composed lds.\n   Type getMatType() const {\n     Type fp32Ty = type::f32Ty(ctx);\n     Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n@@ -3671,7 +4000,7 @@ struct DotOpMmaV2ConversionHelper {\n        \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n   };\n \n-  // vector length per ldmatrix (16*8/elelment_size_in_bits)\n+  // vector length per ldmatrix (16*8/element_size_in_bits)\n   inline static const std::map<TensorCoreType, uint8_t> mmaInstrVec = {\n       {TensorCoreType::FP32_FP16_FP16_FP32, 8},\n       {TensorCoreType::FP32_BF16_BF16_FP32, 8},\n@@ -3811,7 +4140,7 @@ struct MMA16816ConversionHelper {\n       // load from smem\n       loadFn = getLoadMatrixFn(\n           tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[0] /*wpt*/,\n-          1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShpae*/,\n+          1 /*kOrder*/, {mmaInstrM, mmaInstrK} /*instrShape*/,\n           {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, ha /*vals*/);\n     } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n       // load from registers, used in gemm fuse\n@@ -3842,7 +4171,7 @@ struct MMA16816ConversionHelper {\n \n     auto loadFn = getLoadMatrixFn(\n         tensor, smemObj, mmaLayout, mmaLayout.getWarpsPerCTA()[1] /*wpt*/,\n-        0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShpae*/,\n+        0 /*kOrder*/, {mmaInstrK, mmaInstrN} /*instrShape*/,\n         {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, hb /*vals*/);\n \n     for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n@@ -4801,14 +5130,15 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n     addConversion([&](RankedTensorType type) -> llvm::Optional<Type> {\n       return convertTritonTensorType(type);\n     });\n-    // internally store bfloat16 as int16\n-    addConversion([&](BFloat16Type type) -> llvm::Optional<Type> {\n-      return IntegerType::get(type.getContext(), 16);\n+    // Internally store float8 as int8\n+    addConversion([&](triton::Float8Type type) -> llvm::Optional<Type> {\n+      return IntegerType::get(type.getContext(), 8);\n     });\n   }\n \n   Type convertTritonPointerType(triton::PointerType type) {\n-    return LLVM::LLVMPointerType::get(type.getPointeeType(),\n+    // Recursively translate pointee type\n+    return LLVM::LLVMPointerType::get(convertType(type.getPointeeType()),\n                                       type.getAddressSpace());\n   }\n \n@@ -4841,7 +5171,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(type);\n         size_t fcSize = 4 * repM * repN;\n         return LLVM::LLVMStructType::getLiteral(\n-            ctx, SmallVector<Type>(fcSize, type.getElementType()));\n+            ctx, SmallVector<Type>(fcSize, convertType(type.getElementType())));\n       }\n \n       if (mmaLayout.getVersion() == 1) {\n@@ -4850,7 +5180,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         int repN = helper.getRepN(shape[1]);\n         int elems = 8 * repM * repN;\n         return LLVM::LLVMStructType::getLiteral(\n-            ctx, SmallVector<Type>(elems, type.getElementType()));\n+            ctx, SmallVector<Type>(elems, convertType(type.getElementType())));\n       }\n \n       llvm::errs()\n@@ -4861,7 +5191,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n                    layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n       auto mmaLayout = dot_op_layout.getParent().cast<MmaEncodingAttr>();\n       auto wpt = mmaLayout.getWarpsPerCTA();\n-      Type elemTy = type.getElementType();\n+      Type elemTy = convertType(type.getElementType());\n       auto vecSize = 1;\n       if (elemTy.getIntOrFloatBitWidth() == 16) {\n         vecSize = 2;\n@@ -5412,6 +5742,8 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n   POPULATE_UNARY_OP(triton::PtrToIntOp, LLVM::PtrToIntOp)\n #undef POPULATE_UNARY_OP\n \n+  patterns.add<FpToFpOpConversion>(typeConverter, benefit);\n+\n   patterns.add<FDivOpConversion>(typeConverter, benefit);\n \n   patterns.add<ExtElemwiseOpConversion>(typeConverter, benefit);\n@@ -5457,12 +5789,12 @@ class ConvertTritonGPUToLLVM\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n     // step 1: Allocate shared memories and insert barriers\n-    // setp 2: Convert SCF to CFG\n+    // step 2: Convert SCF to CFG\n     // step 3: Convert FuncOp to LLVMFuncOp via partial conversion\n     // step 4: Convert the rest of ops via partial conversion\n     // The reason for putting step 1 before step 2 is that the membar analysis\n     // currently only supports SCF but not CFG.\n-    // The reason for a seperation between 1/4 is that, step 3 is out of\n+    // The reason for a separation between 1/4 is that, step 3 is out of\n     // the scope of Dialect Conversion, thus we need to make sure the smem\n     // is not revised during the conversion of step 4.\n     Allocation allocation(mod);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -371,6 +371,7 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n   patterns.add< // TODO: view should have custom pattern that views the layout\n       TritonGenericPattern<triton::ViewOp>,\n       TritonGenericPattern<triton::BitcastOp>,\n+      TritonGenericPattern<triton::FpToFpOp>,\n       TritonGenericPattern<triton::IntToPtrOp>,\n       TritonGenericPattern<triton::PtrToIntOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -124,6 +124,29 @@ void printStoreOp(OpAsmPrinter &printer, StoreOp storeOp) {\n namespace mlir {\n namespace triton {\n \n+//-- FpToFpOp --\n+bool FpToFpOp::areCastCompatible(::mlir::TypeRange inputs,\n+                               ::mlir::TypeRange outputs) {\n+  if (inputs.size() != 1 || outputs.size() != 1)\n+    return false;\n+  auto srcEltType = inputs.front();\n+  auto dstEltType = outputs.front();\n+  auto srcTensorType = srcEltType.dyn_cast<mlir::RankedTensorType>();\n+  auto dstTensorType = dstEltType.dyn_cast<mlir::RankedTensorType>();\n+  if (srcTensorType && dstTensorType) {\n+    srcEltType = srcTensorType.getElementType();\n+    dstEltType = dstTensorType.getElementType();\n+  }\n+  // Check whether fp8 <=> fp16, bf16, f32, f64\n+  // Make `srcEltType` always the fp8 side\n+  if (dstEltType.dyn_cast<mlir::triton::Float8Type>())\n+    std::swap(srcEltType, dstEltType);\n+  if (!srcEltType.dyn_cast<mlir::triton::Float8Type>())\n+    return false;\n+  return dstEltType.isF16() || dstEltType.isBF16() ||\n+         dstEltType.isF32() || dstEltType.isF64();\n+}\n+\n //-- StoreOp --\n void StoreOp::build(::mlir::OpBuilder &builder, ::mlir::OperationState &state,\n                     ::mlir::Value ptr, ::mlir::Value value) {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -64,7 +64,9 @@ unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n }\n \n unsigned getElemsPerThread(Type type) {\n-  if (type.isIntOrIndexOrFloat() || type.isa<triton::PointerType>())\n+  if (type.isIntOrIndexOrFloat() ||\n+      type.isa<triton::Float8Type>() ||\n+      type.isa<triton::PointerType>())\n     return 1;\n   auto tensorType = type.cast<RankedTensorType>();\n   return getElemsPerThread(tensorType.getEncoding(), tensorType.getShape());"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -32,7 +32,10 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // Thread tile size depends on memory alignment\n     SmallVector<unsigned, 4> sizePerThread(rank, 1);\n     PointerType ptrType = origType.getElementType().cast<PointerType>();\n-    unsigned numBits = ptrType.getPointeeType().getIntOrFloatBitWidth();\n+    auto pointeeType = ptrType.getPointeeType();\n+    unsigned numBits =\n+        pointeeType.isa<triton::Float8Type>() ?\n+        8 : pointeeType.getIntOrFloatBitWidth();\n     unsigned maxMultiple = info.getDivisibility(order[0]);\n     unsigned maxContig = info.getContiguity(order[0]);\n     unsigned alignment = std::min(maxMultiple, maxContig);"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -140,7 +140,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n       /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n   pm.addPass(createConvertTritonGPUToLLVMPass());\n-  // Conanicalize to eliminate the remaining UnrealizedConversionCastOp\n+  // Canonicalize to eliminate the remaining UnrealizedConversionCastOp\n   pm.addPass(mlir::createCanonicalizerPass());\n   pm.addPass(mlir::createCSEPass()); // Simplify the IR to improve readability.\n   pm.addPass(mlir::createSymbolDCEPass());"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "file_content_changes": "@@ -493,10 +493,6 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self) -> mlir::Type {\n              return self.getType<mlir::triton::Float8Type>();\n            })\n-      .def(\"get_bf8_ty\",\n-           [](mlir::OpBuilder &self) -> mlir::Type {\n-             return self.getType<mlir::triton::BFloat8Type>();\n-           })\n       .def(\n           \"get_half_ty\",\n           [](mlir::OpBuilder &self) -> mlir::Type { return self.getF16Type(); })\n@@ -616,14 +612,20 @@ void init_triton_ir(py::module &&m) {\n            })\n \n       // Cast instructions\n+      // Conversions for custom FP types (FP8)\n+      .def(\"create_fp_to_fp\",\n+           [](mlir::OpBuilder &self, mlir::Value &src,\n+              mlir::Type &dstType) -> mlir::Value {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::FpToFpOp>(loc, dstType, src);\n+           })\n+      // Conversions for standard LLVM builtin types\n       .def(\"create_bitcast\",\n            [](mlir::OpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              return self.create<mlir::triton::BitcastOp>(loc, dstType, src);\n            })\n-      // .def(\"create_cast\", &ir::builder::create_cast)\n-      // .def(\"create_ptr_to_int\", &ir::builder::create_ptr_to_int)\n       .def(\"create_si_to_fp\",\n            [](mlir::OpBuilder &self, mlir::Value &src,\n               mlir::Type &dstType) -> mlir::Value {\n@@ -697,7 +699,6 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::arith::IndexCastOp>(loc, input,\n                                                           self.getI32Type());\n            })\n-\n       .def(\"create_fmul\",\n            [](mlir::OpBuilder &self, mlir::Value &lhs,\n               mlir::Value &rhs) -> mlir::Value {"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 80, "deletions": 80, "changes": 160, "file_content_changes": "@@ -780,88 +780,88 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     assert (to_numpy(src).view('uint8') == to_numpy(dst).view('uint8')).all()\n \n \n-# @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n-# def test_f8_xf16_roundtrip(dtype):\n-#     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n-#     check_type_supported(dtype)\n+@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n+def test_f8_xf16_roundtrip(dtype):\n+    \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n+    check_type_supported(dtype)\n \n-#     @triton.jit\n-#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n-#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-#         mask = offsets < n_elements\n-#         input = tl.load(input_ptr + offsets, mask=mask)\n-#         output = input\n-#         tl.store(output_ptr + offsets, output, mask=mask)\n-\n-#     f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n-#     f8 = triton.reinterpret(f8_tensor, tl.float8)\n-#     n_elements = f8_tensor.numel()\n-#     xf16 = torch.empty_like(f8_tensor, dtype=dtype)\n-#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-#     copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n-\n-#     f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n-#     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n-#     copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n-\n-#     assert torch.all(f8_tensor == f8_output_tensor)\n-\n-\n-# def test_f16_to_f8_rounding():\n-#     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n-#     error is the minimum over all float8.\n-#     Or the same explanation a bit mathier:\n-#     for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n-#     @triton.jit\n-#     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n-#         offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-#         mask = offsets < n_elements\n-#         input = tl.load(input_ptr + offsets, mask=mask)\n-#         output = input\n-#         tl.store(output_ptr + offsets, output, mask=mask)\n-\n-#     # torch.view with a dtype isn't supported in triton's torch yet so use numpy's view\n-#     f16_input_np = (\n-#         np.array(\n-#             range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=np.int16,\n-#         )\n-#         .view(np.float16)\n-#     )\n-#     f16_input = torch.tensor(f16_input_np, dtype=torch.float16, device='cuda')\n-#     n_elements = f16_input.numel()\n-#     f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n-#     f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n-#     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n-#     copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n-\n-#     f16_output = torch.empty_like(f16_input, dtype=torch.float16)\n-#     copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n-\n-#     abs_error = torch.abs(f16_input - f16_output)\n-\n-#     all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n-#     all_f8_vals = triton.reinterpret(all_f8_vals_tensor, tl.float8)\n-#     all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=torch.float16)\n-#     copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n-\n-#     all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n-#         torch.isfinite(all_f8_vals_in_f16)\n-#     ]\n+    @triton.jit\n+    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        input = tl.load(input_ptr + offsets, mask=mask)\n+        output = input\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+    f8 = triton.reinterpret(f8_tensor, tl.float8)\n+    n_elements = f8_tensor.numel()\n+    xf16 = torch.empty_like(f8_tensor, dtype=dtype)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    copy_kernel[grid](f8, xf16, n_elements, BLOCK_SIZE=1024)\n+\n+    f8_output_tensor = torch.empty_like(xf16, dtype=torch.int8)\n+    f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n+    copy_kernel[grid](xf16, f8_output, n_elements, BLOCK_SIZE=1024)\n+\n+    assert torch.all(f8_tensor == f8_output_tensor)\n+\n+\n+def test_f16_to_f8_rounding():\n+    \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n+    error is the minimum over all float8.\n+    Or the same explanation a bit mathier:\n+    for all f16 |f16 - fromf8(tof8(f16))| == min over all f8 |f16 - fromf8(f8)|\"\"\"\n+    @triton.jit\n+    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        input = tl.load(input_ptr + offsets, mask=mask)\n+        output = input\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+    # torch.view with a dtype isn't supported in triton's torch yet so use numpy's view\n+    f16_input_np = (\n+        np.array(\n+            range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=np.int16,\n+        )\n+        .view(np.float16)\n+    )\n+    f16_input = torch.tensor(f16_input_np, dtype=torch.float16, device='cuda')\n+    n_elements = f16_input.numel()\n+    f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n+    f8_output = triton.reinterpret(f8_output_tensor, tl.float8)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n+\n+    f16_output = torch.empty_like(f16_input, dtype=torch.float16)\n+    copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n+\n+    abs_error = torch.abs(f16_input - f16_output)\n+\n+    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n+    all_f8_vals = triton.reinterpret(all_f8_vals_tensor, tl.float8)\n+    all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=torch.float16)\n+    copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n+\n+    all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n+        torch.isfinite(all_f8_vals_in_f16)\n+    ]\n \n-#     min_error = torch.min(\n-#         torch.abs(\n-#             f16_input.reshape((-1, 1))\n-#             - all_finite_f8_vals_in_f16.reshape((1, -1))\n-#         ),\n-#         dim=1,\n-#     )[0]\n-#     # 1.9375 is float8 max\n-#     mismatch = torch.logical_and(\n-#         abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.abs(f16_input) < 1.9375)\n-#     )\n-#     assert torch.all(\n-#         torch.logical_not(mismatch)\n-#     ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n+    min_error = torch.min(\n+        torch.abs(\n+            f16_input.reshape((-1, 1))\n+            - all_finite_f8_vals_in_f16.reshape((1, -1))\n+        ),\n+        dim=1,\n+    )[0]\n+    # 1.9375 is float8 max\n+    mismatch = torch.logical_and(\n+        abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.abs(f16_input) < 1.9375)\n+    )\n+    assert torch.all(\n+        torch.logical_not(mismatch)\n+    ), f\"f16_input[mismatch]={f16_input[mismatch]} f16_output[mismatch]={f16_output[mismatch]} abs_error[mismatch]={abs_error[mismatch]} min_error[mismatch]={min_error[mismatch]}\"\n \n \n # # ---------------"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -48,6 +48,8 @@ class dtype:\n     SINT_TYPES = ['int1', 'int8', 'int16', 'int32', 'int64']\n     UINT_TYPES = ['uint8', 'uint16', 'uint32', 'uint64']\n     FP_TYPES = ['fp8', 'fp16', 'bf16', 'fp32', 'fp64']\n+    CUSTOMIZED_FP_TYPES = ['fp8']\n+    STANDARD_FP_TYPES = ['fp16', 'bf16', 'fp32', 'fp64']\n     OTHER_TYPES = ['void']\n \n     class SIGNEDNESS(Enum):\n@@ -129,6 +131,12 @@ def is_uint64(self):\n     def is_floating(self):\n         return self.name in dtype.FP_TYPES\n \n+    def is_customized_floating(self):\n+        return self.name in dtype.CUSTOMIZED_FP_TYPES\n+\n+    def is_standard_floating(self):\n+        return self.name in dtype.STANDARD_FP_TYPES\n+\n     def is_int_signed(self):\n         return self.name in dtype.SINT_TYPES\n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 30, "deletions": 29, "changes": 59, "file_content_changes": "@@ -613,39 +613,45 @@ def cast(input: tl.tensor,\n         dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n     if src_ty == dst_ty:\n         return input\n+\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n-    # fp8 <=> bf16/fp16\n-    if (src_sca_ty.is_bf16() or src_sca_ty.is_fp16()) and dst_sca_ty.is_fp8():\n-        return tl.tensor(builder.create_fp_trunc(input.handle, dst_ty.to_ir(builder)),\n-                         dst_ty)\n-    if src_sca_ty.is_fp8() and (dst_sca_ty.is_bf16() or dst_sca_ty.is_fp16()):\n-        return tl.tensor(builder.create_fp_ext(input.handle, dst_ty.to_ir(builder)),\n+\n+    # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n+    if (src_sca_ty.is_customized_floating() and dst_sca_ty.is_floating()) or \\\n+       (src_sca_ty.is_floating() and dst_sca_ty.is_customized_floating()):\n+        return tl.tensor(builder.create_fp_to_fp(input.handle, dst_ty.to_ir(builder)),\n                          dst_ty)\n-    # bf16 <=> (not fp32)\n-    if (src_sca_ty.is_bf16() and not dst_sca_ty.is_fp32()) or \\\n-       (dst_sca_ty.is_bf16() and not src_sca_ty.is_fp32()):\n+\n+    # Casting types of the same bit width: fp16 <=> bf16\n+    if (src_sca_ty.is_fp16() and dst_sca_ty.is_bf16()) or \\\n+       (src_sca_ty.is_bf16() and dst_sca_ty.is_fp16()):\n         return cast(cast(input, tl.float32, builder), dst_sca_ty, builder)\n \n-    # FP Truncation\n+    # Standard floating types' casting: truncation\n+    #   fp64 => fp32, fp16, bf16\n+    #   fp32 => fp16, bf16\n     truncate_fp = src_sca_ty.is_floating() and \\\n         dst_sca_ty.is_floating() and \\\n-        src_sca_ty.fp_mantissa_width > dst_sca_ty.fp_mantissa_width\n+        src_sca_ty.primitive_bitwidth > dst_sca_ty.primitive_bitwidth\n     if truncate_fp:\n         return tl.tensor(builder.create_fp_trunc(input.handle,\n                                                  dst_ty.to_ir(builder)),\n                          dst_ty)\n \n-    # FP Extension\n+    # Standard floating types' casting: extension\n+    #   fp32 => fp64\n+    #   fp16 => fp32, fp64\n+    #   bf16 => fp32, fp64\n     ext_fp = src_sca_ty.is_floating() and \\\n         dst_sca_ty.is_floating() and \\\n-        src_sca_ty.fp_mantissa_width < dst_sca_ty.fp_mantissa_width\n+        src_sca_ty.primitive_bitwidth < dst_sca_ty.primitive_bitwidth\n     if ext_fp:\n         return tl.tensor(builder.create_fp_ext(input.handle,\n                                                dst_ty.to_ir(builder)),\n                          dst_ty)\n \n-    # Int cast\n+    # Casting between integer types\n     if src_sca_ty.is_int() and dst_sca_ty.is_int() and \\\n        (src_sca_ty.int_bitwidth != dst_sca_ty.int_bitwidth or src_sca_ty.int_signedness != dst_sca_ty.int_signedness):\n         sign_extend = src_sca_ty.is_int_signed() and not src_sca_ty.is_bool()\n@@ -658,8 +664,8 @@ def cast(input: tl.tensor,\n                                                      dst_ty.to_ir(builder), sign_extend),\n                              dst_ty)\n \n-    # Float to Int\n-    if src_sca_ty.is_floating() and dst_sca_ty.is_int():\n+    # Casting standard floating types to integer types\n+    if src_sca_ty.is_standard_floating() and dst_sca_ty.is_int():\n         if dst_sca_ty.is_bool():\n             ty = input.dtype.to_ir(builder)\n             _0 = tl.tensor(builder.get_null_value(ty), input.dtype)\n@@ -673,8 +679,8 @@ def cast(input: tl.tensor,\n                                                      dst_ty.to_ir(builder)),\n                              dst_ty)\n \n-    # int => float\n-    if src_sca_ty.is_int() and dst_sca_ty.is_floating():\n+    # Casting integer types to standard floating types\n+    if src_sca_ty.is_int() and dst_sca_ty.is_standard_floating():\n         if src_sca_ty.is_bool() or not src_sca_ty.is_int_signed():\n             return tl.tensor(builder.create_ui_to_fp(input.handle,\n                                                      dst_ty.to_ir(builder)),\n@@ -684,7 +690,7 @@ def cast(input: tl.tensor,\n                                                      dst_ty.to_ir(builder)),\n                              dst_ty)\n \n-    # ptr => int\n+    # Casting pointer types to integer types\n     if src_sca_ty.is_ptr() and dst_sca_ty.is_int():\n         bitwidth = dst_sca_ty.int_bitwidth\n         if bitwidth == 64:\n@@ -695,19 +701,14 @@ def cast(input: tl.tensor,\n                              tl.tensor(builder.get_int64(0), tl.int64),\n                              builder)\n \n-    if not src_sca_ty.is_ptr() and dst_sca_ty.is_ptr():\n+    # Casting integer types to pointer types\n+    if src_sca_ty.is_int() and dst_sca_ty.is_ptr():\n         return tl.tensor(builder.create_int_to_ptr(input.handle, dst_ty.to_ir(builder)), dst_ty)\n-    # Ptr . Ptr\n+\n+    # Casting pointer types to pointer types\n     if src_sca_ty.is_ptr() and dst_sca_ty.is_ptr():\n         return tl.tensor(builder.create_bitcast(input.handle, dst_ty.to_ir(builder)), dst_ty)\n-    # * . Bool\n-    if dst_sca_ty.is_bool():\n-        if src_sca_ty.is_ptr():\n-            input = cast(input, tl.int64, builder)\n-        other = builder.get_int64(0)\n-        if src_ty.is_bool():\n-            other = builder.create_splat(other, src_ty.get_block_shapes())\n-        return tl.tensor(builder.create_icmpNE(input.handle, other), dst_ty)\n+\n     assert False, f'cannot cast {input} to {dst_ty}'\n \n # ===----------------------------------------------------------------------===//"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -176,6 +176,9 @@ def _type_of(key):\n                 triton.language.uint32: 'u32',\n                 triton.language.uint64: 'u64',\n                 triton.language.float8: 'fp8',\n+                triton.language.float16: 'fp16',\n+                triton.language.bfloat16: 'bf16',\n+                triton.language.float32: 'fp32',\n             }[key]\n             return f'*{ty}'\n         if key is None:"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -6,8 +6,8 @@ func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n   %0 = tt.int_to_ptr %scalar_i64 : i64 -> !tt.ptr<f32>\n   // CHECK: !tt.ptr<f32> -> i64\n   %1 = tt.ptr_to_int %scalar_ptr : !tt.ptr<f32> -> i64\n-  // CHECK: f32 -> f16\n-  %2 = tt.fp_to_fp %scalar_f32 : f32 -> f16\n+  // CHECK: f32 to f16\n+  %2 = arith.truncf %scalar_f32 : f32 to f16\n \n   // 0D tensor -> 0D tensor\n   %tensor_ptr_0d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<!tt.ptr<f32>>\n@@ -18,8 +18,8 @@ func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n   %3 = tt.int_to_ptr %tensor_i64_0d : tensor<i64> -> tensor<!tt.ptr<f32>>\n   // CHECK: tensor<!tt.ptr<f32>> -> tensor<i64>\n   %4 = tt.ptr_to_int %tensor_ptr_0d : tensor<!tt.ptr<f32>> -> tensor<i64>\n-  // CHECK: tensor<f32> -> tensor<f16>\n-  %5 = tt.fp_to_fp %tensor_f32_0d : tensor<f32> -> tensor<f16>\n+  // CHECK: tensor<f32> to tensor<f16>\n+  %5 = arith.truncf %tensor_f32_0d : tensor<f32> to tensor<f16>\n \n   // 1D tensor -> 1D tensor\n   %tensor_ptr_1d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>>\n@@ -30,8 +30,8 @@ func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n   %6 = tt.int_to_ptr %tensor_i64_1d : tensor<16xi64> -> tensor<16x!tt.ptr<f32>>\n   // CHECK: tensor<16x!tt.ptr<f32>> -> tensor<16xi64>\n   %7 = tt.ptr_to_int %tensor_ptr_1d : tensor<16x!tt.ptr<f32>> -> tensor<16xi64>\n-  // CHECK: tensor<16xf32> -> tensor<16xf16>\n-  %8 = tt.fp_to_fp %tensor_f32_1d : tensor<16xf32> -> tensor<16xf16>\n+  // CHECK: tensor<16xf32> to tensor<16xf16>\n+  %8 = arith.truncf %tensor_f32_1d : tensor<16xf32> to tensor<16xf16>\n   return\n }\n "}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PtxAsmFormatTest.cpp", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "file_content_changes": "@@ -125,15 +125,21 @@ TEST_F(PtxAsmFormatTest, onlyAttachMLIRArgs) {\n   PTXBuilder builder;\n   const char *ptxCode =\n       \".param .b64 param0;\\n\" // prepare param0 (format string)\n-      \"st.param.b64 [param0], %0;\\n\";\n+      \"st.param.b64 [param0], %0;\\n\"\n+      \"st.param.b64 [param0], %1;\\n\"\n+      \"st.param.b64 [param0], %2;\\n\";\n \n   auto &ptxSnippet = *builder.create(ptxCode);\n-  auto *opr = builder.newOperand(v[0], \"r\");\n-  ptxSnippet({opr}, true);\n+  auto *opr0 = builder.newOperand(v[0], \"r\");\n+  auto *opr1 = builder.newOperand(v[1], \"r\");\n+  auto *opr2 = builder.newOperand(v[2], \"r\");\n+  ptxSnippet({opr1, opr2, opr0}, true);\n \n   EXPECT_EQ(builder.dump(), ptxCode);\n-  ASSERT_EQ(builder.getAllMLIRArgs()[0], v[0]);\n-  ASSERT_EQ(builder.getAllMLIRArgs().size(), 1);\n+  ASSERT_EQ(builder.getAllMLIRArgs()[0], v[1]);\n+  ASSERT_EQ(builder.getAllMLIRArgs()[1], v[2]);\n+  ASSERT_EQ(builder.getAllMLIRArgs()[2], v[0]);\n+  ASSERT_EQ(builder.getAllMLIRArgs().size(), 3);\n }\n \n } // namespace triton"}]