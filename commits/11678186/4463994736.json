[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1213,7 +1213,7 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, o\n         if (M, N, K, num_warps) == (128, 256, 32, 8):\n             pytest.skip(\"shared memory out of resource\")\n         if out_dtype == 'float16':\n-            #TODO: support out_dtype=float16 for tl.dot on V100\n+            # TODO: support out_dtype=float16 for tl.dot on V100\n             pytest.skip(\"Only test out_dtype=float16 on devices with sm >=80\")\n \n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32"}]