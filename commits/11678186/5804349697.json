[{"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -300,6 +300,13 @@ def forward(ctx, q, k, v, causal, sm_scale, sequence_parallel=False):\n         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n         assert Lq == Lk and Lk == Lv\n         assert Lk in {16, 32, 64, 128}\n+\n+        if capability[0] > 8:\n+            import os\n+            enable_mmav3 = os.environ.get('ENABLE_MMA_V3')\n+            if Lk == 128 and enable_mmav3:\n+                BLOCK_N = 128\n+\n         o = torch.empty_like(q)\n         grid = (cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)"}]