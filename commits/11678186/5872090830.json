[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "file_content_changes": "@@ -170,7 +170,7 @@ def __init__(self, vec, per_phase, max_phase, order, ctas_per_cga, cta_split_num\n         self.cta_split_num = str(cta_split_num)\n         self.cta_order = str(cta_order)\n \n-    def __str__(self):        \n+    def __str__(self):\n         return f\"#{GPU_DIALECT}.shared<{{vec={self.vec}, perPhase={self.per_phase}, maxPhase={self.max_phase}, order={self.order}, CTAsPerCGA={self.ctas_per_cga}, CTASplitNum={self.cta_split_num}, CTAOrder={self.cta_order}}}>\"\n \n \n@@ -1916,7 +1916,7 @@ def test_store_op(M, src_layout, device):\n def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):\n     if is_hip():\n         pytest.skip(\"test_convert1d is not supported in HIP\")\n-      \n+\n     ir = f\"\"\"\n     #dst = {dst_layout}\n     #src = {src_layout}\n@@ -1979,7 +1979,6 @@ def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n     if is_hip():\n         pytest.skip(\"test_chain_reduce is not supported in HIP\")\n \n-\n     op_str = \"\"\n     if op == \"sum\":\n         op_str = f\"\"\"\n@@ -2646,7 +2645,7 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n \n     if is_hip():\n         return\n-    \n+\n     ptx = pgm.asm[\"ptx\"]\n     if N % 16 == 0:\n         assert \"ld.global.v4.b32\" in ptx\n@@ -3465,7 +3464,7 @@ def kernel(Out):\n def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n     if is_hip():\n         pytest.skip(\"test_convert2d is not supported in HIP\")\n-  \n+\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 12, "deletions": 9, "changes": 21, "file_content_changes": "@@ -296,11 +296,8 @@ def get_architecture_descriptor(capability):\n     return capability\n \n \n-def add_cuda_stages(arch, context, num_warps, num_stages, extern_libs, stages):\n-    stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n-                       lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps), num_stages, arch))\n-    stages[\"llir\"] = (lambda path: Path(path).read_text(),\n-                      lambda src: ttgir_to_llir(src, extern_libs, arch))\n+def add_cuda_stages(arch, extern_libs, stages):\n+\n     stages[\"ptx\"] = (lambda path: Path(path).read_text(),\n                      lambda src: llir_to_ptx(src, arch))\n     stages[\"cubin\"] = (lambda path: Path(path).read_bytes(),\n@@ -354,12 +351,18 @@ def compile(fn, **kwargs):\n     stages[\"ttir\"] = (lambda path: parse_mlir_module(path, context),\n                       lambda src: optimize_ttir(ast_to_ttir(src, signature, configs[0], constants, debug=debug, arch=arch), arch))\n     if is_cuda:\n-        add_cuda_stages(arch, context, num_warps, num_stages, extern_libs, stages)\n-    elif is_hip():\n+        stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n+                           lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n+        stages[\"llir\"] = (lambda path: Path(path).read_text(),\n+                          lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n+        add_cuda_stages(arch, extern_libs, stages)\n+    elif device_type == \"hip\":\n         _device_backend.add_stages(arch, extern_libs, stages, context=context, num_warps=num_warps, num_stages=num_stages)\n     elif device_type == \"xpu\":\n         stages[\"ttgir\"] = (lambda path: parse_mlir_module(path, context),\n-                           lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps), num_stages, arch))\n+                           lambda src: optimize_ttgir(ttir_to_ttgir(src, num_warps, num_ctas, arch), num_stages, num_warps, num_ctas, arch, cluster_info, enable_warp_specialization, enable_persistent, optimize_epilogue))\n+        stages[\"llir\"] = (lambda path: Path(path).read_text(),\n+                          lambda src: ttgir_to_llir(src, extern_libs, arch, tma_infos))\n         _device_backend.add_stages(arch, extern_libs, stages)\n     else:\n         _device_backend.add_stages(arch, extern_libs, stages)\n@@ -479,7 +482,7 @@ def compile(fn, **kwargs):\n         if ir_name == \"ttgir\":\n             metadata[\"enable_warp_specialization\"] = ir.is_ws_supported(next_module)\n             if metadata[\"enable_warp_specialization\"]:\n-                metadata[\"num_warps\"] = get_num_warps(next_module)  \n+                metadata[\"num_warps\"] = get_num_warps(next_module)\n         if ir_name == \"ptx\":\n             metadata[\"name\"] = get_kernel_name(next_module, pattern='// .globl')\n         if ir_name == \"amdgcn\":"}]