[{"filename": "python/triton/compiler.py", "status": "modified", "additions": 59, "deletions": 118, "changes": 177, "file_content_changes": "@@ -80,8 +80,7 @@ def mangle_ty(ty):\n def mangle_fn(name, arg_tys, constants):\n     # doesn't mangle ret type, which must be a function of arg tys\n     mangled_arg_names = '_'.join([mangle_ty(ty) for ty in arg_tys])\n-    mangled_constants = '_'.join(\n-        [f'{i}c{repr(constants[i])}' for i in sorted(constants)])\n+    mangled_constants = '_'.join([f'{i}c{repr(constants[i])}' for i in sorted(constants)])\n     mangled_constants = mangled_constants.replace('.', '_d_')\n     mangled_constants = mangled_constants.replace(\"'\", '_sq_')\n     ret = f'{name}__{mangled_arg_names}__{mangled_constants}'\n@@ -194,8 +193,7 @@ def visit_Return(self, node):\n             self.builder.ret([])\n             return None\n         if isinstance(ret_value, tuple):\n-            ret_values = [triton.language.core._to_tensor(\n-                v, self.builder) for v in ret_value]\n+            ret_values = [triton.language.core._to_tensor(v, self.builder) for v in ret_value]\n             ret_types = [v.type for v in ret_values]\n             self.builder.ret([v.handle for v in ret_values])\n             return tuple(ret_types)\n@@ -213,16 +211,13 @@ def visit_FunctionDef(self, node):\n             name = arg_node.arg\n             st_target = ast.Name(id=name, ctx=ast.Store())\n             if annotation is None:\n-                init_node = ast.Assign(\n-                    targets=[st_target], value=default_value)\n+                init_node = ast.Assign(targets=[st_target], value=default_value)\n             else:\n-                init_node = ast.AnnAssign(\n-                    target=st_target, value=default_value, annotation=annotation)\n+                init_node = ast.AnnAssign(target=st_target, value=default_value, annotation=annotation)\n             self.visit(init_node)\n         # initialize function\n         visibility = \"public\" if self.is_kernel else \"private\"\n-        fn = self.builder.get_or_insert_function(\n-            self.module, self.function_name, self.prototype.to_ir(self.builder), visibility)\n+        fn = self.builder.get_or_insert_function(self.module, self.function_name, self.prototype.to_ir(self.builder), visibility)\n         self.module.push_back(fn)\n         entry = fn.add_entry_block()\n         arg_values = []\n@@ -236,10 +231,8 @@ def visit_FunctionDef(self, node):\n                 continue\n             else:\n                 if i in self.attributes:\n-                    fn.set_arg_attr(idx, \"tt.divisibility\",\n-                                    self.attributes[i][1])\n-                arg_values.append(triton.language.tensor(\n-                    fn.args(idx), self.prototype.param_types[idx]))\n+                    fn.set_arg_attr(idx, \"tt.divisibility\", self.attributes[i][1])\n+                arg_values.append(triton.language.tensor(fn.args(idx), self.prototype.param_types[idx]))\n                 idx += 1\n \n         insert_pt = self.builder.get_insertion_block()\n@@ -410,32 +403,25 @@ def visit_If(self, node):\n                 self.builder.set_insertion_point_to_end(ip_block)\n \n                 if then_defs or node.orelse:  # with else block\n-                    if_op = self.builder.create_if_op(\n-                        [ty.to_ir(self.builder) for ty in ret_types], cond.handle, True)\n+                    if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, True)\n                     then_block.merge_block_before(if_op.get_then_block())\n-                    self.builder.set_insertion_point_to_end(\n-                        if_op.get_then_block())\n+                    self.builder.set_insertion_point_to_end(if_op.get_then_block())\n                     if len(names) > 0:\n-                        self.builder.create_yield_op(\n-                            [then_defs[n].handle for n in names])\n+                        self.builder.create_yield_op([then_defs[n].handle for n in names])\n                     if not node.orelse:\n                         else_block = if_op.get_else_block()\n                     else:\n                         else_block.merge_block_before(if_op.get_else_block())\n-                    self.builder.set_insertion_point_to_end(\n-                        if_op.get_else_block())\n+                    self.builder.set_insertion_point_to_end(if_op.get_else_block())\n                     if len(names) > 0:\n-                        self.builder.create_yield_op(\n-                            [else_defs[n].handle for n in names])\n+                        self.builder.create_yield_op([else_defs[n].handle for n in names])\n                 else:  # no else block\n-                    if_op = self.builder.create_if_op(\n-                        [ty.to_ir(self.builder) for ty in ret_types], cond.handle, False)\n+                    if_op = self.builder.create_if_op([ty.to_ir(self.builder) for ty in ret_types], cond.handle, False)\n                     then_block.merge_block_before(if_op.get_then_block())\n \n             # update values yielded by IfOp\n             for i, name in enumerate(names):\n-                new_tensor = triton.language.core.tensor(\n-                    if_op.get_result(i), ret_types[i])\n+                new_tensor = triton.language.core.tensor(if_op.get_result(i), ret_types[i])\n                 self.lscope[name] = new_tensor\n                 self.local_defs[name] = new_tensor\n \n@@ -489,8 +475,7 @@ def visit_Compare(self, node):\n     def visit_UnaryOp(self, node):\n         op = self.visit(node.operand)\n         if type(node.op) == ast.Not:\n-            assert isinstance(\n-                op, triton.language.constexpr), \"`not` only supported for constexpr at the moment\"\n+            assert isinstance(op, triton.language.constexpr), \"`not` only supported for constexpr at the moment\"\n             return triton.language.constexpr(not op)\n         fn = {\n             ast.USub: '__neg__',\n@@ -542,8 +527,7 @@ def visit_While(self, node):\n             cond_block.merge_block_before(before_block)\n             self.builder.set_insertion_point_to_end(before_block)\n             # create ConditionOp: e.g., scf.condition(%cond) %arg0, %arg1, ...\n-            self.builder.create_condition_op(\n-                cond.handle, [before_block.arg(i) for i in range(len(init_args))])\n+            self.builder.create_condition_op(cond.handle, [before_block.arg(i) for i in range(len(init_args))])\n             # merge the loop body\n             after_block = self.builder.create_block_with_parent(while_op.get_after(),\n                                                                 [ty.to_ir(self.builder) for ty in ret_types])\n@@ -553,15 +537,12 @@ def visit_While(self, node):\n \n         # update global uses in while_op\n         for i, name in enumerate(names):\n-            before_block.replace_use_in_block_with(\n-                init_args[i].handle, before_block.arg(i))\n-            after_block.replace_use_in_block_with(\n-                init_args[i].handle, after_block.arg(i))\n+            before_block.replace_use_in_block_with(init_args[i].handle, before_block.arg(i))\n+            after_block.replace_use_in_block_with(init_args[i].handle, after_block.arg(i))\n \n         # WhileOp defines new values, update the symbol table (lscope, local_defs)\n         for i, name in enumerate(names):\n-            new_def = triton.language.core.tensor(\n-                while_op.get_result(i), ret_types[i])\n+            new_def = triton.language.core.tensor(while_op.get_result(i), ret_types[i])\n             self.lscope[name] = new_def\n             self.local_defs[name] = new_def\n \n@@ -589,16 +570,14 @@ def visit_For(self, node):\n         iter_args = [self.visit(arg) for arg in node.iter.args]\n         # collect lower bound (lb), upper bound (ub), and step\n         lb = iter_args[0] if len(iter_args) > 1 else self.visit(ast.Num(0))\n-        ub = iter_args[1] if len(\n-            iter_args) > 1 else self.visit(node.iter.args[0])\n+        ub = iter_args[1] if len(iter_args) > 1 else self.visit(node.iter.args[0])\n         step = iter_args[2] if len(iter_args) > 2 else self.visit(ast.Num(1))\n         # static for loops: all iterator arguments are constexpr\n         if isinstance(lb, triton.language.constexpr) and \\\n            isinstance(ub, triton.language.constexpr) and \\\n            isinstance(step, triton.language.constexpr):\n             sta_range = iterator(lb.value, ub.value, step.value)\n-            static_unrolling = os.environ.get(\n-                'TRITON_STATIC_LOOP_UNROLLING', False)\n+            static_unrolling = os.environ.get('TRITON_STATIC_LOOP_UNROLLING', False)\n             if static_unrolling and len(sta_range) <= 10:\n                 for i in sta_range:\n                     self.lscope[node.target.id] = triton.language.constexpr(i)\n@@ -622,8 +601,7 @@ def visit_For(self, node):\n         step = self.builder.create_to_index(step)\n         # Create placeholder for the loop induction variable\n         iv = self.builder.create_undef(self.builder.get_int32_ty())\n-        self.set_value(node.target.id, triton.language.core.tensor(\n-            iv, triton.language.core.int32))\n+        self.set_value(node.target.id, triton.language.core.tensor(iv, triton.language.core.int32))\n \n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n@@ -642,23 +620,18 @@ def visit_For(self, node):\n             names = []\n             for name in self.local_defs:\n                 if name in liveins:\n-                    assert self.is_triton_tensor(\n-                        self.local_defs[name]), f'{name} is not tensor'\n+                    assert self.is_triton_tensor(self.local_defs[name]), f'{name} is not tensor'\n                     assert self.is_triton_tensor(liveins[name])\n                     if self.local_defs[name].type != liveins[name].type:\n                         local_value = self.local_defs[name]\n-                        self.local_defs[name] = local_value.to(\n-                            liveins[name].dtype, _builder=self.builder)\n+                        self.local_defs[name] = local_value.to(liveins[name].dtype, _builder=self.builder)\n                     names.append(name)\n-                    init_args.append(triton.language.core._to_tensor(\n-                        liveins[name], self.builder))\n-                    yields.append(triton.language.core._to_tensor(\n-                        self.local_defs[name], self.builder))\n+                    init_args.append(triton.language.core._to_tensor(liveins[name], self.builder))\n+                    yields.append(triton.language.core._to_tensor(self.local_defs[name], self.builder))\n \n             # create ForOp\n             self.builder.set_insertion_point_to_end(insert_block)\n-            for_op = self.builder.create_for_op(\n-                lb, ub, step, [arg.handle for arg in init_args])\n+            for_op = self.builder.create_for_op(lb, ub, step, [arg.handle for arg in init_args])\n             block.merge_block_before(for_op.get_body(0))\n \n             # update induction variable with actual value, and replace all uses\n@@ -668,8 +641,7 @@ def visit_For(self, node):\n                 ub_si = self.builder.create_index_to_si(ub)\n                 iv = self.builder.create_sub(ub_si, iv)\n             self.lscope[node.target.id].handle.replace_all_uses_with(iv)\n-            self.set_value(node.target.id, triton.language.core.tensor(\n-                iv, triton.language.core.int32))\n+            self.set_value(node.target.id, triton.language.core.tensor(iv, triton.language.core.int32))\n \n             # create YieldOp\n             self.builder.set_insertion_point_to_end(for_op.get_body(0))\n@@ -680,13 +652,11 @@ def visit_For(self, node):\n             # replace global uses with block arguments\n             for i, name in enumerate(names):\n                 # arg0 is the induction variable\n-                for_op.get_body(0).replace_use_in_block_with(\n-                    init_args[i].handle, for_op.get_body(0).arg(i + 1))\n+                for_op.get_body(0).replace_use_in_block_with(init_args[i].handle, for_op.get_body(0).arg(i + 1))\n \n         # update lscope & local_defs (ForOp defines new values)\n         for i, name in enumerate(names):\n-            self.set_value(name, triton.language.core.tensor(\n-                for_op.get_result(i), yields[i].type))\n+            self.set_value(name, triton.language.core.tensor(for_op.get_result(i), yields[i].type))\n \n         for stmt in node.orelse:\n             assert False, \"Don't know what to do with else after for\"\n@@ -720,21 +690,18 @@ def visit_Call(self, node):\n                     else triton.language.constexpr(arg) for arg in args]\n             # generate function def\n             attributes = dict()\n-            constexprs = [i for i, arg in enumerate(\n-                args) if isinstance(arg, triton.language.constexpr)]\n+            constexprs = [i for i, arg in enumerate(args) if isinstance(arg, triton.language.constexpr)]\n             constants = {i: args[i] for i in constexprs}\n             # generate call\n-            args = [None if i in constexprs else arg for i,\n-                    arg in enumerate(args)]\n+            args = [None if i in constexprs else arg for i, arg in enumerate(args)]\n             arg_vals = [arg.handle for arg in args if arg is not None]\n             arg_types = [arg.type for arg in args if arg is not None]\n             fn_name = mangle_fn(fn.__name__, arg_types, constants)\n             # generate function def if necessary\n             if not self.module.has_function(fn_name):\n                 prototype = triton.language.function_type([], arg_types)\n                 gscope = sys.modules[fn.fn.__module__].__dict__\n-                generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants,\n-                                          module=self.module, function_name=fn_name, function_types=self.function_ret_types)\n+                generator = CodeGenerator(self.builder.context, prototype, gscope, attributes, constants, module=self.module, function_name=fn_name, function_types=self.function_ret_types)\n                 generator.visit(fn.parse())\n                 callee_ret_type = generator.last_ret_type\n                 self.function_ret_types[fn_name] = callee_ret_type\n@@ -750,8 +717,7 @@ def visit_Call(self, node):\n                 # should return a tuple of tl.tensor\n                 results = []\n                 for i in range(call_op.get_num_results()):\n-                    results.append(triton.language.tensor(\n-                        call_op.get_result(i), callee_ret_type[i]))\n+                    results.append(triton.language.tensor(call_op.get_result(i), callee_ret_type[i]))\n                 return tuple(results)\n         if (hasattr(fn, '__self__') and self.is_triton_tensor(fn.__self__)) \\\n                 or impl.is_builtin(fn):\n@@ -812,8 +778,7 @@ def visit(self, node):\n             # The ast library added visit_Constant and deprecated some other\n             # methods but we can't move to that without breaking Python 3.6 and 3.7.\n             warnings.simplefilter(\"ignore\", DeprecationWarning)  # python 3.9\n-            warnings.simplefilter(\n-                \"ignore\", PendingDeprecationWarning)  # python 3.8\n+            warnings.simplefilter(\"ignore\", PendingDeprecationWarning)  # python 3.8\n             return super().visit(node)\n \n     def generic_visit(self, node):\n@@ -874,25 +839,20 @@ def build_triton_ir(fn, signature, specialization, constants):\n     context = _triton.ir.context()\n     context.load_triton()\n     # create kernel prototype\n-    def cst_key(i): return fn.arg_names.index(i) if isinstance(i, str) else i  # noqa: E704\n+    cst_key = lambda i: fn.arg_names.index(i) if isinstance(i, str) else i\n     constants = {cst_key(key): value for key, value in constants.items()}\n     # visit kernel AST\n     gscope = fn.__globals__.copy()\n-    function_name = '_'.join(\n-        [fn.__name__, kernel_suffix(signature.values(), specialization)])\n+    function_name = '_'.join([fn.__name__, kernel_suffix(signature.values(), specialization)])\n     tys = list(signature.values())\n-    new_constants = {\n-        k: True if k in tys and tys[k] == \"i1\" else 1 for k in specialization.equal_to_1}\n-    new_attrs = {k: (\"multiple_of\", 16)\n-                 for k in specialization.divisible_by_16}\n+    new_constants = {k: True if k in tys and tys[k] == \"i1\" else 1 for k in specialization.equal_to_1}\n+    new_attrs = {k: (\"multiple_of\", 16) for k in specialization.divisible_by_16}\n     all_constants = constants.copy()\n     all_constants.update(new_constants)\n-    arg_types = [str_to_ty(v)\n-                 for k, v in signature.items() if k not in constants]\n+    arg_types = [str_to_ty(v) for k, v in signature.items() if k not in constants]\n \n     prototype = triton.language.function_type([], arg_types)\n-    generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants,\n-                              function_name=function_name, attributes=new_attrs, is_kernel=True)\n+    generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants, function_name=function_name, attributes=new_attrs, is_kernel=True)\n     try:\n         generator.visit(fn.parse())\n     except Exception as e:\n@@ -914,7 +874,6 @@ def optimize_triton_ir(mod):\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n     pm.add_licm_pass()\n-\n     pm.run(mod)\n     return mod\n \n@@ -1025,18 +984,15 @@ def path_to_ptxas():\n     for prefix in prefixes:\n         ptxas = os.path.join(prefix, \"bin\", \"ptxas\")\n         if os.path.exists(ptxas):\n-            result = subprocess.check_output(\n-                [ptxas, \"--version\"], stderr=subprocess.STDOUT)\n+            result = subprocess.check_output([ptxas, \"--version\"], stderr=subprocess.STDOUT)\n             if result is not None:\n-                version = re.search(r\".*release (\\d+\\.\\d+).*\",\n-                                    result.decode(\"utf-8\"), flags=re.MULTILINE)\n+                version = re.search(r\".*release (\\d+\\.\\d+).*\", result.decode(\"utf-8\"), flags=re.MULTILINE)\n                 if version is not None:\n                     return ptxas, version.group(1)\n     raise RuntimeError(\"Cannot find ptxas\")\n \n \n-instance_descriptor = namedtuple(\"instance_descriptor\", [\n-                                 \"divisible_by_16\", \"equal_to_1\"], defaults=[set(), set()])\n+instance_descriptor = namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"], defaults=[set(), set()])\n \n \n # ------------------------------------------------------------------------------\n@@ -1078,8 +1034,7 @@ def binary_name_to_header_name(name):\n \n \n def generate_launcher(constants, signature):\n-    arg_decls = ', '.join(\n-        f\"{ty_to_cpp(ty)} arg{i}\" for i, ty in signature.items())\n+    arg_decls = ', '.join(f\"{ty_to_cpp(ty)} arg{i}\" for i, ty in signature.items())\n \n     def _extracted_type(ty):\n         if ty[0] == '*':\n@@ -1109,8 +1064,7 @@ def format_of(ty):\n             \"int64_t\": \"L\",\n         }[ty]\n \n-    format = \"iiiiiKKOOO\" + \\\n-        ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])\n+    format = \"iiiiiKKOOO\" + ''.join([format_of(_extracted_type(ty)) for ty in signature.values()])\n \n     # generate glue code\n     src = f\"\"\"\n@@ -1248,8 +1202,7 @@ def __init__(self, key):\n         self.key = key\n         self.lock_path = None\n         # create cache directory if it doesn't exist\n-        self.cache_dir = os.environ.get(\n-            'TRITON_CACHE_DIR', default_cache_dir())\n+        self.cache_dir = os.environ.get('TRITON_CACHE_DIR', default_cache_dir())\n         if self.cache_dir:\n             self.cache_dir = os.path.join(self.cache_dir, self.key)\n             self.lock_path = os.path.join(self.cache_dir, \"lock\")\n@@ -1284,8 +1237,7 @@ def put(self, data, filename, binary=True):\n \n @functools.lru_cache()\n def libcuda_dirs():\n-    locs = subprocess.check_output(\n-        [\"whereis\", \"libcuda.so\"]).decode().strip().split()[1:]\n+    locs = subprocess.check_output([\"whereis\", \"libcuda.so\"]).decode().strip().split()[1:]\n     return [os.path.dirname(loc) for loc in locs]\n \n \n@@ -1304,8 +1256,7 @@ def _build(name, src, srcdir):\n     cuda_path = os.environ.get('CUDA_PATH', default_cuda_dir())\n     cu_include_dir = os.path.join(cuda_path, \"include\")\n     suffix = sysconfig.get_config_var('EXT_SUFFIX')\n-    so = os.path.join(srcdir, '{name}{suffix}'.format(\n-        name=name, suffix=suffix))\n+    so = os.path.join(srcdir, '{name}{suffix}'.format(name=name, suffix=suffix))\n     # try to avoid setuptools if possible\n     cc = os.environ.get(\"CC\")\n     if cc is None:\n@@ -1315,8 +1266,7 @@ def _build(name, src, srcdir):\n         cc = gcc if gcc is not None else clang\n     py_include_dir = get_paths()[\"include\"]\n \n-    cc_cmd = [cc, src, \"-O3\", f\"-I{cu_include_dir}\", f\"-I{py_include_dir}\",\n-              f\"-I{srcdir}\", \"-shared\", \"-fPIC\", \"-lcuda\", \"-o\", so]\n+    cc_cmd = [cc, src, \"-O3\", f\"-I{cu_include_dir}\", f\"-I{py_include_dir}\", f\"-I{srcdir}\", \"-shared\", \"-fPIC\", \"-lcuda\", \"-o\", so]\n     cc_cmd += [f\"-L{dir}\" for dir in cuda_lib_dirs]\n     ret = subprocess.check_call(cc_cmd)\n \n@@ -1365,8 +1315,7 @@ def make_so_cache_key(version_hash, signature, constants):\n \n def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_stages):\n     # Get unique key for the compiled code\n-    def get_conf_key(conf): return (  # noqa: E704\n-        sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n+    get_conf_key = lambda conf: (sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n     configs_key = [get_conf_key(conf) for conf in configs]\n     key = f\"{fn_hash}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}\"\n     key = hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n@@ -1379,25 +1328,22 @@ def read_or_execute(cache_manager, force_compile, file_name, metadata,\n     suffix = file_name.split(\".\")[1]\n     if not force_compile and cache_manager.has_file(file_name):\n         module = run_if_found(cache_manager._make_path(file_name))\n-        data = module if isinstance(\n-            module, bytes) else str(module).encode(\"utf-8\")\n+        data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n         md5 = hashlib.md5(data).hexdigest()\n         has_changed = metadata and md5 != metadata[\"md5\"][suffix]\n         return module, md5, has_changed, True\n     module = run_if_not_found()\n     data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n     md5 = hashlib.md5(data).hexdigest()\n-    cache_manager.put(\n-        data, file_name, True if isinstance(data, bytes) else data)\n+    cache_manager.put(data, file_name, True if isinstance(data, bytes) else data)\n     return module, md5, True, False\n \n #\n \n \n def make_stub(name, signature, constants):\n     # name of files that are cached\n-    so_cache_key = make_so_cache_key(\n-        triton.runtime.jit.version_key(), signature, constants)\n+    so_cache_key = make_so_cache_key(triton.runtime.jit.version_key(), signature, constants)\n     so_cache_manager = CacheManager(so_cache_key)\n     so_name = f\"{name}.so\"\n     # retrieve stub from cache if it exists\n@@ -1428,8 +1374,7 @@ def make_hash(fn, **kwargs):\n         num_warps = kwargs.get(\"num_warps\", 4)\n         num_stages = kwargs.get(\"num_stages\", 3)\n         # Get unique key for the compiled code\n-        def get_conf_key(conf): return (  # noqa: E704\n-            sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n+        get_conf_key = lambda conf: (sorted(conf.divisible_by_16), sorted(conf.equal_to_1))\n         configs_key = [get_conf_key(conf) for conf in configs]\n         key = f\"{fn.cache_key}-{''.join(signature.values())}-{configs_key}-{constants}-{num_warps}-{num_stages}\"\n         return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n@@ -1502,8 +1447,7 @@ def compile(fn, **kwargs):\n         name = fn.__name__\n         first_stage = 0\n         if isinstance(signature, str):\n-            signature = {k: v.strip()\n-                         for k, v in enumerate(signature.split(\",\"))}\n+            signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n         kwargs[\"signature\"] = signature\n     else:\n         assert isinstance(fn, str)\n@@ -1535,8 +1479,7 @@ def compile(fn, **kwargs):\n         with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n             metadata = json.load(f)\n     else:\n-        metadata = {\"num_warps\": num_warps,\n-                    \"num_stages\": num_stages, \"ctime\": dict()}\n+        metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n         if ext == \"ptx\":\n             assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n             metadata[\"shared\"] = kwargs[\"shared\"]\n@@ -1605,8 +1548,7 @@ def _init_handles(self):\n         max_shared = cuda_utils.get_device_properties(device)[\"max_shared_mem\"]\n         if self.shared > max_shared:\n             raise OutOfResources(self.shared, max_shared, \"shared memory\")\n-        mod, func, n_regs, n_spills = cuda_utils.load_binary(\n-            self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n+        mod, func, n_regs, n_spills = cuda_utils.load_binary(self.metadata[\"name\"], self.asm[\"cubin\"], self.shared, device)\n         self.cu_module = mod\n         self.cu_function = func\n \n@@ -1773,8 +1715,7 @@ def __init__(self):\n                 with open(so, \"rb\") as f:\n                     cache.put(f.read(), fname, binary=True)\n         import importlib.util\n-        spec = importlib.util.spec_from_file_location(\n-            \"cuda_utils\", cache._make_path(fname))\n+        spec = importlib.util.spec_from_file_location(\"cuda_utils\", cache._make_path(fname))\n         mod = importlib.util.module_from_spec(spec)\n         spec.loader.exec_module(mod)\n         self.load_binary = mod.load_binary"}]