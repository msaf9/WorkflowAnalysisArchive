[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 27, "deletions": 3, "changes": 30, "file_content_changes": "@@ -5,12 +5,32 @@ on:\n   pull_request:\n     branches:\n       - main\n+      - triton-mlir\n \n jobs:\n \n+  Runner-Preparation:\n+    runs-on: ubuntu-latest\n+    outputs:\n+      matrix: ${{ steps.set-matrix.outputs.matrix }}\n+    steps:\n+    - name: Prepare runner matrix\n+      id: set-matrix\n+      run: |\n+        if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n+          echo '::set-output name=matrix::[[\"self-hosted\", \"A10\"], \"macos-latest\"]'\n+        else\n+          echo '::set-output name=matrix::[\"ubuntu-latest\", \"macos-latest\"]'\n+        fi\n+\n   Integration-Tests:\n+    needs: Runner-Preparation\n+\n+    runs-on: ${{ matrix.runner }}\n \n-    runs-on: self-hosted\n+    strategy:\n+      matrix:\n+        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix)}}\n \n     steps:\n \n@@ -19,26 +39,29 @@ jobs:\n \n       - name: Clear cache\n         run: |\n-          rm -r ~/.triton/cache/\n-        continue-on-error: true\n+          rm -rf ~/.triton/cache/\n \n       - name: Check imports\n+        if: ${{ matrix.runner != 'macos-latest' }}\n         run: |\n           pip install isort\n           isort -c ./python || ( echo '::error title=Imports not sorted::Please run \\\"isort ./python\\\"' ; exit 1 )\n \n       - name: Check python style\n+        if: ${{ matrix.runner != 'macos-latest' }}\n         run: |\n           pip install autopep8\n           autopep8 -a -r -d --exit-code ./python || ( echo '::error title=Style issues::Please run \\\"autopep8 -a -r -i ./python\\\"' ; exit 1 )\n \n       - name: Check cpp style\n+        if: ${{ matrix.runner != 'macos-latest' }}\n         run: |\n           sudo apt-get install -y clang-format\n           find . -regex '.*\\.\\(cpp\\|hpp\\|h\\|cc\\)' -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file --dry-run -Werror -i ||\n           (echo '::error title=Style issues:: Please run `find . -regex \".*\\.\\(cpp\\|hpp\\|h\\|cc\\)\" -not -path \"./python/build/*\" -not -path \"./include/triton/external/*\" -print0 | xargs -0 -n1 clang-format -style=file -i`' ; exit 1)\n \n       - name: Flake8\n+        if: ${{ matrix.runner != 'macos-latest' }}\n         run: |\n           pip install flake8\n           flake8 --config ./python/setup.cfg ./python || ( echo '::error::Flake8 failed; see logs for errors.' ; exit 1 )\n@@ -59,6 +82,7 @@ jobs:\n           lit -v \"$LIT_TEST_DIR\"\n \n       - name: Run python tests\n+        if: ${{ matrix.runner[0] == 'self-hosted' }}\n         run: |\n           cd python/tests\n           pytest"}, {"filename": "bin/triton-translate.cpp", "status": "modified", "additions": 0, "deletions": 10, "changes": 10, "file_content_changes": "@@ -64,16 +64,6 @@ OwningOpRef<ModuleOp> loadMLIRModule(llvm::StringRef inputFilename,\n     return nullptr;\n   }\n \n-  mlir::PassManager pm(module->getContext());\n-  applyPassManagerCLOptions(pm);\n-\n-  pm.addPass(createConvertTritonGPUToLLVMPass());\n-\n-  if (failed(pm.run(module->getOperation()))) {\n-    llvm::errs() << \"Pass execution failed\";\n-    return nullptr;\n-  }\n-\n   return module;\n }\n "}, {"filename": "cmake/FindLLVM.cmake", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -25,7 +25,7 @@\n #  LLVM_VERSION_STRING - Full LLVM version string (e.g. 6.0.0svn).\n #  LLVM_VERSION_BASE_STRING - Base LLVM version string without git/svn suffix (e.g. 6.0.0).\n #\n-# Note: The variable names were chosen in conformance with the offical CMake\n+# Note: The variable names were chosen in conformance with the official CMake\n # guidelines, see ${CMAKE_ROOT}/Modules/readme.txt.\n \n # Try suffixed versions to pick up the newest LLVM install available on Debian\n@@ -196,4 +196,4 @@ include(FindPackageHandleStandardArgs)\n \n find_package_handle_standard_args(LLVM\n     REQUIRED_VARS LLVM_ROOT_DIR\n-    VERSION_VAR LLVM_VERSION_STRING)\n\\ No newline at end of file\n+    VERSION_VAR LLVM_VERSION_STRING)"}, {"filename": "docs/programming-guide/chapter-2/related-work.rst", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -14,7 +14,7 @@ Traditional compilers typically rely on intermediate representations, such as LL\n Program Representation\n +++++++++++++++++++++++\n \n-Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample litterature on linear and integer programming.\n+Polyhedral compilation is a vast area of research. In this section we only outline the most basic aspects of this topic, but readers interested in the solid mathematical foundations underneath may refer to the ample literature on linear and integer programming.\n \n .. table::\n     :widths: 50 50"}, {"filename": "include/triton/Analysis/Allocation.h", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "file_content_changes": "@@ -14,7 +14,12 @@ namespace mlir {\n \n namespace triton {\n class AllocationAnalysis;\n-}\n+\n+SmallVector<unsigned>\n+getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n+                             unsigned &outVec);\n+\n+} // namespace triton\n \n /// Modified from llvm-15.0: llvm/ADT/AddressRanges.h\n /// A class that represents an interval, specified using a start and an end\n@@ -156,7 +161,7 @@ class Allocation {\n \n private:\n   template <BufferT::BufferKind Kind, typename KeyType, typename... Args>\n-  void addBuffer(KeyType &key, Args &&... args) {\n+  void addBuffer(KeyType &key, Args &&...args) {\n     auto buffer = BufferT(Kind, std::forward<Args>(args)...);\n     bufferSet[buffer.id] = std::move(buffer);\n     if constexpr (Kind == BufferT::BufferKind::Explicit) {"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -2,7 +2,10 @@\n #define TRITON_ANALYSIS_UTILITY_H\n \n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include <algorithm>\n+#include <numeric>\n #include <string>\n+\n namespace mlir {\n \n bool isSharedEncoding(Value value);\n@@ -11,6 +14,12 @@ bool maybeSharedAllocationOp(Operation *op);\n \n std::string getValueOperandName(Value value, AsmState &state);\n \n+template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n+  return std::accumulate(arr.begin(), arr.end(), 1, std::multiplies{});\n+}\n+\n+template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h", "status": "modified", "additions": 72, "deletions": 51, "changes": 123, "file_content_changes": "@@ -4,8 +4,6 @@\n #include \"mlir/IR/Value.h\"\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/StringRef.h\"\n-#include \"llvm/Support/Format.h\"\n-#include \"llvm/Support/FormatVariadic.h\"\n #include <memory>\n #include <string>\n \n@@ -15,6 +13,7 @@ using llvm::StringRef;\n \n class PTXInstr;\n class PTXInstrCommon;\n+class PTXInstrExecution;\n \n // PTXBuilder helps to manage a PTX asm program consists of one or multiple\n // instructions.\n@@ -25,7 +24,8 @@ class PTXInstrCommon;\n // string and C++ if-else code.\n //\n // Usage:\n-// To build: asm(\"@%3 add.s32 %0, %1, %2;\" : \"=r\"(i) : \"r\"(j), \"r\"(k), \"b\"(p));\n+// To build: @$3 asm(\"@%3 add.s32 %0, %1, %2;\" : \"=r\"(i) : \"r\"(j), \"r\"(k),\n+// \"b\"(p));\n //\n // PTXBuilder builder;\n // auto& add = builder.create<>();\n@@ -35,7 +35,7 @@ class PTXInstrCommon;\n // auto* iOpr = builder.newOperand(iVal, \"r\"); // %1 bind to iVal\n // auto* jOpr = builder.newOperand(jVal, \"r\"); // %2 bind to jVal\n // auto* kOpr = builder.newOperand(kVal, \"r\"); // %3 bind to kVal\n-// add(iOpr, jOpr, kOpr); // set operands\n+// add(iOpr, jOpr, kOpr).predicate(predVal);   // set operands and predicate\n //\n // To get the asm code:\n // builder.dump()\n@@ -44,17 +44,26 @@ class PTXInstrCommon;\n //\n // builder.getAllMlirArgs() // get {pVal, iVal, jVal, kVal}\n //\n-// To get the string containing all the contraints with \",\" seperated,\n-// builder.getConstrains() // get \"=r,r,k\"\n+// To get the string containing all the constraints with \",\" separated,\n+// builder.getConstraints() // get \"=r,r,k\"\n //\n // PTXBuilder can build a PTX asm with multiple instructions, sample code:\n //\n // PTXBuilder builder;\n-// auto& instr0 = builder.create<>();\n-// auto& instr1 = builder.create<>();\n-// auto& instr2 = builder.create<>();\n+// auto& mov = builder.create(\"mov\");\n+// auto& cp = builder.create(\"cp\");\n+// mov(...);\n+// cp(...);\n+// This will get a PTX code with two instructions.\n //\n-// NOTE, the instructions will be serialized in the order of creation.\n+// Similar to a C function, a declared PTXInstr instance can be launched\n+// multiple times with different operands, e.g.\n+//\n+//   auto& mov = builder.create(\"mov\");\n+//   mov(... some operands ...);\n+//   mov(... some different operands ...);\n+//\n+// Finally, we will get a PTX code with two mov instructions.\n //\n // There are several derived instruction type for typical instructions, for\n // example, the PtxIOInstr for ld and st instructions.\n@@ -68,6 +77,7 @@ struct PTXBuilder {\n \n     // for list\n     Operand() = default;\n+    Operand(const Operation &) = delete;\n     Operand(Value value, StringRef constraint)\n         : value(value), constraint(constraint) {}\n \n@@ -97,10 +107,10 @@ struct PTXBuilder {\n   // Create a new operand. It will not add to operand list.\n   // @value: the MLIR value bind to this operand.\n   // @constraint: ASM operand constraint, .e.g. \"=r\"\n-  // @formater: extra format to represent this operand in ASM code, default is\n-  //            \"%{0}\".format(operand.idx).\n+  // @formatter: extra format to represent this operand in ASM code, default is\n+  //             \"%{0}\".format(operand.idx).\n   Operand *newOperand(mlir::Value value, StringRef constraint,\n-                      std::function<std::string(int idx)> formater = nullptr);\n+                      std::function<std::string(int idx)> formatter = nullptr);\n \n   // Create a new operand which is written to, that is, the constraint starts\n   // with \"=\", e.g. \"=r\".\n@@ -113,11 +123,11 @@ struct PTXBuilder {\n \n   Operand *newAddrOperand(mlir::Value addr, StringRef constraint, int off = 0);\n \n-  llvm::SmallVector<Operand *> getAllArgs() const;\n+  llvm::SmallVector<Operand *, 4> getAllArgs() const;\n \n   llvm::SmallVector<Value, 4> getAllMLIRArgs() const;\n \n-  std::string getConstrains() const;\n+  std::string getConstraints() const;\n \n   std::string dump() const;\n \n@@ -128,10 +138,12 @@ struct PTXBuilder {\n   }\n \n   friend class PTXInstr;\n+  friend class PTXInstrCommon;\n \n protected:\n   llvm::SmallVector<std::unique_ptr<Operand>, 6> argArchive;\n   llvm::SmallVector<std::unique_ptr<PTXInstrCommon>, 2> instrs;\n+  llvm::SmallVector<std::unique_ptr<PTXInstrExecution>, 4> executions;\n   int oprCounter{};\n };\n \n@@ -142,36 +154,26 @@ struct PTXInstrCommon {\n \n   using Operand = PTXBuilder::Operand;\n \n-  llvm::SmallVector<Operand *> getArgList() const;\n-\n-  std::string dump() const;\n-\n   // clang-format off\n-  void operator()(Operand* a) { operator()({a}); }\n-  void operator()(Operand* a, Operand* b) { operator()({a, b}); }\n-  void operator()(Operand* a, Operand* b, Operand* c) { operator()({a, b, c}); }\n-  void operator()(Operand* a, Operand* b, Operand* c, Operand* d) { operator()({a, b, c, d}); }\n-  void operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e) { operator()({a, b, c, d, e}); }\n-  void operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e, Operand* f) { operator()({a, b, c, d, e, f}); }\n-  void operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e, Operand* f, Operand* g) { operator()({a, b, c, d, e, f, g}); }\n+  PTXInstrExecution& operator()(Operand* a) { return call({a}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b) { return call({a, b}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c) { return call({a, b, c}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d) { return call({a, b, c, d}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e) { return call({a, b, c, d, e}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e, Operand* f) { return call({a, b, c, d, e, f}); }\n+  PTXInstrExecution& operator()(Operand* a, Operand* b, Operand* c, Operand* d, Operand * e, Operand* f, Operand* g) { return call({a, b, c, d, e, f, g}); }\n   // clang-format on\n \n   // Set operands of this instruction.\n-  void operator()(llvm::ArrayRef<Operand *> oprs);\n+  PTXInstrExecution &operator()(llvm::ArrayRef<Operand *> oprs);\n \n protected:\n-  // Append the operand to the instruction's operand list.\n-  Operand *addOperand(Operand *opr) {\n-    assert(std::find(argsInOrder.begin(), argsInOrder.end(), opr) ==\n-           argsInOrder.end());\n-    argsInOrder.push_back(opr);\n-    return opr;\n-  }\n+  PTXInstrExecution &call(llvm::ArrayRef<Operand *> oprs);\n \n   PTXBuilder *builder{};\n-  Operand *pred{};\n   llvm::SmallVector<std::string, 4> instrParts;\n-  llvm::SmallVector<Operand *> argsInOrder;\n+\n+  friend class PTXInstrExecution;\n };\n \n template <class ConcreteT> struct PTXInstrBase : public PTXInstrCommon {\n@@ -192,19 +194,6 @@ template <class ConcreteT> struct PTXInstrBase : public PTXInstrCommon {\n       instrParts.push_back(suffix);\n     return *static_cast<ConcreteT *>(this);\n   }\n-\n-  // Prefix a predicate to the instruction.\n-  ConcreteT &predicate(mlir::Value value, StringRef constraint) {\n-    pred = builder->newOperand(value, constraint);\n-    return *static_cast<ConcreteT *>(this);\n-  }\n-\n-  // Prefix a !predicate to the instruction.\n-  ConcreteT &predicateNot(mlir::Value value, StringRef constraint) {\n-    pred = builder->newOperand(value, constraint);\n-    pred->repr = [](int idx) { return llvm::formatv(\"@!%{0}\", idx); };\n-    return *static_cast<ConcreteT *>(this);\n-  }\n };\n \n struct PTXInstr : public PTXInstrBase<PTXInstr> {\n@@ -228,16 +217,48 @@ struct PtxIOInstr : public PTXInstrBase<PtxIOInstr> {\n   // Add \".v\" suffix to instruction\n   PtxIOInstr &v(int vecWidth, bool predicate = true) {\n     if (vecWidth > 1) {\n-      o(llvm::formatv(\"v{0}\", vecWidth), predicate);\n+      o(\"v\" + std::to_string(vecWidth), predicate);\n     }\n     return *this;\n   }\n \n   // Add \".b\" suffix to instruction\n   PtxIOInstr &b(int width) {\n-    o(llvm::formatv(\"b{0}\", width));\n+    o(\"b\" + std::to_string(width));\n+    return *this;\n+  }\n+};\n+\n+// Record the operands and context for \"launching\" a PtxInstr.\n+struct PTXInstrExecution {\n+  using Operand = PTXBuilder::Operand;\n+\n+  llvm::SmallVector<Operand *> argsInOrder;\n+\n+  PTXInstrExecution() = default;\n+  explicit PTXInstrExecution(PTXInstrCommon *instr,\n+                             llvm::ArrayRef<Operand *> oprs)\n+      : instr(instr), argsInOrder(oprs.begin(), oprs.end()) {}\n+\n+  // Prefix a predicate to the instruction.\n+  PTXInstrExecution &predicate(mlir::Value value, StringRef constraint = \"b\") {\n+    pred = instr->builder->newOperand(value, constraint);\n     return *this;\n   }\n+\n+  // Prefix a !predicate to the instruction.\n+  PTXInstrExecution &predicateNot(mlir::Value value, StringRef constraint) {\n+    pred = instr->builder->newOperand(value, constraint);\n+    pred->repr = [](int idx) { return \"@!%\" + std::to_string(idx); };\n+    return *this;\n+  }\n+\n+  std::string dump() const;\n+\n+  SmallVector<Operand *> getArgList() const;\n+\n+  PTXInstrCommon *instr{};\n+  Operand *pred{};\n };\n \n } // namespace triton"}, {"filename": "include/triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -18,6 +18,14 @@ class TritonLLVMConversionTarget : public ConversionTarget {\n                                       mlir::LLVMTypeConverter &typeConverter);\n };\n \n+class TritonLLVMFunctionConversionTarget : public ConversionTarget {\n+  mlir::LLVMTypeConverter &typeConverter;\n+\n+public:\n+  explicit TritonLLVMFunctionConversionTarget(\n+      MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter);\n+};\n+\n namespace triton {\n \n // Names for identifying different NVVM annotations. It is used as attribute"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 19, "deletions": 13, "changes": 32, "file_content_changes": "@@ -29,22 +29,26 @@ def TT_IntToPtrOp : TT_Op<\"int_to_ptr\", [SameOperandsAndResultShape, NoSideEffec\n                                          /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast int64 to pointer\";\n \n-    let arguments = (ins I64Tensor:$from);\n+    let arguments = (ins TT_I64Like:$from);\n \n-    let results = (outs TT_PtrTensor:$result);\n+    let results = (outs TT_PtrLike:$result);\n+\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n }\n \n def TT_PtrToIntOp : TT_Op<\"ptr_to_int\", [SameOperandsAndResultShape, NoSideEffect,\n-                                      /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+                                         /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Cast pointer to int64\";\n \n-    let arguments = (ins TT_PtrTensor:$from);\n+    let arguments = (ins TT_PtrLike:$from);\n \n-    let results = (outs I64Tensor:$result);\n+    let results = (outs TT_I64Like:$result);\n+\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n }\n \n def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape, NoSideEffect,\n-                                /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n+                                   /*DeclareOpInterfaceMethods<CastOpInterface>*/]> {\n     let summary = \"Floating point casting for custom types\";\n \n     let description = [{\n@@ -54,9 +58,11 @@ def TT_FpToFp : TT_Op<\"fp_to_fp\", [SameOperandsAndResultShape, NoSideEffect,\n         BF8 <-> F8, FP16, FP32\n     }];\n \n-    let arguments = (ins TT_FloatTensor:$from);\n+    let arguments = (ins TT_FloatLike:$from);\n+\n+    let results = (outs TT_FloatLike:$result);\n \n-    let results = (outs TT_FloatTensor:$result);\n+    let assemblyFormat = \"$from attr-dict `:` type($from) `->` type($result)\";\n \n     // TODO: We need a verifier here.\n }\n@@ -127,16 +133,16 @@ def TT_StoreOp : TT_Op<\"store\",\n     let hasCanonicalizer = 1;\n }\n \n-def TT_GEPOp : TT_Op<\"getelementptr\",\n+def TT_AddPtrOp : TT_Op<\"addptr\",\n                      [NoSideEffect, SameOperandsAndResultShape,\n                       TypesMatchWith<\"result type matches ptr type\",\n                                      \"result\", \"ptr\", \"$_self\">,\n                       TypesMatchWith<\"result shape matches offset shape\",\n                                      \"result\", \"offset\",\n                                      \"getI32SameShape($_self)\">]> {\n-    let arguments = (ins TT_PtrTensor:$ptr, I32Tensor:$offset);\n+    let arguments = (ins TT_PtrLike:$ptr, TT_I32Like:$offset);\n \n-    let results = (outs TT_PtrTensor:$result);\n+    let results = (outs TT_PtrLike:$result);\n \n     let assemblyFormat = \"$ptr `,` $offset attr-dict `:` type($result)\";\n }\n@@ -278,7 +284,7 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\"> {\n         return $old\n     }];\n \n-    let arguments = (ins TT_Pointer:$ptr, TT_Type:$cmp, TT_Type:$val);\n+    let arguments = (ins TT_Ptr:$ptr, TT_Type:$cmp, TT_Type:$val);\n \n     let results = (outs TT_Type:$result);\n }\n@@ -318,7 +324,7 @@ def TT_MakeRangeOp : TT_Op<\"make_range\", [NoSideEffect]> {\n \n     let arguments = (ins I32Attr:$start, I32Attr:$end);\n \n-    let results = (outs TT_IntegerTensor:$result);\n+    let results = (outs TT_IntTensor:$result);\n \n     let assemblyFormat = \"attr-dict `:` type($result)\";\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 26, "deletions": 8, "changes": 34, "file_content_changes": "@@ -12,18 +12,36 @@ class TritonTypeDef<string name, string _mnemonic>\n     let mnemonic = _mnemonic;\n }\n \n+// Floating-point Type\n def F8 : TritonTypeDef<\"Float8\", \"f8\">;\n def BF8 : TritonTypeDef<\"BFloat8\", \"bf8\">;\n \n def TT_Float : AnyTypeOf<[F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n+def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n \n-// IntegerType\n+// Boolean Type\n+// TT_Bool -> I1\n+def TT_BoolTensor : TensorOf<[I1]>;\n+def TT_BoolLike : AnyTypeOf<[I1, TT_BoolTensor]>;\n+\n+// Integer Type\n def TT_Int : AnyTypeOf<[I1, I8, I16, I32, I64], \"integer\">;\n-def TT_IntegerTensor : TensorOf<[TT_Int]>;\n+def TT_IntTensor : TensorOf<[TT_Int]>;\n+def TT_IntLike : AnyTypeOf<[TT_Int, TT_IntTensor]>;\n+\n+// I32 Type\n+// TT_I32 -> I32\n+// TT_I32Tensor -> I32Tensor\n+def TT_I32Like: AnyTypeOf<[I32, I32Tensor]>;\n+\n+// I64 Type\n+// TT_I64 -> I64\n+// TT_I64Tensor -> I64Tensor\n+def TT_I64Like: AnyTypeOf<[I64, I64Tensor]>;\n \n-// PointerType\n-def TT_Pointer : TritonTypeDef<\"Pointer\", \"ptr\"> {\n+// Pointer Type\n+def TT_Ptr : TritonTypeDef<\"Pointer\", \"ptr\"> {\n     let summary = \"pointer type\";\n \n     let description = [{\n@@ -43,12 +61,12 @@ def TT_Pointer : TritonTypeDef<\"Pointer\", \"ptr\"> {\n \n     let skipDefaultBuilders = 1;\n }\n-def TT_PtrTensor : TensorOf<[TT_Pointer]>;\n+def TT_PtrTensor : TensorOf<[TT_Ptr]>;\n+def TT_PtrLike : AnyTypeOf<[TT_Ptr, TT_PtrTensor]>;\n \n-def TT_FpIntTensor : AnyTypeOf<[TT_FloatTensor, TT_IntegerTensor]>;\n+def TT_FpIntTensor : AnyTypeOf<[TT_FloatTensor, TT_IntTensor]>;\n def TT_Tensor : AnyTypeOf<[TT_FpIntTensor, TT_PtrTensor]>;\n \n-def TT_Type : AnyTypeOf<[TT_Float, TT_FloatTensor, TT_Int, TT_IntegerTensor, \n-                         TT_Pointer, TT_PtrTensor]>;\n+def TT_Type : AnyTypeOf<[TT_FloatLike, TT_IntLike, TT_PtrLike]>;\n \n #endif"}, {"filename": "include/triton/Dialect/Triton/Transforms/Passes.td", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -8,7 +8,7 @@ def TritonCombineOps : Pass</*cli-arg*/\"triton-combine\", /*Op*/\"mlir::ModuleOp\">\n   let description = [{\n     dot(a, b, 0) + c => dot(a, b, c)\n \n-    gep(gep(ptr, idx0), idx1) => gep(ptr, AddI(idx0, idx1))\n+    addptr(addptr(ptr, idx0), idx1) => addptr(ptr, AddI(idx0, idx1))\n \n     select(cond, load(ptrs, broadcast(cond), ???), other) =>\n         load(ptrs, broadcast(cond), other)"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -16,4 +16,16 @@\n #define GET_OP_CLASSES\n #include \"triton/Dialect/TritonGPU/IR/Ops.h.inc\"\n \n+namespace mlir {\n+namespace triton {\n+namespace gpu {\n+\n+unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape);\n+\n+unsigned getShapePerCTA(const Attribute &layout, unsigned d);\n+\n+} // namespace gpu\n+} // namespace triton\n+} // namespace mlir\n+\n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "file_content_changes": "@@ -31,6 +31,10 @@ Then, attaching $\\mathcal{L} to a tensor $T$ would mean that:\n \n Right now, Triton implements two classes of layouts: shared, and distributed.\n   }];\n+\n+  code extraBaseClassDeclaration = [{\n+    unsigned getElemsPerThread(ArrayRef<int64_t> shape) const;\n+  }];\n }\n \n //===----------------------------------------------------------------------===//\n@@ -64,6 +68,8 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n     \"unsigned\":$vec, \"unsigned\":$perPhase, \"unsigned\":$maxPhase,\n     ArrayRefParameter<\"unsigned\", \"order of axes by the rate of changing\">:$order\n   );\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration;\n }\n \n //===----------------------------------------------------------------------===//\n@@ -93,6 +99,8 @@ Then the data of A would be distributed as follow between the 16 CUDA threads:\n L(A) = [ {0,8} , {1,9} , {2,10}, {3,11}, {0,8} , {1, 9} , {2, 10}, {3, 11},\n          {4,12}, {5,13}, {6,14}, {7,15}, {4,12}, {5, 13}, {6, 14}, {7, 15} ]\n   }];\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration;\n }\n \n //===----------------------------------------------------------------------===//\n@@ -171,11 +179,10 @@ for\n     }]>\n   ];\n \n-  let extraClassDeclaration = [{\n+  let extraClassDeclaration = extraBaseClassDeclaration # [{\n     SliceEncodingAttr squeeze(int axis);\n   }];\n \n-\n   let parameters = (\n     ins\n     ArrayRefParameter<\"unsigned\">:$sizePerThread,\n@@ -282,6 +289,8 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     \"unsigned\":$version,\n     ArrayRefParameter<\"unsigned\">:$warpsPerCTA\n   );\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration;\n }\n \n def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n@@ -311,6 +320,8 @@ def SliceEncodingAttr : DistributedEncoding<\"SliceEncoding\"> {\n     // TODO: constraint here to only take distributed encodings\n     \"Attribute\":$parent\n   );\n+\n+  let extraClassDeclaration = extraBaseClassDeclaration;\n }\n \n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 18, "deletions": 55, "changes": 73, "file_content_changes": "@@ -10,12 +10,6 @@ include \"mlir/IR/OpBase.td\"\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // NoSideEffect\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n \n-def TT_BoolTensor : TensorOf<[I1]>;\n-\n-def TT_BoolLike : AnyTypeOf<[I1, TT_BoolTensor]>;\n-def TT_IntegerLike : AnyTypeOf<[TT_Int, TT_IntegerTensor]>;\n-def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n-\n class TTG_Op<string mnemonic, list<Trait> traits = []> :\n     Op<TritonGPU_Dialect, mnemonic, traits>;\n \n@@ -38,37 +32,6 @@ def TTG_AsyncWaitOp : TTG_Op<\"async_wait\"> {\n   let assemblyFormat = \"attr-dict\";\n }\n \n-def TTG_CopyAsyncOp : TTG_Op<\"copy_async\",\n-                             [MemoryEffects<[MemRead, MemWrite]>,\n-                              SameVariadicOperandSize,\n-                              TypesMatchWith<\"infer mask type from ptr type\",\n-                                             \"ptr\", \"mask\", \"getI1SameShape($_self)\",\n-                                             \"($_op.getOperands().size() <= 1) || std::equal_to<>()\">,\n-                              TypesMatchWith<\"infer other type from ptr type\",\n-                                             \"ptr\", \"other\", \"getPointeeType($_self)\",\n-                                             \"($_op.getOperands().size() <= 2) || std::equal_to<>()\">]> {\n-  let summary = \"copy async\";\n-\n-  let arguments = (ins TT_PtrTensor:$ptr, Optional<I1Tensor>:$mask, Optional<TT_Type>:$other,\n-                       TT_CacheModifierAttr:$cache, TT_EvictionPolicyAttr:$evict,\n-                       BoolAttr:$isVolatile);\n-\n-  let builders = [\n-      OpBuilder<(ins \"Value\":$ptr, \"triton::CacheModifier\":$cache,\n-                     \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile)>,\n-  ];\n-\n-  let results = (outs TT_Tensor:$result);\n-\n-  // let assemblyFormat = \"operands attr-dict `:` type($ptr) `->` type($result)\";\n-  let parser = [{ return parseCopyAsyncOp(parser, result); }];\n-\n-  let printer = [{ return printCopyAsyncOp(p, *this); }];\n-\n-  // result needs to be of shared layout\n-  let verifier = [{ return ::verify(*this); }];\n-}\n-\n // Port Arith_CmpIOp & Arith_CmpFOp to TritonGPU.\n // This is needed because Arith's Cmp ops don't\n // handle encodings\n@@ -79,8 +42,8 @@ def TTG_CmpIOp : TTG_Op<\"cmpi\", [NoSideEffect]> {\n   let description = [{}];\n \n   let arguments = (ins Arith_CmpIPredicateAttr:$predicate,\n-                       TT_IntegerLike:$lhs,\n-                       TT_IntegerLike:$rhs);\n+                       TT_IntLike:$lhs,\n+                       TT_IntLike:$rhs);\n \n   let results = (outs TT_BoolLike:$result);\n }\n@@ -97,7 +60,7 @@ def TTG_CmpFOp : TTG_Op<\"cmpf\"> {\n   let results = (outs TT_BoolLike:$result);\n }\n \n-def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\", \n+def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n                                     [SameVariadicOperandSize,\n                                      MemoryEffects<[MemRead, MemWrite]>,\n                                      TypesMatchWith<\"infer mask type from src type\",\n@@ -110,7 +73,7 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n \n   let description = [{\n       This operation inserts a tensor `$src` into another tensor `$dst` as specified by the operation\u2019s\n-      `$offset` argument and `$axis` attribute.\n+      `$index` argument and `$axis` attribute.\n \n       It returns a copy of `$dst` with the proper slice updated asynchronously with the value of `$src`.\n       This operation is non-blocking, and `$results` will have the updated value after the corresponding async_wait.\n@@ -119,13 +82,13 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n \n       * src: the tensor that is inserted.\n       * dst: the tensor into which the `$src` tensor is inserted.\n-      * offset: the offset of the `$src` tensor at the given `$axis` from which the `$dst` tensor is inserted into\n+      * index: the index of the `$src` tensor at the given `$axis` from which the `$dst` tensor is inserted into\n       * mask: optional tensor-rank number of boolean masks which specify which\n               elements of the `$src` tensor are inserted into the `$dst` tensor.\n       * other: optional tensor-rank number of other tensors which specify what\n               values are inserted into the `$dst` tensor if the corresponding\n               element of the `$mask` tensor is false.\n-      \n+\n       In the future, we may decompose this operation into a sequence of:\n \n       * `async` operation to specify a sequence of asynchronous operations\n@@ -136,24 +99,24 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n \n       ```\n       %1 = triton_gpu.alloc_tensor : tensor<2x32xf32>\n-      %2 = triton_gpu.insert_slice_async %0, %1, %offset { axis = 0 } : tensor<32x!tt.ptr<f32>, #AL> -> tensor<2x32xf32, #A>\n+      %2 = triton_gpu.insert_slice_async %0, %1, %index { axis = 0 } : tensor<32x!tt.ptr<f32>, #AL> -> tensor<2x32xf32, #A>\n       triiton_gpu.async_wait { num = 0 : i32 }\n       ```\n   }];\n \n-  let arguments = (ins TT_PtrTensor:$src, TT_Tensor:$dst, I32:$offset,\n+  let arguments = (ins TT_PtrTensor:$src, TT_Tensor:$dst, I32:$index,\n                        Optional<I1Tensor>:$mask, Optional<TT_Type>:$other,\n                        TT_CacheModifierAttr:$cache, TT_EvictionPolicyAttr:$evict,\n                        BoolAttr:$isVolatile, I32Attr:$axis);\n \n   let builders = [\n-      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$offset,\n+      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$index,\n                      \"triton::CacheModifier\":$cache,\n                      \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile, \"int\":$axis)>,\n-      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$offset, \"Value\":$mask,\n+      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$index, \"Value\":$mask,\n                      \"triton::CacheModifier\":$cache,\n                      \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile, \"int\":$axis)>,\n-      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$offset,\n+      OpBuilder<(ins \"Value\":$src, \"Value\":$dst, \"Value\":$index,\n                      \"Value\":$mask, \"Value\":$other,\n                      \"triton::CacheModifier\":$cache,\n                      \"triton::EvictionPolicy\":$evict, \"bool\":$isVolatile, \"int\":$axis)>,\n@@ -163,7 +126,7 @@ def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n \n   //let assemblyFormat = [{\n   //  $src `,` $dst ``\n-  //  $offset, $mask, $other\n+  //  $index, $mask, $other\n   //  attr-dict `:` type($src) `->` type($dst)\n   //}];\n \n@@ -180,26 +143,26 @@ def TTG_ExtractSliceOp : TTG_Op<\"extract_slice\", [NoSideEffect, InferTypeOpInter\n   let summary = \"extract slice\";\n   let description = [{\n     The \"extract_slice\" operation extracts a `$result` tensor from a `$src` tensor as\n-    specified by the operation's `$offset` and `$axis` arguments.\n+    specified by the operation's `$index` and `$axis` arguments.\n \n     The extract_slice operation supports the following arguments:\n \n     * src: the tensor that is extracted from.\n-    * offset: the offset at the given `$axis` from which the `$src` tensor is extracted\n+    * index: the index at the given `$axis` from which the `$src` tensor is extracted\n \n     Example:\n \n     ```\n     // Rank-reducing extract_slice.\n-    %1 = tensor.extract_slice %0, %offset {axis = 0} : tensor<8x16x4xf32> -> tensor<1x16x4xf32>\n+    %1 = tensor.extract_slice %0, %index {axis = 0} : tensor<8x16x4xf32> -> tensor<1x16x4xf32>\n     ```\n   }];\n \n-  let arguments = (ins TT_Tensor:$src, I32:$offset, I32Attr:$axis);\n+  let arguments = (ins TT_Tensor:$src, I32:$index, I32Attr:$axis);\n \n   let results = (outs TT_Tensor:$result);\n \n-  let assemblyFormat = [{$src `,` $offset attr-dict `:` type($src) `->` type($result)}];\n+  let assemblyFormat = [{$src `,` $index attr-dict `:` type($src) `->` type($result)}];\n \n   let extraClassDeclaration = [{\n     static ::mlir::LogicalResult inferReturnTypes(::mlir::MLIRContext *context,\n@@ -222,7 +185,7 @@ def TTG_AllocTensorOp : TTG_Op<\"alloc_tensor\", [NoSideEffect]> {\n     Note: This op can be repalced to a `bufferization.alloc_tensor` in LLVM 16.\n   }];\n \n-  let assemblyFormat = [{attr-dict `:` type($result)}]; \n+  let assemblyFormat = [{attr-dict `:` type($result)}];\n \n   let results = (outs TT_Tensor:$result);\n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -39,9 +39,6 @@ def TritonGPUCombineOps : Pass<\"tritongpu-combine\", \"mlir::ModuleOp\"> {\n   let summary = \"combine triton gpu ops\";\n \n   let description = [{\n-    convert_layout(load(%ptr, %mask, %other), #SMEM_LAYOUT) => \n-      copy_async(%ptr, %mask, %other), barrier\n-\n     convert_layout(convert_layout(%src, #LAYOUT_0), #LAYOUT_1) =>\n       convert_layout(%src, #LAYOUT_1)\n "}, {"filename": "include/triton/driver/dispatch.h", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -48,8 +48,15 @@ class dispatch {\n     initializer();\n     if (cache == nullptr) {\n       cache = dlsym(lib_h, name);\n-      if (cache == 0)\n+      if (cache == 0) {\n+#ifdef __EXCEPTIONS\n         throw std::runtime_error(\"dlsym unable to load function\");\n+#else\n+        std::cerr << \"Triton: dlsym unable to load function `\" << name << \"`\"\n+                  << std::endl;\n+        std::abort();\n+#endif\n+      }\n     }\n     FunPtrT fptr;\n     *reinterpret_cast<void **>(&fptr) = cache;"}, {"filename": "include/triton/tools/sys/getenv.hpp", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -22,6 +22,7 @@\n #ifndef TDL_TOOLS_SYS_GETENV_HPP\n #define TDL_TOOLS_SYS_GETENV_HPP\n \n+#include <algorithm>\n #include <cstdlib>\n #include <string>\n \n@@ -37,6 +38,14 @@ inline std::string getenv(const char *name) {\n   return result;\n }\n \n+inline bool getBoolEnv(const std::string &env) {\n+  const char *s = std::getenv(env.c_str());\n+  std::string str(s ? s : \"\");\n+  std::transform(str.begin(), str.end(), str.begin(),\n+                 [](unsigned char c) { return std::tolower(c); });\n+  return (str == \"on\" || str == \"true\" || str == \"1\");\n+}\n+\n } // namespace tools\n \n } // namespace triton"}, {"filename": "include/triton/tools/thread_pool.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -35,7 +35,7 @@ class ThreadPool {\n   }\n \n   template <class F, class... Args>\n-  auto enqueue(F &&f, Args &&... args)\n+  auto enqueue(F &&f, Args &&...args)\n       -> std::future<typename std::result_of<F(Args...)>::type> {\n     using return_type = typename std::result_of<F(Args...)>::type;\n "}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 13, "deletions": 14, "changes": 27, "file_content_changes": "@@ -22,25 +22,24 @@ ChangeResult SharedMemoryAliasAnalysis::visitOperation(\n   AliasInfo aliasInfo;\n   bool pessimistic = true;\n   if (maybeSharedAllocationOp(op)) {\n-    // These ops will allocate a new shared memory buffer.\n+    // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n     if (isSharedEncoding(result)) {\n-      aliasInfo.insert(result);\n+      // FIXME(Keren): extract and insert are always alias for now\n+      if (auto extractSliceOp = dyn_cast<triton::gpu::ExtractSliceOp>(op)) {\n+        // extract_slice %src, %index\n+        aliasInfo = AliasInfo(operands[0]->getValue());\n+      } else if (auto insertSliceOp =\n+                     dyn_cast<triton::gpu::InsertSliceAsyncOp>(op)) {\n+        // insert_slice_async %src, %dst, %index\n+        aliasInfo = AliasInfo(operands[1]->getValue());\n+      } else {\n+        aliasInfo.insert(result);\n+      }\n       pessimistic = false;\n-    } else {\n-      llvm::errs() << \"op: \" << op->getName() << \"\\n\";\n     }\n   }\n-  // XXX(Keren): triton ops don't support aliasing yet.\n-  // else if (auto viewOp = dyn_cast<triton::ViewOp>(op) ||\n-  //                         dyn_cast<triton::ExpandDimsOp>(op)) {\n-  //  // These ops will reate a new view of the same shared memory buffer.\n-  //  auto result = op->getResult(0);\n-  //  if (isSharedEncoding(result)) {\n-  //    aliasInfo = AliasInfo(operands[0]->getValue());\n-  //    pessimistic = false;\n-  //  }\n-  //}\n+\n   if (pessimistic) {\n     return markAllPessimisticFixpoint(op->getResults());\n   }"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 81, "deletions": 5, "changes": 86, "file_content_changes": "@@ -8,13 +8,66 @@\n \n #include <algorithm>\n #include <limits>\n+#include <numeric>\n+\n+using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n \n namespace mlir {\n \n //===----------------------------------------------------------------------===//\n // Shared Memory Allocation Analysis\n //===----------------------------------------------------------------------===//\n namespace triton {\n+\n+SmallVector<unsigned>\n+getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n+                             unsigned &outVec) {\n+  auto srcTy = op.src().getType().cast<RankedTensorType>();\n+  auto dstTy = op.result().getType().cast<RankedTensorType>();\n+  Attribute srcLayout = srcTy.getEncoding();\n+  Attribute dstLayout = dstTy.getEncoding();\n+  assert(srcLayout && dstLayout &&\n+         \"Unexpect layout in getScratchConfigForCvtLayout()\");\n+  unsigned rank = dstTy.getRank();\n+  SmallVector<unsigned> paddedRepShape(rank);\n+  // TODO: move to TritonGPUAttrDefs.h.inc\n+  auto getShapePerCTA = [&](const Attribute &layout, unsigned d) -> unsigned {\n+    if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+      return blockedLayout.getSizePerThread()[d] *\n+             blockedLayout.getThreadsPerWarp()[d] *\n+             blockedLayout.getWarpsPerCTA()[d];\n+    } else {\n+      assert(0 && \"Unimplemented usage of getShapePerCTA\");\n+      return 0;\n+    }\n+  };\n+  if (srcLayout.isa<BlockedEncodingAttr>() &&\n+      dstLayout.isa<BlockedEncodingAttr>()) {\n+    auto srcBlockedLayout = srcLayout.cast<BlockedEncodingAttr>();\n+    auto dstBlockedLayout = dstLayout.cast<BlockedEncodingAttr>();\n+    auto inOrd = srcBlockedLayout.getOrder();\n+    auto outOrd = dstBlockedLayout.getOrder();\n+    // TODO: Fix the legacy issue that ourOrd[0] == 0 always means\n+    //       that we cannot do vectorization.\n+    inVec = outOrd[0] == 0  ? 1\n+            : inOrd[0] == 0 ? 1\n+                            : srcBlockedLayout.getSizePerThread()[inOrd[0]];\n+    outVec =\n+        outOrd[0] == 0 ? 1 : dstBlockedLayout.getSizePerThread()[outOrd[0]];\n+    unsigned pad = std::max(inVec, outVec);\n+    for (unsigned d = 0; d < rank; ++d) {\n+      paddedRepShape[d] = std::max(\n+          std::min<unsigned>(srcTy.getShape()[d], getShapePerCTA(srcLayout, d)),\n+          std::min<unsigned>(dstTy.getShape()[d],\n+                             getShapePerCTA(dstLayout, d)));\n+    }\n+    paddedRepShape[outOrd[0]] += pad;\n+  }\n+  return paddedRepShape;\n+}\n+\n class AllocationAnalysis {\n public:\n   AllocationAnalysis(Operation *operation, Allocation *allocation)\n@@ -39,11 +92,13 @@ class AllocationAnalysis {\n \n   /// Initializes explicitly defined shared memory values for a given operation.\n   void getExplicitValueSize(Operation *op) {\n-    /// Values returned from scf.yield will not be allocated even though they\n-    /// have the shared encoding.\n-    /// For example: %a = scf.if -> yield\n-    /// %a must be allocated elsewhere by other operations.\n-    if (!maybeSharedAllocationOp(op)) {\n+    // Values returned from scf.yield will not be allocated even though they\n+    // have the shared encoding.\n+    // For example: %a = scf.if -> yield\n+    // %a must be allocated elsewhere by other operations.\n+    // FIXME(Keren): extract and insert are always alias for now\n+    if (!maybeSharedAllocationOp(op) || isa<triton::gpu::ExtractSliceOp>(op) ||\n+        isa<triton::gpu::InsertSliceAsyncOp>(op)) {\n       return;\n     }\n \n@@ -71,6 +126,27 @@ class AllocationAnalysis {\n                      tensorType.getElementTypeBitWidth() / 8;\n         allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n       }\n+    } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n+      auto srcTy = cvtLayout.src().getType().cast<RankedTensorType>();\n+      auto dstTy = cvtLayout.result().getType().cast<RankedTensorType>();\n+      auto srcEncoding = srcTy.getEncoding();\n+      auto dstEncoding = dstTy.getEncoding();\n+      if (srcEncoding.isa<SharedEncodingAttr>() ||\n+          dstEncoding.isa<SharedEncodingAttr>()) {\n+        // Only blocked -> blocked conversion requires for scratch allocation\n+        return;\n+      }\n+      // ConvertLayoutOp with both input/output non-shared_layout\n+      // TODO: Besides of implementing ConvertLayoutOp via shared memory, it's\n+      //       also possible to realize it with other approaches in restricted\n+      //       conditions, such as warp-shuffle\n+      unsigned inVec = 0;\n+      unsigned outVec = 0;\n+      auto smemShape = getScratchConfigForCvtLayout(cvtLayout, inVec, outVec);\n+      unsigned elems = std::accumulate(smemShape.begin(), smemShape.end(), 1,\n+                                       std::multiplies{});\n+      auto bytes = elems * srcTy.getElementTypeBitWidth() / 8;\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     }\n   }\n "}, {"filename": "lib/Analysis/AxisInfo.cpp", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "file_content_changes": "@@ -1,4 +1,5 @@\n #include \"mlir/Analysis/DataFlowAnalysis.h\"\n+#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"llvm/Support/raw_ostream.h\"\n #include <iostream>\n \n@@ -46,6 +47,11 @@ AxisInfo AxisInfo::getPessimisticValueState(Value value) {\n           fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n       if (attr)\n         divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n+    } else if (auto fun = dyn_cast<LLVM::LLVMFuncOp>(op)) {\n+      Attribute attr =\n+          fun.getArgAttr(blockArg.getArgNumber(), \"tt.divisibility\");\n+      if (attr)\n+        divHint = attr.cast<IntegerAttr>().getValue().getZExtValue();\n     }\n   }\n   DimVectorT contiguity(rank, 1);\n@@ -126,7 +132,7 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     }\n   }\n   // Addition\n-  if (llvm::isa<arith::AddIOp, triton::GEPOp>(op)) {\n+  if (llvm::isa<arith::AddIOp, triton::AddPtrOp>(op)) {\n     auto newContiguity = [&](AxisInfo lhs, AxisInfo rhs, int d) {\n       return std::max(gcd(lhs.getContiguity(d), rhs.getConstancy(d)),\n                       gcd(lhs.getConstancy(d), rhs.getContiguity(d)));\n@@ -203,6 +209,13 @@ ChangeResult AxisInfoAnalysis::visitOperation(\n     }\n     curr = AxisInfo(contiguity, divisibility, constancy);\n   }\n+  // UnrealizedConversionCast\n+  // This is needed by TritonGPUToLLVM, to get AxisInfo when the graph is\n+  // in the process of a PartialConversion, where UnrealizedConversionCast\n+  // may exist\n+  if (llvm::isa<mlir::UnrealizedConversionCastOp>(op)) {\n+    curr = operands[0]->getValue();\n+  }\n   if (curr.getRank() == 0) {\n     return markAllPessimisticFixpoint(op->getResults());\n   }"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "file_content_changes": "@@ -45,19 +45,25 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n   if (op->getNumResults() < 1)\n     return;\n \n-  if (dyn_cast<scf::ForOp>(op) || dyn_cast<scf::IfOp>(op) ||\n-      dyn_cast<scf::YieldOp>(op)) {\n-    // Do not insert barriers before control flow operations.\n+  if (isa<scf::ForOp>(op) || isa<scf::IfOp>(op) || isa<scf::YieldOp>(op) ||\n+      isa<triton::gpu::ExtractSliceOp>(op) ||\n+      isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n+      isa<triton::gpu::AllocTensorOp>(op)) {\n+    // Do not insert barriers before control flow operations and\n+    // alloc/extract/insert\n+    // alloc is an allocation op without memory write.\n+    // In contrast, arith.constant is an allocation op with memory write.\n+    // FIXME(Keren): extract and insert are always alias for now\n     return;\n   }\n \n-  if (dyn_cast<gpu::BarrierOp>(op)) {\n+  if (isa<gpu::BarrierOp>(op)) {\n     // If the current op is a barrier, we sync previous reads and writes\n     regionInfo->sync();\n     return;\n   }\n \n-  if (dyn_cast<triton::gpu::AsyncWaitOp>(op)) {\n+  if (isa<triton::gpu::AsyncWaitOp>(op)) {\n     // If the current op is an async wait, we insert a barrier op and sync\n     // previous reads and writes.\n     OpBuilder::InsertionGuard g(*builder);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,6 +1,6 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n     TritonGPUToLLVM.cpp\n-        PtxAsmFormat.cpp\n+    PtxAsmFormat.cpp\n \n     ADDITIONAL_HEADER_DIRS\n     ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonGPUToLLVM"}, {"filename": "lib/Conversion/TritonGPUToLLVM/PtxAsmFormat.cpp", "status": "modified", "additions": 25, "deletions": 18, "changes": 43, "file_content_changes": "@@ -20,10 +20,10 @@ std::string strJoin(llvm::ArrayRef<std::string> strs,\n \n PTXInstr::Operand *\n PTXBuilder::newOperand(mlir::Value value, StringRef constraint,\n-                       std::function<std::string(int)> formater) {\n+                       std::function<std::string(int)> formatter) {\n   argArchive.emplace_back(std::make_unique<Operand>(value, constraint));\n   auto *opr = argArchive.back().get();\n-  opr->repr = formater;\n+  opr->repr = formatter;\n   opr->idx = oprCounter++;\n   return opr;\n }\n@@ -49,7 +49,7 @@ PTXBuilder::Operand *PTXBuilder::newConstantOperand(int v) {\n   return newConstantOperand(ss.str());\n }\n \n-std::string PTXBuilder::getConstrains() const {\n+std::string PTXBuilder::getConstraints() const {\n   auto args = getAllArgs();\n   llvm::SmallVector<std::string, 4> argReprs;\n   for (auto arg : args)\n@@ -66,7 +66,7 @@ llvm::SmallVector<Value, 4> PTXBuilder::getAllMLIRArgs() const {\n   return res;\n }\n \n-SmallVector<PTXBuilder::Operand *> PTXBuilder::getAllArgs() const {\n+SmallVector<PTXBuilder::Operand *, 4> PTXBuilder::getAllArgs() const {\n   llvm::SmallVector<Operand *, 4> res;\n   for (auto &x : argArchive)\n     if (!x->isList())\n@@ -78,7 +78,7 @@ std::string PTXInstr::Operand::dump() const {\n   if (repr)\n     return repr(idx);\n   if (!isList())\n-    return llvm::formatv(\"${0}\", idx);\n+    return \"$\" + std::to_string(idx);\n \n   llvm::SmallVector<std::string> oprs;\n   for (auto *opr : list)\n@@ -90,22 +90,34 @@ PTXInstr::Operand *PTXBuilder::newAddrOperand(mlir::Value addr,\n                                               StringRef constraint, int off) {\n   auto *opr = newOperand(addr, constraint);\n   opr->repr = [off](int idx) -> std::string {\n-    return llvm::formatv(\"[ ${0} + {1} ]\", idx, off);\n+    std::stringstream ss;\n+    ss << \"[ $\" << idx << \" + \" << off << \" ]\";\n+    return ss.str();\n   };\n \n   return opr;\n }\n \n std::string PTXBuilder::dump() const {\n   llvm::SmallVector<std::string> lines;\n-  for (auto &instr : instrs) {\n-    lines.push_back(instr->dump());\n+  for (auto &exec : executions) {\n+    lines.push_back(exec->dump());\n   }\n \n-  return strJoin(lines, \"\\n\\t\");\n+  return strJoin(lines, \"\\r\\n\");\n }\n \n-std::string PTXInstrCommon::dump() const {\n+PTXInstrExecution &PTXInstrCommon::call(ArrayRef<Operand *> oprs) {\n+  builder->executions.emplace_back(\n+      std::make_unique<PTXInstrExecution>(this, oprs));\n+  return *builder->executions.back();\n+}\n+\n+PTXInstrExecution &PTXInstrCommon::operator()(ArrayRef<Operand *> oprs) {\n+  return call(oprs);\n+}\n+\n+std::string PTXInstrExecution::dump() const {\n   std::string osStr;\n   llvm::raw_string_ostream os(osStr);\n   if (pred)\n@@ -114,7 +126,7 @@ std::string PTXInstrCommon::dump() const {\n     else\n       os << pred->repr(pred->idx);\n \n-  std::string instrRepr = strJoin(instrParts, \".\");\n+  std::string instrRepr = strJoin(instr->instrParts, \".\");\n \n   llvm::SmallVector<std::string, 4> argReprs;\n   for (auto *arg : argsInOrder) {\n@@ -128,7 +140,8 @@ std::string PTXInstrCommon::dump() const {\n   return osStr;\n }\n \n-SmallVector<PTXInstrCommon::Operand *> PTXInstrCommon::getArgList() const {\n+SmallVector<PTXInstrExecution::Operand *>\n+PTXInstrExecution::getArgList() const {\n   SmallVector<Operand *> args;\n   for (auto *arg : argsInOrder) {\n     if (arg->isList())\n@@ -138,11 +151,5 @@ SmallVector<PTXInstrCommon::Operand *> PTXInstrCommon::getArgList() const {\n   }\n   return args;\n }\n-\n-void PTXInstrCommon::operator()(ArrayRef<Operand *> oprs) {\n-  for (auto *opr : oprs) {\n-    addOperand(opr);\n-  }\n-}\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 508, "deletions": 129, "changes": 637, "file_content_changes": "@@ -13,6 +13,7 @@\n #include \"mlir/Transforms/DialectConversion.h\"\n #include \"triton/Analysis/Allocation.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n #include \"triton/Conversion/TritonToTritonGPU/TritonToTritonGPU.h\"\n@@ -27,8 +28,11 @@\n using namespace mlir;\n using namespace mlir::triton;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n+using ::mlir::triton::gpu::SliceEncodingAttr;\n \n namespace mlir {\n namespace LLVM {\n@@ -68,11 +72,6 @@ namespace type = mlir::triton::type;\n \n class TritonGPUToLLVMTypeConverter;\n \n-// TODO(Superjomn) Move to somewhere general utilities locates.\n-template <typename Int> size_t product(llvm::ArrayRef<Int> arr) {\n-  return std::accumulate(arr.begin(), arr.end(), 1, std::multiplies{});\n-}\n-\n // FuncOpConversion/FuncOpConversionBase is borrowed from\n // https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L276\n // since it is not exposed on header files in mlir v14\n@@ -239,36 +238,6 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<::mlir::ReturnOp> {\n   }\n };\n \n-static int64_t getLinearIndex(std::vector<int64_t> multidim_index,\n-                              ArrayRef<int64_t> shape) {\n-  assert(multidim_index.size() == shape.size());\n-  // sizes {a, b, c, d}  ->  acc_mul {b*c*d, c*d, d, 1}\n-  int64_t rank = shape.size();\n-  int64_t acc_mul = 1;\n-  for (int64_t i = 1; i < rank; ++i) {\n-    acc_mul *= shape[i];\n-  }\n-  int64_t linear_index = 0;\n-  for (int64_t i = 0; i < rank; ++i) {\n-    linear_index += multidim_index[i] * acc_mul;\n-    if (i != (rank - 1)) {\n-      acc_mul = acc_mul / shape[i + 1];\n-    }\n-  }\n-  return linear_index;\n-}\n-\n-static unsigned getElemsPerThread(BlockedEncodingAttr layout,\n-                                  ArrayRef<int64_t> shape) {\n-  size_t rank = shape.size();\n-  SmallVector<unsigned> elemsPerThreadPerDim(rank);\n-  for (size_t i = 0; i < rank; ++i) {\n-    unsigned t = layout.getThreadsPerWarp()[i] * layout.getWarpsPerCTA()[i];\n-    elemsPerThreadPerDim[i] = (shape[i] + t - 1) / t;\n-  }\n-  return product<unsigned>(elemsPerThreadPerDim);\n-}\n-\n static Value createIndexAttrConstant(OpBuilder &builder, Location loc,\n                                      Type resultType, int64_t value) {\n   return builder.create<LLVM::ConstantOp>(\n@@ -334,9 +303,9 @@ static T getLinearIndex(ArrayRef<T> multidim_index, ArrayRef<T> shape) {\n }\n \n struct ConvertTritonGPUOpToLLVMPatternBase {\n-  SmallVector<Value>\n+  static SmallVector<Value>\n   getElementsFromStruct(Location loc, Value llvmStruct, unsigned elems,\n-                        ConversionPatternRewriter &rewriter) const {\n+                        ConversionPatternRewriter &rewriter) {\n     SmallVector<Value> results(elems);\n     for (unsigned i = 0; i < elems; ++i) {\n       Type type =\n@@ -379,7 +348,12 @@ class ConvertTritonGPUOpToLLVMPattern\n     for (unsigned i = 0; i < rank; ++i) {\n       reordered[i] = shape[order[i]];\n     }\n-    return delinearize(rewriter, loc, linear, reordered);\n+    auto reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n+    SmallVector<Value> multiDim(rank);\n+    for (unsigned i = 0; i < rank; ++i) {\n+      multiDim[order[i]] = reorderedMultiDim[i];\n+    }\n+    return multiDim;\n   }\n \n   SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n@@ -404,13 +378,29 @@ class ConvertTritonGPUOpToLLVMPattern\n     return multiDim;\n   }\n \n-  // Emit indices calculation within each ConversionPattern\n-  // TODO: [goostavz] Double confirm the redundant indices calculations will\n-  //       be eliminated in the consequent MLIR/LLVM optimization\n-  SmallVector<SmallVector<Value>>\n-  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &b,\n-                              const BlockedEncodingAttr &blocked_layout,\n-                              ArrayRef<int64_t> shape) const {\n+  Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                  ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n+    int rank = multiDim.size();\n+    Value linear = createIndexAttrConstant(\n+        rewriter, loc, this->getTypeConverter()->getIndexType(), 0);\n+    if (rank > 0) {\n+      linear = multiDim.front();\n+      for (auto &&z : llvm::zip(multiDim.drop_front(), shape.drop_front())) {\n+        Value dimSize = createIndexAttrConstant(\n+            rewriter, loc, this->getTypeConverter()->getIndexType(),\n+            std::get<1>(z));\n+        linear = rewriter.create<LLVM::AddOp>(\n+            loc, rewriter.create<LLVM::MulOp>(loc, linear, dimSize),\n+            std::get<0>(z));\n+      }\n+    }\n+    return linear;\n+  }\n+\n+  SmallVector<Value>\n+  emitBaseIndexForBlockedLayout(Location loc, ConversionPatternRewriter &b,\n+                                const BlockedEncodingAttr &blocked_layout,\n+                                ArrayRef<int64_t> shape) const {\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n     Value threadId = getThreadId(b, loc);\n     Value warpSize = createIndexAttrConstant(b, loc, llvmIndexTy, 32);\n@@ -422,7 +412,6 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto warpsPerCTA = blocked_layout.getWarpsPerCTA();\n     auto order = blocked_layout.getOrder();\n     unsigned rank = shape.size();\n-    SmallVector<Value, 4> threadIds(rank);\n \n     // step 1, delinearize threadId to get the base index\n     SmallVector<Value> multiDimWarpId =\n@@ -431,8 +420,19 @@ class ConvertTritonGPUOpToLLVMPattern\n         delinearize(b, loc, laneId, threadsPerWarp, order);\n     SmallVector<Value> multiDimBase(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      // multiDimBase[k] = (multiDimThreadId[k] + multiDimWarpId[k] *\n-      // threadsPerWarp[k]) *\n+      // Wrap around multiDimWarpId/multiDimThreadId incase\n+      // shape[k] > shapePerCTA[k]\n+      unsigned maxWarps =\n+          ceil<unsigned>(shape[k], sizePerThread[k] * threadsPerWarp[k]);\n+      unsigned maxThreads = ceil<unsigned>(shape[k], sizePerThread[k]);\n+      multiDimWarpId[k] = b.create<LLVM::URemOp>(\n+          loc, multiDimWarpId[k],\n+          createIndexAttrConstant(b, loc, llvmIndexTy, maxWarps));\n+      multiDimThreadId[k] = b.create<LLVM::URemOp>(\n+          loc, multiDimThreadId[k],\n+          createIndexAttrConstant(b, loc, llvmIndexTy, maxThreads));\n+      // multiDimBase[k] = (multiDimThreadId[k] +\n+      //                    multiDimWarpId[k] * threadsPerWarp[k]) *\n       //                   sizePerThread[k];\n       Value threadsPerWarpK =\n           createIndexAttrConstant(b, loc, llvmIndexTy, threadsPerWarp[k]);\n@@ -444,17 +444,100 @@ class ConvertTritonGPUOpToLLVMPattern\n               loc, multiDimThreadId[k],\n               b.create<LLVM::MulOp>(loc, multiDimWarpId[k], threadsPerWarpK)));\n     }\n+    return multiDimBase;\n+  }\n+\n+  SmallVector<SmallVector<Value>> emitIndices(Location loc,\n+                                              ConversionPatternRewriter &b,\n+                                              const Attribute &layout,\n+                                              ArrayRef<int64_t> shape) const {\n+    if (auto blocked = layout.dyn_cast<BlockedEncodingAttr>()) {\n+      return emitIndicesForBlockedLayout(loc, b, blocked, shape);\n+    } else if (auto slice = layout.dyn_cast<SliceEncodingAttr>()) {\n+      return emitIndicesForSliceLayout(loc, b, slice, shape);\n+    } else {\n+      assert(0 && \"emitIndices for layouts other than blocked & slice not \"\n+                  \"implemented yet\");\n+      return {};\n+    }\n+  }\n+\n+  SmallVector<SmallVector<Value>>\n+  emitIndicesForSliceLayout(Location loc, ConversionPatternRewriter &b,\n+                            const SliceEncodingAttr &sliceLayout,\n+                            ArrayRef<int64_t> shape) const {\n+    auto parent = sliceLayout.getParent();\n+    unsigned dim = sliceLayout.getDim();\n+    size_t rank = shape.size();\n+    if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      SmallVector<int64_t> paddedShape(rank + 1);\n+      for (unsigned d = 0; d < rank + 1; ++d) {\n+        if (d < dim) {\n+          paddedShape[d] = shape[d];\n+        } else if (d == dim) {\n+          paddedShape[d] = 1;\n+        } else {\n+          paddedShape[d] = shape[d - 1];\n+        }\n+      }\n+      auto paddedIndices =\n+          emitIndicesForBlockedLayout(loc, b, blockedParent, paddedShape);\n+      unsigned numIndices = paddedIndices.size();\n+      SmallVector<SmallVector<Value>> resultIndices(numIndices);\n+      for (unsigned i = 0; i < numIndices; ++i) {\n+        for (unsigned d = 0; d < rank + 1; ++d) {\n+          if (d != dim) {\n+            resultIndices[i].push_back(paddedIndices[i][d]);\n+          }\n+        }\n+      }\n+      return resultIndices;\n+\n+    } else if (auto sliceParent = parent.dyn_cast<SliceEncodingAttr>()) {\n+      assert(0 && \"emitIndicesForSliceLayout with parent of sliceLayout\"\n+                  \"is not implemented yet\");\n+      return {};\n+\n+    } else {\n+      assert(0 && \"emitIndicesForSliceLayout with parent other than blocked & \"\n+                  \"slice not implemented yet\");\n+      return {};\n+    }\n+  }\n+\n+  // Emit indices calculation within each ConversionPattern\n+  // TODO: [goostavz] Double confirm the redundant indices calculations will\n+  //       be eliminated in the consequent MLIR/LLVM optimization. We might\n+  //       implement a indiceCache if necessary.\n+  SmallVector<SmallVector<Value>>\n+  emitIndicesForBlockedLayout(Location loc, ConversionPatternRewriter &b,\n+                              const BlockedEncodingAttr &blockedLayout,\n+                              ArrayRef<int64_t> shape) const {\n+    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n+    auto sizePerThread = blockedLayout.getSizePerThread();\n+    auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n+    auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n+    unsigned rank = shape.size();\n+    SmallVector<unsigned> shapePerCTA(rank);\n+    for (unsigned k = 0; k < rank; ++k) {\n+      shapePerCTA[k] = sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k];\n+    }\n+\n+    // step 1, delinearize threadId to get the base index\n+    auto multiDimBase =\n+        emitBaseIndexForBlockedLayout(loc, b, blockedLayout, shape);\n \n     // step 2, get offset of each element\n     unsigned elemsPerThread = 1;\n     SmallVector<SmallVector<unsigned>> offset(rank);\n     SmallVector<unsigned> multiDimElemsPerThread(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      multiDimElemsPerThread[k] = shape[k] / threadsPerWarp[k] / warpsPerCTA[k];\n+      multiDimElemsPerThread[k] =\n+          ceil<unsigned>(shape[k], shapePerCTA[k]) * sizePerThread[k];\n       elemsPerThread *= multiDimElemsPerThread[k];\n+      // 1 block in minimum if shape[k] is less than shapePerCTA[k]\n       for (unsigned blockOffset = 0;\n-           blockOffset <\n-           shape[k] / (sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k]);\n+           blockOffset < ceil<unsigned>(shape[k], shapePerCTA[k]);\n            ++blockOffset)\n         for (unsigned warpOffset = 0; warpOffset < warpsPerCTA[k]; ++warpOffset)\n           for (unsigned threadOffset = 0; threadOffset < threadsPerWarp[k];\n@@ -476,7 +559,7 @@ class ConvertTritonGPUOpToLLVMPattern\n                         std::multiplies<unsigned>());\n     SmallVector<unsigned> threadsPerDim(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      threadsPerDim[k] = shape[k] / sizePerThread[k];\n+      threadsPerDim[k] = ceil<unsigned>(shape[k], sizePerThread[k]);\n     }\n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n       unsigned linearNanoTileId = n / accumSizePerThread;\n@@ -500,6 +583,20 @@ class ConvertTritonGPUOpToLLVMPattern\n \n     return multiDimIdx;\n   }\n+\n+  Value getSharedMemoryBase(Location loc, ConversionPatternRewriter &rewriter,\n+                            Value smem, const Allocation *allocation,\n+                            Operation *op) const {\n+    auto ptrTy = LLVM::LLVMPointerType::get(\n+        this->getTypeConverter()->convertType(rewriter.getIntegerType(8)), 3);\n+    auto bufferId = allocation->getBufferId(op);\n+    assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n+    size_t offset = allocation->getOffset(bufferId);\n+    auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n+    Value offVal = createIndexAttrConstant(rewriter, loc, llvmIndexTy, offset);\n+    Value base = rewriter.create<LLVM::GEPOp>(loc, ptrTy, smem, offVal);\n+    return base;\n+  }\n };\n \n // Convert SplatOp or arith::ConstantOp with SplatElementsAttr to a\n@@ -513,19 +610,10 @@ Value convertSplatLikeOp(Type elemType, Type resType, Value constVal,\n                          ConversionPatternRewriter &rewriter, Location loc) {\n \n   auto tensorTy = resType.cast<RankedTensorType>();\n-  auto layout = tensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto layout = tensorTy.getEncoding();\n   auto srcType = typeConverter->convertType(elemType);\n   auto llSrc = rewriter.create<LLVM::BitcastOp>(loc, srcType, constVal);\n-\n-  auto numElems = layout.getSizePerThread();\n-  size_t totalElems =\n-      std::accumulate(tensorTy.getShape().begin(), tensorTy.getShape().end(), 1,\n-                      std::multiplies<>{});\n-  size_t numThreads =\n-      product(layout.getWarpsPerCTA()) * product(layout.getThreadsPerWarp());\n-  // TODO(Superjomn) add numElemsPerThread to the layout encodings.\n-  size_t numElemsPerThread = totalElems / numThreads;\n-\n+  size_t numElemsPerThread = getElemsPerThread(layout, tensorTy.getShape());\n   llvm::SmallVector<Value, 4> elems(numElemsPerThread, llSrc);\n   llvm::SmallVector<Type, 4> elemTypes(elems.size(), srcType);\n   auto structTy =\n@@ -611,7 +699,7 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n     auto shape = ty.getShape();\n     // Here, we assume that all inputs should have a blockedLayout\n \n-    unsigned valueElems = getElemsPerThread(layout, shape);\n+    unsigned valueElems = layout.getElemsPerThread(shape);\n \n     auto llvmElemTy = typeConverter->convertType(ty.getElementType());\n     auto llvmElemPtrPtrTy =\n@@ -626,16 +714,15 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n     auto ty = val.getType().cast<RankedTensorType>();\n     // Here, we assume that all inputs should have a blockedLayout\n     auto layout = ty.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    assert(layout && \"unexpected layout in getLayout\");\n     auto shape = ty.getShape();\n-    unsigned valueElems = getElemsPerThread(layout, shape);\n+    unsigned valueElems = layout.getElemsPerThread(shape);\n     return std::make_tuple(layout, valueElems);\n   }\n \n   unsigned getAlignment(Value val, const BlockedEncodingAttr &layout) const {\n     auto axisInfo = getAxisInfo(val);\n-\n     auto order = layout.getOrder();\n-\n     unsigned maxMultiple = axisInfo->getDivisibility(order[0]);\n     unsigned maxContig = axisInfo->getContiguity(order[0]);\n     unsigned alignment = std::min(maxMultiple, maxContig);\n@@ -645,22 +732,18 @@ struct LoadStoreConversionBase : public ConvertTritonGPUOpToLLVMPatternBase {\n   unsigned getVectorizeSize(Value ptr,\n                             const BlockedEncodingAttr &layout) const {\n     auto axisInfo = getAxisInfo(ptr);\n-    auto contig = axisInfo->getContiguity();\n     // Here order should be ordered by contiguous first, so the first element\n     // should have the largest contiguous.\n     auto order = layout.getOrder();\n     unsigned align = getAlignment(ptr, layout);\n \n-    auto getTensorShape = [](Value val) -> ArrayRef<int64_t> {\n-      auto ty = val.getType().cast<RankedTensorType>();\n-      auto shape = ty.getShape();\n-      return shape;\n-    };\n-\n-    // unsigned contigPerThread = layout.getSizePerThread()[order[0]];\n-    unsigned contigPerThread = getElemsPerThread(layout, getTensorShape(ptr));\n+    auto ty = ptr.getType().dyn_cast<RankedTensorType>();\n+    assert(ty);\n+    auto shape = ty.getShape();\n \n+    unsigned contigPerThread = layout.getSizePerThread()[order[0]];\n     unsigned vec = std::min(align, contigPerThread);\n+    vec = std::min<unsigned>(shape[order[0]], vec);\n \n     return vec;\n   }\n@@ -732,7 +815,7 @@ struct StoreOpConversion\n \n     const int numVecs = numElems / vec;\n     for (size_t vecStart = 0; vecStart < numElems; vecStart += vec) {\n-      // TODO: optimization when ptr is GEP with constant offset\n+      // TODO: optimization when ptr is AddPtr with constant offset\n       size_t in_off = 0;\n \n       const int maxWordWidth = std::max<int>(32, valueElemNbits);\n@@ -787,12 +870,12 @@ struct StoreOpConversion\n           llMask ? maskElems[vecStart]\n                  : createLLVMIntegerConstant(rewriter, loc, getTypeConverter(),\n                                              rewriter.getIntegerType(1), 1);\n-      ptxStoreInstr.predicate(maskVal, \"b\").global().b(width).v(nWords);\n+      ptxStoreInstr.global().b(width).v(nWords);\n \n       auto *asmAddr =\n           ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n \n-      ptxStoreInstr(asmAddr, asmArgList);\n+      ptxStoreInstr(asmAddr, asmArgList).predicate(maskVal, \"b\");\n       Type boolTy = getTypeConverter()->convertType(rewriter.getIntegerType(1));\n       llvm::SmallVector<Type> argTys({boolTy, ptr.getType()});\n       for (int i = 0; i < nWords; i++)\n@@ -803,7 +886,7 @@ struct StoreOpConversion\n       auto inlineAsm = rewriter.create<LLVM::InlineAsmOp>(\n           loc, ASMReturnTy, ptxBuilder.getAllMLIRArgs(), // operands\n           ptxBuilder.dump(),                             // asm_string\n-          ptxBuilder.getConstrains(),                    // constraints\n+          ptxBuilder.getConstraints(),                   // constraints\n           // TODO(Superjomn) determine the side effect.\n           true,  // has_side_effects\n           false, // is_align_stack\n@@ -850,44 +933,60 @@ struct BroadcastOpConversion\n     auto srcShape = srcTy.getShape();\n     auto resultShape = resultTy.getShape();\n     unsigned rank = srcTy.getRank();\n-    // TODO: [goostavz] double confirm the op semantics with Phil\n     assert(rank == resultTy.getRank());\n \n     SmallVector<int64_t, 4> srcLogicalShape(2 * rank);\n     SmallVector<int64_t, 4> resultLogicalShape(2 * rank);\n     SmallVector<unsigned, 2> broadcastDims;\n-    SmallVector<int64_t, 2> broadcastSizes;\n-    int64_t duplicates = 1;\n     for (unsigned d = 0; d < rank; ++d) {\n-      int64_t numCtas = resultShape[d] / (resultLayout.getSizePerThread()[d] *\n-                                          resultLayout.getThreadsPerWarp()[d] *\n-                                          resultLayout.getWarpsPerCTA()[d]);\n+      unsigned resultShapePerCTA = resultLayout.getSizePerThread()[d] *\n+                                   resultLayout.getThreadsPerWarp()[d] *\n+                                   resultLayout.getWarpsPerCTA()[d];\n+      int64_t numCtas = ceil<unsigned>(resultShape[d], resultShapePerCTA);\n       if (srcShape[d] != resultShape[d]) {\n         assert(srcShape[d] == 1);\n         broadcastDims.push_back(d);\n-        broadcastSizes.push_back(resultShape[d]);\n         srcLogicalShape[d] = 1;\n-        srcLogicalShape[d + rank] = 1;\n-        duplicates *= resultShape[d];\n+        srcLogicalShape[d + rank] =\n+            std::max(unsigned(1), srcLayout.getSizePerThread()[d]);\n       } else {\n         srcLogicalShape[d] = numCtas;\n         srcLogicalShape[d + rank] = resultLayout.getSizePerThread()[d];\n       }\n       resultLogicalShape[d] = numCtas;\n       resultLogicalShape[d + rank] = resultLayout.getSizePerThread()[d];\n     }\n-    unsigned srcElems = getElemsPerThread(srcLayout, srcShape);\n+    int64_t duplicates = 1;\n+    SmallVector<int64_t, 2> broadcastSizes(broadcastDims.size() * 2);\n+    for (auto it : llvm::enumerate(broadcastDims)) {\n+      // Incase there are multiple indices in the src that is actually\n+      // calculating the same element, srcLogicalShape may not need to be 1.\n+      // Such as the case when src of shape [256, 1], and with a blocked layout:\n+      // sizePerThread: [1, 4];  threadsPerWarp: [1, 32]; warpsPerCTA: [1, 2]\n+      int64_t d = resultLogicalShape[it.value()] / srcLogicalShape[it.value()];\n+      broadcastSizes[it.index()] = d;\n+      duplicates *= d;\n+      d = resultLogicalShape[it.value() + rank] /\n+          srcLogicalShape[it.value() + rank];\n+      broadcastSizes[it.index() + broadcastDims.size()] = d;\n+      duplicates *= d;\n+    }\n+\n+    unsigned srcElems = srcLayout.getElemsPerThread(srcShape);\n     auto elemTy = resultTy.getElementType();\n     auto srcVals = getElementsFromStruct(loc, src, srcElems, rewriter);\n-    unsigned resultElems = getElemsPerThread(resultLayout, resultShape);\n+    unsigned resultElems = resultLayout.getElemsPerThread(resultShape);\n     SmallVector<Value> resultVals(resultElems);\n     for (unsigned i = 0; i < srcElems; ++i) {\n       auto srcMultiDim = getMultiDimIndex<int64_t>(i, srcLogicalShape);\n-      auto resultMultiDim = srcMultiDim;\n       for (int64_t j = 0; j < duplicates; ++j) {\n+        auto resultMultiDim = srcMultiDim;\n         auto bcastMultiDim = getMultiDimIndex<int64_t>(j, broadcastSizes);\n         for (auto bcastDim : llvm::enumerate(broadcastDims)) {\n-          resultMultiDim[bcastDim.value()] = bcastMultiDim[bcastDim.index()];\n+          resultMultiDim[bcastDim.value()] += bcastMultiDim[bcastDim.index()];\n+          resultMultiDim[bcastDim.value() + rank] +=\n+              bcastMultiDim[bcastDim.index() + broadcastDims.size()] *\n+              srcLogicalShape[bcastDim.index() + broadcastDims.size()];\n         }\n         auto resultLinearIndex =\n             getLinearIndex<int64_t>(resultMultiDim, resultLogicalShape);\n@@ -902,27 +1001,29 @@ struct BroadcastOpConversion\n   }\n };\n \n-struct ViewOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::ViewOp> {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::ViewOp>::ConvertTritonGPUOpToLLVMPattern;\n+template <typename SourceOp>\n+struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n+  using OpAdaptor = typename SourceOp::Adaptor;\n+  explicit ViewLikeOpConversion(LLVMTypeConverter &typeConverter,\n+                                PatternBenefit benefit = 1)\n+      : ConvertTritonGPUOpToLLVMPattern<SourceOp>(typeConverter, benefit) {}\n \n   LogicalResult\n-  matchAndRewrite(triton::ViewOp op, OpAdaptor adaptor,\n+  matchAndRewrite(SourceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // We cannot directly\n     //   rewriter.replaceOp(op, adaptor.src());\n     // due to MLIR's restrictions\n     Location loc = op->getLoc();\n-    auto resultTy = op.getType().cast<RankedTensorType>();\n-    auto resultLayout = resultTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    auto resultTy = op.getType().template cast<RankedTensorType>();\n     auto resultShape = resultTy.getShape();\n-    unsigned elems = getElemsPerThread(resultLayout, resultShape);\n+    unsigned elems = getElemsPerThread(resultTy.getEncoding(), resultShape);\n     Type elemTy =\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n-    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n-    auto vals = getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n+    Type structTy = LLVM::LLVMStructType::getLiteral(this->getContext(), types);\n+    auto vals =\n+        this->getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n     Value view = getStructFromElements(loc, vals, rewriter, structTy);\n     rewriter.replaceOp(op, view);\n     return success();\n@@ -942,12 +1043,12 @@ struct MakeRangeOpConversion\n     Location loc = op->getLoc();\n     auto rankedTy = op.result().getType().dyn_cast<RankedTensorType>();\n     auto shape = rankedTy.getShape();\n-    auto layout = rankedTy.getEncoding().cast<BlockedEncodingAttr>();\n+    auto layout = rankedTy.getEncoding();\n \n     auto elemTy = rankedTy.getElementType();\n     assert(elemTy.isInteger(32));\n     Value start = createIndexAttrConstant(rewriter, loc, elemTy, op.start());\n-    auto idxs = emitIndicesForBlockedLayout(loc, rewriter, layout, shape);\n+    auto idxs = emitIndices(loc, rewriter, layout, shape);\n     unsigned elems = idxs.size();\n     SmallVector<Value> retVals(elems);\n     for (auto multiDim : llvm::enumerate(idxs)) {\n@@ -1076,8 +1177,7 @@ struct LoadOpConversion\n           ptxBuilder.newAddrOperand(ptrElems[vecStart], \"l\", in_off);\n \n       // Define the instruction opcode\n-      ld.predicate(pred, \"b\")\n-          .o(\"violatile\", op.isVolatile())\n+      ld.o(\"volatile\", op.isVolatile())\n           .global()\n           .o(\"ca\", op.cache() == triton::CacheModifier::CA)\n           .o(\"cg\", op.cache() == triton::CacheModifier::CG)\n@@ -1095,15 +1195,15 @@ struct LoadOpConversion\n       //   evictOpr = ptxBuilder.newOperand(l2Evict, \"l\");\n \n       if (!evictOpr)\n-        ld(dstsOpr, addrOpr);\n+        ld(dstsOpr, addrOpr).predicate(pred, \"b\");\n       else\n-        ld(dstsOpr, addrOpr, evictOpr);\n+        ld(dstsOpr, addrOpr, evictOpr).predicate(pred, \"b\");\n \n       SmallVector<Value> others;\n       if (other) {\n         for (size_t ii = 0; ii < nWords; ii++) {\n           PTXInstr &mov = *ptxBuilder.create<>(\"mov\");\n-          mov.predicateNot(pred, \"b\").o(\"u\", width);\n+          mov.o(\"u\", width);\n \n           size_t size = width / valueElemNbits;\n \n@@ -1127,7 +1227,7 @@ struct LoadOpConversion\n             others.push_back(v);\n           }\n \n-          mov(dstsOpr->listGet(ii), opr);\n+          mov(dstsOpr->listGet(ii), opr).predicateNot(pred, \"b\");\n         }\n       }\n \n@@ -1145,7 +1245,7 @@ struct LoadOpConversion\n       auto inlineAsmOp = rewriter.create<LLVM::InlineAsmOp>(\n           loc, retTy, /*operands=*/ptxBuilder.getAllMLIRArgs(),\n           /*asm_string=*/ptxBuilder.dump(),\n-          /*constraints=*/ptxBuilder.getConstrains(),\n+          /*constraints=*/ptxBuilder.getConstraints(),\n           /*has_side_effects=*/true,\n           /*is_align_stack=*/false, /*asm_dialect=*/asmDialectAttr,\n           /*operand_attrs=*/ArrayAttr());\n@@ -1205,18 +1305,20 @@ struct GetProgramIdOpConversion\n   }\n };\n \n-struct GEPOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::GEPOp> {\n+struct AddPtrOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::AddPtrOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n-      triton::GEPOp>::ConvertTritonGPUOpToLLVMPattern;\n+      triton::AddPtrOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(triton::GEPOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::AddPtrOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Location loc = op->getLoc();\n     auto resultTy = op.getType().dyn_cast<RankedTensorType>();\n     auto resultLayout = resultTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n+    assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n     auto resultShape = resultTy.getShape();\n-    unsigned elems = getElemsPerThread(resultLayout, resultShape);\n+    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n     Type elemTy =\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n@@ -1256,7 +1358,8 @@ class BinaryOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n     auto resultLayout =\n         resultTy.getEncoding().template dyn_cast<BlockedEncodingAttr>();\n     auto resultShape = resultTy.getShape();\n-    unsigned elems = getElemsPerThread(resultLayout, resultShape);\n+    assert(resultLayout && \"Unexpected resultLayout in BinaryOpConversion\");\n+    unsigned elems = resultLayout.getElemsPerThread(resultShape);\n     Type elemTy =\n         this->getTypeConverter()->convertType(resultTy.getElementType());\n     SmallVector<Type> types(elems, elemTy);\n@@ -1275,6 +1378,228 @@ class BinaryOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   }\n };\n \n+struct ConvertLayoutOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n+public:\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::gpu::ConvertLayoutOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  ConvertLayoutOpConversion(LLVMTypeConverter &converter,\n+                            const Allocation *allocation, Value smem,\n+                            PatternBenefit benefit)\n+      : ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp>(converter,\n+                                                                      benefit),\n+        allocation_(allocation), smem_(smem) {}\n+\n+  LogicalResult\n+  matchAndRewrite(triton::gpu::ConvertLayoutOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op.getLoc();\n+    Value src = op.src();\n+    Value dst = op.result();\n+    auto srcTy = src.getType().cast<RankedTensorType>();\n+    auto dstTy = dst.getType().cast<RankedTensorType>();\n+    Attribute srcLayout = srcTy.getEncoding();\n+    Attribute dstLayout = dstTy.getEncoding();\n+    if ((!srcLayout.isa<BlockedEncodingAttr>()) ||\n+        (!dstLayout.isa<BlockedEncodingAttr>())) {\n+      // TODO: not implemented\n+      assert(0 &&\n+             \"convert_layout except for blocked -> blocked is not implemented\");\n+      return failure();\n+    }\n+    auto llvmElemTy = getTypeConverter()->convertType(dstTy.getElementType());\n+    Value smemBase = getSharedMemoryBase(loc, rewriter, smem_, allocation_,\n+                                         op.getOperation());\n+    auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+    smemBase = rewriter.create<LLVM::BitcastOp>(loc, elemPtrTy, smemBase);\n+\n+    auto shape = dstTy.getShape();\n+    unsigned rank = dstTy.getRank();\n+    auto getContigPerThread = [&](const Attribute &layout,\n+                                  unsigned d) -> unsigned {\n+      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        return blockedLayout.getSizePerThread()[d];\n+      } else {\n+        assert(0 && \"Unimplemented usage of getContigPerThread\");\n+        return 0;\n+      }\n+    };\n+    auto getAccumElemsPerThread = [&](const Attribute &layout) -> unsigned {\n+      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        return product<unsigned>(blockedLayout.getSizePerThread());\n+      } else {\n+        assert(0 && \"Unimplemented usage of getAccumElemsPerThread\");\n+        return 0;\n+      }\n+    };\n+    auto getOrder = [&](const Attribute &layout) -> ArrayRef<unsigned> {\n+      if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+        return blockedLayout.getOrder();\n+      } else {\n+        assert(0 && \"Unimplemented usage of getAccumElemsPerThread\");\n+        return {};\n+      }\n+    };\n+    SmallVector<unsigned> numReplicates(rank);\n+    SmallVector<unsigned> inNumCTAsEachRep(rank);\n+    SmallVector<unsigned> outNumCTAsEachRep(rank);\n+    SmallVector<unsigned> inNumCTAs(rank);\n+    SmallVector<unsigned> outNumCTAs(rank);\n+    for (unsigned d = 0; d < rank; ++d) {\n+      unsigned inPerCTA =\n+          std::min(unsigned(shape[d]), getShapePerCTA(srcLayout, d));\n+      unsigned outPerCTA =\n+          std::min(unsigned(shape[d]), getShapePerCTA(dstLayout, d));\n+      unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n+      numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n+      inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n+      outNumCTAsEachRep[d] = maxPerCTA / outPerCTA;\n+      // TODO: confirm this\n+      assert(maxPerCTA % inPerCTA == 0 && maxPerCTA % outPerCTA == 0);\n+      inNumCTAs[d] = ceil<unsigned>(shape[d], inPerCTA);\n+      outNumCTAs[d] = ceil<unsigned>(shape[d], outPerCTA);\n+    }\n+    // Potentially we need to store for multiple CTAs in this replication\n+    unsigned accumNumReplicates = product<unsigned>(numReplicates);\n+    unsigned accumInSizePerThread = getAccumElemsPerThread(srcLayout);\n+    unsigned elems = getElemsPerThread(srcLayout, srcTy.getShape());\n+    auto vals = getElementsFromStruct(loc, adaptor.src(), elems, rewriter);\n+    unsigned inVec = 0;\n+    unsigned outVec = 0;\n+    auto paddedRepShape = getScratchConfigForCvtLayout(op, inVec, outVec);\n+\n+    unsigned outElems = getElemsPerThread(dstLayout, shape);\n+    auto outOrd = getOrder(dstLayout);\n+    SmallVector<Value> outVals(outElems);\n+    for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n+      auto multiDimRepId = getMultiDimIndex<unsigned>(repId, numReplicates);\n+      rewriter.create<mlir::gpu::BarrierOp>(loc);\n+      if (auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>()) {\n+        processReplicaBlocked(loc, rewriter, /*stNotRd*/ true, srcTy,\n+                              inNumCTAsEachRep, multiDimRepId, inVec,\n+                              paddedRepShape, outOrd, vals, smemBase);\n+      } else {\n+        assert(0 && \"ConvertLayout with input layout not implemented\");\n+        return failure();\n+      }\n+      rewriter.create<mlir::gpu::BarrierOp>(loc);\n+      if (auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>()) {\n+        processReplicaBlocked(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                              outNumCTAsEachRep, multiDimRepId, outVec,\n+                              paddedRepShape, outOrd, outVals, smemBase);\n+      } else {\n+        assert(0 && \"ConvertLayout with output layout not implemented\");\n+        return failure();\n+      }\n+    }\n+\n+    SmallVector<Type> types(outElems, llvmElemTy);\n+    Type structTy = LLVM::LLVMStructType::getLiteral(getContext(), types);\n+    Value result = getStructFromElements(loc, outVals, rewriter, structTy);\n+    rewriter.replaceOp(op, result);\n+    return success();\n+  }\n+\n+private:\n+  template <typename T>\n+  SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) const {\n+    size_t rank = order.size();\n+    assert(input.size() == rank);\n+    SmallVector<T> result(rank);\n+    for (auto it : llvm::enumerate(order)) {\n+      result[rank - 1 - it.value()] = input[it.index()];\n+    }\n+    return result;\n+  };\n+\n+  void processReplicaBlocked(Location loc, ConversionPatternRewriter &rewriter,\n+                             bool stNotRd, RankedTensorType type,\n+                             ArrayRef<unsigned> numCTAsEachRep,\n+                             ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                             ArrayRef<unsigned> paddedRepShape,\n+                             ArrayRef<unsigned> outOrd,\n+                             SmallVector<Value> &vals, Value smemBase) const {\n+    unsigned accumNumCTAsEachRep = product<unsigned>(numCTAsEachRep);\n+    auto layout = type.getEncoding().cast<BlockedEncodingAttr>();\n+    auto rank = type.getRank();\n+    auto sizePerThread = layout.getSizePerThread();\n+    auto accumSizePerThread = product<unsigned>(sizePerThread);\n+    auto llvmIndexTy = getTypeConverter()->getIndexType();\n+    SmallVector<unsigned> numCTAs(rank);\n+    SmallVector<unsigned> shapePerCTA(rank);\n+    for (unsigned d = 0; d < rank; ++d) {\n+      shapePerCTA[d] = layout.getSizePerThread()[d] *\n+                       layout.getThreadsPerWarp()[d] *\n+                       layout.getWarpsPerCTA()[d];\n+      numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n+    }\n+    auto llvmElemTy = getTypeConverter()->convertType(type.getElementType());\n+    auto multiDimOffsetFirstElem =\n+        emitBaseIndexForBlockedLayout(loc, rewriter, layout, type.getShape());\n+    for (unsigned ctaId = 0; ctaId < accumNumCTAsEachRep; ++ctaId) {\n+      auto multiDimCTAInRepId =\n+          getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep);\n+      SmallVector<unsigned> multiDimCTAId(rank);\n+      for (auto it : llvm::enumerate(multiDimCTAInRepId)) {\n+        auto d = it.index();\n+        multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n+      }\n+\n+      unsigned linearCTAId = getLinearIndex<unsigned>(multiDimCTAId, numCTAs);\n+      // TODO: This is actually redundant index calculation, we should\n+      //       consider of caching the index calculation result in case\n+      //       of performance issue observed.\n+      // for (unsigned elemId = linearCTAId * accumSizePerThread;\n+      //      elemId < (linearCTAId + 1) * accumSizePerThread; elemId += vec) {\n+      for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n+        auto multiDimElemId =\n+            getMultiDimIndex<unsigned>(elemId, layout.getSizePerThread());\n+        SmallVector<Value> multiDimOffset(rank);\n+        for (unsigned d = 0; d < rank; ++d) {\n+          multiDimOffset[d] = rewriter.create<LLVM::AddOp>(\n+              loc, multiDimOffsetFirstElem[d],\n+              createIndexAttrConstant(rewriter, loc, llvmIndexTy,\n+                                      multiDimCTAInRepId[d] * shapePerCTA[d] +\n+                                          multiDimElemId[d]));\n+        }\n+        Value offset =\n+            linearize(rewriter, loc, reorder<Value>(multiDimOffset, outOrd),\n+                      reorder<unsigned>(paddedRepShape, outOrd));\n+        auto elemPtrTy = LLVM::LLVMPointerType::get(llvmElemTy, 3);\n+        Value ptr =\n+            rewriter.create<LLVM::GEPOp>(loc, elemPtrTy, smemBase, offset);\n+        auto vecTy = VectorType::get(vec, llvmElemTy);\n+        ptr = rewriter.create<LLVM::BitcastOp>(\n+            loc, LLVM::LLVMPointerType::get(vecTy, 3), ptr);\n+        if (stNotRd) {\n+          Value valVec = rewriter.create<LLVM::UndefOp>(loc, vecTy);\n+          for (unsigned v = 0; v < vec; ++v) {\n+            Value vVal = createIndexAttrConstant(\n+                rewriter, loc, getTypeConverter()->getIndexType(), v);\n+            valVec = rewriter.create<LLVM::InsertElementOp>(\n+                loc, vecTy, valVec,\n+                vals[elemId + linearCTAId * accumSizePerThread + v], vVal);\n+          }\n+          rewriter.create<LLVM::StoreOp>(loc, valVec, ptr);\n+        } else {\n+          Value valVec = rewriter.create<LLVM::LoadOp>(loc, ptr);\n+          for (unsigned v = 0; v < vec; ++v) {\n+            Value vVal = createIndexAttrConstant(\n+                rewriter, loc, getTypeConverter()->getIndexType(), v);\n+            vals[elemId + linearCTAId * accumSizePerThread + v] =\n+                rewriter.create<LLVM::ExtractElementOp>(loc, llvmElemTy, valVec,\n+                                                        vVal);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  const Allocation *allocation_;\n+  Value smem_;\n+};\n+\n /// ====================== dot codegen begin ==========================\n \n class MMA16816SmemLoader {\n@@ -2077,9 +2402,10 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n \n   llvm::Optional<Type> convertTritonTensorType(RankedTensorType type) {\n     Attribute layout = type.getEncoding();\n-    if (auto blocked_layout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    if (layout && (layout.isa<BlockedEncodingAttr>() ||\n+                   layout.isa<SliceEncodingAttr>())) {\n       unsigned numElementsPerThread =\n-          getElemsPerThread(blocked_layout, type.getShape());\n+          getElemsPerThread(layout, type.getShape());\n       SmallVector<Type, 4> types(numElementsPerThread,\n                                  convertType(type.getElementType()));\n       return LLVM::LLVMStructType::getLiteral(&getContext(), types);\n@@ -2096,7 +2422,8 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n \n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n-                                  AxisInfoAnalysis &analysis,\n+                                  AxisInfoAnalysis &axisInfoAnalysis,\n+                                  const Allocation *allocation, Value smem,\n                                   PatternBenefit benefit = 1) {\n   patterns.add<ArithConstantSplatOpConversion>(typeConverter, benefit);\n   patterns.add<BinaryOpConversion<arith::AddIOp, LLVM::AddOp>>(typeConverter,\n@@ -2107,17 +2434,19 @@ void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                                                benefit);\n   patterns.add<BinaryOpConversion<arith::MulFOp, LLVM::FMulOp>>(typeConverter,\n                                                                 benefit);\n-\n   patterns.add<BroadcastOpConversion>(typeConverter, benefit);\n-  patterns.add<FuncOpConversion>(typeConverter, numWarps, benefit);\n-  patterns.add<GEPOpConversion>(typeConverter, benefit);\n+  patterns.add<AddPtrOpConversion>(typeConverter, benefit);\n+  patterns.add<ConvertLayoutOpConversion>(typeConverter, allocation, smem,\n+                                          benefit);\n   patterns.add<GetProgramIdOpConversion>(typeConverter, benefit);\n-  patterns.add<LoadOpConversion>(typeConverter, analysis, benefit);\n+  patterns.add<LoadOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n   patterns.add<MakeRangeOpConversion>(typeConverter, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n   patterns.add<SplatOpConversion>(typeConverter, benefit);\n-  patterns.add<StoreOpConversion>(typeConverter, analysis, benefit);\n-  patterns.add<ViewOpConversion>(typeConverter, benefit);\n+  patterns.add<StoreOpConversion>(typeConverter, axisInfoAnalysis, benefit);\n+  patterns.add<ViewLikeOpConversion<triton::ViewOp>>(typeConverter, benefit);\n+  patterns.add<ViewLikeOpConversion<triton::ExpandDimsOp>>(typeConverter,\n+                                                           benefit);\n }\n \n class ConvertTritonGPUToLLVM\n@@ -2133,19 +2462,34 @@ class ConvertTritonGPUToLLVM\n     // TODO: need confirm\n     option.overrideIndexBitwidth(32);\n     TritonGPUToLLVMTypeConverter typeConverter(context, option);\n+    TritonLLVMFunctionConversionTarget funcTarget(*context, typeConverter);\n     TritonLLVMConversionTarget target(*context, typeConverter);\n \n-    RewritePatternSet patterns(context);\n-\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n+    // step 1: Convert FuncOp to LLVMFuncOp via partial conversion\n+    // step 2: Allocate for shared memories\n+    // step 3: Convert the rest of ops via partial conversion\n+    // The reason for a seperation between 1/3 is that, step 2 is out of\n+    // the scope of Dialect Conversion, thus we need to make sure the smem_\n+    // is not revised during the conversion of step 3.\n+    RewritePatternSet func_patterns(context);\n+    func_patterns.add<FuncOpConversion>(typeConverter, numWarps, 1 /*benefit*/);\n+    if (failed(\n+            applyPartialConversion(mod, funcTarget, std::move(func_patterns))))\n+      return signalPassFailure();\n+\n+    Allocation allocation(mod);\n     auto axisAnalysis = runAxisAnalysis(mod);\n+    initSharedMemory(allocation.getSharedMemorySize(), typeConverter);\n \n     // We set a higher benefit here to ensure triton's patterns runs before\n     // arith patterns for some encoding not supported by the community\n     // patterns.\n+    RewritePatternSet patterns(context);\n     populateTritonToLLVMPatterns(typeConverter, patterns, numWarps,\n-                                 *axisAnalysis, 10 /*benefit*/);\n+                                 *axisAnalysis, &allocation, smem_,\n+                                 10 /*benefit*/);\n \n     // Add arith/math's patterns to help convert scalar expression to LLVM.\n     mlir::arith::populateArithmeticToLLVMConversionPatterns(typeConverter,\n@@ -2163,10 +2507,35 @@ class ConvertTritonGPUToLLVM\n     auto axisAnalysisPass =\n         std::make_unique<AxisInfoAnalysis>(module->getContext());\n     axisAnalysisPass->run(module);\n+\n     return axisAnalysisPass;\n   }\n+\n+  void initSharedMemory(size_t size,\n+                        TritonGPUToLLVMTypeConverter &typeConverter);\n+\n+  Value smem_;\n };\n \n+void ConvertTritonGPUToLLVM::initSharedMemory(\n+    size_t size, TritonGPUToLLVMTypeConverter &typeConverter) {\n+  ModuleOp mod = getOperation();\n+  OpBuilder b(mod.getBodyRegion());\n+  auto loc = mod.getLoc();\n+  auto elemTy = typeConverter.convertType(b.getIntegerType(8));\n+  auto arrayTy = LLVM::LLVMArrayType::get(elemTy, size);\n+  auto global = b.create<LLVM::GlobalOp>(\n+      loc, arrayTy, /*isConstant=*/false, LLVM::Linkage::Internal,\n+      \"global_smem\", /*value=*/Attribute(),\n+      /*alignment=*/0, mlir::gpu::GPUDialect::getWorkgroupAddressSpace());\n+  SmallVector<LLVM::LLVMFuncOp> funcs;\n+  mod.walk([&](LLVM::LLVMFuncOp func) { funcs.push_back(func); });\n+  assert(funcs.size() == 1 &&\n+         \"Inliner pass is expected before TritonGPUToLLVM\");\n+  b.setInsertionPointToStart(&funcs[0].getBody().front());\n+  smem_ = b.create<LLVM::AddressOfOp>(loc, global);\n+}\n+\n } // namespace\n \n namespace mlir {\n@@ -2177,10 +2546,20 @@ TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n   addLegalDialect<LLVM::LLVMDialect>();\n   addLegalDialect<NVVM::NVVMDialect>();\n   // addIllegalDialect<triton::TritonDialect>();\n+  // addIllegalDialect<triton::gpu::TritonGPUDialect>();\n   addIllegalDialect<mlir::gpu::GPUDialect>();\n   addLegalOp<mlir::UnrealizedConversionCastOp>();\n }\n \n+TritonLLVMFunctionConversionTarget::TritonLLVMFunctionConversionTarget(\n+    MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter)\n+    : ConversionTarget(ctx), typeConverter(typeConverter) {\n+  addLegalDialect<LLVM::LLVMDialect>();\n+  // addLegalDialect<NVVM::NVVMDialect>();\n+  addIllegalOp<mlir::FuncOp>();\n+  addLegalOp<mlir::UnrealizedConversionCastOp>();\n+}\n+\n namespace triton {\n \n std::unique_ptr<OperationPass<ModuleOp>> createConvertTritonGPUToLLVMPass() {"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPU.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -323,7 +323,7 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n   patterns.add< // TODO: view should have custom pattern that views the layout\n       TritonGenericPattern<triton::ViewOp>,\n       TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n-      TritonGenericPattern<triton::GEPOp>, TritonReducePattern,\n+      TritonGenericPattern<triton::AddPtrOp>, TritonReducePattern,\n       TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n       TritonLoadPattern, TritonStorePattern, TritonExtElemwisePattern>(\n       typeConverter, context);"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -15,15 +15,15 @@ static Type getI1SameShape(Type type) {\n   if (auto tensorType = type.dyn_cast<RankedTensorType>())\n     return RankedTensorType::get(tensorType.getShape(), i1Type,\n                                  tensorType.getEncoding());\n-  return Type();\n+  return i1Type;\n }\n \n static Type getI32SameShape(Type type) {\n   auto i32Type = IntegerType::get(type.getContext(), 32);\n   if (auto tensorType = type.dyn_cast<RankedTensorType>())\n     return RankedTensorType::get(tensorType.getShape(), i32Type,\n                                  tensorType.getEncoding());\n-  return Type();\n+  return i32Type;\n }\n \n static Type getPointerTypeFromTensor(Type type) {"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -194,7 +194,7 @@ class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {\n     patterns.add<CombineDotAddFRevPattern>(context);\n     // %}\n     patterns.add<CombineSelectMaskedLoadPattern>(context);\n-    patterns.add<CombineGEPPattern>(context);\n+    patterns.add<CombineAddPtrPattern>(context);\n     patterns.add<CombineBroadcastConstantPattern>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.td", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -30,12 +30,12 @@ def CombineDotAddFRevPattern : Pat<\n         [(Constraint<CPred<\"isZero($0)\">> $c)]>;\n \n \n-// gep(gep(%ptr, %idx0), %idx1) => gep(%ptr, AddI(%idx0, %idx1))\n+// addptr(addptr(%ptr, %idx0), %idx1) => addptr(%ptr, AddI(%idx0, %idx1))\n //   Note: leave (sub %c0, %c0) canceling to ArithmeticDialect\n //         (ref: ArithmeticCanonicalization.td)\n-def CombineGEPPattern : Pat<\n-        (TT_GEPOp (TT_GEPOp $ptr, $idx0), $idx1),\n-        (TT_GEPOp $ptr, (Arith_AddIOp $idx0, $idx1))>;\n+def CombineAddPtrPattern : Pat<\n+        (TT_AddPtrOp (TT_AddPtrOp $ptr, $idx0), $idx1),\n+        (TT_AddPtrOp $ptr, (Arith_AddIOp $idx0, $idx1))>;\n \n // broadcast(cst) => cst\n def getConstantValue : NativeCodeCall<\"getConstantValue($_builder, $0, $1)\">;"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 85, "deletions": 47, "changes": 132, "file_content_changes": "@@ -39,6 +39,37 @@ static Type getPointeeType(Type type) {\n   return Type();\n }\n \n+namespace gpu {\n+\n+// TODO: Inheritation of layout attributes\n+unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n+  size_t rank = shape.size();\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return blockedLayout.getElemsPerThread(shape);\n+  } else if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n+    return sliceLayout.getElemsPerThread(shape);\n+  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n+    return mmaLayout.getElemsPerThread(shape);\n+  } else if (auto sharedLayout = layout.dyn_cast<SharedEncodingAttr>()) {\n+    return sharedLayout.getElemsPerThread(shape);\n+  } else {\n+    assert(0 && \"getElemsPerThread not implemented\");\n+    return 0;\n+  }\n+}\n+\n+unsigned getShapePerCTA(const Attribute &layout, unsigned d) {\n+  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n+    return blockedLayout.getSizePerThread()[d] *\n+           blockedLayout.getThreadsPerWarp()[d] *\n+           blockedLayout.getWarpsPerCTA()[d];\n+  } else {\n+    assert(0 && \"Unimplemented usage of getShapePerCTA\");\n+    return 0;\n+  }\n+};\n+\n+} // namespace gpu\n } // namespace triton\n } // namespace mlir\n \n@@ -108,6 +139,55 @@ SliceEncodingAttr BlockedEncodingAttr::squeeze(int axis) {\n   return SliceEncodingAttr::get(getContext(), axis, *this);\n }\n \n+unsigned BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  size_t rank = shape.size();\n+  assert(rank == getSizePerThread().size() &&\n+         \"unexpected rank in BlockedEncodingAttr::getElemsPerThread\");\n+  SmallVector<unsigned> elemsPerThreadPerDim(rank);\n+  for (size_t i = 0; i < rank; ++i) {\n+    unsigned t =\n+        getSizePerThread()[i] * getThreadsPerWarp()[i] * getWarpsPerCTA()[i];\n+    elemsPerThreadPerDim[i] =\n+        ceil<unsigned>(shape[i], t) * getSizePerThread()[i];\n+  }\n+  return product<unsigned>(elemsPerThreadPerDim);\n+}\n+\n+unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  size_t rank = shape.size();\n+  auto parent = getParent();\n+  unsigned dim = getDim();\n+  if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n+    assert(rank == blockedParent.getSizePerThread().size() - 1 &&\n+           \"unexpected rank in SliceEncodingAttr::getElemsPerThread\");\n+    SmallVector<int64_t> paddedShape(rank + 1);\n+    for (unsigned d = 0; d < rank + 1; ++d) {\n+      if (d < dim)\n+        paddedShape[d] = shape[d];\n+      else if (d == dim)\n+        paddedShape[d] = 1;\n+      else\n+        paddedShape[d] = shape[d - 1];\n+    }\n+    return blockedParent.getElemsPerThread(paddedShape);\n+  } else {\n+    assert(0 && \"getElemsPerThread not implemented\");\n+    return 0;\n+  }\n+}\n+\n+unsigned MmaEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  // TODO:\n+  assert(0 && \"MmaEncodingAttr::getElemsPerThread not implemented\");\n+  return 0;\n+}\n+\n+unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n+  // TODO:\n+  assert(0 && \"SharedEncodingAttr::getElemsPerThread not implemented\");\n+  return 0;\n+}\n+\n //===----------------------------------------------------------------------===//\n // Blocked Encoding\n //===----------------------------------------------------------------------===//\n@@ -292,44 +372,6 @@ void SharedEncodingAttr::print(AsmPrinter &printer) const {\n           << \"}>\";\n }\n \n-//===----------------------------------------------------------------------===//\n-// CopyAsyncOp\n-//===----------------------------------------------------------------------===//\n-\n-ParseResult parseCopyAsyncOp(OpAsmParser &parser, OperationState &result) {\n-  SmallVector<OpAsmParser::OperandType, 4> allOperands;\n-  Type resultTypes[1], ptrType;\n-  SMLoc allOperandLoc = parser.getCurrentLocation();\n-  if (parser.parseOperandList(allOperands) ||\n-      parser.parseOptionalAttrDict(result.attributes) || parser.parseColon() ||\n-      parser.parseCustomTypeWithFallback(ptrType) || parser.parseArrow() ||\n-      parser.parseCustomTypeWithFallback(resultTypes[0]))\n-    return failure();\n-  result.addTypes(resultTypes);\n-\n-  SmallVector<Type> operandTypes;\n-  operandTypes.push_back(ptrType); // ptr\n-  if (allOperands.size() >= 2)\n-    operandTypes.push_back(triton::getI1SameShape(ptrType)); // mask\n-  if (allOperands.size() >= 3)\n-    operandTypes.push_back(triton::getPointeeType(ptrType)); // other\n-\n-  if (parser.resolveOperands(allOperands, operandTypes, allOperandLoc,\n-                             result.operands))\n-    return failure();\n-  return success();\n-}\n-\n-void printCopyAsyncOp(OpAsmPrinter &printer, CopyAsyncOp copyAsyncOp) {\n-  printer << \" \";\n-  printer << copyAsyncOp.getOperation()->getOperands();\n-  printer.printOptionalAttrDict(copyAsyncOp->getAttrs(), /*elidedAttrs=*/{});\n-  printer << \" : \";\n-  printer.printStrippedAttrOrType(copyAsyncOp.ptr().getType());\n-  printer << \" -> \";\n-  printer.printStrippedAttrOrType(copyAsyncOp.result().getType());\n-}\n-\n //===----------------------------------------------------------------------===//\n // InsertSliceAsyncOp\n //===----------------------------------------------------------------------===//\n@@ -350,7 +392,7 @@ ParseResult parseInsertSliceAsyncOp(OpAsmParser &parser,\n   operandTypes.push_back(srcType); // src\n   operandTypes.push_back(dstType); // dst\n   operandTypes.push_back(\n-      IntegerType::get(parser.getBuilder().getContext(), 32)); // offset\n+      IntegerType::get(parser.getBuilder().getContext(), 32)); // index\n   if (allOperands.size() >= 4)\n     operandTypes.push_back(triton::getI1SameShape(srcType)); // mask\n   if (allOperands.size() >= 5)\n@@ -389,6 +431,8 @@ mlir::LogicalResult ExtractSliceOp::inferReturnTypes(\n   auto axis = attributes.get(\"axis\").cast<IntegerAttr>().getInt();\n   if (axis < 0 || axis > srcShape.size())\n     return failure();\n+  // Since we only extract a slice from a certain index on the axis,\n+  // the dims before the axis can be dropped.\n   auto dstShape = srcShape.drop_front(axis + 1);\n   auto returnType =\n       RankedTensorType::get(dstShape, srcType.getElementType(), encoding);\n@@ -438,16 +482,10 @@ void TritonGPUDialect::initialize() {\n // Verification\n //===----------------------------------------------------------------------===//\n \n-static LogicalResult verify(CopyAsyncOp op) {\n-  if (!isSharedEncoding(op.getResult())) {\n-    return op.emitOpError(\"copy_async should return a shared memory tensor\");\n-  }\n-  return success();\n-}\n-\n static LogicalResult verify(InsertSliceAsyncOp op) {\n   if (!isSharedEncoding(op.getResult())) {\n-    return op.emitOpError(\"copy_async should return a shared memory tensor\");\n+    return op.emitOpError(\n+        \"insert_slice_async should return a shared memory tensor\");\n   }\n   return success();\n }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Coalesce.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -69,7 +69,7 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n     // convert output types\n     SmallVector<Type, 4> newTypes;\n     for (auto t : op->getResultTypes()) {\n-      bool is_async = std::is_same<T, triton::gpu::CopyAsyncOp>::value;\n+      bool is_async = std::is_same<T, triton::gpu::InsertSliceAsyncOp>::value;\n       newTypes.push_back(is_async ? t : convertType(t));\n     }\n     // construct new op with the new encoding\n@@ -106,9 +106,9 @@ struct CoalescePass : public TritonGPUCoalesceBase<CoalescePass> {\n       builder.setInsertionPoint(curr);\n       if (auto load = dyn_cast<triton::LoadOp>(curr))\n         coalesceOp<triton::LoadOp>(axisInfo, curr, load.ptr(), builder);\n-      if (auto load = dyn_cast<triton::gpu::CopyAsyncOp>(curr))\n-        coalesceOp<triton::gpu::CopyAsyncOp>(axisInfo, curr, load.ptr(),\n-                                             builder);\n+      if (auto load = dyn_cast<triton::gpu::InsertSliceAsyncOp>(curr))\n+        coalesceOp<triton::gpu::InsertSliceAsyncOp>(axisInfo, curr, load.src(),\n+                                                    builder);\n       if (auto store = dyn_cast<triton::StoreOp>(curr))\n         coalesceOp<triton::StoreOp>(axisInfo, curr, store.ptr(), builder);\n     });"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -119,7 +119,9 @@ class PullConversionToSource : public mlir::RewritePattern {\n       return mlir::failure();\n \n     auto blacklist = [](Operation *op) {\n-      if (isa<triton::gpu::CopyAsyncOp, triton::LoadOp, triton::StoreOp>(op))\n+      if (isa<triton::gpu::ExtractSliceOp, triton::gpu::AllocTensorOp,\n+              triton::gpu::InsertSliceAsyncOp, triton::LoadOp, triton::StoreOp>(\n+              op))\n         return true;\n       if (isa<scf::YieldOp, scf::ForOp>(op))\n         return true;\n@@ -201,7 +203,7 @@ bool tryLegalizeOp(Operation *op, DenseSet<Value> toPreserve,\n                                  targetType.getEncoding());\n   };\n   bool hasSameTypes = op->getDialect()->getNamespace() == \"arith\" ||\n-                      isa<triton::SplatOp, triton::GEPOp>(op);\n+                      isa<triton::SplatOp, triton::AddPtrOp>(op);\n   if (hasSameTypes) {\n     // replace argument types\n     for (auto arg : llvm::enumerate(op->getOperands())) {\n@@ -438,4 +440,4 @@ class TritonGPUCombineOpsPass\n \n std::unique_ptr<Pass> mlir::createTritonGPUCombineOpsPass() {\n   return std::make_unique<TritonGPUCombineOpsPass>();\n-}\n\\ No newline at end of file\n+}"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 78, "deletions": 43, "changes": 121, "file_content_changes": "@@ -25,7 +25,7 @@ class LoopPipeliner {\n   /// cache forOp we are working on\n   scf::ForOp forOp;\n \n-  /// cahce YieldOp for this forOp\n+  /// cache YieldOp for this forOp\n   scf::YieldOp yieldOp;\n \n   /// loads to be pipelined\n@@ -34,6 +34,14 @@ class LoopPipeliner {\n   DenseMap<Value, Value> loadsMapping;\n   /// load => buffer\n   DenseMap<Value, Value> loadsBuffer;\n+  /// load => buffer at stage N\n+  DenseMap<Value, SmallVector<Value>> loadStageBuffer;\n+  /// load => after extract\n+  DenseMap<Value, Value> loadsExtract;\n+  ///\n+  Value pipelineIterIdx;\n+  ///\n+  Value loopIterIdx;\n \n   /// value (in loop) => value at stage N\n   DenseMap<Value, SmallVector<Value>> valueMapping;\n@@ -237,6 +245,7 @@ void LoopPipeliner::emitPrologue() {\n \n   // prologue from [0, numStage-1)\n   Value iv = forOp.getLowerBound();\n+  pipelineIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (int stage = 0; stage < numStages - 1; ++stage) {\n     // special handling for induction variable as the increment is implicit\n     if (stage != 0)\n@@ -261,22 +270,21 @@ void LoopPipeliner::emitPrologue() {\n       Operation *newOp = nullptr;\n       if (loads.contains(op->getResult(0))) {\n         // Allocate empty buffer\n-        if (stage == 0)\n+        if (stage == 0) {\n           loadsBuffer[op->getResult(0)] = allocateEmptyBuffer(op, builder);\n+          loadStageBuffer[op->getResult(0)] = {loadsBuffer[op->getResult(0)]};\n+        }\n         // load => copy async\n-        // TODO: check if the hardware supports copyasync\n+        // TODO: check if the hardware supports async copy\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n-          Value sliceIndex = builder.create<arith::IndexCastOp>(\n-              iv.getLoc(), builder.getI32Type(), iv);\n-          Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n+          newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n               op->getLoc(), loadsBuffer[loadOp].getType(),\n-              lookupOrDefault(loadOp.ptr(), stage), loadsBuffer[loadOp],\n-              sliceIndex, lookupOrDefault(loadOp.mask(), stage),\n+              lookupOrDefault(loadOp.ptr(), stage),\n+              loadStageBuffer[loadOp][stage], pipelineIterIdx,\n+              lookupOrDefault(loadOp.mask(), stage),\n               lookupOrDefault(loadOp.other(), stage), loadOp.cache(),\n               loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n-          newOp = builder.create<triton::gpu::ExtractSliceOp>(\n-              op->getLoc(), loadsMapping[loadOp].getType(), insertAsyncOp,\n-              sliceIndex, /*axis*/ 0);\n+          loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n         } else\n           llvm_unreachable(\"This should be LoadOp\");\n       } else {\n@@ -294,9 +302,11 @@ void LoopPipeliner::emitPrologue() {\n \n       // If this is a load/async_copy, we need to update the mask\n       if (llvm::isa<triton::LoadOp, triton::gpu::InsertSliceAsyncOp>(newOp)) {\n-        Value mask = newOp->getOperand(1);\n+        Value mask = llvm::isa<triton::LoadOp>(newOp) ? newOp->getOperand(1)\n+                                                      : newOp->getOperand(3);\n         // assert(I1 or TensorOf<[I1]>);\n         OpBuilder::InsertionGuard g(builder);\n+        // TODO: move this out of the loop\n         builder.setInsertionPoint(newOp);\n         Value splatCond = builder.create<triton::SplatOp>(\n             mask.getLoc(), mask.getType(), loopCond);\n@@ -313,8 +323,11 @@ void LoopPipeliner::emitPrologue() {\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n         // copy_async will update the value of its only use\n-        if (loads.contains(originalResult))\n-          originalResult = loadsMapping[originalResult];\n+        // TODO: load should no be used in the preheader?\n+        if (loads.contains(originalResult)) {\n+          break;\n+          // originalResult = loadsMapping[originalResult];\n+        }\n         setValueMapping(originalResult, newOp->getResult(dstIdx), stage);\n         // update mapping for loop-carried values (args)\n         for (OpOperand &operand : yieldOp->getOpOperands()) {\n@@ -325,17 +338,21 @@ void LoopPipeliner::emitPrologue() {\n         }\n       }\n     }\n-  }\n+\n+    pipelineIterIdx = builder.create<arith::AddIOp>(\n+        iv.getLoc(), pipelineIterIdx,\n+        builder.create<arith::ConstantIntOp>(iv.getLoc(), 1, 32));\n+  } // for (int stage = 0; stage < numStages - 1; ++stage)\n \n   // async.wait & extract_slice\n   Operation *asyncWait = builder.create<triton::gpu::AsyncWaitOp>(\n       loads[0].getLoc(), loads.size() * (numStages - 2));\n-  for (int i = numStages - 2; i >= 0; --i) {\n-    for (auto it = loads.rbegin(); it != loads.rend(); ++it) {\n-      // move extract_slice after asyncWait\n-      Value load = *it;\n-      valueMapping[loadsMapping[load]][i].getDefiningOp()->moveAfter(asyncWait);\n-    }\n+  loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n+  for (Value loadOp : loads) {\n+    Value extractSlice = builder.create<triton::gpu::ExtractSliceOp>(\n+        loadOp.getLoc(), loadsMapping[loadOp].getType(),\n+        loadStageBuffer[loadOp][numStages - 1], loopIterIdx, /*axis*/ 0);\n+    loadsExtract[loadOp] = extractSlice;\n   }\n }\n \n@@ -344,21 +361,25 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   // order of new args:\n   //   (original args),\n-  //   for each load result (after layout conversion) x:\n-  //     (x at stage[0, numStages-1))\n+  //   (insertSliceAsync buffer at stage numStages - 1)  for each load\n+  //   (extracted tensor)  for each load\n   //   (depArgs at stage numStages-1)\n   //   (iv at stage numStages-1)\n+  //   (pipeline iteration index)\n+  //   (loop iteration index)\n   SmallVector<Value> newLoopArgs;\n   // We need this to update operands for yield\n   // original block arg => new arg's idx\n   DenseMap<BlockArgument, size_t> depArgsIdx;\n   for (auto v : forOp.getIterOperands())\n     newLoopArgs.push_back(v);\n \n+  size_t bufferIdx = newLoopArgs.size();\n+  for (Value loadOp : loads)\n+    newLoopArgs.push_back(loadStageBuffer[loadOp].back());\n   size_t loadIdx = newLoopArgs.size();\n   for (Value loadOp : loads)\n-    for (int i = 0; i < numStages - 1; ++i)\n-      newLoopArgs.push_back(valueMapping[loadsMapping[loadOp]][i]);\n+    newLoopArgs.push_back(loadsExtract[loadOp]);\n \n   size_t depArgsBeginIdx = newLoopArgs.size();\n   for (BlockArgument depArg : depArgs) {\n@@ -368,6 +389,8 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   size_t nextIVIdx = newLoopArgs.size();\n   newLoopArgs.push_back(valueMapping[forOp.getInductionVar()][numStages - 2]);\n+  newLoopArgs.push_back(pipelineIterIdx);\n+  newLoopArgs.push_back(loopIterIdx);\n \n   for (size_t i = 0; i < newLoopArgs.size(); ++i)\n     assert(newLoopArgs[i]);\n@@ -399,7 +422,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n            \"we assume that this load has one use (ConvertLayout)\");\n     Value loadUse = load.getUsers().begin()->getResult(0);\n     mapping.lookup(loadUse).replaceAllUsesWith(\n-        newForOp.getRegionIterArgs()[loadIdx + idx * (numStages - 1)]);\n+        newForOp.getRegionIterArgs()[loadIdx + idx]);\n     // delete old load and layout conversion\n     mapping.lookup(loadUse).getDefiningOp()->erase();\n     mapping.lookup(load).getDefiningOp()->erase();\n@@ -432,12 +455,18 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                                     nextIV, newForOp.getUpperBound());\n \n   // slice index\n+  SmallVector<Value> nextBuffers;\n   SmallVector<Value> extractSlices;\n-  Value sliceIndex = builder.create<arith::IndexCastOp>(\n-      nextIV.getLoc(), builder.getI32Type(), nextIV);\n-  sliceIndex = builder.create<arith::RemSIOp>(\n-      nextIV.getLoc(), sliceIndex,\n+\n+  pipelineIterIdx = newForOp.getRegionIterArgs()[nextIVIdx + 1];\n+  Value insertSliceIndex = builder.create<arith::RemSIOp>(\n+      nextIV.getLoc(), pipelineIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n+  loopIterIdx = newForOp.getRegionIterArgs()[nextIVIdx + 2];\n+  Value extractSliceIndex = builder.create<arith::RemSIOp>(\n+      nextIV.getLoc(), loopIterIdx,\n+      builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n+\n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n     // TODO(da): does this work if loadOp has no mask?\n@@ -457,13 +486,15 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n       }\n       Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n           op->getLoc(), loadsBuffer[loadOp].getType(),\n-          nextMapping.lookupOrDefault(loadOp.ptr()), loadsBuffer[loadOp],\n-          sliceIndex, nextMapping.lookupOrDefault(loadOp.mask()),\n+          nextMapping.lookupOrDefault(loadOp.ptr()),\n+          newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n+          insertSliceIndex, nextMapping.lookupOrDefault(loadOp.mask()),\n           nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n+      nextBuffers.push_back(insertAsyncOp);\n       nextOp = builder.create<triton::gpu::ExtractSliceOp>(\n           op->getLoc(), loadsMapping[loadOp].getType(), insertAsyncOp,\n-          sliceIndex, /*axis*/ 0);\n+          extractSliceIndex, /*axis*/ 0);\n       extractSlices.push_back(nextOp->getResult(0));\n     } else\n       nextOp = builder.clone(*op, nextMapping);\n@@ -491,25 +522,29 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n     it->getDefiningOp()->moveAfter(asyncWait);\n   }\n \n+  // bump iteration count\n+  pipelineIterIdx = builder.create<arith::AddIOp>(\n+      nextIV.getLoc(), pipelineIterIdx,\n+      builder.create<arith::ConstantIntOp>(nextIV.getLoc(), 1, 32));\n+  loopIterIdx = builder.create<arith::AddIOp>(\n+      nextIV.getLoc(), loopIterIdx,\n+      builder.create<arith::ConstantIntOp>(nextIV.getLoc(), 1, 32));\n+\n   // Finally, the YieldOp, need to sync with the order of newLoopArgs\n   SmallVector<Value> yieldValues;\n   for (Value v : forOp.getBody()->getTerminator()->getOperands())\n     yieldValues.push_back(mapping.lookup(v));\n-  // shift pipelined args by 1\n-  for (size_t idx = 0; idx < loads.size(); ++idx) {\n-    Value load = loads[idx];\n-    for (int stage = 1; stage < numStages - 1; ++stage) {\n-      yieldValues.push_back(\n-          newForOp\n-              .getRegionIterArgs()[loadIdx + idx * (numStages - 1) + stage]);\n-    }\n-    yieldValues.push_back(nextMapping.lookup(load));\n-  }\n+  for (Value nextBuffer : nextBuffers)\n+    yieldValues.push_back(nextBuffer);\n+  for (Value nextSlice : extractSlices)\n+    yieldValues.push_back(nextSlice);\n \n   for (size_t i = depArgsBeginIdx; i < nextIVIdx; ++i)\n     yieldValues.push_back(\n         depArgsMapping.lookup(newForOp.getRegionIterArgs()[i]));\n   yieldValues.push_back(nextIV);\n+  yieldValues.push_back(pipelineIterIdx);\n+  yieldValues.push_back(loopIterIdx);\n \n   builder.setInsertionPointToEnd(newForOp.getBody());\n   auto test = builder.create<scf::YieldOp>("}, {"filename": "lib/Dialect/TritonGPU/Transforms/Verifier.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -70,7 +70,7 @@ class TritonGPUVerifier : public TritonGPUVerifierBase<TritonGPUVerifier> {\n     if (auto storeOp = llvm::dyn_cast<triton::StoreOp>(op)) {\n       // TODO: fill this\n     }\n-    if (auto gepOp = llvm::dyn_cast<triton::GEPOp>(op)) {\n+    if (auto addptrOp = llvm::dyn_cast<triton::AddPtrOp>(op)) {\n       // TODO: fill this\n     }\n     // Triton builtin Ops"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -14,6 +14,7 @@\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.h\"\n #include \"triton/driver/llvm.h\"\n+#include \"triton/tools/sys/getenv.hpp\"\n #include \"llvm/IR/Constants.h\"\n \n namespace mlir {\n@@ -124,6 +125,17 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                            mlir::ModuleOp module) {\n   mlir::PassManager pm(module->getContext());\n   applyPassManagerCLOptions(pm);\n+  auto printingFlags = mlir::OpPrintingFlags();\n+  printingFlags.elideLargeElementsAttrs(16);\n+  pm.enableIRPrinting(\n+      /*shouldPrintBeforePass=*/nullptr,\n+      /*shouldPrintAfterPass=*/\n+      [](mlir::Pass *pass, mlir::Operation *) {\n+        return ::triton::tools::getBoolEnv(\"MLIR_ENABLE_DUMP\");\n+      },\n+      /*printModuleScope=*/false,\n+      /*printAfterOnlyOnChange=*/true,\n+      /*printAfterOnlyOnFailure*/ false, llvm::dbgs(), printingFlags);\n \n   pm.addPass(createConvertTritonGPUToLLVMPass());\n   // Conanicalize to eliminate the remaining UnrealizedConversionCastOp"}, {"filename": "lib/driver/llvm.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -103,7 +103,7 @@ static bool find_and_replace(std::string &str, const std::string &begin,\n std::string path_to_ptxas(int &version) {\n   std::vector<std::string> rets;\n   std::string ret;\n-  // search pathes for ptxas\n+  // search paths for ptxas\n   std::vector<std::string> ptxas_prefixes = {\"\", \"/usr/local/cuda/bin/\"};\n   std::string triton_ptxas = tools::getenv(\"TRITON_PTXAS_PATH\");\n   if (!triton_ptxas.empty())"}, {"filename": "python/src/pybind11/attr.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -554,13 +554,13 @@ struct process_attribute<keep_alive<Nurse, Patient>>\n \n /// Recursively iterate over variadic template arguments\n template <typename... Args> struct process_attributes {\n-  static void init(const Args &... args, function_record *r) {\n+  static void init(const Args &...args, function_record *r) {\n     int unused[] = {\n         0, (process_attribute<typename std::decay<Args>::type>::init(args, r),\n             0)...};\n     ignore_unused(unused);\n   }\n-  static void init(const Args &... args, type_record *r) {\n+  static void init(const Args &...args, type_record *r) {\n     int unused[] = {\n         0, (process_attribute<typename std::decay<Args>::type>::init(args, r),\n             0)...};"}, {"filename": "python/src/pybind11/cast.h", "status": "modified", "additions": 29, "deletions": 24, "changes": 53, "file_content_changes": "@@ -1231,7 +1231,7 @@ template <> class type_caster<void> : public type_caster<void_type> {\n   }\n \n   template <typename T> using cast_op_type = void *&;\n-  operator void * &() { return value; }\n+  operator void *&() { return value; }\n   static constexpr auto name = _(\"capsule\");\n \n private:\n@@ -1336,9 +1336,12 @@ template <typename StringType, bool IsView = false> struct string_caster {\n #endif\n     }\n \n-    object utfNbytes = reinterpret_steal<object>(PyUnicode_AsEncodedString(\n-        load_src.ptr(),\n-        UTF_N == 8 ? \"utf-8\" : UTF_N == 16 ? \"utf-16\" : \"utf-32\", nullptr));\n+    object utfNbytes = reinterpret_steal<object>(\n+        PyUnicode_AsEncodedString(load_src.ptr(),\n+                                  UTF_N == 8    ? \"utf-8\"\n+                                  : UTF_N == 16 ? \"utf-16\"\n+                                                : \"utf-32\",\n+                                  nullptr));\n     if (!utfNbytes) {\n       PyErr_Clear();\n       return false;\n@@ -1377,20 +1380,21 @@ template <typename StringType, bool IsView = false> struct string_caster {\n private:\n   static handle decode_utfN(const char *buffer, ssize_t nbytes) {\n #if !defined(PYPY_VERSION)\n-    return UTF_N == 8\n-               ? PyUnicode_DecodeUTF8(buffer, nbytes, nullptr)\n-               : UTF_N == 16\n-                     ? PyUnicode_DecodeUTF16(buffer, nbytes, nullptr, nullptr)\n-                     : PyUnicode_DecodeUTF32(buffer, nbytes, nullptr, nullptr);\n+    return UTF_N == 8 ? PyUnicode_DecodeUTF8(buffer, nbytes, nullptr)\n+           : UTF_N == 16\n+               ? PyUnicode_DecodeUTF16(buffer, nbytes, nullptr, nullptr)\n+               : PyUnicode_DecodeUTF32(buffer, nbytes, nullptr, nullptr);\n #else\n     // PyPy seems to have multiple problems related to PyUnicode_UTF*: the UTF8\n     // version sometimes segfaults for unknown reasons, while the UTF16 and 32\n     // versions require a non-const char * arguments, which is also a nuisance,\n     // so bypass the whole thing by just passing the encoding as a string value,\n     // which works properly:\n-    return PyUnicode_Decode(\n-        buffer, nbytes,\n-        UTF_N == 8 ? \"utf-8\" : UTF_N == 16 ? \"utf-16\" : \"utf-32\", nullptr);\n+    return PyUnicode_Decode(buffer, nbytes,\n+                            UTF_N == 8    ? \"utf-8\"\n+                            : UTF_N == 16 ? \"utf-16\"\n+                                          : \"utf-32\",\n+                            nullptr);\n #endif\n   }\n \n@@ -1495,11 +1499,12 @@ struct type_caster<CharT, enable_if_t<is_std_char_type<CharT>::value>> {\n     if (StringCaster::UTF_N == 8 && str_len > 1 && str_len <= 4) {\n       unsigned char v0 = static_cast<unsigned char>(value[0]);\n       size_t char0_bytes =\n-          !(v0 & 0x80) ? 1 :            // low bits only: 0-127\n-              (v0 & 0xE0) == 0xC0 ? 2 : // 0b110xxxxx - start of 2-byte sequence\n-                  (v0 & 0xF0) == 0xE0 ? 3\n-                                      : // 0b1110xxxx - start of 3-byte sequence\n-                      4;                // 0b11110xxx - start of 4-byte sequence\n+          !(v0 & 0x80) ? 1 : // low bits only: 0-127\n+              (v0 & 0xE0) == 0xC0 ? 2\n+                                  : // 0b110xxxxx - start of 2-byte sequence\n+              (v0 & 0xF0) == 0xE0 ? 3\n+                                  : // 0b1110xxxx - start of 3-byte sequence\n+              4;                    // 0b11110xxx - start of 4-byte sequence\n \n       if (char0_bytes == str_len) {\n         // If we have a 128-255 value, we can decode it into a single char:\n@@ -2040,7 +2045,7 @@ tuple make_tuple() {\n \n template <return_value_policy policy = return_value_policy::automatic_reference,\n           typename... Args>\n-tuple make_tuple(Args &&... args_) {\n+tuple make_tuple(Args &&...args_) {\n   constexpr size_t size = sizeof...(Args);\n   std::array<object, size> args{\n       {reinterpret_steal<object>(detail::make_caster<Args>::cast(\n@@ -2261,7 +2266,7 @@ template <typename... Args> class argument_loader {\n template <return_value_policy policy> class simple_collector {\n public:\n   template <typename... Ts>\n-  explicit simple_collector(Ts &&... values)\n+  explicit simple_collector(Ts &&...values)\n       : m_args(pybind11::make_tuple<policy>(std::forward<Ts>(values)...)) {}\n \n   const tuple &args() const & { return m_args; }\n@@ -2285,7 +2290,7 @@ template <return_value_policy policy> class simple_collector {\n /// Python function call\n template <return_value_policy policy> class unpacking_collector {\n public:\n-  template <typename... Ts> explicit unpacking_collector(Ts &&... values) {\n+  template <typename... Ts> explicit unpacking_collector(Ts &&...values) {\n     // Tuples aren't (easily) resizable so a list is needed for collection,\n     // but the actual function call strictly requires a tuple.\n     auto args_list = list();\n@@ -2407,15 +2412,15 @@ template <return_value_policy policy> class unpacking_collector {\n /// Collect only positional arguments for a Python function call\n template <return_value_policy policy, typename... Args,\n           typename = enable_if_t<all_of<is_positional<Args>...>::value>>\n-simple_collector<policy> collect_arguments(Args &&... args) {\n+simple_collector<policy> collect_arguments(Args &&...args) {\n   return simple_collector<policy>(std::forward<Args>(args)...);\n }\n \n /// Collect all arguments, including keywords and unpacking (only instantiated\n /// when needed)\n template <return_value_policy policy, typename... Args,\n           typename = enable_if_t<!all_of<is_positional<Args>...>::value>>\n-unpacking_collector<policy> collect_arguments(Args &&... args) {\n+unpacking_collector<policy> collect_arguments(Args &&...args) {\n   // Following argument order rules for generalized unpacking according to PEP\n   // 448\n   static_assert(constexpr_last<is_positional, Args...>() <\n@@ -2430,14 +2435,14 @@ unpacking_collector<policy> collect_arguments(Args &&... args) {\n \n template <typename Derived>\n template <return_value_policy policy, typename... Args>\n-object object_api<Derived>::operator()(Args &&... args) const {\n+object object_api<Derived>::operator()(Args &&...args) const {\n   return detail::collect_arguments<policy>(std::forward<Args>(args)...)\n       .call(derived().ptr());\n }\n \n template <typename Derived>\n template <return_value_policy policy, typename... Args>\n-object object_api<Derived>::call(Args &&... args) const {\n+object object_api<Derived>::call(Args &&...args) const {\n   return operator()<policy>(std::forward<Args>(args)...);\n }\n "}, {"filename": "python/src/pybind11/detail/common.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -808,9 +808,9 @@ struct is_fmt_numeric<T, enable_if_t<std::is_arithmetic<T>::value>> {\n           ? 0\n           : 1 + (std::is_integral<T>::value\n                      ? detail::log2(sizeof(T)) * 2 + std::is_unsigned<T>::value\n-                     : 8 + (std::is_same<T, double>::value\n-                                ? 1\n-                                : std::is_same<T, long double>::value ? 2 : 0));\n+                     : 8 + (std::is_same<T, double>::value        ? 1\n+                            : std::is_same<T, long double>::value ? 2\n+                                                                  : 0));\n };\n NAMESPACE_END(detail)\n "}, {"filename": "python/src/pybind11/detail/descr.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -104,7 +104,7 @@ constexpr descr<N, Ts...> concat(const descr<N, Ts...> &descr) {\n }\n \n template <size_t N, typename... Ts, typename... Args>\n-constexpr auto concat(const descr<N, Ts...> &d, const Args &... args)\n+constexpr auto concat(const descr<N, Ts...> &d, const Args &...args)\n     -> decltype(std::declval<descr<N + 2, Ts...>>() + concat(args...)) {\n   return d + _(\", \") + concat(args...);\n }"}, {"filename": "python/src/pybind11/detail/init.h", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -66,13 +66,13 @@ template <typename /*Class*/> constexpr bool is_alias(void *) { return false; }\n template <\n     typename Class, typename... Args,\n     detail::enable_if_t<std::is_constructible<Class, Args...>::value, int> = 0>\n-inline Class *construct_or_initialize(Args &&... args) {\n+inline Class *construct_or_initialize(Args &&...args) {\n   return new Class(std::forward<Args>(args)...);\n }\n template <\n     typename Class, typename... Args,\n     detail::enable_if_t<!std::is_constructible<Class, Args...>::value, int> = 0>\n-inline Class *construct_or_initialize(Args &&... args) {\n+inline Class *construct_or_initialize(Args &&...args) {\n   return new Class{std::forward<Args>(args)...};\n }\n \n@@ -200,7 +200,7 @@ void construct(value_and_holder &v_h, Alias<Class> &&result, bool) {\n template <typename... Args> struct constructor {\n   template <typename Class, typename... Extra,\n             enable_if_t<!Class::has_alias, int> = 0>\n-  static void execute(Class &cl, const Extra &... extra) {\n+  static void execute(Class &cl, const Extra &...extra) {\n     cl.def(\n         \"__init__\",\n         [](value_and_holder &v_h, Args... args) {\n@@ -214,7 +214,7 @@ template <typename... Args> struct constructor {\n             enable_if_t<Class::has_alias &&\n                             std::is_constructible<Cpp<Class>, Args...>::value,\n                         int> = 0>\n-  static void execute(Class &cl, const Extra &... extra) {\n+  static void execute(Class &cl, const Extra &...extra) {\n     cl.def(\n         \"__init__\",\n         [](value_and_holder &v_h, Args... args) {\n@@ -232,7 +232,7 @@ template <typename... Args> struct constructor {\n             enable_if_t<Class::has_alias &&\n                             !std::is_constructible<Cpp<Class>, Args...>::value,\n                         int> = 0>\n-  static void execute(Class &cl, const Extra &... extra) {\n+  static void execute(Class &cl, const Extra &...extra) {\n     cl.def(\n         \"__init__\",\n         [](value_and_holder &v_h, Args... args) {\n@@ -249,7 +249,7 @@ template <typename... Args> struct alias_constructor {\n             enable_if_t<Class::has_alias &&\n                             std::is_constructible<Alias<Class>, Args...>::value,\n                         int> = 0>\n-  static void execute(Class &cl, const Extra &... extra) {\n+  static void execute(Class &cl, const Extra &...extra) {\n     cl.def(\n         \"__init__\",\n         [](value_and_holder &v_h, Args... args) {\n@@ -280,7 +280,7 @@ struct factory<Func, void_type (*)(), Return(Args...)> {\n   // either already be an alias instance, or the alias needs to be constructible\n   // from a `Class &&` argument.\n   template <typename Class, typename... Extra>\n-  void execute(Class &cl, const Extra &... extra) && {\n+  void execute(Class &cl, const Extra &...extra) && {\n #if defined(PYBIND11_CPP14)\n     cl.def(\n         \"__init__\",\n@@ -323,7 +323,7 @@ struct factory<CFunc, AFunc, CReturn(CArgs...), AReturn(AArgs...)> {\n   // the direct class (i.e. not inherited), the alias factory when `self` is a\n   // Python-side subtype.\n   template <typename Class, typename... Extra>\n-  void execute(Class &cl, const Extra &... extra) && {\n+  void execute(Class &cl, const Extra &...extra) && {\n     static_assert(Class::has_alias,\n                   \"The two-argument version of `py::init()` can \"\n                   \"only be used if the class has an alias\");\n@@ -389,7 +389,7 @@ struct pickle_factory<Get, Set, RetState(Self), NewInstance(ArgState)> {\n       : get(std::forward<Get>(get)), set(std::forward<Set>(set)) {}\n \n   template <typename Class, typename... Extra>\n-  void execute(Class &cl, const Extra &... extra) && {\n+  void execute(Class &cl, const Extra &...extra) && {\n     cl.def(\"__getstate__\", std::move(get));\n \n #if defined(PYBIND11_CPP14)"}, {"filename": "python/src/pybind11/detail/internals.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -297,7 +297,7 @@ inline type_map<type_info *> &registered_local_types_cpp() {\n /// duration -- the internal strings are only cleared when the program exits or\n /// after interpreter shutdown (when embedding), and so are suitable for c-style\n /// strings needed by Python internals (such as PyTypeObject's tp_name).\n-template <typename... Args> const char *c_str(Args &&... args) {\n+template <typename... Args> const char *c_str(Args &&...args) {\n   auto &strings = get_internals().static_strings;\n   strings.emplace_front(std::forward<Args>(args)...);\n   return strings.front().c_str();"}, {"filename": "python/src/pybind11/eigen.h", "status": "modified", "additions": 11, "deletions": 10, "changes": 21, "file_content_changes": "@@ -164,7 +164,9 @@ template <typename Type_> struct EigenProps {\n   static constexpr EigenIndex\n       inner_stride = if_zero<StrideType::InnerStrideAtCompileTime, 1>::value,\n       outer_stride = if_zero < StrideType::OuterStrideAtCompileTime,\n-      vector ? size : row_major ? cols : rows > ::value;\n+      vector      ? size\n+      : row_major ? cols\n+                  : rows > ::value;\n   static constexpr bool dynamic_stride =\n       inner_stride == Eigen::Dynamic && outer_stride == Eigen::Dynamic;\n   static constexpr bool requires_row_major =\n@@ -471,15 +473,14 @@ struct type_caster<Eigen::Ref<PlainObjectType, 0, StrideType>,\n   using props = EigenProps<Type>;\n   using Scalar = typename props::Scalar;\n   using MapType = Eigen::Map<PlainObjectType, 0, StrideType>;\n-  using Array =\n-      array_t<Scalar, array::forcecast |\n-                          ((props::row_major ? props::inner_stride\n-                                             : props::outer_stride) == 1\n-                               ? array::c_style\n-                               : (props::row_major ? props::outer_stride\n-                                                   : props::inner_stride) == 1\n-                                     ? array::f_style\n-                                     : 0)>;\n+  using Array = array_t<\n+      Scalar,\n+      array::forcecast |\n+          ((props::row_major ? props::inner_stride : props::outer_stride) == 1\n+               ? array::c_style\n+           : (props::row_major ? props::outer_stride : props::inner_stride) == 1\n+               ? array::f_style\n+               : 0)>;\n   static constexpr bool need_writeable = is_eigen_mutable_map<Type>::value;\n   // Delay construction (these have no default constructor)\n   std::unique_ptr<MapType> map;"}, {"filename": "python/src/pybind11/numpy.h", "status": "modified", "additions": 8, "deletions": 9, "changes": 17, "file_content_changes": "@@ -1299,10 +1299,10 @@ template <typename T, typename SFINAE> struct npy_format_descriptor {\n #define PYBIND11_FIELD_DESCRIPTOR_EX(T, Field, Name)                           \\\n   ::pybind11::detail::field_descriptor {                                       \\\n     Name, offsetof(T, Field), sizeof(decltype(std::declval<T>().Field)),       \\\n-        ::pybind11::format_descriptor<decltype(                                \\\n-            std::declval<T>().Field)>::format(),                               \\\n-        ::pybind11::detail::npy_format_descriptor<decltype(                    \\\n-            std::declval<T>().Field)>::dtype()                                 \\\n+        ::pybind11::format_descriptor<                                         \\\n+            decltype(std::declval<T>().Field)>::format(),                      \\\n+        ::pybind11::detail::npy_format_descriptor<                             \\\n+            decltype(std::declval<T>().Field)>::dtype()                        \\\n   }\n \n // Extract name, offset and format descriptor for a struct field\n@@ -1576,10 +1576,9 @@ broadcast_trivial broadcast(const std::array<buffer_info, N> &buffers,\n     }\n   }\n \n-  return trivial_broadcast_c\n-             ? broadcast_trivial::c_trivial\n-             : trivial_broadcast_f ? broadcast_trivial::f_trivial\n-                                   : broadcast_trivial::non_trivial;\n+  return trivial_broadcast_c   ? broadcast_trivial::c_trivial\n+         : trivial_broadcast_f ? broadcast_trivial::f_trivial\n+                               : broadcast_trivial::non_trivial;\n }\n \n template <typename T> struct vectorize_arg {\n@@ -1643,7 +1642,7 @@ struct vectorize_helper {\n   //       we can store vectorized buffer_infos in an array (argument VIndex has\n   //       its buffer at index BIndex in the array).\n   template <size_t... Index, size_t... VIndex, size_t... BIndex>\n-  object run(typename vectorize_arg<Args>::type &... args,\n+  object run(typename vectorize_arg<Args>::type &...args,\n              index_sequence<Index...> i_seq, index_sequence<VIndex...> vi_seq,\n              index_sequence<BIndex...> bi_seq) {\n "}, {"filename": "python/src/pybind11/operators.h", "status": "modified", "additions": 14, "deletions": 12, "changes": 26, "file_content_changes": "@@ -93,7 +93,7 @@ template <op_id, op_type, typename B, typename L, typename R> struct op_impl {};\n /// Operator implementation generator\n template <op_id id, op_type ot, typename L, typename R> struct op_ {\n   template <typename Class, typename... Extra>\n-  void execute(Class &cl, const Extra &... extra) const {\n+  void execute(Class &cl, const Extra &...extra) const {\n     using Base = typename Class::type;\n     using L_type = conditional_t<std::is_same<L, self_t>::value, Base, L>;\n     using R_type = conditional_t<std::is_same<R, self_t>::value, Base, R>;\n@@ -102,12 +102,13 @@ template <op_id id, op_type ot, typename L, typename R> struct op_ {\n #if PY_MAJOR_VERSION < 3\n     if (id == op_truediv || id == op_itruediv)\n       cl.def(id == op_itruediv ? \"__idiv__\"\n-                               : ot == op_l ? \"__div__\" : \"__rdiv__\",\n+             : ot == op_l      ? \"__div__\"\n+                               : \"__rdiv__\",\n              &op::execute, is_operator(), extra...);\n #endif\n   }\n   template <typename Class, typename... Extra>\n-  void execute_cast(Class &cl, const Extra &... extra) const {\n+  void execute_cast(Class &cl, const Extra &...extra) const {\n     using Base = typename Class::type;\n     using L_type = conditional_t<std::is_same<L, self_t>::value, Base, L>;\n     using R_type = conditional_t<std::is_same<R, self_t>::value, Base, R>;\n@@ -116,7 +117,8 @@ template <op_id id, op_type ot, typename L, typename R> struct op_ {\n #if PY_MAJOR_VERSION < 3\n     if (id == op_truediv || id == op_itruediv)\n       cl.def(id == op_itruediv ? \"__idiv__\"\n-                               : ot == op_l ? \"__div__\" : \"__rdiv__\",\n+             : ot == op_l      ? \"__div__\"\n+                               : \"__rdiv__\",\n              &op::execute, is_operator(), extra...);\n #endif\n   }\n@@ -177,19 +179,19 @@ template <op_id id, op_type ot, typename L, typename R> struct op_ {\n \n PYBIND11_BINARY_OPERATOR(sub, rsub, operator-, l - r)\n PYBIND11_BINARY_OPERATOR(add, radd, operator+, l + r)\n-PYBIND11_BINARY_OPERATOR(mul, rmul, operator*, l * r)\n+PYBIND11_BINARY_OPERATOR(mul, rmul, operator*, l *r)\n PYBIND11_BINARY_OPERATOR(truediv, rtruediv, operator/, l / r)\n PYBIND11_BINARY_OPERATOR(mod, rmod, operator%, l % r)\n PYBIND11_BINARY_OPERATOR(lshift, rlshift, operator<<, l << r)\n-PYBIND11_BINARY_OPERATOR(rshift, rrshift, operator>>, l>> r)\n-PYBIND11_BINARY_OPERATOR(and, rand, operator&, l & r)\n+PYBIND11_BINARY_OPERATOR(rshift, rrshift, operator>>, l >> r)\n+PYBIND11_BINARY_OPERATOR(and, rand, operator&, l &r)\n PYBIND11_BINARY_OPERATOR(xor, rxor, operator^, l ^ r)\n PYBIND11_BINARY_OPERATOR(eq, eq, operator==, l == r)\n PYBIND11_BINARY_OPERATOR(ne, ne, operator!=, l != r)\n PYBIND11_BINARY_OPERATOR(or, ror, operator|, l | r)\n-PYBIND11_BINARY_OPERATOR(gt, lt, operator>, l> r)\n+PYBIND11_BINARY_OPERATOR(gt, lt, operator>, l > r)\n PYBIND11_BINARY_OPERATOR(ge, le, operator>=, l >= r)\n-PYBIND11_BINARY_OPERATOR(lt, gt, operator<, l<r)\n+PYBIND11_BINARY_OPERATOR(lt, gt, operator<, l < r)\n PYBIND11_BINARY_OPERATOR(le, ge, operator<=, l <= r)\n // PYBIND11_BINARY_OPERATOR(pow,       rpow,         pow,          std::pow(l,\n // r))\n@@ -203,11 +205,11 @@ PYBIND11_INPLACE_OPERATOR(irshift, operator>>=, l >>= r)\n PYBIND11_INPLACE_OPERATOR(iand, operator&=, l &= r)\n PYBIND11_INPLACE_OPERATOR(ixor, operator^=, l ^= r)\n PYBIND11_INPLACE_OPERATOR(ior, operator|=, l |= r)\n-PYBIND11_UNARY_OPERATOR(neg, operator-, - l)\n-PYBIND11_UNARY_OPERATOR(pos, operator+, + l)\n+PYBIND11_UNARY_OPERATOR(neg, operator-, -l)\n+PYBIND11_UNARY_OPERATOR(pos, operator+, +l)\n PYBIND11_UNARY_OPERATOR(abs, abs, std::abs(l))\n PYBIND11_UNARY_OPERATOR(hash, hash, std::hash<L>()(l))\n-PYBIND11_UNARY_OPERATOR(invert, operator~,(~l))\n+PYBIND11_UNARY_OPERATOR(invert, operator~, (~l))\n PYBIND11_UNARY_OPERATOR(bool, operator!, !!l)\n PYBIND11_UNARY_OPERATOR(int, int_, (int)l)\n PYBIND11_UNARY_OPERATOR(float, float_, (double)l)"}, {"filename": "python/src/pybind11/pybind11.h", "status": "modified", "additions": 36, "deletions": 40, "changes": 76, "file_content_changes": "@@ -74,30 +74,30 @@ class cpp_function : public function {\n \n   /// Construct a cpp_function from a vanilla function pointer\n   template <typename Return, typename... Args, typename... Extra>\n-  cpp_function(Return (*f)(Args...), const Extra &... extra) {\n+  cpp_function(Return (*f)(Args...), const Extra &...extra) {\n     initialize(f, f, extra...);\n   }\n \n   /// Construct a cpp_function from a lambda function (possibly with internal\n   /// state)\n   template <typename Func, typename... Extra,\n             typename = detail::enable_if_t<detail::is_lambda<Func>::value>>\n-  cpp_function(Func &&f, const Extra &... extra) {\n+  cpp_function(Func &&f, const Extra &...extra) {\n     initialize(std::forward<Func>(f),\n                (detail::function_signature_t<Func> *)nullptr, extra...);\n   }\n \n   /// Construct a cpp_function from a class method (non-const)\n   template <typename Return, typename Class, typename... Arg, typename... Extra>\n-  cpp_function(Return (Class::*f)(Arg...), const Extra &... extra) {\n+  cpp_function(Return (Class::*f)(Arg...), const Extra &...extra) {\n     initialize(\n         [f](Class *c, Arg... args) -> Return { return (c->*f)(args...); },\n         (Return(*)(Class *, Arg...)) nullptr, extra...);\n   }\n \n   /// Construct a cpp_function from a class method (const)\n   template <typename Return, typename Class, typename... Arg, typename... Extra>\n-  cpp_function(Return (Class::*f)(Arg...) const, const Extra &... extra) {\n+  cpp_function(Return (Class::*f)(Arg...) const, const Extra &...extra) {\n     initialize(\n         [f](const Class *c, Arg... args) -> Return { return (c->*f)(args...); },\n         (Return(*)(const Class *, Arg...)) nullptr, extra...);\n@@ -114,7 +114,7 @@ class cpp_function : public function {\n \n   /// Special internal constructor for functors, lambda functions, etc.\n   template <typename Func, typename Return, typename... Args, typename... Extra>\n-  void initialize(Func &&f, Return (*)(Args...), const Extra &... extra) {\n+  void initialize(Func &&f, Return (*)(Args...), const Extra &...extra) {\n     using namespace detail;\n     struct capture {\n       remove_reference_t<Func> f;\n@@ -924,7 +924,7 @@ class module : public object {\n       details on the ``Extra&& ... extra`` argument, see section :ref:`extras`.\n   \\endrst */\n   template <typename Func, typename... Extra>\n-  module &def(const char *name_, Func &&f, const Extra &... extra) {\n+  module &def(const char *name_, Func &&f, const Extra &...extra) {\n     cpp_function func(std::forward<Func>(f), name(name_), scope(*this),\n                       sibling(getattr(*this, name_, none())), extra...);\n     // NB: allow overwriting here because cpp_function sets up a chain with the\n@@ -1104,8 +1104,8 @@ class generic_type : public object {\n \n /// Set the pointer to operator new if it exists. The cast is needed because it\n /// can be overloaded.\n-template <typename T, typename = void_t<decltype(\n-                          static_cast<void *(*)(size_t)>(T::operator new))>>\n+template <typename T, typename = void_t<decltype(static_cast<void *(*)(size_t)>(\n+                          T::operator new))>>\n void set_operator_new(type_record *r) {\n   r->operator_new = &T::operator new;\n }\n@@ -1204,7 +1204,7 @@ class class_ : public detail::generic_type {\n   PYBIND11_OBJECT(class_, generic_type, PyType_Check)\n \n   template <typename... Extra>\n-  class_(handle scope, const char *name, const Extra &... extra) {\n+  class_(handle scope, const char *name, const Extra &...extra) {\n     using namespace detail;\n \n     // MI can only be specified via class_ template options, not constructor\n@@ -1263,7 +1263,7 @@ class class_ : public detail::generic_type {\n   static void add_base(detail::type_record &) {}\n \n   template <typename Func, typename... Extra>\n-  class_ &def(const char *name_, Func &&f, const Extra &... extra) {\n+  class_ &def(const char *name_, Func &&f, const Extra &...extra) {\n     cpp_function cf(method_adaptor<type>(std::forward<Func>(f)), name(name_),\n                     is_method(*this), sibling(getattr(*this, name_, none())),\n                     extra...);\n@@ -1272,7 +1272,7 @@ class class_ : public detail::generic_type {\n   }\n \n   template <typename Func, typename... Extra>\n-  class_ &def_static(const char *name_, Func &&f, const Extra &... extra) {\n+  class_ &def_static(const char *name_, Func &&f, const Extra &...extra) {\n     static_assert(\n         !std::is_member_function_pointer<Func>::value,\n         \"def_static(...) called with a non-static member function pointer\");\n@@ -1284,43 +1284,42 @@ class class_ : public detail::generic_type {\n \n   template <detail::op_id id, detail::op_type ot, typename L, typename R,\n             typename... Extra>\n-  class_ &def(const detail::op_<id, ot, L, R> &op, const Extra &... extra) {\n+  class_ &def(const detail::op_<id, ot, L, R> &op, const Extra &...extra) {\n     op.execute(*this, extra...);\n     return *this;\n   }\n \n   template <detail::op_id id, detail::op_type ot, typename L, typename R,\n             typename... Extra>\n-  class_ &def_cast(const detail::op_<id, ot, L, R> &op,\n-                   const Extra &... extra) {\n+  class_ &def_cast(const detail::op_<id, ot, L, R> &op, const Extra &...extra) {\n     op.execute_cast(*this, extra...);\n     return *this;\n   }\n \n   template <typename... Args, typename... Extra>\n   class_ &def(const detail::initimpl::constructor<Args...> &init,\n-              const Extra &... extra) {\n+              const Extra &...extra) {\n     init.execute(*this, extra...);\n     return *this;\n   }\n \n   template <typename... Args, typename... Extra>\n   class_ &def(const detail::initimpl::alias_constructor<Args...> &init,\n-              const Extra &... extra) {\n+              const Extra &...extra) {\n     init.execute(*this, extra...);\n     return *this;\n   }\n \n   template <typename... Args, typename... Extra>\n   class_ &def(detail::initimpl::factory<Args...> &&init,\n-              const Extra &... extra) {\n+              const Extra &...extra) {\n     std::move(init).execute(*this, extra...);\n     return *this;\n   }\n \n   template <typename... Args, typename... Extra>\n   class_ &def(detail::initimpl::pickle_factory<Args...> &&pf,\n-              const Extra &... extra) {\n+              const Extra &...extra) {\n     std::move(pf).execute(*this, extra...);\n     return *this;\n   }\n@@ -1352,7 +1351,7 @@ class class_ : public detail::generic_type {\n   }\n \n   template <typename C, typename D, typename... Extra>\n-  class_ &def_readwrite(const char *name, D C::*pm, const Extra &... extra) {\n+  class_ &def_readwrite(const char *name, D C::*pm, const Extra &...extra) {\n     static_assert(\n         std::is_same<C, type>::value || std::is_base_of<C, type>::value,\n         \"def_readwrite() requires a class member (or base class member)\");\n@@ -1367,7 +1366,7 @@ class class_ : public detail::generic_type {\n \n   template <typename C, typename D, typename... Extra>\n   class_ &def_readonly(const char *name, const D C::*pm,\n-                       const Extra &... extra) {\n+                       const Extra &...extra) {\n     static_assert(\n         std::is_same<C, type>::value || std::is_base_of<C, type>::value,\n         \"def_readonly() requires a class member (or base class member)\");\n@@ -1379,8 +1378,7 @@ class class_ : public detail::generic_type {\n   }\n \n   template <typename D, typename... Extra>\n-  class_ &def_readwrite_static(const char *name, D *pm,\n-                               const Extra &... extra) {\n+  class_ &def_readwrite_static(const char *name, D *pm, const Extra &...extra) {\n     cpp_function fget([pm](object) -> const D & { return *pm; }, scope(*this)),\n         fset([pm](object, const D &value) { *pm = value; }, scope(*this));\n     def_property_static(name, fget, fset, return_value_policy::reference,\n@@ -1390,7 +1388,7 @@ class class_ : public detail::generic_type {\n \n   template <typename D, typename... Extra>\n   class_ &def_readonly_static(const char *name, const D *pm,\n-                              const Extra &... extra) {\n+                              const Extra &...extra) {\n     cpp_function fget([pm](object) -> const D & { return *pm; }, scope(*this));\n     def_property_readonly_static(name, fget, return_value_policy::reference,\n                                  extra...);\n@@ -1400,7 +1398,7 @@ class class_ : public detail::generic_type {\n   /// Uses return_value_policy::reference_internal by default\n   template <typename Getter, typename... Extra>\n   class_ &def_property_readonly(const char *name, const Getter &fget,\n-                                const Extra &... extra) {\n+                                const Extra &...extra) {\n     return def_property_readonly(name, cpp_function(method_adaptor<type>(fget)),\n                                  return_value_policy::reference_internal,\n                                  extra...);\n@@ -1409,14 +1407,14 @@ class class_ : public detail::generic_type {\n   /// Uses cpp_function's return_value_policy by default\n   template <typename... Extra>\n   class_ &def_property_readonly(const char *name, const cpp_function &fget,\n-                                const Extra &... extra) {\n+                                const Extra &...extra) {\n     return def_property(name, fget, nullptr, extra...);\n   }\n \n   /// Uses return_value_policy::reference by default\n   template <typename Getter, typename... Extra>\n   class_ &def_property_readonly_static(const char *name, const Getter &fget,\n-                                       const Extra &... extra) {\n+                                       const Extra &...extra) {\n     return def_property_readonly_static(\n         name, cpp_function(fget), return_value_policy::reference, extra...);\n   }\n@@ -1425,45 +1423,43 @@ class class_ : public detail::generic_type {\n   template <typename... Extra>\n   class_ &def_property_readonly_static(const char *name,\n                                        const cpp_function &fget,\n-                                       const Extra &... extra) {\n+                                       const Extra &...extra) {\n     return def_property_static(name, fget, nullptr, extra...);\n   }\n \n   /// Uses return_value_policy::reference_internal by default\n   template <typename Getter, typename Setter, typename... Extra>\n   class_ &def_property(const char *name, const Getter &fget, const Setter &fset,\n-                       const Extra &... extra) {\n+                       const Extra &...extra) {\n     return def_property(name, fget, cpp_function(method_adaptor<type>(fset)),\n                         extra...);\n   }\n   template <typename Getter, typename... Extra>\n   class_ &def_property(const char *name, const Getter &fget,\n-                       const cpp_function &fset, const Extra &... extra) {\n+                       const cpp_function &fset, const Extra &...extra) {\n     return def_property(name, cpp_function(method_adaptor<type>(fget)), fset,\n                         return_value_policy::reference_internal, extra...);\n   }\n \n   /// Uses cpp_function's return_value_policy by default\n   template <typename... Extra>\n   class_ &def_property(const char *name, const cpp_function &fget,\n-                       const cpp_function &fset, const Extra &... extra) {\n+                       const cpp_function &fset, const Extra &...extra) {\n     return def_property_static(name, fget, fset, is_method(*this), extra...);\n   }\n \n   /// Uses return_value_policy::reference by default\n   template <typename Getter, typename... Extra>\n   class_ &def_property_static(const char *name, const Getter &fget,\n-                              const cpp_function &fset,\n-                              const Extra &... extra) {\n+                              const cpp_function &fset, const Extra &...extra) {\n     return def_property_static(name, cpp_function(fget), fset,\n                                return_value_policy::reference, extra...);\n   }\n \n   /// Uses cpp_function's return_value_policy by default\n   template <typename... Extra>\n   class_ &def_property_static(const char *name, const cpp_function &fget,\n-                              const cpp_function &fset,\n-                              const Extra &... extra) {\n+                              const cpp_function &fset, const Extra &...extra) {\n     static_assert(\n         0 == detail::constexpr_sum(std::is_base_of<arg, Extra>::value...),\n         \"Argument annotations are not allowed for properties\");\n@@ -1782,7 +1778,7 @@ template <typename Type> class enum_ : public class_<Type> {\n   using Scalar = typename std::underlying_type<Type>::type;\n \n   template <typename... Extra>\n-  enum_(const handle &scope, const char *name, const Extra &... extra)\n+  enum_(const handle &scope, const char *name, const Extra &...extra)\n       : class_<Type>(scope, name, extra...), m_base(*this, scope) {\n     constexpr bool is_arithmetic =\n         detail::any_of<std::is_same<arithmetic, Extra>...>::value;\n@@ -1898,7 +1894,7 @@ template <return_value_policy Policy = return_value_policy::reference_internal,\n           typename Iterator, typename Sentinel,\n           typename ValueType = decltype(*std::declval<Iterator>()),\n           typename... Extra>\n-iterator make_iterator(Iterator first, Sentinel last, Extra &&... extra) {\n+iterator make_iterator(Iterator first, Sentinel last, Extra &&...extra) {\n   typedef detail::iterator_state<Iterator, Sentinel, false, Policy> state;\n \n   if (!detail::get_type_info(typeid(state), false)) {\n@@ -1929,7 +1925,7 @@ template <return_value_policy Policy = return_value_policy::reference_internal,\n           typename Iterator, typename Sentinel,\n           typename KeyType = decltype((*std::declval<Iterator>()).first),\n           typename... Extra>\n-iterator make_key_iterator(Iterator first, Sentinel last, Extra &&... extra) {\n+iterator make_key_iterator(Iterator first, Sentinel last, Extra &&...extra) {\n   typedef detail::iterator_state<Iterator, Sentinel, true, Policy> state;\n \n   if (!detail::get_type_info(typeid(state), false)) {\n@@ -1958,15 +1954,15 @@ iterator make_key_iterator(Iterator first, Sentinel last, Extra &&... extra) {\n /// supporting `std::begin()`/`std::end()`\n template <return_value_policy Policy = return_value_policy::reference_internal,\n           typename Type, typename... Extra>\n-iterator make_iterator(Type &value, Extra &&... extra) {\n+iterator make_iterator(Type &value, Extra &&...extra) {\n   return make_iterator<Policy>(std::begin(value), std::end(value), extra...);\n }\n \n /// Makes an iterator over the keys (`.first`) of a stl map-like container\n /// supporting `std::begin()`/`std::end()`\n template <return_value_policy Policy = return_value_policy::reference_internal,\n           typename Type, typename... Extra>\n-iterator make_key_iterator(Type &value, Extra &&... extra) {\n+iterator make_key_iterator(Type &value, Extra &&...extra) {\n   return make_key_iterator<Policy>(std::begin(value), std::end(value),\n                                    extra...);\n }\n@@ -2106,7 +2102,7 @@ NAMESPACE_END(detail)\n \n template <return_value_policy policy = return_value_policy::automatic_reference,\n           typename... Args>\n-void print(Args &&... args) {\n+void print(Args &&...args) {\n   auto c = detail::collect_arguments<policy>(std::forward<Args>(args)...);\n   detail::print(c.args(), c.kwargs());\n }"}, {"filename": "python/src/pybind11/pytypes.h", "status": "modified", "additions": 7, "deletions": 9, "changes": 16, "file_content_changes": "@@ -112,12 +112,12 @@ template <typename Derived> class object_api : public pyobject_tag {\n   template <\n       return_value_policy policy = return_value_policy::automatic_reference,\n       typename... Args>\n-  object operator()(Args &&... args) const;\n+  object operator()(Args &&...args) const;\n   template <\n       return_value_policy policy = return_value_policy::automatic_reference,\n       typename... Args>\n   PYBIND11_DEPRECATED(\"call(...) was deprecated in favor of operator()(...)\")\n-  object call(Args &&... args) const;\n+  object call(Args &&...args) const;\n \n   /// Equivalent to ``obj is other`` in Python.\n   bool is(object_api const &other) const {\n@@ -1109,7 +1109,7 @@ class str : public object {\n     return std::string(buffer, (size_t)length);\n   }\n \n-  template <typename... Args> str format(Args &&... args) const {\n+  template <typename... Args> str format(Args &&...args) const {\n     return attr(\"format\")(std::forward<Args>(args)...);\n   }\n \n@@ -1282,11 +1282,9 @@ class int_ : public object {\n   template <typename T,\n             detail::enable_if_t<std::is_integral<T>::value, int> = 0>\n   operator T() const {\n-    return std::is_unsigned<T>::value\n-               ? detail::as_unsigned<T>(m_ptr)\n-               : sizeof(T) <= sizeof(long)\n-                     ? (T)PyLong_AsLong(m_ptr)\n-                     : (T)PYBIND11_LONG_AS_LONGLONG(m_ptr);\n+    return std::is_unsigned<T>::value  ? detail::as_unsigned<T>(m_ptr)\n+           : sizeof(T) <= sizeof(long) ? (T)PyLong_AsLong(m_ptr)\n+                                       : (T)PYBIND11_LONG_AS_LONGLONG(m_ptr);\n   }\n };\n \n@@ -1438,7 +1436,7 @@ class dict : public object {\n             // defer the collector\n             typename collector =\n                 detail::deferred_t<detail::unpacking_collector<>, Args...>>\n-  explicit dict(Args &&... args)\n+  explicit dict(Args &&...args)\n       : dict(collector(std::forward<Args>(args)...).kwargs()) {}\n \n   size_t size() const { return (size_t)PyDict_Size(m_ptr); }"}, {"filename": "python/src/pybind11/stl.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -356,7 +356,7 @@ struct variant_caster_visitor {\n /// `boost::variant` and `boost::apply_visitor`.\n template <template <typename...> class Variant> struct visit_helper {\n   template <typename... Args>\n-  static auto call(Args &&... args)\n+  static auto call(Args &&...args)\n       -> decltype(visit(std::forward<Args>(args)...)) {\n     return visit(std::forward<Args>(args)...);\n   }"}, {"filename": "python/src/pybind11/stl_bind.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -432,7 +432,7 @@ NAMESPACE_END(detail)\n template <typename Vector, typename holder_type = std::unique_ptr<Vector>,\n           typename... Args>\n class_<Vector, holder_type> bind_vector(handle scope, std::string const &name,\n-                                        Args &&... args) {\n+                                        Args &&...args) {\n   using Class_ = class_<Vector, holder_type>;\n \n   // If the value_type is unregistered (e.g. a converting type) or is itself\n@@ -596,7 +596,7 @@ NAMESPACE_END(detail)\n template <typename Map, typename holder_type = std::unique_ptr<Map>,\n           typename... Args>\n class_<Map, holder_type> bind_map(handle scope, const std::string &name,\n-                                  Args &&... args) {\n+                                  Args &&...args) {\n   using KeyType = typename Map::key_type;\n   using MappedType = typename Map::mapped_type;\n   using Class_ = class_<Map, holder_type>;"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 8, "deletions": 15, "changes": 23, "file_content_changes": "@@ -19,6 +19,7 @@\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Target/LLVMIR/LLVMIRTranslation.h\"\n #include \"triton/Target/PTX/PTXTranslation.h\"\n+#include \"triton/tools/sys/getenv.hpp\"\n \n #include \"llvm/IR/LegacyPassManager.h\"\n #include \"llvm/IR/Module.h\"\n@@ -100,14 +101,6 @@ long pow2_divisor(long N) {\n   return 1;\n }\n \n-bool getBoolEnv(const std::string &env) {\n-  const char *s = std::getenv(env.c_str());\n-  std::string str(s ? s : \"\");\n-  std::transform(str.begin(), str.end(), str.begin(),\n-                 [](unsigned char c) { return std::tolower(c); });\n-  return (str == \"on\" || str == \"true\" || str == \"1\");\n-}\n-\n // Returns something like \"int16\", whether dtype is a torch.dtype or\n // triton.language.dtype.\n std::string dtype_cache_key_part(const py::object &dtype) {\n@@ -229,7 +222,7 @@ void parse_args(py::list &args, py::list do_not_specialize,\n       // copy param\n       std::memcpy(params_ptr, &value, 8);\n       params_ptr += 8;\n-      // udpate cache key\n+      // update cache key\n       cache_key += dtype_cache_key_part(arg.attr(\"dtype\"));\n       cache_key += \"*\";\n       cache_key += \"[multipleof(\";\n@@ -330,7 +323,7 @@ void parse_args(py::list &args, py::list &arg_names, std::string &params,\n       // copy param\n       std::memcpy(params_ptr, &value, 8);\n       params_ptr += 8;\n-      // udpate cache key\n+      // update cache key\n       continue;\n     }\n     // argument is `constexpr`\n@@ -1245,13 +1238,13 @@ void init_triton_ir(py::module &&m) {\n              return mlir::Value(\n                  self.create<mlir::arith::ShRSIOp>(loc, lhs, rhs));\n            })\n-      // GEP\n-      .def(\"create_gep\",\n+      // AddPtr (similar to GEP)\n+      .def(\"create_addptr\",\n            [](mlir::OpBuilder &self, mlir::Value &ptr,\n               mlir::Value &offset) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             return self.create<mlir::triton::GEPOp>(loc, ptr.getType(), ptr,\n-                                                     offset);\n+             return self.create<mlir::triton::AddPtrOp>(loc, ptr.getType(), ptr,\n+                                                        offset);\n            })\n       // Comparison (int)\n       .def(\"create_icmpSLE\",\n@@ -1635,7 +1628,7 @@ void init_triton_ir(py::module &&m) {\n                  /*shouldPrintBeforePass=*/nullptr,\n                  /*shouldPrintAfterPass=*/\n                  [](mlir::Pass *pass, mlir::Operation *) {\n-                   return getBoolEnv(\"MLIR_ENABLE_DUMP\");\n+                   return ::triton::tools::getBoolEnv(\"MLIR_ENABLE_DUMP\");\n                  },\n                  /*printModuleScope=*/false,\n                  /*printAfterOnlyOnChange=*/true,"}, {"filename": "python/tests/test_cast.py", "status": "added", "additions": 56, "deletions": 0, "changes": 56, "file_content_changes": "@@ -0,0 +1,56 @@\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def cast_check():\n+    zero_0d = tl.zeros([], dtype=tl.float32)\n+    zero_1d = tl.zeros([2], dtype=tl.float32)\n+    zero_2d_21 = tl.zeros([2, 1], dtype=tl.float32)\n+    zero_2d_22 = tl.zeros([2, 2], dtype=tl.float32)\n+\n+    # scalar + scalar -> scalar\n+    a0 = 0.0 + 0.0\n+    # scalar + 0D -> 0D\n+    a1 = 0.0 + zero_0d\n+    a2 = zero_0d + 0.0\n+    # scalar + 1D -> 1D\n+    a3 = 0.0 + zero_1d\n+    a4 = zero_1d + 0.0\n+    # scalar + 2D -> 2D\n+    a5 = 0.0 + zero_2d_22\n+    a6 = zero_2d_22 + 0.0\n+\n+    # 0D + 0D -> 0D\n+    b1 = zero_0d + zero_0d\n+    # 0D + 1D -> 1D\n+    b2 = zero_0d + zero_1d\n+    b3 = zero_1d + zero_0d\n+    # 0D + 2D -> 2D\n+    b4 = zero_0d + zero_2d_22\n+    b5 = zero_2d_22 + zero_0d\n+\n+    # 1D + 1D -> 1D\n+    c1 = zero_1d + zero_1d\n+    # 1D + 2D -> 2D\n+    c2 = zero_1d + zero_2d_21\n+    c3 = zero_1d + zero_2d_22\n+    c4 = zero_2d_21 + zero_1d\n+    c5 = zero_2d_22 + zero_1d\n+\n+    # 2D + 2D -> 2D\n+    d1 = zero_2d_21 + zero_2d_21\n+    d2 = zero_2d_22 + zero_2d_22\n+    d3 = zero_2d_21 + zero_2d_22\n+    d4 = zero_2d_22 + zero_2d_21\n+\n+    return a0, a1, a2, a3, a4, a5, a6, b1, b2, b3, b4, b5, c1, c2, c3, c4, c5, d1, d2, d3, d4\n+\n+\n+def test_cast_check():\n+    kernel = triton.compile(cast_check,\n+                            signature=\"\",\n+                            device=0,\n+                            output=\"ttir\")\n+    assert (kernel)\n+    # TODO: Check types of the results"}, {"filename": "python/tests/test_transpose.py", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "file_content_changes": "@@ -0,0 +1,68 @@\n+import pytest\n+import torch\n+from torch.testing import assert_allclose\n+\n+import triton\n+import triton.language as tl\n+import triton.runtime as runtime\n+\n+\n+@triton.jit\n+def kernel(x_ptr, stride_xm,\n+           z_ptr, stride_zn,\n+           SIZE_M: tl.constexpr, SIZE_N: tl.constexpr):\n+    off_m = tl.arange(0, SIZE_M)\n+    off_n = tl.arange(0, SIZE_N)\n+    Xs = x_ptr + off_m[:, None] * stride_xm + off_n[None, :] * 1\n+    Zs = z_ptr + off_m[:, None] * 1 + off_n[None, :] * stride_zn\n+    tl.store(Zs, tl.load(Xs))\n+\n+# These sizes cover the case of:\n+# - blocked layout and sliced layout with block parent\n+#  -- blocked layout in which sizePerThread/threadsPerWarp/warpsPerCTA\n+#     need/need not to be wrapped\n+#  -- sliced layout incase sizePerThread need to be wrapped\n+#  -- different orders\n+# - LayoutConversion from blocked -> blocked\n+# - tt.Broadcast which requires for broadcast in either/both of\n+#   CTA/perThread level\n+\n+# What is not covered and requires for TODO:\n+# - vectorization load/store of shared memory\n+# - multiple replication of layout conversion\n+\n+\n+@pytest.mark.parametrize('NUM_WARPS,SIZE_M,SIZE_N', [\n+    [1, 16, 16],\n+    [1, 32, 32],\n+    [1, 32, 64],\n+    [2, 64, 128],\n+    [2, 128, 64]\n+])\n+def test_convert_layout_impl(NUM_WARPS, SIZE_M, SIZE_N):\n+    # TODO: this is to initialize the cuda context since it is not properly\n+    #       dealed with in the existing runtime, remove this when the runtime\n+    #       is updated\n+    torch.zeros([10], device=torch.device('cuda'))\n+    device = torch.cuda.current_device()\n+    binary = runtime.build_kernel(kernel,\n+                                  \"*fp32,i32,*fp32,i32\",\n+                                  constants={\"SIZE_M\": SIZE_M,\n+                                             \"SIZE_N\": SIZE_N},\n+                                  num_warps=NUM_WARPS,\n+                                  num_stages=3)\n+    grid = lambda META: (1, )\n+\n+    x = torch.randn((SIZE_M, SIZE_N), device='cuda', dtype=torch.float32)\n+    z = torch.empty((SIZE_N, SIZE_M), device=x.device, dtype=x.dtype)\n+    runtime.launch_kernel(kernel=binary,\n+                          device=device,\n+                          grid=grid,\n+                          x_ptr=x,\n+                          stride_xm=x.stride(0),\n+                          z_ptr=z,\n+                          stride_zn=z.stride(0),\n+                          SIZE_M=tl.constexpr(SIZE_M),\n+                          SIZE_N=tl.constexpr(SIZE_N))\n+    golden_z = torch.t(x)\n+    assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)"}, {"filename": "python/tests/test_vecadd_no_scf.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -48,6 +48,7 @@ def kernel(x_ptr,\n \n \n def test_vecadd_no_scf():\n+    vecadd_no_scf_tester(num_warps=4, block_size=256)\n     vecadd_no_scf_tester(num_warps=2, block_size=256)\n     vecadd_no_scf_tester(num_warps=1, block_size=256)\n "}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -53,7 +53,7 @@ def mangle_ty(ty):\n         elt = mangle_ty(ty.scalar)\n         shape = '_'.join(map(str, ty.shape))\n         return f'{elt}S{shape}S'\n-    assert False, \"Unsupport type\"\n+    assert False, \"Unsupported type\"\n \n \n def mangle_fn(name, arg_tys, constants):\n@@ -464,7 +464,7 @@ def visit_While(self, node):\n         with enter_sub_region(self) as sr:\n             liveins, insert_block = sr\n \n-            # condtion (the before region)\n+            # condition (the before region)\n             cond_block = self.builder.create_block()\n             self.builder.set_insertion_point_to_start(cond_block)\n             cond = self.visit(node.test)\n@@ -752,8 +752,11 @@ def make_triton_ir(fn, signature, constants=dict(), attributes=dict()):\n     # create kernel prototype\n     constants = {fn.arg_names.index(name): value for name, value in constants.items()}\n     attributes = {fn.arg_names.index(name): value for name, value in attributes.items()}\n-    arg_types = signature.replace(' ', '').split(',')\n-    arg_types = [str_to_ty(x) for x in arg_types]\n+    if signature.replace(' ', '') != '':\n+        arg_types = signature.replace(' ', '').split(',')\n+        arg_types = [str_to_ty(x) for x in arg_types]\n+    else:\n+        arg_types = []\n     prototype = triton.language.function_type([], arg_types)\n     # visit kernel AST\n     gscope = fn.__globals__.copy()"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -185,7 +185,7 @@ def to_ir(self, builder: ir.builder) -> ir.type:\n             return builder.get_float_ty()\n         elif self.name == 'fp64':\n             return builder.get_double_ty()\n-        raise ValueError(f'fail to covert {self} to ir type')\n+        raise ValueError(f'fail to convert {self} to ir type')\n \n     def __str__(self):\n         return self.name\n@@ -239,8 +239,9 @@ def __init__(self, element_ty: dtype, shape: List):\n \n         # Note that block_type's shape is a list of int\n         # while tensor's shape is a list of constexpr.\n-        assert shape\n-        if isinstance(shape[0], constexpr):\n+\n+        # shape can be empty ([]) when an input is a 0D tensor.\n+        if shape and isinstance(shape[0], constexpr):\n             shape = [s.value for s in shape]\n \n         self.shape = shape\n@@ -894,7 +895,7 @@ def where(condition, x, y, _builder=None):\n \n     Note that :code:`x` and :code:`y` are always evaluated regardless of the value of :code:`condition`.\n \n-    If you want to avoid unintented memory operations, use the :code:`mask` arguments in `triton.load` and `triton.store` instead.\n+    If you want to avoid unintended memory operations, use the :code:`mask` arguments in `triton.load` and `triton.store` instead.\n \n     The shape of :code:`x` and :code:`y` are both broadcast to the shape of :code:`condition`.\n     :code:`x` and :code:`y` must have the data type."}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 18, "deletions": 8, "changes": 26, "file_content_changes": "@@ -121,7 +121,7 @@ def add(input: tl.tensor,\n     if other_scalar_ty.is_ptr() and not input_scalar_ty.is_ptr():\n         input, other = other, input\n     if input_scalar_ty.is_ptr():\n-        return tl.tensor(builder.create_gep(input.handle, other.handle), input.type)\n+        return tl.tensor(builder.create_addptr(input.handle, other.handle), input.type)\n     # float + float\n     elif input_scalar_ty.is_floating():\n         return tl.tensor(builder.create_fadd(input.handle, other.handle), input.type)\n@@ -138,7 +138,7 @@ def sub(input: tl.tensor,\n     scalar_ty = input.type.scalar\n     # ptr - offset\n     if scalar_ty.is_ptr():\n-        return tl.tensor(builder.create_gep(input.handle, minus(other, builder).handle),\n+        return tl.tensor(builder.create_addptr(input.handle, minus(other, builder).handle),\n                          input.type)\n     # float - float\n     if scalar_ty.is_floating():\n@@ -508,8 +508,21 @@ def broadcast_impl_value(lhs: tl.tensor,\n     elif lhs_ty.is_block() and rhs_ty.is_block():\n         lhs_shape = lhs_ty.get_block_shapes()\n         rhs_shape = rhs_ty.get_block_shapes()\n-        if len(lhs_shape) != len(rhs_shape):\n-            raise ValueError(\"Cannot make_shape_compatible: blocks must have the same rank\")\n+\n+        if len(lhs_shape) < len(rhs_shape):\n+            # Add new axes to lhs\n+            for dim in range(len(lhs_shape), len(rhs_shape)):\n+                lhs = tl.tensor(builder.create_expand_dims(lhs.handle, dim), tl.block_type(lhs_ty.scalar, lhs_shape + [1]))\n+                lhs_ty = lhs.type\n+                lhs_shape = lhs_ty.get_block_shapes()\n+        elif len(rhs_shape) < len(lhs_shape):\n+            # Add new axes to rhs\n+            for dim in range(len(rhs_shape), len(lhs_shape)):\n+                rhs = tl.tensor(builder.create_expand_dims(rhs.handle, dim), tl.block_type(rhs_ty.scalar, rhs_shape + [1]))\n+                rhs_ty = rhs.type\n+                rhs_shape = rhs_ty.get_block_shapes()\n+        assert len(rhs_shape) == len(lhs_shape)\n+\n         ret_shape = []\n         for i in range(len(lhs_shape)):\n             left = lhs_shape[i]\n@@ -962,10 +975,7 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n     for i, s in enumerate(shape):\n         if i != axis:\n             ret_shape.append(s)\n-    if len(ret_shape) == 0:\n-        res_ty = scalar_ty\n-    else:\n-        res_ty = tl.block_type(scalar_ty, ret_shape)\n+    res_ty = tl.block_type(scalar_ty, ret_shape)\n \n     if scalar_ty.is_floating():\n         return tl.tensor(builder.create_reduce(input.handle, FLOAT_OP, axis), res_ty)"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -328,7 +328,7 @@ def dsd_lut(layout, block, step, trans, device):\n     # create increments\n     incs = torch.stack((B_incs, A_incs), dim=1).view(-1).contiguous()\n     # pad by a factor 2*MAX_NUM_STAGES\n-    # to accomodate pre-fetching inside the kernel\n+    # to accommodate pre-fetching inside the kernel\n     pad = torch.zeros(20, device=incs.device, dtype=incs.dtype)\n     incs = torch.cat((incs, pad))\n     # create lut"}, {"filename": "python/triton/tools/disasm.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -104,7 +104,7 @@ def extract(file_path, fun):\n             # peek the next line\n             line = sass_lines[line_idx].decode()\n         # Print sass\n-        # label naming convension: LBB#i\n+        # label naming convention: LBB#i\n         for idx, (ctrl, asm) in enumerate(asm_buffer):\n             # Print label if this is BRA target\n             offset = idx * 16"}, {"filename": "python/tutorials/02-fused-softmax.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -78,7 +78,7 @@ def softmax_kernel(\n     input_ptrs = row_start_ptr + col_offsets\n     # Load the row into SRAM, using a mask since BLOCK_SIZE may be > than n_cols\n     row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n-    # Substract maximum for numerical stability\n+    # Subtract maximum for numerical stability\n     row_minus_max = row - tl.max(row, axis=0)\n     # Note that exponentials in Triton are fast but approximate (i.e., think __expf in CUDA)\n     numerator = tl.exp(row_minus_max)"}, {"filename": "python/tutorials/03-matrix-multiplication.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -18,7 +18,7 @@\n # They are notoriously hard to optimize, hence their implementation is generally done by\n # hardware vendors themselves as part of so-called \"kernel libraries\" (e.g., cuBLAS).\n # Unfortunately, these libraries are often proprietary and cannot be easily customized\n-# to accomodate the needs of modern deep learning workloads (e.g., fused activation functions).\n+# to accommodate the needs of modern deep learning workloads (e.g., fused activation functions).\n # In this tutorial, you will learn how to implement efficient matrix multiplications by\n # yourself with Triton, in a way that is easy to customize and extend.\n #"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 46, "deletions": 72, "changes": 118, "file_content_changes": "@@ -26,8 +26,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.getelementptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return\n@@ -37,7 +37,9 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  // CHECK: %0 -> %0\n+  %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A>\n   return\n }\n \n@@ -49,25 +51,28 @@ func @convert(%A : !tt.ptr<f16>) {\n   return\n }\n \n-// CHECK-LABEL: copy_async\n-func @copy_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+// CHECK-LABEL: insert_slice_async\n+func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n-  // CHECK: %2 -> %2\n-  %a = triton_gpu.copy_async %a_ptr, %mask, %other {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<16x16xf16, #A>\n+  // CHECK: %cst_0 -> %cst_0\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %index = arith.constant 0 : i32\n+  // CHECK: %2 -> %cst_0\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n   return\n }\n \n-// COM:  Enable the following test once we support view on shared memory tensors\n-// COM: // CHECK-LABEL: view\n-// COM: func @view(%A : !tt.ptr<f16>) {\n-// COM:   // CHECK: res0:0 -> 0\n-// COM:   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-// COM:   // CHECK-NEXT: res1:0 -> 0\n-// COM:   %cst1 = tt.view %cst0 : (tensor<16x16xf16, #A>) -> tensor<32x8xf16, #A>\n-// COM:   return\n-// COM: }\n+// CHECK-LABEL: extract_slice\n+func @extract_slice(%A : !tt.ptr<f16>) {\n+  // CHECK: %cst -> %cst\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %index = arith.constant 0 : i32\n+  // CHECK-NEXT: %0 -> %cst\n+  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A> -> tensor<16x16xf16, #A>\n+  return\n+}\n \n // CHECK-LABEL: if_cat\n func @if_cat(%i1 : i1) {\n@@ -123,62 +128,31 @@ func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.p\n   return\n }\n \n-// COM: // Enable the following test once we support view on shared memory tensors\n-// COM: // CHECK-LABEL: for_if\n-// COM: func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n-// COM:   // CHECK: res0:0 -> 0\n-// COM:   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-// COM:   // CHECK-NEXT: res1:0 -> 1\n-// COM:   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-// COM:   // CHECK-NEXT: res2:0 -> 2\n-// COM:   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-// COM:   // CHECK-NEXT: arg3:0 -> 0\n-// COM:   // CHECK-NEXT: arg3:1 -> 1\n-// COM:   // CHECK-NEXT: arg3:2 -> 2\n-// COM:   // CHECK-NEXT: res3:0 -> 0,1\n-// COM:   // CHECK-NEXT: res3:1 -> 0,1\n-// COM:   // CHECK-NEXT: res3:2 -> 0,1\n-// COM:   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n-// COM:     scf.if %i1 {\n-// COM:       // CHECK-NEXT: res5:0 -> 0,1\n-// COM:       %cst0 = tt.view %a_shared : (tensor<128x32xf16, #A>) -> tensor<32x128xf16, #A>\n-// COM:       scf.yield\n-// COM:     }\n-// COM:     scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n-// COM:   }\n-// COM:   return\n-// COM: }\n-\n-// COM: // Enable the following test once we support view on shared memory tensors\n-// COM: // CHECK-LABEL: for_if_else\n-// COM: func @for_if_else(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n-// COM:   // CHECK: res0:0 -> 0\n-// COM:   %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-// COM:   // CHECK-NEXT: res1:0 -> 1\n-// COM:   %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-// COM:   // CHECK-NEXT: res2:0 -> 2\n-// COM:   %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-// COM:   // CHECK-NEXT: arg3:0 -> 0\n-// COM:   // CHECK-NEXT: arg3:1 -> 1\n-// COM:   // CHECK-NEXT: arg3:2 -> 2\n-// COM:   // CHECK-NEXT: res3:0 -> 0\n-// COM:   // CHECK-NEXT: res3:1 -> 1\n-// COM:   // CHECK-NEXT: res3:2 -> 0,7\n-// COM:   %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n-// COM:     // CHECK-NEXT: res4:0 -> 0,7\n-// COM:     %c_shared_next = scf.if %i1 -> tensor<128x32xf16, #A> {\n-// COM:       // CHECK-NEXT: res5:0 -> 0\n-// COM:       %cst0 = tt.view %a_shared : (tensor<128x32xf16, #A>) -> tensor<128x32xf16, #A>\n-// COM:       scf.yield %cst0 : tensor<128x32xf16, #A>\n-// COM:     } else {\n-// COM:       // CHECK-NEXT: res7:0 -> 7\n-// COM:       %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-// COM:       scf.yield %cst0 : tensor<128x32xf16, #A>\n-// COM:     }\n-// COM:     scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n-// COM:   }\n-// COM:   return\n-// COM: }\n+// CHECK-LABEL: for_if\n+func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  // CHECK: %cst -> %cst\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  // CHECK-NEXT: %cst_0 -> %cst_0\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  // CHECK-NEXT: %cst_1 -> %cst_1\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  // CHECK-NEXT: %arg7 -> %cst\n+  // CHECK-NEXT: %arg8 -> %cst_0\n+  // CHECK-NEXT: %arg9 -> %cst_1\n+  // CHECK-NEXT: %0#0 -> %cst,%cst_0\n+  // CHECK-NEXT: %0#1 -> %cst,%cst_0\n+  // CHECK-NEXT: %0#2 -> %cst,%cst_0\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n+    scf.if %i1 {\n+      %index = arith.constant 8 : i32\n+      // CHECK-NEXT: %1 -> %cst,%cst_0\n+      %cst0 = triton_gpu.extract_slice %a_shared, %index { axis = 0 : i32 } : tensor<128x32xf16, #A> -> tensor<32xf16, #A>\n+      scf.yield\n+    }\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+  }\n+  return\n+}\n \n // CHECK-LABEL: for_if_for\n func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {"}, {"filename": "test/Analysis/test-alignment.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -18,21 +18,21 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n   %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 1]\n-  %6 = tt.getelementptr %5, %4 : tensor<128x1x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %4 : tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n   %7 = tt.expand_dims %1 {axis = 0 : i32}: (tensor<128xi32>) -> tensor<1x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n   %8 = tt.broadcast %6 : (tensor<128x1x!tt.ptr<f32>>) -> tensor<128x128x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [128, 1]\n   %9 = tt.broadcast %7 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 16] ; Constancy: [1, 1]\n-  %10 = tt.getelementptr %8, %9 : tensor<128x128x!tt.ptr<f32>>\n+  %10 = tt.addptr %8, %9 : tensor<128x128x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [65536, 1] ; Constancy: [1, 1]\n   %11 = tt.expand_dims %0 {axis = 1 : i32}: (tensor<128xi32>) -> tensor<128x1xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [128, 1]\n   %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n-  %13 = tt.getelementptr %12, %11 : tensor<128x1x!tt.ptr<f32>>\n+  %13 = tt.addptr %12, %11 : tensor<128x1x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 128] ; Divisibility: [1, 65536] ; Constancy: [1, 1]\n   %14 = tt.expand_dims %1 {axis = 0 : i32} : (tensor<128xi32>) -> tensor<1x128xi32>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 16] ; Constancy: [1, 128]\n@@ -44,7 +44,7 @@ func @permute_2d(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {t\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [16, 1048576] ; Constancy: [128, 1]\n   %18 = tt.broadcast %16 : (tensor<1x128xi32>) -> tensor<128x128xi32>\n   // CHECK-NEXT: Contiguity: [128, 1] ; Divisibility: [16, 1] ; Constancy: [1, 1]\n-  %19 = tt.getelementptr %17, %18 : tensor<128x128x!tt.ptr<f32>>\n+  %19 = tt.addptr %17, %18 : tensor<128x128x!tt.ptr<f32>>\n   // CHECK-NEXT: Contiguity: [1, 1] ; Divisibility: [1, 1] ; Constancy: [1, 1]\n   %20 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xf32>\n   tt.store %19, %20, %cst : tensor<128x128xf32>"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 56, "deletions": 2, "changes": 58, "file_content_changes": "@@ -30,8 +30,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.getelementptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return\n@@ -149,6 +149,17 @@ func @longlive(%A : !tt.ptr<f16>) {\n   // CHECK-NEXT: size = 2560\n }\n \n+// CHECK-LABEL: alloc\n+func @alloc(%A : !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 512\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n+  // CHECK-NEXT: offset = 0, size = 512\n+  %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A>\n+  return\n+  // CHECK-NEXT: size = 512\n+}\n+\n // CHECK-LABEL: scratch\n func @scratch() {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n@@ -158,6 +169,29 @@ func @scratch() {\n   // CHECK-NEXT: size = 512\n }\n \n+// CHECK-LABEL: insert_slice_async\n+func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+  %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n+  %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n+  %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  // CHECK: offset = 0, size = 512\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %index = arith.constant 0 : i32\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n+  return\n+  // CHECK-NEXT: size = 512\n+}\n+\n+// CHECK-LABEL: extract_slice\n+func @extract_slice(%A : !tt.ptr<f16>) {\n+  // CHECK: offset = 0, size = 512\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %index = arith.constant 0 : i32\n+  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A> -> tensor<16x16xf16, #A>\n+  return\n+  // CHECK-NEXT: size = 512\n+}\n+\n // B0 -> (B1) -> B0\n // Memory used by B1 can be reused by B0.\n // CHECK-LABEL: if\n@@ -226,6 +260,26 @@ func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.p\n   // CHECK-NEXT: size = 24576\n }\n \n+// CHECK-LABEL: for_if_slice\n+func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n+  // CHECK: offset = 0, size = 8192\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  // CHECK-NEXT: offset = 8192, size = 8192\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  // CHECK-NEXT: offset = 16384, size = 8192\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n+    scf.if %i1 {\n+      %index = arith.constant 8 : i32\n+      %cst0 = triton_gpu.extract_slice %a_shared, %index { axis = 0 : i32 } : tensor<128x32xf16, #A> -> tensor<32xf16, #A>\n+      scf.yield\n+    }\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+  }\n+  return\n+  // CHECK-NEXT: size = 24576\n+}\n+\n // a_shared_init, b_shared_init, and c_shared_init's liveness ranges are span over the entire function before cst2.\n // So they cannot be reused by cst0 and cst1, but can be reused by cst2.\n // CHECK-LABEL: for_if_for"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 39, "deletions": 2, "changes": 41, "file_content_changes": "@@ -28,8 +28,8 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n     // CHECK: Membar 13\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.getelementptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return\n@@ -56,6 +56,8 @@ func @war_single_block(%A : !tt.ptr<f16>) {\n   %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n   // CHECK: Membar 5\n   %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A>) -> tensor<128x32xf16, #AL>\n+  // a2's liveness range ends here, and a3 and a2 have the same address range.\n+  // So it makes sense to have a WAR dependency between a2 and a3.\n   // CHECK-NEXT: Membar 7\n   %a3 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n   return\n@@ -82,6 +84,41 @@ func @async_wait() {\n   return\n }\n \n+// CHECK-LABEL: alloc\n+func @alloc() {\n+  %cst0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A>\n+  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  // CHECK: Membar 2\n+  %b = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A>) -> tensor<32x16xf16, #AL>\n+  return\n+}\n+\n+// CHECK-LABEL: extract_slice\n+func @extract_slice() {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %index = arith.constant 0 : i32\n+  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A> -> tensor<16x16xf16, #A>\n+  // CHECK: Membar 3\n+  %cst2 = triton_gpu.convert_layout %cst1 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  // CHECK-NEXT: Membar 5\n+  %cst3 = triton_gpu.convert_layout %cst2 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A>\n+  return\n+}\n+\n+// CHECK-LABEL: insert_slice_async\n+func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n+  %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n+  %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n+  %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n+  %tensor = triton_gpu.alloc_tensor : tensor<1x16x16xf16, #A>\n+  %index = arith.constant 0 : i32\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n+  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A>, tensor<1x16x16xf16, #A>) -> tensor<2x16x16xf16, #A>\n+  // CHECK: Membar 7\n+  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A>, tensor<2x16x16xf16, #A>) -> tensor<4x16x16xf16, #A>\n+  return\n+}\n+\n // If branch inserted a barrier for %cst0 and %cst1, but else didn't, then the barrier should be inserted in the parent region\n // CHECK-LABEL: multi_blocks\n func @multi_blocks(%i1 : i1) {"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "added", "additions": 55, "deletions": 0, "changes": 55, "file_content_changes": "@@ -0,0 +1,55 @@\n+// RUN: triton-opt %s | FileCheck %s\n+\n+func @cast_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_f32: f32, %scalar_i64: i64) {\n+  // scalar -> scalar\n+  // CHECK:  i64 -> !tt.ptr<f32>\n+  %0 = tt.int_to_ptr %scalar_i64 : i64 -> !tt.ptr<f32>\n+  // CHECK: !tt.ptr<f32> -> i64\n+  %1 = tt.ptr_to_int %scalar_ptr : !tt.ptr<f32> -> i64\n+  // CHECK: f32 -> f16\n+  %2 = tt.fp_to_fp %scalar_f32 : f32 -> f16\n+\n+  // 0D tensor -> 0D tensor\n+  %tensor_ptr_0d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<!tt.ptr<f32>>\n+  %tensor_f32_0d = tt.splat %scalar_f32 : (f32) -> tensor<f32>\n+  %tensor_i64_0d = tt.splat %scalar_i64 : (i64) -> tensor<i64>\n+\n+  // CHECK: tensor<i64> -> tensor<!tt.ptr<f32>>\n+  %3 = tt.int_to_ptr %tensor_i64_0d : tensor<i64> -> tensor<!tt.ptr<f32>>\n+  // CHECK: tensor<!tt.ptr<f32>> -> tensor<i64>\n+  %4 = tt.ptr_to_int %tensor_ptr_0d : tensor<!tt.ptr<f32>> -> tensor<i64>\n+  // CHECK: tensor<f32> -> tensor<f16>\n+  %5 = tt.fp_to_fp %tensor_f32_0d : tensor<f32> -> tensor<f16>\n+\n+  // 1D tensor -> 1D tensor\n+  %tensor_ptr_1d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>>\n+  %tensor_f32_1d = tt.splat %scalar_f32 : (f32) -> tensor<16xf32>\n+  %tensor_i64_1d = tt.splat %scalar_i64 : (i64) -> tensor<16xi64>\n+\n+  // CHECK: tensor<16xi64> -> tensor<16x!tt.ptr<f32>>\n+  %6 = tt.int_to_ptr %tensor_i64_1d : tensor<16xi64> -> tensor<16x!tt.ptr<f32>>\n+  // CHECK: tensor<16x!tt.ptr<f32>> -> tensor<16xi64>\n+  %7 = tt.ptr_to_int %tensor_ptr_1d : tensor<16x!tt.ptr<f32>> -> tensor<16xi64>\n+  // CHECK: tensor<16xf32> -> tensor<16xf16>\n+  %8 = tt.fp_to_fp %tensor_f32_1d : tensor<16xf32> -> tensor<16xf16>\n+  return\n+}\n+\n+func @addptr_ops(%scalar_ptr: !tt.ptr<f32>, %scalar_i32: i32) {\n+  // scalar -> scalar\n+  // CHECK: !tt.ptr<f32>\n+  %0 = tt.addptr %scalar_ptr, %scalar_i32 : !tt.ptr<f32>\n+\n+  // 0D tensor -> 0D tensor\n+  %tensor_ptr_0d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<!tt.ptr<f32>>\n+  %tensor_i32_0d = tt.splat %scalar_i32 : (i32) -> tensor<i32>\n+  // CHECK: tensor<!tt.ptr<f32>>\n+  %1 = tt.addptr %tensor_ptr_0d, %tensor_i32_0d : tensor<!tt.ptr<f32>>\n+\n+  // 1D tensor -> 1D tensor\n+  %tensor_ptr_1d = tt.splat %scalar_ptr : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>>\n+  %tensor_i32_1d = tt.splat %scalar_i32 : (i32) -> tensor<16xi32>\n+  // CHECK: tensor<16x!tt.ptr<f32>>\n+  %2 = tt.addptr %tensor_ptr_1d, %tensor_i32_1d : tensor<16x!tt.ptr<f32>>\n+  return\n+}"}, {"filename": "test/Conversion/triton_to_llvm.mlir", "status": "removed", "additions": 0, "deletions": 37, "changes": 37, "file_content_changes": "@@ -1,37 +0,0 @@\n-// RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu=num-warps=2 -convert-triton-gpu-to-llvm | FileCheck %s\n-\n-func @test_splat(%ptr: !tt.ptr<f32>) {\n-  // Here, 128 elements, 64(2*32) threads, so each need to process 2 elements\n-  //\n-  // CHECK: %0 = llvm.bitcast %arg0 : !llvm.ptr<f32, 1> to !llvm.ptr<f32, 1>\n-  // CHECK: %1 = llvm.mlir.undef : !llvm.struct<(ptr<f32, 1>, ptr<f32, 1>)>\n-  // CHECK: %2 = llvm.insertvalue %0, %1[0] : !llvm.struct<(ptr<f32, 1>, ptr<f32, 1>)>\n-  // CHECK: %3 = llvm.insertvalue %0, %2[1] : !llvm.struct<(ptr<f32, 1>, ptr<f32, 1>)>\n-  %ptrs = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n-  %a = arith.constant 1.0 : f32\n-  %true = arith.constant 1 : i1\n-  %b = tt.splat %a : (f32) -> tensor<128xf32>\n-\n-  // Here, each thread process only 1 element\n-  // CHECK: %{{.*}} = llvm.mlir.undef : !llvm.struct<(i1)>\n-  %mask = tt.splat %true : (i1) -> tensor<64xi1>\n-\n-  return\n-}\n-\n-// -----\n-\n-func @test_store_splat(%ptr: !tt.ptr<f32>) {\n-  %ptrs = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n-  %a = arith.constant 1.0 : f32\n-  %true = arith.constant 1 : i1\n-\n-  %vs = tt.splat %a : (f32) -> tensor<128xf32>\n-  %mask = tt.splat %true : (i1) -> tensor<128xi1>\n-\n-  // CHECK: %{{.*}} = llvm.inline_asm has_side_effects asm_dialect = att operand_attrs = [] \"@$1 st.global.b32 [ $2 + 0 ], { $0 };\",\n-  // CHECK-SAME: \"r,b,l\" %{{.*}}, %{{.*}}, %{{.*}} : (i32, i1, !llvm.ptr<f32, 1>) -> !llvm.void\n-  tt.store %ptrs, %vs, %mask : tensor<128xf32>\n-\n-  return\n-}"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "renamed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "test/Conversion/ops.mlir"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 232, "deletions": 26, "changes": 258, "file_content_changes": "@@ -1,16 +1,13 @@\n // RUN: triton-opt %s -split-input-file --convert-triton-gpu-to-llvm | FileCheck %s\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-\n-// CHECK: llvm.func @test_empty_kernel(%arg0: i32, %arg1: !llvm.ptr<f16, 1>)\n-// Here the 128 comes from the 4 in module attribute multiples 32\n-// CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = 128 : si32} {{.*}}\n-func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n-\n-  // CHECK:  llvm.return\n-  return\n-}\n-\n+  // CHECK: llvm.func @test_empty_kernel(%arg0: i32, %arg1: !llvm.ptr<f16, 1>)\n+  // Here the 128 comes from the 4 in module attribute multiples 32\n+  // CHECK:  attributes {nvvm.kernel = 1 : ui1, nvvm.maxntid = 128 : si32} {{.*}}\n+  func @test_empty_kernel(%lb : index, %A : !tt.ptr<f16>) {\n+    // CHECK:  llvm.return\n+    return\n+  }\n } // end module\n \n // -----\n@@ -58,7 +55,7 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n // -----\n \n-// TODO: Pending on the support of isSplat constant\n+// TODO: masked load with vectorization is pending on TODO\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: masked_load_const_other\n@@ -71,40 +68,138 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // -----\n \n+// TODO: masked load with vectorization is pending on TODO\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: masked_load_const_other_vec\n+  func @masked_load_const_other_vec(%a_ptr_init : tensor<256x!tt.ptr<f32>, #blocked0>, %cst : tensor<256xi1, #blocked0>) {\n+    %cst_0 = arith.constant dense<0.000000e+00> : tensor<256xf32, #blocked0>\n+    %1 = tt.load %a_ptr_init, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n-  // CHECK-LABEL: kernel__Pfp32_Pfp32_Pfp32_i32__3c256\n-  func @kernel__Pfp32_Pfp32_Pfp32_i32__3c256(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+  // CHECK-LABEL: global_load_store_no_vec\n+  func @global_load_store_no_vec(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n     %c256_i32 = arith.constant 256 : i32\n     %0 = tt.get_program_id {axis = 0 : i32} : i32\n     %1 = arith.muli %0, %c256_i32 : i32\n     %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n     %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n     %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n     %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %6 = tt.getelementptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n     %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %8 = tt.getelementptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n-\n-    // CHECK: ld.global.v4.b32\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Load 4 elements from vector0\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 4 elements from vector1\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: \"@${{.*}} ld.global.b32 { ${{.*}} }, [ ${{.*}} + 0 ];\n     %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n-    // CHECK: ld.global.v4.b32\n     %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n     %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n-    %13 = tt.getelementptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n \n     // Store 4 elements to global\n-    // CHECK: st.global.b32.v4\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n+    // CHECK: @${{.*}} st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     tt.store %13, %11 : tensor<256xf32, #blocked0>\n     return\n   }\n }\n \n+// -----\n \n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n+  // CHECK-LABEL: global_load_store_vec4\n+  func @global_load_store_vec4(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+    %c256_i32 = arith.constant 256 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c256_i32 : i32\n+    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n+    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Load 4 elements from A with single one vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 4 elements from B with single one vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n+    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Store 4 elements to global with single one vectorized store instruction\n+    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    tt.store %13, %11 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [8], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: global_load_store_vec8\n+    func @global_load_store_vec8(%arg0: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 4 : i32}, %arg3: i32) {\n+    %c256_i32 = arith.constant 256 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.muli %0, %c256_i32 : i32\n+    %2 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n+    %3 = tt.splat %1 : (i32) -> tensor<256xi32, #blocked0>\n+    %4 = arith.addi %3, %2 : tensor<256xi32, #blocked0>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %6 = tt.addptr %5, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %7 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Load 8 elements from A with two vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    // Load 8 elements from B with two vectorized load instruction\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+    // CHECK: @${{.*}} ld.global.v4.b32 { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} }, [ ${{.*}} + 0 ];\n+\n+    %9 = tt.load %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %10 = tt.load %8 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #blocked0>\n+    %11 = arith.addf %9, %10 : tensor<256xf32, #blocked0>\n+    %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #blocked0>\n+    %13 = tt.addptr %12, %4 : tensor<256x!tt.ptr<f32>, #blocked0>\n+\n+    // Store 8 elements to global with two vectorized store instruction\n+    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    // CHECK: @$5 st.global.b32.v4 [ ${{.*}} + 0 ], { ${{.*}}, ${{.*}}, ${{.*}}, ${{.*}} };\n+    tt.store %13, %11 : tensor<256xf32, #blocked0>\n+    return\n+  }\n+}\n \n // TODO: Add a testcase to verify the optimization when ptr of the LoadOp\n-//       is from a GEP with const idx\n+//       is from an addptr with const idx\n \n // -----\n \n@@ -187,11 +282,11 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK-LABEL: basic_gep\n-  func @basic_gep(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n+  // CHECK-LABEL: basic_addptr\n+  func @basic_addptr(%arg0 : tensor<256x!tt.ptr<f32>,#blocked0>, %arg1 : tensor<256xi32,#blocked0>) {\n     // CHECK: llvm.getelementptr\n     // CHECK: llvm.getelementptr\n-    %0 = tt.getelementptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>\n+    %0 = tt.addptr %arg0, %arg1 : tensor<256x!tt.ptr<f32>, #blocked0>\n     return\n   }\n }\n@@ -217,10 +312,121 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: basic_store\n   func @basic_store(%ptrs: tensor<256x!tt.ptr<f32>, #blocked0>, %vals: tensor<256xf32, #blocked0>, %mask: tensor<256xi1, #blocked0>) {\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n-    // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\", \"r,b,l\" %{{.*}}, %{{.*}}, %{{.*}} : (i32, i1, !llvm.ptr<f32, 1>) -> !llvm.void\n+    // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     // CHECK: llvm.inline_asm has_side_effects asm_dialect = att\n-    // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\", \"r,b,l\" %{{.*}}, %{{.*}}, %{{.*}} : (i32, i1, !llvm.ptr<f32, 1>) -> !llvm.void\n+    // CHECK-SAME: st.global.b32 [ ${{.*}} + 0 ], { ${{.*}} };\n     tt.store %ptrs, %vals, %mask : tensor<256xf32, #blocked0>\n     return\n   }\n }\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [0, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<1088 x i8>\n+  // CHECK-LABEL: convert_layout_blocked_blocked\n+  func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n+    // CHECK: llvm.mlir.addressof @global_smem\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [16, 2], warpsPerCTA = [1, 1], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<1280 x i8>\n+  // CHECK-LABEL: convert_layout_blocked_blocked_vec\n+  func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n+    // CHECK: llvm.mlir.addressof @global_smem\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n+    return\n+  }\n+}\n+\n+// -----\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK: llvm.mlir.global internal @global_smem() {addr_space = 3 : i32} : !llvm.array<640 x i8>\n+  // CHECK-LABEL: convert_layout_blocked_blocked_multi_rep\n+  func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n+    // CHECK: llvm.mlir.addressof @global_smem\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.store\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: nvvm.barrier0\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    // CHECK: llvm.load\n+    // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #blocked1>\n+    return\n+  }\n+}\n+\n+// TODO: problems in MLIR's parser on slice layout\n+// #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+// module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+//   func @make_range_sliced_layout() {\n+//     %0 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked0}>>\n+//     return\n+//   }\n+// }\n\\ No newline at end of file"}, {"filename": "test/Triton/combine.mlir", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "file_content_changes": "@@ -22,8 +22,8 @@ func @test_combine_dot_add_pattern() -> (tensor<128x128xf32>, tensor<128x128xf32\n     return %res0, %res1 : tensor<128x128xf32>, tensor<128x128xf32>\n }\n \n-// CHECK-LABEL: @test_combine_gep_pattern\n-func @test_combine_gep_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n+// CHECK-LABEL: @test_combine_addptr_pattern\n+func @test_combine_addptr_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n     %off0 = arith.constant 10 : i32\n     %off1 = arith.constant 15 : i32\n \n@@ -37,9 +37,9 @@ func @test_combine_gep_pattern(%base: !tt.ptr<f32>) -> tensor<8x!tt.ptr<f32>> {\n     %idx0 = tt.broadcast %off0 : (i32) -> tensor<8xi32>\n     %idx1 = tt.broadcast %off1 : (i32) -> tensor<8xi32>\n \n-    // CHECK-NEXT: %1 = tt.getelementptr %[[tmp0]], %[[cst]] : tensor<8x!tt.ptr<f32>>\n-    %ptr0 = tt.getelementptr %base_, %idx0 : tensor<8x!tt.ptr<f32>>\n-    %ptr1 = tt.getelementptr %ptr0, %idx1 : tensor<8x!tt.ptr<f32>>\n+    // CHECK-NEXT: %1 = tt.addptr %[[tmp0]], %[[cst]] : tensor<8x!tt.ptr<f32>>\n+    %ptr0 = tt.addptr %base_, %idx0 : tensor<8x!tt.ptr<f32>>\n+    %ptr1 = tt.addptr %ptr0, %idx1 : tensor<8x!tt.ptr<f32>>\n \n     return %ptr1 : tensor<8x!tt.ptr<f32>>\n }"}, {"filename": "test/Triton/vecadd.mlir", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -11,9 +11,9 @@ module {\n     %5 = tt.broadcast %arg3 : (i32) -> tensor<256xi32>\n     %6 = arith.cmpi slt, %4, %5 : tensor<256xi32>\n     %7 = tt.broadcast %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %8 = tt.getelementptr %7, %4 : tensor<256x!tt.ptr<f32>>\n+    %8 = tt.addptr %7, %4 : tensor<256x!tt.ptr<f32>>\n     %9 = tt.broadcast %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %10 = tt.getelementptr %9, %4 : tensor<256x!tt.ptr<f32>>\n+    %10 = tt.addptr %9, %4 : tensor<256x!tt.ptr<f32>>\n     %cst = arith.constant 0.000000e+00 : f32\n     %11 = tt.broadcast %cst : (f32) -> tensor<256xf32>\n     %c0_i32 = arith.constant 0 : i32\n@@ -31,13 +31,13 @@ module {\n       %22 = arith.addf %19, %21 : tensor<256xf32>\n       %23 = arith.addf %arg7, %22 : tensor<256xf32>\n       %24 = tt.broadcast %arg5 : (i32) -> tensor<256xi32>\n-      %25 = tt.getelementptr %arg8, %24 : tensor<256x!tt.ptr<f32>>\n+      %25 = tt.addptr %arg8, %24 : tensor<256x!tt.ptr<f32>>\n       %26 = tt.broadcast %arg5 : (i32) -> tensor<256xi32>\n-      %27 = tt.getelementptr %arg9, %26 : tensor<256x!tt.ptr<f32>>\n+      %27 = tt.addptr %arg9, %26 : tensor<256x!tt.ptr<f32>>\n       scf.yield %23, %25, %27 : tensor<256xf32>, tensor<256x!tt.ptr<f32>>, tensor<256x!tt.ptr<f32>>\n     }\n     %16 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>>\n-    %17 = tt.getelementptr %16, %4 : tensor<256x!tt.ptr<f32>>\n+    %17 = tt.addptr %16, %4 : tensor<256x!tt.ptr<f32>>\n     tt.store %17, %15#0, %6 : tensor<256xf32>\n     return\n   }\n@@ -57,9 +57,9 @@ module {\n //     %5 = tt.broadcast %arg3 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %6 = \"triton_gpu.cmpi\"(%4, %5) {predicate = 2 : i64} : (tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %7 = tt.broadcast %arg0 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %8 = tt.getelementptr %7, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %8 = tt.addptr %7, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %9 = tt.broadcast %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %10 = tt.getelementptr %9, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %10 = tt.addptr %9, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %11 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %12 = arith.index_cast %arg4 : i32 to index\n //     %13 = arith.cmpi slt, %c0, %12 : index\n@@ -72,9 +72,9 @@ module {\n //     %20 = arith.andi %6, %19 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %21 = triton_gpu.copy_async %10, %20, %18 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %22 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %23 = tt.getelementptr %8, %22, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %23 = tt.addptr %8, %22, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %24 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %25 = tt.getelementptr %10, %24, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %25 = tt.addptr %10, %24, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %26 = arith.cmpi slt, %c32, %12 : index\n //     %27 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %28 = tt.broadcast %26 : (i1) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -85,9 +85,9 @@ module {\n //     %33 = arith.andi %6, %32 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %34 = triton_gpu.copy_async %25, %33, %31 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %35 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %36 = tt.getelementptr %23, %35, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %36 = tt.addptr %23, %35, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %37 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %38 = tt.getelementptr %25, %37, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %38 = tt.addptr %25, %37, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %39 = arith.cmpi slt, %c64, %12 : index\n //     %40 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %41 = tt.broadcast %39 : (i1) -> tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -98,16 +98,16 @@ module {\n //     %46 = arith.andi %6, %45 : tensor<256xi1, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %47 = triton_gpu.copy_async %38, %46, %44 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %48 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %49 = tt.getelementptr %36, %48, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %49 = tt.addptr %36, %48, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %50 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %51 = tt.getelementptr %38, %50, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %51 = tt.addptr %38, %50, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     %52:12 = scf.for %arg6 = %c0 to %12 step %c32 iter_args(%arg7 = %11, %arg8 = %8, %arg9 = %10, %arg10 = %17, %arg11 = %30, %arg12 = %43, %arg13 = %21, %arg14 = %34, %arg15 = %47, %arg16 = %51, %arg17 = %49, %arg18 = %c64) -> (tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, index) {\n //       %55 = arith.addf %arg10, %arg13 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %56 = arith.addf %arg7, %55 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %57 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %58 = tt.getelementptr %arg8, %57, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %58 = tt.addptr %arg8, %57, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %59 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %60 = tt.getelementptr %arg9, %59, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %60 = tt.addptr %arg9, %59, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %61 = arith.addi %arg18, %c32 : index\n //       %62 = arith.cmpi slt, %61, %12 : index\n //       %63 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n@@ -117,13 +117,13 @@ module {\n //       %67 = tt.broadcast %cst : (f32) -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %68 = triton_gpu.copy_async %arg16, %65, %67 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">> -> tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %69 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %70 = tt.getelementptr %arg17, %69, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %70 = tt.addptr %arg17, %69, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       %71 = tt.broadcast %arg5 : (i32) -> tensor<256xi32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//       %72 = tt.getelementptr %arg16, %71, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//       %72 = tt.addptr %arg16, %71, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //       scf.yield %56, %58, %60, %arg11, %arg12, %66, %arg14, %arg15, %68, %72, %70, %61 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>, index\n //     }\n //     %53 = tt.broadcast %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n-//     %54 = tt.getelementptr %53, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n+//     %54 = tt.addptr %53, %4, : tensor<256x!tt.ptr<f32>, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     tt.store %54, %52#0, %6 : tensor<256xf32, #triton_gpu<\"coalesced encoding<threadTileSize = 1, blockTileSize = 32, order = 0>\">>\n //     return\n //   }"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -28,20 +28,20 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n   %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n   %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %5 = tt.getelementptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n   %6 = tt.expand_dims %0 {axis = 0 : i32} : (tensor<64xi32, #blocked0>) -> tensor<1x64xi32, #blocked2>\n   %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %10 = tt.getelementptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %12 = tt.getelementptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n   %13 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n   %14 = arith.muli %6, %13 : tensor<1x64xi32, #blocked2>\n   %15 = tt.broadcast %12 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %16 = tt.broadcast %14 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %17 = triton_gpu.convert_layout %16 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %18 = tt.getelementptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %19 = tt.load %10, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, #blocked1>\n   tt.store %18, %19, %cst : tensor<64x64xf32, #blocked1>\n   return"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "file_content_changes": "@@ -70,21 +70,21 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt\n   // CHECK: %5 = arith.muli %2, %3 : tensor<64x1xi32, [[row_layout]]>\n   // CHECK: %6 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[col_layout]]}>>\n   // CHECK: %7 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[row_layout]]}>>\n-  // CHECK: %8 = tt.getelementptr %4, %5 : tensor<64x1x!tt.ptr<f32>, [[row_layout]]>\n+  // CHECK: %8 = tt.addptr %4, %5 : tensor<64x1x!tt.ptr<f32>, [[row_layout]]>\n   // CHECK: %9 = tt.expand_dims %7 {axis = 0 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[row_layout]]}>>) -> tensor<1x64xi32, [[row_layout]]>\n   // CHECK: %10 = tt.broadcast %8 : (tensor<64x1x!tt.ptr<f32>, [[row_layout]]>) -> tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n   // CHECK: %11 = tt.broadcast %9 : (tensor<1x64xi32, [[row_layout]]>) -> tensor<64x64xi32, [[row_layout]]>\n   // CHECK: %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, [[col_layout]]>\n   // CHECK: %13 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 1, parent = [[col_layout]]}>>) -> tensor<64x1xi32, [[col_layout]]>\n   // CHECK: %14 = tt.expand_dims %6 {axis = 0 : i32} : (tensor<64xi32, #triton_gpu.slice<{dim = 0, parent = [[col_layout]]}>>) -> tensor<1x64xi32, [[col_layout]]>\n   // CHECK: %15 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, [[col_layout]]>\n-  // CHECK: %16 = tt.getelementptr %12, %13 : tensor<64x1x!tt.ptr<f32>, [[col_layout]]>\n+  // CHECK: %16 = tt.addptr %12, %13 : tensor<64x1x!tt.ptr<f32>, [[col_layout]]>\n   // CHECK: %17 = arith.muli %14, %15 : tensor<1x64xi32, [[col_layout]]>\n   // CHECK: %18 = tt.broadcast %16 : (tensor<64x1x!tt.ptr<f32>, [[col_layout]]>) -> tensor<64x64x!tt.ptr<f32>, [[col_layout]]>\n   // CHECK: %19 = tt.broadcast %17 : (tensor<1x64xi32, [[col_layout]]>) -> tensor<64x64xi32, [[col_layout]]>\n-  // CHECK: %20 = tt.getelementptr %10, %11 : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n+  // CHECK: %20 = tt.addptr %10, %11 : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n   // CHECK: %21 = tt.load %20, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n-  // CHECK: %22 = tt.getelementptr %18, %19 : tensor<64x64x!tt.ptr<f32>, [[col_layout]]>\n+  // CHECK: %22 = tt.addptr %18, %19 : tensor<64x64x!tt.ptr<f32>, [[col_layout]]>\n   // CHECK: %23 = triton_gpu.convert_layout %21 : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout]]>\n   // CHECK: tt.store %22, %23, %cst_1 : tensor<64x64xf32, [[col_layout]]>\n   // CHECK: return\n@@ -95,20 +95,20 @@ func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt\n   %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n   %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n   %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %5 = tt.getelementptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n   %6 = tt.expand_dims %0 {axis = 0 : i32} : (tensor<64xi32, #blocked0>) -> tensor<1x64xi32, #blocked2>\n   %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %10 = tt.getelementptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %11 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-  %12 = tt.getelementptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+  %12 = tt.addptr %11, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n   %13 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n   %14 = arith.muli %6, %13 : tensor<1x64xi32, #blocked2>\n   %15 = tt.broadcast %12 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %16 = tt.broadcast %14 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n   %17 = triton_gpu.convert_layout %16 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-  %18 = tt.getelementptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+  %18 = tt.addptr %15, %17 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n   %19 = triton_gpu.convert_layout %10 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n   %20 = triton_gpu.convert_layout %cst_0 : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n   %21 = triton_gpu.convert_layout %cst : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n@@ -127,7 +127,7 @@ func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %ar\n     // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>)\n     // CHECK-NEXT: {{.*}} = tt.load {{.*}} : tensor<64x64xf32, [[row_layout]]>\n     // CHECK-NEXT: {{.*}} = arith.addf {{.*}} : tensor<64x64xf32, [[row_layout]]>\n-    // CHECK-NEXT: {{.*}} = tt.getelementptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n+    // CHECK-NEXT: {{.*}} = tt.addptr {{.*}} : tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n     // CHECK-NEXT: scf.yield {{.*}} : tensor<64x64xf32, [[row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n     // CHECK-NEXT: }\n     // CHECK-NEXT: {{.*}} = triton_gpu.convert_layout [[loop_ret]]#0 : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout_novec]]>\n@@ -143,30 +143,30 @@ func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %ar\n     %2 = tt.splat %arg1 : (i32) -> tensor<64x1xi32, #blocked1>\n     %3 = arith.muli %1, %2 : tensor<64x1xi32, #blocked1>\n     %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %5 = tt.getelementptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+    %5 = tt.addptr %4, %3 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n     %6 = tt.expand_dims %0 {axis = 0 : i32} : (tensor<64xi32, #blocked0>) -> tensor<1x64xi32, #blocked2>\n     %7 = tt.broadcast %5 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %8 = tt.broadcast %6 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n     %9 = triton_gpu.convert_layout %8 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %10 = tt.getelementptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+    %10 = tt.addptr %7, %9 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %11:2 = scf.for %arg5 = %c0 to %c32 step %c1 iter_args(%arg6 = %cst_1, %arg7 = %10) -> (tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>) {\n       %23 = triton_gpu.convert_layout %arg7 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked3>\n       %24 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked3>\n       %25 = triton_gpu.convert_layout %cst_1 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked3>\n       %26 = tt.load %23, %24, %25 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<64x64xf32, #blocked3>\n       %27 = triton_gpu.convert_layout %26 : (tensor<64x64xf32, #blocked3>) -> tensor<64x64xf32, #blocked1>\n       %28 = arith.addf %arg6, %27 : tensor<64x64xf32, #blocked1>\n-      %29 = tt.getelementptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+      %29 = tt.addptr %arg7, %cst_0 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n       scf.yield %28, %29 : tensor<64x64xf32, #blocked1>, tensor<64x64x!tt.ptr<f32>, #blocked1>\n     }\n     %12 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x1x!tt.ptr<f32>, #blocked1>\n-    %13 = tt.getelementptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n+    %13 = tt.addptr %12, %1 : tensor<64x1x!tt.ptr<f32>, #blocked1>\n     %14 = tt.splat %arg3 : (i32) -> tensor<1x64xi32, #blocked2>\n     %15 = arith.muli %6, %14 : tensor<1x64xi32, #blocked2>\n     %16 = tt.broadcast %13 : (tensor<64x1x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %17 = tt.broadcast %15 : (tensor<1x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked2>\n     %18 = triton_gpu.convert_layout %17 : (tensor<64x64xi32, #blocked2>) -> tensor<64x64xi32, #blocked1>\n-    %19 = tt.getelementptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n+    %19 = tt.addptr %16, %18 : tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %20 = triton_gpu.convert_layout %19 : (tensor<64x64x!tt.ptr<f32>, #blocked1>) -> tensor<64x64x!tt.ptr<f32>, #blocked1>\n     %21 = triton_gpu.convert_layout %11#0 : (tensor<64x64xf32, #blocked1>) -> tensor<64x64xf32, #blocked1>\n     %22 = triton_gpu.convert_layout %cst : (tensor<64x64xi1, #blocked1>) -> tensor<64x64xi1, #blocked1>\n@@ -190,17 +190,17 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   %9 = arith.addi %6, %7 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %11 = arith.addi %4, %5 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %12 = tt.getelementptr %8, %9 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n+  %12 = tt.addptr %8, %9 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %13 = tt.load %12 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %14 = triton_gpu.convert_layout %13 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n-  %15 = tt.getelementptr %10, %11 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n+  %15 = tt.addptr %10, %11 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %16 = tt.load %15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %17 = triton_gpu.convert_layout %16 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n   %18 = arith.addf %14, %17 : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>\n   %19 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %20 = arith.addi %2, %3 : tensor<256xi32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n-  %21 = tt.getelementptr %19, %20 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n+  %21 = tt.addptr %19, %20 : tensor<256x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   %22 = triton_gpu.convert_layout %18 : (tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [2], order = [0]}>>) -> tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   tt.store %21, %22 : tensor<256xf32, #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>>\n   return\n-}\n\\ No newline at end of file\n+}"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 59, "deletions": 40, "changes": 99, "file_content_changes": "@@ -9,25 +9,31 @@\n #C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n \n // CHECK: func @matmul_loop\n+// CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n+// CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n+// CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n+// CHECK-DAG: %[[CONSTANT_3:.*]] = arith.constant 3 : i32\n // CHECK: %[[ABUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async\n+// CHECK: %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n // CHECK: %[[BBUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async\n-// CHECK: %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async\n-// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async\n+// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n+// CHECK: %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n+// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK: %[[A0:.*]] = triton_gpu.extract_slice %[[A0BUFFER]]\n-// CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B0BUFFER]]\n-// CHECK: %[[A1:.*]] = triton_gpu.extract_slice %[[A1BUFFER]]\n-// CHECK: %[[B1:.*]] = triton_gpu.extract_slice %[[B1BUFFER]]\n-// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_a1:.*]] = %[[A1]], %[[arg_b0:.*]] = %[[B0]], %[[arg_b1:.*]] = %[[B1]], {{.*}})\n+// CHECK: %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]], %[[CONSTANT_0]]\n+// CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n // CHECK:   tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n-// CHECK:   %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async\n-// CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK:   %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n+// CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:   %[[NEXT_A:.*]] = triton_gpu.extract_slice %[[NEXT_A_BUFFER]]\n-// CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]]\n-// CHECK:   scf.yield {{.*}}, {{.*}}, {{.*}}, %[[arg_a1]], %[[NEXT_A]], %[[arg_b1]], %[[NEXT_B]]\n+// CHECK:   %[[NEXT_A:.*]] = triton_gpu.extract_slice %[[NEXT_A_BUFFER]], %[[EXTRACT_IDX]]\n+// CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]], %[[EXTRACT_IDX]]\n+// CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n+// CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n+// CHECK:   scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n@@ -49,35 +55,41 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-    %next_a_ptr = tt.getelementptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-    %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return\n }\n \n \n // CHECK: func @matmul_loop_nested\n+// CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n+// CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n+// CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n+// CHECK-DAG: %[[CONSTANT_3:.*]] = arith.constant 3 : i32\n // CHECK: scf.for\n // CHECK:   %[[ABUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK:   %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async\n+// CHECK:   %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n // CHECK:   %[[BBUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK:   %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async\n-// CHECK:   %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async\n-// CHECK:   %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async\n+// CHECK:   %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n+// CHECK:   %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n+// CHECK:   %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:   %[[A0:.*]] = triton_gpu.extract_slice %[[A0BUFFER]]\n-// CHECK:   %[[B0:.*]] = triton_gpu.extract_slice %[[B0BUFFER]]\n-// CHECK:   %[[A1:.*]] = triton_gpu.extract_slice %[[A1BUFFER]]\n-// CHECK:   %[[B1:.*]] = triton_gpu.extract_slice %[[B1BUFFER]]\n-// CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_a1:.*]] = %[[A1]], %[[arg_b0:.*]] = %[[B0]], %[[arg_b1:.*]] = %[[B1]], {{.*}})\n+// CHECK:   %[[A0:.*]] = triton_gpu.extract_slice %[[A1BUFFER]], %[[CONSTANT_0]]\n+// CHECK:   %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n+// CHECK:   scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, {{.*}}, {{.*}}, %[[arg_a0:.*]] = %[[A0]], %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n // CHECK:     tt.dot %[[arg_a0]], %[[arg_b0]], {{.*}}\n-// CHECK:     %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async\n-// CHECK:     %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK:     %[[NEXT_A_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n+// CHECK:     %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:     triton_gpu.async_wait {num = 2 : i32}\n-// CHECK:     %[[NEXT_A:.*]] = triton_gpu.extract_slice %[[NEXT_A_BUFFER]]\n-// CHECK:     %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]]\n-// CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[arg_a1]], %[[NEXT_A]], %[[arg_b1]], %[[NEXT_B]]\n+// CHECK:     %[[NEXT_A:.*]] = triton_gpu.extract_slice %[[NEXT_A_BUFFER]], %[[EXTRACT_IDX]]\n+// CHECK:     %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]], %[[EXTRACT_IDX]]\n+// CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n+// CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n+// CHECK:     scf.yield {{.*}}, {{.*}}, {{.*}}, %[[NEXT_A_BUFFER]], %[[NEXT_B_BUFFER]], %[[NEXT_A]], %[[NEXT_B]], {{.*}}, {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   scf.for %iv0 = %lb to %ub step %step {\n     %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n@@ -100,8 +112,8 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n \n       %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n \n-      %next_a_ptr = tt.getelementptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n-      %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+      %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n+      %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n       scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n     }\n   }\n@@ -110,18 +122,25 @@ func @matmul_loop_nested(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f\n \n \n // CHECK: func @matmul_loop_single_pipeline\n+// CHECK-DAG: %[[CONSTANT_0:.*]] = arith.constant 0 : i32\n+// CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n+// CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n+// CHECK-DAG: %[[CONSTANT_3:.*]] = arith.constant 3 : i32\n // CHECK: %[[BBUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async\n-// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async\n+// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n+// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n // CHECK: triton_gpu.async_wait {num = 1 : i32}\n-// CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B0BUFFER]]\n-// CHECK: %[[B1:.*]] = triton_gpu.extract_slice %[[B1BUFFER]]\n-// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], %[[arg_b1:.*]] = %[[B1]], {{.*}})\n+// CHECK: %[[B0:.*]] = triton_gpu.extract_slice %[[B1BUFFER]], %[[CONSTANT_0]]\n+// CHECK: scf.for {{.*}} iter_args({{.*}}, {{.*}}, {{.*}}, %[[arg_b0:.*]] = %[[B0]], {{.*}}, {{.*}}, %[[PIPELINE_IDX:.*]] = %[[CONSTANT_2]], %[[LOOP_IDX:.*]] = %[[CONSTANT_0]]\n // CHECK:   tt.dot {{.*}}, %[[arg_b0]], {{.*}}\n-// CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async\n+// CHECK-DAG: %[[INSERT_IDX:.*]] = arith.remsi %[[PIPELINE_IDX]], %[[CONSTANT_3]]\n+// CHECK-DAG: %[[EXTRACT_IDX:.*]] = arith.remsi %[[LOOP_IDX]], %[[CONSTANT_3]]\n+// CHECK:   %[[NEXT_B_BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[INSERT_IDX]]\n // CHECK:   triton_gpu.async_wait {num = 1 : i32}\n // CHECK:   %[[NEXT_B:.*]] = triton_gpu.extract_slice %[[NEXT_B_BUFFER]]\n-// CHECK:   scf.yield {{.*}}, {{.*}}, %[[arg_b1]], %[[NEXT_B]]\n+// CHECK-DAG: %[[NEXT_PIPELINE_IDX:.*]] = arith.addi %[[PIPELINE_IDX]], %[[CONSTANT_1]]\n+// CHECK-DAG: %[[NEXT_LOOP_IDX:.*]] = arith.addi %[[LOOP_IDX]], %[[CONSTANT_1]]\n+// CHECK:   scf.yield {{.*}}, {{.*}}, %[[NEXT_B_BUFFER]], %[[NEXT_B]], {{.*}}, {{.*}}, %[[NEXT_PIPELINE_IDX]], %[[NEXT_LOOP_IDX]]\n func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n@@ -142,7 +161,7 @@ func @matmul_loop_single_pipeline(%lb : index, %ub : index, %step : index, %A :\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n     %c = tt.dot %a, %b, %prev_c {allowTF32 = true} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n-    %next_b_ptr = tt.getelementptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n+    %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_b_ptr, %c : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return"}, {"filename": "test/TritonGPU/matmul.mlir", "status": "added", "additions": 106, "deletions": 0, "changes": 106, "file_content_changes": "@@ -0,0 +1,106 @@\n+// RUN: triton-opt %s -split-input-file -convert-triton-to-tritongpu -tritongpu-pipeline=num-stages=3 -tritongpu-combine -test-print-allocation 2>&1 | FileCheck %s\n+\n+// CHECK: offset = 0, size = 49152\n+// CHECK: offset = 49152, size = 49152\n+// CHECK: size = 98304\n+module {\n+func @matmul_kernel__Pfp32_Pfp32_Pfp32_i32_i32_i32_i32_i32_i32_i32_i32_i32__12c64_13c64_14c64_15c8(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: i32, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32, %arg10: i32 {tt.divisibility = 16 : i32}, %arg11: i32) {\n+    %cst = arith.constant dense<true> : tensor<64x64xi1>\n+    %c64 = arith.constant 64 : index\n+    %c0 = arith.constant 0 : index\n+    %cst_0 = arith.constant dense<0.000000e+00> : tensor<64x64xf32>\n+    %c64_i32 = arith.constant 64 : i32\n+    %c63_i32 = arith.constant 63 : i32\n+    %c8_i32 = arith.constant 8 : i32\n+    %0 = tt.get_program_id {axis = 0 : i32} : i32\n+    %1 = arith.addi %arg3, %c63_i32 : i32\n+    %2 = arith.divsi %1, %c64_i32 : i32\n+    %3 = arith.addi %arg4, %c63_i32 : i32\n+    %4 = arith.divsi %3, %c64_i32 : i32\n+    %5 = arith.muli %4, %c8_i32 : i32\n+    %6 = arith.divsi %0, %5 : i32\n+    %7 = arith.muli %6, %c8_i32 : i32\n+    %8 = arith.subi %2, %7 : i32\n+    %9 = arith.cmpi slt, %8, %c8_i32 : i32\n+    %10 = select %9, %8, %c8_i32 : i32\n+    %11 = arith.remsi %0, %10 : i32\n+    %12 = arith.addi %7, %11 : i32\n+    %13 = arith.remsi %0, %5 : i32\n+    %14 = arith.divsi %13, %10 : i32\n+    %15 = arith.muli %12, %c64_i32 : i32\n+    %16 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %17 = tt.splat %15 : (i32) -> tensor<64xi32>\n+    %18 = arith.addi %17, %16 : tensor<64xi32>\n+    %19 = arith.muli %14, %c64_i32 : i32\n+    %20 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %21 = tt.splat %19 : (i32) -> tensor<64xi32>\n+    %22 = arith.addi %21, %20 : tensor<64xi32>\n+    %23 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %24 = tt.expand_dims %18 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n+    %25 = tt.splat %arg6 : (i32) -> tensor<64x1xi32>\n+    %26 = arith.muli %24, %25 : tensor<64x1xi32>\n+    %27 = tt.expand_dims %23 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %28 = tt.splat %arg7 : (i32) -> tensor<1x64xi32>\n+    %29 = arith.muli %27, %28 : tensor<1x64xi32>\n+    %30 = tt.broadcast %26 : (tensor<64x1xi32>) -> tensor<64x64xi32>\n+    %31 = tt.broadcast %29 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n+    %32 = arith.addi %30, %31 : tensor<64x64xi32>\n+    %33 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n+    %34 = tt.addptr %33, %32 : tensor<64x64x!tt.ptr<f32>>\n+    %35 = tt.expand_dims %23 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n+    %36 = tt.splat %arg8 : (i32) -> tensor<64x1xi32>\n+    %37 = arith.muli %35, %36 : tensor<64x1xi32>\n+    %38 = tt.expand_dims %22 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %39 = tt.splat %arg9 : (i32) -> tensor<1x64xi32>\n+    %40 = arith.muli %38, %39 : tensor<1x64xi32>\n+    %41 = tt.broadcast %37 : (tensor<64x1xi32>) -> tensor<64x64xi32>\n+    %42 = tt.broadcast %40 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n+    %43 = arith.addi %41, %42 : tensor<64x64xi32>\n+    %44 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n+    %45 = tt.addptr %44, %43 : tensor<64x64x!tt.ptr<f32>>\n+    %46 = arith.index_cast %arg5 : i32 to index\n+    %47:3 = scf.for %arg12 = %c0 to %46 step %c64 iter_args(%arg13 = %cst_0, %arg14 = %34, %arg15 = %45) -> (tensor<64x64xf32>, tensor<64x64x!tt.ptr<f32>>, tensor<64x64x!tt.ptr<f32>>) {\n+      %76 = tt.load %arg14, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32>\n+      %77 = tt.load %arg15, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32>\n+      %78 = tt.dot %76, %77, %cst_0 {allowTF32 = true} : tensor<64x64xf32> * tensor<64x64xf32> -> tensor<64x64xf32>\n+      %79 = arith.addf %arg13, %78 : tensor<64x64xf32>\n+      %80 = arith.muli %arg7, %c64_i32 : i32\n+      %81 = tt.splat %80 : (i32) -> tensor<64x64xi32>\n+      %82 = tt.addptr %arg14, %81 : tensor<64x64x!tt.ptr<f32>>\n+      %83 = arith.muli %arg8, %c64_i32 : i32\n+      %84 = tt.splat %83 : (i32) -> tensor<64x64xi32>\n+      %85 = tt.addptr %arg15, %84 : tensor<64x64x!tt.ptr<f32>>\n+      scf.yield %79, %82, %85 : tensor<64x64xf32>, tensor<64x64x!tt.ptr<f32>>, tensor<64x64x!tt.ptr<f32>>\n+    }\n+    %48 = arith.muli %12, %c64_i32 : i32\n+    %49 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %50 = tt.splat %48 : (i32) -> tensor<64xi32>\n+    %51 = arith.addi %50, %49 : tensor<64xi32>\n+    %52 = arith.muli %14, %c64_i32 : i32\n+    %53 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32>\n+    %54 = tt.splat %52 : (i32) -> tensor<64xi32>\n+    %55 = arith.addi %54, %53 : tensor<64xi32>\n+    %56 = tt.expand_dims %51 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n+    %57 = tt.splat %arg10 : (i32) -> tensor<64x1xi32>\n+    %58 = arith.muli %57, %56 : tensor<64x1xi32>\n+    %59 = tt.expand_dims %55 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %60 = tt.splat %arg11 : (i32) -> tensor<1x64xi32>\n+    %61 = arith.muli %59, %60 : tensor<1x64xi32>\n+    %62 = tt.broadcast %58 : (tensor<64x1xi32>) -> tensor<64x64xi32>\n+    %63 = tt.broadcast %61 : (tensor<1x64xi32>) -> tensor<64x64xi32>\n+    %64 = arith.addi %62, %63 : tensor<64x64xi32>\n+    %65 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<64x64x!tt.ptr<f32>>\n+    %66 = tt.addptr %65, %64 : tensor<64x64x!tt.ptr<f32>>\n+    %67 = tt.expand_dims %51 {axis = 1 : i32} : (tensor<64xi32>) -> tensor<64x1xi32>\n+    %68 = tt.splat %arg3 : (i32) -> tensor<64x1xi32>\n+    %69 = arith.cmpi slt, %67, %68 : tensor<64x1xi32>\n+    %70 = tt.expand_dims %55 {axis = 0 : i32} : (tensor<64xi32>) -> tensor<1x64xi32>\n+    %71 = tt.splat %arg4 : (i32) -> tensor<1x64xi32>\n+    %72 = arith.cmpi slt, %70, %71 : tensor<1x64xi32>\n+    %73 = tt.broadcast %69 : (tensor<64x1xi1>) -> tensor<64x64xi1>\n+    %74 = tt.broadcast %72 : (tensor<1x64xi1>) -> tensor<64x64xi1>\n+    %75 = arith.andi %73, %74 : tensor<64x64xi1>\n+    tt.store %66, %47#0, %75 : tensor<64x64xf32>\n+    return\n+  }\n+}"}, {"filename": "test/lib/Analysis/TestMembar.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -29,7 +29,7 @@ struct TestMembarPass\n     MembarAnalysis analysis(&allocation);\n     size_t operationId = 0;\n     operation->walk<WalkOrder::PreOrder>([&](Operation *op) {\n-      if (dyn_cast<gpu::BarrierOp>(op)) {\n+      if (isa<gpu::BarrierOp>(op)) {\n         os << \"Membar \" << operationId << \"\\n\";\n       }\n       if (op->getNumRegions() == 0) {"}, {"filename": "unittest/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -16,6 +16,7 @@ function(add_triton_ut)\n           ${__SRCS})\n   target_link_libraries(\n           ${__NAME}\n+          PRIVATE\n           GTest::gtest_main\n           gmock\n           ${__LIBS})"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1,5 +1,5 @@\n add_triton_ut(\n-\tNAME TritonGPUToLLVMTests\n-\tSRCS TritonGPUToLLVMTests.cpp\n+\tNAME PtxAsmFormatTest\n+\tSRCS PtxAsmFormatTest.cpp\n \tLIBS TritonGPUToLLVM\n )"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/PtxAsmFormatTest.cpp", "status": "added", "additions": 125, "deletions": 0, "changes": 125, "file_content_changes": "@@ -0,0 +1,125 @@\n+#include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n+#include \"mlir/Dialect/Arithmetic/IR/Arithmetic.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include <gtest/gtest.h>\n+\n+namespace mlir {\n+namespace triton {\n+class PtxAsmFormatTest : public ::testing::Test {\n+protected:\n+  static constexpr int numValues = 4;\n+\n+  PtxAsmFormatTest() {\n+    ctx.loadDialect<arith::ArithmeticDialect>();\n+\n+    createValues();\n+  }\n+\n+  // Creates the test values.\n+  void createValues() {\n+    OpBuilder builder(&ctx);\n+    builder.setInsertionPointToStart(&block);\n+\n+    // a b1 value for predicate.\n+    v[0] = builder.create<arith::ConstantIntOp>(builder.getUnknownLoc(), 1, 1);\n+    for (int i = 0; i < numValues; i++) {\n+      v[i + 1] =\n+          builder.create<arith::ConstantIntOp>(builder.getUnknownLoc(), i, 32);\n+    }\n+  }\n+\n+  MLIRContext ctx;\n+  Block block;\n+  Value v[numValues + 1];\n+};\n+\n+TEST_F(PtxAsmFormatTest, basic) {\n+  PTXBuilder builder;\n+\n+  // Create the operands needed by the instructions in the PTX code.\n+  auto *cst = builder.newConstantOperand(1);\n+  auto *val = builder.newOperand(v[1], \"=r\");\n+\n+  // create an instruction\n+  auto &mov = *builder.create(\"mov.b16\");\n+\n+  mov(val, cst).predicate(v[0]);\n+  ASSERT_EQ(builder.dump(), \"@$1 mov.b16 $0, 0x1;\");\n+\n+  auto values = builder.getAllMLIRArgs();\n+  ASSERT_EQ(values[0], v[1]); // $0 -> v[1]\n+  ASSERT_EQ(values[1], v[0]); // $1 -> v[0]\n+\n+  auto constraints = builder.getConstraints();\n+  ASSERT_EQ(constraints, \"=r,b\"); // $0 -> =r, $1 -> b\n+}\n+\n+TEST_F(PtxAsmFormatTest, complexInstruction) {\n+  using triton::CacheModifier;\n+  using triton::EvictionPolicy;\n+\n+  PTXBuilder builder;\n+\n+  int width = 16;\n+  int nWords = 2;\n+\n+  Value predicateVal = v[0];\n+  Value addrVal = v[1];\n+\n+  auto addr = builder.newAddrOperand(addrVal, \"l\", 128 /*offset*/);\n+\n+  bool isVolatile = false;\n+  auto cache = triton::CacheModifier::CA;\n+  auto cachePriority = triton::EvictionPolicy::EVICT_FIRST;\n+  bool hasL2EvictPolicy = true;\n+\n+  auto &ld =\n+      builder\n+          .create<PtxIOInstr>(\"ld\") //\n+          ->o(\"volatile\", isVolatile)\n+          .global()\n+          .o(\"ca\", cache == CacheModifier::CA)\n+          .o(\"cg\", cache == CacheModifier::CG)\n+          .o(\"L1::evict_first\", cachePriority == EvictionPolicy::EVICT_FIRST)\n+          .o(\"L1::evict_last\", cachePriority == EvictionPolicy::EVICT_LAST)\n+          .o(\"L1::cache_hint\", hasL2EvictPolicy)\n+          .v(nWords)\n+          .b(width);\n+\n+  // Link the instruction to operands\n+  ld(addr).predicate(predicateVal);\n+\n+  EXPECT_EQ(\n+      builder.dump(),\n+      \"@$1 ld.global.ca.L1::evict_first.L1::cache_hint.v2.b16 [ $0 + 128 ];\");\n+  auto values = builder.getAllMLIRArgs();\n+  EXPECT_EQ(values[0], addrVal);      // $0 -> predicate\n+  EXPECT_EQ(values[1], predicateVal); // $1 -> addr\n+  EXPECT_EQ(builder.getConstraints(), \"l,b\");\n+}\n+\n+TEST_F(PtxAsmFormatTest, MultiLinePTX) {\n+  PTXBuilder builder;\n+\n+  auto *constVal = builder.newConstantOperand(1);\n+  auto *valVal0 = builder.newOperand(v[1], \"=r\");\n+  auto *valVal1 = builder.newOperand(v[2], \"=r\");\n+\n+  auto &mov = *builder.create(\"mov\");\n+\n+  mov(valVal0, constVal);\n+  mov(valVal1, constVal);\n+  mov(valVal1, valVal0);\n+\n+  EXPECT_EQ(builder.dump(), \"mov $0, 0x1;\\r\\n\"\n+                            \"mov $1, 0x1;\\r\\n\"\n+                            \"mov $1, $0;\");\n+\n+  auto values = builder.getAllMLIRArgs();\n+  EXPECT_EQ(values[0], v[1]); // $0 -> v[1]\n+  EXPECT_EQ(values[1], v[2]); // $1 -> v[2]\n+}\n+\n+} // namespace triton\n+} // namespace mlir"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/TritonGPUToLLVMTests.cpp", "status": "removed", "additions": 0, "deletions": 14, "changes": 14, "file_content_changes": "@@ -1,14 +0,0 @@\n-//===- TritonGPUToLLVMTests.cpp - Tests for\n-// TritonGPUToLLVM----------------------------------===//\n-//\n-//===----------------------------------------------------------------------===//\n-\n-#include \"triton/Conversion/TritonGPUToLLVM/PtxAsmFormat.h\"\n-#include <gmock/gmock.h>\n-#include <gtest/gtest.h>\n-\n-namespace mlir {\n-\n-TEST(PtxAsmFormatTest, BasicTest) { EXPECT_EQ(true, true); }\n-\n-} // namespace mlir"}]