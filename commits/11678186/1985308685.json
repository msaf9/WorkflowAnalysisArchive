[{"filename": "python/triton/code_gen.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -104,7 +104,7 @@ def _get_tensor(self, name: str, bb: Optional[_triton.ir.basic_block] = None) ->\n \n     def _get_tensor_recursive(self, name: str, bb: _triton.ir.basic_block) -> triton.language.tensor:\n         preds = bb.get_predecessors()\n-        type = self.lscope[name].dtype\n+        type = self.lscope[name].type\n         # some preds haven't been filled, create a phi as a proxy of the value\n         if bb not in self.sealed_blocks:\n             result = self._make_phi(type, len(preds), bb)\n@@ -170,7 +170,7 @@ def _try_remove_trivial_phi(self, phi: triton.language.tensor) -> triton.languag\n         phi.handle.replace_all_uses_with(v)\n         phi.handle.erase_from_parent()\n         # TODO: remove trivial phis recursively\n-        return triton.language.tensor(v, phi.dtype)\n+        return triton.language.tensor(v, phi.type)\n \n     def is_triton_tensor(self, value):\n         return isinstance(value, triton.language.tensor)"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "file_content_changes": "@@ -431,12 +431,9 @@ def __init__(self, handle, type: dtype):\n         for s in self.shape:\n             self.numel *= s\n         self.numel = constexpr(self.numel)\n-        # Data-type wrapper\n-        self.dtype = type\n-        # # if type is not provided, infer from ir type\n-        # if not self.dtype:\n-        #     self.dtype = tensor._to_dtype(self.handle.type)\n-        # Shape is a constexpr\n+        self.type = type  # Tensor type (can be block_type)\n+        # Following the practice in pytorch, dtype is scalar type\n+        self.dtype = type.scalar\n         self.shape = [constexpr(s) for s in self.shape]\n \n     def __str__(self) -> str:"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 123, "deletions": 126, "changes": 249, "file_content_changes": "@@ -98,8 +98,8 @@ def binary_op_type_checking_impl(lhs: tl.tensor,\n     # implicit broadcasting\n     lhs, rhs = broadcast_impl_value(lhs, rhs, builder)\n     # implicit typecasting\n-    lhs_sca_ty = lhs.dtype.scalar\n-    rhs_sca_ty = rhs.dtype.scalar\n+    lhs_sca_ty = lhs.type.scalar\n+    rhs_sca_ty = rhs.type.scalar\n     check_ptr_type_impl(lhs_sca_ty, rhs_sca_ty, allow_lhs_ptr)\n     check_ptr_type_impl(rhs_sca_ty, lhs_sca_ty, allow_rhs_ptr)\n     if arithmetic_check and not lhs_sca_ty.is_ptr() and not rhs_sca_ty.is_ptr():\n@@ -113,62 +113,62 @@ def add(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder, True, True)\n-    input_scalar_ty = input.dtype.scalar\n-    other_scalar_ty = other.dtype.scalar\n+    input_scalar_ty = input.type.scalar\n+    other_scalar_ty = other.type.scalar\n \n     # offset + ptr\n     # ptr + offset\n     if other_scalar_ty.is_ptr() and not input_scalar_ty.is_ptr():\n         input, other = other, input\n     if input_scalar_ty.is_ptr():\n-        return tl.tensor(builder.create_gep(input.handle, [other.handle]), input.dtype)\n+        return tl.tensor(builder.create_gep(input.handle, [other.handle]), input.type)\n     # float + float\n     elif input_scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fadd(input.handle, other.handle), input.dtype)\n+        return tl.tensor(builder.create_fadd(input.handle, other.handle), input.type)\n     # int + int\n     elif input_scalar_ty.is_int():\n-        return tl.tensor(builder.create_add(input.handle, other.handle), input.dtype)\n+        return tl.tensor(builder.create_add(input.handle, other.handle), input.type)\n     assert False\n \n \n def sub(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder, True, False)\n-    scalar_ty = input.dtype.scalar\n+    scalar_ty = input.type.scalar\n     # ptr - offset\n     if scalar_ty.is_ptr():\n         return tl.tensor(builder.create_gep(input.handle, [minus(other, builder).handle]),\n-                         input.dtype)\n+                         input.type)\n     # float - float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fsub(input.handle, other.handle), input.dtype)\n+        return tl.tensor(builder.create_fsub(input.handle, other.handle), input.type)\n     # int - int\n     elif scalar_ty.is_int():\n-        return tl.tensor(builder.create_sub(input.handle, other.handle), input.dtype)\n+        return tl.tensor(builder.create_sub(input.handle, other.handle), input.type)\n     assert False\n \n \n def mul(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder)\n-    scalar_ty = input.dtype.scalar\n+    scalar_ty = input.type.scalar\n     # float * float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_fmul(input.handle, other.handle), input.dtype)\n+        return tl.tensor(builder.create_fmul(input.handle, other.handle), input.type)\n     # * int\n     elif scalar_ty.is_int():\n-        return tl.tensor(builder.create_mul(input.handle, other.handle), input.dtype)\n+        return tl.tensor(builder.create_mul(input.handle, other.handle), input.type)\n     assert False\n \n \n def truediv(input: tl.tensor,\n             other: tl.tensor,\n             builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder, False, False, True, True)\n-    input_scalar_ty = input.dtype.scalar\n-    other_scalar_ty = other.dtype.scalar\n+    input_scalar_ty = input.type.scalar\n+    other_scalar_ty = other.type.scalar\n     # float / int\n     if input_scalar_ty.is_floating() and other_scalar_ty.is_int():\n         other = cast(other, input_scalar_ty, builder)\n@@ -188,59 +188,59 @@ def truediv(input: tl.tensor,\n     # unreachable\n     else:\n         assert False\n-    return tl.tensor(builder.create_fdiv(input.handle, other.handle), input.dtype)\n+    return tl.tensor(builder.create_fdiv(input.handle, other.handle), input.type)\n \n \n def floordiv(input: tl.tensor,\n              other: tl.tensor,\n              builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder, False, False, True, True)\n-    input_scalar_ty = input.dtype.scalar\n-    other_scalar_ty = other.dtype.scalar\n+    input_scalar_ty = input.type.scalar\n+    other_scalar_ty = other.type.scalar\n     if input_scalar_ty.is_int() and other_scalar_ty.is_int():\n         ret_ty = integer_promote_impl(input_scalar_ty, other_scalar_ty)\n         input = cast(input, ret_ty, builder)\n         other = cast(other, ret_ty, builder)\n         if ret_ty.is_int_signed():\n-            return tl.tensor(builder.create_sdiv(input.handle, other.handle), input.dtype)\n+            return tl.tensor(builder.create_sdiv(input.handle, other.handle), input.type)\n         else:\n-            return tl.tensor(builder.create_udiv(input.handle, other.handle), input.dtype)\n+            return tl.tensor(builder.create_udiv(input.handle, other.handle), input.type)\n     assert False\n \n \n def fdiv(input: tl.tensor,\n          other: tl.tensor,\n          ieee_rounding: bool,\n          builder: ir.builder) -> tl.tensor:\n-    input_scalar_ty = input.dtype.scalar\n-    other_scalar_ty = other.dtype.scalar\n+    input_scalar_ty = input.type.scalar\n+    other_scalar_ty = other.type.scalar\n     if not input_scalar_ty.is_floating() or not other_scalar_ty.is_floating():\n         raise ValueError(\"both operands of fdiv must have floating poscalar type\")\n     input, other = binary_op_type_checking_impl(input, other, builder, False, False, False, True)\n     ret = builder.create_fdiv(input.handle, other.handle)\n     ret.set_fdiv_ieee_rounding(ieee_rounding)\n-    return tl.tensor(ret, input.dtype)\n+    return tl.tensor(ret, input.type)\n \n \n def mod(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder, False, False, True, True)\n-    scalar_ty = input.dtype.scalar\n-    other_scalar_ty = other.dtype.scalar\n+    scalar_ty = input.type.scalar\n+    other_scalar_ty = other.type.scalar\n     # float % float\n     if scalar_ty.is_floating():\n-        return tl.tensor(builder.create_frem(input.handle, other.handle), input.dtype)\n+        return tl.tensor(builder.create_frem(input.handle, other.handle), input.type)\n     # % int\n     elif scalar_ty.is_int():\n         if scalar_ty.int_signedness != other_scalar_ty.int_signedness:\n             raise ValueError(\"Cannot mod \" + scalar_ty.__repr__() + \" by \" + other_scalar_ty.__repr__() + \" \"\n                              \"because they have different signedness;\"\n                              \"this is unlikely to result in a useful answer. Cast them to the same signedness.\")\n         if scalar_ty.is_int_signed():\n-            return tl.tensor(builder.create_srem(input.handle, other.handle), input.dtype)\n+            return tl.tensor(builder.create_srem(input.handle, other.handle), input.type)\n         else:\n-            return tl.tensor(builder.create_urem(input.handle, other.handle), input.dtype)\n+            return tl.tensor(builder.create_urem(input.handle, other.handle), input.type)\n     assert False\n \n ##############\n@@ -252,8 +252,8 @@ def bitwise_op_type_checking_impl(input: tl.tensor,\n                                   other: tl.tensor,\n                                   builder: ir.builder) -> Tuple[tl.tensor, tl.tensor]:\n     input, other = binary_op_type_checking_impl(input, other, builder, False, False, False)\n-    input_sca_ty = input.dtype.scalar\n-    other_sca_ty = other.dtype.scalar\n+    input_sca_ty = input.type.scalar\n+    other_sca_ty = other.type.scalar\n     if not input_sca_ty.is_int() or not other_sca_ty.is_int():\n         raise IncompatibleTypeErrorimpl(input_sca_ty, other_sca_ty)\n     ret_sca_ty = integer_promote_impl(input_sca_ty, other_sca_ty)\n@@ -268,35 +268,35 @@ def and_(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n-    return tl.tensor(builder.create_and(input.handle, other.handle), input.dtype)\n+    return tl.tensor(builder.create_and(input.handle, other.handle), input.type)\n \n \n def or_(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n-    return tl.tensor(builder.create_or(input.handle, other.handle), input.dtype)\n+    return tl.tensor(builder.create_or(input.handle, other.handle), input.type)\n \n \n def xor_(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n-    return tl.tensor(builder.create_xor(input.handle, other.handle), input.dtype)\n+    return tl.tensor(builder.create_xor(input.handle, other.handle), input.type)\n \n \n def lshr(input: tl.tensor,\n          other: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n-    return tl.tensor(builder.create_lshr(input.handle, other.handle), input.dtype)\n+    return tl.tensor(builder.create_lshr(input.handle, other.handle), input.type)\n \n \n def shl(input: tl.tensor,\n         other: tl.tensor,\n         builder: ir.builder) -> tl.tensor:\n     input, other = bitwise_op_type_checking_impl(input, other, builder)\n-    return tl.tensor(builder.create_shl(input.handle, other.handle), input.dtype)\n+    return tl.tensor(builder.create_shl(input.handle, other.handle), input.type)\n \n # ===----------------------------------------------------------------------===//\n #                               Unary Operators\n@@ -309,7 +309,7 @@ def plus(input: tl.tensor) -> tl.tensor:\n \n def minus(input: tl.tensor,\n           builder: ir.builder) -> tl.tensor:\n-    input_sca_ty = input.dtype.scalar\n+    input_sca_ty = input.type.scalar\n     if input_sca_ty.is_ptr():\n         raise ValueError(\"wrong type argument to unary minus (\" + input_sca_ty.__repr__() + \")\")\n     _0 = tl.tensor(ir.constant.get_null_value(input_sca_ty.to_ir(builder)), input_sca_ty)\n@@ -318,7 +318,7 @@ def minus(input: tl.tensor,\n \n def invert(input: tl.tensor,\n            builder: tl.tensor) -> tl.tensor:\n-    input_sca_ty = input.dtype.scalar\n+    input_sca_ty = input.type.scalar\n     if input_sca_ty.is_ptr() or input_sca_ty.is_floating():\n         raise ValueError(\"wrong type argument to unary invert (\" + input_sca_ty.__repr__() + \")\")\n     _1 = tl.tensor(ir.constant.get_all_ones_value(input_sca_ty.to_ir(builder)), input_sca_ty)\n@@ -329,17 +329,17 @@ def invert(input: tl.tensor,\n #                               Comparison Operators\n # ===----------------------------------------------------------------------===//\n def _bool_like(v: tl.tensor) -> tl.block_type:\n-    if not v.dtype.is_block():\n+    if not v.type.is_block():\n         return tl.int1\n-    shape = v.dtype.shape\n+    shape = v.type.shape\n     return tl.block_type(tl.int1, shape)\n \n \n def greater_than(input: tl.tensor,\n                  other: tl.tensor,\n                  builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder)\n-    scalar_ty = input.dtype.scalar\n+    scalar_ty = input.type.scalar\n     # float > float\n     if scalar_ty.is_floating():\n         return tl.tensor(builder.create_fcmpOGT(input.handle, other.handle), _bool_like(input))\n@@ -356,7 +356,7 @@ def greater_equal(input: tl.tensor,\n                   other: tl.tensor,\n                   builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder)\n-    scalar_ty = input.dtype.scalar\n+    scalar_ty = input.type.scalar\n     # float >= float\n     if scalar_ty.is_floating():\n         return tl.tensor(builder.create_fcmpOGE(input.handle, other.handle), _bool_like(input))\n@@ -373,7 +373,7 @@ def less_than(input: tl.tensor,\n               other: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder)\n-    scalar_ty = input.dtype.scalar\n+    scalar_ty = input.type.scalar\n     # float < float\n     if scalar_ty.is_floating():\n         return tl.tensor(builder.create_fcmpOLT(input.handle, other.handle), _bool_like(input))\n@@ -390,7 +390,7 @@ def less_equal(input: tl.tensor,\n                other: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder)\n-    scalar_ty = input.dtype.scalar\n+    scalar_ty = input.type.scalar\n     # float < float\n     if scalar_ty.is_floating():\n         return tl.tensor(builder.create_fcmpOLE(input.handle, other.handle), _bool_like(input))\n@@ -407,7 +407,7 @@ def equal(input: tl.tensor,\n           other: tl.tensor,\n           builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder)\n-    scalar_ty = input.dtype.scalar\n+    scalar_ty = input.type.scalar\n     # float == float\n     if scalar_ty.is_floating():\n         return tl.tensor(builder.create_fcmpOEQ(input.handle, other.handle), _bool_like(input))\n@@ -421,7 +421,7 @@ def not_equal(input: tl.tensor,\n               other: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n     input, other = binary_op_type_checking_impl(input, other, builder)\n-    scalar_ty = input.dtype.scalar\n+    scalar_ty = input.type.scalar\n     # float == float\n     if scalar_ty.is_floating():\n         return tl.tensor(builder.create_fcmpUNE(input.handle, other.handle), _bool_like(input))\n@@ -457,37 +457,37 @@ def reshape(input: tl.tensor,\n     numel = 1\n     for s in dst_shape:\n         numel *= s\n-    if input.dtype.numel != numel:\n+    if input.type.numel != numel:\n         raise ValueError(\"cannot reshape block of different shape\")\n-    ret_ty = tl.block_type(input.dtype.scalar, dst_shape)\n+    ret_ty = tl.block_type(input.type.scalar, dst_shape)\n     return tl.tensor(builder.create_reshape(input.handle, dst_shape), ret_ty)\n \n \n def cat(lhs: tl.tensor, rhs: tl.tensor, builder: ir.builder) -> tl.tensor:\n     # TODO: check types\n-    return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), lhs.dtype)\n+    return tl.tensor(builder.create_cat(lhs.handle, rhs.handle), lhs.type)\n \n \n def broadcast_impl_shape(input: tl.tensor,\n                          shape: List[int],\n                          builder: ir.builder) -> tl.tensor:\n-    if not input.dtype.is_block():\n-        ret_ty = tl.block_type(input.dtype, shape)\n+    if not input.type.is_block():\n+        ret_ty = tl.block_type(input.type, shape)\n         return tl.tensor(builder.create_splat(input.handle, shape), ret_ty)\n-    src_shape = input.dtype.get_block_shapes()\n+    src_shape = input.type.get_block_shapes()\n     if len(src_shape) != len(shape):\n         raise ValueError(f\"Cannot broadcast, rank mismatch: {src_shape}, {shape}\")\n     if shape == src_shape:\n         return input\n-    ret_ty = tl.block_type(input.dtype.scalar, shape)\n+    ret_ty = tl.block_type(input.type.scalar, shape)\n     return tl.tensor(builder.create_broadcast(input.handle, shape), ret_ty)\n \n \n def broadcast_impl_value(lhs: tl.tensor,\n                          rhs: tl.tensor,\n                          builder: ir.builder) -> tl.tensor:\n-    lhs_ty = lhs.dtype\n-    rhs_ty = rhs.dtype\n+    lhs_ty = lhs.type\n+    rhs_ty = rhs.type\n \n     # make_shape_compatible(block, scalar)\n     if lhs_ty.is_block() and not rhs_ty.is_block():\n@@ -533,9 +533,9 @@ def broadcast_impl_value(lhs: tl.tensor,\n def bitcast(input: tl.tensor,\n             dst_ty: tl.dtype,\n             builder: ir.builder) -> tl.tensor:\n-    src_ty = input.dtype\n+    src_ty = input.type\n     if src_ty.is_block():\n-        dst_ty = tl.block_type(dst_ty, input.dtype.get_block_shapes())\n+        dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n     if src_ty == dst_ty:\n         return input\n     src_sca_ty = src_ty.scalar\n@@ -555,9 +555,9 @@ def bitcast(input: tl.tensor,\n def cast(input: tl.tensor,\n          dst_ty: tl.dtype,\n          builder: ir.builder) -> tl.tensor:\n-    src_ty = input.dtype\n+    src_ty = input.type\n     if src_ty.is_block():\n-        dst_ty = tl.block_type(dst_ty, input.dtype.get_block_shapes())\n+        dst_ty = tl.block_type(dst_ty, input.type.get_block_shapes())\n     if src_ty == dst_ty:\n         return input\n     src_sca_ty = src_ty.scalar\n@@ -655,17 +655,17 @@ def load(ptr: tl.tensor,\n          eviction_policy: str,\n          is_volatile: bool,\n          builder: ir.builder) -> tl.tensor:\n-    if not ptr.dtype.scalar.is_ptr():\n-        raise ValueError(\"Pointer argument of load instruction is \" + ptr.dtype.__repr__())\n-    if ptr.dtype.is_block():\n+    if not ptr.type.scalar.is_ptr():\n+        raise ValueError(\"Pointer argument of load instruction is \" + ptr.type.__repr__())\n+    if ptr.type.is_block():\n         if mask:\n-            mask = broadcast_impl_shape(mask, ptr.dtype.get_block_shapes(), builder)\n+            mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n         if other:\n-            other = broadcast_impl_shape(other, ptr.dtype.get_block_shapes(), builder)\n+            other = broadcast_impl_shape(other, ptr.type.get_block_shapes(), builder)\n \n     if other:\n-        other = cast(other, ptr.dtype.scalar.element_ty, builder)\n-    ptr_ty = ptr.dtype.scalar\n+        other = cast(other, ptr.type.scalar.element_ty, builder)\n+    ptr_ty = ptr.type.scalar\n     elt_ty = ptr_ty.element_ty\n     # treat bool* as tl.int8*\n     if elt_ty == tl.int1:\n@@ -693,11 +693,8 @@ def load(ptr: tl.tensor,\n         else:\n             raise ValueError(f\"Eviction policy {eviction_policy} not supported\")\n \n-    # assert ptr.dtype.is_block()\n-    # shape = ptr.dtype.get_block_shapes()\n-    # dst_ty = tl.block_type(elt_ty, shape)\n-    if ptr.dtype.is_block():\n-        shape = ptr.dtype.get_block_shapes()\n+    if ptr.type.is_block():\n+        shape = ptr.type.get_block_shapes()\n         dst_ty = tl.block_type(elt_ty, shape)\n     else:\n         dst_ty = elt_ty\n@@ -710,8 +707,8 @@ def load(ptr: tl.tensor,\n \n     if not other:\n         other_ir = ir.undef.get(elt_ty.to_ir(builder))\n-        if ptr.dtype.is_block():\n-            other_ir = builder.create_splat(other_ir, ptr.dtype.get_block_shapes())\n+        if ptr.type.is_block():\n+            other_ir = builder.create_splat(other_ir, ptr.type.get_block_shapes())\n         other = tl.tensor(other_ir, dst_ty)\n \n     return tl.tensor(builder.create_masked_load(ptr.handle,\n@@ -725,13 +722,13 @@ def store(ptr: tl.tensor,\n           val: tl.tensor,\n           mask: Optional[tl.tensor],\n           builder: ir.builder) -> tl.tensor:\n-    if not ptr.dtype.scalar.is_ptr():\n-        raise ValueError(\"Pointer argument of store instruction is \" + ptr.dtype.__repr__())\n-    if ptr.dtype.is_block():\n-        val = broadcast_impl_shape(val, ptr.dtype.get_block_shapes(), builder)\n+    if not ptr.type.scalar.is_ptr():\n+        raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+    if ptr.type.is_block():\n+        val = broadcast_impl_shape(val, ptr.type.get_block_shapes(), builder)\n     if mask:\n-        mask = broadcast_impl_shape(mask, ptr.dtype.get_block_shapes(), builder)\n-    ptr_ty = ptr.dtype.scalar\n+        mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n+    ptr_ty = ptr.type.scalar\n     elt_ty = ptr_ty.element_ty\n     # treat bool* as tl.int8*\n     if elt_ty == tl.int1:\n@@ -743,7 +740,7 @@ def store(ptr: tl.tensor,\n     val = cast(val, elt_ty, builder)\n     if not mask:\n         return tl.tensor(builder.create_store(ptr.handle, val.handle), tl.void)\n-    if not mask.dtype.scalar.is_bool():\n+    if not mask.type.scalar.is_bool():\n         raise ValueError(\"Mask must have boolean scalar type\")\n     return tl.tensor(builder.create_masked_store(ptr.handle, val.handle, mask.handle), tl.void)\n \n@@ -757,27 +754,27 @@ def atomic_cas(ptr: tl.tensor,\n                val: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     # TODO: type checking\n-    return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.dtype)\n+    return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.type)\n \n \n def atom_red_typechecking_impl(ptr: tl.tensor,\n                                val: tl.tensor,\n                                mask: tl.tensor,\n                                builder: ir.builder) -> Tuple[tl.tensor, tl.tensor, tl.tensor]:\n-    if not ptr.dtype.scalar.is_ptr():\n-        raise ValueError(\"Pointer argument of store instruction is \" + ptr.dtype.__repr__())\n-    if ptr.dtype.is_block():\n+    if not ptr.type.scalar.is_ptr():\n+        raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+    if ptr.type.is_block():\n         if mask:\n-            mask = broadcast_impl_shape(mask, ptr.dtype.get_block_shapes(), builder)\n+            mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n         if val:\n-            val = broadcast_impl_shape(val, ptr.dtype.get_block_shapes(), builder)\n-    val = cast(val, ptr.dtype.scalar.element_ty, builder)\n+            val = broadcast_impl_shape(val, ptr.type.get_block_shapes(), builder)\n+    val = cast(val, ptr.type.scalar.element_ty, builder)\n     if not mask:\n         mask_ir = builder.get_int1(True)\n         mask_ty = tl.int1\n-        if ptr.dtype.is_block():\n-            mask_ir = builder.create_splat(mask_ir, ptr.dtype.get_block_shapes())\n-            mask_ty = tl.block_type(tl.int1, ptr.dtype.get_block_shapes())\n+        if ptr.type.is_block():\n+            mask_ir = builder.create_splat(mask_ir, ptr.type.get_block_shapes())\n+            mask_ty = tl.block_type(tl.int1, ptr.type.get_block_shapes())\n         mask = tl.tensor(mask_ir, mask_ty)\n     return ptr, val, mask\n \n@@ -787,30 +784,30 @@ def atomic_max(ptr: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n-    sca_ty = val.dtype.scalar\n+    sca_ty = val.type.scalar\n     # direct call to atomic_max for integers\n     if sca_ty.is_int():\n         if sca_ty.is_int_signed():\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX,\n                                                        ptr.handle,\n                                                        val.handle,\n                                                        mask.handle),\n-                             val.dtype)\n+                             val.type)\n         else:\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMAX,\n                                                        ptr.handle,\n                                                        val.handle,\n                                                        mask.handle),\n-                             val.dtype)\n+                             val.type)\n     # for float\n     # return atomic_smax(i_ptr, i_val) if val >= 0\n     # return atomic_umin(i_ptr, i_val) if val < 0\n     i_val = bitcast(val, tl.int32, builder)\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n     pos = greater_equal(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n     neg = less_than(val, tl.tensor(ir.constant_float.get(sca_ty.to_ir(builder), 0), sca_ty), builder)\n-    pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.dtype)\n-    neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.dtype)\n+    pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n+    neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n \n \n@@ -819,21 +816,21 @@ def atomic_min(ptr: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n-    sca_ty = val.dtype.scalar\n+    sca_ty = val.type.scalar\n     # direct call to atomic_min for integers\n     if sca_ty.is_int():\n         if sca_ty.is_int_signed():\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                        ptr.handle,\n                                                        val.handle,\n                                                        mask.handle),\n-                             val.dtype)\n+                             val.type)\n         else:\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN,\n                                                        ptr.handle,\n                                                        val.handle,\n                                                        mask.handle),\n-                             val.dtype)\n+                             val.type)\n     # for float\n     # return atomic_smin(i_ptr, i_val) if val >= 0\n     # return atomic_umax(i_ptr, i_val) if val < 0\n@@ -845,12 +842,12 @@ def atomic_min(ptr: tl.tensor,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n                                                   and_(mask, pos, builder).handle),\n-                        i_val.dtype)\n+                        i_val.type)\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMAX,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n                                                   and_(mask, neg, builder).handle),\n-                        i_val.dtype)\n+                        i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n \n \n@@ -859,41 +856,41 @@ def atomic_add(ptr: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n-    sca_ty = val.dtype.scalar\n+    sca_ty = val.type.scalar\n     op = ir.ATOMIC_OP.FADD if sca_ty.is_floating() else ir.ATOMIC_OP.ADD\n-    return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.dtype)\n+    return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_and(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.dtype)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_or(ptr: tl.tensor,\n               val: tl.tensor,\n               mask: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.dtype)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xor(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.dtype)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xchg(ptr: tl.tensor,\n                 val: tl.tensor,\n                 mask: tl.tensor,\n                 builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.dtype)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.type)\n \n # ===----------------------------------------------------------------------===//\n #                               Linear Algebra\n@@ -904,15 +901,15 @@ def dot(lhs: tl.tensor,\n         rhs: tl.tensor,\n         allow_tf32: bool,\n         builder: ir.builder) -> tl.tensor:\n-    assert lhs.dtype.is_block() and rhs.dtype.is_block()\n-    if lhs.dtype.scalar.is_int():\n+    assert lhs.type.is_block() and rhs.type.is_block()\n+    if lhs.type.scalar.is_int():\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n     else:\n         _0 = builder.get_float32(0)\n         ret_scalar_ty = tl.float32\n-    M = lhs.dtype.shape[0]\n-    N = rhs.dtype.shape[1]\n+    M = lhs.type.shape[0]\n+    N = rhs.type.shape[1]\n     _0 = builder.create_splat(_0, [M, N])\n     ret_ty = tl.block_type(ret_scalar_ty, [M, N])\n     return tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n@@ -928,18 +925,18 @@ def where(condition: tl.tensor,\n           y: tl.tensor,\n           builder: ir.builder) -> tl.tensor:\n     condition = cast(condition, tl.int1, builder)\n-    if condition.dtype.is_block():\n-        x = broadcast_impl_shape(x, condition.dtype.get_block_shapes(), builder)\n-        y = broadcast_impl_shape(y, condition.dtype.get_block_shapes(), builder)\n+    if condition.type.is_block():\n+        x = broadcast_impl_shape(x, condition.type.get_block_shapes(), builder)\n+        y = broadcast_impl_shape(y, condition.type.get_block_shapes(), builder)\n \n     # TODO: we need to check x's and y's shape?\n-    x_ty = x.dtype.scalar\n-    y_ty = y.dtype.scalar\n+    x_ty = x.type.scalar\n+    y_ty = y.type.scalar\n     ty = computation_type_impl(x_ty, y_ty, div_or_mod=False)\n     x = cast(x, ty, builder)\n     y = cast(y, ty, builder)\n-    if x.dtype.is_block():\n-        ret_ty = tl.block_type(ty, x.dtype.shape)\n+    if x.type.is_block():\n+        ret_ty = tl.block_type(ty, x.type.shape)\n     else:\n         ret_ty = ty\n     return tl.tensor(builder.create_select(condition.handle, x.handle, y.handle), ret_ty)\n@@ -951,15 +948,15 @@ def where(condition: tl.tensor,\n \n def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n                 FLOAT_OP: ir.REDUCE_OP, INT_OP: ir.REDUCE_OP) -> tl.tensor:\n-    scalar_ty = input.dtype.scalar\n+    scalar_ty = input.type.scalar\n     # input is extended to 32-bits if necessary\n     # this increases numerical accuracy and can be done pretty much for free\n     # on GPUs\n     if scalar_ty.is_int() and scalar_ty.int_bitwidth <= 32:\n         input = cast(input, tl.int32, builder)\n \n     # get result type\n-    shape = input.dtype.shape\n+    shape = input.type.shape\n     ret_shape = []\n     for i, s in enumerate(shape):\n         if i != axis:\n@@ -989,7 +986,7 @@ def sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n \n \n def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n-    scalar_ty = input.dtype.scalar\n+    scalar_ty = input.type.scalar\n     if not scalar_ty.is_int():\n         raise ValueError(\"xor_sum only supported for integers\")\n     return reduce_impl(input, axis, builder, \"sum\", ir.REDUCE_OP.XOR, ir.REDUCE_OP.XOR)\n@@ -1001,27 +998,27 @@ def xor_sum(input: tl.tensor, axis: int, builder: ir.builder) -> tl.tensor:\n \n def umulhi(x: tl.tensor, y: tl.tensor, builder: ir.builder) -> tl.tensor:\n     x, y = binary_op_type_checking_impl(x, y, builder)\n-    return tl.tensor(builder.create_umulhi(x.handle, y.handle), x.dtype)\n+    return tl.tensor(builder.create_umulhi(x.handle, y.handle), x.type)\n \n \n def exp(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n-    return tl.tensor(builder.create_exp(x.handle), x.dtype)\n+    return tl.tensor(builder.create_exp(x.handle), x.type)\n \n \n def log(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n-    return tl.tensor(builder.create_log(x.handle), x.dtype)\n+    return tl.tensor(builder.create_log(x.handle), x.type)\n \n \n def cos(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n-    return tl.tensor(builder.create_cos(x.handle), x.dtype)\n+    return tl.tensor(builder.create_cos(x.handle), x.type)\n \n \n def sin(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n-    return tl.tensor(builder.create_sin(x.handle), x.dtype)\n+    return tl.tensor(builder.create_sin(x.handle), x.type)\n \n \n def sqrt(x: tl.tensor, builder: ir.builder) -> tl.tensor:\n-    return tl.tensor(builder.create_sqrt(x.handle), x.dtype)\n+    return tl.tensor(builder.create_sqrt(x.handle), x.type)\n \n \n ##"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -87,8 +87,8 @@ def _layer_norm_bwd_dx_fused(DX, DY, DW, DB, X, W, B, M, V, Lock, stride, N, eps\n     # write-back dx\n     tl.store(DX + cols, dx, mask=mask)\n     # accumulate partial sums for dw/db\n-    partial_dw = (dy * xhat).to(w.dtype.scalar)\n-    partial_db = (dy).to(w.dtype.scalar)\n+    partial_dw = (dy * xhat).to(w.dtype)\n+    partial_db = (dy).to(w.dtype)\n     while tl.atomic_cas(Lock, 0, 1) == 1:\n         pass\n     count = tl.load(Count)"}]