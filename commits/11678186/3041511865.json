[{"filename": "lib/codegen/pass.cc", "status": "modified", "additions": 124, "deletions": 130, "changes": 254, "file_content_changes": "@@ -28,142 +28,136 @@\n #include \"triton/ir/module.h\"\n #include \"triton/ir/print.h\"\n \n-namespace triton\n-{\n-  namespace codegen\n-  {\n+namespace triton {\n+namespace codegen {\n \n-    static void link_extern_libs(const ExternLibMap& user_extern_lib_map,\n-                                 const ExternLibMap& target_extern_lib_map,\n-                                 ir::module& ir, llvm::LLVMContext& ctx,\n-                                 std::unique_ptr<llvm::Module>& llvm) {\n-      for (const auto& iter : target_extern_lib_map) {\n-        auto &lib_name = iter.first;\n-        if (user_extern_lib_map.count(lib_name) != 0 &&\n-            user_extern_lib_map.at(lib_name)->path() != \"\") {\n-          // If the user specified a path for this library, use it.\n-          user_extern_lib_map.at(lib_name)->install(ctx, llvm);\n-        } else {\n-          // Otherwise, use the default path.\n-          iter.second->install(ctx, llvm);\n-        }\n-      }\n+static void link_extern_libs(const ExternLibMap& user_extern_lib_map,\n+                             const ExternLibMap& target_extern_lib_map,\n+                             ir::module& ir, llvm::LLVMContext& ctx,\n+                             std::unique_ptr<llvm::Module>& llvm) {\n+  for (const auto& iter : target_extern_lib_map) {\n+    auto &lib_name = iter.first;\n+    if (user_extern_lib_map.count(lib_name) != 0 &&\n+        user_extern_lib_map.at(lib_name)->path() != \"\") {\n+      // If the user specified a path for this library, use it.\n+      user_extern_lib_map.at(lib_name)->install(ctx, llvm);\n+    } else {\n+      // Otherwise, use the default path.\n+      iter.second->install(ctx, llvm);\n+    }\n+  }\n \n-      std::set<llvm::StringRef> function_names;\n-      for (auto& func : ir.get_function_list()) {\n-        function_names.insert(func->get_name());\n-      }\n-      llvm::legacy::PassManager pass;\n-      pass.add(llvm::createInternalizePass([&](const llvm::GlobalValue& v) -> bool {\n-        if (function_names.count(v.getName()) != 0) {\n-          // Preserve global functions\n-          return true;\n-        }\n-        // Internalize all device functions\n-        return false;\n-      }));\n+  std::set<llvm::StringRef> function_names;\n+  for (auto& func : ir.get_function_list()) {\n+    function_names.insert(func->get_name());\n+  }\n+  llvm::legacy::PassManager pass;\n+  pass.add(llvm::createInternalizePass([&](const llvm::GlobalValue& v) -> bool {\n+    if (function_names.count(v.getName()) != 0) {\n+      // Preserve global functions\n+      return true;\n+    }\n+    // Internalize all device functions\n+    return false;\n+  }));\n \n-      llvm::legacy::PassManager pm;\n-      pm.add(llvm::createVerifierPass());\n-      pm.run(*llvm);\n+  llvm::legacy::PassManager pm;\n+  pm.add(llvm::createVerifierPass());\n+  pm.run(*llvm);\n \n-      llvm::PassManagerBuilder builder;\n-      builder.OptLevel = 3;\n-      builder.SizeLevel = 0;\n-      builder.populateModulePassManager(pass);\n+  llvm::PassManagerBuilder builder;\n+  builder.OptLevel = 3;\n+  builder.SizeLevel = 0;\n+  builder.populateModulePassManager(pass);\n \n-      pass.run(*llvm);\n-    }\n+  pass.run(*llvm);\n+}\n \n-    // TODO:\n-    // There should be a proper pass manager there!\n-    std::unique_ptr<llvm::Module> add_passes_to_emit_bin(\n-        ir::module &ir, llvm::LLVMContext &ctx, codegen::target *target,\n-        int num_warps, int num_stages, int &shared_static,\n-        const ExternLibMap &extern_lib_map)\n-    {\n-      // generate llvm code\n-      std::string name = ir.get_function_list()[0]->get_name();\n-      std::unique_ptr<llvm::Module> llvm(new llvm::Module(name, ctx));\n-      // optimizations\n-      bool has_sm80 = target->as_nvidia() && target->as_nvidia()->sm() >= 80;\n-      // create passes\n-      codegen::analysis::align align;\n-      codegen::transform::inliner inliner;\n-      codegen::analysis::axes axes;\n-      codegen::transform::pipeline pipeline(has_sm80, num_stages);\n-      codegen::transform::disassociate disassociate;\n-      codegen::analysis::layouts layouts(&axes, &align, num_warps, target);\n-      codegen::transform::cts cts(&layouts, has_sm80);\n-      codegen::analysis::liveness liveness(&layouts);\n-      codegen::analysis::swizzle swizzle(&layouts, target);\n-      codegen::analysis::allocation allocation(&liveness);\n-      codegen::transform::dce dce;\n-      codegen::transform::peephole peephole(target, &layouts);\n-      codegen::transform::coalesce coalesce(&align, &layouts, has_sm80);\n-      codegen::transform::prefetch prefetch_s(target);\n-      codegen::transform::membar barriers(&liveness, &layouts, &allocation,\n-                                          &prefetch_s, target);\n-      codegen::generator isel(&axes, &layouts, &align, &allocation, &swizzle,\n-                              target, num_warps);\n-      // run passes\n-      inliner.run(ir);\n-      dce.run(ir);\n-      // ir.print(std::cout);\n-      peephole.run(ir);\n-      dce.run(ir);\n-      pipeline.run(ir);\n-      dce.run(ir);\n-      disassociate.run(ir);\n-      dce.run(ir);\n-      align.run(ir);\n-      axes.run(ir);\n-      layouts.run(ir);\n-      peephole.run(ir);\n-      dce.run(ir);\n-      if (target->is_gpu())\n-        cts.run(ir);\n-      align.run(ir);\n-      axes.run(ir);\n-      layouts.run(ir);\n-      coalesce.run(ir);\n-      dce.run(ir);\n-      align.run(ir);\n-      dce.run(ir);\n-      if (target->is_gpu())\n-        cts.run(ir);\n-      dce.run(ir);\n-      align.run(ir);\n-      axes.run(ir);\n-      layouts.run(ir);\n-      peephole.run(ir);\n-      dce.run(ir);\n-      align.run(ir);\n-      axes.run(ir);\n-      layouts.run(ir);\n-      swizzle.run(ir);\n-      // std::cout << \"---\" << std::endl;\n-      // std::cout << \"---\" << std::endl;\n-      // ir.print(std::cout);\n-      liveness.run(ir);\n-      allocation.run(ir);\n-      prefetch_s.run(ir);\n-      barriers.run(ir);\n-      // exit(1);\n-      // ir.print(std::cout);\n-      // ir.print(std::cout);\n-      isel.visit(ir, *llvm);\n-      shared_static = allocation.allocated_size();\n+// TODO:\n+// There should be a proper pass manager there!\n+std::unique_ptr<llvm::Module> add_passes_to_emit_bin(\n+    ir::module& ir, llvm::LLVMContext& ctx, codegen::target* target,\n+    int num_warps, int num_stages, int& shared_static,\n+    const ExternLibMap& extern_lib_map) {\n+  // generate llvm code\n+  std::string name = ir.get_function_list()[0]->get_name();\n+  std::unique_ptr<llvm::Module> llvm(new llvm::Module(name, ctx));\n+  // optimizations\n+  bool has_sm80 = target->as_nvidia() && target->as_nvidia()->sm() >= 80;\n+  // create passes\n+  codegen::analysis::align align;\n+  codegen::transform::inliner inliner;\n+  codegen::analysis::axes axes;\n+  codegen::transform::pipeline pipeline(has_sm80, num_stages);\n+  codegen::transform::disassociate disassociate;\n+  codegen::analysis::layouts layouts(&axes, &align, num_warps, target);\n+  codegen::transform::cts cts(&layouts, has_sm80);\n+  codegen::analysis::liveness liveness(&layouts);\n+  codegen::analysis::swizzle swizzle(&layouts, target);\n+  codegen::analysis::allocation allocation(&liveness);\n+  codegen::transform::dce dce;\n+  codegen::transform::peephole peephole(target, &layouts);\n+  codegen::transform::coalesce coalesce(&align, &layouts, has_sm80);\n+  codegen::transform::prefetch prefetch_s(target);\n+  codegen::transform::membar barriers(&liveness, &layouts, &allocation,\n+                                      &prefetch_s, target);\n+  codegen::generator isel(&axes, &layouts, &align, &allocation, &swizzle,\n+                          target, num_warps);\n+  // run passes\n+  inliner.run(ir);\n+  dce.run(ir);\n+  // ir.print(std::cout);\n+  peephole.run(ir);\n+  dce.run(ir);\n+  pipeline.run(ir);\n+  dce.run(ir);\n+  disassociate.run(ir);\n+  dce.run(ir);\n+  align.run(ir);\n+  axes.run(ir);\n+  layouts.run(ir);\n+  peephole.run(ir);\n+  dce.run(ir);\n+  if (target->is_gpu()) cts.run(ir);\n+  align.run(ir);\n+  axes.run(ir);\n+  layouts.run(ir);\n+  coalesce.run(ir);\n+  dce.run(ir);\n+  align.run(ir);\n+  dce.run(ir);\n+  if (target->is_gpu()) cts.run(ir);\n+  dce.run(ir);\n+  align.run(ir);\n+  axes.run(ir);\n+  layouts.run(ir);\n+  peephole.run(ir);\n+  dce.run(ir);\n+  align.run(ir);\n+  axes.run(ir);\n+  layouts.run(ir);\n+  swizzle.run(ir);\n+  // std::cout << \"---\" << std::endl;\n+  // ir.print(std::cout);\n+  // std::cout << \"---\" << std::endl;\n+  // ir.print(std::cout);\n+  liveness.run(ir);\n+  allocation.run(ir);\n+  prefetch_s.run(ir);\n+  barriers.run(ir);\n+  // exit(1);\n+  // ir.print(std::cout);\n+  isel.visit(ir, *llvm);\n+  shared_static = allocation.allocated_size();\n \n-      if (isel.get_extern_lib_map().size() > 0)\n-      {\n-        // If there's any extern lib calls,\n-        // we need to link them in.\n-        link_extern_libs(extern_lib_map, isel.get_extern_lib_map(), ir, ctx, llvm);\n-      }\n+  if (isel.get_extern_lib_map().size() > 0) {\n+    // If there's any extern lib calls,\n+    // we need to link them in.\n+    link_extern_libs(extern_lib_map, isel.get_extern_lib_map(), ir, ctx, llvm);\n+  }\n \n-      return llvm;\n-    }\n+  return llvm;\n+}\n \n-  } // namespace codegen\n-} // namespace triton\n+}  // namespace codegen\n+}  // namespace triton"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 9, "deletions": 10, "changes": 19, "file_content_changes": "@@ -295,8 +295,6 @@ def visit_FunctionDef(self, node):\n                 if not isinstance(cst, triton.language.constexpr):\n                     cst = triton.language.constexpr(self.constants[i])\n                 arg_values.append(cst)\n-                if i in self.spec_to_1:\n-                    idx += 1\n             else:\n                 if i in self.attributes:\n                     is_ptr = fn.args[idx].type.is_ptr()\n@@ -818,18 +816,18 @@ def make_triton_ir(fn, signature, specialization, constants):\n     # create kernel prototype\n     cst_key = lambda i: fn.arg_names.index(i) if isinstance(i, str) else i\n     constants = {cst_key(key): value for key, value in constants.items()}\n-    arg_types = signature.replace(' ', '').split(',')\n-    arg_types = [str_to_ty(x) for i, x in enumerate(arg_types) if i not in constants]\n     # visit kernel AST\n     gscope = fn.__globals__.copy()\n     function_name = '_'.join([fn.__name__, kernel_suffix(signature, specialization)])\n     new_constants = {k: 1 for k in specialization.equal_to_1}\n     new_attrs = {k: (\"multiple_of\", 16) for k in specialization.divisible_by_16}\n     all_constants = constants.copy()\n     all_constants.update(new_constants)\n+    arg_types = signature.replace(' ', '').split(',')\n+    arg_types = [str_to_ty(x) for i, x in enumerate(arg_types) if i not in all_constants]\n \n     prototype = triton.language.function_type(triton.language.void, arg_types)\n-    generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants, function_name=function_name, spec_to_1=specialization.equal_to_1, attributes=new_attrs, is_kernel=True)\n+    generator = CodeGenerator(context, prototype, gscope=gscope, constants=all_constants, function_name=function_name, attributes=new_attrs, is_kernel=True)\n     try:\n         generator.visit(fn.parse())\n     except Exception as e:\n@@ -969,7 +967,7 @@ def _build(name, src, path):\n     return so\n \n \n-def generate_torch_glue(kernel_name, signature, num_warps, binaries, tmpdir):\n+def generate_torch_glue(kernel_name, config, signature, num_warps, binaries, tmpdir):\n     headers = dict()\n     tys = signature.split(',')\n     # write all cubins to header files\n@@ -1075,7 +1073,7 @@ def format_of(ty):\n   if(function == 0)\n     init_module(device);\n \n-  void *params[] = {{ {', '.join(f\"&arg{i}\" if tys[i][0]=='*' else f\"(void*)&arg{i}\" for i in range(n_args))} }};\n+  void *params[] = {{ {', '.join(f\"&arg{i}\" if tys[i][0]=='*' else f\"(void*)&arg{i}\" for i in range(n_args) if i not in config.equal_to_1)} }};\n   CUDA_CHECK(cuLaunchKernel(function, gridX, gridY, gridZ, 32*{num_warps}, 1, 1, {name}_shmem, stream, params, 0));\n }}\n \n@@ -1196,8 +1194,8 @@ def make_cache_key(fn, signature, configs, constants, num_warps, num_stages):\n     return key\n \n \n-def make_shared_object(fn, signature, num_warps, binaries, tmpdir):\n-    src = generate_torch_glue(fn.__name__, signature, num_warps, binaries, tmpdir)\n+def make_shared_object(fn, config, signature, num_warps, binaries, tmpdir):\n+    src = generate_torch_glue(fn.__name__, config, signature, num_warps, binaries, tmpdir)\n     src_path = os.path.join(tmpdir, \"main.c\")\n     with open(src_path, \"w\") as f:\n         f.write(src)\n@@ -1214,6 +1212,7 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n         ref, _ = make_triton_ir(fn, signature, _triton.code_gen.instance_descriptor(), constants)\n         fns = ref.get_functions()\n         configs = _triton.infer_specialization_configs(fns[0])\n+    assert len(configs) == 1\n     # cache manager\n     cache_key = make_cache_key(fn, signature, configs, constants, num_warps, num_stages)\n     cache_manager = CacheManager(cache_key)\n@@ -1226,7 +1225,7 @@ def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: i\n         binaries.append(_compile(fn, signature, device, constants, config, num_warps, num_stages, extern_libs, \"cubin\"))\n     # generate and compile glue code into shared object\n     with tempfile.TemporaryDirectory() as tmpdir:\n-        so = make_shared_object(fn, signature, num_warps, binaries, tmpdir)\n+        so = make_shared_object(fn, configs[0], signature, num_warps, binaries, tmpdir)\n \n     # write shared object to cache\n     cache_manager.put(so)"}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "file_content_changes": "@@ -237,11 +237,7 @@ def dsd_matmul(a, b, trans_a, trans_b, trans_c, spdims, block, lut, width, out=N\n         TILE_M=block, TILE_N=TILE_N, TILE_K=min(block, 32), BLOCK=block, num_stages=4,\n         num_warps=4, GROUP_SIZE_M=4,\n     )\n-    # print(pgm.asm[\"ptx\"])\n-    print(a.stride())\n-    print(b.stride())\n-    print(c.stride())\n-    print(BS3, AS1)\n+    print(pgm.asm[\"ptx\"])\n     # exit()\n     return c\n "}]