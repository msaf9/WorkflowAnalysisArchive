[{"filename": "python/test/unit/operators/test_flash_attention.py", "status": "modified", "additions": 0, "deletions": 7, "changes": 7, "file_content_changes": "@@ -13,13 +13,6 @@\n @pytest.mark.parametrize('causal', [True, False])\n @pytest.mark.parametrize('seq_par', [True, False])\n def test_op(Z, H, N_CTX, D_HEAD, dtype, causal, seq_par):\n-    # with ENABLE_TMA=0 and ENABLE_MMA_V3=0\n-    import os\n-    enable_mmav3 = os.environ.get('ENABLE_MMA_V3', 'not found').lower()\n-    enable_tma = os.environ.get('ENABLE_TMA', 'not found').lower()\n-    if enable_mmav3 in [\"on\", \"true\", \"1\"] and enable_tma in [\"on\", \"true\", \"1\"]:\n-        pytest.skip('Segmentation fault')\n-\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 8:\n         pytest.skip(\"Flash attention only supported for compute capability < 80\")"}]