[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 86, "deletions": 0, "changes": 86, "file_content_changes": "@@ -1346,6 +1346,92 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n     )\n \n \n+def test_sum_kernel_ttgir():\n+    ir = \"\"\"\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  tt.func public @sum_kernel_0d1d(%arg0: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n+    %cst = arith.constant dense<128> : tensor<128x1xi32, #blocked>\n+    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<128x1xi32, #blocked>\n+    %2 = arith.muli %1, %cst : tensor<128x1xi32, #blocked>\n+    %3 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+    %4 = tt.expand_dims %3 {axis = 0 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x128xi32, #blocked>\n+    %5 = tt.broadcast %2 : (tensor<128x1xi32, #blocked>) -> tensor<128x128xi32, #blocked>\n+    %6 = tt.broadcast %4 : (tensor<1x128xi32, #blocked>) -> tensor<128x128xi32, #blocked>\n+    %7 = arith.addi %5, %6 : tensor<128x128xi32, #blocked>\n+    %8 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<128x128x!tt.ptr<i32>, #blocked>\n+    %9 = tt.addptr %8, %7 : tensor<128x128x!tt.ptr<i32>, #blocked>, tensor<128x128xi32, #blocked>\n+    %10 = tt.load %9 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x128xi32, #blocked>\n+    %11 = \"tt.reduce\"(%10) ({\n+    ^bb0(%arg2: i32, %arg3: i32):\n+      %13 = arith.addi %arg2, %arg3 : i32\n+      tt.reduce.return %13 : i32\n+    }) {axis = 1 : i32} : (tensor<128x128xi32, #blocked>) -> tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %12 = tt.expand_dims %11 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<128x1xi32, #blocked>\n+    %13 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %14 = tt.expand_dims %13 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<128x1xi32, #blocked>\n+    %18 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<128x1x!tt.ptr<i32>, #blocked>\n+    %20 = tt.addptr %18, %14 : tensor<128x1x!tt.ptr<i32>, #blocked>, tensor<128x1xi32, #blocked>\n+    tt.store %20, %12 {cache = 1 : i32, evict = 1 : i32} : tensor<128x1xi32, #blocked>\n+    tt.return\n+  }\n+}\n+    \"\"\"\n+\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+\n+    BLOCK_SIZE = 128\n+    x = np.ones((BLOCK_SIZE, BLOCK_SIZE), dtype=np.int32)\n+    y = np.zeros((BLOCK_SIZE, 1), dtype=np.int32)\n+    x_tri = torch.tensor(x, device='cuda')\n+    y_tri = torch.tensor(y, device='cuda')\n+\n+    kernel[(1, 1, 1)](x_tri, y_tri)\n+    y_ref = np.sum(x, axis=1, keepdims=True)\n+\n+    np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n+def test_sum_kernel():\n+\n+    @triton.jit\n+    def sum_kernel(\n+        x_ptr,\n+        y_ptr,\n+        STRIDE: tl.constexpr,\n+        BLOCK_SIZE: tl.constexpr\n+    ):\n+        arange = tl.arange(0, BLOCK_SIZE)\n+        off = arange[:, None] * STRIDE + arange[None, :]\n+        x_val = tl.load(x_ptr + off)\n+\n+        # This 2D reduction doesn't work\n+        x_sum = tl.sum(x_val, axis=1)\n+        x_sum = tl.sum(x_sum, axis=0)\n+\n+        # This 1D reduction works\n+        #x_sum = tl.sum(tl.view(x_val, (BLOCK_SIZE*BLOCK_SIZE,)), axis=0)\n+\n+        tl.store(y_ptr, x_sum)\n+\n+    BLOCK_SIZE = 128\n+    x = np.ones((BLOCK_SIZE, BLOCK_SIZE), dtype=np.long)\n+    y = np.zeros((1,), dtype=np.long)\n+    x_tri = torch.tensor(x, device='cuda')\n+    y_tri = torch.tensor(y, device='cuda')\n+\n+    sum_kernel[(1,)](x_tri, y_tri, x_tri.stride(0), BLOCK_SIZE)\n+\n+    y_ref = np.sum(x)\n+\n+    np.testing.assert_allclose(y_ref, y_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+\n+\n def test_generic_reduction(device='cuda'):\n \n     @triton.jit"}]