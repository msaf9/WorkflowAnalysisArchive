[{"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 32, "deletions": 9, "changes": 41, "file_content_changes": "@@ -1285,20 +1285,43 @@ void generator::visit_atomic_rmw_inst(ir::atomic_rmw_inst *atom) {\n \n   // vector size\n   int vec = 1;\n+  Value *mask = builder_->getInt1(true);\n   if(atom->get_type()->is_block_ty()){\n+    auto shape = atom->get_type()->get_block_shapes();\n     int ld = ords_.at(ptr)[0];\n     unsigned alignment = alignment_->get(ptr, ld);\n     vec = std::min<int>(layouts_->get(ptr)->to_scanline()->nts(ld), alignment);\n     vec = std::min(vec, val->get_type()->get_tile_element_ty()->is_fp16_ty() ? 2 : 1);\n+    // mask out inactive threads\n+    analysis::data_layout* layout = layouts_->get(val);\n+    auto curr_axes = a_axes_->get(val);\n+    auto layt_axes = layout->get_axes();\n+    for(unsigned k = 0; k < layt_axes.size(); k++){\n+      unsigned ax = layt_axes.at(k);\n+      distributed_axis dax = axes_.at(ax);\n+      // axis is part of the original layout: thread id should be 0\n+      // but not the current layout\n+      if(std::find(curr_axes.begin(), curr_axes.end(), ax) == curr_axes.end())\n+        mask = and_(mask, icmp_eq(dax.thread_id, i32(0)));\n+    }\n+    // last axis may spillover\n+    Value *thread_id = tgt_->get_local_id(mod_, *builder_, 0);\n+    int per_thread = 1;\n+    for(int ax: layt_axes) { per_thread *= axes_.at(ax).contiguous; }\n+    int numel = 1;\n+    for(int s: layout->get_shape()) { numel *= s; }\n+    mask = and_(mask, icmp_ult(mul(thread_id, i32(per_thread)), i32(numel)));\n   }\n \n+\n   for(int i = 0; i < idxs_.at(val).size(); i += vec){\n     auto idx = idxs_[val][i];\n     Value *rmw_val = UndefValue::get(vec_ty(vals_[val][idx]->getType(), vec));\n     for(int ii = 0; ii < vec; ii++)\n       rmw_val = insert_elt(rmw_val, vals_[val][idxs_[val][i+ii]], ii);\n     Value *rmw_ptr = vals_[ptr][idx];\n     Value *rmw_msk = vals_[msk][idx];\n+    rmw_msk = and_(rmw_msk, mask);\n     if(vec == 1)\n       rmw_val = extract_elt(rmw_val, i32(0));\n     Type* ty = rmw_val->getType();\n@@ -3402,36 +3425,36 @@ void generator::visit_layout_mma(analysis::mma_layout* layout) {\n }\n \n void generator::visit_layout_scanline(analysis::scanline_layout* layout) {\n-  Value* u_thread_id = tgt_->get_local_id(mod_, *builder_, 0);\n+  Value* thread_id = tgt_->get_local_id(mod_, *builder_, 0);\n   auto order = layout->get_order();\n   const auto& shape = layout->get_shape();\n   // Delinearize\n   size_t dim = shape.size();\n-  std::vector<Value*> thread_id(dim);\n+  std::vector<Value*> thread_ids(dim);\n   for(unsigned k = 0; k < dim - 1; k++){\n     Constant *dim_k = i32(layout->mts(order[k]));\n-    Value *rem = urem(u_thread_id, dim_k);\n-    u_thread_id = udiv(u_thread_id, dim_k);\n-    thread_id[order[k]] = rem;\n+    Value *rem = urem(thread_id, dim_k);\n+    thread_id = udiv(thread_id, dim_k);\n+    thread_ids[order[k]] = rem;\n   }\n   Constant *dim_k = i32(layout->mts(order[dim - 1]));\n-  thread_id[order[dim - 1]] = urem(u_thread_id, dim_k);\n+  thread_ids[order[dim - 1]] = urem(thread_id, dim_k);\n \n   // Create axes\n   for(unsigned k = 0; k < dim; k++) {\n     int nts = layout->nts(k);\n     int mts = layout->mts(k);\n     std::string str_k = std::to_string(k);\n     Value *contiguous_k = i32(nts);\n-    Value *scaled_thread_id = mul(thread_id[k], contiguous_k);\n+    Value *scaled_thread_ids = mul(thread_ids[k], contiguous_k);\n     unsigned per_cta  = layout->shape_per_cta(k);\n     unsigned per_thread = nts * shape[k] / per_cta;\n     std::vector<Value*> idx_list(per_thread);\n     for(unsigned n = 0 ; n < per_thread; n++){\n       unsigned offset = n / nts * per_cta + n % nts;\n-      idx_list[n] = add(scaled_thread_id, i32(offset), \"idx_\" + str_k + \"_\" + std::to_string(n));\n+      idx_list[n] = add(scaled_thread_ids, i32(offset), \"idx_\" + str_k + \"_\" + std::to_string(n));\n     }\n-    axes_[layout->get_axis(k)] = distributed_axis{nts, idx_list, thread_id[k]};\n+    axes_[layout->get_axis(k)] = distributed_axis{nts, idx_list, thread_ids[k]};\n   }\n }\n "}, {"filename": "lib/codegen/transform/coalesce.cc", "status": "modified", "additions": 7, "deletions": 41, "changes": 48, "file_content_changes": "@@ -15,42 +15,6 @@ namespace transform{\n coalesce::coalesce(analysis::align* align, analysis::layouts *layouts, bool has_sm80)\n   : align_(align), layout_(layouts), has_sm80_(has_sm80) { }\n \n-\n-// simplify layout conversions using the following simple rules:\n-//   - cvt_1(cvt_2(x)) if convert1 is the inverse of convert2\n-//   - cvt_1(elementwise(x, y)) = elementwise(convert(x), convert(y))\n-//ir::value* coalesce::simplify(ir::instruction *inst, ir::builder& builder){\n-//  ir::value* _op = inst->get_operand(0);\n-//  ir::instruction* op = dynamic_cast<ir::instruction*>(_op);\n-//  analysis::mma_layout* mma_in  = layout_->get(op)  ->to_mma();\n-//  analysis::mma_layout* mma_out = layout_->get(inst)->to_mma();\n-//  std::cout << 1 << std::endl;\n-//  // i must be layout conversion instruction\n-//  if(!mma_in && !mma_out)\n-//    return inst;\n-//  //   - cvt_1(cvt_2(x)) if convert1 is the inverse of convert2\n-//  bool is_op_cvt = op->get_id() == ir::INST_CVT_LAYOUT;\n-//  if((mma_in || mma_out) && is_op_cvt &&\n-//     (layout_->get(inst) == layout_->get(op->get_operand(0))))\n-//    return op->get_operand(0);\n-//  //   - cvt_1(elementwise(x, y)) = elementwise(cvt_1(x), cvt_2(y))\n-//  if(op->get_id() != ir::INST_BINOP && op->get_id() != ir::INST_GETELEMENTPTR)\n-//    return inst;\n-//  std::cout << 1 << std::endl;\n-//  for(size_t i = 0; i < op->get_num_operands(); i++){\n-//    ir::value* arg_i = op->get_operand(i);\n-//    builder.set_insert_point(op);\n-//    // create new layout transform\n-//    ir::instruction* new_arg_i = inst->clone();\n-//    builder.insert(new_arg_i);\n-//    // set the right args\n-//    new_arg_i->replace_uses_of_with(new_arg_i->get_operand(0), arg_i);\n-//    op->replace_uses_of_with(arg_i, simplify(new_arg_i, builder));\n-//  }\n-//  std::cout << 2 << std::endl;\n-//  return op;\n-//}\n-\n void coalesce::run(ir::module &mod) {\n   std::set<analysis::data_layout*> invalidated;\n   ir::builder& builder = mod.get_builder();\n@@ -62,7 +26,7 @@ void coalesce::run(ir::module &mod) {\n     if(dynamic_cast<ir::store_inst*>(i) || dynamic_cast<ir::atomic_rmw_inst*>(i))\n     if(ir::value* op = i->get_operand(1))\n     if(op->get_type()->is_block_ty())\n-    if(op->get_type()->get_tile_rank() == 2)\n+    if(op->get_type()->get_tile_ranks1() == 2)\n     if(invalidated.find(layout_->get(op)) == invalidated.end())\n     if(layout_->get(op)->to_mma())\n     if(dynamic_cast<ir::io_inst*>(i)->get_eviction_policy()==ir::io_inst::NORMAL){\n@@ -78,7 +42,7 @@ void coalesce::run(ir::module &mod) {\n     if(dynamic_cast<ir::copy_to_shared_inst*>(i) || dynamic_cast<ir::reduce_inst*>(i))\n     if(ir::value* op = i->get_operand(0))\n     if(op->get_type()->is_block_ty())\n-    if(op->get_type()->get_tile_rank() == 2)\n+    if(op->get_type()->get_tile_ranks1() == 2)\n     if(invalidated.find(layout_->get(op)) == invalidated.end())\n     if(layout_->get(op)->to_mma()){\n       ir::instruction* new_op = ir::cvt_layout_inst::create(op);\n@@ -91,7 +55,7 @@ void coalesce::run(ir::module &mod) {\n     // uncoalesce after load\n     if(auto x = dynamic_cast<ir::load_inst*>(i))\n     if(x->get_type()->is_block_ty())\n-    if(x->get_type()->get_tile_rank()==2)\n+    if(x->get_type()->get_tile_ranks1()==2)\n     if(layout_->get(x)->to_mma())\n     if(!has_sm80_ || dynamic_cast<ir::io_inst*>(i)->get_eviction_policy()==ir::io_inst::NORMAL){\n         builder.set_insert_point_after(x);\n@@ -111,9 +75,11 @@ void coalesce::run(ir::module &mod) {\n       auto out_contig = align_->contiguous(ptr);\n       auto val_inst = dynamic_cast<ir::instruction*>(val);\n       if(!val_inst)\n-        break;\n+        continue;\n       if(dynamic_cast<ir::cvt_layout_inst*>(val))\n-        break;\n+        continue;\n+      if(!val->get_type()->is_block_ty() || val->get_type()->get_tile_ranks1()==1)\n+        continue;\n       std::vector<unsigned> in_contig;\n       std::vector<ir::instruction*> queue = {val_inst};\n       std::set<ir::instruction*> seen;"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "file_content_changes": "@@ -532,6 +532,29 @@ def kernel(X, Z):\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n \n+@pytest.mark.parametrize(\"axis\", [0, 1])\n+def test_tensor_atomic_rmw(axis, device=\"cuda\"):\n+    shape0, shape1 = 8, 8\n+    # triton kernel\n+\n+    @triton.jit\n+    def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n+        off0 = tl.arange(0, SHAPE0)\n+        off1 = tl.arange(0, SHAPE1)\n+        x = tl.load(X + off0[:, None] * SHAPE1 + off1[None, :])\n+        z = tl.sum(x, axis=AXIS)\n+        tl.atomic_add(Z + off0, z)\n+    rs = RandomState(17)\n+    x = numpy_random((shape0, shape1), dtype_str=\"float32\", rs=rs)\n+    # reference result\n+    z_ref = np.sum(x, axis=axis)\n+    # triton result\n+    x_tri = to_triton(x, device=device)\n+    z_tri = to_triton(np.zeros((shape0,), dtype=\"float32\"), device=device)\n+    kernel[(1,)](z_tri, x_tri, axis, shape0, shape1)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n+\n+\n def test_atomic_cas():\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -370,6 +370,17 @@ def __eq__(self, other):\n     def __call__(self, *args, **kwds):\n         return self.value(*args, **kwds)\n \n+    def to(self, dtype, bitcast=False, _builder=None):\n+        if dtype in [float8, float16, bfloat16]:\n+            raise ValueError(\"floating point constexpr must be float64\")\n+        if dtype.is_int():\n+            ret_ty = int\n+        elif dtype.is_bool():\n+            ret_ty = bool\n+        elif dtype.is_floating():\n+            ret_ty = float\n+        return constexpr(ret_ty(self.value))\n+\n \n class tensor:\n     # infer dtype from ir type"}]