[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -20,6 +20,18 @@ template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n \n template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n \n+// output[i] = input[order[i]]\n+template <typename T>\n+SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n+  size_t rank = order.size();\n+  assert(input.size() == rank);\n+  SmallVector<T> result(rank);\n+  for (auto it : llvm::enumerate(order)) {\n+    result[it.index()] = input[it.value()];\n+  }\n+  return result;\n+}\n+\n } // namespace mlir\n \n #endif // TRITON_ANALYSIS_UTILITY_H"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -22,7 +22,7 @@ namespace gpu {\n \n unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape);\n \n-unsigned getShapePerCTA(const Attribute &layout, unsigned d);\n+SmallVector<unsigned> getShapePerCTA(const BlockedEncodingAttr &layout);\n \n } // namespace gpu\n } // namespace triton"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 80, "deletions": 117, "changes": 197, "file_content_changes": "@@ -48,10 +48,28 @@ Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n                                            IntegerAttr::get(i32ty, v));\n }\n \n+// Create a index type constant.\n+Value createIndexConstant(OpBuilder &builder, Location loc,\n+\n+                          TypeConverter *converter, int64_t value) {\n+  Type ty = converter->convertType(builder.getIndexType());\n+  return builder.create<LLVM::ConstantOp>(loc, ty,\n+                                          builder.getIntegerAttr(ty, value));\n+}\n+\n+// Create an integer constant of \\param width bits.\n+Value createLLVMIntegerConstant(OpBuilder &builder, Location loc, int width,\n+                                int64_t value) {\n+  Type ty = builder.getIntegerType(width);\n+  return builder.create<LLVM::ConstantOp>(loc, ty,\n+                                          builder.getIntegerAttr(ty, value));\n+}\n+\n // Add other specification if needed...\n \n } // namespace\n \n+// Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n #define urem(...) rewriter.create<LLVM::URemOp>(loc, __VA_ARGS__)\n #define add(...) rewriter.create<LLVM::AddOp>(loc, __VA_ARGS__)\n@@ -67,9 +85,15 @@ Value createConstantI32(Location loc, PatternRewriter &rewriter, int32_t v) {\n #define extract_element(...)                                                   \\\n   rewriter.create<LLVM::ExtractElementOp>(loc, __VA_ARGS__)\n #define address_of(...) rewriter.create<LLVM::AddressOfOp>(loc, __VA_ARGS__)\n+#define i32_ty() rewriter.getIntegerType(32)\n \n+// Creator for constant\n #define i32_val(...) LLVM::createConstantI32(loc, rewriter, __VA_ARGS__)\n-#define i32_ty() rewriter.getIntegerType(32)\n+#define int_val(width, val)                                                    \\\n+  LLVM::createLLVMIntegerConstant(rewriter, loc, width, val)\n+#define idx_val(...)                                                           \\\n+  LLVM::createIndexConstant(rewriter, loc, this->getTypeConverter(),           \\\n+                            __VA_ARGS__)\n \n } // namespace LLVM\n } // namespace mlir\n@@ -245,19 +269,6 @@ struct ReturnOpConversion : public ConvertOpToLLVMPattern<::mlir::ReturnOp> {\n   }\n };\n \n-static Value createIndexAttrConstant(OpBuilder &builder, Location loc,\n-                                     Type resultType, int64_t value) {\n-  return builder.create<LLVM::ConstantOp>(\n-      loc, resultType, builder.getIntegerAttr(builder.getIndexType(), value));\n-}\n-\n-static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n-                                       LLVMTypeConverter *converter, Type ty,\n-                                       int64_t value) {\n-  return builder.create<LLVM::ConstantOp>(loc, converter->convertType(ty),\n-                                          builder.getIntegerAttr(ty, value));\n-}\n-\n Value getStructFromElements(Location loc, ValueRange resultVals,\n                             ConversionPatternRewriter &rewriter,\n                             Type structType) {\n@@ -270,42 +281,36 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n }\n \n template <typename T>\n-static SmallVector<T> getMultiDimIndex(T linear_index, ArrayRef<T> shape) {\n-  // sizes {a, b, c, d}  ->  acc_mul {b*c*d, c*d, d, 1}\n+static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n+  // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n   size_t rank = shape.size();\n-  T acc_mul = 1;\n-  for (size_t i = 1; i < rank; ++i) {\n-    acc_mul *= shape[i];\n-  }\n-  T linear_remain = linear_index;\n-  SmallVector<T> multidim_index(rank);\n+  T accMul = product(shape.drop_front());\n+  T linearRemain = linearIndex;\n+  SmallVector<T> multiDimIndex(rank);\n   for (size_t i = 0; i < rank; ++i) {\n-    multidim_index[i] = linear_remain / acc_mul;\n-    linear_remain = linear_remain % acc_mul;\n+    multiDimIndex[i] = linearRemain / accMul;\n+    linearRemain = linearRemain % accMul;\n     if (i != (rank - 1)) {\n-      acc_mul = acc_mul / shape[i + 1];\n+      accMul = accMul / shape[i + 1];\n     }\n   }\n-  return multidim_index;\n+  return multiDimIndex;\n }\n \n template <typename T>\n-static T getLinearIndex(ArrayRef<T> multidim_index, ArrayRef<T> shape) {\n-  assert(multidim_index.size() == shape.size());\n-  // sizes {a, b, c, d}  ->  acc_mul {b*c*d, c*d, d, 1}\n+static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n+  assert(multiDimIndex.size() == shape.size());\n+  // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n   size_t rank = shape.size();\n-  T acc_mul = 1;\n-  for (size_t i = 1; i < rank; ++i) {\n-    acc_mul *= shape[i];\n-  }\n-  T linear_index = 0;\n+  T accMul = product(shape.drop_front());\n+  T linearIndex = 0;\n   for (size_t i = 0; i < rank; ++i) {\n-    linear_index += multidim_index[i] * acc_mul;\n+    linearIndex += multiDimIndex[i] * accMul;\n     if (i != (rank - 1)) {\n-      acc_mul = acc_mul / shape[i + 1];\n+      accMul = accMul / shape[i + 1];\n     }\n   }\n-  return linear_index;\n+  return linearIndex;\n }\n \n struct ConvertTritonGPUOpToLLVMPatternBase {\n@@ -343,16 +348,15 @@ class ConvertTritonGPUOpToLLVMPattern\n     return threadId;\n   }\n \n+  // Convert an \\param index to a multi-dim coordinate given \\param shape and\n+  // \\param order.\n   SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n                                  Location loc, Value linear,\n                                  ArrayRef<unsigned> shape,\n                                  ArrayRef<unsigned> order) const {\n     unsigned rank = shape.size();\n     assert(rank == order.size());\n-    SmallVector<unsigned> reordered(rank);\n-    for (unsigned i = 0; i < rank; ++i) {\n-      reordered[i] = shape[order[i]];\n-    }\n+    auto reordered = reorder(shape, order);\n     auto reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n     SmallVector<Value> multiDim(rank);\n     for (unsigned i = 0; i < rank; ++i) {\n@@ -372,9 +376,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     } else {\n       Value remained = linear;\n       for (auto &&en : llvm::enumerate(llvm::reverse(shape.drop_front()))) {\n-        Value dimSize = createIndexAttrConstant(\n-            rewriter, loc, this->getTypeConverter()->getIndexType(),\n-            en.value());\n+        Value dimSize = idx_val(en.value());\n         multiDim[rank - 1 - en.index()] = urem(remained, dimSize);\n         remained = udiv(remained, dimSize);\n       }\n@@ -386,28 +388,27 @@ class ConvertTritonGPUOpToLLVMPattern\n   Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n                   ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n     int rank = multiDim.size();\n-    Value linear = createIndexAttrConstant(\n-        rewriter, loc, this->getTypeConverter()->getIndexType(), 0);\n+    Value linear = idx_val(0);\n     if (rank > 0) {\n       linear = multiDim.front();\n-      for (auto &&z : llvm::zip(multiDim.drop_front(), shape.drop_front())) {\n-        Value dimSize = createIndexAttrConstant(\n-            rewriter, loc, this->getTypeConverter()->getIndexType(),\n-            std::get<1>(z));\n-        linear = add(mul(linear, dimSize), std::get<0>(z));\n+      for (auto [dim, shape] :\n+           llvm::zip(multiDim.drop_front(), shape.drop_front())) {\n+        Value dimSize = idx_val(shape);\n+        linear = add(mul(linear, dimSize), dim);\n       }\n     }\n     return linear;\n   }\n \n+  // Get an index-base for each dimension for a \\param blocked_layout.\n   SmallVector<Value>\n   emitBaseIndexForBlockedLayout(Location loc,\n                                 ConversionPatternRewriter &rewriter,\n                                 const BlockedEncodingAttr &blocked_layout,\n                                 ArrayRef<int64_t> shape) const {\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n     Value threadId = getThreadId(rewriter, loc);\n-    Value warpSize = createIndexAttrConstant(rewriter, loc, llvmIndexTy, 32);\n+    Value warpSize = idx_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n     auto sizePerThread = blocked_layout.getSizePerThread();\n@@ -428,19 +429,13 @@ class ConvertTritonGPUOpToLLVMPattern\n       unsigned maxWarps =\n           ceil<unsigned>(shape[k], sizePerThread[k] * threadsPerWarp[k]);\n       unsigned maxThreads = ceil<unsigned>(shape[k], sizePerThread[k]);\n-      multiDimWarpId[k] =\n-          urem(multiDimWarpId[k],\n-               createIndexAttrConstant(rewriter, loc, llvmIndexTy, maxWarps));\n-      multiDimThreadId[k] =\n-          urem(multiDimThreadId[k],\n-               createIndexAttrConstant(rewriter, loc, llvmIndexTy, maxThreads));\n+      multiDimWarpId[k] = urem(multiDimWarpId[k], idx_val(maxWarps));\n+      multiDimThreadId[k] = urem(multiDimThreadId[k], idx_val(maxThreads));\n       // multiDimBase[k] = (multiDimThreadId[k] +\n       //                    multiDimWarpId[k] * threadsPerWarp[k]) *\n       //                   sizePerThread[k];\n-      Value threadsPerWarpK = createIndexAttrConstant(\n-          rewriter, loc, llvmIndexTy, threadsPerWarp[k]);\n-      Value sizePerThreadK =\n-          createIndexAttrConstant(rewriter, loc, llvmIndexTy, sizePerThread[k]);\n+      Value threadsPerWarpK = idx_val(threadsPerWarp[k]);\n+      Value sizePerThreadK = idx_val(sizePerThread[k]);\n       multiDimBase[k] =\n           mul(sizePerThreadK, add(multiDimThreadId[k],\n                                   mul(multiDimWarpId[k], threadsPerWarpK)));\n@@ -473,25 +468,22 @@ class ConvertTritonGPUOpToLLVMPattern\n     if (auto blockedParent = parent.dyn_cast<BlockedEncodingAttr>()) {\n       SmallVector<int64_t> paddedShape(rank + 1);\n       for (unsigned d = 0; d < rank + 1; ++d) {\n-        if (d < dim) {\n+        if (d < dim)\n           paddedShape[d] = shape[d];\n-        } else if (d == dim) {\n+        else if (d == dim)\n           paddedShape[d] = 1;\n-        } else {\n+        else\n           paddedShape[d] = shape[d - 1];\n-        }\n       }\n       auto paddedIndices = emitIndicesForBlockedLayout(\n           loc, rewriter, blockedParent, paddedShape);\n       unsigned numIndices = paddedIndices.size();\n       SmallVector<SmallVector<Value>> resultIndices(numIndices);\n-      for (unsigned i = 0; i < numIndices; ++i) {\n-        for (unsigned d = 0; d < rank + 1; ++d) {\n-          if (d != dim) {\n+      for (unsigned i = 0; i < numIndices; ++i)\n+        for (unsigned d = 0; d < rank + 1; ++d)\n+          if (d != dim)\n             resultIndices[i].push_back(paddedIndices[i][d]);\n-          }\n-        }\n-      }\n+\n       return resultIndices;\n \n     } else if (auto sliceParent = parent.dyn_cast<SliceEncodingAttr>()) {\n@@ -519,23 +511,16 @@ class ConvertTritonGPUOpToLLVMPattern\n     auto threadsPerWarp = blockedLayout.getThreadsPerWarp();\n     auto warpsPerCTA = blockedLayout.getWarpsPerCTA();\n     unsigned rank = shape.size();\n-    SmallVector<unsigned> shapePerCTA(rank);\n-    for (unsigned k = 0; k < rank; ++k) {\n-      shapePerCTA[k] = sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k];\n-    }\n+    SmallVector<unsigned> shapePerCTA = getShapePerCTA(blockedLayout);\n \n     // step 1, delinearize threadId to get the base index\n     auto multiDimBase =\n         emitBaseIndexForBlockedLayout(loc, rewriter, blockedLayout, shape);\n \n     // step 2, get offset of each element\n-    unsigned elemsPerThread = 1;\n+    unsigned elemsPerThread = blockedLayout.getElemsPerThread(shape);\n     SmallVector<SmallVector<unsigned>> offset(rank);\n-    SmallVector<unsigned> multiDimElemsPerThread(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n-      multiDimElemsPerThread[k] =\n-          ceil<unsigned>(shape[k], shapePerCTA[k]) * sizePerThread[k];\n-      elemsPerThread *= multiDimElemsPerThread[k];\n       // 1 block in minimum if shape[k] is less than shapePerCTA[k]\n       for (unsigned blockOffset = 0;\n            blockOffset < ceil<unsigned>(shape[k], shapePerCTA[k]);\n@@ -551,34 +536,29 @@ class ConvertTritonGPUOpToLLVMPattern\n                                       threadsPerWarp[k] +\n                                   threadOffset * sizePerThread[k] + elemOffset);\n     }\n-    // step 3, add offset to base, and reorder the sequence of indices,\n-    //         to guarantee that elems in a same sizePerThread are adjacent in\n-    //         order\n+    // step 3, add offset to base, and reorder the sequence of indices to\n+    // guarantee that elems in a same sizePerThread are adjacent in order\n     SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread);\n-    unsigned accumSizePerThread =\n-        std::accumulate(sizePerThread.begin(), sizePerThread.end(), 1,\n-                        std::multiplies<unsigned>());\n+    unsigned accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> threadsPerDim(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n       threadsPerDim[k] = ceil<unsigned>(shape[k], sizePerThread[k]);\n     }\n     for (unsigned n = 0; n < elemsPerThread; ++n) {\n       unsigned linearNanoTileId = n / accumSizePerThread;\n-      unsigned linearElemsInNanoTileId = n % accumSizePerThread;\n+      unsigned linearNanoTileElemId = n % accumSizePerThread;\n       SmallVector<unsigned> multiDimNanoTileId =\n           getMultiDimIndex<unsigned>(linearNanoTileId, threadsPerDim);\n       SmallVector<unsigned> multiElemsInNanoTileId =\n-          getMultiDimIndex<unsigned>(linearElemsInNanoTileId, sizePerThread);\n+          getMultiDimIndex<unsigned>(linearNanoTileElemId, sizePerThread);\n       multiDimIdx[n].resize(rank);\n       for (unsigned k = 0; k < rank; ++k) {\n         unsigned reorderedMultiDimId =\n             multiDimNanoTileId[k] *\n                 (sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k]) +\n             multiElemsInNanoTileId[k];\n         multiDimIdx[n][k] =\n-            add(multiDimBase[k],\n-                createIndexAttrConstant(rewriter, loc, llvmIndexTy,\n-                                        offset[k][reorderedMultiDimId]));\n+            add(multiDimBase[k], idx_val(offset[k][reorderedMultiDimId]));\n       }\n     }\n \n@@ -594,7 +574,7 @@ class ConvertTritonGPUOpToLLVMPattern\n     assert(bufferId != Allocation::InvalidBufferId && \"BufferId not found\");\n     size_t offset = allocation->getOffset(bufferId);\n     auto llvmIndexTy = this->getTypeConverter()->getIndexType();\n-    Value offVal = createIndexAttrConstant(rewriter, loc, llvmIndexTy, offset);\n+    Value offVal = idx_val(offset);\n     Value base = gep(ptrTy, smem, offVal);\n     return base;\n   }\n@@ -867,10 +847,7 @@ struct StoreOpConversion\n       // TODO(Superjomn) Need to check masks before vectorize the load for all\n       // the values share one predicate? Here assume all the mask values are\n       // the same.\n-      Value maskVal =\n-          llMask ? maskElems[vecStart]\n-                 : createLLVMIntegerConstant(rewriter, loc, getTypeConverter(),\n-                                             rewriter.getIntegerType(1), 1);\n+      Value maskVal = llMask ? maskElems[vecStart] : int_val(1, 1);\n       ptxStoreInstr.global().b(width).v(nWords);\n \n       auto *asmAddr =\n@@ -1146,10 +1123,7 @@ struct LoadOpConversion\n       // TODO(Superjomn) Need to check masks before vectorize the load for all\n       // the values share one predicate? Here assume all the mask values are\n       // the same.\n-      Value pred =\n-          mask ? maskElems[vecStart]\n-               : createLLVMIntegerConstant(rewriter, loc, getTypeConverter(),\n-                                           rewriter.getIntegerType(1), 1);\n+      Value pred = mask ? maskElems[vecStart] : int_val(1, 1);\n \n       const std::string readConstraint =\n           (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n@@ -1249,7 +1223,7 @@ struct LoadOpConversion\n             curr);\n         rets.push_back(curr);\n       }\n-      int tmp = (width / valueElemNbits);\n+      int tmp = width / valueElemNbits;\n       for (size_t ii = 0; ii < vec; ii++) {\n         Value vecIdx = createIndexAttrConstant(\n             rewriter, loc, this->getTypeConverter()->getIndexType(), ii % tmp);\n@@ -1424,11 +1398,11 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> outNumCTAsEachRep(rank);\n     SmallVector<unsigned> inNumCTAs(rank);\n     SmallVector<unsigned> outNumCTAs(rank);\n+    auto srcShapePerCTA = getShapePerCTA(srcLayout.cast<BlockedEncodingAttr>());\n+    auto dstShapePerCTA = getShapePerCTA(dstLayout.cast<BlockedEncodingAttr>());\n     for (unsigned d = 0; d < rank; ++d) {\n-      unsigned inPerCTA =\n-          std::min(unsigned(shape[d]), getShapePerCTA(srcLayout, d));\n-      unsigned outPerCTA =\n-          std::min(unsigned(shape[d]), getShapePerCTA(dstLayout, d));\n+      unsigned inPerCTA = std::min(unsigned(shape[d]), srcShapePerCTA[d]);\n+      unsigned outPerCTA = std::min(unsigned(shape[d]), dstShapePerCTA[d]);\n       unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n       numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n       inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n@@ -1480,17 +1454,6 @@ struct ConvertLayoutOpConversion\n   }\n \n private:\n-  template <typename T>\n-  SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) const {\n-    size_t rank = order.size();\n-    assert(input.size() == rank);\n-    SmallVector<T> result(rank);\n-    for (auto it : llvm::enumerate(order)) {\n-      result[rank - 1 - it.value()] = input[it.index()];\n-    }\n-    return result;\n-  };\n-\n   void processReplicaBlocked(Location loc, ConversionPatternRewriter &rewriter,\n                              bool stNotRd, RankedTensorType type,\n                              ArrayRef<unsigned> numCTAsEachRep,"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 17, "deletions": 16, "changes": 33, "file_content_changes": "@@ -58,16 +58,17 @@ unsigned getElemsPerThread(Attribute layout, ArrayRef<int64_t> shape) {\n   }\n }\n \n-unsigned getShapePerCTA(const Attribute &layout, unsigned d) {\n-  if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n-    return blockedLayout.getSizePerThread()[d] *\n-           blockedLayout.getThreadsPerWarp()[d] *\n-           blockedLayout.getWarpsPerCTA()[d];\n-  } else {\n-    assert(0 && \"Unimplemented usage of getShapePerCTA\");\n-    return 0;\n+SmallVector<unsigned> getShapePerCTA(const BlockedEncodingAttr &layout) {\n+  auto sizePerThread = layout.getSizePerThread();\n+  auto threadsPerWarp = layout.getThreadsPerWarp();\n+  auto warpsPerCTA = layout.getWarpsPerCTA();\n+  unsigned rank = sizePerThread.size();\n+  SmallVector<unsigned> shapePerCTA(rank);\n+  for (unsigned k = 0; k < rank; ++k) {\n+    shapePerCTA[k] = sizePerThread[k] * threadsPerWarp[k] * warpsPerCTA[k];\n   }\n-};\n+  return shapePerCTA;\n+}\n \n } // namespace gpu\n } // namespace triton\n@@ -143,14 +144,14 @@ unsigned BlockedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {\n   size_t rank = shape.size();\n   assert(rank == getSizePerThread().size() &&\n          \"unexpected rank in BlockedEncodingAttr::getElemsPerThread\");\n-  SmallVector<unsigned> elemsPerThreadPerDim(rank);\n-  for (size_t i = 0; i < rank; ++i) {\n-    unsigned t =\n-        getSizePerThread()[i] * getThreadsPerWarp()[i] * getWarpsPerCTA()[i];\n-    elemsPerThreadPerDim[i] =\n-        ceil<unsigned>(shape[i], t) * getSizePerThread()[i];\n+  SmallVector<unsigned> elemsPerThread(rank);\n+  for (size_t d = 0; d < rank; ++d) {\n+    unsigned elemsPerCTAThreads =\n+        getSizePerThread()[d] * getThreadsPerWarp()[d] * getWarpsPerCTA()[d];\n+    unsigned rep = ceil<unsigned>(shape[d], elemsPerCTAThreads);\n+    elemsPerThread[d] = rep * getSizePerThread()[d];\n   }\n-  return product<unsigned>(elemsPerThreadPerDim);\n+  return product<unsigned>(elemsPerThread);\n }\n \n unsigned SliceEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape) const {"}, {"filename": "unittest/Analysis/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,5 +1,5 @@\n add_triton_ut(\n-  NAME TritonAnalysisTests\n+  NAME TestTritonAnalysis\n   SRCS UtilityTest.cpp\n   LIBS TritonAnalysis\n )"}, {"filename": "unittest/Analysis/UtilityTest.cpp", "status": "modified", "additions": 17, "deletions": 2, "changes": 19, "file_content_changes": "@@ -4,11 +4,26 @@\n //===----------------------------------------------------------------------===//\n \n #include \"triton/Analysis/Utility.h\"\n-#include <gmock/gmock.h>\n #include <gtest/gtest.h>\n \n namespace mlir {\n \n-TEST(UtilityTest, DummyTest) { EXPECT_EQ(true, true); }\n+TEST(Analysis, reorder) {\n+  SmallVector<int> shape({10, 20, 30});\n+  {\n+    SmallVector<unsigned> order({2, 1, 0});\n+    auto reordered = reorder<int>(shape, order);\n+    EXPECT_EQ(reordered[0], 30);\n+    EXPECT_EQ(reordered[1], 20);\n+    EXPECT_EQ(reordered[2], 10);\n+  }\n+  {\n+    SmallVector<unsigned> order({1, 0, 2});\n+    auto reordered = reorder<int>(shape, order);\n+    EXPECT_EQ(reordered[0], 20);\n+    EXPECT_EQ(reordered[1], 10);\n+    EXPECT_EQ(reordered[2], 30);\n+  }\n+}\n \n } // namespace mlir"}, {"filename": "unittest/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,5 +1,5 @@\n add_triton_ut(\n-\tNAME PtxAsmFormatTest\n+\tNAME TestPtxAsmFormat\n \tSRCS PtxAsmFormatTest.cpp\n \tLIBS TritonGPUToLLVM\n )"}]