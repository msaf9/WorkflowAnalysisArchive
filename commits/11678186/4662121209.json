[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -34,6 +34,8 @@ class ReduceOpHelper {\n \n   unsigned getScratchSizeInBytes();\n \n+  bool isSupportedLayout();\n+\n private:\n   triton::ReduceOp op;\n   RankedTensorType srcTy{};"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "file_content_changes": "@@ -91,6 +91,19 @@ unsigned ReduceOpHelper::getScratchSizeInBytes() {\n   return bytes;\n }\n \n+bool ReduceOpHelper::isSupportedLayout() {\n+  auto srcLayout = srcTy.getEncoding();\n+  if (srcLayout.isa<triton::gpu::BlockedEncodingAttr>()) {\n+    return true;\n+  }\n+  if (auto mmaLayout = srcLayout.dyn_cast<triton::gpu::MmaEncodingAttr>()) {\n+    if (mmaLayout.isAmpere()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n bool isSharedEncoding(Value value) {\n   auto type = value.getType();\n   if (auto tensorType = type.dyn_cast<RankedTensorType>()) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 39, "deletions": 17, "changes": 56, "file_content_changes": "@@ -131,18 +131,47 @@ struct ReduceOpConversion\n     }\n   }\n \n+  void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n+                          Attribute layout, SmallVector<Value> &index,\n+                          SmallVector<Value> &writeIdx,\n+                          std::map<int, Value> &ints, unsigned axis) const {\n+    writeIdx = index;\n+    auto sizePerThread = triton::gpu::getSizePerThread(layout);\n+    Value axisSizePerThread = ints[sizePerThread[axis]];\n+    Value _8 = ints[8];\n+    Value _16 = ints[16];\n+    if (layout.isa<BlockedEncodingAttr>()) {\n+      writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+    }\n+    auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>();\n+    if (mmaLayout && mmaLayout.isAmpere()) {\n+      if (axis == 0) {\n+        // Because warpTileSize = [16, 8] and threadsPerWarp = [8, 4], each 8\n+        // rows in smem would correspond to a warp. The mapping\n+        // is: (warp_index) x 8 + (row index within warp)\n+        writeIdx[axis] =\n+            add(mul(udiv(index[axis], _16), _8), urem(index[axis], _8));\n+      } else {\n+        // A single thread owns axisSizePerThread contiguous values\n+        // on the reduction axis, so after within thread reduction,\n+        // writeIdx[axis] = index[axis] / axisSizePerThread\n+        writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+      }\n+    }\n+  }\n+\n   // Use shared memory for reduction within warps and across warps\n   LogicalResult\n   matchAndRewriteBasic(triton::ReduceOp op, OpAdaptor adaptor,\n                        ConversionPatternRewriter &rewriter) const {\n+    ReduceOpHelper helper(op);\n     Location loc = op->getLoc();\n     unsigned axis = op.getAxis();\n     bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n     auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n-    if (!(srcLayout.isa<BlockedEncodingAttr>() ||\n-          srcLayout.isa<MmaEncodingAttr>())) {\n+    if (!helper.isSupportedLayout()) {\n       assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n     }\n     auto srcOrd = triton::gpu::getOrder(srcLayout);\n@@ -156,7 +185,6 @@ struct ReduceOpConversion\n     Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     smemBase = bitcast(smemBase, elemPtrTy);\n \n-    ReduceOpHelper helper(op);\n     auto smemShape = helper.getScratchConfigBasic();\n     unsigned elems = product<unsigned>(smemShape);\n     Value indexSmemBase = gep(elemPtrTy, smemBase, i32_val(elems));\n@@ -196,9 +224,9 @@ struct ReduceOpConversion\n     ints[0] = i32_val(0);\n     for (int N = smemShape[axis] / 2; N > 0; N >>= 1)\n       ints[N] = i32_val(N);\n-    Value axisSizePerThread = i32_val(sizePerThread[axis]);\n-    Value _8 = i32_val(8);\n-    Value _16 = i32_val(16);\n+    ints[sizePerThread[axis]] = i32_val(sizePerThread[axis]);\n+    ints[8] = i32_val(8);\n+    ints[16] = i32_val(16);\n \n     // reduce across threads\n     for (auto it : accs) {\n@@ -207,14 +235,9 @@ struct ReduceOpConversion\n       Value accIndex;\n       if (withIndex)\n         accIndex = accIndices[key];\n-      SmallVector<Value> writeIdx = indices[key];\n-\n-      if (srcLayout.isa<BlockedEncodingAttr>()) {\n-        writeIdx[axis] = udiv(writeIdx[axis], axisSizePerThread);\n-      } else {\n-        writeIdx[axis] =\n-            add(mul(udiv(writeIdx[axis], _16), _8), urem(writeIdx[axis], _8));\n-      }\n+      SmallVector<Value> writeIdx;\n+      getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n+                         axis);\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n       Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n       Value indexWritePtr = gep(indexPtrTy, indexSmemBase, writeOffset);\n@@ -286,14 +309,14 @@ struct ReduceOpConversion\n   // exchange across warps\n   LogicalResult matchAndRewriteFast(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n+    ReduceOpHelper helper(op);\n     Location loc = op->getLoc();\n     unsigned axis = adaptor.getAxis();\n     bool withIndex = triton::ReduceOp::withIndex(op.getRedOp());\n \n     auto srcTy = op.getOperand().getType().cast<RankedTensorType>();\n     auto srcLayout = srcTy.getEncoding();\n-    if (!(srcLayout.isa<BlockedEncodingAttr>() ||\n-          srcLayout.isa<MmaEncodingAttr>())) {\n+    if (!helper.isSupportedLayout()) {\n       assert(false && \"Unexpected srcLayout in ReduceOpConversion\");\n     }\n     auto srcShape = srcTy.getShape();\n@@ -309,7 +332,6 @@ struct ReduceOpConversion\n     Value smemBase = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     smemBase = bitcast(smemBase, elemPtrTy);\n \n-    ReduceOpHelper helper(op);\n     auto smemShapes = helper.getScratchConfigsFast();\n     unsigned elems = product<unsigned>(smemShapes[0]);\n     unsigned maxElems = std::max(elems, product<unsigned>(smemShapes[1]));"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 112, "deletions": 89, "changes": 201, "file_content_changes": "@@ -110,6 +110,26 @@ def check_type_supported(dtype):\n         pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n+class MmaLayout:\n+    def __init__(self, version, warps_per_cta):\n+        self.version = version\n+        self.warps_per_cta = str(warps_per_cta)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}}}>\"\n+\n+\n+class BlockedLayout:\n+    def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n+        self.sz_per_thread = str(size_per_thread)\n+        self.threads_per_warp = str(threads_per_warp)\n+        self.warps_per_cta = str(warps_per_cta)\n+        self.order = str(order)\n+\n+    def __str__(self):\n+        return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n+\n+\n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n@@ -1240,81 +1260,105 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n         else:\n             np.testing.assert_equal(z_ref, z_tri)\n \n+layouts = [\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    MmaLayout(version=(2, 0), warps_per_cta=[4,1])\n+]\n+@pytest.mark.parametrize(\"M, N\", [[128, 16]])\n+@pytest.mark.parametrize(\"src_layout\", layouts)\n+@pytest.mark.parametrize(\"axis\", [0, 1])\n+def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n+    ir_axis0 = f\"\"\"\n+    #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}}>\n+    #blocked1 = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}}>\n+    #src = {src_layout}\n+    \"\"\" + \"\"\"\n+    module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+    func.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+        %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+        %1 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<128x1xi32, #blocked>\n+        %2 = tt.splat %arg1 : (i32) -> tensor<128x1xi32, #blocked>\n+        %3 = arith.muli %1, %2 : tensor<128x1xi32, #blocked>\n+        %4 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked>\n+        %5 = tt.addptr %4, %3 : tensor<128x1x!tt.ptr<f32>, #blocked>, tensor<128x1xi32, #blocked>\n+        %6 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+        %7 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+        %8 = tt.expand_dims %6 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x16xi32, #blocked>\n+        %9 = tt.expand_dims %7 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x16xi32, #blocked1>\n+        %10 = tt.broadcast %5 : (tensor<128x1x!tt.ptr<f32>, #blocked>) -> tensor<128x16x!tt.ptr<f32>, #blocked>\n+        %11 = tt.broadcast %8 : (tensor<1x16xi32, #blocked>) -> tensor<128x16xi32, #blocked>\n+        %12 = tt.addptr %10, %11 : tensor<128x16x!tt.ptr<f32>, #blocked>, tensor<128x16xi32, #blocked>\n+        %13 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<1x16x!tt.ptr<f32>, #blocked1>\n+        %14 = tt.addptr %13, %9 : tensor<1x16x!tt.ptr<f32>, #blocked1>, tensor<1x16xi32, #blocked1>\n+        %15 = tt.load %12 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x16xf32, #blocked>\n+        %19 = triton_gpu.convert_layout %15 : (tensor<128x16xf32, #blocked>) -> tensor<128x16xf32, #src>\n+        %16 = tt.reduce %19 {axis = 0 : i32, redOp = 12 : i32} : tensor<128x16xf32, #src> -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #src}>>\n+        %17 = triton_gpu.convert_layout %16 : (tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #src}>>) -> tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+        %18 = tt.expand_dims %17 {axis = 0 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x16xf32, #blocked1>\n+        tt.store %14, %18 {cache = 1 : i32, evict = 1 : i32} : tensor<1x16xf32, #blocked1>\n+        return\n+    }\n+    }\n+    \"\"\"\n \n-@pytest.mark.parametrize(\"M, N, K, num_warps, allow_tf32, in_dtype, out_dtype, axis\",\n-                         [(*shape_nw, allow_tf32, in_dtype, out_dtype, axis)\n-                          for shape_nw in [[128, 16, 16, 4]]\n-                          for allow_tf32 in [True]\n-                          for in_dtype, out_dtype in [('float32', 'float32')]\n-                          for axis in [0, 1]])\n-def test_reduce_mma(M, N, K, num_warps, allow_tf32, in_dtype, out_dtype, axis, device='cuda'):\n-    capability = torch.cuda.get_device_capability()\n-    torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n+    ir_axis1 = f\"\"\"\n+    #blocked = #triton_gpu.blocked<{{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}}>\n+    #blocked1 = #triton_gpu.blocked<{{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}}>\n+    #src = {src_layout}\n+    \"\"\" + \"\"\"\n+    module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+    func.func public @kernel_0d1d2c3d4c(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n+        %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+        %1 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+        %2 = tt.expand_dims %0 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<128x1xi32, #blocked>\n+        %3 = tt.expand_dims %1 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<128x1xi32, #blocked1>\n+        %4 = tt.splat %arg1 : (i32) -> tensor<128x1xi32, #blocked1>\n+        %5 = arith.muli %3, %4 : tensor<128x1xi32, #blocked1>\n+        %6 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked1>\n+        %7 = tt.addptr %6, %5 : tensor<128x1x!tt.ptr<f32>, #blocked1>, tensor<128x1xi32, #blocked1>\n+        %8 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+        %9 = tt.expand_dims %8 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x16xi32, #blocked1>\n+        %10 = tt.broadcast %7 : (tensor<128x1x!tt.ptr<f32>, #blocked1>) -> tensor<128x16x!tt.ptr<f32>, #blocked1>\n+        %11 = tt.broadcast %9 : (tensor<1x16xi32, #blocked1>) -> tensor<128x16xi32, #blocked1>\n+        %12 = tt.addptr %10, %11 : tensor<128x16x!tt.ptr<f32>, #blocked1>, tensor<128x16xi32, #blocked1>\n+        %13 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked>\n+        %14 = tt.addptr %13, %2 : tensor<128x1x!tt.ptr<f32>, #blocked>, tensor<128x1xi32, #blocked>\n+        %15 = tt.load %12 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x16xf32, #blocked1>\n+        %19 = triton_gpu.convert_layout %15 : (tensor<128x16xf32, #blocked1>) -> tensor<128x16xf32, #src>\n+        %16 = tt.reduce %19 {axis = 1 : i32, redOp = 12 : i32} : tensor<128x16xf32, #src> -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #src}>>\n+        %17 = triton_gpu.convert_layout %16 : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #src}>>) -> tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+        %18 = tt.expand_dims %17 {axis = 1 : i32} : (tensor<128xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<128x1xf32, #blocked>\n+        tt.store %14, %18 {cache = 1 : i32, evict = 1 : i32} : tensor<128x1xf32, #blocked>\n+        return\n+    }\n+    }\n+    \"\"\"\n+\n+    ir = ir_axis0 if axis == 0 else ir_axis1\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n \n-    # triton kernel\n-    @triton.jit\n-    def kernel(X, stride_xm, stride_xk,\n-               Y, stride_yk, stride_yn,\n-               W, stride_wn, stride_wl,\n-               Z, stride_zm, stride_zn,\n-               out_dtype: tl.constexpr,\n-               BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n-               ALLOW_TF32: tl.constexpr,\n-               AXIS: tl.constexpr):\n-        off_m = tl.arange(0, BLOCK_M)\n-        off_n = tl.arange(0, BLOCK_N)\n-        off_l = tl.arange(0, BLOCK_N)\n-        off_k = tl.arange(0, BLOCK_K)\n-        Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n-        Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n-        Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n-        Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n-        x = tl.load(Xs)\n-        y = tl.load(Ys)\n-        z = tl.dot(x, y, allow_tf32=ALLOW_TF32, out_dtype=out_dtype)\n-        max = tl.max(z, AXIS)\n-        if AXIS == 1:\n-            z = z - max[:, None]\n-        else:\n-            z = z - max[None, :]\n-        min = tl.min(z, AXIS)\n-        if AXIS == 1:\n-            z = z - min[:, None]\n-        else:\n-            z = z - min[None, :]\n-        w = tl.load(Ws)\n-        z = tl.dot(z.to(w.dtype), w, out_dtype=out_dtype)\n-        tl.store(Zs, z)\n-    # input\n     rs = RandomState(17)\n-    x = rs.randint(0, 4, (M, K)).astype(in_dtype)\n-    y = rs.randint(0, 4, (K, N)).astype(in_dtype)\n-    w = np.ones((N, N)).astype(in_dtype)\n-    z = np.zeros((M, N)).astype(in_dtype)\n-    if in_dtype == 'float32' and allow_tf32:\n-        x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-        y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n-        w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+    x = rs.randint(0, 4, (M, N)).astype('float32')\n+    x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+\n+    if axis == 0:\n+        zeros = [[0 for i in range(N)]]\n+    else:\n+        zeros = [[0] for j in range(M)]\n+    z = np.array(zeros).astype('float32')\n+    \n     x_tri = torch.tensor(x, device=device)\n-    y_tri = torch.tensor(y, device=device)\n-    w_tri = torch.tensor(w, device=device)\n     z_tri = torch.tensor(z, device=device)\n-    out_dtype = tl.float32\n \n-    pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n-                         y_tri, y_tri.stride(0), y_tri.stride(1),\n-                         w_tri, w_tri.stride(0), w_tri.stride(1),\n-                         z_tri, z_tri.stride(0), z_tri.stride(1),\n-                         out_dtype,\n-                         BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n-                         AXIS=axis,\n-                         ALLOW_TF32=allow_tf32,\n-                         num_warps=num_warps)\n+    pgm = kernel[(1, 1, 4)](x_tri, x_tri.stride(0), z_tri)\n+\n+    z_ref = np.max(x, axis=axis, keepdims=True)\n \n-    z_ref = np.matmul(x, y)\n-    z_ref = z_ref - np.max(z_ref, axis=axis, keepdims=True)\n-    z_ref = z_ref - np.min(z_ref, axis=axis, keepdims=True)\n-    z_ref = np.matmul(z_ref, w)\n     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n@@ -2265,27 +2309,6 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n # -----------------------\n # TODO: backend should be tested separately\n \n-\n-class MmaLayout:\n-    def __init__(self, version, warps_per_cta):\n-        self.version = version\n-        self.warps_per_cta = str(warps_per_cta)\n-\n-    def __str__(self):\n-        return f\"#triton_gpu.mma<{{versionMajor={self.version[0]}, versionMinor={self.version[1]}, warpsPerCTA={self.warps_per_cta}}}>\"\n-\n-\n-class BlockedLayout:\n-    def __init__(self, size_per_thread, threads_per_warp, warps_per_cta, order):\n-        self.sz_per_thread = str(size_per_thread)\n-        self.threads_per_warp = str(threads_per_warp)\n-        self.warps_per_cta = str(warps_per_cta)\n-        self.order = str(order)\n-\n-    def __str__(self):\n-        return f\"#triton_gpu.blocked<{{sizePerThread={self.sz_per_thread}, threadsPerWarp={self.threads_per_warp}, warpsPerCTA={self.warps_per_cta}, order={self.order}}}>\"\n-\n-\n layouts = [\n     # MmaLayout(version=1, warps_per_cta=[1, 4]),\n     MmaLayout(version=(2, 0), warps_per_cta=[1, 4]),"}]