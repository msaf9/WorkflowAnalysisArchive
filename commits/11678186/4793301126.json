[{"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -87,6 +87,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n+    if (computeCapability < 70)\n+      return failure();\n     auto dotOp = cast<triton::DotOp>(op);\n     // TODO: Check data-types and SM compatibility\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();"}, {"filename": "python/setup.py", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "file_content_changes": "@@ -108,11 +108,20 @@ def get_thirdparty_packages(triton_cache_path):\n def download_and_copy_ptxas():\n     base_dir = os.path.dirname(__file__)\n     src_path = \"bin/ptxas\"\n-    url = \"https://conda.anaconda.org/nvidia/label/cuda-12.0.0/linux-64/cuda-nvcc-12.0.76-0.tar.bz2\"\n+    version = \"12.1.105\"\n+    url = f\"https://conda.anaconda.org/nvidia/label/cuda-12.1.1/linux-64/cuda-nvcc-{version}-0.tar.bz2\"\n     dst_prefix = os.path.join(base_dir, \"triton\")\n     dst_suffix = os.path.join(\"third_party\", \"cuda\", src_path)\n     dst_path = os.path.join(dst_prefix, dst_suffix)\n-    if not os.path.exists(dst_path):\n+    is_linux = platform.system() == \"Linux\"\n+    download = False\n+    if is_linux:\n+        download = True\n+        if os.path.exists(dst_path):\n+            curr_version = subprocess.check_output([dst_path, \"--version\"]).decode(\"utf-8\").strip()\n+            curr_version = re.search(r\"V([.|\\d]+)\", curr_version).group(1)\n+            download = curr_version != version\n+    if download:\n         print(f'downloading and extracting {url} ...')\n         ftpstream = urllib.request.urlopen(url)\n         file = tarfile.open(fileobj=ftpstream, mode=\"r|*\")"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -2124,6 +2124,10 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         x = np.abs(x)\n         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{expr}(x, x)'})\n         y_ref = np.power(x, x)\n+    elif expr == 'math.pow_dtype':\n+        x = np.abs(x)\n+        kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': 'tl.math.pow(x, 0.5)'})\n+        y_ref = np.power(x, 0.5)\n     elif expr == 'math.norm4d':\n         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{expr}(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))"}, {"filename": "python/test/unit/runtime/test_driver.py", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+import sys\n+\n+import triton\n+\n+\n+def test_is_lazy():\n+    from importlib import reload\n+    reload(sys.modules[\"triton.runtime.driver\"])\n+    reload(sys.modules[\"triton.runtime\"])\n+    mod = sys.modules[triton.runtime.driver.__module__]\n+    assert isinstance(triton.runtime.driver, getattr(mod, \"LazyProxy\"))\n+    assert triton.runtime.driver._obj is None\n+    utils = triton.runtime.driver.utils  # noqa: F841\n+    assert issubclass(triton.runtime.driver._obj.__class__, getattr(mod, \"DriverBase\"))"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -3,6 +3,9 @@\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n+import torch\n+\n+import triton\n from . import core as tl\n from triton._C.libtriton.triton import ir\n \n@@ -1180,6 +1183,14 @@ def dot(lhs: tl.tensor,\n         allow_tf32: bool,\n         out_dtype: tl.dtype,\n         builder: ir.builder) -> tl.tensor:\n+    if torch.version.hip is None:\n+        device = triton.runtime.jit.get_current_device()\n+        capability = triton.runtime.jit.get_device_capability(device)\n+        capability = capability[0] * 10 + capability[1]\n+        if capability < 70:\n+            assert (\n+                not rhs.dtype.is_fp16() and not rhs.dtype.is_fp8()\n+            ), \"Float8 and Float16 types are not supported for compute capability < 70 (use Float32 or above)\"\n     assert lhs.type.is_block() and rhs.type.is_block()\n     assert lhs.dtype == rhs.dtype, \"lhs and rhs must have the same dtype!\"\n     assert len(lhs.shape) == 2 and len(rhs.shape) == 2"}, {"filename": "python/triton/runtime/cache.py", "status": "modified", "additions": 13, "deletions": 8, "changes": 21, "file_content_changes": "@@ -1,11 +1,10 @@\n import json\n import os\n+import random\n from abc import ABC, abstractmethod\n from pathlib import Path\n from typing import Dict, Optional\n \n-from filelock import FileLock\n-\n \n def default_cache_dir():\n     return os.path.join(Path.home(), \".triton\", \"cache\")\n@@ -96,12 +95,18 @@ def put(self, data, filename, binary=True) -> str:\n             data = str(data)\n         assert self.lock_path is not None\n         filepath = self._make_path(filename)\n-        with FileLock(self.lock_path):\n-            # use tempfile to be robust against program interruptions\n-            mode = \"wb\" if binary else \"w\"\n-            with open(filepath + \".tmp\", mode) as f:\n-                f.write(data)\n-            os.rename(filepath + \".tmp\", filepath)\n+        # Random ID to avoid any collisions\n+        rnd_id = random.randint(0, 1000000)\n+        # we use the PID incase a bunch of these around so we can see what PID made it\n+        pid = os.getpid()\n+        # use tempfile to be robust against program interruptions\n+        temp_path = f\"{filepath}.tmp.pid_{pid}_{rnd_id}\"\n+        mode = \"wb\" if binary else \"w\"\n+        with open(temp_path, mode) as f:\n+            f.write(data)\n+        # Replace is guaranteed to be atomic on POSIX systems if it succeeds\n+        # so filepath cannot see a partial write\n+        os.replace(temp_path, filepath)\n         return filepath\n \n "}, {"filename": "python/triton/runtime/driver.py", "status": "modified", "additions": 36, "deletions": 2, "changes": 38, "file_content_changes": "@@ -140,7 +140,41 @@ def __init__(self):\n # -----------------------------\n \n \n-def create_driver():\n+class LazyProxy:\n+    def __init__(self, init_fn):\n+        self._init_fn = init_fn\n+        self._obj = None\n+\n+    def _initialize_obj(self):\n+        if self._obj is None:\n+            self._obj = self._init_fn()\n+\n+    def __getattr__(self, name):\n+        self._initialize_obj()\n+        return getattr(self._obj, name)\n+\n+    def __setattr__(self, name, value):\n+        if name in ['_init_fn', '_obj']:\n+            super().__setattr__(name, value)\n+        else:\n+            self._initialize_obj()\n+            setattr(self._obj, name, value)\n+\n+    def __delattr__(self, name):\n+        self._initialize_obj()\n+        delattr(self._obj, name)\n+\n+    def __repr__(self):\n+        if self._obj is None:\n+            return f\"<{self.__class__.__name__} for {self._init_fn} not yet initialized>\"\n+        return repr(self._obj)\n+\n+    def __str__(self):\n+        self._initialize_obj()\n+        return str(self._obj)\n+\n+\n+def initialize_driver():\n     import torch\n     if torch.version.hip is not None:\n         return HIPDriver()\n@@ -150,4 +184,4 @@ def create_driver():\n         return UnsupportedDriver()\n \n \n-driver = create_driver()\n+driver = LazyProxy(initialize_driver)"}, {"filename": "python/triton/tools/build_extern.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -296,7 +296,7 @@ def _output_stubs(self) -> str:\n         header_str = \"\"\n         header_str += \"@functools.lru_cache()\\n\"\n         header_str += \"def libdevice_path():\\n\"\n-        header_str += \"    return os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", driver().libdevice_path)\\n\\n\"\n+        header_str += \"    return os.getenv(\\\"TRITON_LIBDEVICE_PATH\\\", driver.libdevice_path)\\n\\n\"\n \n         func_str = \"\"\n         for symbols in self._symbol_groups.values():"}]