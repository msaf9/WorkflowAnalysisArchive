[{"filename": "python/test/backend/test_device_backend.py", "status": "modified", "additions": 33, "deletions": 27, "changes": 60, "file_content_changes": "@@ -1,12 +1,11 @@\n import functools\n-import gc\n import hashlib\n+import importlib\n import os\n import shutil\n import subprocess\n import sysconfig\n import tempfile\n-import tracemalloc\n from pathlib import Path\n \n import setuptools\n@@ -121,6 +120,8 @@ def __init__(self):\n \n \n class ExtensionBackend(BaseBackend):\n+    stub_so_path = \"\"\n+\n     def __init__(self, device_type: str) -> None:\n         super(ExtensionBackend, self).__init__(device_type)\n         self.driver = ExtensionDriver()\n@@ -177,8 +178,11 @@ def make_launcher_stub(self, name, signature, constants):\n                     f.write(src)\n                 so = build_for_backend(name, src_path, tmpdir)\n                 with open(so, \"rb\") as f:\n-                    return so_cache_manager.put(f.read(), so_name, binary=True)\n+                    so_path = so_cache_manager.put(f.read(), so_name, binary=True)\n+                    type(self).stub_so_path = so_path\n+                    return so_path\n         else:\n+            type(self).stub_so_path = cache_path\n             return cache_path\n \n     def _generate_launcher(self, constants, signature):\n@@ -188,20 +192,25 @@ def _generate_launcher(self, constants, signature):\n         #include <Python.h>\n         #include <stdio.h>\n \n-        static PyObject* launch(PyObject* self, PyObject* args) {\n-        printf(\"Launch empty kernel for extension backend\\\\n\");\n+        static PyObject* launch_counter(PyObject* self, PyObject* args) {\n+        static int64_t launch_counter = 0;\n+        launch_counter += 1;\n+        return PyLong_FromLong(launch_counter);\n+        }\n \n+        static PyObject* launch(PyObject* self, PyObject* args) {\n         if (PyErr_Occurred()) {\n             return NULL;\n         }\n-\n+        launch_counter(self, args);\n         // return None\n         Py_INCREF(Py_None);\n         return Py_None;\n         }\n \n         static PyMethodDef ModuleMethods[] = {\n         {\"launch\", launch, METH_VARARGS, \"Entry point for all kernels with this signature\"},\n+        {\"launch_counter\", launch_counter, METH_VARARGS, \"Entry point to get launch counter\"},\n         {NULL, NULL, 0, NULL} // sentinel\n         };\n \n@@ -226,31 +235,28 @@ def _generate_launcher(self, constants, signature):\n         return src\n \n \n-register_backend(\"cpu\", ExtensionBackend)\n+def test_thirdparty_backend():\n+    register_backend(\"cpu\", ExtensionBackend)\n \n+    @triton.jit\n+    def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n+        xnumel = 10\n+        xoffset = tl.program_id(0) * XBLOCK\n+        xindex = xoffset + tl.arange(0, XBLOCK)[:]\n+        xmask = xindex < xnumel\n+        x0 = xindex\n+        tmp0 = tl.load(in_ptr0 + (x0), xmask)\n+        tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp0, xmask)\n \n-@triton.jit\n-def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n-    xnumel = 10\n-    xoffset = tl.program_id(0) * XBLOCK\n-    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n-    xmask = xindex < xnumel\n-    x0 = xindex\n-    tmp0 = tl.load(in_ptr0 + (x0), xmask)\n-    tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp0, xmask)\n-\n-\n-tracemalloc.start()\n-try:\n     inp = torch.randn(10)\n     out = torch.randn(10)\n     kernel[(10,)](inp, out, 10, XBLOCK=16)\n-    gc.collect()\n-    begin, _ = tracemalloc.get_traced_memory()\n+    spec = importlib.util.spec_from_file_location(\"__triton_launcher\", ExtensionBackend.stub_so_path)\n+    mod = importlib.util.module_from_spec(spec)\n+    spec.loader.exec_module(mod)\n+    launch_counter = getattr(mod, \"launch_counter\")\n+\n     for _ in range(100):\n         kernel[(10,)](inp, out, 10, XBLOCK=16)\n-    gc.collect()\n-    end, _ = tracemalloc.get_traced_memory()\n-    assert end - begin < 1000\n-finally:\n-    tracemalloc.stop()\n+\n+    assert launch_counter() > 0"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 46, "deletions": 12, "changes": 58, "file_content_changes": "@@ -8,7 +8,8 @@\n import subprocess\n import textwrap\n from collections import defaultdict, namedtuple\n-from typing import Callable, Generic, Iterable, Optional, TypeVar, Union, cast, overload\n+from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n+                    overload)\n \n import triton\n from triton.common.backend import get_backend\n@@ -162,9 +163,18 @@ def _key_of(arg):\n     @staticmethod\n     def _device_of(arg):\n         if hasattr(arg, \"device\"):\n-            return arg.device.type\n-        else:\n-            return ''\n+            if hasattr(arg.device, 'type'):\n+                return arg.device.type\n+\n+        return ''\n+\n+    @staticmethod\n+    def _pinned_memory_of(arg):\n+        if hasattr(arg, \"is_pinned\"):\n+            if isinstance(arg.is_pinned, Callable):\n+                return arg.is_pinned()\n+\n+        return False\n \n     @staticmethod\n     def _spec_of(arg):\n@@ -269,13 +279,28 @@ def _get_arg_sig_key(self, arg) -> str:\n         else:\n             return f'_key_of({arg})'\n \n+    def _conclude_device_type(self, device_types: List[str], pinned_memory_flags: List[bool]) -> str:\n+        device_types = [device_type for device_type in device_types if device_type != '']\n+        # Return cuda if one of the input tensors is cuda\n+        if 'cuda' in device_types:\n+            return 'cuda'\n+\n+        is_cpu = all(device_type == 'cpu' for device_type in device_types)\n+        is_pinned_memory = any(pinned_memory_flag for pinned_memory_flag in pinned_memory_flags)\n+        # Return cuda if all the input tensors are cpu while the memory is pinned\n+        if is_cpu and is_pinned_memory:\n+            return 'cuda'\n+\n+        return device_types[0] if len(device_types) > 0 else 'cuda'\n+\n     def _make_launcher(self):\n         regular_args = [f'{arg}' for i, arg in enumerate(self.arg_names) if i not in self.constexprs]\n         constexpr_args = [f'{arg}' for i, arg in enumerate(self.arg_names) if i in self.constexprs]\n         args = ', '.join(regular_args)\n         # cache key for regular argument type\n         sig_keys = ', '.join([self._get_arg_sig_key(arg) for arg in regular_args])\n         device_types = '[' + ', '.join([f'_device_of({arg})' for arg in regular_args]) + ']'\n+        pinned_memory_flags = '[' + ', '.join([f'_pinned_memory_of({arg})' for arg in regular_args]) + ']'\n         # cache key for constexpr argument values\n         constexpr_keys = ', '.join(constexpr_args)\n         # cache key for argument specialization\n@@ -303,23 +328,26 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     grid_0 = grid[0]\n     grid_1 = grid[1] if grid_size > 1 else 1\n     grid_2 = grid[2] if grid_size > 2 else 1\n+\n     device_types = [device_type for device_type in {device_types} if device_type != '']\n-    device_type = device_types[0] if len(device_types) > 0 else 'cuda'\n+    device_type = self._conclude_device_type(device_types, {pinned_memory_flags})\n+    device_backend = None\n+    if device_type not in ['cuda', 'hip']:\n+        device_backend = get_backend(device_type)\n+        if device_backend is None:\n+            raise ValueError('Cannot find backend for ' + device_type)\n+\n     if device is None:\n         if device_type in ['cuda', 'hip']:\n             device = get_current_device()\n             set_current_device(device)\n         else:\n-            device_backend = get_backend(device_type)\n-            assert device_backend\n             device = device_backend.get_current_device()\n             device_backend.set_current_device(device)\n     if stream is None and not warmup:\n         if device_type in ['cuda', 'hip']:\n             stream = get_cuda_stream(device)\n         else:\n-            device_backend = get_backend(device_type)\n-            assert device_backend\n             stream = device_backend.get_stream()\n     try:\n       bin = cache[device][key]\n@@ -349,9 +377,15 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n         return bin\n       return None\n \"\"\"\n-        scope = {\"version_key\": version_key(), \"get_cuda_stream\": get_cuda_stream,\n-                 \"self\": self, \"_spec_of\": self._spec_of, \"_key_of\": self._key_of,\n-                 \"_device_of\": self._device_of, \"cache\": self.cache, \"triton\": triton,\n+        scope = {\"version_key\": version_key(),\n+                 \"get_cuda_stream\": get_cuda_stream,\n+                 \"self\": self,\n+                 \"_spec_of\": self._spec_of,\n+                 \"_key_of\": self._key_of,\n+                 \"_device_of\": self._device_of,\n+                 \"_pinned_memory_of\": self._pinned_memory_of,\n+                 \"cache\": self.cache,\n+                 \"triton\": triton,\n                  \"get_backend\": get_backend,\n                  \"get_current_device\": get_current_device,\n                  \"set_current_device\": set_current_device}"}]