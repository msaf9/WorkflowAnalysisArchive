[{"filename": "python/examples/empty.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -1,6 +1,7 @@\n+import torch\n+\n import triton\n import triton.language as tl\n-import torch\n \n \n @triton.jit\n@@ -9,4 +10,4 @@ def kernel(X, stride_xm, stride_xn, BLOCK: tl.constexpr):\n \n \n X = torch.randn(1, device=\"cuda\")\n-pgm = kernel[(1,)](X, 1, 1, BLOCK=1024)\n\\ No newline at end of file\n+pgm = kernel[(1,)](X, 1, 1, BLOCK=1024)"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 24, "deletions": 12, "changes": 36, "file_content_changes": "@@ -544,7 +544,6 @@ void init_triton_runtime(py::module &&m) {\n /*****************************************************************************/\n typedef std::map<std::string, py::object> asm_map_t;\n \n-\n /*****************************************************************************/\n /* Python bindings for triton::ir                                            */\n /*****************************************************************************/\n@@ -1683,8 +1682,11 @@ void init_triton_translation(py::module &m) {\n           return bytes;\n         });\n \n-  m.def(\"load_binary\", [](const std::string& name, const std::string& data, size_t n_shared_bytes, uint64_t device){\n-\t      py::gil_scoped_release allow_threads;\n+  m.def(\n+      \"load_binary\",\n+      [](const std::string &name, const std::string &data,\n+         size_t n_shared_bytes, uint64_t device) {\n+        py::gil_scoped_release allow_threads;\n         // create driver handles\n         CUfunction fun;\n         CUmodule mod;\n@@ -1693,21 +1695,31 @@ void init_triton_translation(py::module &m) {\n         // get allocated registers and spilled registers from the function\n         int n_regs = 0;\n         int n_spills = 0;\n-        drv::dispatch::cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS, fun);\n-        drv::dispatch::cuFuncGetAttribute(&n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun);\n+        drv::dispatch::cuFuncGetAttribute(&n_regs, CU_FUNC_ATTRIBUTE_NUM_REGS,\n+                                          fun);\n+        drv::dispatch::cuFuncGetAttribute(\n+            &n_spills, CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES, fun);\n         n_spills /= 4;\n         // set dynamic shared memory if necessary\n         int shared_optin;\n-        drv::dispatch::cuDeviceGetAttribute(&shared_optin, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device);\n-        if(n_shared_bytes > 49152 && shared_optin > 49152){\n+        drv::dispatch::cuDeviceGetAttribute(\n+            &shared_optin,\n+            CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, device);\n+        if (n_shared_bytes > 49152 && shared_optin > 49152) {\n           drv::dispatch::cuFuncSetCacheConfig(fun, CU_FUNC_CACHE_PREFER_SHARED);\n           int shared_total, shared_static;\n-          drv::dispatch::cuDeviceGetAttribute(&shared_total, CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, device);\n-          drv::dispatch::cuFuncGetAttribute(&shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun);\n-          drv::dispatch::cuFuncSetAttribute(fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES, shared_optin - shared_static);\n+          drv::dispatch::cuDeviceGetAttribute(\n+              &shared_total,\n+              CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, device);\n+          drv::dispatch::cuFuncGetAttribute(\n+              &shared_static, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, fun);\n+          drv::dispatch::cuFuncSetAttribute(\n+              fun, CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,\n+              shared_optin - shared_static);\n         }\n-        return std::make_tuple((uint64_t)mod, (uint64_t)fun, (uint64_t)n_regs, (uint64_t)n_spills);\n-      }, \n+        return std::make_tuple((uint64_t)mod, (uint64_t)fun, (uint64_t)n_regs,\n+                               (uint64_t)n_spills);\n+      },\n       py::return_value_policy::take_ownership);\n }\n "}, {"filename": "python/tests/test_compiler.py", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -1,8 +1,6 @@\n import torch\n-\n import triton\n import triton.language as tl\n-import triton.runtime as runtime\n \n # trigger the torch.device implicitly to ensure cuda context initialization\n torch.zeros([10], device=torch.device('cuda'))\n@@ -17,9 +15,9 @@ def test_empty_kernel_cubin_compile():\n \n     device = torch.cuda.current_device()\n     kernel = triton.compile(empty_kernel,\n-                           \"*fp32,i32,i32\",\n-                           device=device,\n-                           constants={\"BLOCK\": 256})\n+                            \"*fp32,i32,i32\",\n+                            device=device,\n+                            constants={\"BLOCK\": 256})\n \n     assert len(kernel.asm[\"cubin\"]) > 0\n "}, {"filename": "python/tests/test_math_ops.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -24,10 +24,10 @@ def math_kernel(x1_ptr, x2_ptr, x3_ptr, x4_ptr, n, BLOCK_SIZE: tl.constexpr):\n \n def test_empty_kernel_cubin_compile():\n     kernel = triton.compiler._compile(math_kernel,\n-                                     \"*fp32,*fp32,*fp32,*fp32,i32\",\n-                                     device=0,\n-                                     constants={\"BLOCK_SIZE\": 256},\n-                                     output=\"ttgir\")  # \"cubin\"\n+                                      \"*fp32,*fp32,*fp32,*fp32,i32\",\n+                                      device=0,\n+                                      constants={\"BLOCK_SIZE\": 256},\n+                                      output=\"ttgir\")  # \"cubin\"\n     assert kernel\n     # TODO: Check if the values are correct.\n     # TODO: Cover all the math operators"}, {"filename": "python/tests/test_transpose.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,10 +1,8 @@\n import pytest\n import torch\n from torch.testing import assert_allclose\n-\n import triton\n import triton.language as tl\n-import triton.runtime as runtime\n \n \n @triton.jit"}, {"filename": "python/tests/test_vecadd_no_scf.py", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "file_content_changes": "@@ -3,7 +3,6 @@\n \n import triton\n import triton.language as tl\n-import triton.runtime as runtime\n \n \n def vecadd_no_scf_tester(num_warps, block_size):\n@@ -22,15 +21,13 @@ def kernel(x_ptr,\n         z_ptrs = z_ptr + offset\n         tl.store(z_ptrs, z)\n \n-    \n-\n     x = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n     y = torch.randn((block_size,), device='cuda', dtype=torch.float32)\n     z = torch.empty((block_size,), device=x.device, dtype=x.dtype)\n \n     grid = lambda EA: (x.shape.numel() // block_size,)\n     kernel[grid](x_ptr=x, y_ptr=y, z_ptr=z, BLOCK_SIZE_N=block_size, num_warps=num_warps)\n-    \n+\n     golden_z = x + y\n     assert_allclose(z, golden_z, rtol=1e-7, atol=1e-7)\n "}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -12,4 +12,4 @@\n from .compiler import compile, CompilationError\n from . import language\n from . import testing\n-from . import ops\n\\ No newline at end of file\n+from . import ops"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "file_content_changes": "@@ -1,7 +1,6 @@\n from __future__ import annotations\n \n import ast\n-from collections import namedtuple\n import contextlib\n import functools\n import hashlib\n@@ -14,8 +13,9 @@\n import sysconfig\n import tempfile\n import warnings\n+from collections import namedtuple\n from sysconfig import get_paths\n-from typing import Any, Dict, Set, Tuple, Union\n+from typing import Any, Dict, Tuple, Union\n \n import setuptools\n import torch\n@@ -732,6 +732,7 @@ def generic_visit(self, node):\n         typename = type(node).__name__\n         raise NotImplementedError(\"Unsupported node: {}\".format(typename))\n \n+\n class CompilationError(Exception):\n     def __init__(self, src, node):\n         self.message = f'at {node.lineno}:{node.col_offset}:\\n'\n@@ -776,6 +777,7 @@ def kernel_suffix(signature, specialization):\n # ------------------------------------------------------------------------------\n # ------------------------------------------------------------------------------\n \n+\n def make_triton_ir(fn, signature, specialization, constants):\n     context = _triton.ir.context()\n     context.load_triton()\n@@ -874,11 +876,13 @@ def ptx_get_kernel_name(ptx: str) -> str:\n         if line.startswith('// .globl'):\n             return line.split()[-1]\n \n+\n instance_descriptor = namedtuple(\"instance_descriptor\", [\"divisible_by_16\", \"equal_to_1\"], defaults=[set(), set()])\n \n+\n def _compile(fn, signature: str, device: int = -1, constants=dict(), specialization=instance_descriptor(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, output: str = \"ttgir\") -> Tuple[str, int, str]:\n     if isinstance(signature, str):\n-      signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n+        signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n     valid_outputs = (\"ttir\", \"ttgir\", \"ptx\", \"cubin\")\n     assert output in valid_outputs, \"output should be one of [%s], but get \\\"%s\\\"\" % (','.join(valid_outputs), output)\n \n@@ -887,7 +891,6 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     module = optimize_triton_ir(module)\n     if output == \"ttir\":\n         return module.str()\n-    \n \n     # tritongpu-ir\n     module = make_tritongpu_ir(module, num_warps)\n@@ -909,7 +912,6 @@ def _compile(fn, signature: str, device: int = -1, constants=dict(), specializat\n     assert False\n \n \n-\n # ------------------------------------------------------------------------------\n # compiler\n # ------------------------------------------------------------------------------\n@@ -1185,10 +1187,10 @@ def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_sta\n \n def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n     if isinstance(signature, str):\n-      signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n+        signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n     # we get the kernel, i.e. the first function generated in the module\n     if configs is None:\n-      configs = [instance_descriptor()]\n+        configs = [instance_descriptor()]\n     assert len(configs) == 1\n     # cache manager\n     name = fn.__name__\n@@ -1258,6 +1260,4 @@ def runner(*args, stream=None):\n             if stream is None:\n                 stream = torch.cuda.current_stream().cuda_stream\n             self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function, *args)\n-        return \n-\n-\n+        return"}, {"filename": "python/triton/runtime/__init__.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,2 +1,2 @@\n from .autotuner import Config, Heuristics, autotune, heuristics  # noqa: F401\n-from .jit import JITFunction, KernelInterface, version_key  # noqa: F401\n\\ No newline at end of file\n+from .jit import JITFunction, KernelInterface, version_key  # noqa: F401"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -212,4 +212,4 @@ def kernel(x_ptr, x_size, **META):\n     def decorator(fn):\n         return Heuristics(fn, fn.arg_names, values)\n \n-    return decorator\n\\ No newline at end of file\n+    return decorator"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -412,4 +412,4 @@ def reinterpret(tensor, dtype):\n         # A new wrapper is needed around an unwrapped tensor.\n         return TensorWrapper(tensor, dtype)\n     else:\n-        raise TypeError(f'Cannot reinterpret a {type(tensor)}.')\n\\ No newline at end of file\n+        raise TypeError(f'Cannot reinterpret a {type(tensor)}.')"}, {"filename": "python/triton/utils.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -63,4 +63,4 @@ def reinterpret(tensor, dtype):\n         # A new wrapper is needed around an unwrapped tensor.\n         return TensorWrapper(tensor, dtype)\n     else:\n-        raise TypeError(f'Cannot reinterpret a {type(tensor)}.')\n\\ No newline at end of file\n+        raise TypeError(f'Cannot reinterpret a {type(tensor)}.')"}]