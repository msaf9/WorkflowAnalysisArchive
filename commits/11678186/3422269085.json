[{"filename": "CMakeLists.txt", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -15,6 +15,10 @@ endif()\n option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n \n+# Ensure Python3 vars are set correctly\n+#  used conditionally in this file and by lit tests\n+find_package(Python3 REQUIRED COMPONENTS Development Interpreter)\n+\n # Default build type\n if(NOT CMAKE_BUILD_TYPE)\n   message(STATUS \"Default build type: Release\")\n@@ -133,24 +137,22 @@ endif()\n if(TRITON_BUILD_PYTHON_MODULE)\n     message(STATUS \"Adding Python module\")\n     set(PYTHON_SRC_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/src)\n+    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n     include_directories(\".\" ${PYTHON_SRC_PATH})\n     if (PYTHON_INCLUDE_DIRS)\n       include_directories(${PYTHON_INCLUDE_DIRS})\n     else()\n-      find_package(Python3 REQUIRED COMPONENTS Development)\n       include_directories(${Python3_INCLUDE_DIRS})\n       link_directories(${Python3_LIBRARY_DIRS})\n       link_libraries(${Python3_LIBRARIES})\n       add_link_options(${Python3_LINK_OPTIONS})\n     endif()\n-    set(PYTHON_SRC ${PYTHON_SRC_PATH}/main.cc ${PYTHON_SRC_PATH}/triton.cc)\n endif()\n \n \n # # Triton\n # file(GLOB_RECURSE LIBTRITON_SRC lib/*.cc)\n # if (WIN32 AND TRITON_BUILD_PYTHON_MODULE)\n-#     find_package(Python3 REQUIRED COMPONENTS Development)\n #     Python3_add_library(triton SHARED ${LIBTRITON_SRC} ${PYTHON_SRC})\n #     set_target_properties(triton PROPERTIES SUFFIX \".pyd\")\n #     set_target_properties(triton PROPERTIES PREFIX \"lib\")"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -21,11 +21,11 @@ template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n \n // output[i] = input[order[i]]\n-template <typename T>\n-SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n+template <typename T, typename RES_T = T>\n+SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   size_t rank = order.size();\n   assert(input.size() == rank);\n-  SmallVector<T> result(rank);\n+  SmallVector<RES_T> result(rank);\n   for (auto it : llvm::enumerate(order)) {\n     result[it.index()] = input[it.value()];\n   }"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "file_content_changes": "@@ -163,18 +163,19 @@ for\n                      \"ArrayRef<unsigned>\":$order,\n                      \"unsigned\":$numWarps), [{\n       int rank = sizePerThread.size();\n-      int remainingWarps = numWarps;\n-      int remainingLanes = 32;\n+      unsigned remainingLanes = 32;\n+      unsigned remainingThreads = numWarps*32;\n+      unsigned remainingWarps = numWarps;\n       SmallVector<unsigned, 4> threadsPerWarp(rank);\n       SmallVector<unsigned, 4> warpsPerCTA(rank);\n       for (int _dim = 0; _dim < rank; ++_dim) {\n-        int dim = order[_dim];\n-        int maxNumThreads = int(shape[dim]) / sizePerThread[dim];\n-        warpsPerCTA[dim] = std::clamp(remainingWarps, 1, maxNumThreads);\n-        maxNumThreads = maxNumThreads / warpsPerCTA[dim];\n-        threadsPerWarp[dim] = std::clamp(remainingLanes, 1, maxNumThreads);\n-        remainingWarps /= warpsPerCTA[dim];\n-        remainingLanes /= threadsPerWarp[dim];\n+        int i = order[_dim];\n+        unsigned threadsPerCTA = std::clamp<unsigned>(remainingThreads, 1, shape[i] / sizePerThread[i]);\n+        threadsPerWarp[i] = std::clamp<unsigned>(threadsPerCTA, 1, remainingLanes);\n+        warpsPerCTA[i] = std::clamp<unsigned>(threadsPerCTA / threadsPerWarp[i], 1, remainingWarps);\n+        remainingWarps /= warpsPerCTA[i];\n+        remainingLanes /= threadsPerWarp[i];\n+        remainingThreads /= threadsPerCTA;\n       }\n \n       return $_get(context, sizePerThread, threadsPerWarp, warpsPerCTA, order);"}, {"filename": "include/triton/Target/LLVMIR/LLVMIRTranslation.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -1,5 +1,6 @@\n #ifndef TRITON_TARGET_LLVMIRTRANSLATION_H\n #define TRITON_TARGET_LLVMIRTRANSLATION_H\n+#include \"llvm/ADT/StringRef.h\"\n #include <memory>\n #include <vector>\n \n@@ -29,6 +30,8 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n std::unique_ptr<llvm::Module>\n translateLLVMToLLVMIR(llvm::LLVMContext *llvmContext, mlir::ModuleOp module);\n \n+bool linkExternLib(llvm::Module &module, llvm::StringRef path);\n+\n } // namespace triton\n } // namespace mlir\n "}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -91,7 +91,7 @@ SmallVector<unsigned> getScratchConfigForReduce(triton::ReduceOp op) {\n   auto srcShape = srcTy.getShape();\n   auto axis = op.axis();\n \n-  bool fastReduce = axis == 1; // FIXME(Qingyi): The fastest-changing dimension\n+  bool fastReduce = axis == srcLayout.getOrder()[0];\n \n   SmallVector<unsigned> smemShape;\n   for (auto d : srcShape)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 277, "deletions": 241, "changes": 518, "file_content_changes": "@@ -83,6 +83,11 @@ static Value createLLVMIntegerConstant(OpBuilder &builder, Location loc,\n \n } // namespace\n \n+// A helper function for using printf in LLVM conversion.\n+void llPrintf(StringRef msg, ValueRange args,\n+              ConversionPatternRewriter &rewriter);\n+\n+// Shortcuts for some commonly used LLVM ops to keep code simple and intuitive//\n // Shortcuts for some commonly used LLVM ops to keep code simple and intuitive\n #define zext(...) rewriter.create<LLVM::ZExtOp>(loc, __VA_ARGS__)\n #define udiv(...) rewriter.create<LLVM::UDivOp>(loc, __VA_ARGS__)\n@@ -338,6 +343,7 @@ Value getStructFromElements(Location loc, ValueRange resultVals,\n   return llvmStruct;\n }\n \n+// Delinearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n template <typename T>\n static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   // shape: {a, b, c, d}  ->  accMul: {b*c*d, c*d, d, 1}\n@@ -355,6 +361,7 @@ static SmallVector<T> getMultiDimIndex(T linearIndex, ArrayRef<T> shape) {\n   return multiDimIndex;\n }\n \n+// Linearize on compile-time consts, assuming the order is [n, .. 2, 1, 0]\n template <typename T>\n static T getLinearIndex(ArrayRef<T> multiDimIndex, ArrayRef<T> shape) {\n   assert(multiDimIndex.size() == shape.size());\n@@ -510,12 +517,12 @@ class ConvertTritonGPUOpToLLVMPattern\n       multiDim[0] = linear;\n     } else {\n       Value remained = linear;\n-      for (auto &&en : llvm::enumerate(llvm::reverse(shape.drop_front()))) {\n+      for (auto &&en : llvm::enumerate(shape.drop_back())) {\n         Value dimSize = idx_val(en.value());\n-        multiDim[rank - 1 - en.index()] = urem(remained, dimSize);\n+        multiDim[en.index()] = urem(remained, dimSize);\n         remained = udiv(remained, dimSize);\n       }\n-      multiDim[0] = remained;\n+      multiDim[rank - 1] = remained;\n     }\n     return multiDim;\n   }\n@@ -525,9 +532,9 @@ class ConvertTritonGPUOpToLLVMPattern\n     int rank = multiDim.size();\n     Value linear = idx_val(0);\n     if (rank > 0) {\n-      linear = multiDim.front();\n+      linear = multiDim.back();\n       for (auto [dim, shape] :\n-           llvm::zip(multiDim.drop_front(), shape.drop_front())) {\n+           llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n         Value dimSize = idx_val(shape);\n         linear = add(mul(linear, dimSize), dim);\n       }\n@@ -566,6 +573,7 @@ class ConvertTritonGPUOpToLLVMPattern\n         delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n     SmallVector<Value> multiDimThreadId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+\n     SmallVector<Value> multiDimBase(rank);\n     for (unsigned k = 0; k < rank; ++k) {\n       // Wrap around multiDimWarpId/multiDimThreadId incase\n@@ -1362,7 +1370,9 @@ struct ReduceOpConversion\n LogicalResult\n ReduceOpConversion::matchAndRewrite(triton::ReduceOp op, OpAdaptor adaptor,\n                                     ConversionPatternRewriter &rewriter) const {\n-  if (op.axis() == 1) // FIXME(Qingyi): The fastest-changing dimension\n+  auto srcTy = op.operand().getType().cast<RankedTensorType>();\n+  auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  if (op.axis() == srcLayout.getOrder()[0])\n     return matchAndRewriteFast(op, adaptor, rewriter);\n   return matchAndRewriteBasic(op, adaptor, rewriter);\n }\n@@ -1444,6 +1454,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n \n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto srcOrd = srcLayout.getOrder();\n   auto srcShape = srcTy.getShape();\n \n   auto llvmElemTy = getTypeConverter()->convertType(srcTy.getElementType());\n@@ -1487,16 +1498,21 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     SmallVector<Value> writeIdx = indices[key];\n \n     writeIdx[axis] = udiv(writeIdx[axis], sizePerThread);\n-    Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n+    Value writeOffset =\n+        linearize(rewriter, loc, reorder<Value>(writeIdx, srcOrd),\n+                  reorder<unsigned>(smemShape, srcOrd));\n     Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n     store(acc, writePtr);\n \n     SmallVector<Value> readIdx(writeIdx.size(), ints[0]);\n     for (int N = smemShape[axis] / 2; N > 0; N >>= 1) {\n       readIdx[axis] = ints[N];\n       Value readMask = icmp_slt(writeIdx[axis], ints[N]);\n-      Value readOffset = select(\n-          readMask, linearize(rewriter, loc, readIdx, smemShape), ints[0]);\n+      Value readOffset =\n+          select(readMask,\n+                 linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n+                           reorder<unsigned>(smemShape, srcOrd)),\n+                 ints[0]);\n       Value readPtr = gep(elemPtrTy, writePtr, readOffset);\n       barrier();\n       accumulate(rewriter, loc, op.redOp(), acc, load(readPtr), false);\n@@ -1519,7 +1535,9 @@ LogicalResult ReduceOpConversion::matchAndRewriteBasic(\n     for (unsigned i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n       readIdx.insert(readIdx.begin() + axis, ints[0]);\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readOffset =\n+          linearize(rewriter, loc, reorder<Value>(readIdx, srcOrd),\n+                    reorder<unsigned>(smemShape, srcOrd));\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -1548,6 +1566,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n   auto srcTy = op.operand().getType().cast<RankedTensorType>();\n   auto srcLayout = srcTy.getEncoding().cast<BlockedEncodingAttr>();\n   auto srcShape = srcTy.getShape();\n+  auto srcRank = srcTy.getRank();\n \n   auto threadsPerWarp = srcLayout.getThreadsPerWarp();\n   auto warpsPerCTA = srcLayout.getWarpsPerCTA();\n@@ -1592,6 +1611,7 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n       delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n   SmallVector<Value> multiDimWarpId =\n       delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+\n   Value laneIdAxis = multiDimLaneId[axis];\n   Value warpIdAxis = multiDimWarpId[axis];\n \n@@ -1609,56 +1629,77 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n       accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n     }\n \n-    if (sizeInterWarps == 1) {\n-      SmallVector<Value> writeIdx = indices[key];\n-      writeIdx[axis] = zero;\n-      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, laneZero);\n-    } else {\n-      SmallVector<Value> writeIdx = indices[key];\n-      writeIdx[axis] =\n-          warpIdAxis; // axis must be the fastest-changing dimension\n-      Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, laneZero);\n-      barrier();\n+    SmallVector<Value> writeIdx = indices[key];\n+    writeIdx[axis] = (sizeInterWarps == 1) ? zero : warpIdAxis;\n+    Value writeOffset =\n+        linearize(rewriter, loc, reorder<Value>(writeIdx, order),\n+                  reorder<unsigned>(smemShape, order));\n+    Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    storeShared(rewriter, loc, writePtr, acc, laneZero);\n+  }\n \n-      SmallVector<Value> readIdx = writeIdx;\n-      readIdx[axis] = urem(laneId, i32_val(sizeInterWarps));\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n-      Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n-      acc = load(readPtr);\n+  barrier();\n \n-      // reduce across warps\n-      for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n-        Value shfl = shflSync(rewriter, loc, acc, N);\n-        accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n-      }\n+  // the second round of shuffle reduction\n+  //   now the problem size: sizeInterWarps, s1, s2, .. , sn  =>\n+  //                                      1, s1, s2, .. , sn\n+  //   where sizeInterWarps is 2^m\n+  //\n+  // each thread needs to process:\n+  //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n+  unsigned elems = product<unsigned>(smemShape);\n+  unsigned numThreads = product<unsigned>(srcLayout.getWarpsPerCTA()) * 32;\n+  unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n+  Value readOffset = threadId;\n+  for (unsigned round = 0; round < elemsPerThread; ++round) {\n+    Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n+    Value acc = load(readPtr);\n+\n+    for (unsigned N = sizeInterWarps / 2; N > 0; N >>= 1) {\n+      Value shfl = shflSync(rewriter, loc, acc, N);\n+      accumulate(rewriter, loc, op.redOp(), acc, shfl, false);\n+    }\n \n-      writeIdx[axis] = zero;\n-      writeOffset = linearize(rewriter, loc, writeIdx, smemShape);\n-      writePtr = gep(elemPtrTy, smemBase, writeOffset);\n-      storeShared(rewriter, loc, writePtr, acc, and_(laneZero, warpZero));\n+    Value writeOffset = udiv(readOffset, i32_val(sizeInterWarps));\n+    Value writePtr = gep(elemPtrTy, smemBase, writeOffset);\n+    Value threadIsNeeded = icmp_slt(threadId, i32_val(elems));\n+    Value laneIdModSizeInterWarps = urem(laneId, i32_val(sizeInterWarps));\n+    Value laneIdModSizeInterWarpsIsZero =\n+        icmp_eq(laneIdModSizeInterWarps, zero);\n+    storeShared(rewriter, loc, writePtr, acc,\n+                and_(threadIsNeeded, laneIdModSizeInterWarpsIsZero));\n+\n+    if (round != elemsPerThread - 1) {\n+      readOffset = add(readOffset, i32_val(numThreads));\n     }\n   }\n \n+  // We could avoid this barrier in some of the layouts, however this is not\n+  // the general case. TODO: optimize the barrier incase the layouts are\n+  // accepted.\n+  barrier();\n+\n   // set output values\n   if (auto resultTy = op.getType().dyn_cast<RankedTensorType>()) {\n     // nd-tensor where n >= 1\n     auto resultLayout = resultTy.getEncoding().cast<SliceEncodingAttr>();\n     auto resultShape = resultTy.getShape();\n+    SmallVector<unsigned> resultOrd;\n+    for (auto ord : order) {\n+      if (ord != 0)\n+        resultOrd.push_back(ord - 1);\n+    }\n \n     unsigned resultElems = getElemsPerThread(resultTy);\n     auto resultIndices = emitIndices(loc, rewriter, resultLayout, resultShape);\n     assert(resultIndices.size() == resultElems);\n \n-    barrier();\n     SmallVector<Value> resultVals(resultElems);\n     for (size_t i = 0; i < resultElems; ++i) {\n       SmallVector<Value> readIdx = resultIndices[i];\n-      readIdx.insert(readIdx.begin() + axis, i32_val(0));\n-      Value readOffset = linearize(rewriter, loc, readIdx, smemShape);\n+      Value readOffset =\n+          linearize(rewriter, loc, reorder<Value>(readIdx, resultOrd),\n+                    reorder<int64_t, unsigned>(resultShape, resultOrd));\n       Value readPtr = gep(elemPtrTy, smemBase, readOffset);\n       resultVals[i] = load(readPtr);\n     }\n@@ -1670,7 +1711,6 @@ LogicalResult ReduceOpConversion::matchAndRewriteFast(\n     rewriter.replaceOp(op, ret);\n   } else {\n     // 0d-tensor -> scalar\n-    barrier();\n     Value resultVal = load(smemBase);\n     rewriter.replaceOp(op, resultVal);\n   }\n@@ -1707,6 +1747,191 @@ struct ViewLikeOpConversion : public ConvertTritonGPUOpToLLVMPattern<SourceOp> {\n   }\n };\n \n+struct PrintfOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto loc = op->getLoc();\n+    SmallVector<Value, 16> operands;\n+    for (auto operand : adaptor.getOperands()) {\n+      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n+      for (auto elem : sub_operands) {\n+        operands.push_back(elem);\n+      }\n+    }\n+    std::string formatStr;\n+    llvm::raw_string_ostream os(formatStr);\n+    os << op.prefix();\n+    if (operands.size() > 0) {\n+      os << getFormatSubstr(operands[0]);\n+    }\n+\n+    for (size_t i = 1; i < operands.size(); ++i) {\n+      os << \", \" << getFormatSubstr(operands[i]);\n+    }\n+    llPrintf(formatStr, operands, rewriter);\n+    rewriter.eraseOp(op);\n+    return success();\n+  }\n+  // get format specific for each input value\n+  // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n+  std::string getFormatSubstr(Value value) const {\n+    Type type = value.getType();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+\n+    if (type.isa<LLVM::LLVMPointerType>()) {\n+      return \"%p\";\n+    } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n+      return \"%f\";\n+    } else if (type.isSignedInteger()) {\n+      return \"%i\";\n+    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n+      return \"%u\";\n+    }\n+    assert(false && \"not supported type\");\n+  }\n+\n+  // declare vprintf(i8*, i8*) as external function\n+  static LLVM::LLVMFuncOp\n+  getVprintfDeclaration(ConversionPatternRewriter &rewriter) {\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    StringRef funcName(\"vprintf\");\n+    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n+    if (funcOp)\n+      return cast<LLVM::LLVMFuncOp>(*funcOp);\n+\n+    auto *context = rewriter.getContext();\n+\n+    SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n+                               ptr_ty(IntegerType::get(context, 8))};\n+    auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n+\n+    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+    rewriter.setInsertionPointToStart(moduleOp.getBody());\n+\n+    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n+                                             funcType);\n+  }\n+\n+  // extend integer to int32, extend float to float64\n+  // this comes from vprintf alignment requirements.\n+  static std::pair<Type, Value>\n+  promoteValue(ConversionPatternRewriter &rewriter, Value value) {\n+    auto *context = rewriter.getContext();\n+    auto type = value.getType();\n+    type.dump();\n+    unsigned width = type.getIntOrFloatBitWidth();\n+    Value newOp = value;\n+    Type newType = type;\n+\n+    bool bUnsigned = type.isUnsignedInteger();\n+    if (type.isIntOrIndex() && width < 32) {\n+      if (bUnsigned) {\n+        newType = ui32_ty;\n+        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      } else {\n+        newType = i32_ty;\n+        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n+                                              value);\n+      }\n+    } else if (type.isBF16() || type.isF16() || type.isF32()) {\n+      newType = f64_ty;\n+      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n+                                             value);\n+    }\n+\n+    return {newType, newOp};\n+  }\n+\n+  static void llPrintf(StringRef msg, ValueRange args,\n+                       ConversionPatternRewriter &rewriter) {\n+    static const char formatStringPrefix[] = \"printfFormat_\";\n+    assert(!msg.empty() && \"printf with empty string not support\");\n+    Type int8Ptr = ptr_ty(i8_ty);\n+\n+    auto *context = rewriter.getContext();\n+    auto moduleOp =\n+        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n+    auto funcOp = getVprintfDeclaration(rewriter);\n+\n+    Value one = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n+    Value zero = rewriter.create<LLVM::ConstantOp>(\n+        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n+\n+    unsigned stringNumber = 0;\n+    SmallString<16> stringConstName;\n+    do {\n+      stringConstName.clear();\n+      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n+    } while (moduleOp.lookupSymbol(stringConstName));\n+\n+    llvm::SmallString<64> formatString(msg);\n+    formatString.push_back('\\n');\n+    formatString.push_back('\\0');\n+    size_t formatStringSize = formatString.size_in_bytes();\n+    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n+\n+    LLVM::GlobalOp global;\n+    {\n+      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n+      rewriter.setInsertionPointToStart(moduleOp.getBody());\n+      global = rewriter.create<LLVM::GlobalOp>(\n+          UnknownLoc::get(context), globalType,\n+          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n+          rewriter.getStringAttr(formatString));\n+    }\n+\n+    Value globalPtr =\n+        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n+    Value stringStart =\n+        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n+                                     globalPtr, mlir::ValueRange({zero, zero}));\n+\n+    Value bufferPtr =\n+        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n+\n+    SmallVector<Value, 16> newArgs;\n+    if (args.size() >= 1) {\n+      SmallVector<Type> argTypes;\n+      for (auto arg : args) {\n+        Type newType;\n+        Value newArg;\n+        std::tie(newType, newArg) = promoteValue(rewriter, arg);\n+        argTypes.push_back(newType);\n+        newArgs.push_back(newArg);\n+      }\n+\n+      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n+      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n+                                                       ptr_ty(structTy), one,\n+                                                       /*alignment=*/0);\n+\n+      for (const auto &entry : llvm::enumerate(newArgs)) {\n+        auto index = rewriter.create<LLVM::ConstantOp>(\n+            UnknownLoc::get(context), i32_ty,\n+            rewriter.getI32IntegerAttr(entry.index()));\n+        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n+            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n+            allocated, ArrayRef<Value>{zero, index});\n+        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n+                                       fieldPtr);\n+      }\n+      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n+                                                   int8Ptr, allocated);\n+    }\n+\n+    ValueRange operands{stringStart, bufferPtr};\n+    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n+  }\n+};\n+\n struct MakeRangeOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::MakeRangeOp> {\n \n@@ -1770,9 +1995,6 @@ struct AddPtrOpConversion\n     auto resultTy = op.getType();\n     auto resultTensorTy = resultTy.dyn_cast<RankedTensorType>();\n     if (resultTensorTy) {\n-      auto resultLayout =\n-          resultTensorTy.getEncoding().dyn_cast<BlockedEncodingAttr>();\n-      assert(resultLayout && \"Unexpected resultLayout in AddPtrOpConversion\");\n       unsigned elems = getElemsPerThread(resultTy);\n       Type elemTy =\n           getTypeConverter()->convertType(resultTensorTy.getElementType());\n@@ -2073,17 +2295,6 @@ struct ConvertLayoutOpConversion\n   }\n \n private:\n-  template <typename T>\n-  SmallVector<T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) const {\n-    size_t rank = order.size();\n-    assert(input.size() == rank);\n-    SmallVector<T> result(rank);\n-    for (auto it : llvm::enumerate(order)) {\n-      result[rank - 1 - it.value()] = input[it.index()];\n-    }\n-    return result;\n-  };\n-\n   // shared memory rd/st for blocked or mma layout with data padding\n   void processReplica(Location loc, ConversionPatternRewriter &rewriter,\n                       bool stNotRd, RankedTensorType type,\n@@ -4486,7 +4697,7 @@ struct InsertSliceAsyncOpConversion\n     auto numVecCols = std::max<unsigned>(inVec / outVec, 1);\n \n     auto srcIndices = emitIndices(loc, rewriter, srcBlockedLayout, srcShape);\n-    // <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n+    //  <<tileVecIdxRow, tileVecIdxCol>, TileOffset>\n     DenseMap<std::pair<unsigned, unsigned>, Value> tileOffsetMap;\n     for (unsigned elemIdx = 0; elemIdx < numElems; elemIdx += minVec) {\n       // minVec = 2, inVec = 4, outVec = 2\n@@ -4677,190 +4888,6 @@ struct FDivOpConversion\n   }\n };\n \n-struct PrintfOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<triton::PrintfOp> {\n-  using ConvertTritonGPUOpToLLVMPattern<\n-      triton::PrintfOp>::ConvertTritonGPUOpToLLVMPattern;\n-\n-  LogicalResult\n-  matchAndRewrite(triton::PrintfOp op, OpAdaptor adaptor,\n-                  ConversionPatternRewriter &rewriter) const override {\n-    auto loc = op->getLoc();\n-    SmallVector<Value, 16> operands;\n-    for (auto operand : adaptor.getOperands()) {\n-      auto sub_operands = this->getElementsFromStruct(loc, operand, rewriter);\n-      for (auto elem : sub_operands) {\n-        operands.push_back(elem);\n-      }\n-    }\n-    std::string formatStr;\n-    llvm::raw_string_ostream os(formatStr);\n-    os << op.prefix();\n-    if (operands.size() > 0) {\n-      os << getFormatSubstr(operands[0]);\n-    }\n-\n-    for (size_t i = 1; i < operands.size(); ++i) {\n-      os << \", \" << getFormatSubstr(operands[i]);\n-    }\n-    llPrintf(formatStr, operands, rewriter);\n-    rewriter.eraseOp(op);\n-    return success();\n-  }\n-  // get format specific for each input value\n-  // currently support pointer, i8, i16, i32, i64, f16, bf16, f32, f64\n-  std::string getFormatSubstr(Value value) const {\n-    Type type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-\n-    if (type.isa<LLVM::LLVMPointerType>()) {\n-      return \"%p\";\n-    } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n-      return \"%f\";\n-    } else if (type.isSignedInteger()) {\n-      return \"%i\";\n-    } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n-      return \"%u\";\n-    }\n-    assert(false && \"not supported type\");\n-  }\n-\n-  // declare vprintf(i8*, i8*) as external function\n-  LLVM::LLVMFuncOp\n-  getVprintfDeclaration(ConversionPatternRewriter &rewriter) const {\n-    auto moduleOp =\n-        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n-    StringRef funcName(\"vprintf\");\n-    Operation *funcOp = moduleOp.lookupSymbol(funcName);\n-    if (funcOp)\n-      return cast<LLVM::LLVMFuncOp>(*funcOp);\n-\n-    auto *context = rewriter.getContext();\n-\n-    SmallVector<Type> argsType{ptr_ty(IntegerType::get(context, 8)),\n-                               ptr_ty(IntegerType::get(context, 8))};\n-    auto funcType = LLVM::LLVMFunctionType::get(i32_ty, argsType);\n-\n-    ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-    rewriter.setInsertionPointToStart(moduleOp.getBody());\n-\n-    return rewriter.create<LLVM::LLVMFuncOp>(UnknownLoc::get(context), funcName,\n-                                             funcType);\n-  }\n-\n-  // extend integer to int32, extend float to float64\n-  // this comes from vprintf alignment requirements.\n-  std::pair<Type, Value> promoteValue(ConversionPatternRewriter &rewriter,\n-                                      Value value) const {\n-    auto *context = rewriter.getContext();\n-    auto type = value.getType();\n-    unsigned width = type.getIntOrFloatBitWidth();\n-    Value newOp = value;\n-    Type newType = type;\n-\n-    bool bUnsigned = type.isUnsignedInteger();\n-    if (type.isIntOrIndex() && width < 32) {\n-      if (bUnsigned) {\n-        newType = ui32_ty;\n-        newOp = rewriter.create<LLVM::ZExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n-      } else {\n-        newType = i32_ty;\n-        newOp = rewriter.create<LLVM::SExtOp>(UnknownLoc::get(context), newType,\n-                                              value);\n-      }\n-    } else if (type.isBF16() || type.isF16() || type.isF32()) {\n-      newType = f64_ty;\n-      newOp = rewriter.create<LLVM::FPExtOp>(UnknownLoc::get(context), newType,\n-                                             value);\n-    }\n-\n-    return {newType, newOp};\n-  }\n-\n-  void llPrintf(StringRef msg, ValueRange args,\n-                ConversionPatternRewriter &rewriter) const {\n-    static const char formatStringPrefix[] = \"printfFormat_\";\n-    assert(!msg.empty() && \"printf with empty string not support\");\n-    Type int8Ptr = ptr_ty(i8_ty);\n-\n-    auto *context = rewriter.getContext();\n-    auto moduleOp =\n-        rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();\n-    auto funcOp = getVprintfDeclaration(rewriter);\n-\n-    Value one = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(1));\n-    Value zero = rewriter.create<LLVM::ConstantOp>(\n-        UnknownLoc::get(context), i32_ty, rewriter.getI32IntegerAttr(0));\n-\n-    unsigned stringNumber = 0;\n-    SmallString<16> stringConstName;\n-    do {\n-      stringConstName.clear();\n-      (formatStringPrefix + Twine(stringNumber++)).toStringRef(stringConstName);\n-    } while (moduleOp.lookupSymbol(stringConstName));\n-\n-    llvm::SmallString<64> formatString(msg);\n-    formatString.push_back('\\n');\n-    formatString.push_back('\\0');\n-    size_t formatStringSize = formatString.size_in_bytes();\n-    auto globalType = LLVM::LLVMArrayType::get(i8_ty, formatStringSize);\n-\n-    LLVM::GlobalOp global;\n-    {\n-      ConversionPatternRewriter::InsertionGuard guard(rewriter);\n-      rewriter.setInsertionPointToStart(moduleOp.getBody());\n-      global = rewriter.create<LLVM::GlobalOp>(\n-          UnknownLoc::get(context), globalType,\n-          /*isConstant=*/true, LLVM::Linkage::Internal, stringConstName,\n-          rewriter.getStringAttr(formatString));\n-    }\n-\n-    Value globalPtr =\n-        rewriter.create<LLVM::AddressOfOp>(UnknownLoc::get(context), global);\n-    Value stringStart =\n-        rewriter.create<LLVM::GEPOp>(UnknownLoc::get(context), int8Ptr,\n-                                     globalPtr, mlir::ValueRange({zero, zero}));\n-\n-    Value bufferPtr =\n-        rewriter.create<LLVM::NullOp>(UnknownLoc::get(context), int8Ptr);\n-\n-    SmallVector<Value, 16> newArgs;\n-    if (args.size() >= 1) {\n-      SmallVector<Type> argTypes;\n-      for (auto arg : args) {\n-        Type newType;\n-        Value newArg;\n-        std::tie(newType, newArg) = promoteValue(rewriter, arg);\n-        argTypes.push_back(newType);\n-        newArgs.push_back(newArg);\n-      }\n-\n-      Type structTy = LLVM::LLVMStructType::getLiteral(context, argTypes);\n-      auto allocated = rewriter.create<LLVM::AllocaOp>(UnknownLoc::get(context),\n-                                                       ptr_ty(structTy), one,\n-                                                       /*alignment=*/0);\n-\n-      for (const auto &entry : llvm::enumerate(newArgs)) {\n-        auto index = rewriter.create<LLVM::ConstantOp>(\n-            UnknownLoc::get(context), i32_ty,\n-            rewriter.getI32IntegerAttr(entry.index()));\n-        auto fieldPtr = rewriter.create<LLVM::GEPOp>(\n-            UnknownLoc::get(context), ptr_ty(argTypes[entry.index()]),\n-            allocated, ArrayRef<Value>{zero, index});\n-        rewriter.create<LLVM::StoreOp>(UnknownLoc::get(context), entry.value(),\n-                                       fieldPtr);\n-      }\n-      bufferPtr = rewriter.create<LLVM::BitcastOp>(UnknownLoc::get(context),\n-                                                   int8Ptr, allocated);\n-    }\n-\n-    ValueRange operands{stringStart, bufferPtr};\n-    rewriter.create<LLVM::CallOp>(UnknownLoc::get(context), funcOp, operands);\n-  }\n-};\n-\n void populateTritonToLLVMPatterns(mlir::LLVMTypeConverter &typeConverter,\n                                   RewritePatternSet &patterns, int numWarps,\n                                   AxisInfoAnalysis &axisInfoAnalysis,\n@@ -5065,6 +5092,15 @@ void ConvertTritonGPUToLLVM::initSharedMemory(\n \n namespace mlir {\n \n+namespace LLVM {\n+\n+void llPrintf(StringRef msg, ValueRange args,\n+              ConversionPatternRewriter &rewriter) {\n+  PrintfOpConversion::llPrintf(msg, args, rewriter);\n+}\n+\n+} // namespace LLVM\n+\n TritonLLVMConversionTarget::TritonLLVMConversionTarget(\n     MLIRContext &ctx, mlir::LLVMTypeConverter &typeConverter)\n     : ConversionTarget(ctx) {"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 33, "deletions": 38, "changes": 71, "file_content_changes": "@@ -17,6 +17,14 @@ namespace ttg = triton::gpu;\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n+static Type getI1SameShape(Value v) {\n+  Type vType = v.getType();\n+  auto i1Type = IntegerType::get(vType.getContext(), 1);\n+  auto tensorType = vType.cast<RankedTensorType>();\n+  return RankedTensorType::get(tensorType.getShape(), i1Type,\n+                               tensorType.getEncoding());\n+}\n+\n namespace {\n \n class LoopPipeliner {\n@@ -325,13 +333,23 @@ void LoopPipeliner::emitPrologue() {\n           loadStageBuffer[op->getResult(0)] = {loadsBuffer[op->getResult(0)]};\n         }\n         // load => copy async\n-        // TODO: check if the hardware supports async copy\n         if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(op)) {\n-          newOp = builder.create<ttg::InsertSliceAsyncOp>(\n-              op->getLoc(), loadsBufferType[loadOp],\n+          Value mask = lookupOrDefault(loadOp.mask(), stage);\n+          Value newMask;\n+          if (mask) {\n+            Value splatCond = builder.create<triton::SplatOp>(\n+                mask.getLoc(), mask.getType(), loopCond);\n+            newMask =\n+                builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n+          } else {\n+            newMask = builder.create<triton::SplatOp>(\n+                loopCond.getLoc(), getI1SameShape(loadOp), loopCond);\n+          }\n+          // TODO: check if the hardware supports async copy\n+          newOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n+              op->getLoc(), loadsBuffer[loadOp].getType(),\n               lookupOrDefault(loadOp.ptr(), stage),\n-              loadStageBuffer[loadOp][stage], pipelineIterIdx,\n-              lookupOrDefault(loadOp.mask(), stage),\n+              loadStageBuffer[loadOp][stage], pipelineIterIdx, newMask,\n               lookupOrDefault(loadOp.other(), stage), loadOp.cache(),\n               loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n           loadStageBuffer[loadOp].push_back(newOp->getResult(0));\n@@ -350,32 +368,6 @@ void LoopPipeliner::emitPrologue() {\n         }\n       }\n \n-      // If this is a load/async_copy, we need to update the mask\n-      if (Value mask = [&]() {\n-            if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(newOp)) {\n-              return loadOp.mask();\n-            } else if (auto insertSliceAsyncOp =\n-                           llvm::dyn_cast<ttg::InsertSliceAsyncOp>(newOp)) {\n-              return insertSliceAsyncOp.mask();\n-            } else {\n-              return mlir::Value();\n-            }\n-          }()) {\n-        // assert(I1 or TensorOf<[I1]>);\n-        OpBuilder::InsertionGuard g(builder);\n-        // TODO: move this out of the loop\n-        builder.setInsertionPoint(newOp);\n-        Value splatCond = builder.create<triton::SplatOp>(\n-            mask.getLoc(), mask.getType(), loopCond);\n-        Value newMask =\n-            builder.create<arith::AndIOp>(mask.getLoc(), mask, splatCond);\n-        // TODO: better way to do this?\n-        if (llvm::isa<triton::LoadOp>(newOp))\n-          newOp->setOperand(1, newMask);\n-        else // InsertSliceAsyncOp\n-          newOp->setOperand(3, newMask);\n-      }\n-\n       // update mapping of results\n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults())) {\n         Value originalResult = op->getResult(dstIdx);\n@@ -394,7 +386,7 @@ void LoopPipeliner::emitPrologue() {\n                 newOp->getResult(dstIdx), stage + 1);\n         }\n       }\n-    }\n+    } // for (Operation *op : orderedDeps)\n \n     pipelineIterIdx = builder.create<arith::AddIOp>(\n         iv.getLoc(), pipelineIterIdx,\n@@ -552,26 +544,29 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n \n   for (Operation *op : orderedDeps) {\n     Operation *nextOp = nullptr;\n-    // TODO(da): does this work if loadOp has no mask?\n     // update loading mask\n     if (loads.contains(op->getResult(0))) {\n       auto loadOp = llvm::cast<triton::LoadOp>(op);\n       Value mask = loadOp.mask();\n+      Value newMask;\n       if (mask) {\n         Value splatCond = builder.create<triton::SplatOp>(\n             mask.getLoc(), mask.getType(), nextLoopCond);\n-        Value newMask = builder.create<arith::AndIOp>(\n+        newMask = builder.create<arith::AndIOp>(\n             mask.getLoc(), splatCond, nextMapping.lookupOrDefault(mask));\n         // if mask is defined outside the loop, don't update the map more than\n         // once\n         if (!(forOp.isDefinedOutsideOfLoop(mask) && nextMapping.contains(mask)))\n           nextMapping.map(mask, newMask);\n-      }\n-      Value insertAsyncOp = builder.create<ttg::InsertSliceAsyncOp>(\n-          op->getLoc(), loadsBufferType[loadOp],\n+        newMask = nextMapping.lookupOrDefault(loadOp.mask());\n+      } else\n+        newMask = builder.create<triton::SplatOp>(\n+            loadOp.getLoc(), getI1SameShape(loadOp), nextLoopCond);\n+      Value insertAsyncOp = builder.create<triton::gpu::InsertSliceAsyncOp>(\n+          op->getLoc(), loadsBuffer[loadOp].getType(),\n           nextMapping.lookupOrDefault(loadOp.ptr()),\n           newForOp.getRegionIterArgs()[bufferIdx + nextBuffers.size()],\n-          insertSliceIndex, nextMapping.lookupOrDefault(loadOp.mask()),\n+          insertSliceIndex, newMask,\n           nextMapping.lookupOrDefault(loadOp.other()), loadOp.cache(),\n           loadOp.evict(), loadOp.isVolatile(), /*axis*/ 0);\n       nextBuffers.push_back(insertAsyncOp);"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 27, "deletions": 16, "changes": 43, "file_content_changes": "@@ -151,7 +151,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n     return nullptr;\n   }\n \n-  std::map<std::string, std::string> extern_libs;\n+  std::map<std::string, std::string> externLibs;\n   SmallVector<LLVM::LLVMFuncOp> funcs;\n   module.walk([&](LLVM::LLVMFuncOp func) {\n     if (func.isExternal())\n@@ -166,7 +166,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n           func.getOperation()->getAttr(\"libpath\").dyn_cast<StringAttr>();\n       if (name) {\n         std::string lib_name = name.str();\n-        extern_libs[lib_name] = path.str();\n+        externLibs[lib_name] = path.str();\n       }\n     }\n   }\n@@ -176,7 +176,7 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n                     ->getAttr(\"triton_gpu.externs\")\n                     .dyn_cast<DictionaryAttr>();\n     for (auto &attr : dict) {\n-      extern_libs[attr.getName().strref().trim().str()] =\n+      externLibs[attr.getName().strref().trim().str()] =\n           attr.getValue().dyn_cast<StringAttr>().strref().trim().str();\n     }\n   }\n@@ -188,20 +188,9 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   }\n \n   llvm::SMDiagnostic err;\n-  for (auto &lib : extern_libs) {\n-    auto ext_mod = llvm::parseIRFile(lib.second, err, *llvmContext);\n-    if (!ext_mod) {\n-      llvm::errs() << \"Failed to load extern lib \" << lib.first;\n+  for (auto &lib : externLibs) {\n+    if (linkExternLib(*llvmir, lib.second))\n       return nullptr;\n-    }\n-    ext_mod->setTargetTriple(llvmir->getTargetTriple());\n-    ext_mod->setDataLayout(llvmir->getDataLayout());\n-\n-    if (llvm::Linker::linkModules(*llvmir, std::move(ext_mod),\n-                                  llvm::Linker::Flags::LinkOnlyNeeded)) {\n-      llvm::errs() << \"Failed to link extern lib \" << lib.first;\n-      return nullptr;\n-    }\n   }\n \n   return llvmir;\n@@ -227,5 +216,27 @@ void addExternalLibs(mlir::ModuleOp &module,\n   return;\n }\n \n+bool linkExternLib(llvm::Module &module, llvm::StringRef path) {\n+  llvm::SMDiagnostic err;\n+  auto &ctx = module.getContext();\n+\n+  auto extMod = llvm::parseIRFile(path, err, ctx);\n+  if (!extMod) {\n+    llvm::errs() << \"Failed to load \" << path;\n+    return true;\n+  }\n+\n+  extMod->setTargetTriple(module.getTargetTriple());\n+  extMod->setDataLayout(module.getDataLayout());\n+\n+  if (llvm::Linker::linkModules(module, std::move(extMod),\n+                                llvm::Linker::Flags::LinkOnlyNeeded)) {\n+    llvm::errs() << \"Failed to link \" << path;\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n } // namespace triton\n } // namespace mlir"}, {"filename": "lib/Target/PTX/PTXTranslation.cpp", "status": "modified", "additions": 38, "deletions": 0, "changes": 38, "file_content_changes": "@@ -29,6 +29,7 @@\n #include \"llvm/Target/TargetOptions.h\"\n #include \"llvm/Transforms/Scalar.h\"\n #include \"llvm/Transforms/Utils/Cloning.h\"\n+#include <filesystem>\n #include <regex>\n \n namespace triton {\n@@ -61,6 +62,43 @@ static bool find_and_replace(std::string &str, const std::string &begin,\n }\n \n static std::string llir_to_ptx(llvm::Module *module, int capability, int ptx) {\n+  bool hasExternal = false;\n+  for (auto &func : *module) {\n+    if (func.hasExternalLinkage()) {\n+      hasExternal = true;\n+      break;\n+    }\n+  }\n+\n+  if (hasExternal) {\n+    namespace fs = std::filesystem;\n+    // [triton root dir]/python/triton/language/libdevice.10.bc\n+    static const fs::path libdevice = fs::path(__FILE__)\n+                                          .parent_path()\n+                                          .parent_path()\n+                                          .parent_path()\n+                                          .parent_path() /\n+                                      \"python\" / \"triton\" / \"language\" /\n+                                      \"libdevice.10.bc\";\n+    if (mlir::triton::linkExternLib(*module, libdevice.string()))\n+      llvm::errs() << \"link failed for: \" << libdevice.string();\n+  }\n+\n+  // please check https://llvm.org/docs/NVPTXUsage.html#reflection-parameters\n+  // this will enable fast math path in libdevice\n+  // for example, when enable nvvm-reflect-ftz, sqrt.approx.f32 will change to\n+  // sqrt.approx.ftz.f32\n+  {\n+    auto &ctx = module->getContext();\n+    llvm::Type *I32 = llvm::Type::getInt32Ty(ctx);\n+    llvm::Metadata *mdFour =\n+        llvm::ConstantAsMetadata::get(llvm::ConstantInt::getSigned(I32, 4));\n+    llvm::Metadata *mdName = llvm::MDString::get(ctx, \"nvvm-reflect-ftz\");\n+    llvm::Metadata *mdOne =\n+        llvm::ConstantAsMetadata::get(llvm::ConstantInt::getSigned(I32, 1));\n+    llvm::MDNode *reflect = llvm::MDNode::get(ctx, {mdFour, mdName, mdOne});\n+    module->addModuleFlag(reflect);\n+  }\n   // LLVM version in use may not officially support target hardware\n   int max_nvvm_cc = 75;\n   // int max_nvvm_ptx = 74;"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 9, "deletions": 13, "changes": 22, "file_content_changes": "@@ -319,28 +319,22 @@ void init_triton_ir(py::module &&m) {\n   m.def(\n       \"parse_mlir_module\",\n       [](const std::string &inputFilename, mlir::MLIRContext &context) {\n-        // open file\n-        std::string errorMessage;\n-        auto input = mlir::openInputFile(inputFilename, &errorMessage);\n-        if (!input)\n-          throw std::runtime_error(errorMessage);\n-\n         // initialize registry\n         mlir::DialectRegistry registry;\n         registry.insert<mlir::triton::TritonDialect,\n                         mlir::triton::gpu::TritonGPUDialect,\n                         mlir::math::MathDialect, mlir::arith::ArithmeticDialect,\n                         mlir::StandardOpsDialect, mlir::scf::SCFDialect>();\n-\n         context.appendDialectRegistry(registry);\n         context.loadAllAvailableDialects();\n-        context.allowUnregisteredDialects();\n \n         // parse module\n-        llvm::SourceMgr sourceMgr;\n-        sourceMgr.AddNewSourceBuffer(std::move(input), llvm::SMLoc());\n         mlir::OwningOpRef<mlir::ModuleOp> module(\n-            mlir::parseSourceFile(sourceMgr, &context));\n+            mlir::parseSourceFile(inputFilename, &context));\n+        // locations are incompatible with ptx < 7.5 !\n+        module->walk([](mlir::Operation *op) {\n+          op->setLoc(mlir::UnknownLoc::get(op->getContext()));\n+        });\n         if (!module)\n           throw std::runtime_error(\"Parse MLIR file failed.\");\n \n@@ -1080,7 +1074,8 @@ void init_triton_ir(py::module &&m) {\n            [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n               mlir::Value &val) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = ptr.getType().dyn_cast<mlir::triton::PointerType>();\n+             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n+                                .cast<mlir::triton::PointerType>();\n              mlir::Type dstType = ptrType.getPointeeType();\n              return self.create<mlir::triton::AtomicCASOp>(loc, dstType, ptr,\n                                                            cmp, val);\n@@ -1090,7 +1085,8 @@ void init_triton_ir(py::module &&m) {\n               mlir::Value &ptr, mlir::Value &val,\n               mlir::Value &mask) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n-             auto ptrType = ptr.getType().dyn_cast<mlir::triton::PointerType>();\n+             auto ptrType = mlir::getElementTypeOrSelf(ptr)\n+                                .cast<mlir::triton::PointerType>();\n              mlir::Type dstType = ptrType.getPointeeType();\n              return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n                                                            ptr, val, mask);"}, {"filename": "python/tests/test_core.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -144,7 +144,7 @@ def kernel(Z, X, SIZE: tl.constexpr):\n     # triton result\n     x_tri = to_triton(x, device=device, dst_type=dtype_x)\n     z_tri = to_triton(np.empty_like(z_ref), device=device, dst_type=dtype_x)\n-    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4, extern_libs={\"libdevice\": \"/usr/local/cuda/nvvm/libdevice/libdevice.10.bc\"})\n+    kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4)\n     # compare\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n \n@@ -940,7 +940,9 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n # shape (128, 256) and (32, 1024) are not enabled on sm86 because the required shared memory\n # exceeds the limit of 99KB\n-reduce2d_shapes = [(2, 32), (4, 32), (4, 128), (32, 64), (64, 128)]\n+reduce2d_shapes = [(2, 32), (4, 32), (4, 128)]\n+# TODO: fix and uncomment\n+#, (32, 64), (64, 128)]\n if 'V100' in torch.cuda.get_device_name(0):\n     reduce2d_shapes += [(128, 256) and (32, 1024)]\n "}, {"filename": "python/tests/test_reduce.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -126,7 +126,6 @@ def test_reduce2d(op, dtype, shape, axis):\n         golden_z = torch.min(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n     else:\n         golden_z = torch.max(x, dim=axis, keepdim=False)[0].to(reduced_dtype)\n-\n     if dtype.is_floating_point and op == 'sum':\n         if shape[axis] >= 256:\n             assert_close(z, golden_z, rtol=0.05, atol=0.1)"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -1283,17 +1283,17 @@ def make_fn_cache_key(fn_hash, signature, configs, constants, num_warps, num_sta\n def read_or_execute(cache_manager, force_compile, file_name, metadata,\n                     run_if_found: Callable[[str], bytes] = None,\n                     run_if_not_found: Callable = None):\n+    suffix = file_name.split(\".\")[1]\n     if not force_compile and cache_manager.has_file(file_name):\n       module = run_if_found(cache_manager._make_path(file_name))\n       data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n       md5 = hashlib.md5(data).hexdigest()\n-      suffix = file_name.split(\".\")[1]\n       has_changed = metadata and md5 != metadata[\"md5\"][suffix]\n       return module, md5, has_changed, True\n     module = run_if_not_found()\n     data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n     md5 = hashlib.md5(data).hexdigest()\n-    cache_manager.put(data, file_name, True)\n+    cache_manager.put(data, file_name, True if isinstance(data, bytes) else data)\n     return module, md5, True, False\n \n "}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: python3 -m triton.tools.aot %s --target=llvm-ir | FileCheck %s\n+// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir | FileCheck %s\n \n // == LLVM IR check begin ==\n // CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'"}, {"filename": "test/Target/tritongpu_to_ptx.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: python3 -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n+// RUN: %PYTHON -m triton.tools.aot %s --target=ptx --sm=80 --ptx-version=63 | FileCheck %s\n // CHECK-LABEL: // Generated by LLVM NVPTX Back-End\n // CHECK: .version 6.3\n // CHECK: .target sm_80"}, {"filename": "test/TritonGPU/coalesce.mlir", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -9,8 +9,8 @@\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n \n-// CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [1, 0]}>\n-// CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [0, 1]}>\n+// CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>\n+// CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 4], order = [0, 1]}>\n // CHECK: [[load_ptr:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64x!tt.ptr<f32>, [[row_layout]]>\n // CHECK: [[load_mask:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xi1, [[row_layout]]>\n // CHECK: [[load_other:%.*]] = triton_gpu.convert_layout {{.*}} -> tensor<64x64xf32, [[row_layout]]>"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 12, "deletions": 5, "changes": 17, "file_content_changes": "@@ -13,12 +13,19 @@\n // CHECK-DAG: %[[CONSTANT_1:.*]] = arith.constant 1 : i32\n // CHECK-DAG: %[[CONSTANT_2:.*]] = arith.constant 2 : i32\n // CHECK-DAG: %[[CONSTANT_3:.*]] = arith.constant 3 : i32\n+// CHECK-DAG: %[[LOOP_COND_0:.*]] = arith.cmpi slt, %[[LB:.*]], %[[UB:.*]]\n // CHECK: %[[ABUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n+// CHECK-DAG: %[[LOOP_COND_0_SPLAT_A:.*]] = tt.splat %[[LOOP_COND_0]]\n+// CHECK: %[[A0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]], %[[LOOP_COND_0_SPLAT_A]]\n // CHECK: %[[BBUFFER:.*]] = triton_gpu.alloc_tensor\n-// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]]\n-// CHECK: %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n-// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]]\n+// CHECK-DAG: %[[LOOP_COND_0_SPLAT_B:.*]] = tt.splat %[[LOOP_COND_0]]\n+// CHECK: %[[B0BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_0]], %[[LOOP_COND_0_SPLAT_B]]\n+// CHECK-DAG: %[[IV_1:.*]] = arith.addi %[[LB]], %[[STEP:.*]]\n+// CHECK-DAG: %[[LOOP_COND_1:.*]] = arith.cmpi slt, %[[IV_1]], %[[UB]]\n+// CHECK-DAG: %[[LOOP_COND_1_SPLAT_A:.*]] = tt.splat %[[LOOP_COND_1]]\n+// CHECK: %[[A1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]], %[[LOOP_COND_1_SPLAT_A]]\n+// CHECK-DAG: %[[LOOP_COND_1_SPLAT_B:.*]] = tt.splat %[[LOOP_COND_1]]\n+// CHECK: %[[B1BUFFER:.*]] = triton_gpu.insert_slice_async {{.*}}, {{.*}}, %[[CONSTANT_1]], %[[LOOP_COND_1_SPLAT_B]]\n // CHECK:   triton_gpu.async_wait {num = 2 : i32}\n // CHECK: %[[A0:.*]] = tensor.extract_slice %[[A1BUFFER]][0, 0, 0]\n // CHECK: %[[B0:.*]] = tensor.extract_slice %[[B1BUFFER]][0, 0, 0]\n@@ -51,7 +58,7 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n \n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n-    %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+    %a_ = tt.load %a_ptr {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n     %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n     %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>"}]