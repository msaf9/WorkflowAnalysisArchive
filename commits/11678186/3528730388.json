[{"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -13,7 +13,7 @@ std::unique_ptr<Pass> createTritonGPUCanonicalizeLoopsPass();\n \n std::unique_ptr<Pass> createTritonGPUCoalescePass();\n \n-std::unique_ptr<Pass> createTritonGPUCombineOpsPass();\n+std::unique_ptr<Pass> createTritonGPUCombineOpsPass(int computeCapability = 80);\n \n std::unique_ptr<Pass> createTritonGPUVerifier();\n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "file_content_changes": "@@ -41,7 +41,7 @@ def TritonGPUCoalesce: Pass<\"tritongpu-coalesce\", \"mlir::ModuleOp\"> {\n   let summary = \"coalesce\";\n \n   let description = [{\n-    TODO  \n+    TODO\n   }];\n \n   let constructor = \"mlir::createTritonGPUCoalescePass()\";\n@@ -63,6 +63,12 @@ def TritonGPUCombineOps : Pass<\"tritongpu-combine\", \"mlir::ModuleOp\"> {\n \n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\",\n                            \"mlir::triton::TritonDialect\"];\n+\n+  let options = [\n+    Option<\"computeCapability\", \"compute-capability\",\n+           \"int32_t\", /*default*/\"80\",\n+           \"device compute capability\">\n+  ];\n }\n \n def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::ModuleOp\"> {"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -86,9 +86,10 @@ void MembarAnalysis::transfer(Operation *op, RegionInfo *regionInfo,\n       curRegionInfo.syncWriteBuffers.insert(bufferId);\n     }\n   }\n-  // Scratch buffer is considered as a shared memory read\n+  // Scratch buffer is considered as both shared memory write & read\n   auto bufferId = allocation->getBufferId(op);\n   if (bufferId != Allocation::InvalidBufferId) {\n+    curRegionInfo.syncWriteBuffers.insert(bufferId);\n     curRegionInfo.syncReadBuffers.insert(bufferId);\n   }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 2, "deletions": 6, "changes": 8, "file_content_changes": "@@ -3079,7 +3079,8 @@ LogicalResult ConvertLayoutOpConversion::lowerDistributedToDistributed(\n   for (unsigned repId = 0; repId < accumNumReplicates; ++repId) {\n     auto multiDimRepId =\n         getMultiDimIndex<unsigned>(repId, numReplicates, outOrd);\n-    barrier();\n+    if (repId != 0)\n+      barrier();\n     if (srcLayout.isa<BlockedEncodingAttr>() ||\n         srcLayout.isa<SliceEncodingAttr>() ||\n         srcLayout.isa<MmaEncodingAttr>()) {\n@@ -3171,11 +3172,6 @@ LogicalResult ConvertLayoutOpConversion::lowerBlockedToShared(\n   auto retVal = getStructFromSharedMemoryObject(loc, smemObj, rewriter);\n   auto numWordsEachRep = product<unsigned>(wordsInEachRep);\n   SmallVector<Value> wordVecs(numWordsEachRep);\n-  // TODO: We should get less barriers if it is handled by membar pass\n-  //       instead of the backend, since the later can only handle it in\n-  //       the most conservative way. However just keep for now and revisit\n-  //       in the future in case necessary.\n-  barrier();\n   for (unsigned i = 0; i < numElems; ++i) {\n     if (i % srcAccumSizeInThreads == 0) {\n       // start of a replication"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 45, "deletions": 11, "changes": 56, "file_content_changes": "@@ -15,6 +15,7 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n \n #include <memory>\n \n@@ -560,19 +561,46 @@ class RematerializeForward : public mlir::RewritePattern {\n // -----------------------------------------------------------------------------\n //\n // -----------------------------------------------------------------------------\n+static int computeCapabilityToMMAVersion(int computeCapability) {\n+  if (computeCapability < 80) {\n+    return 1;\n+  } else if (computeCapability < 90) {\n+    return 2;\n+  } else {\n+    assert(false && \"computeCapability > 90 not supported\");\n+    return 0;\n+  }\n+}\n+\n+static SmallVector<int64_t, 2>\n+mmaVersionToShapePerWarp(int version, const ArrayRef<int64_t> &shape,\n+                         int numWarps) {\n+  if (version == 1) {\n+    return {16, 16};\n+  } else if (version == 2) {\n+    return {16, 8};\n+  } else {\n+    assert(false && \"version not supported\");\n+    return {0, 0};\n+  }\n+}\n \n class BlockedToMMA : public mlir::RewritePattern {\n+  int computeCapability;\n+\n public:\n-  BlockedToMMA(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::DotOp::getOperationName(), 2, context) {}\n+  BlockedToMMA(mlir::MLIRContext *context, int computeCapability)\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 2, context),\n+        computeCapability(computeCapability) {}\n \n   static SmallVector<unsigned, 2>\n   getWarpsPerTile(const ArrayRef<int64_t> &shape, int version, int numWarps) {\n     assert(version == 2);\n     // TODO: Handle one warp per row for fused matmuls\n     // TODO: unsigned -> int64_t to keep things uniform\n     SmallVector<unsigned, 2> ret = {1, 1};\n-    SmallVector<int64_t, 2> shapePerWarp = {16, 8};\n+    SmallVector<int64_t, 2> shapePerWarp =\n+        mmaVersionToShapePerWarp(version, shape, numWarps);\n     bool changed = false;\n     // TODO (@daadaada): double-check.\n     // original logic in\n@@ -615,11 +643,12 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto retShape = oldRetType.getShape();\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n-    auto newRetType =\n-        RankedTensorType::get(retShape, oldRetType.getElementType(),\n-                              triton::gpu::MmaEncodingAttr::get(\n-                                  oldRetType.getContext(), 2,\n-                                  getWarpsPerTile(retShape, 2, numWarps)));\n+    int version = computeCapabilityToMMAVersion(computeCapability);\n+    auto newRetType = RankedTensorType::get(\n+        retShape, oldRetType.getElementType(),\n+        triton::gpu::MmaEncodingAttr::get(\n+            oldRetType.getContext(), version,\n+            getWarpsPerTile(retShape, version, numWarps)));\n     // convert accumulator\n     auto oldAcc = dotOp.getOperand(2);\n     auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n@@ -656,6 +685,10 @@ class BlockedToMMA : public mlir::RewritePattern {\n class TritonGPUCombineOpsPass\n     : public TritonGPUCombineOpsBase<TritonGPUCombineOpsPass> {\n public:\n+  TritonGPUCombineOpsPass() = default;\n+  TritonGPUCombineOpsPass(int computeCapability) {\n+    this->computeCapability = computeCapability;\n+  }\n   void runOnOperation() override {\n     MLIRContext *context = &getContext();\n     ModuleOp m = getOperation();\n@@ -667,14 +700,15 @@ class TritonGPUCombineOpsPass\n     patterns.add<RematerializeBackward>(context);\n     patterns.add<RematerializeForward>(context);\n     patterns.add<MoveConvertOutOfLoop>(context);\n-    patterns.add<BlockedToMMA>(context);\n+    patterns.add<BlockedToMMA>(context, computeCapability);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed()) {\n       signalPassFailure();\n     }\n   }\n };\n \n-std::unique_ptr<Pass> mlir::createTritonGPUCombineOpsPass() {\n-  return std::make_unique<TritonGPUCombineOpsPass>();\n+std::unique_ptr<Pass>\n+mlir::createTritonGPUCombineOpsPass(int computeCapability) {\n+  return std::make_unique<TritonGPUCombineOpsPass>(computeCapability);\n }"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1279,8 +1279,9 @@ void init_triton_ir(py::module &&m) {\n              self.addPass(mlir::createTritonGPUPrefetchPass());\n            })\n       .def(\"add_triton_gpu_combine_pass\",\n-           [](mlir::PassManager &self) {\n-             self.addPass(mlir::createTritonGPUCombineOpsPass());\n+           [](mlir::PassManager &self, int computeCapability) {\n+             self.addPass(\n+                 mlir::createTritonGPUCombineOpsPass(computeCapability));\n            })\n       .def(\"add_triton_gpu_to_llvm\",\n            [](mlir::PassManager &self) {\n@@ -1301,7 +1302,7 @@ void init_triton_translation(py::module &m) {\n \n   m.def(\n       \"translate_triton_gpu_to_llvmir\",\n-      [](mlir::ModuleOp op) {\n+      [](mlir::ModuleOp op, int computeCapability) {\n         llvm::LLVMContext llvmContext;\n         auto llvmModule =\n             ::mlir::triton::translateTritonGPUToLLVMIR(&llvmContext, op);"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 60, "deletions": 62, "changes": 122, "file_content_changes": "@@ -15,10 +15,10 @@\n import tempfile\n import warnings\n from collections import namedtuple\n+from pathlib import Path\n from sysconfig import get_paths\n from typing import Any, Callable, Dict, Tuple, Union\n \n-from pathlib import Path\n import setuptools\n import torch\n from filelock import FileLock\n@@ -828,7 +828,7 @@ def kernel_suffix(signature, specialization):\n def build_triton_ir(fn, signature, specialization, constants):\n     # canonicalize signature\n     if isinstance(signature, str):\n-      signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n+        signature = {k: v.strip() for k, v in enumerate(signature.split(\",\"))}\n     context = _triton.ir.context()\n     context.load_triton()\n     # create kernel prototype\n@@ -876,23 +876,23 @@ def ast_to_ttir(fn, signature, specialization, constants):\n     return optimize_triton_ir(mod)\n \n \n-def ttir_to_ttgir(mod, num_warps, num_stages):\n+def ttir_to_ttgir(mod, num_warps, num_stages, compute_capability):\n     pm = _triton.ir.pass_manager(mod.context)\n     pm.add_convert_triton_to_tritongpu_pass(num_warps)\n     pm.enable_debug()\n     # Convert blocked layout to mma layout for dot ops so that pipeline\n     # can get shared memory swizzled correctly.\n     pm.add_coalesce_pass()\n-    pm.add_triton_gpu_combine_pass()\n+    pm.add_triton_gpu_combine_pass(compute_capability)\n     pm.add_tritongpu_pipeline_pass(num_stages)\n     # Prefetch must be done after pipeline pass because pipeline pass\n     # extracts slices from the original tensor.\n     pm.add_tritongpu_prefetch_pass()\n     pm.add_canonicalizer_pass()\n     pm.add_cse_pass()\n-    pm.add_triton_gpu_combine_pass()\n+    pm.add_triton_gpu_combine_pass(compute_capability)\n     pm.add_licm_pass()\n-    pm.add_triton_gpu_combine_pass()\n+    pm.add_triton_gpu_combine_pass(compute_capability)\n     pm.add_cse_pass()\n     pm.run(mod)\n     return mod\n@@ -905,40 +905,34 @@ def add_external_libs(mod, libs):\n     _triton.add_external_libs(mod, list(libs.keys()), list(libs.values()))\n \n \n-def ttgir_to_llir(mod, extern_libs):\n+def ttgir_to_llir(mod, extern_libs, compute_capability):\n     if extern_libs:\n         add_external_libs(mod, extern_libs)\n-    return _triton.translate_triton_gpu_to_llvmir(mod)\n+    return _triton.translate_triton_gpu_to_llvmir(mod, compute_capability)\n \n \n-def llir_to_ptx(mod: Any, compute_capability: int = None, ptx_version: int = None) -> Tuple[str, int]:\n+def llir_to_ptx(mod: Any, compute_capability: int, ptx_version: int = None) -> Tuple[str, int]:\n     '''\n     Translate TritonGPU module to PTX code.\n     :param mod: a TritonGPU dialect module\n     :return:\n         - PTX code\n         - shared memory allocation size\n     '''\n-    if compute_capability is None:\n-        device = torch.cuda.current_device()\n-        compute_capability = torch.cuda.get_device_capability(device)\n-        compute_capability = compute_capability[0] * 10 + compute_capability[1]\n     if ptx_version is None:\n         _, cuda_version = path_to_ptxas()\n         ptx_version = ptx_get_version(cuda_version)\n     return _triton.translate_llvmir_to_ptx(mod, compute_capability, ptx_version)\n \n \n-def ptx_to_cubin(ptx: str, device: int):\n+def ptx_to_cubin(ptx: str, compute_capability: int):\n     '''\n     Compile TritonGPU module to cubin.\n     :param ptx: ptx code\n-    :param device: CUDA device\n+    :param compute_capability: compute capability\n     :return: str\n     '''\n     ptxas, _ = path_to_ptxas()\n-    compute_capability = torch.cuda.get_device_capability(device)\n-    compute_capability = compute_capability[0] * 10 + compute_capability[1]\n     return _triton.compile_ptx_to_cubin(ptx, ptxas, compute_capability)\n \n \n@@ -1190,7 +1184,7 @@ def put(self, data, filename, binary=True):\n             return\n         binary = isinstance(data, bytes)\n         if not binary:\n-          data = str(data)\n+            data = str(data)\n         assert self.lock_path is not None\n         filepath = self._make_path(filename)\n         with FileLock(self.lock_path):\n@@ -1292,18 +1286,20 @@ def read_or_execute(cache_manager, force_compile, file_name, metadata,\n                     run_if_not_found: Callable = None):\n     suffix = file_name.split(\".\")[1]\n     if not force_compile and cache_manager.has_file(file_name):\n-      module = run_if_found(cache_manager._make_path(file_name))\n-      data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n-      md5 = hashlib.md5(data).hexdigest()\n-      has_changed = metadata and md5 != metadata[\"md5\"][suffix]\n-      return module, md5, has_changed, True\n+        module = run_if_found(cache_manager._make_path(file_name))\n+        data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n+        md5 = hashlib.md5(data).hexdigest()\n+        has_changed = metadata and md5 != metadata[\"md5\"][suffix]\n+        return module, md5, has_changed, True\n     module = run_if_not_found()\n     data = module if isinstance(module, bytes) else str(module).encode(\"utf-8\")\n     md5 = hashlib.md5(data).hexdigest()\n     cache_manager.put(data, file_name, True if isinstance(data, bytes) else data)\n     return module, md5, True, False\n \n-# \n+#\n+\n+\n def make_stub(name, signature, constants):\n     # name of files that are cached\n     so_cache_key = make_so_cache_key(signature, constants)\n@@ -1325,9 +1321,10 @@ def make_stub(name, signature, constants):\n def convert_type_repr(x):\n     match = re.search(r'!tt\\.ptr<(.*)>', x)\n     if match is not None:\n-      return '*' + convert_type_repr(match.group(1))\n+        return '*' + convert_type_repr(match.group(1))\n     return x\n \n+\n def make_hash(fn, **kwargs):\n     if isinstance(fn, triton.runtime.JITFunction):\n         configs = kwargs[\"configs\"]\n@@ -1344,14 +1341,13 @@ def make_hash(fn, **kwargs):\n     return hashlib.md5(Path(fn).read_text().encode(\"utf-8\")).hexdigest()\n \n \n-\n # def compile(fn, signature: str, device: int = -1, constants=dict(), num_warps: int = 4, num_stages: int = 3, extern_libs=None, configs=None):\n def compile(fn, **kwargs):\n     # we get the kernel, i.e. the first function generated in the module\n-    # if fn is not a JITFunction, then it \n+    # if fn is not a JITFunction, then it\n     # has to be a path to a file\n     context = _triton.ir.context()\n-    asm, md5 = dict(), dict()\n+    asm = dict()\n     constants = kwargs.get(\"constants\", dict())\n     if isinstance(fn, triton.runtime.JITFunction):\n         configs = kwargs.get(\"configs\", None)\n@@ -1374,64 +1370,66 @@ def compile(fn, **kwargs):\n         param_tys = [convert_type_repr(str(ty)) for ty in function.type.param_types()]\n         signature = {k: v for k, v in enumerate(param_tys)}\n         first_stage = 2\n-        \n+\n     # cache manager\n     so_path = make_stub(name, signature, constants)\n     # create cache manager\n     fn_cache_manager = CacheManager(make_hash(fn, **kwargs))\n     # determine name and extension type of provided function\n     if isinstance(fn, triton.runtime.JITFunction):\n-      name, ext = fn.__name__, \"ast\"\n+        name, ext = fn.__name__, \"ast\"\n     else:\n-      name, ext = os.path.basename(fn).split(\".\")\n+        name, ext = os.path.basename(fn).split(\".\")\n     # initialize compilation params\n     num_warps = kwargs.get(\"num_warps\", 4)\n     num_stages = kwargs.get(\"num_stages\", 3)\n     extern_libs = kwargs.get(\"extern_libs\", dict())\n     device = kwargs.get(\"device\", torch.cuda.current_device())\n+    compute_capability = torch.cuda.get_device_capability(device)\n+    compute_capability = compute_capability[0] * 10 + compute_capability[1]\n     # load metadata if any\n     metadata = None\n     if fn_cache_manager.has_file(f'{name}.json'):\n-      with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n+        with open(fn_cache_manager._make_path(f\"{name}.json\")) as f:\n             metadata = json.load(f)\n     else:\n-      metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n+        metadata = {\"num_warps\": num_warps, \"num_stages\": num_stages, \"ctime\": dict()}\n     # build compilation stages\n     stages = {\n-      \"ast\" : (lambda path: fn, None),\n-      \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n-               lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n-      \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context), \n-                lambda src: ttir_to_ttgir(src, num_warps, num_stages)),\n-      \"llir\": (lambda path: Path(path).read_bytes(), \n-              lambda src: ttgir_to_llir(src, extern_libs)),\n-      \"ptx\":  (lambda path: Path(path).read_text(), \n-              llir_to_ptx),\n-      \"cubin\": (lambda path: Path(path).read_bytes(), \n-               lambda src: ptx_to_cubin(src, device))\n+        \"ast\": (lambda path: fn, None),\n+        \"ttir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n+                 lambda src: ast_to_ttir(src, signature, configs[0], constants)),\n+        \"ttgir\": (lambda path: _triton.ir.parse_mlir_module(path, context),\n+                  lambda src: ttir_to_ttgir(src, num_warps, num_stages, compute_capability)),\n+        \"llir\": (lambda path: Path(path).read_bytes(),\n+                 lambda src: ttgir_to_llir(src, extern_libs, compute_capability)),\n+        \"ptx\": (lambda path: Path(path).read_text(),\n+                lambda src: llir_to_ptx(src, compute_capability)),\n+        \"cubin\": (lambda path: Path(path).read_bytes(),\n+                  lambda src: ptx_to_cubin(src, compute_capability))\n     }\n     first_stage = list(stages.keys()).index(ext)\n     asm = dict()\n     module = fn\n     # run compilation pipeline  and populate metadata\n     for ir, (parse, compile) in list(stages.items())[first_stage:]:\n-      path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n-      if ir == ext:\n-        next_module = parse(fn)\n-      elif os.path.exists(path) and\\\n-           os.path.getctime(path) == metadata[\"ctime\"][ir]:\n-        next_module = parse(path)\n-      else:\n-        next_module = compile(module)\n-        fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n-      if os.path.exists(path):\n-        metadata[\"ctime\"][ir] = os.path.getctime(path)\n-      asm[ir] = next_module if ir == \"cubin\" else str(next_module)\n-      if ir == \"llir\" and \"shared\" not in metadata:\n-        metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n-      if ir == \"ptx\":\n-        metadata[\"name\"] = ptx_get_kernel_name(next_module)\n-      module = next_module\n+        path = fn_cache_manager._make_path(f\"{name}.{ir}\")\n+        if ir == ext:\n+            next_module = parse(fn)\n+        elif os.path.exists(path) and\\\n+                os.path.getctime(path) == metadata[\"ctime\"][ir]:\n+            next_module = parse(path)\n+        else:\n+            next_module = compile(module)\n+            fn_cache_manager.put(next_module, f\"{name}.{ir}\")\n+        if os.path.exists(path):\n+            metadata[\"ctime\"][ir] = os.path.getctime(path)\n+        asm[ir] = next_module if ir == \"cubin\" else str(next_module)\n+        if ir == \"llir\" and \"shared\" not in metadata:\n+            metadata[\"shared\"] = _triton.get_shared_memory_size(module)\n+        if ir == \"ptx\":\n+            metadata[\"name\"] = ptx_get_kernel_name(next_module)\n+        module = next_module\n     # write-back metadata\n     fn_cache_manager.put(json.dumps(metadata), f\"{name}.json\", binary=False)\n     # return handle to compiled kernel\n@@ -1494,7 +1492,7 @@ def _generate_src(self):\n         #include <cuda.h>\n \n         #include \\\"cuda.h\\\"\n-        #define PY_SSIZE_T_CLEAN \n+        #define PY_SSIZE_T_CLEAN\n         #include <Python.h>\n \n         static inline void gpuAssert(CUresult code, const char *file, int line)"}, {"filename": "python/triton/tools/aot.py", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "file_content_changes": "@@ -37,20 +37,21 @@\n         print(module.str())\n         exit(0)\n \n+    if not args.sm:\n+        raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n+\n     # triton-ir -> triton-gpu-ir\n-    module = triton.compiler.ttir_to_ttgir(module, num_warps=4, num_stages=3)\n+    module = triton.compiler.ttir_to_ttgir(module, num_warps=4, num_stages=3, compute_capability=args.sm)\n     if args.target == 'triton-gpu-ir':\n         print(module.str())\n         exit(0)\n \n     # triton-gpu-ir -> llvm-ir\n-    module = triton.compiler.ttgir_to_llir(module, extern_libs=None)\n+    module = triton.compiler.ttgir_to_llir(module, extern_libs=None, compute_capability=args.sm)\n     if args.target == 'llvm-ir':\n         print(module)\n         exit(0)\n \n-    if not args.sm:\n-        raise argparse.ArgumentError(None, \"Must specify --sm for PTX compilation\")\n     if not args.ptx_version:\n         raise argparse.ArgumentError(None, \"Must specify --ptx-version for PTX compilation\")\n "}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "file_content_changes": "@@ -577,7 +577,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_layout_blocked_blocked\n   func @convert_layout_blocked_blocked(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n-    // CHECK: nvvm.barrier0\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<1xf32>, 3>\n     // CHECK: llvm.store\n@@ -625,7 +624,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_layout_blocked_blocked_vec\n   func @convert_layout_blocked_blocked_vec(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n-    // CHECK: nvvm.barrier0\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     // CHECK: llvm.store\n@@ -649,7 +647,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_layout_blocked_blocked_multi_rep\n   func @convert_layout_blocked_blocked_multi_rep(%arg0: tensor<16x16xf32, #blocked0>) {\n     // CHECK: llvm.mlir.addressof @global_smem\n-    // CHECK: nvvm.barrier0\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n     // CHECK: nvvm.barrier0\n@@ -717,7 +714,6 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mma_block\n   func @convert_layout_mma_blocked(%arg0: tensor<32x16xf32, #mma>) {\n-    // CHECK: nvvm.barrier0\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store"}, {"filename": "test/Target/tritongpu_to_llvmir.mlir", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir | FileCheck %s\n+// RUN: %PYTHON -m triton.tools.aot %s --target=llvm-ir --sm=80 | FileCheck %s\n \n // == LLVM IR check begin ==\n // CHECK-LABEL: ; ModuleID = 'LLVMDialectModule'"}]