[{"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 1, "deletions": 8, "changes": 9, "file_content_changes": "@@ -11,11 +11,6 @@\n from ..common.build import is_hip\n from . import core as tl\n \n-rocm_module_path = os.path.join(Path(__file__).parents[3], \"third_party\", \"amd_hip_backend\", \"python\", \"rocm.py\")\n-spec = importlib.util.spec_from_file_location(\"rocm\", rocm_module_path)\n-rocm = importlib.util.module_from_spec(spec)\n-spec.loader.exec_module(rocm)\n-\n T = TypeVar('T')\n \n # Create custom exception that prints message \"hello\"\n@@ -1265,9 +1260,7 @@ def atomic_xchg(ptr: tl.tensor,\n def gpu_has_mfma() -> bool:\n     if not is_hip():\n         return False\n-    arch_info = rocm.get_amdgpu_arch_fulldetails()\n-    gfx_arch = arch_info[1]\n-    return gfx_arch in ['gfx908', 'gfx90a']\n+    return True # mfma supported in ['gfx908', 'gfx90a']\n \n \n def mfma_supported(M, N, K, allow_tf32, ret_scalar_ty) -> bool:"}]