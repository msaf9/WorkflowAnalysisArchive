[{"filename": "python/triton/code_gen.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -956,7 +956,7 @@ def add_to_cache(self, key, wargs, device_idx, num_warps, num_stages):\n         return self.fn._warmup(key, arg_types=arg_types, device=device_idx, attributes=attributes, constants=constants, num_warps=num_warps, num_stages=num_stages, is_manual_warmup=False)\n \n     def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n-        assert num_warps != 0 and (num_warps & (num_warps - 1)) == 0, f\"{num_warps=} must be a power of 2.\"\n+        assert num_warps != 0 and (num_warps & (num_warps - 1)) == 0, f\"num_warps={num_warps} must be a power of 2.\"\n         # handle arguments passed by name\n         kwargs = {self.fn.arg_names.index(name): value for name, value in kwargs.items()}\n         wargs = list(wargs)"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 26, "deletions": 22, "changes": 48, "file_content_changes": "@@ -128,17 +128,19 @@ def _layer_norm_bwd_dwdb(\n     cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n     db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n-    for i in range(0, M, BLOCK_SIZE_M):\n-        rows = i + tl.arange(0, BLOCK_SIZE_M)\n-        mask = (rows[:, None] < M) & (cols[None, :] < N)\n-        offs = rows[:, None] * N + cols[None, :]\n-        a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n-        dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n-        mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n-        rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n-        a_hat = (a - mean[:, None]) * rstd[:, None]\n-        dw += dout * a_hat\n-        db += dout\n+    UNROLL: tl.constexpr = 4\n+    for i in range(0, M, BLOCK_SIZE_M * UNROLL):\n+        for j in range(UNROLL):\n+            rows = i + j * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n+            mask = (rows[:, None] < M) & (cols[None, :] < N)\n+            offs = rows[:, None] * N + cols[None, :]\n+            a = tl.load(A + offs, mask=mask, other=0.).to(tl.float32)\n+            dout = tl.load(DOut + offs, mask=mask, other=0.).to(tl.float32)\n+            mean = tl.load(Mean + rows, mask=rows < M, other=0.)\n+            rstd = tl.load(Var + rows, mask=rows < M, other=0.)\n+            a_hat = (a - mean[:, None]) * rstd[:, None]\n+            dw += dout * a_hat\n+            db += dout\n     sum_dw = tl.sum(dw, axis=0)\n     sum_db = tl.sum(db, axis=0)\n     tl.store(DW + cols, sum_dw, mask=cols < N)\n@@ -211,7 +213,15 @@ def backward(ctx, dout):\n             BLOCK_SIZE_N=ctx.BLOCK_SIZE,\n             num_warps=ctx.num_warps,\n         )\n-        # accumulate partial sums in separate kernel\n+        if N > 10240:\n+            BLOCK_SIZE_N = 128\n+            BLOCK_SIZE_M = 32\n+            num_warps = 4\n+        else:\n+            # maximize occupancy for small N\n+            BLOCK_SIZE_N = 16\n+            BLOCK_SIZE_M = 16\n+            num_warps = 8\n         grid = lambda meta: [triton.cdiv(N, meta[\"BLOCK_SIZE_N\"])]\n         _layer_norm_bwd_dwdb[grid](\n             a, dout,\n@@ -220,17 +230,11 @@ def backward(ctx, dout):\n             dbias,\n             M,\n             N,\n-            BLOCK_SIZE_M=32,\n-            BLOCK_SIZE_N=128,\n+            BLOCK_SIZE_M=BLOCK_SIZE_M,\n+            BLOCK_SIZE_N=BLOCK_SIZE_N,\n+            num_warps=num_warps\n         )\n-        return (da, None, dweight, dbias, None, None,\n-                None, None, None, None,\n-                None,\n-                None, None, None,\n-                None,\n-                None, None, None,\n-                None, None, None,\n-                None, None, None)\n+        return (da, None, dweight, dbias, None)\n \n \n def layer_norm(a, normalized_shape, weight, bias, eps):"}]