[{"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "file_content_changes": "@@ -85,12 +85,13 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n         # mixed-precision\n         *[\n             [\n-                (16, 16, 16, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (64, 64, 64, 1, 2, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n-                (32, 64, 16, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n+                (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n+                (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n                 (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n             ] for ADTYPE, BDTYPE in [(\"float8\", \"float8\"),\n+                                     (\"float8\", \"float16\"),\n+                                     (\"float16\", \"float8\"),\n                                      (\"float16\", \"float32\"),\n                                      (\"float32\", \"float16\"),\n                                      (\"bfloat16\", \"float32\"),\n@@ -138,9 +139,9 @@ def init_input(n, m, t, dtype):\n     b = init_input(K, N, BT, BDTYPE)\n     # run test\n     th_a = maybe_upcast(a, ADTYPE).to(torch.float32)\n-    if AT and ADTYPE == \"float8\": th_a = th_a.T\n+    if AT and ADTYPE == \"float8\": th_a = th_a.view(th_a.shape[::-1]).T\n     th_b = maybe_upcast(b, BDTYPE).to(torch.float32)\n-    if BT and BDTYPE == \"float8\": th_b = th_b.T\n+    if BT and BDTYPE == \"float8\": th_b = th_b.view(th_b.shape[::-1]).T\n     th_c = torch.matmul(th_a, th_b)\n     try:\n         if ADTYPE == \"float8\":"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 0, "deletions": 11, "changes": 11, "file_content_changes": "@@ -1,6 +1,5 @@\n from __future__ import annotations  # remove after python 3.11\n \n-import warnings\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n@@ -677,16 +676,6 @@ def cast(input: tl.tensor,\n     src_sca_ty = src_ty.scalar\n     dst_sca_ty = dst_ty.scalar\n \n-    if builder.arch < 89 and \\\n-       (src_sca_ty.is_fp8e4() or dst_sca_ty.is_fp8e4()):\n-        warnings.warn(\"Standard tl.float8e4 format will be deprecated on SM < 89. \"\n-                      \"Please use tl.float8e4b15.\", DeprecationWarning)\n-\n-    # Unsupported conversion:\n-    if (src_sca_ty.is_fp8e4b15() and not dst_sca_ty.is_fp16()) or \\\n-       (dst_sca_ty.is_fp8e4b15() and not src_sca_ty.is_fp16()):\n-        raise ValueError('fp8e4b15 can only be converted to/from fp16')\n-\n     # Casting with customized floating types involved: fp8 <=> bf16, fp16, fp32, fp64\n     if (src_sca_ty.is_fp8() and dst_sca_ty.is_floating()) or \\\n        (src_sca_ty.is_floating() and dst_sca_ty.is_fp8()):"}]