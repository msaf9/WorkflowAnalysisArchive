[{"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -30,7 +30,7 @@ jobs:\n           #export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n           export CIBW_BEFORE_BUILD=\"pip install cmake;\"\n           export CIBW_SKIP=\"{cp,pp}35-*\"\n-          export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n+          export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64 cp3*-musllinux_x86_64\"\n           python3 -m cibuildwheel python --output-dir wheelhouse\n \n "}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -249,6 +249,7 @@ def build_extension(self, ext):\n         \"triton/_C\",\n         \"triton/common\",\n         \"triton/compiler\",\n+        \"triton/debugger\",\n         \"triton/language\",\n         \"triton/language/extra\",\n         \"triton/ops\","}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "@@ -262,6 +262,11 @@ void init_triton_ir(py::module &&m) {\n              return !self.empty() &&\n                     self.back().hasTrait<mlir::OpTrait::IsTerminator>();\n            })\n+      .def(\"has_return\",\n+           [](mlir::Block &self) {\n+             return !self.empty() &&\n+                    self.back().hasTrait<mlir::OpTrait::ReturnLike>();\n+           })\n       .def(\"erase\", [](mlir::Block &self) { self.erase(); });\n \n   // using eattr = ir::attribute_kind_t;\n@@ -428,6 +433,25 @@ void init_triton_ir(py::module &&m) {\n             self.setArgAttr(arg_no, name, mlir::IntegerAttr::get(attrTy, val));\n           },\n           ret::reference)\n+      .def(\"finalize\",\n+           [](mlir::triton::FuncOp &self) -> void {\n+             // Remove dead code\n+             // 1. Unreachable code after return\n+             self.walk([&](mlir::Block *block) {\n+               mlir::Operation *retOp = nullptr;\n+               block->walk([&](mlir::Operation *op) {\n+                 if (mlir::isa<mlir::triton::ReturnOp>(op))\n+                   if (retOp == nullptr)\n+                     retOp = op;\n+               });\n+               if (retOp && retOp != &block->back()) {\n+                 auto pos = retOp->getIterator();\n+                 pos++;\n+                 auto *newBlock = block->splitBlock(pos);\n+                 newBlock->erase();\n+               }\n+             });\n+           })\n       .def_property_readonly(\"type\", &mlir::triton::FuncOp::getFunctionType)\n       .def(\"reset_type\", &mlir::triton::FuncOp::setType);\n "}, {"filename": "python/test/unit/debugger/test_debugger.py", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "file_content_changes": "@@ -0,0 +1,69 @@\n+import random\n+\n+import torch\n+\n+import triton\n+import triton.language as tl\n+from triton.debugger.debugger import program_ids_from_grid\n+\n+\n+def test_addition():\n+\n+    @triton.jit(interpret=True)\n+    def add_kernel(\n+        x_ptr,\n+        y_ptr,\n+        output_ptr,\n+        n_elements,\n+        BLOCK_SIZE: tl.constexpr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        block_start = pid * BLOCK_SIZE\n+        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        x = tl.load(x_ptr + offsets, mask=mask)\n+        y = tl.load(y_ptr + offsets, mask=mask)\n+        output = x + y\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+    a = torch.rand((128,), device=\"cuda\")\n+    b = torch.rand((128,), device=\"cuda\")\n+    expected = a + b\n+    output = torch.empty((128,), device=\"cuda\")\n+\n+    def grid(meta):\n+        return (triton.cdiv(128, meta[\"BLOCK_SIZE\"]),)\n+\n+    add_kernel[grid](a, b, output, 128, BLOCK_SIZE=32)\n+\n+    assert torch.allclose(expected, output, atol=1e-2, rtol=0)\n+\n+\n+def test_program_ids_from_grid():\n+    random.seed(123)\n+    grid = (3, 4)\n+    expected_combinations = 3 * 4\n+    unique_combinations = set(program_ids_from_grid(grid))\n+    assert len(unique_combinations) == expected_combinations\n+\n+    first_run = list(program_ids_from_grid(grid))\n+    second_run = list(program_ids_from_grid(grid))\n+    assert first_run != second_run\n+\n+\n+def test_atomic():\n+    @triton.jit(interpret=True)\n+    def atomic(\n+        x_ptr,\n+    ):\n+        pid = tl.program_id(axis=0)\n+        tl.atomic_add(x_ptr + pid, 1)\n+        t = tl.atomic_xchg(x_ptr + pid, 3)\n+        t += 1  # 2\n+        tl.atomic_cas(x_ptr + pid, 3, t)  # match\n+        tl.atomic_cas(x_ptr + pid, 40, 9)  # no match\n+    nb_dim = 16\n+    a = torch.zeros((nb_dim, ), dtype=torch.int32, device=\"cuda\")\n+\n+    atomic[(nb_dim, )](a)\n+    assert torch.allclose(a, torch.full_like(a, 2))"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 126, "deletions": 72, "changes": 198, "file_content_changes": "@@ -1605,68 +1605,68 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n     )\n \n \n-layouts = [\n-    BlockedLayout([1, 4], [1, 32], [4, 1], [1, 0]),\n-    BlockedLayout([1, 4], [1, 32], [2, 2], [1, 0]),\n-    BlockedLayout([1, 4], [1, 32], [1, 4], [1, 0]),\n-    BlockedLayout([1, 4], [8, 4], [2, 2], [0, 1])\n-]\n-\n-\n-@pytest.mark.parametrize(\"M, N\", [[32, 128], [128, 128], [128, 32]])\n-@pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_reduce_2d(M, N, src_layout, device='cuda'):\n-    ir = f\"\"\"\n-    #src = {src_layout}\n-    module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n-    tt.func public @sum_kernel_0d1d(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n-        %cst = arith.constant dense<{M}> : tensor<{M}x1xi32, #src>\n-        %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-        %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n-        %2 = arith.muli %1, %cst : tensor<{M}x1xi32, #src>\n-        %3 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>\n-        %4 = tt.expand_dims %3 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n-        %5 = tt.broadcast %2 : (tensor<{M}x1xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n-        %6 = tt.broadcast %4 : (tensor<1x{N}xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n-        %7 = arith.addi %5, %6 : tensor<{M}x{N}xi32, #src>\n-        %8 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x{N}x!tt.ptr<i32>, #src>\n-        %9 = tt.addptr %8, %7 : tensor<{M}x{N}x!tt.ptr<i32>, #src>, tensor<{M}x{N}xi32, #src>\n-        %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #src>\n-        %11 = \"tt.reduce\"(%10) ({{\n-        ^bb0(%arg2: i32, %arg3: i32):\n-        %13 = arith.addi %arg2, %arg3 : i32\n-        tt.reduce.return %13 : i32\n-        }}) {{axis = 1 : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n-        %12 = \"tt.reduce\"(%11) ({{\n-        ^bb0(%arg2: i32, %arg3: i32):\n-        %13 = arith.addi %arg2, %arg3 : i32\n-        tt.reduce.return %13 : i32\n-        }}) {{axis = 0 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> i32\n-        tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n-        tt.return\n-    }}\n-    }}\n-    \"\"\"\n-    import tempfile\n-    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n-        f.write(ir)\n-        f.flush()\n-        kernel = triton.compile(f.name)\n-\n-    rs = RandomState(17)\n-    x = rs.randint(0, 4, (M, N)).astype('int32')\n-    x = (x.view('uint32') & np.uint32(0xffffe000)).view('int32')\n-\n-    z = np.zeros((1,)).astype('int32')\n-\n-    x_tri = torch.tensor(x, device=device)\n-    z_tri = torch.tensor(z, device=device)\n-\n-    pgm = kernel[(1, 1, 1)](x_tri, z_tri)\n-\n-    z_ref = np.sum(x)\n-\n-    np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n+# layouts = [\n+#     BlockedLayout([1, 4], [1, 32], [4, 1], [1, 0]),\n+#     BlockedLayout([1, 4], [1, 32], [2, 2], [1, 0]),\n+#     BlockedLayout([1, 4], [1, 32], [1, 4], [1, 0]),\n+#     BlockedLayout([1, 4], [8, 4], [2, 2], [0, 1])\n+# ]\n+\n+\n+# @pytest.mark.parametrize(\"M, N\", [[32, 128], [128, 128], [128, 32]])\n+# @pytest.mark.parametrize(\"src_layout\", layouts)\n+# def test_reduce_2d(M, N, src_layout, device='cuda'):\n+#     ir = f\"\"\"\n+#     #src = {src_layout}\n+#     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n+#     tt.func public @sum_kernel_0d1d(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n+#         %cst = arith.constant dense<{M}> : tensor<{M}x1xi32, #src>\n+#         %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+#         %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> tensor<{M}x1xi32, #src>\n+#         %2 = arith.muli %1, %cst : tensor<{M}x1xi32, #src>\n+#         %3 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>\n+#         %4 = tt.expand_dims %3 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #src}}>>) -> tensor<1x{N}xi32, #src>\n+#         %5 = tt.broadcast %2 : (tensor<{M}x1xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n+#         %6 = tt.broadcast %4 : (tensor<1x{N}xi32, #src>) -> tensor<{M}x{N}xi32, #src>\n+#         %7 = arith.addi %5, %6 : tensor<{M}x{N}xi32, #src>\n+#         %8 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x{N}x!tt.ptr<i32>, #src>\n+#         %9 = tt.addptr %8, %7 : tensor<{M}x{N}x!tt.ptr<i32>, #src>, tensor<{M}x{N}xi32, #src>\n+#         %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #src>\n+#         %11 = \"tt.reduce\"(%10) ({{\n+#         ^bb0(%arg2: i32, %arg3: i32):\n+#         %13 = arith.addi %arg2, %arg3 : i32\n+#         tt.reduce.return %13 : i32\n+#         }}) {{axis = 1 : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+#         %12 = \"tt.reduce\"(%11) ({{\n+#         ^bb0(%arg2: i32, %arg3: i32):\n+#         %13 = arith.addi %arg2, %arg3 : i32\n+#         tt.reduce.return %13 : i32\n+#         }}) {{axis = 0 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> i32\n+#         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n+#         tt.return\n+#     }}\n+#     }}\n+#     \"\"\"\n+#     import tempfile\n+#     with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+#         f.write(ir)\n+#         f.flush()\n+#         kernel = triton.compile(f.name)\n+#\n+#     rs = RandomState(17)\n+#     x = rs.randint(0, 4, (M, N)).astype('int32')\n+#     x = (x.view('uint32') & np.uint32(0xffffe000)).view('int32')\n+#\n+#     z = np.zeros((1,)).astype('int32')\n+#\n+#     x_tri = torch.tensor(x, device=device)\n+#     z_tri = torch.tensor(z, device=device)\n+#\n+#     pgm = kernel[(1, 1, 1)](x_tri, z_tri)\n+#\n+#     z_ref = np.sum(x)\n+#\n+#     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n def test_generic_reduction(device='cuda'):\n@@ -2550,24 +2550,30 @@ def kernel(Cond, TrueVal, FalseVal, Out):\n     assert to_numpy(out)[0] == false_val[0]\n \n \n-def test_if_return():\n+@pytest.mark.parametrize(\"mode\", [\"dynamic\", \"static\"])\n+def test_if_return(mode):\n \n     @triton.jit\n-    def kernel(ExitEarly, Out):\n-        if tl.load(ExitEarly):\n-            tl.store(Out, 0)\n-            return\n+    def kernel(ExitEarly, Out, cond: tl.constexpr, mode: tl.constexpr):\n+        if mode == \"dynamic\":\n+            if tl.load(ExitEarly):\n+                tl.store(Out, 0)\n+                return\n+        else:\n+            if cond:\n+                tl.store(Out, 0)\n+                return\n         tl.store(Out, 1)\n \n     out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n     exit_early = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n     # exit early path taken\n     exit_early[0] = 1\n-    kernel[(1,)](exit_early, out)\n+    kernel[(1,)](exit_early, out, True, mode)\n     assert to_numpy(out)[0] == 0\n     # exit early path not taken\n     exit_early[0] = 0\n-    kernel[(1,)](exit_early, out)\n+    kernel[(1,)](exit_early, out, False, mode)\n     assert to_numpy(out)[0] == 1\n \n \n@@ -2576,21 +2582,69 @@ def add_fn(x):\n     return x + 1\n \n \n-@pytest.mark.parametrize(\"call_type\", [\"attribute\", \"jit_function\"])\n+@triton.jit(noinline=True)\n+def add_fn_noinline(x):\n+    return x + 1\n+\n+\n+@triton.jit\n+def add_fn_return(x, pid):\n+    if pid == 0:\n+        return x + 1\n+    else:\n+        return x + 2\n+\n+\n+@triton.jit\n+def add_fn_expr(Out, x):\n+    tl.store(Out, x)\n+\n+\n+@triton.jit\n+def add_fn_static_cond(x, cond: tl.constexpr):\n+    if cond == \"\":\n+        return x\n+    else:\n+        return x + 1\n+\n+\n+@pytest.mark.parametrize(\"call_type\", [\"attribute\", \"jit_function\", \"jit_function_return\",\n+                                       \"ifexp\", \"expr\", \"jit_function_static_cond\", \"jit_function_noinline\"])\n def test_if_call(call_type):\n     @triton.jit\n     def kernel(Out, call_type: tl.constexpr):\n         pid = tl.program_id(0)\n         o = tl.load(Out)\n         if pid == 0:\n             if call_type == \"attribute\":\n+                # call attribute\n                 a = o + 1\n-                a = a.to(tl.int32)\n+                a = a.to(tl.int32).to(tl.int32)\n                 o = a\n             else:\n                 a = o\n-                a = add_fn(a)\n+                if call_type == \"jit_function\":\n+                    # regular function call\n+                    a = add_fn(a)\n+                elif call_type == \"jit_function_return\":\n+                    # function without end_if block\n+                    a = add_fn_return(a, pid)\n+                elif call_type == \"ifexp\":\n+                    # ifexp expression\n+                    a = add_fn(a) if pid == 0 else add_fn_return(a, pid)\n+                elif call_type == \"expr\":\n+                    if pid == 1:\n+                        return\n+                    a = add_fn(a)\n+                    if pid == 0:\n+                        # call without return\n+                        add_fn_expr(Out, a)\n+                elif call_type == \"jit_function_static_cond\":\n+                    a = add_fn_static_cond(a, call_type)\n+                elif call_type == \"jit_function_noinline\":\n+                    a = add_fn_noinline(a)\n                 o = a\n+\n         tl.store(Out, o)\n \n     out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')"}, {"filename": "python/triton/__init__.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -18,6 +18,8 @@\n )\n from .runtime.jit import jit\n from .compiler import compile, CompilationError\n+from .debugger.debugger import program_ids_from_grid\n+\n from . import language\n from . import testing\n \n@@ -41,6 +43,7 @@\n     \"runtime\",\n     \"TensorWrapper\",\n     \"testing\",\n+    \"program_ids_from_grid\",\n ]\n \n "}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 52, "deletions": 19, "changes": 71, "file_content_changes": "@@ -104,6 +104,7 @@ def __init__(self, context, prototype, gscope, attributes, constants, function_n\n         self.debug = debug\n         self.noinline = noinline\n         self.scf_stack = []\n+        self.last_ret_type = None\n         # SSA-construction\n         # name => language.tensor\n         self.local_defs: Dict[str, tensor] = {}\n@@ -138,7 +139,7 @@ def name_lookup(name: str) -> Any:\n     def set_value(self, name: str,\n                   value: Union[tensor, constexpr]) -> None:\n         ''' This function:\n-          called by visit_Assign() & visit_FuncDef() to store left value (lvalue)\n+            called by visit_Assign() & visit_FunctionDef() to store left value (lvalue)\n         1. record local defined name (FIXME: should consider control flow)\n         2. store tensor in self.lvalue\n         '''\n@@ -150,10 +151,9 @@ def set_value(self, name: str,\n     #\n     def visit_compound_statement(self, stmts):\n         for stmt in stmts:\n-            self.last_ret_type = self.visit(stmt)\n-            if isinstance(stmt, ast.Return):\n-                break\n-        return stmts and isinstance(stmt, ast.Return)\n+            ret_type = self.visit(stmt)\n+            if ret_type is not None and isinstance(stmt, ast.Return):\n+                self.last_ret_type = ret_type\n \n     # TODO: should be its own AST visitor\n     def contains_return_op(self, node):\n@@ -168,10 +168,23 @@ def contains_return_op(self, node):\n             pred = lambda s: self.contains_return_op(s)\n             return any(pred(s) for s in node.body)\n         elif isinstance(node, ast.Call):\n-            if isinstance(node.func, ast.Attribute):\n+            def check_undefined_name(cur_node):\n+                # Check if name is an undefined local variable,\n+                # which can only be a tensor or a constexpr\n+                if isinstance(cur_node.func, ast.Attribute):\n+                    if isinstance(cur_node.func.value, ast.Name):\n+                        name = cur_node.func.value.id\n+                        if name not in self.lscope and name not in self.gscope:\n+                            return True\n+                        return False\n+                    # chain of calls\n+                    # e.g., tl.load(a).to(tl.float32)\n+                    return check_undefined_name(cur_node.func.value)\n+                return False\n+            if check_undefined_name(node):\n                 return False\n             fn = self.visit(node.func)\n-            if isinstance(fn, JITFunction):\n+            if isinstance(fn, JITFunction) and fn.noinline is False:\n                 old_gscope = self.gscope\n                 self.gscope = sys.modules[fn.fn.__module__].__dict__\n                 ret = self.contains_return_op(fn.parse())\n@@ -184,6 +197,18 @@ def contains_return_op(self, node):\n             if node.orelse:\n                 ret = ret or any(pred(s) for s in node.orelse)\n             return ret\n+        elif isinstance(node, ast.IfExp):\n+            return self.contains_return_op(node.body) or self.contains_return_op(node.orelse)\n+        elif isinstance(node, ast.Expr):\n+            ret = False\n+            for _, value in ast.iter_fields(node):\n+                if isinstance(value, list):\n+                    for item in value:\n+                        if isinstance(item, ast.AST):\n+                            ret = ret or self.contains_return_op(item)\n+                elif isinstance(value, ast.AST):\n+                    ret = ret or self.contains_return_op(value)\n+            return ret\n         else:\n             return False\n \n@@ -257,9 +282,9 @@ def visit_FunctionDef(self, node):\n             self.set_value(arg_name, arg_value)\n         self.builder.set_insertion_point_to_start(entry)\n         # visit function body\n-        has_ret = self.visit_compound_statement(node.body)\n+        self.visit_compound_statement(node.body)\n         # finalize function\n-        if not has_ret:\n+        if self.last_ret_type is None:\n             self.builder.ret([])\n         else:\n             # update return type\n@@ -271,6 +296,8 @@ def visit_FunctionDef(self, node):\n                 fn.reset_type(self.prototype.to_ir(self.builder))\n         if insert_pt:\n             self.builder.set_insertion_point_to_end(insert_pt)\n+        # Remove dead code\n+        fn.finalize()\n \n     def visit_arguments(self, node):\n         arg_names = []\n@@ -421,6 +448,7 @@ def visit_then_else_blocks(self, node, liveins, then_block, else_block):\n         return then_defs, else_defs, then_block, else_block, names, ret_types, ir_ret_types\n \n     def visit_if_top_level(self, cond, node):\n+        has_endif_block = True\n         with enter_sub_region(self) as sr:\n             liveins, ip_block = sr\n             then_block = self.builder.create_block()\n@@ -435,20 +463,25 @@ def visit_if_top_level(self, cond, node):\n                 self.visit_then_else_blocks(node, liveins, then_block, else_block)\n             # then terminator\n             self.builder.set_insertion_point_to_end(then_block)\n-            if not then_block.has_terminator():\n+            if then_block.has_return() and else_block.has_return():\n+                has_endif_block = False\n+                endif_block.erase()\n+            if not then_block.has_terminator() and has_endif_block:\n                 self.builder.create_branch(endif_block, [then_defs[n].handle for n in names])\n             # else terminator\n             self.builder.set_insertion_point_to_end(else_block)\n-            if not else_block.has_terminator():\n+            if not else_block.has_terminator() and has_endif_block:\n                 self.builder.create_branch(endif_block, [else_defs[n].handle for n in names])\n-            for ty in ir_ret_types:\n-                endif_block.add_argument(ty)\n-        # change block\n-        self.builder.set_insertion_point_to_start(endif_block)\n-        # update value\n-        for i, name in enumerate(names):\n-            new_tensor = language.core.tensor(endif_block.arg(i), ret_types[i])\n-            self.set_value(name, new_tensor)\n+            if has_endif_block:\n+                for ty in ir_ret_types:\n+                    endif_block.add_argument(ty)\n+        if has_endif_block:\n+            # change block\n+            self.builder.set_insertion_point_to_start(endif_block)\n+            # update value\n+            for i, name in enumerate(names):\n+                new_tensor = language.core.tensor(endif_block.arg(i), ret_types[i])\n+                self.set_value(name, new_tensor)\n \n     # TODO: refactor\n     def visit_if_scf(self, cond, node):"}, {"filename": "python/triton/debugger/__init__.py", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "python/triton/debugger/core.py", "status": "added", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -0,0 +1,9 @@\n+from typing import Tuple\n+\n+import dataclasses\n+\n+\n+@dataclasses.dataclass\n+class ExecutionContext:\n+    program_id: Tuple[int]\n+    program_size: Tuple[int]"}, {"filename": "python/triton/debugger/debugger.py", "status": "added", "additions": 170, "deletions": 0, "changes": 170, "file_content_changes": "@@ -0,0 +1,170 @@\n+import itertools\n+import random\n+from typing import Tuple\n+\n+import triton\n+import triton.language as tl\n+from .core import ExecutionContext\n+from .memory_map import MemoryMap\n+from .tl_lang import (TritonLangProxy, WrappedTensor, _primitive_to_tensor,\n+                      debugger_constexpr)\n+from triton.debugger import torch_wrapper\n+\n+torch = torch_wrapper.torch\n+tl_method_backup = {}\n+\n+\n+def get_proxy_method(proxy, name):\n+    method = getattr(proxy, name)\n+\n+    def fun(*args, **kwarg):\n+        return method(*args, **kwarg)\n+\n+    return fun\n+\n+\n+def attach_triton(module, proxy):\n+    method_list = [func for func in dir(TritonLangProxy) if func[0] != \"_\"]\n+    for name in method_list:\n+        if hasattr(module, name):\n+            attr = getattr(module, name)\n+            tl_method_backup[name] = attr\n+            if callable(attr):\n+                setattr(module, name, get_proxy_method(proxy, name))\n+            else:\n+                setattr(module, name, getattr(proxy, name))\n+\n+\n+def detach_triton(module):\n+    for name, method in tl_method_backup.items():\n+        setattr(module, name, method)\n+\n+\n+def program_ids_from_grid(grid: Tuple[int, ...]) -> Tuple[int, ...]:\n+    # reverse the grid dimensions and generate the range for each dimension\n+    reversed_grid = reversed(grid)\n+    ranges_for_each_dimension = [range(dim) for dim in reversed_grid]\n+\n+    # gen all combinations\n+    index_combinations = list(itertools.product(*ranges_for_each_dimension))\n+    random.shuffle(index_combinations)\n+\n+    for index_combination in index_combinations:\n+        yield index_combination\n+\n+\n+class DebuggerFunction:\n+    def __init__(self, func, grid=(1,)):\n+        self.func = func\n+        self.grid = grid\n+\n+    def _is_constexpr(self, name):\n+        return name in self.func.__annotations__ and self.func.__annotations__[name] is triton.language.core.constexpr\n+\n+    def _get_constexpr(self):\n+        result = []\n+        for name, annotation in self.func.__annotations__.items():\n+            if annotation is triton.language.core.constexpr:\n+                result.append(name)\n+        return result\n+\n+    def _assert_constexpr(self, **kwargs):\n+        constexp = self._get_constexpr()\n+        missing = [i for i in constexp if i not in kwargs.keys()]\n+        assert len(missing) == 0, f\"You must specify constexpr {missing}\"\n+\n+    def _get_grid(self, **kwargs):\n+        if callable(self.grid):\n+            return self.grid(kwargs)\n+        else:\n+            return self.grid\n+\n+    def __call__(self, *args, **kwargs):\n+        self._assert_constexpr(**kwargs)\n+\n+        memory = MemoryMap()\n+\n+        def convert_arg(v):\n+            name, arg = v\n+            if torch.is_tensor(arg):\n+                ptr = memory.add_tensor(arg)\n+                return WrappedTensor(torch.tensor([ptr], dtype=torch.int64, device=\"cuda\"))\n+            if self._is_constexpr(name):\n+                return debugger_constexpr(arg)\n+            return WrappedTensor(_primitive_to_tensor(arg))\n+\n+        new_args = tuple(map(convert_arg, zip(self.func.__code__.co_varnames, args)))\n+        new_kwargs = {k: convert_arg((k, v)) for (k, v) in kwargs.items() if k not in [\"num_warps\", \"num_stages\"]}\n+\n+        grid = self._get_grid(**kwargs)\n+        for program_id in program_ids_from_grid(grid):\n+            proxy = TritonLangProxy(memory, ExecutionContext(program_id, grid))\n+            attach_triton(tl, proxy)\n+            self.func(*new_args, **new_kwargs)\n+            detach_triton(tl)\n+\n+\n+class GridSelector:\n+    \"\"\"\n+    Entry point of the debugger\n+    \"\"\"\n+\n+    def __init__(self, func):\n+        version = torch.__version__\n+        assert version[0] == \"2\", f\"Triton Debugger only supports torch >= 2.0, using {version}\"\n+        self.func = func\n+\n+    def __getitem__(self, grid):\n+        return DebuggerFunction(self.func, grid)\n+\n+    def __call__(self, *args, **kwargs):\n+        return DebuggerFunction(self.func)(*args, **kwargs)\n+\n+\n+class AutotuneGridSelector:\n+    def __init__(self, func, autotune_params):\n+        self.func = func\n+        self.autotune_params = autotune_params\n+\n+    def __getitem__(self, grid):\n+        return AutotuneRunner(self.func, self.autotune_params, grid)\n+\n+    def __call__(self, *args, **kwargs):\n+        return AutotuneRunner(self.func, self.autotune_params)(*args, **kwargs)\n+\n+\n+class AutotuneRunner:\n+    def __init__(self, func, autotune_params, grid=None):\n+        self.func = func\n+        self.autotune_params = autotune_params\n+        self.grid = grid\n+\n+    def __call__(self, *args, **kwargs):\n+        assert len(self.autotune_params[\"configs\"]) >= 1\n+\n+        for config in self.autotune_params[\"configs\"][1:]:\n+\n+            def convert_arg(v):\n+                if torch.is_tensor(v):\n+                    return torch.clone(v)\n+                return v\n+\n+            new_args = tuple(map(convert_arg, args))\n+            new_kwargs = {k: convert_arg(v) for k, v in kwargs.items()}\n+            if self.grid:\n+                self.func[self.grid](*new_args, **new_kwargs, **config.kwargs)\n+            else:\n+                self.func(*new_args, **new_kwargs, **config.kwargs)\n+\n+        main_config = self.autotune_params[\"configs\"][0]\n+        if self.grid:\n+            self.func[self.grid](*args, **kwargs, **main_config.kwargs)\n+        else:\n+            self.func(*args, **kwargs, **main_config.kwargs)\n+\n+\n+def triton_debug_autotune(**kwars):\n+    def wrapper(func):\n+        return AutotuneGridSelector(func, kwars)\n+\n+    return wrapper"}, {"filename": "python/triton/debugger/memory_map.py", "status": "added", "additions": 100, "deletions": 0, "changes": 100, "file_content_changes": "@@ -0,0 +1,100 @@\n+import dataclasses\n+\n+from triton.debugger import torch_wrapper\n+\n+torch = torch_wrapper.torch\n+\n+\n+@dataclasses.dataclass\n+class RegisteredStorage:\n+    storage: torch.Storage\n+    dtype: torch.dtype\n+    size: int\n+    ptr: int\n+\n+    @property\n+    def end_ptr(self) -> int:\n+        return self.ptr + self.size\n+\n+    @property\n+    def access_tensor(self) -> torch.Tensor:\n+        return torch.tensor(self.storage, dtype=self.dtype, device=self.storage.device)\n+\n+    def ensure_immutable(self):\n+        assert self.storage.data_ptr() == self.ptr and self.storage.size() == self.size\n+\n+\n+class MemoryMap:\n+    storages: [RegisteredStorage]\n+\n+    def __init__(self):\n+        self.storages = []\n+\n+    def _get_registered_storage(self, pointer: torch.Tensor):\n+        max_pointer = torch.max(pointer).item()\n+        min_pointer = torch.min(pointer).item()\n+\n+        registered_storage = next(\n+            filter(\n+                lambda registered: min_pointer >= registered.ptr and max_pointer < registered.end_ptr, self.storages\n+            ),\n+            None,\n+        )\n+        if registered_storage is None:\n+            raise Exception(\"Storage not found or pointers spanning multiple tensors\")\n+        registered_storage.ensure_immutable()\n+        return registered_storage\n+\n+    def add_tensor(self, t: torch.Tensor):\n+        storage = t.untyped_storage()\n+        self.storages.append(RegisteredStorage(storage, t.dtype, storage.size(), storage.data_ptr()))\n+        return t.data_ptr()\n+\n+    def load(\n+        self,\n+        pointer: torch.Tensor,\n+        mask: torch.Tensor = None,\n+        other=0.0,\n+    ):\n+        assert pointer.is_cuda\n+        assert 0 < pointer.dim() < 3\n+        assert pointer.dtype == torch.int64\n+\n+        if mask is None:\n+            mask = torch.ones_like(pointer).bool()\n+        assert mask.is_cuda\n+        assert 0 < mask.dim() < 3\n+        assert mask.dtype == torch.bool\n+        mask = mask.expand(pointer.size())\n+\n+        if torch.all(~mask):\n+            # Todo: The type is wrong here, we can't determine the correct type\n+            return torch.full_like(pointer, fill_value=other, dtype=torch.float16, device=\"cuda\")\n+\n+        registered_storage = self._get_registered_storage(pointer[mask])\n+        access_tensor = registered_storage.access_tensor\n+\n+        index_tensor = pointer - registered_storage.ptr\n+\n+        block = torch.full_like(pointer, fill_value=other, dtype=access_tensor.dtype, device=\"cuda\")\n+        block[mask] = access_tensor[index_tensor[mask]]\n+        return block\n+\n+    def store(self, pointer: torch.Tensor, value: torch.Tensor, mask=None):\n+        assert 0 < pointer.dim() < 3\n+        assert pointer.dtype == torch.int64\n+\n+        if mask is None:\n+            mask = torch.ones_like(pointer).bool()\n+        assert 0 < mask.dim() < 3\n+        assert mask.dtype == torch.bool\n+        mask = mask.expand(pointer.size())\n+\n+        if torch.all(~mask):\n+            return\n+\n+        registered_storage = self._get_registered_storage(pointer[mask])\n+        access_tensor = registered_storage.access_tensor\n+\n+        index_tensor = pointer - registered_storage.ptr\n+        access_tensor[index_tensor[mask]] = value[mask].to(access_tensor.dtype)"}, {"filename": "python/triton/debugger/tl_lang.py", "status": "added", "additions": 621, "deletions": 0, "changes": 621, "file_content_changes": "@@ -0,0 +1,621 @@\n+import triton\n+from .core import ExecutionContext\n+from .memory_map import MemoryMap\n+from triton.debugger import torch_wrapper\n+\n+torch = torch_wrapper.torch\n+\n+\n+def _primitive_to_tensor(x):\n+    \"\"\"\n+    Converts various Python primitive data types to PyTorch tensor.\n+    \"\"\"\n+    tensor_args = {\"device\": \"cuda\"}\n+    if isinstance(x, bool):\n+        return torch.tensor([x], dtype=torch.bool, **tensor_args)\n+    elif isinstance(x, int):\n+        if -(2**31) <= x < 2**31:\n+            return torch.tensor([x], dtype=torch.int32, **tensor_args)\n+        elif -(2**63) <= x < 2**63:\n+            return torch.tensor([x], dtype=torch.int64, **tensor_args)\n+        else:\n+            raise RuntimeError(f\"Nonrepresentable integer {x}.\")\n+    elif isinstance(x, float):\n+        return torch.tensor([x], dtype=torch.float32, **tensor_args)\n+    elif torch.is_tensor(x):\n+        return x\n+    elif isinstance(x, WrappedTensor):\n+        return x\n+    elif isinstance(x, debugger_constexpr):\n+        if x.value is None:\n+            return None\n+        return _primitive_to_tensor(x.value)\n+    elif x is None:\n+        return None\n+    assert False, f\"cannot convert {x} of type {type(x)} to tensor\"\n+\n+\n+def _infer_tensor(func):\n+    \"\"\"\n+    A decorator function to harmonize function args:\n+        - converts primitives to PyTorch tensors\n+        - wraps PyTorch tensors with WrappedTensors\n+    \"\"\"\n+    def wrapper(*args):\n+        new_args = tuple(map(lambda v: _primitive_to_tensor(v), args))\n+        new_args = tuple(map(lambda v: WrappedTensor(v) if torch.is_tensor(v) else v, new_args))\n+\n+        return func(*new_args)\n+\n+    return wrapper\n+\n+\n+def _tensor_operation(func):\n+    \"\"\"\n+    A decorator function to unwrap WrappedTensors and debugger_constexpr before calling the function.\n+    Can be combined with _infer_tensor decorator to harmonize args (everything to torch tensor).\n+    \"\"\"\n+    def wrapper(*args, **kwargs):\n+        for arg in args:\n+            assert not torch.is_tensor(arg), \"unexpected tensor argument\"\n+\n+        def unwrap_tensor(v):\n+            if isinstance(v, WrappedTensor):\n+                return v.tensor\n+            if isinstance(v, debugger_constexpr):\n+                return v.value\n+            return v\n+\n+        new_args = tuple(map(unwrap_tensor, args))\n+        new_kwargs = {k: unwrap_tensor(v) for k, v in kwargs.items()}\n+\n+        result = func(args[0], *new_args[1:], **new_kwargs)\n+        return WrappedTensor(result) if torch.is_tensor(result) else result\n+\n+    return wrapper\n+\n+\n+class debugger_constexpr:\n+    def __init__(self, value):\n+        if isinstance(value, debugger_constexpr):\n+            self.value = value.value\n+        else:\n+            self.value = value\n+\n+    def __str__(self) -> str:\n+        return \"debugger_constexpr(\" + str(self.value) + \")\"\n+\n+    def __index__(self) -> int:\n+        return self.value\n+\n+    def __bool__(self):\n+        return bool(self.value)\n+\n+    def __ge__(self, other):\n+        other = other.value if isinstance(other, debugger_constexpr) else other\n+        return self.value >= other\n+\n+    def __gt__(self, other):\n+        other = other.value if isinstance(other, debugger_constexpr) else other\n+        return self.value > other\n+\n+    def __le__(self, other):\n+        other = other.value if isinstance(other, debugger_constexpr) else other\n+        return self.value <= other\n+\n+    def __lt__(self, other):\n+        other = other.value if isinstance(other, debugger_constexpr) else other\n+        return self.value < other\n+\n+    def __eq__(self, other):\n+        other = other.value if isinstance(other, debugger_constexpr) else other\n+        return self.value == other\n+\n+    def __or__(self, other):\n+        other = other.value if isinstance(other, debugger_constexpr) else other\n+        return self.value | other\n+\n+    def __ror__(self, other):\n+        other = other.value if isinstance(other, debugger_constexpr) else other\n+        return self.value | other\n+\n+    def __and__(self, other):\n+        other = other.value if isinstance(other, debugger_constexpr) else other\n+        return self.value & other\n+\n+    def __rand__(self, other):\n+        other = other.value if isinstance(other, debugger_constexpr) else other\n+        return self.value & other\n+\n+    def to(self, dtype, bitcast=False, _builder=None):\n+        if dtype in [torch.int64]:\n+            ret_ty = int\n+        elif dtype == torch.bool:\n+            ret_ty = bool\n+        elif dtype in [torch.float64]:\n+            ret_ty = float\n+        else:\n+            raise ValueError(\"dtype not supported in debugger\")\n+        return debugger_constexpr(ret_ty(self.value))\n+\n+\n+class WrappedTensor:\n+    def __init__(self, tensor):\n+        self.tensor = tensor\n+\n+    def __index__(self) -> int:\n+        return self.tensor.item()\n+\n+    def __str__(self) -> str:\n+        return \"wrapped_\" + str(self.tensor)\n+\n+    def __bool__(self) -> bool:\n+        return torch.all(self.tensor == True).item()  # noqa: E712\n+\n+    @property\n+    def dtype(self):\n+        return self.tensor.dtype\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __add__(self, other):\n+        return torch.add(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __radd__(self, other):\n+        return self.__add__(other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __sub__(self, other):\n+        return torch.sub(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __rsub__(self, other):\n+        return torch.sub(other, self.tensor)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __mul__(self, other):\n+        return torch.mul(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __rmul__(self, other):\n+        return self.__mul__(other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __truediv__(self, other):\n+        return torch.div(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __rtruediv__(self, other):\n+        return torch.div(other, self.tensor)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __floordiv__(self, other):\n+        return torch.floor_divide(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __rfloordiv__(self, other):\n+        return torch.floor_divide(other, self.tensor)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __mod__(self, other):\n+        return torch.remainder(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __rmod__(self, other):\n+        return torch.remainder(other, self.tensor)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __neg__(self):\n+        return -self.tensor\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __invert__(self):\n+        return ~self.tensor\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __and__(self, other):\n+        return torch.bitwise_and(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __or__(self, other):\n+        return torch.bitwise_or(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __xor__(self, other):\n+        return torch.bitwise_xor(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __lshift__(self, other):\n+        return torch.bitwise_left_shift(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __rshift__(self, other):\n+        return torch.bitwise_right_shift(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __gt__(self, other):\n+        return self.tensor > other\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __rgt__(self, other):\n+        return other > self.tensor\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __ge__(self, other):\n+        return self.tensor >= other\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __rge__(self, other):\n+        return other >= self.tensor\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __lt__(self, other):\n+        return self.tensor < other\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __rlt__(self, other):\n+        return other < self.tensor\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __le__(self, other):\n+        return self.tensor <= other\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __rle__(self, other):\n+        return other <= self.tensor\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __eq__(self, other):\n+        return torch.equal(self.tensor, other)\n+\n+    @_infer_tensor\n+    @_tensor_operation\n+    def __ne__(self, other):\n+        return not torch.equal(self.tensor, other)\n+\n+    @_tensor_operation\n+    def __getitem__(self, slices):\n+        return self.tensor.__getitem__(slices)\n+        # if isinstance(slices, slice):\n+        #     slices = [slices]\n+        # src_shape = self.shape\n+        # dst_shape = []\n+        # curr = 0\n+        # for sl in slices:\n+        #     if isinstance(sl, constexpr) and sl.value is None:\n+        #         dst_shape.append(1)\n+        #     elif sl == slice(None, None, None):\n+        #         dst_shape.append(src_shape[curr].value)\n+        #         curr += 1\n+        # ret = torch.reshape(self.tensor, dst_shape, )\n+        # return ret\n+\n+    @_tensor_operation\n+    def to(self, dtype, bitcast=False):\n+        return self.tensor.to(dtype)\n+        # if isinstance(bitcast, constexpr):\n+        #     bitcast = bitcast.value\n+        # if bitcast:\n+        #     return semantic.bitcast(self, dtype, )\n+        # return semantic.cast(self, dtype, )\n+\n+\n+def _constexpr_to_value(v):\n+    if isinstance(v, debugger_constexpr):\n+        return v.value\n+    return v\n+\n+\n+class TritonLangProxy:\n+    _memory_map: MemoryMap\n+    _context: ExecutionContext\n+\n+    def __init__(self, memory_map: MemoryMap, context: ExecutionContext):\n+        self._memory_map = memory_map\n+        self._context = context\n+\n+    # Types\n+    # Removed void, int1, float8, uint16, uint32, uint64, pi32_t\n+\n+    # constexpr = debugger_constexpr\n+\n+    # Program functions\n+\n+    @_tensor_operation\n+    def load(\n+        self,\n+        pointer: torch.Tensor,\n+        mask: torch.Tensor = None,\n+        other=0.0,\n+        cache_modifier=\"\",\n+        eviction_policy=\"\",\n+        volatile=False,\n+    ):\n+        return self._memory_map.load(pointer, mask, other)\n+\n+    @_tensor_operation\n+    def store(self, pointer: torch.Tensor, value: torch.Tensor, mask=None):\n+        return self._memory_map.store(pointer, value, mask)\n+\n+    @_tensor_operation\n+    def program_id(self, axis):\n+        assert axis < len(self._context.program_id)\n+        return torch.tensor([self._context.program_id[axis]], dtype=torch.int32, device=\"cuda\")\n+\n+    @_tensor_operation\n+    def num_programs(self, axis):\n+        assert axis < len(self._context.program_size)\n+        return torch.tensor([self._context.program_size[axis]], dtype=torch.int32, device=\"cuda\")\n+\n+    @_tensor_operation\n+    def arange(self, start, end):\n+        return torch.arange(start=start, end=end, dtype=torch.int32, device=\"cuda\")\n+\n+    @_tensor_operation\n+    def zeros(self, shape, dtype):\n+        for i, d in enumerate(shape):\n+            if not isinstance(d, debugger_constexpr):\n+                raise TypeError(f\"Shape element {i} must have type `constexpr`\")\n+            if not isinstance(d.value, int):\n+                raise TypeError(f\"Shape element {i} must have type `constexpr[int]`, got `constexpr[{type(d.value)}]\")\n+        shape = [x.value for x in shape]\n+        if isinstance(dtype, triton.language.core.dtype):\n+            if dtype.is_fp32():\n+                dtype = torch.float32\n+            elif dtype.is_fp16():\n+                dtype = torch.float16\n+            elif dtype.is_bf16():\n+                dtype = torch.bfloat16\n+            elif dtype.is_int32():\n+                dtype = torch.int32\n+            elif dtype.is_int16():\n+                dtype = torch.int16\n+            elif dtype.is_int8():\n+                dtype = torch.int8\n+            else:\n+                raise TypeError(f\"Unsupported dtype {dtype}\")\n+        return torch.zeros(size=shape, dtype=dtype, device=\"cuda\")\n+\n+    @_tensor_operation\n+    def dequantize(self, input, scale, shift, nbit, dst_ty=torch.float16):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def broadcast(self, input, other):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def broadcast_to(self, input, shape):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def cat(self, input, shape):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def reshape(self, input, shape):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def dot(self, input, other, trans_a=False, trans_b=False, allow_tf32=True):\n+        assert input.dtype == other.dtype\n+        if trans_a:\n+            input = input.T\n+        if trans_b:\n+            other = other.T\n+        return torch.matmul(input=input, other=other)\n+\n+    @_tensor_operation\n+    def atomic_cas(self, pointer, cmp, val):\n+        stored = self._memory_map.load(pointer, None, 0.0)\n+        if not isinstance(cmp, torch.Tensor):\n+            cmp = torch.tensor([cmp], dtype=stored.dtype, device=\"cuda\")\n+        if not isinstance(val, torch.Tensor):\n+            val = torch.tensor([val], dtype=stored.dtype, device=\"cuda\")\n+        if stored == cmp:\n+            self._memory_map.store(pointer, val, None)\n+        return stored\n+\n+    @_tensor_operation\n+    def atomic_xchg(self, pointer, val, mask=None):\n+        if isinstance(val, int):\n+            val = torch.tensor([val], dtype=torch.int32, device=\"cuda\")\n+        stored = self._memory_map.load(pointer, mask, 0.0)\n+        self._memory_map.store(pointer, val, mask)\n+        return stored\n+\n+    @_tensor_operation\n+    def atomic_add(self, pointer, val, mask=None):\n+        # arbitrary other value as it will masked during storing\n+        stored = self._memory_map.load(pointer, mask, 0.0)\n+        result = stored + val\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n+\n+    @_tensor_operation\n+    def atomic_max(self, pointer, val, mask=None):\n+        stored = self._memory_map.load(pointer, mask, 0.0)\n+        result = torch.maximum(stored, val)\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n+\n+    @_tensor_operation\n+    def atomic_min(self, pointer, val, mask=None):\n+        stored = self._memory_map.load(pointer, mask, 0.0)\n+        result = torch.minimum(stored, val)\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n+\n+    @_tensor_operation\n+    def atomic_and(self, pointer, val, mask=None):\n+        stored = self._memory_map.load(pointer, mask, 0)\n+        result = torch.bitwise_and(stored, val)\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n+\n+    @_tensor_operation\n+    def atomic_or(self, pointer, val, mask=None):\n+        stored = self._memory_map.load(pointer, mask, 0)\n+        result = torch.bitwise_or(stored, val)\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n+\n+    @_tensor_operation\n+    def atomic_xor(self, pointer, val, mask=None):\n+        stored = self._memory_map.load(pointer, mask, 0)\n+        result = torch.bitwise_xor(stored, val)\n+        self._memory_map.store(pointer, result, mask)\n+        return stored\n+\n+    @_tensor_operation\n+    def where(self, condition, x, y):\n+        condition = _primitive_to_tensor(condition)\n+        x = _primitive_to_tensor(x)\n+        y = _primitive_to_tensor(y)\n+        return torch.where(condition, x, y)\n+\n+    @_tensor_operation\n+    def umulhi(self, x, y):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def fdiv(self, x, y, ieee_rounding=False):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def exp(self, x):\n+        return torch.exp(x)\n+\n+    @_tensor_operation\n+    def log(self, x):\n+        return torch.log(x)\n+\n+    @_tensor_operation\n+    def cos(self, x):\n+        return torch.cos(x)\n+\n+    @_tensor_operation\n+    def sin(self, x):\n+        return torch.sin(x)\n+\n+    @_tensor_operation\n+    def sqrt(self, x):\n+        return torch.sqrt(x)\n+\n+    @_tensor_operation\n+    def globaltimer(self):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def clock(self):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def debug_barrier(self):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def multiple_of(self, input, values):\n+        return input\n+\n+    @_tensor_operation\n+    def max_contiguous(self, input, values):\n+        return input\n+\n+    @_tensor_operation\n+    def abs(self, x):\n+        return torch.abs(x)\n+\n+    @_tensor_operation\n+    def cdiv(self, x, div):\n+        return (x + div - 1) // div\n+\n+    @_tensor_operation\n+    def minimum(self, x, y):\n+        if isinstance(x, int):\n+            x = torch.tensor(x, device=\"cuda\")\n+        if isinstance(y, int):\n+            y = torch.tensor(y, device=\"cuda\")\n+        return torch.minimum(x, y)\n+\n+    @_tensor_operation\n+    def maximum(self, x, y):\n+        return torch.maximum(x, y)\n+\n+    @_tensor_operation\n+    def sigmoid(self, x):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def softmax(self, x, ieee_rounding=False):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def ravel(self, x):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def swizzle2d(self, i, j, size_i, size_j, size_g):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def zeros_like(self, input):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def max(self, input, axis=None):\n+        if axis is None:\n+            return torch.max(input)\n+        return torch.max(input, dim=axis).values\n+\n+    @_tensor_operation\n+    def argmax(self, input, axis):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def min(self, input, axis=None):\n+        if axis is None:\n+            return torch.min(input)\n+        return torch.min(input, dim=axis).values\n+\n+    @_tensor_operation\n+    def argmin(self, input, axis):\n+        raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def sum(self, input, axis=None):\n+        if axis is None:\n+            return torch.sum(input)\n+        return torch.sum(input, dim=axis)\n+\n+    @_tensor_operation\n+    def xor_sum(self, input, axis):\n+        raise NotImplementedError()"}, {"filename": "python/triton/debugger/torch_wrapper.py", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -0,0 +1,18 @@\n+try:\n+    import torch as _torch\n+except ImportError:\n+    _torch = None\n+\n+\n+class TorchWrapper:\n+    \"\"\"\n+    Helps in making torch an optional dependency\n+    \"\"\"\n+\n+    def __getattr__(self, name):\n+        if _torch is None:\n+            raise ImportError(\"Triton requires PyTorch to be installed\")\n+        return getattr(_torch, name)\n+\n+\n+torch = TorchWrapper()"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 12, "deletions": 8, "changes": 20, "file_content_changes": "@@ -439,6 +439,7 @@ def jit(\n     do_not_specialize: Optional[Iterable[int]] = None,\n     debug: Optional[bool] = None,\n     noinline: Optional[bool] = None,\n+    interpret: Optional[bool] = None,\n ) -> Union[JITFunction[T], Callable[[T], JITFunction[T]]]:\n     \"\"\"\n     Decorator for JIT-compiling a function using the Triton compiler.\n@@ -460,14 +461,17 @@ def jit(\n \n     def decorator(fn: T) -> JITFunction[T]:\n         assert callable(fn)\n-        return JITFunction(\n-            fn,\n-            version=version,\n-            do_not_specialize=do_not_specialize,\n-            debug=debug,\n-            noinline=noinline,\n-        )\n-\n+        if interpret:\n+            from ..debugger.debugger import GridSelector\n+            return GridSelector(fn)\n+        else:\n+            return JITFunction(\n+                fn,\n+                version=version,\n+                do_not_specialize=do_not_specialize,\n+                debug=debug,\n+                noinline=noinline,\n+            )\n     if fn is not None:\n         return decorator(fn)\n "}]