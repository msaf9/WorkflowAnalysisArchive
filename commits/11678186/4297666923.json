[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUOps.td", "status": "modified", "additions": 65, "deletions": 0, "changes": 65, "file_content_changes": "@@ -9,6 +9,8 @@ include \"triton/Dialect/Triton/IR/TritonAttrDefs.td\"\n include \"mlir/IR/OpBase.td\"\n include \"mlir/Interfaces/SideEffectInterfaces.td\" // Pure\n include \"mlir/Interfaces/InferTypeOpInterface.td\" // SameOperandsAndResultType\n+include \"mlir/Interfaces/DestinationStyleOpInterface.td\"\n+include \"mlir/Interfaces/ViewLikeInterface.td\"\n \n def ResultsAreSharedEncoding: NativeOpTrait<\"ResultsAreSharedEncoding\">;\n \n@@ -105,6 +107,69 @@ def TTG_SelectOp : TTG_Op<\"select\", [Pure, Elementwise,\n }\n \n \n+\n+def TTG_ExtractSliceOp : TTG_Op<\"extract_slice\",\n+                                [AttrSizedOperandSegments,\n+                                 ResultsAreSharedEncoding,\n+                                 Pure,\n+                                 OffsetSizeAndStrideOpInterface\n+                                 ]> {\n+  let summary = \"extract slice operation\";\n+  let description = [{\n+    same as tensor.extract_slice, but with int32 index. The motivations for re-implementing it are:\n+    We reimplement ExtractSliceOp with int32 index, because:\n+    - we want to enforce int32 indexing on GPUs since Triton tensors fit in SRAM\n+    - we still want to use indexWidth = 64 when lowering to LLVM because our loops can have\n+      64-bit induction variables and scf.for uses indexType for bounds/ivs\n+  }];\n+\n+  let arguments = (ins\n+    AnyRankedTensor:$source,\n+    Variadic<I32>:$offsets,\n+    Variadic<I32>:$sizes,\n+    Variadic<I32>:$strides,\n+    DenseI64ArrayAttr:$static_offsets,\n+    DenseI64ArrayAttr:$static_sizes,\n+    DenseI64ArrayAttr:$static_strides\n+  );\n+  let results = (outs AnyRankedTensor:$result);\n+\n+  let builders = [\n+    // Build an ExtractSliceOp with mixed static and dynamic entries and custom\n+    // result type. If the type passed is nullptr, it is inferred.\n+    OpBuilder<(ins \"RankedTensorType\":$resultType, \"Value\":$source,\n+      \"ArrayRef<OpFoldResult>\":$offsets, \"ArrayRef<OpFoldResult>\":$sizes,\n+      \"ArrayRef<OpFoldResult>\":$strides,\n+      CArg<\"ArrayRef<NamedAttribute>\", \"{}\">:$attrs)>,\n+  ];\n+\n+  let extraClassDeclaration = [{\n+    /// Return the number of leading operands before the `offsets`, `sizes` and\n+    /// and `strides` operands.\n+    static unsigned getOffsetSizeAndStrideStartOperandIndex() { return 1; }\n+\n+    /// Returns the type of the base tensor operand.\n+    RankedTensorType getSourceType() {\n+      return getSource().getType().cast<RankedTensorType>();\n+    }\n+\n+    std::array<unsigned, 3> getArrayAttrMaxRanks() {\n+      unsigned rank = getSourceType().getRank();\n+      return {rank, rank, rank};\n+    }\n+  }];\n+\n+  let assemblyFormat = [{\n+    $source ``\n+    custom<DynamicIndexList>($offsets, $static_offsets)\n+    custom<DynamicIndexList>($sizes, $static_sizes)\n+    custom<DynamicIndexList>($strides, $static_strides)\n+    attr-dict `:` type($source) `to` type($result)\n+  }];\n+}\n+\n+//\n+\n def TTG_InsertSliceAsyncOp : TTG_Op<\"insert_slice_async\",\n                                     [AttrSizedOperandSegments,\n                                      ResultsAreSharedEncoding,"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -27,7 +27,7 @@ void SharedMemoryAliasAnalysis::visitOperation(\n     // These ops may allocate a new shared memory buffer.\n     auto result = op->getResult(0);\n     // XXX(Keren): the following ops are always aliasing for now\n-    if (isa<tensor::ExtractSliceOp, triton::TransOp>(op)) {\n+    if (isa<triton::gpu::ExtractSliceOp, triton::TransOp>(op)) {\n       // extract_slice %src\n       // trans %src\n       aliasInfo = AliasInfo(operands[0]->getValue());"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -78,8 +78,8 @@ void MembarAnalysis::visitTerminator(Operation *op,\n \n void MembarAnalysis::update(Operation *op, BlockInfo *blockInfo,\n                             OpBuilder *builder) {\n-  if (isa<tensor::ExtractSliceOp>(op) || isa<triton::gpu::AllocTensorOp>(op) ||\n-      isa<triton::TransOp>(op)) {\n+  if (isa<triton::gpu::ExtractSliceOp>(op) ||\n+      isa<triton::gpu::AllocTensorOp>(op) || isa<triton::TransOp>(op)) {\n     // alloc is an allocation op without memory write.\n     // FIXME(Keren): extract_slice is always alias for now\n     return;"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -114,7 +114,7 @@ bool maybeSharedAllocationOp(Operation *op) {\n }\n \n bool maybeAliasOp(Operation *op) {\n-  return isa<tensor::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n+  return isa<triton::gpu::ExtractSliceOp>(op) || isa<triton::TransOp>(op) ||\n          isa<triton::gpu::InsertSliceAsyncOp>(op) ||\n          isa<tensor::InsertSliceOp>(op);\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -428,12 +428,12 @@ struct AllocTensorOpConversion\n };\n \n struct ExtractSliceOpConversion\n-    : public ConvertTritonGPUOpToLLVMPattern<tensor::ExtractSliceOp> {\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ExtractSliceOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n-      tensor::ExtractSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n+      triton::gpu::ExtractSliceOp>::ConvertTritonGPUOpToLLVMPattern;\n \n   LogicalResult\n-  matchAndRewrite(tensor::ExtractSliceOp op, OpAdaptor adaptor,\n+  matchAndRewrite(triton::gpu::ExtractSliceOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     // %dst = extract_slice %src[%offsets]\n     Location loc = op->getLoc();"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 25, "deletions": 2, "changes": 27, "file_content_changes": "@@ -851,7 +851,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     return mlir::success();\n   }\n   // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n-  auto extract_slice = dyn_cast<tensor::ExtractSliceOp>(arg);\n+  auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n   if (extract_slice) {\n     if (!isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n@@ -872,7 +872,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n     rewriter.setInsertionPoint(extract_slice);\n     auto newArg = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         op->getLoc(), newType, extract_slice.getSource());\n-    rewriter.replaceOpWithNewOp<tensor::ExtractSliceOp>(\n+    rewriter.replaceOpWithNewOp<triton::gpu::ExtractSliceOp>(\n         op, resType, newArg.getResult(), extract_slice.offsets(),\n         extract_slice.sizes(), extract_slice.strides(),\n         extract_slice.static_offsets(), extract_slice.static_sizes(),\n@@ -925,6 +925,29 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n \n //===----------------------------------------------------------------------===//\n \n+/// Build an ExtractSliceOp with mixed static and dynamic entries and custom\n+/// result type. If the type passed is nullptr, it is inferred.\n+void ExtractSliceOp::build(OpBuilder &b, OperationState &result,\n+                           RankedTensorType resultType, Value source,\n+                           ArrayRef<OpFoldResult> offsets,\n+                           ArrayRef<OpFoldResult> sizes,\n+                           ArrayRef<OpFoldResult> strides,\n+                           ArrayRef<NamedAttribute> attrs) {\n+  SmallVector<int64_t> staticOffsets, staticSizes, staticStrides;\n+  SmallVector<Value> dynamicOffsets, dynamicSizes, dynamicStrides;\n+  dispatchIndexOpFoldResults(offsets, dynamicOffsets, staticOffsets);\n+  dispatchIndexOpFoldResults(sizes, dynamicSizes, staticSizes);\n+  dispatchIndexOpFoldResults(strides, dynamicStrides, staticStrides);\n+  auto sourceRankedTensorType = source.getType().cast<RankedTensorType>();\n+  build(b, result, resultType, source, dynamicOffsets, dynamicSizes,\n+        dynamicStrides, b.getDenseI64ArrayAttr(staticOffsets),\n+        b.getDenseI64ArrayAttr(staticSizes),\n+        b.getDenseI64ArrayAttr(staticStrides));\n+  result.addAttributes(attrs);\n+}\n+\n+//===----------------------------------------------------------------------===//\n+\n void TritonGPUDialect::initialize() {\n   addAttributes<\n #define GET_ATTRDEF_LIST"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -394,7 +394,7 @@ void LoopPipeliner::emitPrologue() {\n     sliceType = RankedTensorType::get({bufferShape[1], bufferShape[2]},\n                                       sliceType.getElementType(),\n                                       loadsBufferType[loadOp].getEncoding());\n-    Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n+    Value extractSlice = builder.create<triton::gpu::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{int_attr(0), int_attr(0), int_attr(0)},\n         SmallVector<OpFoldResult>{int_attr(1),\n@@ -532,8 +532,6 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n   Value extractSliceIndex = builder.create<arith::RemSIOp>(\n       nextIV.getLoc(), loopIterIdx,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n-  extractSliceIndex = builder.create<arith::IndexCastOp>(\n-      extractSliceIndex.getLoc(), builder.getIndexType(), extractSliceIndex);\n \n   for (Operation *op : orderedDeps)\n     if (!loads.contains(op->getResult(0))) {\n@@ -591,7 +589,7 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n                                         sliceType.getElementType(),\n                                         loadsBufferType[loadOp].getEncoding());\n \n-      nextOp = builder.create<tensor::ExtractSliceOp>(\n+      nextOp = builder.create<triton::gpu::ExtractSliceOp>(\n           op->getLoc(), sliceType, insertAsyncOp,\n           SmallVector<OpFoldResult>{extractSliceIndex, int_attr(0),\n                                     int_attr(0)},"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "file_content_changes": "@@ -103,11 +103,9 @@ Value Prefetcher::generatePrefetch(Value v, unsigned opIdx, bool isPrologue,\n   if (offsetK)\n     offset[kIdx] = *offsetK;\n \n-  Value newSmem = builder.create<tensor::ExtractSliceOp>(\n-      v.getLoc(),\n-      // TODO: encoding?\n-      RankedTensorType::get(shape, elementType, type.getEncoding()), v,\n-      SmallVector<OpFoldResult>{intAttr(offset[0]), intAttr(offset[1])},\n+  Value newSmem = builder.create<triton::gpu::ExtractSliceOp>(\n+      v.getLoc(), RankedTensorType::get(shape, elementType, type.getEncoding()),\n+      v, SmallVector<OpFoldResult>{intAttr(offset[0]), intAttr(offset[1])},\n       SmallVector<OpFoldResult>{intAttr(shape[0]), intAttr(shape[1])},\n       SmallVector<OpFoldResult>{intAttr(1), intAttr(1)});\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -202,7 +202,7 @@ inline bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n     return expensiveLoadOrStore(op, targetEncoding);\n-  if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n+  if (isa<triton::gpu::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n     return true;"}, {"filename": "lib/Target/LLVMIR/LLVMIRTranslation.cpp", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "file_content_changes": "@@ -302,21 +302,16 @@ translateTritonGPUToLLVMIR(llvm::LLVMContext *llvmContext,\n   pm.addPass(mlir::createConvertSCFToCFPass());\n   pm.addPass(mlir::triton::createConvertTritonFuncToLLVMPass());\n   pm.addPass(mlir::createArithToLLVMConversionPass(arithOpt));\n-  // pm.addPass(createConvertTritonGPUToLLVMPass(computeCapability));\n+  pm.addPass(createConvertTritonGPUToLLVMPass(computeCapability));\n   pm.addPass(mlir::createCanonicalizerPass());\n-\n-  // Canonicalize to eliminate the remaining UnrealizedConversionCastOp\n-  // pm.addPass(mlir::createCanonicalizerPass());\n   // Simplify the IR\n   pm.addPass(mlir::createCSEPass());\n   pm.addPass(mlir::createSymbolDCEPass());\n \n   if (failed(pm.run(module))) {\n-    llvm::outs() << module << \"\\n\";\n     llvm::errs() << \"Pass execution failed\";\n     return nullptr;\n   }\n-  llvm::outs() << module << \"\\n\";\n \n   auto llvmIR = translateLLVMToLLVMIR(llvmContext, module);\n   if (!llvmIR) {"}]