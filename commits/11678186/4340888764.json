[{"filename": "include/triton/Dialect/Triton/IR/TritonTypes.td", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -13,7 +13,10 @@ class TritonTypeDef<string name, string _mnemonic>\n     let mnemonic = _mnemonic;\n }\n \n-def TT_Float : AnyTypeOf<[F8E4M3FN, F8E5M2, F16, BF16, F32, F64], \"floating-point\">;\n+// Floating-point Type\n+def F8 : TritonTypeDef<\"Float8\", \"f8\">;\n+\n+def TT_Float : AnyTypeOf<[F8, F16, BF16, F32, F64], \"floating-point\">;\n def TT_FloatTensor : TensorOf<[TT_Float]>;\n def TT_FloatLike : AnyTypeOf<[TT_Float, TT_FloatTensor]>;\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -57,6 +57,7 @@ struct ConvertLayoutOpConversion\n                                        unsigned elemId, RankedTensorType type,\n                                        ArrayRef<unsigned> multiDimCTAInRepId,\n                                        ArrayRef<unsigned> shapePerCTA) const {\n+    auto shape = type.getShape();\n     unsigned rank = shape.size();\n     if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n       auto multiDimOffsetFirstElem ="}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -852,6 +852,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto offset = emitOffsetForLayout(layout, type);\n     // step 3, add offset to base, and reorder the sequence of indices to\n     // guarantee that elems in the same sizePerThread are adjacent in order\n+    auto shape = type.getShape();\n     unsigned rank = shape.size();\n     unsigned elemsPerThread = offset.size();\n     SmallVector<SmallVector<Value>> multiDimIdx(elemsPerThread,"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -138,8 +138,11 @@ bool FpToFpOp::areCastCompatible(::mlir::TypeRange inputs,\n   }\n   // Check whether fp8 <=> fp16, bf16, f32, f64\n   // Make `srcEltType` always the fp8 side\n-  return dstEltType.isFloat8E5M2() || dstEltType.isFloat8E4M3FN() ||\n-         dstEltType.isF16() || dstEltType.isBF16() || dstEltType.isF32() ||\n+  if (dstEltType.dyn_cast<mlir::triton::Float8Type>())\n+    std::swap(srcEltType, dstEltType);\n+  if (!srcEltType.dyn_cast<mlir::triton::Float8Type>())\n+    return false;\n+  return dstEltType.isF16() || dstEltType.isBF16() || dstEltType.isF32() ||\n          dstEltType.isF64();\n }\n "}, {"filename": "lib/Dialect/Triton/IR/Types.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -43,7 +43,9 @@ namespace mlir {\n unsigned getPointeeBitWidth(RankedTensorType tensorTy) {\n   auto ptrTy = tensorTy.getElementType().cast<triton::PointerType>();\n   auto pointeeType = ptrTy.getPointeeType();\n-  return pointeeType.getIntOrFloatBitWidth();\n+  return pointeeType.isa<triton::Float8Type>()\n+             ? 8\n+             : pointeeType.getIntOrFloatBitWidth();\n }\n \n } // namespace mlir"}]