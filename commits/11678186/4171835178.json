[{"filename": "docs/getting-started/tutorials/parallel_reduction.png", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -434,15 +434,15 @@ struct MMA16816ConversionHelper {\n \n   // Get a warpId for M axis.\n   Value getWarpM(int M) const {\n-    auto matShape = helper.getMmaMatShape();\n-    return urem(urem(warp, i32_val(wpt[0])), i32_val(M / matShape[0]));\n+    auto matInstrShape = helper.getMmaInstrShape();\n+    return urem(urem(warp, i32_val(wpt[0])), i32_val(M / matInstrShape[0]));\n   }\n \n   // Get a warpId for N axis.\n   Value getWarpN(int N) const {\n-    auto matShape = helper.getMmaMatShape();\n+    auto matInstrShape = helper.getMmaInstrShape();\n     Value warpMN = udiv(warp, i32_val(wpt[0]));\n-    return urem(urem(warpMN, i32_val(wpt[1])), i32_val(N / matShape[1]));\n+    return urem(urem(warpMN, i32_val(wpt[1])), i32_val(N / matInstrShape[1]));\n   }\n \n   // Get the mmaInstrShape deducing either from $a or $b."}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "file_content_changes": "@@ -392,8 +392,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         getSwizzledSharedPtrs(loc, inVec, srcTy, dstSharedLayout, dstElemTy,\n                               smemObj, rewriter, offsetVals, srcStrides);\n \n-    std::map<unsigned, Value> cache0;\n-    std::map<unsigned, Value> cache1;\n     for (unsigned i = 0; i < numElems; ++i) {\n       if (i % minVec == 0)\n         word = undef(wordTy);\n@@ -808,8 +806,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     Value warpSize = idx_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n-    Value warpId0 = urem(warpId, warpsPerCTA[0]);\n-    Value warpId1 = urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]);\n+    Value warpId0 = urem(urem(warpId, warpsPerCTA[0]), idx_val(shape[0] / 16));\n+    Value warpId1 = urem(urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]),\n+                         idx_val(shape[1] / 8));\n     Value offWarp0 = mul(warpId0, idx_val(16));\n     Value offWarp1 = mul(warpId1, idx_val(8));\n "}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 0, "deletions": 10, "changes": 10, "file_content_changes": "@@ -219,16 +219,6 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout,\n       assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n                   \"supported yet\");\n     }\n-  } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n-    if (mmaLayout.isAmpere()) {\n-      return {16 * mmaLayout.getWarpsPerCTA()[0],\n-              8 * mmaLayout.getWarpsPerCTA()[1]};\n-    } else if (mmaLayout.isVolta()) {\n-      return {16 * mmaLayout.getWarpsPerCTA()[0],\n-              16 * mmaLayout.getWarpsPerCTA()[1]};\n-    } else {\n-      llvm_unreachable(\"Unexpected mma version\");\n-    }\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "file_content_changes": "@@ -1083,7 +1083,7 @@ def kernel(X, stride_xm, stride_xn,\n \n @pytest.mark.parametrize(\"M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype\",\n                          [(*shape, 4, False, False, epilogue, allow_tf32, dtype)\n-                          for shape in [(64, 64, 64)]\n+                          for shape in [(64, 64, 64), (16, 16, 16)]\n                           for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n                           for dtype in ['float16', 'float32']\n@@ -1228,8 +1228,10 @@ def kernel(X, stride_xm, stride_xk,\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']\n-    assert 'ld.global.v4' in ptx\n-    assert 'st.global.v4' in ptx\n+    if K > 16 or N > 16 or M > 16:\n+        # XXX: skip small sizes because they are not vectorized\n+        assert 'ld.global.v4' in ptx\n+        assert 'st.global.v4' in ptx\n     if dtype == 'float32' and allow_tf32:\n         assert 'mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32' in ptx\n     elif dtype == 'float32' and allow_tf32:"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -780,6 +780,11 @@ def load(ptr: tl.tensor,\n          builder: ir.builder) -> tl.tensor:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of load instruction is \" + ptr.type.__repr__())\n+    if not ptr.type.is_block():\n+        if mask and mask.type.is_block():\n+            raise ValueError(\"Mask argument cannot be block type if pointer argument is not a block\")\n+        if other and other.type.is_block():\n+            raise ValueError(\"Other argument cannot be block type if pointer argument is not a block\")\n     if ptr.type.is_block():\n         if mask:\n             mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n@@ -830,6 +835,11 @@ def store(ptr: tl.tensor,\n           builder: ir.builder) -> tl.tensor:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+    if not ptr.type.is_block():\n+        if val.type.is_block():\n+            raise ValueError(\"Value argument cannot be block type if pointer argument is not a block\")\n+        if mask and mask.type.is_block():\n+            raise ValueError(\"Mask argument cannot be block type if pointer argument is not a block\")\n     if ptr.type.is_block():\n         val = broadcast_impl_shape(val, ptr.type.get_block_shapes(), builder)\n     if mask and ptr.type.is_block():"}, {"filename": "python/tutorials/05-layer-norm.py", "status": "modified", "additions": 140, "deletions": 51, "changes": 191, "file_content_changes": "@@ -1,8 +1,30 @@\n \"\"\"\n Layer Normalization\n ====================\n+In this tutorial, you will write a high-performance layer normalization\n+kernel that runs faster than the PyTorch implementation.\n+You will specifically learn about:\n+\n+- How to implement backward pass in Triton\n+- How to implement parallel reduction in Triton\n \"\"\"\n \n+# %%\n+# Motivations\n+# -------------\n+# The *LayerNorm* operator was first introduced in [BA2016]_ as a way to improve the performance\n+# of sequential models (e.g., Transformers) or neural networks with small batch size.\n+# It takes a vector :math:`x` as input and produces a vector :math:`y` of the same shape as output.\n+# The normalization is performed by subtracting the mean and dividing by the standard deviation of :math:`x`.\n+# After the normalization, a learnable linear transformation with weights :math:`w` and biases :math:`b` is applied.\n+# The forward pass can be expressed as follows:\n+#\n+# .. math::\n+#    y = \\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} } * w + b\n+#\n+# where :math:`\\epsilon` is a small constant added to the denominator for numerical stability.\n+# Let\u2019s first take a look at the foward pass implementation.\n+\n import torch\n \n import triton\n@@ -19,128 +41,189 @@\n \n @triton.jit\n def _layer_norm_fwd_fused(\n-    A,\n-    Out,\n-    Weight,\n-    Bias,\n-    Mean, Rstd,\n-    stride, N, eps,\n+    X,  # pointer to the input\n+    Y,  # pointer to the output\n+    W,  # pointer to the weights\n+    B,  # pointer to the biases\n+    Mean,  # pointer to the mean\n+    Rstd,  # pointer to the 1/std\n+    stride,  # how much to increase the pointer when moving by 1 row\n+    N,  # number of columns in X\n+    eps,  # epsilon to avoid division by zero\n     BLOCK_SIZE: tl.constexpr,\n ):\n-    # position of elements processed by this program\n+    # Map the program id to the row of X and Y it should compute.\n     row = tl.program_id(0)\n-    Out += row * stride\n-    A += row * stride\n-    # compute mean\n+    Y += row * stride\n+    X += row * stride\n+    # Compute mean\n     mean = 0\n     _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n-        a = tl.load(A + cols, mask=cols < N, other=0.).to(tl.float32)\n+        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n         _mean += a\n     mean = tl.sum(_mean, axis=0) / N\n-    # compute variance\n+    # Compute variance\n     _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n-        a = tl.load(A + cols, mask=cols < N, other=0.).to(tl.float32)\n-        a = tl.where(cols < N, a - mean, 0.)\n-        _var += a * a\n+        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)\n+        x = tl.where(cols < N, x - mean, 0.)\n+        _var += x * x\n     var = tl.sum(_var, axis=0) / N\n     rstd = 1 / tl.sqrt(var + eps)\n-    # write-back mean/rstd\n+    # Write mean / rstd\n     tl.store(Mean + row, mean)\n     tl.store(Rstd + row, rstd)\n-    # multiply by weight and add bias\n+    # Normalize and apply linear transformation\n     for off in range(0, N, BLOCK_SIZE):\n         cols = off + tl.arange(0, BLOCK_SIZE)\n         mask = cols < N\n-        weight = tl.load(Weight + cols, mask=mask)\n-        bias = tl.load(Bias + cols, mask=mask)\n-        a = tl.load(A + cols, mask=mask, other=0.).to(tl.float32)\n-        a_hat = (a - mean) * rstd\n-        out = a_hat * weight + bias\n-        # # write-back\n-        tl.store(Out + cols, out, mask=mask)\n+        w = tl.load(W + cols, mask=mask)\n+        b = tl.load(B + cols, mask=mask)\n+        x = tl.load(X + cols, mask=mask, other=0.).to(tl.float32)\n+        x_hat = (x - mean) * rstd\n+        y = x_hat * w + b\n+        # Write output\n+        tl.store(Y + cols, y, mask=mask)\n \n \n-# Backward pass (DX + partial DW + partial DB)\n+# %%\n+# Backward pass\n+# ---------------------------------\n+# The backward pass for the layer normalization operator is a bit more involved than the forward pass.\n+# Let :math:`\\hat{x}` be the normalized inputs :math:`\\frac{ x - \\text{E}[x] }{ \\sqrt{\\text{Var}(x) + \\epsilon} }` before the linear transformation,\n+# the Vector-Jacobian Products (VJP) :math:`\\nabla_{x}` of :math:`x` are given by:\n+#\n+# .. math::\n+#    \\nabla_{x} = \\frac{1}{\\sigma}\\Big( \\nabla_{y} \\odot w - \\underbrace{ \\big( \\frac{1}{N} \\hat{x} \\cdot (\\nabla_{y} \\odot w) \\big) }_{c_1} \\odot \\hat{x} - \\underbrace{ \\frac{1}{N} \\nabla_{y} \\cdot w }_{c_2} \\Big)\n+#\n+# where :math:`\\odot` denotes the element-wise multiplication, :math:`\\cdot` denotes the dot product, and :math:`\\sigma` is the standard deviation.\n+# :math:`c_1` and :math:`c_2` are intermediate constants that improve the readability of the following implementation.\n+#\n+# For the weights :math:`w` and biases :math:`b`, the VJPs :math:`\\nabla_{w}` and :math:`\\nabla_{b}` are more straightforward:\n+#\n+# .. math::\n+#    \\nabla_{w} = \\nabla_{y} \\odot \\hat{x} \\quad \\text{and} \\quad \\nabla_{b} = \\nabla_{y}\n+#\n+# Since the same weights :math:`w` and biases :math:`b` are used for all rows in the same batch, their gradients need to sum up.\n+# To perform this step efficiently, we use a parallel reduction strategy: each kernel instance accumulates\n+# partial :math:`\\nabla_{w}` and :math:`\\nabla_{b}` across certain rows into one of :math:`\\text{GROUP_SIZE_M}` independent buffers.\n+# These buffers stay in the L2 cache and then are further reduced by another function to compute the actual :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+#\n+# Let the number of input rows :math:`M = 4` and :math:`\\text{GROUP_SIZE_M} = 2`,\n+# here's a diagram of the parallel reduction strategy for :math:`\\nabla_{w}` (:math:`\\nabla_{b}` is omitted for brevity):\n+#\n+#   .. image:: parallel_reduction.png\n+#\n+# In Stage 1, the rows of X that have the same color share the same buffer and thus a lock is used to ensure that only one kernel instance writes to the buffer at a time.\n+# In Stage 2, the buffers are further reduced to compute the final :math:`\\nabla_{w}` and :math:`\\nabla_{b}`.\n+# In the following implementation, Stage 1 is implemented by the function :code:`_layer_norm_bwd_dx_fused` and Stage 2 is implemented by the function :code:`_layer_norm_bwd_dwdb`.\n+\n @triton.jit\n-def _layer_norm_bwd_dx_fused(DX, DY, DW, DB, X, W, B, M, V, Lock, stride, N, eps,\n-                             GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n-    # position of elements processed by this program\n+def _layer_norm_bwd_dx_fused(\n+    DX,  # pointer to the input gradient\n+    DY,  # pointer to the output gradient\n+    DW,  # pointer to the partial sum of weights gradient\n+    DB,  # pointer to the partial sum of biases gradient\n+    X,   # pointer to the input\n+    W,   # pointer to the weights\n+    B,   # pointer to the biases\n+    Mean,   # pointer to the mean\n+    Rstd,   # pointer to the 1/std\n+    Lock,  # pointer to the lock\n+    stride,  # how much to increase the pointer when moving by 1 row\n+    N,  # number of columns in X\n+    eps,  # epsilon to avoid division by zero\n+    GROUP_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr\n+):\n+    # Map the program id to the elements of X, DX, and DY it should compute.\n     row = tl.program_id(0)\n     cols = tl.arange(0, BLOCK_SIZE_N)\n     mask = cols < N\n-    # offset data pointers to start at the row of interest\n     X += row * stride\n     DY += row * stride\n     DX += row * stride\n-    # offset locks and weight/bias gradient pointer\n-    # each kernel instance accumulates partial sums for\n-    # DW and DB into one of GROUP_SIZE_M independent buffers\n-    # these buffers stay in the L2, which allow this kernel\n-    # to be fast\n+    # Offset locks and weights/biases gradient pointer for parallel reduction\n     lock_id = row % GROUP_SIZE_M\n     Lock += lock_id\n     Count = Lock + GROUP_SIZE_M\n     DW = DW + lock_id * N + cols\n     DB = DB + lock_id * N + cols\n-    # load data to SRAM\n+    # Load data to SRAM\n     x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n     dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n     w = tl.load(W + cols, mask=mask).to(tl.float32)\n-    mean = tl.load(M + row)\n-    rstd = tl.load(V + row)\n-    # compute dx\n+    mean = tl.load(Mean + row)\n+    rstd = tl.load(Rstd + row)\n+    # Compute dx\n     xhat = (x - mean) * rstd\n     wdy = w * dy\n     xhat = tl.where(mask, xhat, 0.)\n     wdy = tl.where(mask, wdy, 0.)\n-    mean1 = tl.sum(xhat * wdy, axis=0) / N\n-    mean2 = tl.sum(wdy, axis=0) / N\n-    dx = (wdy - (xhat * mean1 + mean2)) * rstd\n-    # write-back dx\n+    c1 = tl.sum(xhat * wdy, axis=0) / N\n+    c2 = tl.sum(wdy, axis=0) / N\n+    dx = (wdy - (xhat * c1 + c2)) * rstd\n+    # Write dx\n     tl.store(DX + cols, dx, mask=mask)\n-    # accumulate partial sums for dw/db\n+    # Accumulate partial sums for dw/db\n     partial_dw = (dy * xhat).to(w.dtype)\n     partial_db = (dy).to(w.dtype)\n     while tl.atomic_cas(Lock, 0, 1) == 1:\n         pass\n     count = tl.load(Count)\n-    # first store doesn't accumulate\n+    # First store doesn't accumulate\n     if count == 0:\n         tl.atomic_xchg(Count, 1)\n     else:\n         partial_dw += tl.load(DW, mask=mask)\n         partial_db += tl.load(DB, mask=mask)\n     tl.store(DW, partial_dw, mask=mask)\n     tl.store(DB, partial_db, mask=mask)\n-    # release lock\n+    # Release the lock\n     tl.atomic_xchg(Lock, 0)\n \n-# Backward pass (total DW + total DB)\n-\n \n @triton.jit\n-def _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N,\n-                         BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):\n+def _layer_norm_bwd_dwdb(\n+    DW,  # pointer to the partial sum of weights gradient\n+    DB,  # pointer to the partial sum of biases gradient\n+    FINAL_DW,  # pointer to the weights gradient\n+    FINAL_DB,  # pointer to the biases gradient\n+    M,  # GROUP_SIZE_M\n+    N,  # number of columns\n+    BLOCK_SIZE_M: tl.constexpr,\n+    BLOCK_SIZE_N: tl.constexpr\n+):\n+    # Map the program id to the elements of DW and DB it should compute.\n     pid = tl.program_id(0)\n     cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n     db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n+    # Iterate through the rows of DW and DB to sum the partial sums.\n     for i in range(0, M, BLOCK_SIZE_M):\n         rows = i + tl.arange(0, BLOCK_SIZE_M)\n         mask = (rows[:, None] < M) & (cols[None, :] < N)\n         offs = rows[:, None] * N + cols[None, :]\n         dw += tl.load(DW + offs, mask=mask, other=0.)\n         db += tl.load(DB + offs, mask=mask, other=0.)\n+    # Write the final sum to the output.\n     sum_dw = tl.sum(dw, axis=0)\n     sum_db = tl.sum(db, axis=0)\n     tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n     tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n \n \n+# %%\n+# Benchmark\n+# ---------------------------------\n+# We can now compare the performance of our kernel against that of PyTorch.\n+# Here we focus on inputs that have Less than 64KB per feature.\n+# Specifically, one can set :code:`'mode': 'backward'` to benchmark the backward pass.\n+\n+\n class LayerNorm(torch.autograd.Function):\n \n     @staticmethod\n@@ -172,7 +255,7 @@ def forward(ctx, x, normalized_shape, weight, bias, eps):\n     @staticmethod\n     def backward(ctx, dy):\n         x, w, b, m, v = ctx.saved_tensors\n-        # heuristics for amount of parallel reduction stream for DG/DB\n+        # heuristics for amount of parallel reduction stream for DW/DB\n         N = w.shape[0]\n         GROUP_SIZE_M = 64\n         if N <= 8192: GROUP_SIZE_M = 96\n@@ -275,4 +358,10 @@ def bench_layer_norm(M, N, dtype, provider, mode='backward', eps=1e-5, device='c\n \n \n test_layer_norm(1151, 8192, torch.float16)\n-# bench_layer_norm.run(save_path='.', print_data=True)\n+bench_layer_norm.run(save_path='.', print_data=True)\n+\n+# %%\n+# References\n+# --------------\n+#\n+# .. [BA2016] Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton, \"Layer Normalization\", Arxiv 2016"}]