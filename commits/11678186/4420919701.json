[{"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -555,11 +555,11 @@ int DotOpMmaV2ConversionHelper::getMmaRetSize() const {\n }\n \n DotOpMmaV2ConversionHelper::TensorCoreType\n-DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(Type operandTy, bool allowF16Acc) {\n+DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(Type operandTy) {\n   auto tensorTy = operandTy.cast<RankedTensorType>();\n   auto elemTy = tensorTy.getElementType();\n   if (elemTy.isF16())\n-    return allowF16Acc? TensorCoreType::FP16_FP16_FP16_FP16 : TensorCoreType::FP32_FP16_FP16_FP32;\n+    return TensorCoreType::FP32_FP16_FP16_FP32;\n   if (elemTy.isF32())\n     return TensorCoreType::FP32_TF32_TF32_FP32;\n   if (elemTy.isBF16())"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -177,7 +177,10 @@ struct DotOpMmaV2ConversionHelper {\n   }\n \n   // Deduce the TensorCoreType from either $a or $b's type.\n-  static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy, bool allowF16Acc = false);\n+  // TODO: both the input type ($a or $b) and output type ($c or $d) are\n+  // needed to differentiate TensorCoreType::FP32_FP16_FP16_FP32 from\n+  // TensorCoreType::FP16_FP16_FP16_FP16\n+  static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy);\n \n   int getVec() const {\n     assert(mmaType != TensorCoreType::NOT_APPLICABLE &&"}]