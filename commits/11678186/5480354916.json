[{"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 19, "deletions": 17, "changes": 36, "file_content_changes": "@@ -57,6 +57,17 @@ void castInt32To2xF16(SmallVector<Value> &vals, Value i32Val, unsigned index,\n   vals[index + 1] = extract_element(f16_ty, vec, i32_val(1));\n }\n \n+bool canUseLdMatrix(RankedTensorType srcTy, RankedTensorType dstTy) {\n+  auto srcLayout = srcTy.getEncoding();\n+  auto dstLayout = dstTy.getEncoding();\n+  bool isDstMmaV1 = false;\n+  if (auto mmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>()) {\n+    isDstMmaV1 = mmaLayout.isVolta();\n+  }\n+  return dstLayout.isa<MmaEncodingAttr>() && !isDstMmaV1 &&\n+         triton::gpu::getWarpsPerCTA(dstLayout)[1] == 1 &&\n+         srcTy.getElementType().getIntOrFloatBitWidth() != 32;\n+}\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:\n@@ -321,8 +332,7 @@ struct ConvertLayoutOpConversion\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTAs(rank);\n     auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n-    shapePerCTA[1] = std::min<unsigned>(type.getShape()[1],\n-                                        shapePerCTA[1] * 2); // TODO: hack\n+    shapePerCTA[1] = 16; // todo: hack\n     auto order = getOrder(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n       numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n@@ -493,12 +503,12 @@ struct ConvertLayoutOpConversion\n   lowerDistributedToDistributed(triton::gpu::ConvertLayoutOp op,\n                                 OpAdaptor adaptor,\n                                 ConversionPatternRewriter &rewriter) const {\n-    bool enabled = false;\n     auto loc = op.getLoc();\n     Value src = op.getSrc();\n     Value dst = op.getResult();\n     auto srcTy = src.getType().cast<RankedTensorType>();\n     auto dstTy = dst.getType().cast<RankedTensorType>();\n+    bool useLdMatrix = canUseLdMatrix(srcTy, dstTy);\n     Attribute srcLayout = srcTy.getEncoding();\n     Attribute dstLayout = dstTy.getEncoding();\n     if (!isaDistributedLayout(srcLayout)) {\n@@ -523,6 +533,9 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> outNumCTAs(rank);\n     auto srcShapePerCTA = getShapePerCTA(srcLayout, srcTy.getShape());\n     auto dstShapePerCTA = getShapePerCTA(dstLayout, shape);\n+    if (useLdMatrix) {\n+      dstShapePerCTA[1] = 16; // todo: hack\n+    }\n \n     // For Volta, all the coords for a CTA are calculated.\n     bool isSrcMmaV1{}, isDstMmaV1{};\n@@ -540,20 +553,9 @@ struct ConvertLayoutOpConversion\n       isDstMmaV1 = sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n                    sliceLayout.getParent().cast<MmaEncodingAttr>().isVolta();\n     }\n-    if (srcLayout.isa<BlockedEncodingAttr>() &&\n-        dstLayout.isa<MmaEncodingAttr>() && !isDstMmaV1 &&\n-        triton::gpu::getWarpsPerCTA(dstLayout)[1] == 1 &&\n-        srcTy.getElementType().getIntOrFloatBitWidth() != 32) {\n-      enabled = true;\n-    }\n-    std::string enabled_str = enabled ? \"true\" : \"false\";\n-    std::cout << \"Enabled = \" << enabled_str << std::endl;\n     for (unsigned d = 0; d < rank; ++d) {\n       unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n       unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n-      if (enabled && d == 1) {\n-        outPerCTA = 16; // TODO: fix hack\n-      }\n       unsigned maxPerCTA = std::max(inPerCTA, outPerCTA);\n       numReplicates[d] = ceil<unsigned>(shape[d], maxPerCTA);\n       inNumCTAsEachRep[d] = maxPerCTA / inPerCTA;\n@@ -588,8 +590,8 @@ struct ConvertLayoutOpConversion\n       // We rely on shared layout constructor to calculate vec, perPhase, and\n       // maxPhase\n       repShape = getScratchConfigForCvtLayout(op, inVec, outVec, false);\n-      if (enabled)\n-        repShape[1] = ceil<unsigned>(repShape[1], 16) * 16;\n+      if (useLdMatrix)\n+        repShape[1] = ceil<unsigned>(repShape[1], 16) * 16; // todo: hack\n       auto dstDotOp = triton::gpu::DotOperandEncodingAttr::get(\n           getContext(), 0, dstLayout, dstTy.getElementType());\n       sharedLayout = triton::gpu::SharedEncodingAttr::get(\n@@ -628,7 +630,7 @@ struct ConvertLayoutOpConversion\n                                smemBase, shape,\n                                /*isDestMma=*/true);\n       } else {\n-        if (enabled) {\n+        if (useLdMatrix) {\n           processReplicaLdMatrix(loc, rewriter, /*stNotRd*/ false, dstTy,\n                                  outNumCTAsEachRep, multiDimRepId, outVec,\n                                  repShape, outOrd, outVals, smemBase,"}]