[{"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 13, "deletions": 7, "changes": 20, "file_content_changes": "@@ -11,6 +11,7 @@\n #include <numeric>\n \n using ::mlir::triton::gpu::BlockedEncodingAttr;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getShapePerCTA;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n@@ -35,18 +36,22 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   SmallVector<unsigned> paddedRepShape(rank);\n   auto srcBlockedLayout = srcLayout.dyn_cast<BlockedEncodingAttr>();\n   auto srcMmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>();\n+  auto srcDotLayout = srcLayout.dyn_cast<DotOperandEncodingAttr>();\n   auto dstBlockedLayout = dstLayout.dyn_cast<BlockedEncodingAttr>();\n   auto dstMmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>();\n-  assert((srcBlockedLayout || srcMmaLayout) &&\n+  auto dstDotLayout = dstLayout.dyn_cast<DotOperandEncodingAttr>();\n+  assert((srcBlockedLayout || srcMmaLayout || srcDotLayout) &&\n          \"Unexpected srcLayout in getScratchConfigForCvtLayout\");\n-  assert((dstBlockedLayout || dstMmaLayout) &&\n+  assert((dstBlockedLayout || dstMmaLayout || dstDotLayout) &&\n          \"Unexpected dstLayout in getScratchConfigForCvtLayout\");\n   assert(!(srcMmaLayout && dstMmaLayout) &&\n          \"Unexpected mma -> mma layout conversion\");\n-  auto inOrd =\n-      srcMmaLayout ? dstBlockedLayout.getOrder() : srcBlockedLayout.getOrder();\n-  auto outOrd =\n-      dstMmaLayout ? srcBlockedLayout.getOrder() : dstBlockedLayout.getOrder();\n+  assert(!(srcDotLayout && dstDotLayout) &&\n+         \"Unexpected dotOperand -> dotOperand layout conversion\");\n+  auto inOrd = (srcMmaLayout || srcDotLayout) ? dstBlockedLayout.getOrder()\n+                                              : srcBlockedLayout.getOrder();\n+  auto outOrd = (dstMmaLayout || dstDotLayout) ? srcBlockedLayout.getOrder()\n+                                               : dstBlockedLayout.getOrder();\n   unsigned srcContigPerThread =\n       srcBlockedLayout ? srcBlockedLayout.getSizePerThread()[inOrd[0]] : 2;\n   unsigned dstContigPerThread =\n@@ -138,7 +143,8 @@ class AllocationAnalysis {\n       auto dstEncoding = dstTy.getEncoding();\n       if (srcEncoding.isa<SharedEncodingAttr>() ||\n           dstEncoding.isa<SharedEncodingAttr>()) {\n-        // Only blocked -> blocked conversion requires for scratch allocation\n+        // Only conversions between the blocked and mma layouts require scratch\n+        // allocation.\n         return;\n       }\n       // ConvertLayoutOp with both input/output non-shared_layout"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 25, "deletions": 6, "changes": 31, "file_content_changes": "@@ -100,6 +100,25 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n            \"mmaLayout version = 1 is not implemented yet\");\n     return {16 * mmaLayout.getWarpsPerCTA()[0],\n             8 * mmaLayout.getWarpsPerCTA()[1]};\n+  } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n+    auto parentLayout = dotLayout.getParent();\n+    assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n+    if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n+      assert(parentMmaLayout.getVersion() == 2 &&\n+             \"mmaLayout version = 1 is not implemented yet\");\n+      auto parentShapePerCTA = getShapePerCTA(parentLayout);\n+      auto opIdx = dotLayout.getOpIdx();\n+      if (opIdx == 0) {\n+        return {parentShapePerCTA[0], 16};\n+      } else if (opIdx == 1) {\n+        return {16, parentShapePerCTA[1]};\n+      } else {\n+        assert(0 && \"DotOperandEncodingAttr opIdx must be 0 or 1\");\n+      }\n+    } else {\n+      assert(0 && \"DotOperandEncodingAttr non-MmaEncodingAttr parent not \"\n+                  \"supported yet\");\n+    }\n   } else {\n     assert(0 && \"Unimplemented usage of getShapePerCTA\");\n   }\n@@ -550,11 +569,11 @@ struct TritonGPUInferLayoutInterface\n                                           Optional<Location> location) const {\n     auto sliceEncoding = operandEncoding.dyn_cast<SliceEncodingAttr>();\n     if (!sliceEncoding)\n-      return emitOptionalError(location,\n-          \"ExpandDimsOp operand encoding must be SliceEncodingAttr\");\n+      return emitOptionalError(\n+          location, \"ExpandDimsOp operand encoding must be SliceEncodingAttr\");\n     if (sliceEncoding.getDim() != axis)\n-      return emitOptionalError(location,\n-          \"Incompatible slice dimension for ExpandDimsOp operand\");\n+      return emitOptionalError(\n+          location, \"Incompatible slice dimension for ExpandDimsOp operand\");\n     resultEncoding = sliceEncoding.getParent();\n     return success();\n   }\n@@ -568,8 +587,8 @@ struct TritonGPUInferLayoutInterface\n       if (retEncoding != dotOpEnc.getParent())\n         return emitOptionalError(location, \"Incompatible parent encoding\");\n     } else\n-      return emitOptionalError(location,\n-        \"Dot's a/b's encoding should be of DotOperandEncodingAttr\");\n+      return emitOptionalError(\n+          location, \"Dot's a/b's encoding should be of DotOperandEncodingAttr\");\n     return success();\n   }\n };"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 12, "deletions": 24, "changes": 36, "file_content_changes": "@@ -12,21 +12,13 @@\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"mlir/Transforms/RegionUtils.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n \n #include <memory>\n \n using namespace mlir;\n-\n-static bool isSharedLayout(Value v) {\n-  if (auto tensorType = v.getType().dyn_cast<RankedTensorType>()) {\n-    Attribute encoding = tensorType.getEncoding();\n-    return encoding.isa<triton::gpu::SharedEncodingAttr>();\n-  }\n-  return false;\n-}\n-\n namespace {\n #include \"TritonGPUCombine.inc\"\n \n@@ -189,9 +181,9 @@ static LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n inline bool expensive_to_remat(Operation *op) {\n   if (!op)\n     return true;\n-  if (isa<triton::gpu::ExtractSliceOp, triton::gpu::AllocTensorOp,\n-          triton::gpu::InsertSliceAsyncOp, triton::LoadOp, triton::StoreOp,\n-          triton::DotOp>(op))\n+  if (isa<tensor::ExtractSliceOp, triton::gpu::ExtractSliceOp,\n+          triton::gpu::AllocTensorOp, triton::gpu::InsertSliceAsyncOp,\n+          triton::LoadOp, triton::StoreOp, triton::DotOp>(op))\n     return true;\n   if (isa<scf::YieldOp, scf::ForOp>(op))\n     return true;\n@@ -240,8 +232,8 @@ class RematerializeBackward : public mlir::RewritePattern {\n     if (!op)\n       return mlir::failure();\n     // we don't want to rematerialize any conversion to/from shared\n-    if (isSharedLayout(cvt->getResults()[0]) ||\n-        isSharedLayout(cvt->getOperand(0)))\n+    if (isSharedEncoding(cvt->getResults()[0]) ||\n+        isSharedEncoding(cvt->getOperand(0)))\n       return mlir::failure();\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accomodate fused attention\n@@ -343,8 +335,6 @@ class RematerializeBackward : public mlir::RewritePattern {\n //\n // -----------------------------------------------------------------------------\n \n-// int test = 0;\n-\n class MoveConvertOutOfLoop : public mlir::RewritePattern {\n public:\n   MoveConvertOutOfLoop(mlir::MLIRContext *context)\n@@ -550,15 +540,13 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto oldAType = a.getType().cast<RankedTensorType>();\n     auto oldBType = b.getType().cast<RankedTensorType>();\n     auto newAType = RankedTensorType::get(\n-      oldAType.getShape(), oldAType.getElementType(),\n-      triton::gpu::DotOperandEncodingAttr::get(oldAType.getContext(), 0,\n-                                               newRetType.getEncoding())\n-    );\n+        oldAType.getShape(), oldAType.getElementType(),\n+        triton::gpu::DotOperandEncodingAttr::get(oldAType.getContext(), 0,\n+                                                 newRetType.getEncoding()));\n     auto newBType = RankedTensorType::get(\n-      oldBType.getShape(), oldBType.getElementType(),\n-      triton::gpu::DotOperandEncodingAttr::get(oldBType.getContext(), 1,\n-                                               newRetType.getEncoding())\n-    );\n+        oldBType.getShape(), oldBType.getElementType(),\n+        triton::gpu::DotOperandEncodingAttr::get(oldBType.getContext(), 1,\n+                                                 newRetType.getEncoding()));\n     a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n     b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n     auto newDot = rewriter.create<triton::DotOp>("}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 28, "deletions": 33, "changes": 61, "file_content_changes": "@@ -72,6 +72,7 @@ class LoopPipeliner {\n   /// compute type of shared buffers (with swizzled shared layouts)\n   RankedTensorType getSwizzleType(ttg::DotOperandEncodingAttr dotOpEnc,\n                                   RankedTensorType tensorType);\n+\n public:\n   LoopPipeliner(scf::ForOp forOp, int numStages)\n       : forOp(forOp), numStages(numStages) {\n@@ -140,14 +141,14 @@ bool LoopPipeliner::isDirectUserOfAsyncLoad(Operation &op) {\n   return false;\n }\n \n-ttg::AllocTensorOp\n-LoopPipeliner::allocateEmptyBuffer(Operation *op, OpBuilder &builder) {\n+ttg::AllocTensorOp LoopPipeliner::allocateEmptyBuffer(Operation *op,\n+                                                      OpBuilder &builder) {\n   // allocate a buffer for each pipelined tensor\n   // shape: e.g. (numStages==4), <32x64xbf16> -> <4x32x64xbf16>\n   Value convertLayout = loadsMapping[op->getResult(0)];\n   if (auto tensorType = convertLayout.getType().dyn_cast<RankedTensorType>()) {\n-    return builder.create<ttg::AllocTensorOp>(convertLayout.getLoc(),\n-                                            loadsBufferType[op->getResult(0)]);\n+    return builder.create<ttg::AllocTensorOp>(\n+        convertLayout.getLoc(), loadsBufferType[op->getResult(0)]);\n   }\n   llvm_unreachable(\"Async copy's return should be of RankedTensorType\");\n }\n@@ -168,7 +169,7 @@ LoopPipeliner::getSwizzleType(ttg::DotOperandEncodingAttr dotOpEnc,\n     order = tyEncoding.getOrder();\n     // number of rows per phase\n     perPhase = 128 / (ty.getShape()[order[0]] *\n-                          (ty.getElementType().getIntOrFloatBitWidth() / 8));\n+                      (ty.getElementType().getIntOrFloatBitWidth() / 8));\n     perPhase = std::max<int>(perPhase, 1);\n \n     // index of the inner dimension in `order`\n@@ -180,33 +181,31 @@ LoopPipeliner::getSwizzleType(ttg::DotOperandEncodingAttr dotOpEnc,\n     } else if (version == 2) {\n       auto eltTy = ty.getElementType();\n       std::vector<size_t> mat_shape = {8, 8,\n-                                        2 * 64 / eltTy.getIntOrFloatBitWidth()};\n+                                       2 * 64 / eltTy.getIntOrFloatBitWidth()};\n       // for now, disable swizzle when using transposed int8 tensor cores\n       if (ty.getElementType().isInteger(8) && order[0] == inner)\n         perPhase = 1;\n       else {\n-        if (opIdx == 0) {  // compute swizzling for A operand\n+        if (opIdx == 0) { // compute swizzling for A operand\n           vec = order[0] == 1 ? mat_shape[2] : mat_shape[0]; // k : m\n           int mmaStride = order[0] == 1 ? mat_shape[0] : mat_shape[2];\n           maxPhase = mmaStride / perPhase;\n-        } else if (opIdx == 1) {  // compute swizzling for B operand\n+        } else if (opIdx == 1) { // compute swizzling for B operand\n           vec = order[0] == 1 ? mat_shape[1] : mat_shape[2]; // n : k\n           int mmaStride = order[0] == 1 ? mat_shape[2] : mat_shape[1];\n           maxPhase = mmaStride / perPhase;\n         } else\n           llvm_unreachable(\"invalid operand index\");\n       }\n-    } else  // version not in [1, 2]\n+    } else // version not in [1, 2]\n       llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n-  } else {  // If the layout of dot is not mma, we don't need to swizzle\n+  } else { // If the layout of dot is not mma, we don't need to swizzle\n     auto blockedEnc = dotOpEnc.getParent().cast<ttg::BlockedEncodingAttr>();\n     order = blockedEnc.getOrder();\n   }\n-  auto newEncoding = ttg::SharedEncodingAttr::get(\n-    ty.getContext(), vec, perPhase, maxPhase, order\n-  );\n-  SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n-                                   ty.getShape().end());\n+  auto newEncoding = ttg::SharedEncodingAttr::get(ty.getContext(), vec,\n+                                                  perPhase, maxPhase, order);\n+  SmallVector<int64_t> bufferShape(ty.getShape().begin(), ty.getShape().end());\n   bufferShape.insert(bufferShape.begin(), numStages);\n   return RankedTensorType::get(bufferShape, ty.getElementType(), newEncoding);\n }\n@@ -255,17 +254,16 @@ LogicalResult LoopPipeliner::initialize() {\n     if (isCandiate && loadOp.getResult().hasOneUse()) {\n       isCandiate = false;\n       Operation *use = *loadOp.getResult().getUsers().begin();\n-      if (auto convertLayout =\n-              llvm::dyn_cast<ttg::ConvertLayoutOp>(use)) {\n+      if (auto convertLayout = llvm::dyn_cast<ttg::ConvertLayoutOp>(use)) {\n         if (auto tensorType = convertLayout.getResult()\n                                   .getType()\n                                   .dyn_cast<RankedTensorType>()) {\n           if (auto dotOpEnc = tensorType.getEncoding()\n-                                     .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n+                                  .dyn_cast<ttg::DotOperandEncodingAttr>()) {\n             isCandiate = true;\n             loadsMapping[loadOp] = convertLayout;\n-            loadsBufferType[loadOp] = getSwizzleType(dotOpEnc,\n-                                    loadOp.getType().cast<RankedTensorType>());\n+            loadsBufferType[loadOp] = getSwizzleType(\n+                dotOpEnc, loadOp.getType().cast<RankedTensorType>());\n           }\n         }\n       }\n@@ -370,8 +368,7 @@ void LoopPipeliner::emitPrologue() {\n             if (auto loadOp = llvm::dyn_cast<triton::LoadOp>(newOp)) {\n               return loadOp.mask();\n             } else if (auto insertSliceAsyncOp =\n-                           llvm::dyn_cast<ttg::InsertSliceAsyncOp>(\n-                               newOp)) {\n+                           llvm::dyn_cast<ttg::InsertSliceAsyncOp>(newOp)) {\n               return insertSliceAsyncOp.mask();\n             } else {\n               return mlir::Value();\n@@ -423,9 +420,9 @@ void LoopPipeliner::emitPrologue() {\n   loopIterIdx = builder.create<arith::ConstantIntOp>(iv.getLoc(), 0, 32);\n   for (Value loadOp : loads) {\n     auto sliceType = loadsMapping[loadOp].getType().cast<RankedTensorType>();\n-    sliceType = RankedTensorType::get(sliceType.getShape(),\n-                                      sliceType.getElementType(),\n-                                      loadsBufferType[loadOp].getEncoding());\n+    sliceType =\n+        RankedTensorType::get(sliceType.getShape(), sliceType.getElementType(),\n+                              loadsBufferType[loadOp].getEncoding());\n     Value extractSlice = builder.create<tensor::ExtractSliceOp>(\n         loadOp.getLoc(), sliceType, loadStageBuffer[loadOp][numStages - 1],\n         SmallVector<OpFoldResult>{intAttr(0), intAttr(0), intAttr(0)},\n@@ -618,14 +615,12 @@ scf::ForOp LoopPipeliner::createNewForOp() {\n           auto tensorType = dotOperand.getType().cast<RankedTensorType>();\n           if (!tensorType.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n             auto newEncoding = ttg::DotOperandEncodingAttr::get(\n-              tensorType.getContext(), opIdx, dotType.getEncoding()\n-            );\n-            auto newType = RankedTensorType::get(tensorType.getShape(),\n-                                                tensorType.getElementType(),\n-                                                newEncoding);\n-            return builder.create<ttg::ConvertLayoutOp>(\n-              dotOperand.getLoc(), newType, dotOperand\n-            );\n+                tensorType.getContext(), opIdx, dotType.getEncoding());\n+            auto newType =\n+                RankedTensorType::get(tensorType.getShape(),\n+                                      tensorType.getElementType(), newEncoding);\n+            return builder.create<ttg::ConvertLayoutOp>(dotOperand.getLoc(),\n+                                                        newType, dotOperand);\n           }\n           return dotOperand;\n         };"}, {"filename": "test/Analysis/test-alias.mlir", "status": "modified", "additions": 50, "deletions": 49, "changes": 99, "file_content_changes": "@@ -2,11 +2,14 @@\n \n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#A = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#B = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n+#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n // CHECK-LABEL: matmul_loop\n+// There shouldn't be any aliasing with the dot op encoding.\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n@@ -19,12 +22,10 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n   %b_off = arith.constant dense<4> : tensor<32x128xi32, #BL>\n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<128x32xf16, #AL>\n-    // CHECK: %4 -> %4\n-    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<32x128xf16, #BL>\n-    // CHECK-NEXT: %6 -> %6 \n-    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n-    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n+    %c = tt.dot %a, %b, %prev_c {transA = false, transB = false, allowTF32 = true} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n     %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n@@ -36,18 +37,18 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n // CHECK-LABEL: alloc\n func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n   // CHECK: %0 -> %0\n-  %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A>\n+  %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: convert\n func @convert(%A : !tt.ptr<f16>) {\n   %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: %0 -> %0\n-  %cst1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A>\n+  %cst1 = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n@@ -57,134 +58,134 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: %cst_0 -> %cst_0\n-  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   // CHECK: %2 -> %cst_0\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: extract_slice\n func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n   // CHECK-NEXT: %0 -> %cst\n-  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A> -> tensor<16x16xf16, #A>\n+  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A_SHARED> -> tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: if_cat\n func @if_cat(%i1 : i1) {\n   // CHECK: %cst -> %cst\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: %cst_0 -> %cst_0\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: %0 -> %1,%1\n-  %cst2 = scf.if %i1 -> tensor<32x16xf16, #A> {\n+  %cst2 = scf.if %i1 -> tensor<32x16xf16, #A_SHARED> {\n     // CHECK: %1 -> %1\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n-    scf.yield %a : tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %a : tensor<32x16xf16, #A_SHARED>\n   } else {\n     // CHECK: %1 -> %1\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n-    scf.yield %b : tensor<32x16xf16, #A>\n+    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %b : tensor<32x16xf16, #A_SHARED>\n   }\n   return\n }\n \n // CHECK-LABEL: if_alias\n func @if_alias(%i1 : i1) {\n   // CHECK: %cst -> %cst\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: %0 -> %cst,%cst_0\n-  %cst2 = scf.if %i1 -> tensor<16x16xf16, #A> {\n-    scf.yield %cst0 : tensor<16x16xf16, #A>\n+  %cst2 = scf.if %i1 -> tensor<16x16xf16, #A_SHARED> {\n+    scf.yield %cst0 : tensor<16x16xf16, #A_SHARED>\n   } else {\n-    scf.yield %cst1 : tensor<16x16xf16, #A>\n+    scf.yield %cst1 : tensor<16x16xf16, #A_SHARED>\n   }\n   return\n }\n \n // CHECK-LABEL: for\n func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   // CHECK: %cst -> %cst\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_1 -> %cst_1\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %arg6 -> %cst\n   // CHECK-NEXT: %arg7 -> %cst_0\n   // CHECK-NEXT: %arg8 -> %cst_1\n   // CHECK-NEXT: %0#0 -> %cst,%cst_0\n   // CHECK-NEXT: %0#1 -> %cst,%cst_0\n   // CHECK-NEXT: %0#2 -> %cst,%cst_0\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n-    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n }\n \n // CHECK-LABEL: for_if\n func @for_if(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_1 -> %cst_1\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %arg7 -> %cst\n   // CHECK-NEXT: %arg8 -> %cst_0\n   // CHECK-NEXT: %arg9 -> %cst_1\n   // CHECK-NEXT: %0#0 -> %cst,%cst_0\n   // CHECK-NEXT: %0#1 -> %cst,%cst_0\n   // CHECK-NEXT: %0#2 -> %cst,%cst_0\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.if %i1 {\n       %index = arith.constant 8 : i32\n       // CHECK-NEXT: %1 -> %cst,%cst_0\n-      %cst0 = triton_gpu.extract_slice %a_shared, %index { axis = 0 : i32 } : tensor<128x32xf16, #A> -> tensor<32xf16, #A>\n+      %cst0 = triton_gpu.extract_slice %a_shared, %index { axis = 0 : i32 } : tensor<128x32xf16, #A_SHARED> -> tensor<32xf16, #A_SHARED>\n       scf.yield\n     }\n-    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n }\n \n // CHECK-LABEL: for_if_for\n func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: %cst -> %cst\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_0 -> %cst_0\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %cst_1 -> %cst_1\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: %arg7 -> %cst\n   // CHECK-NEXT: %arg8 -> %cst_0\n   // CHECK-NEXT: %arg9 -> %cst_1\n   // CHECK-NEXT: %0#0 -> %cst\n   // CHECK-NEXT: %0#1 -> %cst_0\n   // CHECK-NEXT: %0#2 -> %cst_2,%cst_2\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     // CHECK-NEXT: %arg11 -> %cst_1,%cst_2,%cst_2\n     // CHECK-NEXT: %1 -> %cst_2,%cst_2\n-    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A>) {\n+    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A_SHARED>) {\n       // CHECK-NEXT: %2 -> %cst_2,%cst_2\n-      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A> {\n+      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A_SHARED> {\n         // CHECK-NEXT: %cst_2 -> %cst_2\n-        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-        scf.yield %cst0 : tensor<128x32xf16, #A>\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n       } else {\n         // CHECK-NEXT: %cst_2 -> %cst_2\n-        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-        scf.yield %cst0 : tensor<128x32xf16, #A>\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n       }\n-      scf.yield %c_shared_next_next : tensor<128x32xf16, #A>\n+      scf.yield %c_shared_next_next : tensor<128x32xf16, #A_SHARED>\n     }\n-    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n }"}, {"filename": "test/Analysis/test-allocation.mlir", "status": "modified", "additions": 95, "deletions": 95, "changes": 190, "file_content_changes": "@@ -3,11 +3,11 @@\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#A_SMEM = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#B_SMEM = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n-#A = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n-#B = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n+#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n+#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n // CHECK-LABEL: matmul_loop\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n@@ -25,20 +25,20 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-    // CHECK: offset = 0, size = 8192\n-    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+    // CHECK: offset = 0, size = 4608\n+    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n-    // CHECK-NEXT: offset = 8192, size = 8192\n-    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n+    // CHECK-NEXT: offset = 0, size = 4224\n+    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n \n-    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n     %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n     scf.yield %next_a_ptr, %next_b_ptr, %c : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>\n   }\n   return\n-  // CHECK-NEXT: size = 16384\n+  // CHECK-NEXT: size = 4608\n }\n \n // Shared memory is available after a tensor's liveness range ends\n@@ -53,21 +53,21 @@ func @reusable(%A : !tt.ptr<f16>) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #AL>\n   %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  // CHECK-NEXT: offset = 0, size = 8192\n-  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+  // CHECK-NEXT: offset = 0, size = 4608\n+  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n   %a2_ = tt.load %b_ptr, %cst3, %cst4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #AL>\n-  // CHECK-NEXT: offset = 8192, size = 8192\n-  %a2 = triton_gpu.convert_layout %a2_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #B>\n+  // CHECK-NEXT: offset = 0, size = 1152\n+  %a2 = triton_gpu.convert_layout %a2_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #B_DOT>\n   %a3_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n-  // CHECK-NEXT: offset = 16384, size = 8192\n-  %a3 = triton_gpu.convert_layout %a3_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n-  %c = tt.dot %a1, %a2, %c_init {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+  // CHECK-NEXT: offset = 0, size = 4608\n+  %a3 = triton_gpu.convert_layout %a3_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n+  %c = tt.dot %a1, %a2, %c_init {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n   %a4_ = tt.load %b_ptr, %cst3, %cst4 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #AL>\n-  // CHECK-NEXT: offset = 0, size = 8192\n-  %a4 = triton_gpu.convert_layout %a4_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #A>\n-  %c1 = tt.dot %a3, %a4, %c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+  // CHECK-NEXT: offset = 0, size = 1152\n+  %a4 = triton_gpu.convert_layout %a4_ : (tensor<32x128xf16, #AL>) -> tensor<32x128xf16, #B_DOT>\n+  %c1 = tt.dot %a3, %a4, %c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n   return\n-  // CHECK-NEXT: size = 24576\n+  // CHECK-NEXT: size = 4608\n }\n \n // A tensor's shared memory offset is larger than it needs to accommodate further tensors\n@@ -77,33 +77,33 @@ func @reusable(%A : !tt.ptr<f16>) {\n // CHECK-LABEL: preallocate\n func @preallocate(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 512\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 512\n-  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 2048, size = 1024\n-  %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 3072, size = 1024\n-  %b = tt.cat %cst0, %cst2 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %b = tt.cat %cst0, %cst2 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 0, size = 1024\n-  %c = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %c = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 1024\n-  %cst4 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A>\n+  %cst4 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 6144, size = 2048\n-  %e = tt.cat %a, %cst4 {axis = 0} : (tensor<32x16xf16, #A>, tensor<32x16xf16, #A>) -> tensor<64x16xf16, #A>\n+  %e = tt.cat %a, %cst4 {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 2048\n-  %d = tt.cat %b, %cst4 {axis = 0} : (tensor<32x16xf16, #A>, tensor<32x16xf16, #A>) -> tensor<64x16xf16, #A>\n+  %d = tt.cat %b, %cst4 {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 10240, size = 2048\n-  %f = tt.cat %c, %cst4 {axis = 0} : (tensor<32x16xf16, #A>, tensor<32x16xf16, #A>) -> tensor<64x16xf16, #A>\n+  %f = tt.cat %c, %cst4 {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 0, size = 2048\n-  %cst5 = arith.constant dense<0.000000e+00> : tensor<64x16xf16, #A>\n+  %cst5 = arith.constant dense<0.000000e+00> : tensor<64x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 2048, size = 4096\n-  %g = tt.cat %e, %cst5 {axis = 0} : (tensor<64x16xf16, #A>, tensor<64x16xf16, #A>) -> tensor<128x16xf16, #A>\n+  %g = tt.cat %e, %cst5 {axis = 0} : (tensor<64x16xf16, #A_SHARED>, tensor<64x16xf16, #A_SHARED>) -> tensor<128x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 2048, size = 4096\n-  %h = tt.cat %d, %cst5 {axis = 0} : (tensor<64x16xf16, #A>, tensor<64x16xf16, #A>) -> tensor<128x16xf16, #A>\n+  %h = tt.cat %d, %cst5 {axis = 0} : (tensor<64x16xf16, #A_SHARED>, tensor<64x16xf16, #A_SHARED>) -> tensor<128x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 2048, size = 4096\n-  %i = tt.cat %f, %cst5 {axis = 0} : (tensor<64x16xf16, #A>, tensor<64x16xf16, #A>) -> tensor<128x16xf16, #A>\n+  %i = tt.cat %f, %cst5 {axis = 0} : (tensor<64x16xf16, #A_SHARED>, tensor<64x16xf16, #A_SHARED>) -> tensor<128x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 12288\n }\n@@ -112,13 +112,13 @@ func @preallocate(%A : !tt.ptr<f16>) {\n // CHECK-LABEL: unused\n func @unused(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 1024\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 0, size = 512\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 1024\n-  %a = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n   // CHECK: size = 2048\n }\n@@ -127,38 +127,38 @@ func @unused(%A : !tt.ptr<f16>) {\n // CHECK-LABEL: longlive\n func @longlive(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 512\n-  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 1024\n-  %a = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst1, %cst2 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 512\n-  %cst4 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst4 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 1024\n-  %b = tt.cat %cst3, %cst4 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %b = tt.cat %cst3, %cst4 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 512\n-  %cst5 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst5 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 512\n-  %cst6 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst6 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1536, size = 1024\n-  %c = tt.cat %cst3, %cst4 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %c = tt.cat %cst3, %cst4 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 1024\n-  %d = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %d = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 2560\n }\n \n // CHECK-LABEL: alloc\n func @alloc(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   %cst1 = arith.constant dense<0.000000e+00> : tensor<16x32xf16, #AL>\n   // CHECK-NEXT: offset = 0, size = 512\n-  %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SMEM>\n+  %cst2 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 512\n }\n@@ -178,19 +178,19 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n   // CHECK: offset = 0, size = 512\n-  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SMEM>\n+  %tensor = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SMEM>\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 512\n }\n \n // CHECK-LABEL: extract_slice\n func @extract_slice(%A : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SMEM>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n-  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A_SMEM> -> tensor<16x16xf16, #A_SMEM>\n+  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A_SHARED> -> tensor<16x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 512\n }\n@@ -200,21 +200,21 @@ func @extract_slice(%A : !tt.ptr<f16>) {\n // CHECK-LABEL: if\n func @if(%i1 : i1) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     // CHECK-NEXT: offset = 1024, size = 1024\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     // CHECK-NEXT: offset = 1024, size = 1024\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: offset = 0, size = 512\n-  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 1024, size = 1024\n-  %a = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 2048\n }\n@@ -224,24 +224,24 @@ func @if(%i1 : i1) {\n // CHECK-LABEL: if_else\n func @if_else(%i1 : i1) {\n   // CHECK: offset = 0, size = 512\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 512, size = 512\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     // CHECK-NEXT: offset = 1024, size = 1024\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     // CHECK-NEXT: offset = 1024, size = 1024\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   } else {\n     // CHECK-NEXT: offset = 1024, size = 512\n-    %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+    %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n     // CHECK-NEXT: offset = 1536, size = 512\n-    %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+    %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n     // CHECK-NEXT: offset = 2048, size = 1024\n-    %a = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: offset = 1024, size = 1024\n-  %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 3072\n }\n@@ -251,13 +251,13 @@ func @if_else(%i1 : i1) {\n // CHECK-LABEL: for\n func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   // CHECK: offset = 0, size = 8192\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 16384, size = 8192\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n-    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n   // CHECK-NEXT: size = 24576\n@@ -266,18 +266,18 @@ func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.p\n // CHECK-LABEL: for_if_slice\n func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SMEM>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SMEM>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 16384, size = 8192\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SMEM>\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SMEM>, tensor<128x32xf16, #A_SMEM>, tensor<128x32xf16, #A_SMEM>) {\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     scf.if %i1 {\n       %index = arith.constant 8 : i32\n-      %cst0 = triton_gpu.extract_slice %a_shared, %index { axis = 0 : i32 } : tensor<128x32xf16, #A_SMEM> -> tensor<32xf16, #A_SMEM>\n+      %cst0 = triton_gpu.extract_slice %a_shared, %index { axis = 0 : i32 } : tensor<128x32xf16, #A_SHARED> -> tensor<32xf16, #A_SHARED>\n       scf.yield\n     }\n-    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SMEM>, tensor<128x32xf16, #A_SMEM>, tensor<128x32xf16, #A_SMEM>\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n   // CHECK-NEXT: size = 24576\n@@ -288,28 +288,28 @@ func @for_if_slice(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %\n // CHECK-LABEL: for_if_for\n func @for_if_for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>, %i1 : i1) {\n   // CHECK: offset = 0, size = 8192\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 8192, size = 8192\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: offset = 16384, size = 8192\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n-    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A>) {\n-      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A> {\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %c_shared_next = scf.for %jv = %lb to %ub step %step iter_args(%c_shared_next = %c_shared) -> (tensor<128x32xf16, #A_SHARED>) {\n+      %c_shared_next_next = scf.if %i1 -> tensor<128x32xf16, #A_SHARED> {\n         // CHECK-NEXT: offset = 24576, size = 8192\n-        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-        scf.yield %cst0 : tensor<128x32xf16, #A>\n+        %cst0 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst0 : tensor<128x32xf16, #A_SHARED>\n       } else {\n         // CHECK-NEXT: offset = 32768, size = 8192\n-        %cst1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-        scf.yield %cst1 : tensor<128x32xf16, #A>\n+        %cst1 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+        scf.yield %cst1 : tensor<128x32xf16, #A_SHARED>\n       }\n-      scf.yield %c_shared_next_next : tensor<128x32xf16, #A>\n+      scf.yield %c_shared_next_next : tensor<128x32xf16, #A_SHARED>\n     }\n-    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    scf.yield %a_shared, %b_shared, %c_shared_next : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: offset = 0, size = 8192\n-  %cst2 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %cst2 = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   return\n   // CHECK-NEXT: size = 40960\n }"}, {"filename": "test/Analysis/test-membar.mlir", "status": "modified", "additions": 75, "deletions": 73, "changes": 148, "file_content_changes": "@@ -3,11 +3,14 @@\n #AL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n #sliceAd0 = #triton_gpu.slice<{dim = 0, parent = #AL}>\n #BL = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n-#A = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n-#B = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#A_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n+#B_SHARED = #triton_gpu.shared<{vec = 2, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n #C = #triton_gpu.mma<{version = 2, warpsPerCTA = [4, 1]}>\n+#A_DOT = #triton_gpu.dot_op<{opIdx = 0, parent = #C}>\n+#B_DOT = #triton_gpu.dot_op<{opIdx = 1, parent = #C}>\n \n // CHECK-LABEL: matmul_loop\n+// There shouldn't be any membar with the dot op encoding.\n func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n   %a_ptr_init = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %b_ptr_init = tt.broadcast %B : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n@@ -23,11 +26,10 @@ func @matmul_loop(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B\n \n   scf.for %iv = %lb to %ub step %step iter_args(%a_ptr = %a_ptr_init, %b_ptr = %b_ptr_init, %prev_c = %c_init) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32x128x!tt.ptr<f16>, #BL>, tensor<128x128xf32, #C>) {\n     %a_ = tt.load %a_ptr, %a_mask, %a_other {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<128x32xf16, #AL>\n-    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+    %a = triton_gpu.convert_layout %a_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_DOT>\n     %b_ = tt.load %b_ptr, %b_mask, %b_other {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<32x128xf16, #BL>\n-    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B>\n-    // CHECK: Membar 13\n-    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A> * tensor<32x128xf16, #B> -> tensor<128x128xf32, #C>\n+    %b = triton_gpu.convert_layout %b_ : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #B_DOT>\n+    %c = tt.dot %a, %b, %prev_c {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #A_DOT> * tensor<32x128xf16, #B_DOT> -> tensor<128x128xf32, #C>\n \n     %next_a_ptr = tt.addptr %a_ptr, %a_off : tensor<128x32x!tt.ptr<f16>, #AL>\n     %next_b_ptr = tt.addptr %b_ptr, %b_off : tensor<32x128x!tt.ptr<f16>, #BL>\n@@ -42,9 +44,9 @@ func @raw_single_block(%A : !tt.ptr<f16>) {\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<128x32xf16, #AL>\n-  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n   // CHECK: Membar 5\n-  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A>) -> tensor<128x32xf16, #A>\n+  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #A_SHARED>\n   return\n }\n \n@@ -54,56 +56,56 @@ func @war_single_block(%A : !tt.ptr<f16>) {\n   %cst2 = arith.constant dense<0.000000e+00> : tensor<128x32xf16, #AL>\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n   %a1_ = tt.load %a_ptr, %cst1, %cst2 {cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<128x32xf16, #AL>\n-  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+  %a1 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n   // CHECK: Membar 5\n-  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A>) -> tensor<128x32xf16, #AL>\n+  %a2 = triton_gpu.convert_layout %a1 : (tensor<128x32xf16, #A_SHARED>) -> tensor<128x32xf16, #AL>\n   // a2's liveness range ends here, and a3 and a2 have the same address range.\n   // So it makes sense to have a WAR dependency between a2 and a3.\n   // CHECK-NEXT: Membar 7\n-  %a3 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A>\n+  %a3 = triton_gpu.convert_layout %a1_ : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #A_SHARED>\n   return\n }\n \n // CHECK-LABEL: scratch\n func @scratch() {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: Membar 1\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK-NEXT: Membar 3\n-  %aa = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A>) -> tensor<32x16xf16, #AL>\n+  %aa = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   %b = tt.reduce %aa {redOp = 1 : i32, axis = 0 : i32} : tensor<32x16xf16, #AL> -> tensor<16xf16, #sliceAd0>\n   return\n }\n \n // CHECK-LABEL: async_wait\n func @async_wait() {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   // CHECK: Membar 1\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   triton_gpu.async_wait {num = 4 : i32}\n   // CHECK-NEXT: Membar 4\n-  %a_ = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A>) -> tensor<32x16xf16, #AL>\n+  %a_ = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: alloc\n func @alloc() {\n-  %cst0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A>\n-  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %cst0 = triton_gpu.alloc_tensor : tensor<16x16xf16, #A_SHARED>\n+  %a = tt.cat %cst0, %cst0 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   // CHECK: Membar 2\n-  %b = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A>) -> tensor<32x16xf16, #AL>\n+  %b = triton_gpu.convert_layout %a : (tensor<32x16xf16, #A_SHARED>) -> tensor<32x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: extract_slice\n func @extract_slice() {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n-  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A> -> tensor<16x16xf16, #A>\n+  %cst1 = triton_gpu.extract_slice %cst0, %index { axis = 0 : i32 } : tensor<1x16x16xf16, #A_SHARED> -> tensor<16x16xf16, #A_SHARED>\n   // CHECK: Membar 3\n-  %cst2 = triton_gpu.convert_layout %cst1 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  %cst2 = triton_gpu.convert_layout %cst1 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   // CHECK-NEXT: Membar 5\n-  %cst3 = triton_gpu.convert_layout %cst2 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A>\n+  %cst3 = triton_gpu.convert_layout %cst2 : (tensor<16x16xf16, #AL>) -> tensor<16x16xf16, #A_SHARED>\n   return\n }\n \n@@ -112,119 +114,119 @@ func @insert_slice_async(%A : !tt.ptr<f16>, %i1 : i1) {\n   %a_ptr = tt.broadcast %A : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #AL>\n   %mask = tt.splat %i1 : (i1) -> tensor<16x16xi1, #AL>\n   %other = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #AL>\n-  %tensor = triton_gpu.alloc_tensor : tensor<1x16x16xf16, #A>\n+  %tensor = triton_gpu.alloc_tensor : tensor<1x16x16xf16, #A_SHARED>\n   %index = arith.constant 0 : i32\n-  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A>\n-  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A>, tensor<1x16x16xf16, #A>) -> tensor<2x16x16xf16, #A>\n+  %a = triton_gpu.insert_slice_async %a_ptr, %tensor, %index, %mask, %other {axis = 0 : i32, cache = 1 : i32, evict = 1 : i32, isOtherUnspecified = false, isVolatile = false} : tensor<16x16x!tt.ptr<f16>, #AL> -> tensor<1x16x16xf16, #A_SHARED>\n+  %b = tt.cat %a, %a {axis = 0} : (tensor<1x16x16xf16, #A_SHARED>, tensor<1x16x16xf16, #A_SHARED>) -> tensor<2x16x16xf16, #A_SHARED>\n   // CHECK: Membar 7\n-  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A>, tensor<2x16x16xf16, #A>) -> tensor<4x16x16xf16, #A>\n+  %c = tt.cat %b, %b {axis = 0} : (tensor<2x16x16xf16, #A_SHARED>, tensor<2x16x16xf16, #A_SHARED>) -> tensor<4x16x16xf16, #A_SHARED>\n   return\n }\n \n // If branch inserted a barrier for %cst0 and %cst1, but else didn't, then the barrier should be inserted in the parent region\n // CHECK-LABEL: multi_blocks\n func @multi_blocks(%i1 : i1) {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   } else {\n-    %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-    %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+    %cst2 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+    %cst3 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n     // CHECK-NEXT: Membar 7\n-    %b = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %b = tt.cat %cst2, %cst3 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n   // CHECK-NEXT: Membar 10\n-  %c = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+  %c = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n   return\n }\n \n // Both branches inserted a barrier for %cst0 and %cst1, then the barrier doesn't need to be inserted in the parent region\n // CHECK-LABEL: multi_blocks_join_barrier\n func @multi_blocks_join_barrier(%i1 : i1) {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   } else {\n     // CHECK-NEXT: Membar 5\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   return\n }\n \n // Read yielded tensor requires a barrier\n // CHECK-LABEL: multi_blocks_yield\n func @multi_blocks_yield(%i1 : i1) {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %a = scf.if %i1 -> (tensor<32x16xf16, #A>) {\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %a = scf.if %i1 -> (tensor<32x16xf16, #A_SHARED>) {\n     // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n-    scf.yield %a : tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %a : tensor<32x16xf16, #A_SHARED>\n   } else {\n     // CHECK-NEXT: Membar 5\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n-    scf.yield %b : tensor<32x16xf16, #A>\n+    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n+    scf.yield %b : tensor<32x16xf16, #A_SHARED>\n   }\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   // CHECK-NEXT: Membar 9\n-  %b = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A>, tensor<32x16xf16, #A>) -> tensor<64x16xf16, #A>\n+  %b = tt.cat %a, %a {axis = 0} : (tensor<32x16xf16, #A_SHARED>, tensor<32x16xf16, #A_SHARED>) -> tensor<64x16xf16, #A_SHARED>\n   return\n }\n \n // Conservatively add a barrier as if the branch (%i1) is never taken\n // CHECK-LABEL: multi_blocks_noelse\n func @multi_blocks_noelse(%i1 : i1) {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     // CHECK: Membar 2\n-    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %a = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   return\n }\n \n // Conservatively add a barrier as if the branch (%i2) is never taken\n // CHECK-LABEL: multi_blocks_nested_scf\n func @multi_blocks_nested_scf(%i1 : i1, %i2 : i1) {\n-  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n-  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A>\n+  %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n+  %cst1 = arith.constant dense<0.000000e+00> : tensor<16x16xf16, #A_SHARED>\n   scf.if %i1 {\n     scf.if %i2 {\n       // CHECK: Membar 2\n-      %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+      %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n       scf.yield\n     }\n     scf.yield\n   } else {\n     // CHECK-NEXT: Membar 6\n-    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A>, tensor<16x16xf16, #A>) -> tensor<32x16xf16, #A>\n+    %b = tt.cat %cst0, %cst1 {axis = 0} : (tensor<16x16xf16, #A_SHARED>, tensor<16x16xf16, #A_SHARED>) -> tensor<32x16xf16, #A_SHARED>\n     scf.yield\n   }\n   // CHECK-NEXT: Membar 9\n-  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A>) -> tensor<16x16xf16, #AL>\n+  %a_ = triton_gpu.convert_layout %cst0 : (tensor<16x16xf16, #A_SHARED>) -> tensor<16x16xf16, #AL>\n   return\n }\n \n // CHECK-LABEL: for\n func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n     // CHECK-NEXT: Membar 3\n-    %cst0 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) -> tensor<256x32xf16, #A>\n-    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    %cst0 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    scf.yield %b_shared, %a_shared, %a_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   return\n }\n@@ -233,18 +235,18 @@ func @for(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.p\n // they are reassociated with aliases (c_shared) and thus require a barrier.\n // CHECK-LABEL: for_alias\n func @for_alias(%lb : index, %ub : index, %step : index, %A : !tt.ptr<f16>, %B : !tt.ptr<f16>) {\n-  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n+  %a_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %b_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n   // CHECK-NEXT: Membar 2\n-  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) -> tensor<256x32xf16, #A>\n-  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A>\n-  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) {\n-    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) -> tensor<256x32xf16, #A>\n+  %cst0 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+  %c_shared_init = arith.constant dense<0.00e+00> : tensor<128x32xf16, #A_SHARED>\n+  %a_shared, %b_shared, %c_shared = scf.for %iv = %lb to %ub step %step iter_args(%a_shared = %a_shared_init, %b_shared = %b_shared_init, %c_shared = %c_shared_init) -> (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) {\n+    %cst1 = tt.cat %a_shared_init, %b_shared_init {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n     // CHECK-NEXT: Membar 6\n-    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A>, tensor<128x32xf16, #A>) -> tensor<256x32xf16, #A>\n-    scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A>, tensor<128x32xf16, #A>, tensor<128x32xf16, #A>\n+    %cst2 = tt.cat %a_shared, %b_shared {axis = 0} : (tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>) -> tensor<256x32xf16, #A_SHARED>\n+    scf.yield %c_shared, %a_shared, %b_shared : tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>, tensor<128x32xf16, #A_SHARED>\n   }\n   // CHECK-NEXT: Membar 9\n-  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A>, tensor<256x32xf16, #A>) -> tensor<512x32xf16, #A>\n+  %cst3 = tt.cat %cst0, %cst0 {axis = 0} : (tensor<256x32xf16, #A_SHARED>, tensor<256x32xf16, #A_SHARED>) -> tensor<512x32xf16, #A_SHARED>\n   return\n }"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 17, "deletions": 15, "changes": 32, "file_content_changes": "@@ -644,26 +644,28 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n // -----\n \n+// TODO(Keren): Enable this test when conversion from blocked to dot is implemented.\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n-#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n #mma0 = #triton_gpu.mma<{version=2, warpsPerCTA=[1,1]}>\n+#dot0 = #triton_gpu.dot_op<{opIdx = 0, parent = #mma0}>\n+#dot1 = #triton_gpu.dot_op<{opIdx = 1, parent = #mma0}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK-LABEL: convert_dot\n   func @convert_dot(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) {\n-    %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #shared0>\n-    %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #shared0>\n-    %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma0>\n-    // CHECK: llvm.inline_asm\n-    // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4\n-    // CHECK: llvm.inline_asm\n-    // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4\n-\n-    // CHECK: llvm.inline_asm\n-    // CHECK-SAME: mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n-    // CHECK: llvm.inline_asm\n-    // CHECK-SAME: mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n-    %D = tt.dot %AA, %BB, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #shared0> * tensor<16x16xf16, #shared0> -> tensor<16x16xf32, #mma0>\n-\n+// COM:     %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot0>\n+// COM:     %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot1>\n+// COM:     %cst0 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #mma0>\n+// COM:     // CHECK: llvm.inline_asm\n+// COM:     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4\n+// COM:     // CHECK: llvm.inline_asm\n+// COM:     // CHECK-SAME: ldmatrix.sync.aligned.m8n8.x4\n+// COM: \n+// COM:     // CHECK: llvm.inline_asm\n+// COM:     // CHECK-SAME: mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n+// COM:     // CHECK: llvm.inline_asm\n+// COM:     // CHECK-SAME: mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\n+// COM:     %D = tt.dot %AA, %BB, %cst0 {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #dot0> * tensor<16x16xf16, #dot1> -> tensor<16x16xf32, #mma0>\n+// COM: \n     return\n   }\n }"}]