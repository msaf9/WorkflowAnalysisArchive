[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 190, "deletions": 57, "changes": 247, "file_content_changes": "@@ -19,25 +19,28 @@ jobs:\n   Runner-Preparation:\n     runs-on: ubuntu-latest\n     outputs:\n-      matrix: ${{ steps.set-matrix.outputs.matrix }}\n+      matrix-required: ${{ steps.set-matrix.outputs.matrix-required }}\n+      matrix-optional: ${{ steps.set-matrix.outputs.matrix-optional }}\n     steps:\n       - name: Prepare runner matrix\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"H100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]' >> $GITHUB_OUTPUT\n+            echo '::set-output name=matrix-required::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"H100\"]]' >> $GITHUB_OUTPUT\n+            echo '::set-output name=matrix-optional::[[\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]' >> $GITHUB_OUTPUT\n           else\n-            echo '::set-output name=matrix::[\"ubuntu-latest\"]' >> $GITHUB_OUTPUT\n+            echo '::set-output name=matrix-required::[\"ubuntu-latest\"]' >> $GITHUB_OUTPUT\n+            echo '::set-output name=matrix-optional::[\"ubuntu-latest\"]' >> $GITHUB_OUTPUT\n           fi\n \n-  Integration-Tests:\n+  Integration-Tests-Nvidia:\n     needs: Runner-Preparation\n \n     runs-on: ${{ matrix.runner }}\n \n     strategy:\n       matrix:\n-        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix)}}\n+        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix-required)}}\n \n     steps:\n       - name: Checkout\n@@ -48,6 +51,81 @@ jobs:\n         run: |\n           echo \"BACKEND=CUDA\" >> \"${GITHUB_ENV}\"\n \n+      - name: Clear cache\n+        run: |\n+          rm -rf ~/.triton\n+\n+      - name: Update PATH\n+        run: |\n+          echo \"PATH=${HOME}/.local/bin:${PATH}\" >> \"${GITHUB_ENV}\"\n+\n+      - name: Install Triton\n+        if: ${{ env.BACKEND == 'CUDA'}}\n+        run: |\n+          cd python\n+          python3 -m pip install --upgrade pip\n+          python3 -m pip install cmake==3.24\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n+\n+      - name: Run lit tests\n+        if: ${{ env.BACKEND == 'CUDA'}}\n+        run: |\n+          python3 -m pip install lit\n+          cd python\n+          LIT_TEST_DIR=\"build/$(ls build | grep -i cmake)/test\"\n+          if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n+            echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n+          fi\n+          lit -v \"${LIT_TEST_DIR}\"\n+\n+      - name: Run python tests on CUDA\n+        if: ${{ env.BACKEND == 'CUDA'}}\n+        run: |\n+          cd python/test/unit\n+          python3 -m pytest\n+\n+      - name: Create artifacts archive\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n+        run: |\n+          cd ~/.triton\n+          tar -czvf artifacts.tar.gz cache\n+\n+      - name: Upload artifacts archive\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n+        uses: actions/upload-artifact@v2\n+        with:\n+          name: artifacts ${{ matrix.runner[1] }}\n+          path: ~/.triton/artifacts.tar.gz\n+\n+      - name: Run CXX unittests\n+        if: ${{ env.BACKEND == 'CUDA'}}\n+        run: |\n+          cd python\n+          cd \"build/$(ls build | grep -i cmake)\"\n+          ctest\n+\n+      - name: Regression tests\n+        if: ${{ contains(matrix.runner, 'A100') }}\n+        run: |\n+          cd python/test/regression\n+          sudo nvidia-smi -i 0 -pm 1\n+          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n+          python3 -m pytest -vs .\n+          sudo nvidia-smi -i 0 -rgc\n+\n+  Integration-Tests-Third-Party:\n+    needs: Runner-Preparation\n+\n+    runs-on: ${{ matrix.runner }}\n+\n+    strategy:\n+      matrix:\n+        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix-optional)}}\n+\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+\n       - name: Set ROCM ENV\n         if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'gfx908')}}\n         run: |\n@@ -81,14 +159,6 @@ jobs:\n           python3 -m pip install --upgrade pre-commit\n           python3 -m pre_commit run --all-files\n \n-      - name: Install Triton\n-        if: ${{ env.BACKEND == 'CUDA'}}\n-        run: |\n-          cd python\n-          python3 -m pip install --upgrade pip\n-          python3 -m pip install cmake==3.24\n-          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n-\n       - name: Install Triton on ROCM\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n@@ -113,43 +183,6 @@ jobs:\n           python3 setup.py build\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n-      - name: Run lit tests\n-        if: ${{ env.BACKEND == 'CUDA'}}\n-        run: |\n-          python3 -m pip install lit\n-          cd python\n-          LIT_TEST_DIR=\"build/$(ls build | grep -i cmake)/test\"\n-          if [ ! -d \"${LIT_TEST_DIR}\" ]; then\n-            echo \"Coult not find '${LIT_TEST_DIR}'\" ; exit -1\n-          fi\n-          lit -v \"${LIT_TEST_DIR}\"\n-\n-      - name: Run python tests on CUDA\n-        if: ${{ env.BACKEND == 'CUDA'}}\n-        run: |\n-          cd python/test/unit\n-          python3 -m pytest\n-\n-      - name: Create artifacts archive\n-        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n-        run: |\n-          cd ~/.triton\n-          tar -czvf artifacts.tar.gz cache\n-\n-      - name: Upload artifacts archive\n-        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'V100' || matrix.runner[1] == 'A100' || matrix.runner[1] == 'H100')}}\n-        uses: actions/upload-artifact@v2\n-        with:\n-          name: artifacts\n-          path: ~/.triton/artifacts.tar.gz\n-\n-      - name: Run CXX unittests\n-        if: ${{ env.BACKEND == 'CUDA'}}\n-        run: |\n-          cd python\n-          cd \"build/$(ls build | grep -i cmake)\"\n-          ctest\n-\n       - name: Run python tests on ROCM\n         if: ${{ env.BACKEND == 'ROCM'}}\n         run: |\n@@ -165,11 +198,111 @@ jobs:\n           cd python/test/backend/third_party_backends\n           python3 -m pytest --capture=tee-sys -rfs --verbose --backend xpu\n \n-      - name: Regression tests\n-        if: ${{ contains(matrix.runner, 'A100') }}\n+  Compare-artifacts:\n+    needs: Integration-Tests-Nvidia\n+\n+    runs-on: ubuntu-latest\n+\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v2\n+\n+      - name: Install gh CLI\n         run: |\n-          cd python/test/regression\n-          sudo nvidia-smi -i 0 -pm 1\n-          sudo nvidia-smi -i 0 --lock-gpu-clocks=1350,1350\n-          python3 -m pytest -vs .\n-          sudo nvidia-smi -i 0 -rgc\n+          sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key 23F3D4EA75716059\n+          echo \"deb [arch=$(dpkg --print-architecture)] https://cli.github.com/packages focal main\" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null\n+          sudo apt update\n+          sudo apt install gh\n+\n+      - name: Download latest main artifacts\n+        env:\n+          ARTIFACT_NAME: artifacts A100\n+          ARTIFACT_JOB_NAME: Integration-Tests-Nvidia\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+        run: |\n+          OWNER_REPO=\"${{ github.repository }}\"\n+          PR_NUMBER=$(gh api --method GET repos/$OWNER_REPO/pulls -f state=closed | jq \".[] | select(.merged_at != null) | .number\" | head -1)\n+          echo \"Last merged PR number: $PR_NUMBER\"\n+\n+          BRANCH_NAME=$(gh api repos/$OWNER_REPO/pulls/$PR_NUMBER --jq '.head.ref')\n+          echo \"BRANCH_NAME: $BRANCH_NAME\"\n+          WORKFLOW_RUN_ID=$(gh api --method GET repos/$OWNER_REPO/actions/runs | jq --arg branch_name \"$BRANCH_NAME\" '.workflow_runs[] | select(.head_branch == $branch_name)' | jq '.id' | head -1)\n+          echo \"WORKFLOW_RUN_ID: $WORKFLOW_RUN_ID\"\n+          ARTIFACT_URL=$(gh api repos/$OWNER_REPO/actions/runs/$WORKFLOW_RUN_ID/artifacts | jq --arg artifact_name \"$ARTIFACT_NAME\" '.artifacts[] | select(.name == $artifact_name).archive_download_url' --raw-output)\n+          echo \"ARTIFACT_URL: $ARTIFACT_URL\"\n+\n+          if [ -n \"$ARTIFACT_URL\" ]; then\n+            echo \"Downloading artifact: $ARTIFACT_URL\"\n+            curl --location --remote-header-name -H \"Authorization: token $GH_TOKEN\" -o reference.zip \"$ARTIFACT_URL\"\n+            # Print the size of the downloaded artifact\n+            echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" reference.zip)\"\n+            echo \"Artifact size (du): $(du -sh reference.zip)\"\n+            unzip reference.zip\n+            tar -xzf artifacts.tar.gz\n+            rm reference.zip\n+            rm artifacts.tar.gz\n+            mv cache reference\n+          else\n+            echo \"No artifact found with the name: $ARTIFACT_NAME\"\n+            exit 1\n+          fi\n+      - name: Download current job artifacts\n+        uses: actions/download-artifact@v2\n+        with:\n+          name: artifacts A100\n+      - name: Unzip current job artifacts\n+        run: |\n+          # Print the size of the downloaded artifact\n+          echo \"Artifact size (stat): $(stat --printf=\"%s bytes\" artifacts.tar.gz)\"\n+          echo \"Artifact size (du): $(du -sh artifacts.tar.gz)\"\n+          tar -xzf artifacts.tar.gz\n+          rm artifacts.tar.gz\n+          mv cache current\n+      - name: Compare artifacts\n+        run: |\n+          set +e\n+          python3 python/test/tools/compare_files.py --path1 reference --path2 current --kernels python/test/kernel_comparison/kernels.yml\n+          exit_code=$?\n+          set -e\n+          echo $exit_code\n+          if [ $exit_code -eq 0 ]; then\n+            echo \"Artifacts are identical\"\n+            echo \"COMPARISON_RESULT=true\" >> $GITHUB_ENV\n+          elif [ $exit_code -eq 1 ]; then\n+            echo \"Artifacts are different\"\n+            echo \"COMPARISON_RESULT=false\" >> $GITHUB_ENV\n+          else\n+            echo \"Error while comparing artifacts\"\n+            echo \"COMPARISON_RESULT=error\" >> $GITHUB_ENV\n+          fi\n+          echo \"COMPARISON_RESULT=env.COMPARISON_RESULT\"\n+      - name: Check exit code and handle failure\n+        if: ${{ env.COMPARISON_RESULT == 'error' }}\n+        run: |\n+          echo \"Error while comparing artifacts\"\n+          exit 1\n+      - name: Fetch Run ID\n+        id: get_run_id\n+        run: echo \"RUN_ID=${{ github.run_id }}\" >> $GITHUB_ENV\n+\n+      - name: Upload results as artifact\n+        uses: actions/upload-artifact@v2\n+        with:\n+          name: kernels-reference-check\n+          path: kernels_reference_check.txt\n+\n+      - name: Check output and comment on PR\n+        if: ${{ env.COMPARISON_RESULT == 'false' }}\n+        uses: actions/github-script@v5\n+        with:\n+          github-token: ${{ secrets.CI_ACCESS_TOKEN }}\n+          script: |\n+            const run_id = ${{ env.RUN_ID }};\n+            const issue_number = context.payload.pull_request.number;\n+            const message = `:warning: **This PR does not produce bitwise identical kernels as the branch it's merged against.** Please check artifacts for details. [Download the output file here](https://github.com/${{ github.repository }}/actions/runs/${run_id}).`;\n+            await github.rest.issues.createComment({\n+                owner: context.repo.owner,\n+                repo: context.repo.repo,\n+                issue_number: issue_number,\n+                body: message\n+            });"}, {"filename": ".github/workflows/wheels.yml", "status": "modified", "additions": 42, "deletions": 17, "changes": 59, "file_content_changes": "@@ -2,7 +2,7 @@ name: Wheels\n on:\n   workflow_dispatch:\n   schedule:\n-    - cron: \"0 2 * * *\"\n+    - cron: \"20 2 * * *\"\n \n jobs:\n \n@@ -18,25 +18,20 @@ jobs:\n       - name: Checkout\n         uses: actions/checkout@v3\n \n-      - name: Install Azure CLI\n+      # The LATEST_DATE here should be kept in sync with the one in Patch setup.py\n+      - id: check-version\n+        name: Check latest version\n         run: |\n-          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n-\n-      - name: Azure login\n-        uses: azure/login@v1\n-        with:\n-          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n-          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n-          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n-\n-      - id: generate-token\n-        name: Generate token\n-        run: |\n-          AZ_TOKEN=$(az account get-access-token --query accessToken)\n-          echo \"::add-mask::$AZ_TOKEN\"\n-          echo \"access_token=$AZ_TOKEN\" >> \"$GITHUB_OUTPUT\"\n+          export PACKAGE_DATE=$(python3 -m pip install --user --index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ --dry-run triton-nightly== |& grep -oP '(?<=, )[0-9\\.]+dev[0-9]+(?=\\))' | grep -oP '(?<=dev)[0-9]+')\n+          export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d%H%M%S' --format=\"%cd\")\n+          if cmp -s <(echo $PACKAGE_DATE) <(echo $LATEST_DATE); then\n+            echo \"new_commit=false\" >> \"$GITHUB_OUTPUT\"\n+          else\n+            echo \"new_commit=true\" >> \"$GITHUB_OUTPUT\"\n+          fi\n \n       - name: Patch setup.py\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n         run: |\n           sed -i 's/name\\=\\\"triton\\\"/name=\"triton-nightly\"/g' python/setup.py\n           export LATEST_DATE=$(TZ=UTC0 git show --quiet --date='format-local:%Y%m%d%H%M%S' --format=\"%cd\")\n@@ -46,6 +41,7 @@ jobs:\n           echo \"base-dir=/project\" >> python/setup.cfg\n \n       - name: Build wheels\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n         run: |\n           export CIBW_MANYLINUX_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n           #export CIBW_MANYLINUX_PYPY_X86_64_IMAGE=\"quay.io/pypa/manylinux2014_x86_64:latest\"\n@@ -54,6 +50,35 @@ jobs:\n           export CIBW_BUILD=\"{cp,pp}3*-manylinux_x86_64\"\n           python3 -m cibuildwheel python --output-dir wheelhouse\n \n+      - name: Install Azure CLI\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n+        run: |\n+          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n+\n+      - name: Azure login\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n+        uses: azure/login@v1\n+        with:\n+          client-id: ${{ secrets.AZURE_CLIENT_ID }}\n+          tenant-id: ${{ secrets.AZURE_TENANT_ID }}\n+          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}\n+\n+      - id: generate-token\n+        name: Generate token\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n+        run: |\n+          AZ_TOKEN=$(az account get-access-token --query accessToken)\n+          echo \"::add-mask::$AZ_TOKEN\"\n+          echo \"access_token=$AZ_TOKEN\" >> \"$GITHUB_OUTPUT\"\n+\n       - name: Publish wheels to Azure DevOps\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' }}\n         run: |\n           python3 -m twine upload -r Triton-Nightly -u TritonArtifactsSP -p ${{ steps.generate-token.outputs.access_token }} --config-file utils/nightly.pypirc --non-interactive --verbose wheelhouse/*\n+\n+      - name: Azure Logout\n+        if: ${{ steps.check-version.outputs.new_commit == 'true' && (success() || failure()) }}\n+        run: |\n+          az logout\n+          az cache purge\n+          az account clear"}, {"filename": ".hypothesis/unicode_data/13.0.0/charmap.json.gz", "status": "removed", "additions": 0, "deletions": 0, "changes": 0, "file_content_changes": "N/A"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 53, "deletions": 53, "changes": 106, "file_content_changes": "@@ -86,59 +86,59 @@ if(NOT MLIR_DIR)\n   else()\n     set(LLVM_LDFLAGS \"-L${LLVM_LIBRARY_DIR}\")\n     set(LLVM_LIBRARIES\n-      libLLVMNVPTXCodeGen.a\n-      libLLVMNVPTXDesc.a\n-      libLLVMNVPTXInfo.a\n-      libLLVMAMDGPUDisassembler.a\n-      libLLVMMCDisassembler.a\n-      libLLVMAMDGPUCodeGen.a\n-      libLLVMMIRParser.a\n-      libLLVMGlobalISel.a\n-      libLLVMSelectionDAG.a\n-      libLLVMipo.a\n-      libLLVMInstrumentation.a\n-      libLLVMVectorize.a\n-      libLLVMLinker.a\n-      libLLVMIRReader.a\n-      libLLVMAsmParser.a\n-      libLLVMFrontendOpenMP.a\n-      libLLVMAsmPrinter.a\n-      libLLVMDebugInfoDWARF.a\n-      libLLVMCodeGen.a\n-      libLLVMTarget.a\n-      libLLVMScalarOpts.a\n-      libLLVMInstCombine.a\n-      libLLVMAggressiveInstCombine.a\n-      libLLVMTransformUtils.a\n-      libLLVMBitWriter.a\n-      libLLVMAnalysis.a\n-      libLLVMProfileData.a\n-      libLLVMObject.a\n-      libLLVMTextAPI.a\n-      libLLVMBitReader.a\n-      libLLVMAMDGPUAsmParser.a\n-      libLLVMMCParser.a\n-      libLLVMAMDGPUDesc.a\n-      libLLVMAMDGPUUtils.a\n-      libLLVMMC.a\n-      libLLVMDebugInfoCodeView.a\n-      libLLVMDebugInfoMSF.a\n-      libLLVMCore.a\n-      libLLVMRemarks.a\n-      libLLVMBitstreamReader.a\n-      libLLVMBinaryFormat.a\n-      libLLVMAMDGPUInfo.a\n-      libLLVMSupport.a\n-      libLLVMDemangle.a\n-      libLLVMPasses.a\n-      libLLVMAnalysis.a\n-      libLLVMTransformUtils.a\n-      libLLVMScalarOpts.a\n-      libLLVMTransformUtils.a\n-      libLLVMipo.a\n-      libLLVMObjCARCOpts.a\n-      libLLVMCoroutines.a\n-      libLLVMAnalysis.a\n+      LLVMNVPTXCodeGen\n+      LLVMNVPTXDesc\n+      LLVMNVPTXInfo\n+      LLVMAMDGPUDisassembler\n+      LLVMMCDisassembler\n+      LLVMAMDGPUCodeGen\n+      LLVMMIRParser\n+      LLVMGlobalISel\n+      LLVMSelectionDAG\n+      LLVMipo\n+      LLVMInstrumentation\n+      LLVMVectorize\n+      LLVMLinker\n+      LLVMIRReader\n+      LLVMAsmParser\n+      LLVMFrontendOpenMP\n+      LLVMAsmPrinter\n+      LLVMDebugInfoDWARF\n+      LLVMCodeGen\n+      LLVMTarget\n+      LLVMScalarOpts\n+      LLVMInstCombine\n+      LLVMAggressiveInstCombine\n+      LLVMTransformUtils\n+      LLVMBitWriter\n+      LLVMAnalysis\n+      LLVMProfileData\n+      LLVMObject\n+      LLVMTextAPI\n+      LLVMBitReader\n+      LLVMAMDGPUAsmParser\n+      LLVMMCParser\n+      LLVMAMDGPUDesc\n+      LLVMAMDGPUUtils\n+      LLVMMC\n+      LLVMDebugInfoCodeView\n+      LLVMDebugInfoMSF\n+      LLVMCore\n+      LLVMRemarks\n+      LLVMBitstreamReader\n+      LLVMBinaryFormat\n+      LLVMAMDGPUInfo\n+      LLVMSupport\n+      LLVMDemangle\n+      LLVMPasses\n+      LLVMAnalysis\n+      LLVMTransformUtils\n+      LLVMScalarOpts\n+      LLVMTransformUtils\n+      LLVMipo\n+      LLVMObjCARCOpts\n+      LLVMCoroutines\n+      LLVMAnalysis\n     )\n   endif()\n "}, {"filename": "README.md", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -25,7 +25,7 @@ You can install the latest stable release of Triton from pip:\n ```bash\n pip install triton\n ```\n-Binary wheels are available for CPython 3.8-3.11 and PyPy 3.8-3.9.\n+Binary wheels are available for CPython 3.7-3.11 and PyPy 3.8-3.9.\n \n And the latest nightly release:\n "}, {"filename": "docs/getting-started/installation.rst", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -12,7 +12,7 @@ You can install the latest stable release of Triton from pip:\n \n       pip install triton\n \n-Binary wheels are available for CPython 3.8-3.11 and PyPy 3.8-3.9.\n+Binary wheels are available for CPython 3.7-3.11 and PyPy 3.8-3.9.\n \n And the latest nightly release:\n "}, {"filename": "docs/python-api/triton.language.rst", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -111,6 +111,16 @@ Reduction Ops\n     sum\n     xor_sum\n \n+Scan Ops\n+-------------\n+\n+.. autosummary::\n+    :toctree: generated\n+    :nosignatures:\n+\n+    associative_scan\n+    cumsum\n+    cumprod\n \n Atomic Ops\n ----------"}, {"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 54, "deletions": 8, "changes": 62, "file_content_changes": "@@ -12,19 +12,19 @@ namespace mlir {\n \n class ReduceOpHelper {\n public:\n-  explicit ReduceOpHelper(triton::ReduceOp rop)\n-      : op(rop.getOperation()), axis(rop.getAxis()) {\n-    auto firstTy = rop.getOperands()[0].getType().cast<RankedTensorType>();\n+  explicit ReduceOpHelper(triton::ReduceOp op)\n+      : op(op.getOperation()), axis(op.getAxis()) {\n+    auto firstTy = op.getOperands()[0].getType().cast<RankedTensorType>();\n     srcShape = firstTy.getShape();\n     srcEncoding = firstTy.getEncoding();\n-    srcElementTypes = rop.getElementTypes();\n+    srcElementTypes = op.getElementTypes();\n \n-    for (const auto &t : rop.getInputTypes()) {\n+    for (const auto &t : op.getInputTypes()) {\n       if (t.getShape() != srcShape) {\n-        rop.emitError() << \"shape mismatch\";\n+        op.emitError() << \"shape mismatch\";\n       }\n       if (t.getEncoding() != srcEncoding) {\n-        rop.emitError() << \"encoding mismatch\";\n+        op.emitError() << \"encoding mismatch\";\n       }\n     }\n   }\n@@ -33,6 +33,8 @@ class ReduceOpHelper {\n \n   Attribute getSrcLayout() { return srcEncoding; }\n \n+  triton::ReduceOp getOperation() { return op; }\n+\n   bool isFastReduction();\n \n   unsigned getInterWarpSize();\n@@ -54,13 +56,57 @@ class ReduceOpHelper {\n   bool isSupportedLayout();\n \n private:\n-  Operation *op;\n+  triton::ReduceOp op;\n   ArrayRef<int64_t> srcShape;\n   Attribute srcEncoding;\n   SmallVector<Type> srcElementTypes;\n   int axis;\n };\n \n+class ScanLoweringHelper {\n+public:\n+  explicit ScanLoweringHelper(triton::ScanOp op) : scanOp(op) {\n+    auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+    srcEncoding = type.getEncoding();\n+  }\n+  // Return true if the lowering of the scan op is supported.\n+  bool isSupported();\n+  // Return the number of elements per thread along axis dim.\n+  unsigned getAxisNumElementsPerThread();\n+  // Return the number of elements per thread along non-axis dims.\n+  unsigned getNonAxisNumElementsPerThread();\n+  // Return the number of threads per warp along non-axis dims.\n+  unsigned getNonAxisNumThreadsPerWarp();\n+  // Return the flat numbers of threads computing independent scan results.\n+  unsigned getNonAxisNumThreadsPerCTA();\n+  // Return the number of warps per CTA along axis dim.\n+  unsigned getAxisNumWarps();\n+  // Return the number of threads per warp along axis dim.\n+  unsigned getAxisNumThreadsPerWarp();\n+  // Return the number of blocks along axis dim.\n+  unsigned getAxisNumBlocks();\n+  // Return the number of blocks along non axis dim.\n+  unsigned getNonAxisNumBlocks();\n+  // Return the size of the scratch space needed for scan lowering.\n+  unsigned getScratchSizeInBytes();\n+\n+  // Stride between contiguous element along axis dim.\n+  unsigned getAxisElementStride();\n+  // Stride between contiguous threads along axis dim.\n+  unsigned getAxisThreadStride();\n+  // Stride between contiguous blocks along axis dim.\n+  unsigned getAxisBlockStride();\n+\n+  Location getLoc() { return scanOp.getLoc(); }\n+  unsigned getAxis() { return scanOp.getAxis(); }\n+  triton::gpu::BlockedEncodingAttr getEncoding();\n+  Region &getCombineOp();\n+\n+private:\n+  triton::ScanOp scanOp;\n+  Attribute srcEncoding;\n+};\n+\n bool maybeSharedAllocationOp(Operation *op);\n \n bool maybeAliasOp(Operation *op);"}, {"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -3,13 +3,16 @@\n \n include \"mlir/IR/EnumAttr.td\"\n \n-// Attributes for LoadOp\n+// Attributes for LoadOp and StoreOp\n def TT_CacheModifierAttr : I32EnumAttr<\n     \"CacheModifier\", \"\",\n     [\n         I32EnumAttrCase<\"NONE\", 1, \"none\">,\n         I32EnumAttrCase<\"CA\", 2, \"ca\">,\n         I32EnumAttrCase<\"CG\", 3, \"cg\">,\n+        I32EnumAttrCase<\"WB\", 4, \"wb\">,\n+        I32EnumAttrCase<\"CS\", 5, \"cs\">,\n+        I32EnumAttrCase<\"WT\", 6, \"wt\">,\n     ]> {\n     let cppNamespace = \"::mlir::triton\";\n }"}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 27, "deletions": 0, "changes": 27, "file_content_changes": "@@ -392,6 +392,7 @@ def TT_DotOp : TT_Op<\"dot\", [Pure,\n     let results = (outs TT_FpIntTensor:$d);\n \n     let assemblyFormat = \"$a`,` $b`,` $c attr-dict `:` type($a) `*` type($b) `->` type($d)\";\n+    let hasVerifier = 1;\n }\n \n //\n@@ -425,6 +426,32 @@ def TT_ReduceReturnOp: TT_Op<\"reduce.return\",\n     let assemblyFormat = \"$result attr-dict `:` type($result)\";\n }\n \n+//\n+// Scan Op\n+//\n+def TT_ScanOp: TT_Op<\"scan\",\n+                       [Pure,\n+                        SameOperandsAndResultEncoding,\n+                        SameOperandsAndResultElementType,\n+                        SingleBlock,\n+                        DeclareOpInterfaceMethods<InferTypeOpInterface>]> {\n+    let summary = \"Reduction using generic combination algorithm\";\n+    let arguments = (ins Variadic<TT_Tensor>:$operands, I32Attr:$axis);\n+    let results = (outs Variadic<TT_Tensor>:$result);\n+    let regions = (region SizedRegion<1>:$combineOp);\n+    let builders = [\n+        OpBuilder<(ins \"ValueRange\":$operands, \"int\":$axis)>,\n+    ];\n+    let hasVerifier = 1;\n+}\n+\n+def TT_ScanReturnOp: TT_Op<\"scan.return\",\n+                             [HasParent<\"ScanOp\">, Pure, Terminator, ReturnLike]> {\n+    let summary = \"terminator for scan operator\";\n+    let arguments = (ins Variadic<AnyType>:$result);\n+    let assemblyFormat = \"$result attr-dict `:` type($result)\";\n+}\n+\n \n //\n // External Elementwise op"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Attributes.h", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -0,0 +1,7 @@\n+#ifndef TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_\n+#define TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_\n+\n+#define GET_ATTRDEF_CLASSES\n+#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n+\n+#endif // TRITON_DIALECT_TRITONGPU_IR_ATTRIBUTES_H_"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "file_content_changes": "@@ -8,12 +8,10 @@\n \n // TritonGPU depends on Triton\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Traits.h\"\n \n-#define GET_ATTRDEF_CLASSES\n-#include \"triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.h.inc\"\n-\n #define GET_OP_CLASSES\n #include \"triton/Dialect/TritonGPU/IR/Ops.h.inc\"\n \n@@ -28,8 +26,13 @@ unsigned getTotalElemsPerThread(Attribute layout, ArrayRef<int64_t> shape,\n \n SmallVector<unsigned> getElemsPerThread(Type type);\n \n+// Returns the number of threads per warp that may have access to replicated\n+// elements. If you want non-replicated threads, use\n+// getThreadsPerWarpWithUniqueData.\n SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n \n+// Returns the number of warps per CTA that may have access to replicated\n+// elements. If you want non-replicated warps, use getWarpsPerCTAWithUniqueData.\n SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n SmallVector<unsigned> getSizePerThread(Attribute layout);\n@@ -77,9 +80,10 @@ bool isaDistributedLayout(Attribute layout);\n \n bool isSharedEncoding(Value value);\n \n+bool isExpensiveCat(CatOp cat, Attribute &targetEncoding);\n+\n } // namespace gpu\n } // namespace triton\n-\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 12, "deletions": 4, "changes": 16, "file_content_changes": "@@ -77,7 +77,7 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n     AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n                      \"ArrayRef<int64_t>\":$shape,\n                      \"ArrayRef<unsigned>\":$order,\n-                     \"Type\":$eltTy), [{\n+                     \"unsigned\":$typeWidthInBit), [{\n         auto mmaEnc = dotOpEnc.getParent().dyn_cast<MmaEncodingAttr>();\n \n         if(!mmaEnc)\n@@ -87,7 +87,7 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         int opIdx = dotOpEnc.getOpIdx();\n \n         // number of rows per phase\n-        int perPhase = 128 / (shape[order[0]] * (eltTy.getIntOrFloatBitWidth() / 8));\n+        int perPhase = 128 / (shape[order[0]] * (typeWidthInBit / 8));\n         perPhase = std::max<int>(perPhase, 1);\n \n         // index of the inner dimension in `order`\n@@ -109,9 +109,9 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         // ---- begin Ampere ----\n         if (mmaEnc.isAmpere()) {\n           std::vector<size_t> matShape = {8, 8,\n-                                          2 * 64 / eltTy.getIntOrFloatBitWidth()};\n+                                          2 * 64 / typeWidthInBit};\n           // for now, disable swizzle when using transposed int8 tensor cores\n-          if (eltTy.isInteger(8) && order[0] == inner)\n+          if (typeWidthInBit == 8 && order[0] == inner)\n             return $_get(context, 1, 1, 1, order);\n \n           // --- handle A operand ---\n@@ -135,6 +135,14 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n \n         // ---- not implemented ----\n         llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n+    }]>,\n+\n+    AttrBuilder<(ins \"DotOperandEncodingAttr\":$dotOpEnc,\n+                     \"ArrayRef<int64_t>\":$shape,\n+                     \"ArrayRef<unsigned>\":$order,\n+                     \"Type\":$eltTy), [{\n+      unsigned bitwidth = eltTy.getIntOrFloatBitWidth();\n+      return get(context, dotOpEnc, shape, order, bitwidth);\n     }]>\n   ];\n "}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -18,8 +18,6 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n \n bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n \n-bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n-\n bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n // skipInit is True when we only consider the operands of the initOp but"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -168,6 +168,10 @@ class AllocationAnalysis {\n       ReduceOpHelper helper(reduceOp);\n       unsigned bytes = helper.getScratchSizeInBytes();\n       allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n+    } else if (auto scanOp = dyn_cast<triton::ScanOp>(op)) {\n+      ScanLoweringHelper helper(scanOp);\n+      unsigned bytes = helper.getScratchSizeInBytes();\n+      allocation->addBuffer<BufferT::BufferKind::Scratch>(op, bytes);\n     } else if (auto cvtLayout = dyn_cast<triton::gpu::ConvertLayoutOp>(op)) {\n       auto srcTy = cvtLayout.getSrc().getType().cast<RankedTensorType>();\n       auto dstTy = cvtLayout.getResult().getType().cast<RankedTensorType>();"}, {"filename": "lib/Analysis/Membar.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -86,7 +86,8 @@ void MembarAnalysis::visitTerminator(Operation *op,\n     return;\n   }\n   // Otherwise, it could be a return op\n-  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ReturnOp>(op)) {\n+  if (isa<triton::ReduceReturnOp>(op) || isa<triton::ScanReturnOp>(op) ||\n+      isa<triton::ReturnOp>(op)) {\n     return;\n   }\n   llvm_unreachable(\"Unknown terminator encountered in membar analysis\");"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 153, "deletions": 1, "changes": 154, "file_content_changes": "@@ -5,12 +5,36 @@\n #include \"mlir/IR/Matchers.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n #include <deque>\n \n namespace mlir {\n \n+namespace {\n+\n+int getParentAxis(Attribute layout, int axis) {\n+  if (auto sliceEncoding = layout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    axis = axis < sliceEncoding.getDim() ? axis : axis + 1;\n+    return getParentAxis(sliceEncoding.getParent(), axis);\n+  }\n+  return axis;\n+}\n+\n+SmallVector<unsigned> getParentOrder(Attribute layout) {\n+  if (auto sliceEncoding = layout.dyn_cast<triton::gpu::SliceEncodingAttr>()) {\n+    return getParentOrder(sliceEncoding.getParent());\n+  }\n+  return triton::gpu::getOrder(layout);\n+}\n+\n+} // namespace\n+\n bool ReduceOpHelper::isFastReduction() {\n-  return axis == triton::gpu::getOrder(getSrcLayout())[0];\n+  // Disable fast reduction only for debugging purpose\n+  if (::triton::tools::getBoolEnv(\"DISABLE_FAST_REDUCTION\"))\n+    return false;\n+  return getParentAxis(getSrcLayout(), axis) ==\n+         getParentOrder(getSrcLayout())[0];\n }\n \n unsigned ReduceOpHelper::getInterWarpSize() {\n@@ -113,6 +137,134 @@ bool ReduceOpHelper::isSupportedLayout() {\n   return false;\n }\n \n+unsigned ScanLoweringHelper::getAxisNumElementsPerThread() {\n+  return getEncoding().getSizePerThread()[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumElementsPerThread() {\n+  SmallVector<unsigned> sizePerThreads = getContigPerThread(getEncoding());\n+  sizePerThreads[getAxis()] = 1;\n+  return product<unsigned>(sizePerThreads);\n+}\n+\n+Region &ScanLoweringHelper::getCombineOp() { return scanOp.getCombineOp(); }\n+\n+unsigned ScanLoweringHelper::getAxisNumThreadsPerWarp() {\n+  return triton::gpu::getThreadsPerWarp(getEncoding())[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumThreadsPerWarp() {\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(getEncoding());\n+  threadsPerWarp[getAxis()] = 1;\n+  return product<unsigned>(threadsPerWarp);\n+}\n+\n+// Return the flat numbers of threads computing independent scan results.\n+unsigned ScanLoweringHelper::getNonAxisNumThreadsPerCTA() {\n+  unsigned numParallelThreadsPerWarp = getNonAxisNumThreadsPerWarp();\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(getEncoding());\n+  warpsPerCTA[getAxis()] = 1;\n+  unsigned numParallelWarpsPerCTA = product<unsigned>(warpsPerCTA);\n+  return numParallelThreadsPerWarp * numParallelWarpsPerCTA;\n+}\n+unsigned ScanLoweringHelper::getAxisNumWarps() {\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  return warpsPerCTA[getAxis()];\n+}\n+\n+unsigned ScanLoweringHelper::getAxisNumBlocks() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  unsigned axis = getAxis();\n+  return ceil<unsigned>(\n+      type.getShape()[axis],\n+      (sizePerThreads[axis] * threadsPerWarp[axis] * warpsPerCTA[axis]));\n+}\n+\n+unsigned ScanLoweringHelper::getNonAxisNumBlocks() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  unsigned axis = getAxis();\n+  unsigned numBlocks = 1;\n+  for (unsigned i = 0; i < sizePerThreads.size(); i++) {\n+    if (i == axis)\n+      continue;\n+    numBlocks *= ceil<unsigned>(\n+        type.getShape()[i],\n+        (sizePerThreads[i] * threadsPerWarp[i] * warpsPerCTA[i]));\n+  }\n+  return numBlocks;\n+}\n+\n+bool ScanLoweringHelper::isSupported() {\n+  // TODO: Support the following cases:\n+  // 1. Scan on non-blocking encodings\n+  // 2. Scan with multiple operands\n+  if (!isa<triton::gpu::BlockedEncodingAttr>(srcEncoding))\n+    return false;\n+  if (scanOp.getNumOperands() != 1)\n+    return false;\n+  return true;\n+}\n+\n+unsigned ScanLoweringHelper::getScratchSizeInBytes() {\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  unsigned elementSizeInBytes = type.getElementTypeBitWidth() / 8;\n+  auto mod = scanOp->getParentOfType<ModuleOp>();\n+  unsigned numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+  unsigned numNonAxisElementsPerWapr =\n+      getNonAxisNumThreadsPerWarp() * getNonAxisNumElementsPerThread();\n+  unsigned numElements = numWarps * numNonAxisElementsPerWapr *\n+                         getAxisNumBlocks() * getNonAxisNumBlocks();\n+  return elementSizeInBytes * numElements;\n+}\n+\n+triton::gpu::BlockedEncodingAttr ScanLoweringHelper::getEncoding() {\n+  return srcEncoding.cast<triton::gpu::BlockedEncodingAttr>();\n+}\n+\n+unsigned ScanLoweringHelper::getAxisElementStride() {\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  unsigned stride = 1;\n+  for (unsigned dim : order) {\n+    if (dim == getAxis())\n+      return stride;\n+    stride *= getContigPerThread(getEncoding())[dim];\n+  }\n+  llvm_unreachable(\"Axis not found in order\");\n+}\n+\n+unsigned ScanLoweringHelper::getAxisThreadStride() {\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  unsigned stride = 1;\n+  for (unsigned dim : order) {\n+    if (dim == getAxis())\n+      return stride;\n+    stride *= getEncoding().getThreadsPerWarp()[dim];\n+  }\n+  llvm_unreachable(\"Axis not found in order\");\n+}\n+\n+unsigned ScanLoweringHelper::getAxisBlockStride() {\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  unsigned stride = 1;\n+  auto type = scanOp.getOperand(0).getType().cast<RankedTensorType>();\n+  auto sizePerThreads = triton::gpu::getSizePerThread(srcEncoding);\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  for (unsigned dim : order) {\n+    if (dim == getAxis())\n+      return stride;\n+    stride *= type.getShape()[dim] /\n+              (sizePerThreads[dim] * threadsPerWarp[dim] * warpsPerCTA[dim]);\n+  }\n+  llvm_unreachable(\"Axis not found in order\");\n+}\n+\n bool maybeSharedAllocationOp(Operation *op) {\n   // TODO(Keren): This function can be replaced by adding\n   // MemoryEffectOpInterface. We can then use the MemoryEffectOpInterface to"}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -17,6 +17,7 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     TritonGPUToLLVMPass.cpp\n     PTXAsmFormat.cpp\n     ReduceOpToLLVM.cpp\n+    ScanOpToLLVM.cpp\n     Utility.cpp\n     TypeConverter.cpp\n     ViewOpToLLVM.cpp"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "file_content_changes": "@@ -1,8 +1,10 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n #include \"Utility.h\"\n \n+using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::LLVM::linearize;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getOrder;\n@@ -143,9 +145,10 @@ struct ConvertLayoutOpConversion\n       Value laneId = urem(threadId, warpSize);\n       Value warpId = udiv(threadId, warpSize);\n       // TODO: fix the bug in MMAEncodingAttr document\n-      SmallVector<Value> multiDimWarpId(2);\n-      multiDimWarpId[0] = urem(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n-      multiDimWarpId[1] = udiv(warpId, i32_val(mmaLayout.getWarpsPerCTA()[0]));\n+      auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+      auto order = triton::gpu::getOrder(mmaLayout);\n+      SmallVector<Value> multiDimWarpId =\n+          delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n       Value _1 = i32_val(1);\n       Value _2 = i32_val(2);\n       Value _4 = i32_val(4);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "file_content_changes": "@@ -4,6 +4,7 @@\n using namespace mlir;\n \n using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n+using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n@@ -552,14 +553,14 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n   int kWidth = encoding.getMMAv2kWidth();\n \n   auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+  auto order = triton::gpu::getOrder(mmaLayout);\n   Value warp = udiv(thread, i32_val(32));\n   Value lane = urem(thread, i32_val(32));\n-  // Note: warps are currently column major in MMA layout\n-  Value warpRowIndex = urem(warp, i32_val(warpsPerCTA[0]));\n-  Value warpColIndex =\n-      urem(udiv(warp, i32_val(warpsPerCTA[0])), i32_val(warpsPerCTA[1]));\n-  Value warpM = urem(warpRowIndex, i32_val(shape[0] / 16));\n-  Value warpN = urem(warpColIndex, i32_val(shape[1] / 8));\n+\n+  SmallVector<Value> multiDimWarpId =\n+      delinearize(rewriter, loc, warp, warpsPerCTA, order);\n+  Value warpM = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n+  Value warpN = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n \n   int warpsPerTile;\n   if (isA)"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -354,6 +354,10 @@ struct StoreOpConversion\n       auto &ptxStoreInstr =\n           ptxBuilder.create<>(\"st\")\n               ->global()\n+              .o(\"wb\", op.getCache() == triton::CacheModifier::WB)\n+              .o(\"cg\", op.getCache() == triton::CacheModifier::CG)\n+              .o(\"cs\", op.getCache() == triton::CacheModifier::CS)\n+              .o(\"wt\", op.getCache() == triton::CacheModifier::WT)\n               .o(\"L1::evict_first\",\n                  op.getEvict() == triton::EvictionPolicy::EVICT_FIRST)\n               .o(\"L1::evict_last\","}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 23, "deletions": 14, "changes": 37, "file_content_changes": "@@ -1,8 +1,11 @@\n #include \"ReduceOpToLLVM.h\"\n+#include \"Utility.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n+using ::mlir::LLVM::delinearize;\n+using ::mlir::LLVM::linearize;\n using ::mlir::LLVM::shflSync;\n using ::mlir::LLVM::storeShared;\n using ::mlir::triton::gpu::getOrder;\n@@ -86,13 +89,14 @@ struct ReduceOpConversion\n   void getWriteIndexBasic(ConversionPatternRewriter &rewriter, Location loc,\n                           Attribute layout, SmallVector<Value> &index,\n                           SmallVector<Value> &writeIdx,\n-                          std::map<int, Value> &ints, unsigned axis) const {\n+                          std::map<int, Value> &ints, unsigned originalAxis,\n+                          unsigned axis) const {\n     if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n-      auto dim = sliceLayout.getDim();\n-      assert(dim != axis && \"Reduction axis cannot be sliced\");\n+      // Recover the axis in the parent layout\n+      auto parentAxis = axis < sliceLayout.getDim() ? axis : axis + 1;\n       auto parentLayout = sliceLayout.getParent();\n       getWriteIndexBasic(rewriter, loc, parentLayout, index, writeIdx, ints,\n-                         axis);\n+                         originalAxis, parentAxis);\n       return;\n     }\n \n@@ -107,21 +111,21 @@ struct ReduceOpConversion\n       // we would have a single accumulation every `axisSizePerThread`\n       // contiguous values in the original tensor, so we would need\n       // to map every `axisSizePerThread` to 1 value in smem as:\n-      // writeIdx[axis] = index[axis] / axisSizePerThread\n-      writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+      // writeIdx[originalAxis] = index[originalAxis] / axisSizePerThread\n+      writeIdx[originalAxis] = udiv(index[originalAxis], axisSizePerThread);\n     } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n       if (!mmaLayout.isAmpere()) {\n         llvm::report_fatal_error(\"Unsupported layout\");\n       }\n-      if (axis == 0) {\n+      if (originalAxis == 0) {\n         // Because warpTileSize = [16, 8] and threadsPerWarp = [8, 4], each 8\n         // rows in smem would correspond to a warp. The mapping\n         // is: (warp_index) x 8 + (row index within warp)\n-        writeIdx[axis] =\n-            add(mul(udiv(index[axis], _16), _8), urem(index[axis], _8));\n+        writeIdx[originalAxis] = add(mul(udiv(index[originalAxis], _16), _8),\n+                                     urem(index[originalAxis], _8));\n       } else {\n         // Same as BlockedEncodingAttr case\n-        writeIdx[axis] = udiv(index[axis], axisSizePerThread);\n+        writeIdx[originalAxis] = udiv(index[originalAxis], axisSizePerThread);\n       }\n     } else {\n       llvm::report_fatal_error(\"Unsupported layout\");\n@@ -211,7 +215,7 @@ struct ReduceOpConversion\n       // get the writeIdx at which to write in smem\n       SmallVector<Value> writeIdx;\n       getWriteIndexBasic(rewriter, loc, srcLayout, indices[key], writeIdx, ints,\n-                         axis);\n+                         axis, axis);\n \n       // calculate the offset in smem for that writeIdx\n       Value writeOffset = linearize(rewriter, loc, writeIdx, smemShape, srcOrd);\n@@ -367,8 +371,10 @@ struct ReduceOpConversion\n     Value warpId = udiv(threadId, warpSize);\n     Value laneId = urem(threadId, warpSize);\n \n-    auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n-    auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n+    auto threadsPerWarp =\n+        triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout, srcShape);\n+    auto warpsPerCTA =\n+        triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape);\n     auto order = getOrder(srcLayout);\n     SmallVector<Value> multiDimLaneId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n@@ -412,8 +418,11 @@ struct ReduceOpConversion\n     //\n     // Each thread needs to process:\n     //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n+\n+    auto mod = op.getOperation()->getParentOfType<ModuleOp>();\n     unsigned numThreads =\n-        product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) * 32;\n+        product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) *\n+        triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n     unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n     Value readOffset = threadId;\n     for (unsigned round = 0; round < elemsPerThread; ++round) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.cpp", "status": "added", "additions": 324, "deletions": 0, "changes": 324, "file_content_changes": "@@ -0,0 +1,324 @@\n+#include \"ScanOpToLLVM.h\"\n+#include \"TritonGPUToLLVMBase.h\"\n+#include \"triton/Analysis/Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::LLVM::delinearize;\n+using ::mlir::LLVM::linearize;\n+using ::mlir::LLVM::shflUpSync;\n+using ::mlir::LLVM::storeShared;\n+\n+// Apply the region of the scan op to the acc and cur values and update acc\n+// inplace with the result.\n+static void accumulate(ConversionPatternRewriter &rewriter, Region &combineOp,\n+                       Value &acc, Value cur) {\n+  if (!acc) {\n+    acc = cur;\n+    return;\n+  }\n+  // Create a new copy of the reduce block, and inline it\n+  Block *currentBlock = rewriter.getBlock();\n+  Region &parent = *currentBlock->getParent();\n+  rewriter.cloneRegionBefore(combineOp, &parent.front());\n+  auto &newScan = parent.front();\n+  auto returnOp = dyn_cast<triton::ScanReturnOp>(newScan.getTerminator());\n+  llvm::SmallVector<Value> combineArgs = {acc, cur};\n+  rewriter.inlineBlockBefore(&newScan, &*rewriter.getInsertionPoint(),\n+                             combineArgs);\n+  auto results = returnOp.getResult();\n+  acc = results[0];\n+  // Delete the terminator, which is no longer used\n+  rewriter.eraseOp(returnOp);\n+}\n+\n+// Scan a contiguous elements within a thread and update `srcValues` in place.\n+static void scanThreadContiguousElements(SmallVector<Value> &srcValues,\n+                                         ConversionPatternRewriter &rewriter,\n+                                         ScanLoweringHelper &helper) {\n+  // Depending on layout contiguous elements along axis dim may not be\n+  // contiguous in srcValues. Keep track of what elements belong to the same\n+  // chunk of contiguous elements.\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n+  unsigned parallelElementsPerThread = helper.getAxisNumElementsPerThread();\n+  unsigned numChunks = srcValues.size() / scanElementsPerThreads;\n+  unsigned stride = helper.getAxisElementStride();\n+  SmallVector<Value> accs(numChunks);\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned accIndex = (srcIndex % stride) +\n+                        ((srcIndex / stride) / scanElementsPerThreads) * stride;\n+\n+    accumulate(rewriter, helper.getCombineOp(), accs[accIndex],\n+               srcValues[srcIndex]);\n+    srcValues[srcIndex] = accs[accIndex];\n+  }\n+}\n+\n+// Apply a scan across threads of the warp for the last element of each\n+// contiguous group of elements.\n+static void warpScan(SmallVector<Value> &srcValues,\n+                     ConversionPatternRewriter &rewriter,\n+                     ScanLoweringHelper &helper, Value laneId) {\n+  Location loc = helper.getLoc();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n+  unsigned elementStride = helper.getAxisElementStride();\n+  unsigned threadStride = helper.getAxisThreadStride();\n+  unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned elementIdx = (srcIndex / elementStride) % scanElementsPerThreads;\n+    // Only consider the last element of each contiguous chunk of elements.\n+    if (elementIdx != scanElementsPerThreads - 1)\n+      continue;\n+    // Reduce within warps.\n+    Value acc = srcValues[srcIndex];\n+    for (unsigned i = 1; i <= (scanDim) / 2; i = i << 1) {\n+      Value shfl = shflUpSync(loc, rewriter, acc, i * threadStride);\n+      Value tempAcc = acc;\n+      accumulate(rewriter, helper.getCombineOp(), tempAcc, shfl);\n+      Value mask = icmp_slt(laneId, i32_val(i));\n+      acc = select(mask, acc, tempAcc);\n+    }\n+    srcValues[srcIndex] = acc;\n+  }\n+}\n+\n+// For each set of contiguous elements within a thread we store the partial\n+// reduction into shared memory. Each parallel scan and each warp will store its\n+// own partial reductions. The shared memory is organized as follow:\n+//          -----------------------------------------------------------------\n+// chunk 0: | acc[0] warp 0 | acc[1] warp 0 | acc[0] warp 1 | acc[1] warp 1 |\n+// chunk 1: | acc[0] warp 0 | acc[1] warp 0 | acc[0] warp 1 | acc[1] warp 1 |\n+static void storeWarpAccumulator(SmallVector<Value> &srcValues,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ScanLoweringHelper &helper, Value laneId,\n+                                 Value warpId, Value baseSharedMemPtr,\n+                                 Value parallelLaneId) {\n+  Location loc = helper.getLoc();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n+  unsigned scanDim = helper.getAxisNumThreadsPerWarp();\n+  unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();\n+  unsigned numWarps = helper.getAxisNumWarps();\n+  unsigned chunkId = 0;\n+  unsigned elementStride = helper.getAxisElementStride();\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned elementIdx = (srcIndex / elementStride) % scanElementsPerThreads;\n+    // Only consider the last element of each contiguous chunk of elements.\n+    if (elementIdx != scanElementsPerThreads - 1)\n+      continue;\n+    Value lastElement = srcValues[srcIndex];\n+    Value mask = icmp_eq(laneId, i32_val(scanDim - 1));\n+    Value index = add(parallelLaneId, mul(warpId, i32_val(numParallelLane)));\n+    index = add(index, i32_val(chunkId * numParallelLane * numWarps));\n+    Value writePtr = gep(baseSharedMemPtr.getType(), baseSharedMemPtr, index);\n+    storeShared(rewriter, loc, writePtr, lastElement, mask);\n+    chunkId++;\n+  }\n+}\n+\n+// Read the partial reductions from shared memory from each chunk of contiguous\n+// elements for each warp and parallel scan. Then combine the partial reduction\n+// with the right elements. Within a given contiguous element chunk we update\n+// all the elements by accumulating the value from the last element of the\n+// reduced value from the previous lane.\n+static void AddPartialReduce(SmallVector<Value> &srcValues,\n+                             ConversionPatternRewriter &rewriter,\n+                             ScanLoweringHelper &helper, Value sharedMemoryPtr,\n+                             Value warpId, Value laneId, Value parallelLaneId) {\n+  Location loc = helper.getLoc();\n+  unsigned numParallelLane = helper.getNonAxisNumThreadsPerCTA();\n+  unsigned numWarps = helper.getAxisNumWarps();\n+  unsigned scanElementsPerThreads = helper.getAxisNumElementsPerThread();\n+  unsigned parallelElementsPerThread = helper.getNonAxisNumElementsPerThread();\n+  unsigned elementStride = helper.getAxisElementStride();\n+  unsigned threadStride = helper.getAxisThreadStride();\n+  Value maskFirstWarp = icmp_eq(warpId, i32_val(0));\n+  Value maskFirstLane = icmp_eq(laneId, i32_val(0));\n+  Value maskFirstThread = and_(maskFirstWarp, maskFirstLane);\n+  struct Accumulator {\n+    Value acc;\n+    Value maskedAcc;\n+  };\n+  unsigned numScanBlocks = helper.getAxisNumBlocks();\n+  unsigned numParallelBlocks = helper.getNonAxisNumBlocks();\n+  assert(numScanBlocks * numParallelBlocks * parallelElementsPerThread *\n+             scanElementsPerThreads ==\n+         srcValues.size());\n+  SmallVector<Accumulator> accumulators(numParallelBlocks *\n+                                        parallelElementsPerThread);\n+  unsigned chunkId = 0;\n+  unsigned blockStride = helper.getAxisBlockStride();\n+  for (unsigned srcIndex = 0; srcIndex < srcValues.size(); srcIndex++) {\n+    unsigned elementIdx = (srcIndex / elementStride) % scanElementsPerThreads;\n+    // Only consider the last element of each contiguous chunk of elements.\n+    if (elementIdx != scanElementsPerThreads - 1)\n+      continue;\n+    // Accumulate the partial reduction from shared memory. Decide which\n+    // accumulator to combine based on whether the elements belong to the same\n+    // dimension along axis.\n+    unsigned blockId = chunkId / parallelElementsPerThread;\n+    unsigned parallelBlockId =\n+        blockId % blockStride +\n+        ((blockId / blockStride) / numScanBlocks) * blockStride;\n+    unsigned accumulatorIndex = chunkId % parallelElementsPerThread +\n+                                parallelBlockId * parallelElementsPerThread;\n+    Accumulator &accumulator = accumulators[accumulatorIndex];\n+    for (unsigned i = 0; i < numWarps; ++i) {\n+      Value index = add(parallelLaneId,\n+                        i32_val(numParallelLane * (i + chunkId * numWarps)));\n+      Value ptr = gep(sharedMemoryPtr.getType(), sharedMemoryPtr, index);\n+      Value partialReduce = load(ptr);\n+      if (!accumulator.acc) {\n+        accumulator.acc = partialReduce;\n+        accumulator.maskedAcc = partialReduce;\n+        continue;\n+      }\n+      accumulate(rewriter, helper.getCombineOp(), accumulator.acc,\n+                 partialReduce);\n+      Value mask = icmp_slt(warpId, i32_val(i + 1));\n+      accumulator.maskedAcc =\n+          select(mask, accumulator.maskedAcc, accumulator.acc);\n+    }\n+    Value temp = srcValues[srcIndex];\n+    accumulate(rewriter, helper.getCombineOp(), temp, accumulator.maskedAcc);\n+    unsigned axisBlockId = (blockId / blockStride) % numScanBlocks;\n+    if (axisBlockId == 0) {\n+      // For the first warp and first chunk we don't have anything to\n+      // accumulate.\n+      temp = select(maskFirstWarp, srcValues[srcIndex], temp);\n+    }\n+    srcValues[srcIndex] = temp;\n+    // Update the rest of the contiguous elements.\n+    Value lastElement =\n+        shflUpSync(loc, rewriter, srcValues[srcIndex], threadStride);\n+    lastElement = select(maskFirstLane, accumulator.maskedAcc, lastElement);\n+    for (unsigned i = 1; i < scanElementsPerThreads; ++i) {\n+      Value laneValue = srcValues[srcIndex - i * elementStride];\n+      accumulate(rewriter, helper.getCombineOp(), laneValue, lastElement);\n+      if (axisBlockId == 0) {\n+        // For the first warp and first chunk we don't have anything to\n+        // accumulate.\n+        laneValue = select(maskFirstThread,\n+                           srcValues[srcIndex - i * elementStride], laneValue);\n+      }\n+      srcValues[srcIndex - i * elementStride] = laneValue;\n+    }\n+    // For the next chunk start back from the value containing the\n+    // accumulated value of all the warps.\n+    accumulator.maskedAcc = accumulator.acc;\n+    chunkId++;\n+  }\n+}\n+\n+namespace {\n+struct ScanOpConversion\n+    : public ConvertTritonGPUOpToLLVMPattern<triton::ScanOp> {\n+public:\n+  using ConvertTritonGPUOpToLLVMPattern<\n+      triton::ScanOp>::ConvertTritonGPUOpToLLVMPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    if (succeeded(emitFastScan(op, adaptor, rewriter)))\n+      return success();\n+    return failure();\n+  }\n+\n+private:\n+  std::tuple<Value, Value, Value>\n+  getDelinearizedIds(ConversionPatternRewriter &rewriter,\n+                     ScanLoweringHelper &helper) const;\n+  LogicalResult emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,\n+                             ConversionPatternRewriter &rewriter) const;\n+};\n+\n+// Break up the threadId into lane and warp id along the scan dimension and\n+// compute a flat id for the parallel dimensions.\n+std::tuple<Value, Value, Value>\n+ScanOpConversion::getDelinearizedIds(ConversionPatternRewriter &rewriter,\n+                                     ScanLoweringHelper &helper) const {\n+  auto loc = helper.getLoc();\n+  unsigned axis = helper.getAxis();\n+  auto srcEncoding = helper.getEncoding();\n+\n+  Value threadId = getThreadId(rewriter, loc);\n+  Value warpSize = i32_val(32);\n+  Value warpId = udiv(threadId, warpSize);\n+  Value laneId = urem(threadId, warpSize);\n+\n+  auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcEncoding);\n+  auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcEncoding);\n+  auto order = triton::gpu::getOrder(srcEncoding);\n+  SmallVector<Value> multiDimLaneId =\n+      delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n+  SmallVector<Value> multiDimWarpId =\n+      delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+\n+  Value laneIdAxis = multiDimLaneId[axis];\n+  Value warpIdAxis = multiDimWarpId[axis];\n+\n+  multiDimLaneId[axis] = i32_val(0);\n+  threadsPerWarp[axis] = 1;\n+  Value laneIdParallel =\n+      linearize(rewriter, loc, multiDimLaneId, threadsPerWarp, order);\n+  multiDimWarpId[axis] = i32_val(0);\n+  warpsPerCTA[axis] = 1;\n+  Value warpIdParallel =\n+      linearize(rewriter, loc, multiDimWarpId, warpsPerCTA, order);\n+  Value flatIdParallel =\n+      add(laneIdParallel,\n+          mul(warpIdParallel, i32_val(helper.getNonAxisNumThreadsPerWarp())));\n+  return std::make_tuple(laneIdAxis, warpIdAxis, flatIdParallel);\n+}\n+\n+// Lowering using warp shuffle operations to do warp level scan.\n+LogicalResult\n+ScanOpConversion::emitFastScan(triton::ScanOp op, triton::ScanOpAdaptor adaptor,\n+                               ConversionPatternRewriter &rewriter) const {\n+  ScanLoweringHelper helper(op);\n+  auto loc = helper.getLoc();\n+  if (!helper.isSupported())\n+    return failure();\n+\n+  auto [laneIdAxis, warpIdAxis, flatIdParallel] =\n+      getDelinearizedIds(rewriter, helper);\n+  auto input = adaptor.getOperands()[0];\n+  auto type = op.getOperand(0).getType().cast<RankedTensorType>();\n+  SmallVector<Value> srcValues =\n+      getTypeConverter()->unpackLLElements(loc, input, rewriter, type);\n+\n+  // Scan contigous elements in a thread and update `srcValues`.\n+  scanThreadContiguousElements(srcValues, rewriter, helper);\n+  // Apply warp level scan to the last element of each chunk of contiguous\n+  // elements.\n+  warpScan(srcValues, rewriter, helper, laneIdAxis);\n+\n+  // Store the partial reducing for each warp into shared memory.\n+  Type elemPtrTys = LLVM::LLVMPointerType::get(srcValues[0].getType(), 3);\n+  Value baseSharedMemPtr = bitcast(\n+      getSharedMemoryBase(loc, rewriter, op.getOperation()), elemPtrTys);\n+  storeWarpAccumulator(srcValues, rewriter, helper, laneIdAxis, warpIdAxis,\n+                       baseSharedMemPtr, flatIdParallel);\n+  barrier();\n+  // Read back the partial reduction of each warp and accumulate them based on\n+  // warpId. Then update each chunk of contiguous elements by adding the\n+  // accumulated value from the previous lane.\n+  AddPartialReduce(srcValues, rewriter, helper, baseSharedMemPtr, warpIdAxis,\n+                   laneIdAxis, flatIdParallel);\n+\n+  Value results = getTypeConverter()->packLLElements(loc, srcValues, rewriter,\n+                                                     input.getType());\n+  rewriter.replaceOp(op, results);\n+  return success();\n+}\n+} // namespace\n+\n+void populateScanOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit) {\n+  patterns.add<ScanOpConversion>(typeConverter, allocation, indexCacheInfo,\n+                                 benefit);\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ScanOpToLLVM.h", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "file_content_changes": "@@ -0,0 +1,15 @@\n+#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_SCAN_OP_H\n+#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_SCAN_OP_H\n+\n+#include \"TritonGPUToLLVMBase.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+void populateScanOpToLLVMPatterns(\n+    TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n+    ModuleAllocation &allocation,\n+    ConvertTritonGPUOpToLLVMPatternBase::IndexCacheInfo &indexCacheInfo,\n+    PatternBenefit benefit);\n+\n+#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "file_content_changes": "@@ -129,9 +129,15 @@ struct PrintOpConversion\n     } else if (type.isBF16() || type.isF16() || type.isF32() || type.isF64()) {\n       return \"%f\";\n     } else if (type.isSignedInteger()) {\n-      return \"%i\";\n+      if (type.getIntOrFloatBitWidth() == 64)\n+        return \"%lli\";\n+      else\n+        return \"%i\";\n     } else if (type.isUnsignedInteger() || type.isSignlessInteger()) {\n-      return \"%u\";\n+      if (type.getIntOrFloatBitWidth() == 64)\n+        return \"%llu\";\n+      else\n+        return \"%u\";\n     }\n     assert(false && \"not supported type\");\n     return \"\";"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 14, "deletions": 68, "changes": 82, "file_content_changes": "@@ -15,6 +15,7 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n+using ::mlir::LLVM::delinearize;\n using ::mlir::LLVM::SharedMemoryObject;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n@@ -504,65 +505,6 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     return mask;\n   }\n \n-  // Convert an \\param index to a multi-dim coordinate given \\param shape and\n-  // \\param order.\n-  SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n-                                 Location loc, Value linear,\n-                                 ArrayRef<unsigned> shape,\n-                                 ArrayRef<unsigned> order) const {\n-    unsigned rank = shape.size();\n-    assert(rank == order.size());\n-    auto reordered = reorder(shape, order);\n-    auto reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n-    SmallVector<Value> multiDim(rank);\n-    for (unsigned i = 0; i < rank; ++i) {\n-      multiDim[order[i]] = reorderedMultiDim[i];\n-    }\n-    return multiDim;\n-  }\n-\n-  SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n-                                 Location loc, Value linear,\n-                                 ArrayRef<unsigned> shape) const {\n-    unsigned rank = shape.size();\n-    assert(rank > 0);\n-    SmallVector<Value> multiDim(rank);\n-    if (rank == 1) {\n-      multiDim[0] = linear;\n-    } else {\n-      Value remained = linear;\n-      for (auto &&en : llvm::enumerate(shape.drop_back())) {\n-        Value dimSize = i32_val(en.value());\n-        multiDim[en.index()] = urem(remained, dimSize);\n-        remained = udiv(remained, dimSize);\n-      }\n-      multiDim[rank - 1] = remained;\n-    }\n-    return multiDim;\n-  }\n-\n-  Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n-                  ArrayRef<Value> multiDim, ArrayRef<unsigned> shape,\n-                  ArrayRef<unsigned> order) const {\n-    return linearize(rewriter, loc, reorder<Value>(multiDim, order),\n-                     reorder<unsigned>(shape, order));\n-  }\n-\n-  Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n-                  ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) const {\n-    auto rank = multiDim.size();\n-    Value linear = i32_val(0);\n-    if (rank > 0) {\n-      linear = multiDim.back();\n-      for (auto [dim, dimShape] :\n-           llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n-        Value dimSize = i32_val(dimShape);\n-        linear = add(mul(linear, dimSize), dim);\n-      }\n-    }\n-    return linear;\n-  }\n-\n   Value dot(ConversionPatternRewriter &rewriter, Location loc,\n             ArrayRef<Value> offsets, ArrayRef<Value> strides) const {\n     assert(offsets.size() == strides.size());\n@@ -927,19 +869,23 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                               const MmaEncodingAttr &mmaLayout,\n                               RankedTensorType type) const {\n     auto shape = type.getShape();\n-    auto _warpsPerCTA = mmaLayout.getWarpsPerCTA();\n-    assert(_warpsPerCTA.size() == 2);\n-    SmallVector<Value> warpsPerCTA = {i32_val(_warpsPerCTA[0]),\n-                                      i32_val(_warpsPerCTA[1])};\n+    auto warpsPerCTA = mmaLayout.getWarpsPerCTA();\n+    assert(warpsPerCTA.size() == 2);\n+    auto order = triton::gpu::getOrder(mmaLayout);\n     Value threadId = getThreadId(rewriter, loc);\n     Value warpSize = i32_val(32);\n     Value laneId = urem(threadId, warpSize);\n     Value warpId = udiv(threadId, warpSize);\n-    Value warpId0 = urem(urem(warpId, warpsPerCTA[0]), i32_val(shape[0] / 16));\n-    Value warpId1 = urem(urem(udiv(warpId, warpsPerCTA[0]), warpsPerCTA[1]),\n-                         i32_val(shape[1] / 8));\n-    Value offWarp0 = mul(warpId0, i32_val(16));\n-    Value offWarp1 = mul(warpId1, i32_val(8));\n+\n+    SmallVector<Value> multiDimWarpId =\n+        delinearize(rewriter, loc, warpId, warpsPerCTA, order);\n+    unsigned lastAxis = order[order.size() - 1];\n+    multiDimWarpId[lastAxis] =\n+        urem(multiDimWarpId[lastAxis], i32_val(warpsPerCTA[lastAxis]));\n+    multiDimWarpId[0] = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n+    multiDimWarpId[1] = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n+    Value offWarp0 = mul(multiDimWarpId[0], i32_val(16));\n+    Value offWarp1 = mul(multiDimWarpId[1], i32_val(8));\n \n     SmallVector<Value> multiDimBase(2);\n     multiDimBase[0] = add(udiv(laneId, i32_val(4)), offWarp0);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -26,6 +26,7 @@\n #include \"ElementwiseOpToLLVM.h\"\n #include \"LoadStoreOpToLLVM.h\"\n #include \"ReduceOpToLLVM.h\"\n+#include \"ScanOpToLLVM.h\"\n #include \"TritonGPUToLLVM.h\"\n #include \"TypeConverter.h\"\n #include \"ViewOpToLLVM.h\"\n@@ -372,6 +373,8 @@ class ConvertTritonGPUToLLVM\n                                       /*benefit=*/1);\n     populateReduceOpToLLVMPatterns(typeConverter, patterns, allocation,\n                                    indexCacheInfo, /*benefit=*/1);\n+    populateScanOpToLLVMPatterns(typeConverter, patterns, allocation,\n+                                 indexCacheInfo, /*benefit=*/1);\n     populateViewOpToLLVMPatterns(typeConverter, patterns, /*benefit=*/1);\n \n     // Native lowering patterns"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 93, "deletions": 6, "changes": 99, "file_content_changes": "@@ -70,6 +70,82 @@ getStridesFromShapeAndOrder(ArrayRef<int64_t> shape, ArrayRef<unsigned> order,\n   return strides;\n }\n \n+// Convert an \\param index to a multi-dim coordinate given \\param shape and\n+// \\param order.\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, Value linear,\n+                               ArrayRef<unsigned> shape,\n+                               ArrayRef<unsigned> order) {\n+  unsigned rank = shape.size();\n+  assert(rank == order.size());\n+  auto reordered = reorder(shape, order);\n+  SmallVector<Value> reorderedMultiDim(rank);\n+  if (auto constantOp = linear.getDefiningOp<arith::ConstantOp>()) {\n+    unsigned intVal =\n+        constantOp.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n+    reorderedMultiDim = delinearize(rewriter, loc, intVal, reordered);\n+  } else {\n+    reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n+  }\n+  SmallVector<Value> multiDim(rank);\n+  for (unsigned i = 0; i < rank; ++i) {\n+    multiDim[order[i]] = reorderedMultiDim[i];\n+  }\n+  return multiDim;\n+}\n+\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, unsigned linear,\n+                               ArrayRef<unsigned> shape) {\n+  unsigned rank = shape.size();\n+  assert(rank > 0);\n+  SmallVector<Value> multiDim(rank);\n+  unsigned remained = linear;\n+  for (auto &&en : llvm::enumerate(shape)) {\n+    unsigned dimSize = en.value();\n+    multiDim[en.index()] = i32_val(remained % dimSize);\n+    remained = remained / dimSize;\n+  }\n+  return multiDim;\n+}\n+\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, Value linear,\n+                               ArrayRef<unsigned> shape) {\n+  unsigned rank = shape.size();\n+  assert(rank > 0);\n+  SmallVector<Value> multiDim(rank);\n+  Value remained = linear;\n+  for (auto &&en : llvm::enumerate(shape)) {\n+    Value dimSize = i32_val(en.value());\n+    multiDim[en.index()] = urem(remained, dimSize);\n+    remained = udiv(remained, dimSize);\n+  }\n+  return multiDim;\n+}\n+\n+Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                ArrayRef<Value> multiDim, ArrayRef<unsigned> shape,\n+                ArrayRef<unsigned> order) {\n+  return linearize(rewriter, loc, reorder<Value>(multiDim, order),\n+                   reorder<unsigned>(shape, order));\n+}\n+\n+Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                ArrayRef<Value> multiDim, ArrayRef<unsigned> shape) {\n+  auto rank = multiDim.size();\n+  Value linear = i32_val(0);\n+  if (rank > 0) {\n+    linear = multiDim.back();\n+    for (auto [dim, dimShape] :\n+         llvm::reverse(llvm::zip(multiDim.drop_back(), shape.drop_back()))) {\n+      Value dimSize = i32_val(dimShape);\n+      linear = add(mul(linear, dimSize), dim);\n+    }\n+  }\n+  return linear;\n+}\n+\n Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n                   Value val, Value pred) {\n   MLIRContext *ctx = rewriter.getContext();\n@@ -84,34 +160,45 @@ Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n   return builder.launch(rewriter, loc, void_ty(ctx));\n }\n \n-Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n-               int i) {\n+static Value commonShflSync(Location loc, ConversionPatternRewriter &rewriter,\n+                            Value val, int i, const std::string &shuffleType,\n+                            const std::string &clamp) {\n   unsigned bits = val.getType().getIntOrFloatBitWidth();\n \n   if (bits == 64) {\n     Type vecTy = vec_ty(f32_ty, 2);\n     Value vec = bitcast(val, vecTy);\n     Value val0 = extract_element(f32_ty, vec, i32_val(0));\n     Value val1 = extract_element(f32_ty, vec, i32_val(1));\n-    val0 = shflSync(loc, rewriter, val0, i);\n-    val1 = shflSync(loc, rewriter, val1, i);\n+    val0 = commonShflSync(loc, rewriter, val0, i, shuffleType, clamp);\n+    val1 = commonShflSync(loc, rewriter, val1, i, shuffleType, clamp);\n     vec = undef(vecTy);\n     vec = insert_element(vecTy, vec, val0, i32_val(0));\n     vec = insert_element(vecTy, vec, val1, i32_val(1));\n     return bitcast(vec, val.getType());\n   }\n \n   PTXBuilder builder;\n-  auto &shfl = builder.create(\"shfl.sync\")->o(\"bfly\").o(\"b32\");\n+  auto &shfl = builder.create(\"shfl.sync\")->o(shuffleType).o(\"b32\");\n   auto *dOpr = builder.newOperand(\"=r\");\n   auto *aOpr = builder.newOperand(val, \"r\");\n   auto *bOpr = builder.newConstantOperand(i);\n-  auto *cOpr = builder.newConstantOperand(\"0x1f\");\n+  auto *cOpr = builder.newConstantOperand(clamp);\n   auto *maskOpr = builder.newConstantOperand(\"0xffffffff\");\n   shfl(dOpr, aOpr, bOpr, cOpr, maskOpr);\n   return builder.launch(rewriter, loc, val.getType(), false);\n }\n \n+Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+               int i) {\n+  return commonShflSync(loc, rewriter, val, i, \"bfly\", \"0x1f\");\n+}\n+\n+Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+                 int i) {\n+  return commonShflSync(loc, rewriter, val, i, \"up\", \"0x0\");\n+}\n+\n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content) {\n   auto moduleOp = rewriter.getBlock()->getParent()->getParentOfType<ModuleOp>();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "file_content_changes": "@@ -258,11 +258,35 @@ SharedMemoryObject\n getSharedMemoryObjectFromStruct(Location loc, Value llvmStruct,\n                                 ConversionPatternRewriter &rewriter);\n \n+// Convert an \\param index to a multi-dim coordinate given \\param shape and\n+// \\param order.\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, Value linear,\n+                               ArrayRef<unsigned> shape,\n+                               ArrayRef<unsigned> order);\n+\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, unsigned linear,\n+                               ArrayRef<unsigned> shape);\n+\n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, Value linear,\n+                               ArrayRef<unsigned> shape);\n+\n+Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                ArrayRef<Value> multiDim, ArrayRef<unsigned> shape,\n+                ArrayRef<unsigned> order);\n+\n+Value linearize(ConversionPatternRewriter &rewriter, Location loc,\n+                ArrayRef<Value> multiDim, ArrayRef<unsigned> shape);\n+\n Value storeShared(ConversionPatternRewriter &rewriter, Location loc, Value ptr,\n                   Value val, Value pred);\n \n Value shflSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n                int i);\n+Value shflUpSync(Location loc, ConversionPatternRewriter &rewriter, Value val,\n+                 int i);\n \n Value addStringToModule(Location loc, ConversionPatternRewriter &rewriter,\n                         StringRef key, StringRef content);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 20, "deletions": 1, "changes": 21, "file_content_changes": "@@ -22,7 +22,26 @@ struct SplatOpConversion\n                                   ConversionPatternRewriter &rewriter,\n                                   Location loc) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n-    auto srcType = typeConverter->convertType(elemType);\n+    // Check the converted type for the tensor as depending on the encoding the\n+    // converter may pick different element types.\n+    auto srcType = typeConverter->convertType(tensorTy);\n+    if (auto structTy = dyn_cast<LLVM::LLVMStructType>(srcType))\n+      srcType = structTy.getBody()[0];\n+    // If the type sizes don't match we need to pack constants.\n+    if (srcType.isIntOrFloat() && constVal.getType().getIntOrFloatBitWidth() !=\n+                                      srcType.getIntOrFloatBitWidth()) {\n+      unsigned cstBitWidth = constVal.getType().getIntOrFloatBitWidth();\n+      unsigned srcBitWidth = srcType.getIntOrFloatBitWidth();\n+      assert(cstBitWidth <= srcBitWidth && srcBitWidth % cstBitWidth == 0);\n+      unsigned ratio = srcBitWidth / cstBitWidth;\n+      Type intTy = IntegerType::get(elemType.getContext(), cstBitWidth);\n+      VectorType vecType = VectorType::get(ratio, intTy);\n+      Value intCst = bitcast(constVal, intTy);\n+      Value vec = undef(vecType);\n+      for (unsigned i = 0; i < ratio; ++i)\n+        vec = insert_element(vecType, vec, intCst, int_val(32, i));\n+      constVal = vec;\n+    }\n     auto llSrc = bitcast(constVal, srcType);\n     size_t elemsPerThread = getTotalElemsPerThread(tensorTy);\n     llvm::SmallVector<Value> elems(elemsPerThread, llSrc);"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 54, "deletions": 7, "changes": 61, "file_content_changes": "@@ -154,10 +154,24 @@ class StdSelectPattern : public OpConversionPattern<arith::SelectOp> {\n   matchAndRewrite(arith::SelectOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n     Type retType = this->getTypeConverter()->convertType(op.getType());\n-    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n-                      op, retType, adaptor.getCondition(),\n-                      adaptor.getTrueValue(), adaptor.getFalseValue()),\n-                  adaptor.getAttributes());\n+\n+    Value cond = adaptor.getCondition();\n+    if (llvm::isa<RankedTensorType>(retType) &&\n+        !llvm::isa<TensorType>(cond.getType())) {\n+      // triton_gpu.select doesn't support scalar condition values, so add a\n+      // splat\n+      auto retTypeTensor = llvm::cast<RankedTensorType>(retType);\n+      auto retShape = retTypeTensor.getShape();\n+      auto retEncoding = retTypeTensor.getEncoding();\n+      Type condTy =\n+          RankedTensorType::get(retShape, cond.getType(), retEncoding);\n+      cond = rewriter.create<triton::SplatOp>(op.getLoc(), condTy, cond);\n+    }\n+\n+    addNamedAttrs(\n+        rewriter.replaceOpWithNewOp<triton::gpu::SelectOp>(\n+            op, retType, cond, adaptor.getTrueValue(), adaptor.getFalseValue()),\n+        adaptor.getAttributes());\n     return success();\n   }\n };\n@@ -537,6 +551,38 @@ struct TritonReduceReturnPattern\n   }\n };\n \n+struct TritonScanPattern : public OpConversionPattern<triton::ScanOp> {\n+  using OpConversionPattern<triton::ScanOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    auto newScan = rewriter.create<triton::ScanOp>(\n+        op.getLoc(), adaptor.getOperands(), adaptor.getAxis());\n+    addNamedAttrs(newScan, adaptor.getAttributes());\n+\n+    auto &newCombineOp = newScan.getCombineOp();\n+    rewriter.cloneRegionBefore(op.getCombineOp(), newCombineOp,\n+                               newCombineOp.end());\n+    rewriter.replaceOp(op, newScan.getResult());\n+    return success();\n+  }\n+};\n+\n+struct TritonScanReturnPattern\n+    : public OpConversionPattern<triton::ScanReturnOp> {\n+  using OpConversionPattern<triton::ScanReturnOp>::OpConversionPattern;\n+\n+  LogicalResult\n+  matchAndRewrite(triton::ScanReturnOp op, OpAdaptor adaptor,\n+                  ConversionPatternRewriter &rewriter) const override {\n+    addNamedAttrs(rewriter.replaceOpWithNewOp<triton::ScanReturnOp>(\n+                      op, adaptor.getResult()),\n+                  adaptor.getAttributes());\n+    return success();\n+  }\n+};\n+\n struct TritonPrintPattern : public OpConversionPattern<triton::PrintOp> {\n   using OpConversionPattern<triton::PrintOp>::OpConversionPattern;\n \n@@ -623,9 +669,10 @@ void populateTritonPatterns(TritonGPUTypeConverter &typeConverter,\n           TritonGenericPattern<triton::PtrToIntOp>,\n           TritonGenericPattern<triton::SplatOp>, TritonBroadcastPattern,\n           TritonGenericPattern<triton::AddPtrOp>, TritonCatPattern,\n-          TritonReducePattern, TritonReduceReturnPattern, TritonTransPattern,\n-          TritonExpandDimsPattern, TritonMakeRangePattern, TritonDotPattern,\n-          TritonLoadPattern, TritonStorePattern,\n+          TritonReducePattern, TritonReduceReturnPattern, TritonScanPattern,\n+          TritonScanReturnPattern, TritonTransPattern, TritonExpandDimsPattern,\n+          TritonMakeRangePattern, TritonDotPattern, TritonLoadPattern,\n+          TritonStorePattern,\n           TritonExternElementwisePattern<triton::PureExternElementwiseOp>,\n           TritonExternElementwisePattern<triton::ImpureExternElementwiseOp>,\n           TritonPrintPattern, TritonAssertPattern, TritonAtomicRMWPattern,"}, {"filename": "lib/Dialect/Triton/IR/Ops.cpp", "status": "modified", "additions": 50, "deletions": 0, "changes": 50, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"mlir/IR/OperationSupport.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/Triton/IR/Types.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Attributes.h\"\n \n namespace mlir {\n namespace triton {\n@@ -398,6 +399,25 @@ mlir::LogicalResult mlir::triton::DotOp::inferReturnTypes(\n   return mlir::success();\n }\n \n+LogicalResult mlir::triton::DotOp::verify() {\n+  auto aTy = getOperand(0).getType().cast<RankedTensorType>();\n+  auto bTy = getOperand(1).getType().cast<RankedTensorType>();\n+  if (aTy.getElementType() != bTy.getElementType())\n+    return emitError(\"element types of operands A and B must match\");\n+  auto aEncoding =\n+      aTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  auto bEncoding =\n+      bTy.getEncoding().dyn_cast_or_null<triton::gpu::DotOperandEncodingAttr>();\n+  if (!aEncoding && !bEncoding)\n+    return mlir::success();\n+  // Verify that the encodings are valid.\n+  if (!aEncoding || !bEncoding)\n+    return emitError(\"mismatching encoding between A and B operands\");\n+  if (aEncoding.getMMAv2kWidth() != bEncoding.getMMAv2kWidth())\n+    return emitError(\"mismatching kWidth between A and B operands\");\n+  return mlir::success();\n+}\n+\n //-- ReduceOp --\n static mlir::LogicalResult\n inferReduceReturnShape(const RankedTensorType &argTy, const Type &retEltTy,\n@@ -539,6 +559,36 @@ llvm::SmallVector<Type> ReduceOp::getElementTypes() {\n \n unsigned ReduceOp::getNumOperands() { return this->getOperands().size(); }\n \n+//-- ScanOp --\n+void ScanOp::build(mlir::OpBuilder &builder, mlir::OperationState &state,\n+                   mlir::ValueRange operands, int axis) {\n+  SmallVector<Type> inferredReturnTypes;\n+  for (auto arg : operands)\n+    inferredReturnTypes.push_back(arg.getType());\n+  ReduceOp::build(builder, state, inferredReturnTypes, operands, axis);\n+}\n+\n+mlir::LogicalResult mlir::triton::ScanOp::inferReturnTypes(\n+    MLIRContext *context, std::optional<Location> location, ValueRange operands,\n+    DictionaryAttr attributes, OpaqueProperties properties, RegionRange regions,\n+    SmallVectorImpl<Type> &inferredReturnTypes) {\n+  for (auto arg : operands)\n+    inferredReturnTypes.push_back(arg.getType());\n+  return success();\n+}\n+\n+mlir::LogicalResult mlir::triton::ScanOp::verify() {\n+  if (this->getOperands().size() < 1) {\n+    return this->emitOpError() << \"must have at least 1 operand\";\n+  }\n+  for (const auto &operand : this->getOperands()) {\n+    if (!dyn_cast<RankedTensorType>(operand.getType())) {\n+      return this->emitOpError() << \"operands must be RankedTensorType\";\n+    }\n+  }\n+  return success();\n+}\n+\n //-- SplatOp --\n OpFoldResult SplatOp::fold(FoldAdaptor adaptor) {\n   auto value = adaptor.getSrc();"}, {"filename": "lib/Dialect/Triton/Transforms/Combine.cpp", "status": "modified", "additions": 88, "deletions": 0, "changes": 88, "file_content_changes": "@@ -101,6 +101,93 @@ class CombineSelectMaskedLoadPattern : public mlir::RewritePattern {\n   }\n };\n \n+// sum(x[:, :, None] * y[None, :, :], 1)\n+// -> dot(x, y)\n+class CombineBroadcastMulReducePattern : public mlir::RewritePattern {\n+private:\n+  static bool isAddF32(const Operation *op) {\n+    if (auto addf = dyn_cast_or_null<arith::AddFOp>(op))\n+      return addf.getType().getIntOrFloatBitWidth() <= 32;\n+    return false;\n+  }\n+\n+  static SmallVector<int> getEqualIndices(ArrayRef<int64_t> x,\n+                                          ArrayRef<int64_t> y) {\n+    SmallVector<int> res;\n+    for (int i = 0; i < x.size(); ++i)\n+      if (x[i] == y[i])\n+        res.push_back(i);\n+    return res;\n+  }\n+\n+public:\n+  CombineBroadcastMulReducePattern(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(triton::ReduceOp::getOperationName(), 1, context) {\n+  }\n+\n+  mlir::LogicalResult matchAndRewrite(mlir::Operation *op,\n+                                      mlir::PatternRewriter &rewriter) const {\n+    auto reduceOp = llvm::dyn_cast<triton::ReduceOp>(op);\n+    if (!reduceOp)\n+      return mlir::failure();\n+    // only support reduce with simple addition\n+    Region &combineOp = reduceOp.getCombineOp();\n+    bool isReduceAdd = combineOp.hasOneBlock() &&\n+                       combineOp.front().getOperations().size() == 2 &&\n+                       isAddF32(&*combineOp.front().getOperations().begin());\n+    if (!isReduceAdd)\n+      return mlir::failure();\n+    // operand of reduce has to be mul\n+    auto mulOp = llvm::dyn_cast_or_null<arith::MulFOp>(\n+        reduceOp.getOperand(0).getDefiningOp());\n+    if (!mulOp)\n+      return mlir::failure();\n+    // mul operand has to be broadcast\n+    auto broadcastLhsOp = llvm::dyn_cast_or_null<triton::BroadcastOp>(\n+        mulOp.getOperand(0).getDefiningOp());\n+    if (!broadcastLhsOp)\n+      return mlir::failure();\n+    auto broadcastRhsOp = llvm::dyn_cast_or_null<triton::BroadcastOp>(\n+        mulOp.getOperand(1).getDefiningOp());\n+    if (!broadcastRhsOp)\n+      return mlir::failure();\n+    // broadcast operand is expand dims\n+    auto expandLhsOp = llvm::dyn_cast_or_null<triton::ExpandDimsOp>(\n+        broadcastLhsOp.getOperand().getDefiningOp());\n+    if (!expandLhsOp)\n+      return mlir::failure();\n+    auto expandRhsOp = llvm::dyn_cast_or_null<triton::ExpandDimsOp>(\n+        broadcastRhsOp.getOperand().getDefiningOp());\n+    if (!expandRhsOp)\n+      return mlir::failure();\n+    // get not-broadcast dimensions\n+    int expandLhsAxis = expandLhsOp.getAxis();\n+    int expandRhsAxis = expandRhsOp.getAxis();\n+    if (expandLhsAxis != 2 || expandRhsAxis != 0)\n+      return mlir::failure();\n+    auto broadcastLhsShape =\n+        broadcastLhsOp.getType().cast<ShapedType>().getShape();\n+    auto broadcastRhsShape =\n+        broadcastLhsOp.getType().cast<ShapedType>().getShape();\n+    if (broadcastLhsShape[2] < 16 || broadcastRhsShape[0] < 16)\n+      return mlir::failure();\n+    Type newAccType =\n+        RankedTensorType::get({broadcastLhsShape[0], broadcastRhsShape[2]},\n+                              broadcastLhsOp.getOperand()\n+                                  .getType()\n+                                  .cast<ShapedType>()\n+                                  .getElementType());\n+    rewriter.setInsertionPoint(op);\n+    auto newAcc = rewriter.create<triton::SplatOp>(\n+        op->getLoc(), newAccType,\n+        rewriter.create<arith::ConstantOp>(op->getLoc(),\n+                                           rewriter.getF32FloatAttr(0)));\n+    rewriter.replaceOpWithNewOp<triton::DotOp>(\n+        op, expandLhsOp.getOperand(), expandRhsOp.getOperand(), newAcc, true);\n+    return mlir::success();\n+  }\n+};\n+\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/Triton/Transforms/Passes.h.inc\"\n \n@@ -120,6 +207,7 @@ class CombineOpsPass : public TritonCombineOpsBase<CombineOpsPass> {\n     patterns.add<CombineSelectMaskedLoadPattern>(context);\n     // patterns.add<CombineAddPtrPattern>(context);\n     patterns.add<CombineBroadcastConstantPattern>(context);\n+    patterns.add<CombineBroadcastMulReducePattern>(context);\n \n     if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n       signalPassFailure();"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 17, "deletions": 2, "changes": 19, "file_content_changes": "@@ -7,7 +7,6 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.cpp.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n \n using namespace mlir;\n@@ -85,6 +84,8 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parent = sliceLayout.getParent();\n     auto parentThreadsPerWarp = getThreadsPerWarp(parent);\n+    assert(parentThreadsPerWarp.size() == 2 &&\n+           \"getThreadsPerWarp only implemented for 2D slice layout\");\n     SmallVector<unsigned> threadsPerWarp = parentThreadsPerWarp;\n     threadsPerWarp.erase(threadsPerWarp.begin() + sliceLayout.getDim());\n     for (unsigned i = 0; i < threadsPerWarp.size(); i++)\n@@ -129,6 +130,8 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parent = sliceLayout.getParent();\n     auto parentWarpsPerCTA = getWarpsPerCTA(parent);\n+    assert(parentWarpsPerCTA.size() == 2 &&\n+           \"getWarpsPerCTA only implemented for 2D slice layout\");\n     SmallVector<unsigned> warpsPerCTA = parentWarpsPerCTA;\n     warpsPerCTA.erase(warpsPerCTA.begin() + sliceLayout.getDim());\n     for (unsigned i = 0; i < warpsPerCTA.size(); i++)\n@@ -364,9 +367,21 @@ bool isSharedEncoding(Value value) {\n   return false;\n }\n \n+bool isExpensiveCat(CatOp cat, Attribute &targetEncoding) {\n+  // If the new elements per thread is less than the old one, we will need to do\n+  // convert encoding that goes through shared memory anyway. So we consider it\n+  // as expensive.\n+  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n+  auto totalElemsPerThread = gpu::getTotalElemsPerThread(tensorTy);\n+  auto shape = tensorTy.getShape();\n+  auto elemTy = tensorTy.getElementType();\n+  auto newTotalElemsPerThread =\n+      gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n+  return newTotalElemsPerThread < totalElemsPerThread;\n+}\n+\n } // namespace gpu\n } // namespace triton\n-\n } // namespace mlir\n \n static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 189, "deletions": 50, "changes": 239, "file_content_changes": "@@ -14,6 +14,7 @@ using triton::DotOp;\n using triton::gpu::ConvertLayoutOp;\n using triton::gpu::DotOperandEncodingAttr;\n using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SharedEncodingAttr;\n using triton::gpu::SliceEncodingAttr;\n \n // convert(trans(convert(arg)))\n@@ -78,16 +79,15 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n \n public:\n   MoveOpAfterLayoutConversion(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n-                             1, context) {}\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, context) {}\n \n   static mlir::LogicalResult\n   isBlockedToDotOperand(mlir::Operation *op,\n                         triton::gpu::DotOperandEncodingAttr &retEncoding,\n                         triton::gpu::BlockedEncodingAttr &srcEncoding) {\n-    if (!op)\n+    auto cvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(op);\n+    if (!cvt)\n       return failure();\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n     auto srcTy = cvt.getOperand().getType().cast<RankedTensorType>();\n     auto retTy = cvt.getResult().getType().dyn_cast<RankedTensorType>();\n     retEncoding =\n@@ -135,63 +135,198 @@ class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n-    triton::gpu::DotOperandEncodingAttr retEncoding;\n-    triton::gpu::BlockedEncodingAttr srcEncoding;\n-    if (isBlockedToDotOperand(op, retEncoding, srcEncoding).failed())\n-      return mlir::failure();\n-\n+    auto dotOp = cast<triton::DotOp>(op);\n     // only supports dot NT\n-    auto users = cvt->getUsers();\n-    auto dotOp = dyn_cast_or_null<DotOp>(*users.begin());\n-    if (!dotOp)\n-      return failure();\n     if (!isDotNT(dotOp))\n       return failure();\n+    bool changed = false;\n+    for (Value operand : {dotOp.getOperand(0), dotOp.getOperand(1)}) {\n+      auto cvt = operand.getDefiningOp<triton::gpu::ConvertLayoutOp>();\n+      triton::gpu::DotOperandEncodingAttr retEncoding;\n+      triton::gpu::BlockedEncodingAttr srcEncoding;\n+      bool failed =\n+          isBlockedToDotOperand(cvt, retEncoding, srcEncoding).failed();\n+      assert(!failed);\n \n-    // don't move things around when cvt operand is a block arg\n-    Operation *argOp = cvt.getOperand().getDefiningOp();\n-    if (!argOp)\n-      return failure();\n-    //\n-    SetVector<Operation *> processed;\n-    SetVector<Attribute> layout;\n-    llvm::MapVector<Value, Attribute> toConvert;\n-    int numCvts = simulateBackwardRematerialization(cvt, processed, layout,\n-                                                    toConvert, retEncoding);\n-    if (numCvts > 1 || toConvert.size() == 1)\n-      return failure();\n-    for (Operation *op : processed) {\n-      if (op->getNumOperands() != 1)\n+      // don't move things around when cvt operand is a block arg\n+      Operation *argOp = cvt.getOperand().getDefiningOp();\n+      if (!argOp)\n+        continue;\n+      SetVector<Operation *> processed;\n+      SetVector<Attribute> layout;\n+      llvm::MapVector<Value, Attribute> toConvert;\n+      int numCvts = simulateBackwardRematerialization(cvt, processed, layout,\n+                                                      toConvert, retEncoding);\n+      if (numCvts > 1 || toConvert.size() == 1)\n         continue;\n-      auto srcTy = op->getOperand(0).getType().cast<RankedTensorType>();\n-      auto dstTy = op->getResult(0).getType().cast<RankedTensorType>();\n-      // we don't want to push conversions backward if there is a downcast\n-      // since it would result in more shared memory traffic\n-      if (srcTy.getElementType().getIntOrFloatBitWidth() >\n-          dstTy.getElementType().getIntOrFloatBitWidth())\n-        return failure();\n-      // we only push back when the first op in the chain has a load operand\n-      if ((op == processed.back()) &&\n-          !isa<triton::LoadOp>(op->getOperand(0).getDefiningOp()))\n-        return failure();\n-      // we don't want to use ldmatrix for 8-bit data that requires trans\n-      // since Nvidia GPUs can't do it efficiently\n-      int kOrder = retEncoding.getOpIdx() ^ 1;\n-      bool isTrans = kOrder != srcEncoding.getOrder()[0];\n-      bool isInt8 = srcTy.getElementType().getIntOrFloatBitWidth() == 8;\n-      if (isTrans && isInt8)\n-        return failure();\n+      bool replaceOperand = true;\n+      for (Operation *op : processed) {\n+        if (op->getNumOperands() != 1)\n+          continue;\n+        auto srcTy = op->getOperand(0).getType().cast<RankedTensorType>();\n+        auto dstTy = op->getResult(0).getType().cast<RankedTensorType>();\n+        // we don't want to push conversions backward if there is a downcast\n+        // since it would result in more shared memory traffic\n+        if (srcTy.getElementType().getIntOrFloatBitWidth() >\n+            dstTy.getElementType().getIntOrFloatBitWidth()) {\n+          replaceOperand = false;\n+          break;\n+        }\n+        // we only push back when the first op in the chain has a load operand\n+        if ((op == processed.back()) &&\n+            !isa<triton::LoadOp>(op->getOperand(0).getDefiningOp())) {\n+          replaceOperand = false;\n+          break;\n+        }\n+        // we don't want to use ldmatrix for 8-bit data that requires trans\n+        // since Nvidia GPUs can't do it efficiently\n+        int kOrder = retEncoding.getOpIdx() ^ 1;\n+        bool isTrans = kOrder != srcEncoding.getOrder()[0];\n+        bool isInt8 = srcTy.getElementType().getIntOrFloatBitWidth() == 8;\n+        if (isTrans && isInt8) {\n+          replaceOperand = false;\n+          break;\n+        }\n+      }\n+      if (!replaceOperand)\n+        continue;\n+      IRMapping mapping;\n+      rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n+      rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n+      changed = true;\n     }\n-    IRMapping mapping;\n-    rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n-    rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n-    return mlir::success();\n+    return mlir::success(changed);\n   }\n };\n \n } // namespace\n \n+static bool isConvertToDotEncoding(Operation *op) {\n+  auto convertLayout = llvm::dyn_cast<ConvertLayoutOp>(op);\n+  if (!convertLayout)\n+    return false;\n+  auto tensorType =\n+      convertLayout.getResult().getType().cast<RankedTensorType>();\n+  return tensorType.getEncoding().isa<DotOperandEncodingAttr>();\n+}\n+\n+static ConvertLayoutOp updateConvert(OpBuilder &builder, ConvertLayoutOp cvt,\n+                                     IRMapping &mapping, Type smallestType) {\n+  auto cvtDstTy = cvt.getResult().getType().cast<RankedTensorType>();\n+  auto cvtDstEnc = cvtDstTy.getEncoding().cast<DotOperandEncodingAttr>();\n+  Value operand = cvt.getOperand();\n+  if (mapping.contains(operand))\n+    operand = mapping.lookup(operand);\n+  auto newDstTy = RankedTensorType::get(\n+      cvtDstTy.getShape(), cvtDstTy.getElementType(),\n+      DotOperandEncodingAttr::get(cvtDstEnc.getContext(), cvtDstEnc.getOpIdx(),\n+                                  cvtDstEnc.getParent(), smallestType));\n+  auto newCvt =\n+      builder.create<ConvertLayoutOp>(cvt.getLoc(), newDstTy, operand);\n+  mapping.map(cvt.getResult(), newCvt.getResult());\n+  return newCvt;\n+}\n+\n+// Update kWidth based on the smallestType found in the given convert ops and\n+// propagate the type change.\n+static void\n+updateDotEncodingLayout(SmallVector<ConvertLayoutOp> &convertsToDotEncoding,\n+                        Type smallestType) {\n+  IRMapping mapping;\n+  OpBuilder builder(smallestType.getContext());\n+  SetVector<Operation *> slices(convertsToDotEncoding.begin(),\n+                                convertsToDotEncoding.end());\n+  // Collect all the operations where the type needs to be propagated.\n+  for (auto cvt : convertsToDotEncoding) {\n+    auto forwardFilter = [&](Operation *op) {\n+      if (op == cvt.getOperation())\n+        return true;\n+      for (Value operand : op->getOperands()) {\n+        auto tensorType = operand.getType().dyn_cast<RankedTensorType>();\n+        if (tensorType &&\n+            tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n+          return true;\n+      }\n+      return false;\n+    };\n+    auto backwardFilter = [&](Operation *op) {\n+      for (Value results : op->getResults()) {\n+        auto tensorType = results.getType().dyn_cast<RankedTensorType>();\n+        if (tensorType &&\n+            tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n+          return true;\n+      }\n+      return false;\n+    };\n+    SetVector<Operation *> opSlice =\n+        getSlice(cvt.getOperation(), {backwardFilter}, {forwardFilter});\n+    slices.insert(opSlice.begin(), opSlice.end());\n+  }\n+  // Apply the type change by walking ops in topological order.\n+  slices = mlir::topologicalSort(slices);\n+  for (Operation *op : slices) {\n+    builder.setInsertionPoint(op);\n+    if (isConvertToDotEncoding(op)) {\n+      auto cvt = cast<ConvertLayoutOp>(op);\n+      ConvertLayoutOp newCvt =\n+          updateConvert(builder, cvt, mapping, smallestType);\n+      continue;\n+    }\n+    auto *newOp = cloneWithInferType(builder, op, mapping);\n+    for (auto [result, newResult] :\n+         llvm::zip(op->getResults(), newOp->getResults())) {\n+      result.replaceUsesWithIf(newResult, [&](OpOperand &operand) {\n+        return slices.count(operand.getOwner()) == 0;\n+      });\n+    }\n+  }\n+  for (Operation *op : llvm::reverse(slices))\n+    op->erase();\n+}\n+\n+// Change the layout of dotOperand layout to use the kWidth from the smallest\n+// loaded type. This allows better code generation for mixed-mode matmul.\n+static void optimizeKWidth(triton::FuncOp func) {\n+  SmallVector<ConvertLayoutOp> convertsToDotEncoding;\n+  Type smallestType;\n+  func->walk([&](triton::LoadOp loadOp) {\n+    if (!loadOp.getResult().hasOneUse())\n+      return;\n+    Operation *use = *loadOp.getResult().getUsers().begin();\n+\n+    // Advance to the first conversion as long as the use resides in shared\n+    // memory and it has a single use itself\n+    while (use) {\n+      if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n+        break;\n+      auto tensorType =\n+          use->getResult(0).getType().dyn_cast<RankedTensorType>();\n+      if (!tensorType || !tensorType.getEncoding().isa<SharedEncodingAttr>())\n+        break;\n+      use = *use->getResult(0).getUsers().begin();\n+    }\n+\n+    auto convertLayout = llvm::dyn_cast<ConvertLayoutOp>(use);\n+    if (!convertLayout)\n+      return;\n+    auto tensorType =\n+        convertLayout.getResult().getType().cast<RankedTensorType>();\n+    if (!tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n+      return;\n+    convertsToDotEncoding.push_back(convertLayout);\n+\n+    // Update the smallest type.\n+    auto ty = loadOp.getType().cast<RankedTensorType>();\n+    Type eltTy = ty.getElementType();\n+    if (!smallestType ||\n+        (eltTy.getIntOrFloatBitWidth() < smallestType.getIntOrFloatBitWidth()))\n+      smallestType = eltTy;\n+  });\n+  if (!smallestType)\n+    return;\n+  updateDotEncodingLayout(convertsToDotEncoding, smallestType);\n+}\n+\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n@@ -216,6 +351,10 @@ class TritonGPUOptimizeDotOperandsPass\n       signalPassFailure();\n     if (fixupLoops(m).failed())\n       signalPassFailure();\n+\n+    // Change the layout of dotOperand layout to use the kWidth from the\n+    // smallest loaded type.\n+    m->walk([](triton::FuncOp func) { optimizeKWidth(func); });\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 36, "deletions": 37, "changes": 73, "file_content_changes": "@@ -111,9 +111,6 @@ class LoopPipeliner {\n \n   /// Loads to be pipelined\n   SetVector<Value> validLoads;\n-  /// Smallest data-type for each load (used to optimize swizzle and\n-  /// (create DotOpEncoding layout)\n-  DenseMap<Value, Type> loadsSmallestType;\n   /// The value that each load will be mapped to (after layout conversion)\n   DenseMap<Value, Value> loadsMapping;\n   /// load => buffer\n@@ -485,21 +482,6 @@ Value LoopPipeliner::lookupOrDefault(Value origin, int stage) {\n }\n \n void LoopPipeliner::createBufferTypes() {\n-  // We need to find the smallest common dtype since this determines the layout\n-  // of `mma.sync` operands in mixed-precision mode\n-  Type smallestType;\n-  for (auto loadCvt : loadsMapping) {\n-    auto loadOp = loadCvt.first;\n-    auto ty = loadOp.getType().cast<RankedTensorType>();\n-    Type eltTy = ty.getElementType();\n-    if (!smallestType ||\n-        (eltTy.getIntOrFloatBitWidth() < smallestType.getIntOrFloatBitWidth()))\n-      smallestType = eltTy;\n-  }\n-\n-  for (auto loadCvt : loadsMapping)\n-    loadsSmallestType[loadCvt.first] = smallestType;\n-\n   for (auto loadCvt : loadsMapping) {\n     auto loadOp = loadCvt.first;\n     Value cvt = loadCvt.second;\n@@ -511,9 +493,12 @@ void LoopPipeliner::createBufferTypes() {\n     SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n                                      ty.getShape().end());\n     bufferShape.insert(bufferShape.begin(), numStages);\n-    auto sharedEnc = ttg::SharedEncodingAttr::get(\n-        ty.getContext(), dotOpEnc, ty.getShape(),\n-        ttg::getOrder(ty.getEncoding()), loadsSmallestType[loadOp]);\n+    unsigned bitWidth = dotOpEnc.getMMAv2kWidth()\n+                            ? 32 / dotOpEnc.getMMAv2kWidth()\n+                            : ty.getElementType().getIntOrFloatBitWidth();\n+    auto sharedEnc =\n+        ttg::SharedEncodingAttr::get(ty.getContext(), dotOpEnc, ty.getShape(),\n+                                     ttg::getOrder(ty.getEncoding()), bitWidth);\n     loadsBufferType[loadOp] =\n         RankedTensorType::get(bufferShape, ty.getElementType(), sharedEnc);\n   }\n@@ -789,19 +774,12 @@ scf::ForOp LoopPipeliner::cloneForOp(ArrayRef<Value> newLoopArgs,\n     // we replace the use new load use with a convert layout\n     size_t i = std::distance(validLoads.begin(), it);\n     auto cvtDstTy = op.getResult(0).getType().cast<RankedTensorType>();\n-    auto cvtDstEnc =\n-        cvtDstTy.getEncoding().dyn_cast<ttg::DotOperandEncodingAttr>();\n-    if (!cvtDstEnc) {\n+    if (!cvtDstTy.getEncoding().isa<ttg::DotOperandEncodingAttr>()) {\n       builder.clone(op, mapping);\n       continue;\n     }\n-    auto newDstTy = RankedTensorType::get(\n-        cvtDstTy.getShape(), cvtDstTy.getElementType(),\n-        ttg::DotOperandEncodingAttr::get(\n-            cvtDstEnc.getContext(), cvtDstEnc.getOpIdx(), cvtDstEnc.getParent(),\n-            loadsSmallestType[op.getOperand(0)]));\n     auto cvt = builder.create<ttg::ConvertLayoutOp>(\n-        op.getResult(0).getLoc(), newDstTy,\n+        op.getResult(0).getLoc(), cvtDstTy,\n         newForOp.getRegionIterArgs()[loadIdx + i]);\n     mapping.map(op.getResult(0), cvt.getResult());\n   }\n@@ -838,28 +816,49 @@ void LoopPipeliner::prefetchNextIteration(scf::ForOp newForOp,\n       builder.create<arith::ConstantIntOp>(nextIV.getLoc(), numStages, 32));\n \n   // Prefetch load deps\n+  // If a load-dependent instruction that uses a block argument, we\n+  // shouldn't update the new mapping of the block argument in the current\n+  // iteration.\n+  // For example.\n+  // %a = add %arg0, %c\n+  // %b = add %arg0, %d\n+  //\n+  // Update %arg0 will cause the value of %b to be incorrect.\n+  // We do need to use the next iteration value of %arg0 because it could be a\n+  // immediate arg of a load op.\n+  // load %arg0\n+  // %a = add %arg0, %c\n+  // yield %a\n+  //\n+  // We reroder instructions so %a and yield are actually before load. load\n+  // %arg0 should use the updated %arg0.\n+  IRMapping curMapping = nextMapping;\n   for (Operation *op : orderedDeps)\n     if (!validLoads.contains(op->getResult(0))) {\n       if (immediateOpStages[op].contains(numStages - 2))\n         // A post load op that provides values for numStage - 2\n-        nextMapping.map(forOp.getInductionVar(), curIV);\n+        curMapping.map(forOp.getInductionVar(), curIV);\n       else\n-        nextMapping.map(forOp.getInductionVar(), nextIV);\n+        curMapping.map(forOp.getInductionVar(), nextIV);\n       Operation *nextOp;\n       if (auto loadOp = dyn_cast<triton::LoadOp>(op)) {\n         auto newMask =\n-            getLoadMask(loadOp, nextMapping.lookupOrDefault(loadOp.getMask()),\n+            getLoadMask(loadOp, curMapping.lookupOrDefault(loadOp.getMask()),\n                         nextLoopCond, builder);\n         nextOp = builder.create<triton::LoadOp>(\n             loadOp.getLoc(), loadOp.getResult().getType(),\n-            nextMapping.lookupOrDefault(loadOp.getPtr()), newMask,\n-            nextMapping.lookupOrDefault(loadOp.getOther()),\n+            curMapping.lookupOrDefault(loadOp.getPtr()), newMask,\n+            curMapping.lookupOrDefault(loadOp.getOther()),\n             loadOp.getBoundaryCheckAttr(), loadOp.getPaddingAttr(),\n             loadOp.getCache(), loadOp.getEvict(), loadOp.getIsVolatile());\n         addNamedAttrs(nextOp, op->getDiscardableAttrDictionary());\n+        curMapping.map(loadOp.getResult(), nextOp->getResult(0));\n         nextMapping.map(loadOp.getResult(), nextOp->getResult(0));\n-      } else\n-        nextOp = builder.clone(*op, nextMapping);\n+      } else {\n+        nextOp = builder.clone(*op, curMapping);\n+        for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n+          nextMapping.map(op->getResult(dstIdx), nextOp->getResult(dstIdx));\n+      }\n \n       for (unsigned dstIdx : llvm::seq(unsigned(0), op->getNumResults()))\n         setValueMappingYield(newForOp, op->getResult(dstIdx),"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 18, "deletions": 15, "changes": 33, "file_content_changes": "@@ -345,6 +345,14 @@ class RematerializeForward : public mlir::RewritePattern {\n     // heuristics for flash attention\n     if (srcEncoding.isa<triton::gpu::SliceEncodingAttr>())\n       return failure();\n+    // For cases like:\n+    // %0 = convert_layout %arg0\n+    // We should try to move %0 out of scf.for first, if it couldn't be moved\n+    // out additional conversions will be added to the loop body.\n+    if (!cvt.getOperand().getDefiningOp() &&\n+        isa<scf::ForOp>(cvt->getParentOp()))\n+      return failure();\n+\n     SetVector<Operation *> cvtSlices;\n     auto filter = [&](Operation *op) {\n       return op->getBlock() == cvt->getBlock() &&\n@@ -363,7 +371,8 @@ class RematerializeForward : public mlir::RewritePattern {\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n           !op->hasTrait<mlir::OpTrait::Elementwise>() &&\n-          !isa<triton::StoreOp, triton::ReduceOp>(op))\n+          !isa<triton::StoreOp, triton::AssertOp, triton::PrintOp,\n+               triton::ReduceOp>(op))\n         return failure();\n       // don't rematerialize if it adds an extra conversion that can't\n       // be removed\n@@ -380,6 +389,10 @@ class RematerializeForward : public mlir::RewritePattern {\n       }\n     }\n \n+    // Call SimplifyReduceCvt instead of the general push conversion forward\n+    if (isa<triton::ReduceOp>(cvtSlices.front()))\n+      return failure();\n+\n     pushConversionForward(cvt, cvtSlices, rewriter);\n     return success();\n   }\n@@ -425,8 +438,8 @@ class RematerializeBackward : public mlir::RewritePattern {\n \n     IRMapping mapping;\n     rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n-\n     rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n+\n     return mlir::success();\n   }\n };\n@@ -459,31 +472,21 @@ class MoveConvertOutOfLoop : public mlir::RewritePattern {\n     for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n       mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n     mapping.map(origConversion.getResult(), newForOp.getRegionIterArgs()[i]);\n-    // the iter arg of interest may have other uses than the conversion\n-    // we're hoisting out of the loop. If that's the case we will\n-    // need to add extra conversions for all uses... which is only useful\n-    // if these extra conversions can be removed by another pattern\n-    auto oldArg = forOp.getRegionIterArgs()[i];\n-    auto newArg = newForOp.getRegionIterArgs()[i];\n-    auto newArgFallback = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        newForOp.getLoc(), origType, newArg);\n \n     mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n     for (Operation &op : forOp.getBody()->without_terminator()) {\n       if (&op == (Operation *)(&origConversion))\n         continue;\n       Operation *newOp = rewriter.clone(op, mapping);\n-      if (find(oldArg.getUsers(), &op) != oldArg.getUsers().end())\n-        newOp->replaceUsesOfWith(newArg, newArgFallback);\n     }\n-\n     // create yield, inserting conversions if necessary\n     auto yieldOp = forOp.getBody()->getTerminator();\n     SmallVector<Value, 4> newYieldArgs;\n     for (Value arg : yieldOp->getOperands())\n       newYieldArgs.push_back(mapping.lookup(arg));\n-    newYieldArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n-        yieldOp->getLoc(), newType, newYieldArgs[i]);\n+    if (newYieldArgs[i].getType() != newType)\n+      newYieldArgs[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+          yieldOp->getLoc(), newType, newYieldArgs[i]);\n     rewriter.create<scf::YieldOp>(forOp.getLoc(), newYieldArgs);\n \n     // replace"}, {"filename": "lib/Dialect/TritonGPU/Transforms/ReorderInstructions.cpp", "status": "modified", "additions": 13, "deletions": 6, "changes": 19, "file_content_changes": "@@ -1,6 +1,7 @@\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n+#include \"mlir/IR/Dominance.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n@@ -42,6 +43,7 @@ class TritonGPUReorderInstructionsPass\n \n   void runOnOperation() override {\n     ModuleOp m = getOperation();\n+    mlir::DominanceInfo dom(m);\n     // Sink conversions into loops when they will increase\n     // register pressure\n     DenseMap<Operation *, Operation *> opToMove;\n@@ -78,26 +80,31 @@ class TritonGPUReorderInstructionsPass\n         return;\n       op->moveAfter(argOp);\n     });\n-    // Move `dot` operand so that conversions to opIdx=0 happens before\n-    // conversions to opIdx=1\n+    // Move `dot` operand so that conversions to opIdx=1 happens after\n+    // conversions to opIdx=0\n     m.walk([&](triton::gpu::ConvertLayoutOp op) {\n       auto dstType = op.getResult().getType().cast<RankedTensorType>();\n       auto dstEncoding =\n           dstType.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n       if (!dstEncoding)\n         return;\n       int opIdx = dstEncoding.getOpIdx();\n-      if (opIdx != 0)\n+      if (opIdx != 1)\n         return;\n       if (op->getUsers().empty())\n         return;\n       auto dotUser = dyn_cast<triton::DotOp>(*op->user_begin());\n       if (!dotUser)\n         return;\n-      auto BOp = dotUser.getOperand(1).getDefiningOp();\n-      if (!BOp)\n+      auto AOp =\n+          dotUser.getOperand(0).getDefiningOp<triton::gpu::ConvertLayoutOp>();\n+      if (!AOp)\n         return;\n-      op->moveBefore(BOp);\n+      // Check that the conversion to OpIdx=1 happens before and can be moved\n+      // after the conversion to OpIdx=0.\n+      if (!dom.dominates(op.getOperation(), AOp.getOperation()))\n+        return;\n+      op->moveAfter(AOp);\n     });\n     return;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 23, "deletions": 17, "changes": 40, "file_content_changes": "@@ -104,26 +104,13 @@ bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   return true;\n }\n \n-bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n-  // If the new elements per thread is less than the old one, we will need to do\n-  // convert encoding that goes through shared memory anyway. So we consider it\n-  // as expensive.\n-  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n-  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n-  auto shape = tensorTy.getShape();\n-  auto elemTy = tensorTy.getElementType();\n-  auto newTotalElemsPerThread =\n-      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n-  return newTotalElemsPerThread < totalElemsPerThread;\n-}\n-\n bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n     return isExpensiveLoadOrStore(op, targetEncoding);\n   if (isa<triton::CatOp>(op))\n-    return isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return triton::gpu::isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n@@ -136,7 +123,8 @@ bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n \n bool canFoldConversion(Operation *op, Attribute &targetEncoding) {\n   if (isa<triton::CatOp>(op))\n-    return !isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return !triton::gpu::isExpensiveCat(cast<triton::CatOp>(op),\n+                                        targetEncoding);\n   return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n              triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }\n@@ -297,7 +285,7 @@ LogicalResult canMoveOutOfLoop(BlockArgument arg,\n   // 2. There is only a single conversion\n   // 3. Moving this conversion out of the loop will not generate any extra\n   // non-removable conversion\n-  DenseSet<Type> cvtTypes;\n+  SetVector<RankedTensorType> cvtTypes;\n   SetVector<Operation *> others;\n   auto oldType = arg.getType().cast<RankedTensorType>();\n   for (auto user : arg.getUsers()) {\n@@ -326,16 +314,34 @@ LogicalResult canMoveOutOfLoop(BlockArgument arg,\n     // Second condition\n     if (others.empty())\n       return success();\n-    // Third condition: not complete\n+    // Third condition - part 1:\n     // If the other or the cvt is in the different block, we cannot push the\n     // conversion forward or backward\n     for (auto *cvt : cvts) {\n       if (cvt->getBlock() != forOp.getBody())\n         return failure();\n     }\n+    auto targetEncoding = cvtTypes.front().getEncoding();\n     for (auto *other : others) {\n+      // Third condition - part 2:\n+      // If the other non-cvt op is in the different block, we cannot push the\n+      // conversion forward or backward\n       if (other->getBlock() != forOp.getBody())\n         return failure();\n+      // Third condition - part 3:\n+      // %0 (enc1) = cvt %arg (enc0)\n+      // other %0 (enc1), %1 (enc0) => other %0 (enc1), %1 (enc1)\n+      // Check if %2 (enc1) = cvt %1 (enc0) can be eliminated\n+      SetVector<Operation *> processed;\n+      SetVector<Attribute> layout;\n+      llvm::MapVector<Value, Attribute> toConvert;\n+      for (auto operand : other->getOperands()) {\n+        auto argOp = operand.getDefiningOp();\n+        if (argOp && !isa<triton::gpu::ConvertLayoutOp>(argOp) &&\n+            simulateBackwardRematerialization(argOp, processed, layout,\n+                                              toConvert, targetEncoding) > 0)\n+          return failure();\n+      }\n     }\n     return success();\n   }"}, {"filename": "lib/Target/LLVMIR/CMakeLists.txt", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -4,7 +4,9 @@ add_mlir_translation_library(TritonLLVMIR\n         LINK_COMPONENTS\n         Core\n \n-        LINK_LIBS PUBLIC\n+        LINK_LIBS\n+        ${CMAKE_DL_LIBS}\n+        PUBLIC\n         MLIRArithToLLVM\n         MLIRBuiltinToLLVMIRTranslation\n         MLIRExecutionEngineUtils"}, {"filename": "python/setup.py", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "file_content_changes": "@@ -13,6 +13,7 @@\n \n from setuptools import Extension, setup\n from setuptools.command.build_ext import build_ext\n+from setuptools.command.build_py import build_py\n \n \n # Taken from https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/env.py\n@@ -146,6 +147,11 @@ def download_and_copy_ptxas():\n \n # ---- cmake extension ----\n \n+class CMakeBuildPy(build_py):\n+    def run(self) -> None:\n+        self.run_command('build_ext')\n+        return super().run()\n+\n \n class CMakeExtension(Extension):\n     def __init__(self, name, path, sourcedir=\"\"):\n@@ -205,6 +211,7 @@ def build_extension(self, ext):\n         # python directories\n         python_include_dir = sysconfig.get_path(\"platinclude\")\n         cmake_args = [\n+            \"-DCMAKE_EXPORT_COMPILE_COMMANDS=ON\",\n             \"-DLLVM_ENABLE_WERROR=ON\",\n             \"-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\" + extdir,\n             \"-DTRITON_BUILD_TUTORIALS=OFF\",\n@@ -280,7 +287,7 @@ def build_extension(self, ext):\n     ],\n     include_package_data=True,\n     ext_modules=[CMakeExtension(\"triton\", \"triton/_C/\")],\n-    cmdclass={\"build_ext\": CMakeBuild},\n+    cmdclass={\"build_ext\": CMakeBuild, \"build_py\": CMakeBuildPy},\n     zip_safe=False,\n     # for PyPI\n     keywords=[\"Compiler\", \"Deep Learning\"],"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "file_content_changes": "@@ -89,6 +89,9 @@ void init_triton_ir(py::module &&m) {\n       .value(\"NONE\", mlir::triton::CacheModifier::NONE)\n       .value(\"CA\", mlir::triton::CacheModifier::CA)\n       .value(\"CG\", mlir::triton::CacheModifier::CG)\n+      .value(\"WB\", mlir::triton::CacheModifier::WB)\n+      .value(\"CS\", mlir::triton::CacheModifier::CS)\n+      .value(\"WT\", mlir::triton::CacheModifier::WT)\n       .export_values();\n \n   py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\")\n@@ -1403,6 +1406,21 @@ void init_triton_ir(py::module &&m) {\n              return self.create<mlir::triton::ReduceReturnOp>(loc,\n                                                               return_values);\n            })\n+      .def(\"create_scan\",\n+           [](mlir::OpBuilder &self, std::vector<mlir::Value> operands,\n+              int axis) -> mlir::OpState {\n+             auto loc = self.getUnknownLoc();\n+             return self.create<mlir::triton::ScanOp>(loc, operands, axis);\n+           })\n+      .def(\"create_scan_ret\",\n+           [](mlir::OpBuilder &self, py::args args) -> mlir::OpState {\n+             auto loc = self.getUnknownLoc();\n+             llvm::SmallVector<mlir::Value> return_values;\n+             for (const auto &arg : args) {\n+               return_values.push_back(py::cast<mlir::Value>(arg));\n+             }\n+             return self.create<mlir::triton::ScanReturnOp>(loc, return_values);\n+           })\n       .def(\"create_ptr_to_int\",\n            [](mlir::OpBuilder &self, mlir::Value &val,\n               mlir::Type &type) -> mlir::Value {"}, {"filename": "python/test/kernel_comparison/kernels.yml", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "file_content_changes": "@@ -0,0 +1,31 @@\n+name_and_extension:\n+  - name: _kernel_0d1d2d34567c89c1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6d7c8d9c10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6d7c8c9d10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3456c789c1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6d7c8c9d1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d34567c8c91011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3456c78c91011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6c78c9d1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6c789c1011c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6c7d8d9c10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d3d4d5d6c7d8c9d10d11c\n+    extension: ptx\n+  - name: _kernel_0d1d2d345d6d7c89c1011c\n+    extension: ptx\n+  - name: _bwd_kernel_0d1d2d34d5d6d7d8d9d10d11d12d13d14d15c16d17d18d19c20d21d22d23c2425d26d27\n+    extension: ptx\n+  - name: _fwd_kernel_0d1d2d34d5d6d7d8d9d10c11d12d13d14c15d16d17d18c19d20d21d22c2324d25d\n+    extension: ptx\n+  - name: _bwd_preprocess_0d1d2d3d4d\n+    extension: ptx"}, {"filename": "python/test/tools/compare_files.py", "status": "added", "additions": 261, "deletions": 0, "changes": 261, "file_content_changes": "@@ -0,0 +1,261 @@\n+import argparse\n+import difflib\n+import glob\n+import os\n+import sys\n+from typing import Dict, List, Optional, Tuple\n+\n+import yaml\n+\n+\n+class ComparisonResult:\n+    def __init__(self, name: str, extension: str, numComparisons: int, diffs: List[str] = None, errors: List[str] = None):\n+        self.name = name\n+        self.extension = extension\n+        self.numComparisons = numComparisons\n+        self.diffs = [] if diffs is None else diffs\n+        self.errors = [] if errors is None else errors\n+\n+    def isSuccess(self) -> bool:\n+        return len(self.diffs) == 0 and len(self.errors) == 0\n+\n+    def __str__(self) -> str:\n+        return f\"name={self.name}, extension={self.extension}, numComparisons={self.numComparisons}, success={self.isSuccess()}\"\n+\n+\n+def listFilesWithExtension(path: str, extension: str) -> List[str]:\n+    \"\"\"\n+        Returns a list of files in the given path with the given extension\n+        The files are returned with their full path\n+    \"\"\"\n+    files = glob.glob(os.path.join(path, f'*.{extension}'))\n+    return files\n+\n+\n+def getFileWithExtension(path: str, ext: str) -> Optional[str]:\n+    \"\"\"\n+        Returns a single file in the given path with the given extension\n+    \"\"\"\n+    # get all files in directory with extension\n+    files = listFilesWithExtension(path, ext)\n+    if len(files) == 0:\n+        return None\n+    # filter out files with grp in their name\n+    files = [f for f in files if \"__grp__\" not in f]\n+    if len(files) != 1:\n+        print(f\"Found {len(files)} files in {path} with extension {ext}!\")\n+        sys.exit(2)\n+    return files[0]\n+\n+\n+def loadYamlFile(filePath: str) -> List[Dict[str, str]]:\n+    \"\"\"\n+        Loads a yaml file and returns its content as a list of dictionaries\n+    \"\"\"\n+    with open(filePath, 'r') as file:\n+        content = yaml.safe_load(file)\n+    return content\n+\n+\n+def compareFiles(file1: str, file2: str) -> bool:\n+    \"\"\"\n+        Compares two files and returns True if they are the same, False otherwise\n+    \"\"\"\n+    with open(file1, 'rb') as f1, open(file2, 'rb') as f2:\n+        content1 = f1.read()\n+        content2 = f2.read()\n+\n+    return content1 == content2\n+\n+\n+def diffFiles(file1, file2):\n+    with open(file1, 'r') as f1:\n+        file1_lines = f1.readlines()\n+    with open(file2, 'r') as f2:\n+        file2_lines = f2.readlines()\n+\n+    diff = list(difflib.unified_diff(file1_lines, file2_lines, file1, file2))\n+    return diff\n+\n+\n+def getFileVec(path: str) -> List[Tuple[str, str]]:\n+    \"\"\"\n+        Returns a list of tuples (extension, file) for the given path (note: the path includes the hash)\n+        The returned list must have extensions (json, ttir, ttgir)\n+        in this particular order, unless a file with a certain extension does not exist\n+    \"\"\"\n+    vec = []\n+    for ext in [\"json\", \"ttir\", \"ttgir\"]:\n+        file = getFileWithExtension(path, ext)\n+        if file is not None:\n+            vec.append((ext, file))\n+    return vec\n+\n+\n+def getNameToHashesDict(path: str) -> Dict[str, List[str]]:\n+    \"\"\"\n+        Returns a dictionary that maps kernel names to a list of hashes that have the same kernel name\n+        in the given path\n+        Note: the hashes must have a json file and either a ttir or ttgir file, otherwise they are ignored\n+    \"\"\"\n+    nameToHashes = {}\n+    for hash in os.listdir(path):\n+        fullPath = os.path.join(path, hash)\n+        if not os.path.isdir(fullPath):\n+            print(f\"Path {fullPath} is not a directory!\")\n+            sys.exit(2)\n+        fileVec = getFileVec(fullPath)\n+        if len(fileVec) < 2 or fileVec[0][0] != \"json\":\n+            continue\n+        jsonFile = fileVec[0][1]\n+        # load json file\n+        with open(jsonFile, 'r') as file:\n+            content = yaml.safe_load(file)\n+            # get name\n+            name = content[\"name\"]\n+            nameToHashes.setdefault(name, []).append(hash)\n+    return nameToHashes\n+\n+\n+def doFilesMatch(path1: str, path2: str) -> bool:\n+    \"\"\"\n+        Returns True if the files in the given paths match, False otherwise\n+        The files are considered to match if:\n+        1. The number of files in both paths match\n+        2. The json files match\n+        3. Both paths have a ttir that match, if a ttir does not exist, the ttgir file must exist and match\n+    \"\"\"\n+    filesVec1 = getFileVec(path1)\n+    filesVec2 = getFileVec(path2)\n+    # The number of files must match\n+    if len(filesVec1) != len(filesVec2):\n+        return False\n+\n+    for (ext1, file1), (ext2, file2) in zip(filesVec1, filesVec2):\n+        if ext1 != ext2:\n+            return False\n+        if not compareFiles(file1, file2):\n+            return False\n+        else:\n+            # once we actually compared a ttir or ttgir file, we can break\n+            if ext1 in (\"ttir\", \"ttgir\"):\n+                break\n+    return True\n+\n+\n+def compareMatchingFiles(name: str, extension: str, nameToHashes1: Dict[str, List[str]], nameToHashes2: Dict[str, List[str]], args) -> ComparisonResult:\n+    \"\"\"\n+        Compare files with the given name/extension in all hashes in both paths\n+        Return the first mismatching files as a tuple (file1, file2), otherwise, return an empty tuple\n+    \"\"\"\n+    hashes1 = nameToHashes1.get(name, [])\n+    hashes2 = nameToHashes2.get(name, [])\n+    diffs = []\n+    errors = []\n+    numComparisons = 0\n+    for hash1 in hashes1:\n+        path1 = os.path.join(args.path1, hash1)\n+        for hash2 in hashes2:\n+            path2 = os.path.join(args.path2, hash2)\n+            # check whether both paths have:\n+            # 1. json files that match\n+            # 2. ttir files that match (if they exist), otherwise ttgir files that match (if they exist)\n+            # if any of these contraints is not met, then we can skip this pair of hashes since they are not a match\n+            if not doFilesMatch(path1, path2):\n+                continue\n+            numComparisons += 1\n+            extFile1 = listFilesWithExtension(path1, extension)[0]\n+            extFile2 = listFilesWithExtension(path2, extension)[0]\n+            diff = diffFiles(extFile1, extFile2)\n+            if len(diff) > 0:\n+                diffs.append(diffFiles(extFile2, extFile1))\n+    if numComparisons == 0:\n+        errors.append(f\"Did not find any matching files for {name}\")\n+    return ComparisonResult(name=name, extension=extension, numComparisons=numComparisons, diffs=diffs, errors=errors)\n+\n+\n+def dumpResults(results: List[ComparisonResult], fileName: str):\n+    \"\"\"\n+        Dumps the results to the given file\n+    \"\"\"\n+    with open(fileName, 'w') as file:\n+        for result in results:\n+            file.write(str(result) + \"\\n\")\n+            file.write(\"Diffs:\\n\")\n+            for diff in result.diffs:\n+                for line in diff:\n+                    file.write(line)\n+            file.write(\"Errors:\\n\")\n+            for error in result.errors:\n+                file.write(error)\n+            file.write(\"\\n\\n\")\n+\n+\n+def main(args) -> bool:\n+    \"\"\"\n+        Iterates over all kernels in the given yaml file and compares them\n+        in the given paths\n+    \"\"\"\n+    if args.path1 == args.path2:\n+        print(\"Cannot compare files in the same directory!\")\n+        sys.exit(2)\n+    # Get kernel name to hashes dict, these hashes would have the same kernel name\n+    nameToHashes1 = getNameToHashesDict(args.path1)\n+    nameToHashes2 = getNameToHashesDict(args.path2)\n+\n+    yamlFilePath = args.kernels\n+    if not os.path.exists(yamlFilePath):\n+        print(f\"Path {yamlFilePath} does not exist!\")\n+        sys.exit(2)\n+    nameAndExtension = loadYamlFile(yamlFilePath)[\"name_and_extension\"]\n+\n+    results = []\n+    # iterate over the kernels that need to be checked\n+    for d in nameAndExtension:\n+        name = d[\"name\"]  # kernel name\n+        extension = d[\"extension\"]  # extension of the file to be compared (e.g. ptx)\n+        # Compare all hashes on path 1 with all hashes on path 2\n+        # result is either the mismatching (file1, file2) with \"extension\" or empty tuple if no mismatch\n+        result = compareMatchingFiles(name, extension, nameToHashes1, nameToHashes2, args)\n+        print(result)\n+        # Otherwise, add it to the mismatches\n+        results.append(result)\n+\n+    # Dump results\n+    dumpResults(results, \"kernels_reference_check.txt\")\n+\n+    success = all(result.isSuccess() for result in results)\n+\n+    if not success:\n+        print(\"Failed!\")\n+        sys.exit(1)\n+\n+    print(\"Passed!\")\n+    sys.exit(0)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--path1\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=(\"Path to first cache directory\"),\n+    )\n+    parser.add_argument(\n+        \"--path2\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=(\"Path to second cache directory\"),\n+    )\n+    parser.add_argument(\n+        \"--kernels\",\n+        type=str,\n+        default=None,\n+        required=True,\n+        help=(\"Path to kernels yaml file\"),\n+    )\n+    args = parser.parse_args()\n+    main(args)"}, {"filename": "python/test/unit/language/conftest.py", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+# content of conftest.py\n+\n+import pytest\n+\n+\n+def pytest_addoption(parser):\n+    parser.addoption(\n+        \"--device\", action=\"store\", default='cuda'\n+    )\n+\n+\n+@pytest.fixture\n+def device(request):\n+    return request.config.getoption(\"--device\")"}, {"filename": "python/test/unit/language/test_annotations.py", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -7,13 +7,13 @@\n import triton.language as tl\n \n \n-def test_annotations():\n+def test_annotations(device):\n \n     @triton.jit\n     def _kernel(X: torch.Tensor, N: int, BLOCK_SIZE: tl.constexpr):\n         pass\n \n-    x = torch.empty(1, device='cuda')\n+    x = torch.empty(1, device=device)\n     _kernel[(1,)](x, x.shape[0], 32)\n     try:\n         _kernel[(1,)](x.shape[0], x.shape[0], 32)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 458, "deletions": 226, "changes": 684, "file_content_changes": "@@ -101,13 +101,19 @@ def patch_kernel(template, to_replace):\n     return kernel\n \n \n-def check_type_supported(dtype):\n+def check_cuda_only(device):\n+    if device not in ['cuda']:\n+        pytest.skip(\"Only for cuda\")\n+\n+\n+def check_type_supported(dtype, device):\n     '''\n     skip test if dtype is not supported on the current device\n     '''\n-    cc = torch.cuda.get_device_capability()\n-    if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n-        pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+    if device in ['cuda']:\n+        cc = torch.cuda.get_device_capability()\n+        if cc[0] < 8 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n+            pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n \n \n class MmaLayout:\n@@ -142,20 +148,20 @@ def __str__(self):\n \n \n @pytest.mark.parametrize(\"dtype_x\", list(dtypes) + [\"bfloat16\"])\n-def test_empty_kernel(dtype_x, device='cuda'):\n+def test_empty_kernel(dtype_x, device):\n     SIZE = 128\n \n     @triton.jit\n     def kernel(X, SIZE: tl.constexpr):\n         pass\n-    check_type_supported(dtype_x)\n+    check_type_supported(dtype_x, device)\n     x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device, dst_type=dtype_x)\n     kernel[(1, )](x, SIZE=SIZE, num_warps=4)\n \n \n # generic test functions\n def _test_unary(dtype_x, expr, numpy_expr=None, device='cuda'):\n-    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_x, device)  # early return if dtype_x is not supported\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -213,8 +219,8 @@ def _binary_op_dtype_override(a: str, b: str) -> Optional[np.dtype]:\n \n \n def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real', mode_y='real', device='cuda', y_low=None, y_high=None):\n-    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n-    check_type_supported(dtype_y)\n+    check_type_supported(dtype_x, device)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_y, device)\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -284,7 +290,7 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n     for dtype_x in dtypes_with_bfloat16\n     for dtype_y in dtypes_with_bfloat16\n ])\n-def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_bin_op(dtype_x, dtype_y, op, device):\n     expr = f' x {op} y'\n     if op == '%' and dtype_x in int_dtypes + uint_dtypes and dtype_y in int_dtypes + uint_dtypes:\n         # LLVM has 'numpy.fmod', not 'numpy.remainder', semantics on integer remainders.\n@@ -317,7 +323,7 @@ def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n                          [(dtype_x, dtype_y) for dtype_x in int_dtypes for dtype_y in int_dtypes] +\n                          [(dtype_x, dtype_y) for dtype_x in uint_dtypes for dtype_y in uint_dtypes]\n                          )\n-def test_floordiv(dtype_x, dtype_y, device='cuda'):\n+def test_floordiv(dtype_x, dtype_y, device):\n     # Triton has IEEE, not numpy/torch, semantics for %, and those carry\n     # through to //, so we have to use a nonstandard expression to get a\n     # reference result for //.\n@@ -326,7 +332,7 @@ def test_floordiv(dtype_x, dtype_y, device='cuda'):\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, device=device)\n \n \n-def test_unsigned_name_mangling(device='cuda'):\n+def test_unsigned_name_mangling(device):\n     # Test that uint32 and int32 are mangled differently by the compiler\n     SIZE = 128\n     # define the kernel / launch-grid\n@@ -372,7 +378,7 @@ def kernel(O1, O2, X, Y, SIZE: tl.constexpr):\n     for dtype_x in dtypes + dtypes_with_bfloat16\n     for dtype_y in dtypes + dtypes_with_bfloat16\n ])\n-def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_bitwise_op(dtype_x, dtype_y, op, device):\n     expr = f'x {op} y'\n     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n@@ -395,7 +401,7 @@ def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n     for dtype_x in int_dtypes + uint_dtypes\n     for dtype_y in int_dtypes + uint_dtypes\n ])\n-def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n+def test_shift_op(dtype_x, dtype_y, op, device):\n     expr = f'x {op} y'\n     bw = max(_bitwidth(dtype_x), _bitwidth(dtype_y))\n     if dtype_x.startswith('int'):\n@@ -428,7 +434,7 @@ def test_shift_op(dtype_x, dtype_y, op, device='cuda'):\n                                                     ('nan', 'nan')]\n \n                           ])\n-def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n+def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device):\n     expr = f'x {op} y'\n     if (dtype_x in uint_dtypes and dtype_y in int_dtypes and _bitwidth(dtype_x) >= _bitwidth(dtype_y)):\n         numpy_expr = f'x.astype(np.{dtype_x}) {op} y.astype(np.{dtype_x})'\n@@ -443,7 +449,7 @@ def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n # test broadcast\n # ---------------\n @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16)\n-def test_broadcast(dtype):\n+def test_broadcast(dtype, device):\n     @triton.jit\n     def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.constexpr):\n         offset1 = tl.arange(0, M)\n@@ -460,9 +466,9 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n     y = numpy_random(N, dtype_str=dtype, rs=rs)\n     _, y_broadcasted_np = np.broadcast_arrays(x, y)\n \n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n-    y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device='cuda', dst_type=dtype)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    y_tri = to_triton(y, device=device, dst_type=dtype)\n+    y_broadcasted_tri = to_triton(np.empty((M, N), dtype=y_broadcasted_np.dtype), device=device, dst_type=dtype)\n \n     broadcast_kernel[(1,)](x_tri, y_tri, y_broadcasted_tri, M=M, N=N)\n     assert (y_broadcasted_np == to_numpy(y_broadcasted_tri)).all()\n@@ -472,8 +478,8 @@ def broadcast_kernel(x_ptr, y_ptr, y_broadcasted_ptr, M: tl.constexpr, N: tl.con\n # ------------------\n \n \n-def test_invalid_slice():\n-    dst = torch.empty(128, device='cuda')\n+def test_invalid_slice(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -486,7 +492,7 @@ def _kernel(dst):\n # ----------------\n # test expand_dims\n # ----------------\n-def test_expand_dims():\n+def test_expand_dims(device):\n     @triton.jit\n     def expand_dims_kernel(dummy, N: tl.constexpr):\n         offset1 = tl.arange(0, N)\n@@ -516,11 +522,11 @@ def expand_dims_kernel(dummy, N: tl.constexpr):\n         tl.static_assert(t.shape == [N, 1, 1, 1])\n \n     N = 32\n-    dummy_tensor = torch.empty((), device=\"cuda\")\n+    dummy_tensor = torch.empty((), device=device)\n     expand_dims_kernel[(1,)](dummy_tensor, N)\n \n \n-def test_expand_dims_error_cases():\n+def test_expand_dims_error_cases(device):\n     @triton.jit\n     def dim_out_of_range1(dummy, N: tl.constexpr):\n         offset1 = tl.arange(0, N)\n@@ -548,7 +554,7 @@ def duplicate_dim2(dummy, N: tl.constexpr):\n         t = tl.expand_dims(offset1, (0, -3))\n \n     N = 32\n-    dummy_tensor = torch.empty((), device=\"cuda\")\n+    dummy_tensor = torch.empty((), device=device)\n \n     with pytest.raises(triton.CompilationError, match=\"invalid axis -3\"):\n         dim_out_of_range1[(1,)](dummy_tensor, N)\n@@ -566,8 +572,8 @@ def duplicate_dim2(dummy, N: tl.constexpr):\n # ----------------------------\n # test invalid program id axis\n # ----------------------------\n-def test_invalid_pid_axis():\n-    dst = torch.empty(128, device='cuda')\n+def test_invalid_pid_axis(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -581,12 +587,12 @@ def _kernel(dst):\n # test where\n # ---------------\n @pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16 + [\"*int32\"])\n-def test_where(dtype):\n+def test_where(dtype, device):\n     select_ptrs = False\n     if dtype == \"*int32\":\n         dtype = \"int64\"\n         select_ptrs = True\n-    check_type_supported(dtype)\n+    check_type_supported(dtype, device)\n \n     @triton.jit\n     def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n@@ -616,10 +622,10 @@ def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n     y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n     z = np.where(cond, x, y)\n \n-    cond_tri = to_triton(cond, device='cuda')\n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n-    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n+    cond_tri = to_triton(cond, device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    y_tri = to_triton(y, device=device, dst_type=dtype)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device=device, dst_type=dtype)\n \n     grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n     where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs, TEST_SCALAR_POINTERS=False)\n@@ -630,7 +636,7 @@ def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n         assert (z == to_numpy(z_tri)).all()\n \n \n-def test_where_broadcast():\n+def test_where_broadcast(device):\n     @triton.jit\n     def where_kernel(cond_ptr, a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n         xoffsets = tl.arange(0, BLOCK_SIZE)[:, None]\n@@ -656,9 +662,9 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n     x = numpy_random((SIZE, SIZE), dtype_str=dtype, rs=rs)\n     mask = numpy_random(SIZE, 'bool', rs=rs)\n     z = np.where(mask, x, 0)\n-    cond_tri = to_triton(mask, device=\"cuda\")\n-    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n-    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device='cuda', dst_type=dtype)\n+    cond_tri = to_triton(mask, device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype)\n+    z_tri = to_triton(np.empty((SIZE, SIZE), dtype=z.dtype), device=device, dst_type=dtype)\n     where_kernel[(1,)](cond_tri, x_tri, z_tri, SIZE)\n     assert (z == to_numpy(z_tri)).all()\n     where_scalar_condition[(1,)](x_tri, z_tri, SIZE)\n@@ -675,7 +681,7 @@ def where_scalar_condition(a_ptr, out_ptr, BLOCK_SIZE: tl.constexpr):\n ] + [\n     (dtype_x, ' ~x') for dtype_x in int_dtypes\n ])\n-def test_unary_op(dtype_x, expr, device='cuda'):\n+def test_unary_op(dtype_x, expr, device):\n     _test_unary(dtype_x, expr, device=device)\n \n # ----------------\n@@ -684,7 +690,7 @@ def test_unary_op(dtype_x, expr, device='cuda'):\n \n \n @pytest.mark.parametrize(\"dtype_x, expr\", [(dtype_x, expr) for dtype_x in [\"float32\", \"float64\"] for expr in ['exp', 'log', 'cos', 'sin']])\n-def test_math_op(dtype_x, expr, device='cuda'):\n+def test_math_op(dtype_x, expr, device):\n     _test_unary(dtype_x, f'tl.{expr}(x)', f'np.{expr}(x) ', device=device)\n \n # ----------------\n@@ -696,21 +702,21 @@ def test_math_op(dtype_x, expr, device='cuda'):\n     (dtype_x)\n     for dtype_x in dtypes_with_bfloat16\n ])\n-def test_abs(dtype_x, device='cuda'):\n+def test_abs(dtype_x, device):\n     _test_unary(dtype_x, 'tl.abs(x)', 'np.abs(x) ', device=device)\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n-def test_abs_f8(in_dtype):\n+def test_abs_f8(in_dtype, device):\n \n     @triton.jit\n-    def abs_kernel(Z, X, SIZE: tl.constexpr):\n+    def abs_kernel(X, Z, SIZE: tl.constexpr):\n         off = tl.arange(0, SIZE)\n         x = tl.load(X + off)\n         z = tl.abs(x)\n         tl.store(Z + off, z)\n \n-    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device=device)\n     # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n     all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n     f8_tensor[all_exp_ones] = 0\n@@ -723,7 +729,7 @@ def abs_kernel(Z, X, SIZE: tl.constexpr):\n     f32_tensor = convert_float_to_float32(f8_tensor, in_dtype)\n     expect = f32_tensor.abs()\n     actual_f8 = convert_float_to_float32(out_f8, in_dtype)\n-    torch.testing.assert_allclose(expect, actual_f8)\n+    torch.testing.assert_allclose(actual_f8, expect)\n \n \n # ----------------\n@@ -750,7 +756,7 @@ def make_ptr_str(name, shape):\n               ':, :, None']\n     for d in ['int32', 'uint32', 'uint16']\n ])\n-def test_index1d(expr, dtype_str, device='cuda'):\n+def test_index1d(expr, dtype_str, device):\n     rank_x = expr.count(':')\n     rank_y = expr.count(',') + 1\n     shape_x = [32 for _ in range(rank_x)]\n@@ -785,7 +791,7 @@ def generate_kernel(shape_x, shape_z):\n     z_ref = eval(expr) + y\n     # triton result\n     z_tri = to_triton(np.empty_like(z_ref), device=device)\n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     kernel_match[(1, )](z_tri, x_tri, num_warps=1, SIZE=shape_x[0])\n     # compare\n     assert (z_ref == to_numpy(z_tri)).all()\n@@ -814,9 +820,7 @@ def tuples_fn(a, b):\n         a * b\n \n \n-def test_tuples():\n-    device = 'cuda'\n-\n+def test_tuples(device):\n     @triton.jit\n     def with_fn(X, Y, A, B, C):\n         x = tl.load(X)\n@@ -907,9 +911,7 @@ def noinline_multi_values_fn(x, y, Z):\n \n \n @pytest.mark.parametrize(\"mode\", [\"simple\", \"call_graph\", \"shared\", \"dynamic\", \"multi_values\"])\n-def test_noinline(mode):\n-    device = 'cuda'\n-\n+def test_noinline(mode, device):\n     @triton.jit\n     def kernel(X, Y, Z):\n         x = tl.load(X)\n@@ -946,7 +948,9 @@ def kernel(X, Y, Z):\n     ]\n     for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']\n     for sem in [None, 'acquire', 'release', 'acq_rel', 'relaxed']]))\n-def test_atomic_rmw(op, dtype_x_str, mode, sem, device='cuda'):\n+def test_atomic_rmw(op, dtype_x_str, mode, sem, device):\n+    check_cuda_only(device)\n+\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         if dtype_x_str == 'float16':\n@@ -996,7 +1000,7 @@ def kernel(X, Z):\n     assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n-def test_atomic_rmw_predicate(device=\"cuda\"):\n+def test_atomic_rmw_predicate(device):\n     @triton.jit\n     def kernel(X):\n         val = tl.program_id(0)\n@@ -1009,7 +1013,7 @@ def kernel(X):\n \n @pytest.mark.parametrize(\"shape, axis\",\n                          [(shape, axis) for shape in [(2, 2), (2, 8), (8, 2), (8, 8), (32, 32)] for axis in [0, 1]])\n-def test_tensor_atomic_rmw(shape, axis, device=\"cuda\"):\n+def test_tensor_atomic_rmw(shape, axis, device):\n     shape0, shape1 = shape\n     # triton kernel\n \n@@ -1035,7 +1039,7 @@ def kernel(Z, X, AXIS: tl.constexpr, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=1e-4)\n \n \n-def test_tensor_atomic_rmw_block(device=\"cuda\"):\n+def test_tensor_atomic_rmw_block(device):\n     shape = (8, 8)\n \n     @triton.jit\n@@ -1052,13 +1056,13 @@ def kernel(X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"sem\", [None, 'acquire', 'release', 'acq_rel', 'relaxed'])\n-def test_atomic_cas(sem):\n+def test_atomic_cas(sem, device):\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n     def change_value(Lock):\n         tl.atomic_cas(Lock, 0, 1)\n \n-    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n+    Lock = torch.zeros((1,), device=device, dtype=torch.int32)\n     change_value[(1,)](Lock)\n \n     assert (Lock[0] == 1)\n@@ -1075,8 +1079,8 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n         # release lock\n         tl.atomic_xchg(Lock, 0)\n \n-    Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n-    data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n+    Lock = torch.zeros((1,), device=device, dtype=torch.int32)\n+    data = torch.zeros((128,), device=device, dtype=torch.float32)\n     ref = torch.full((128,), 64.0)\n     h = serialized_add[(64,)](data, Lock, SEM=sem)\n     sem_str = \"acq_rel\" if sem is None else sem\n@@ -1103,10 +1107,10 @@ def serialized_add(data, Lock, SEM: tl.constexpr):\n ] + [\n     (f'int{x}', f'uint{x}', True) for x in [8, 16, 32, 64]\n ])\n-def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n+def test_cast(dtype_x, dtype_z, bitcast, device):\n     # bfloat16 on cc < 80 will not be tested\n-    check_type_supported(dtype_x)\n-    check_type_supported(dtype_z)\n+    check_type_supported(dtype_x, device)\n+    check_type_supported(dtype_z, device)\n \n     # This is tricky because numpy doesn't have bfloat, and torch doesn't have uints.\n     x0 = 43 if dtype_x in int_dtypes else 43.5\n@@ -1116,7 +1120,7 @@ def test_cast(dtype_x, dtype_z, bitcast, device='cuda'):\n         x_tri = torch.tensor([x0], dtype=getattr(torch, dtype_x), device=device)\n     else:\n         x = np.array([x0], dtype=getattr(np, dtype_x))\n-        x_tri = to_triton(x)\n+        x_tri = to_triton(x, device=device)\n \n     # triton kernel\n     @triton.jit\n@@ -1148,8 +1152,8 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"dtype_str, num_warps\", [(dtype_str, num_warps) for dtype_str in int_dtypes + float_dtypes for num_warps in [4, 8]])\n-def test_cat(dtype_str, num_warps):\n-    check_type_supported(dtype_str)\n+def test_cat(dtype_str, num_warps, device):\n+    check_type_supported(dtype_str, device)\n \n     @triton.jit\n     def kernel(X, Y, Z, N: tl.constexpr):\n@@ -1159,19 +1163,19 @@ def kernel(X, Y, Z, N: tl.constexpr):\n         z = tl.cat(x, y, can_reorder=True)\n         tl.store(Z + tl.arange(0, 2 * N), z)\n \n-    x = torch.arange(0, 128, device='cuda').to(getattr(torch, dtype_str))\n-    y = torch.arange(-128, 0, device='cuda').to(getattr(torch, dtype_str))\n+    x = torch.arange(0, 128, device=device).to(getattr(torch, dtype_str))\n+    y = torch.arange(-128, 0, device=device).to(getattr(torch, dtype_str))\n     z_ref = torch.cat([x, y], dim=0).sum()\n-    z = torch.zeros((256,), dtype=getattr(torch, dtype_str), device='cuda')\n+    z = torch.zeros((256,), dtype=getattr(torch, dtype_str), device=device)\n     kernel[(1, )](x, y, z, N=128, num_warps=num_warps)\n     assert z.sum() == z_ref\n     # check if there's no duplicate value in z\n     assert z.unique().size(0) == z.size(0)\n \n \n @pytest.mark.parametrize(\"dtype_str\", list(torch_dtypes))\n-def test_store_constant(dtype_str):\n-    check_type_supported(dtype_str)\n+def test_store_constant(dtype_str, device):\n+    check_type_supported(dtype_str, device)\n \n     \"\"\"Tests that boolean True is stored as 1\"\"\"\n     @triton.jit\n@@ -1184,14 +1188,14 @@ def kernel(output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     triton_dtype_str = 'uint8' if dtype_str == 'bool' else dtype_str\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.zeros([BLOCK_SIZE], dtype=tl.{triton_dtype_str}) + 1'})\n     block_size = 128\n-    ref = torch.ones([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n-    output = torch.zeros([block_size], dtype=getattr(torch, dtype_str), device='cuda')\n+    ref = torch.ones([block_size], dtype=getattr(torch, dtype_str), device=device)\n+    output = torch.zeros([block_size], dtype=getattr(torch, dtype_str), device=device)\n     kernel[(1,)](output, block_size, BLOCK_SIZE=block_size)\n \n     assert torch.all(output == ref)\n \n \n-def test_load_store_same_ptr():\n+def test_load_store_same_ptr(device):\n     @triton.jit()\n     def kernel(in_out_ptr):\n         pid = tl.program_id(axis=0)\n@@ -1200,7 +1204,7 @@ def kernel(in_out_ptr):\n         tl.store(in_out_ptr + pid, out)\n \n     for _ in range(1000):\n-        x = torch.ones((65536,), device=\"cuda\", dtype=torch.float32)\n+        x = torch.ones((65536,), device=device, dtype=torch.float32)\n         kernel[(65536,)](x, num_warps=32)\n         assert torch.all(x == 2)\n \n@@ -1236,9 +1240,9 @@ def convert_float_to_float32(fp: torch.tensor, dtype=None):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [torch.float16, torch.bfloat16])\n-def test_convert_float16_to_float32(in_dtype):\n+def test_convert_float16_to_float32(in_dtype, device):\n     \"\"\"Tests that check convert_float_to_float32 function\"\"\"\n-    check_type_supported(in_dtype)\n+    check_type_supported(in_dtype, device)\n \n     f16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16).view(in_dtype)\n     f32_output = convert_float_to_float32(f16_input)\n@@ -1253,9 +1257,9 @@ def test_convert_float16_to_float32(in_dtype):\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16, torch.float32])\n-def test_f8_xf16_roundtrip(in_dtype, out_dtype):\n+def test_f8_xf16_roundtrip(in_dtype, out_dtype, device):\n     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\"\n-    check_type_supported(out_dtype)\n+    check_type_supported(out_dtype, device)\n \n     @triton.jit\n     def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n@@ -1265,7 +1269,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device='cuda')\n+    f8_tensor = torch.tensor(range(-128, 128), dtype=torch.int8, device=device)\n     # f32_to_f8 doesn't handle nan, so we make sure f8_tensor doesn't contain any nan\n     all_exp_ones = (f8_tensor & 0b01111100) == 128 - 2**in_dtype.fp_mantissa_width\n     f8_tensor[all_exp_ones] = 0\n@@ -1292,7 +1296,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4, tl.float8e5])\n @pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16])\n-def test_f16_to_f8_rounding(in_dtype, out_dtype):\n+def test_f16_to_f8_rounding(in_dtype, out_dtype, device):\n     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n     error is the minimum over all float8.\n     Or the same explanation a bit mathier:\n@@ -1305,7 +1309,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    i16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device='cuda')\n+    i16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device=device)\n     f16_input = i16_input.view(out_dtype)\n     n_elements = f16_input.numel()\n     f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n@@ -1318,7 +1322,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n     abs_error = torch.abs(f16_input - f16_output)\n \n-    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n+    all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device=device)\n     all_f8_vals = triton.reinterpret(all_f8_vals_tensor, in_dtype)\n     all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=out_dtype)\n     copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n@@ -1370,12 +1374,13 @@ def get_reduced_dtype(dtype_str, op):\n                           for op in ['min', 'max',\n                                      'min-with-indices',\n                                      'max-with-indices',\n-                                     'argmin', 'argmax',\n+                                     'argmin-tie-break-left',\n+                                     'argmax-tie-break-left',\n                                      'sum']\n                           for dtype in dtypes_with_bfloat16\n                           for shape in [32, 64, 128, 512]])\n-def test_reduce1d(op, dtype_str, shape, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_reduce1d(op, dtype_str, shape, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1386,18 +1391,26 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n     if 'with-indices' in op:\n         patch = f'z, _ = tl.{op.split(\"-\")[0]}(x, axis=0, return_indices=True)'\n+    elif 'arg' in op:\n+        tie_break_left = 'tie-break-left' in op\n+        patch = f'z = tl.{op.split(\"-\")[0]}(x, axis=0, tie_break_left={tie_break_left})'\n     else:\n         patch = f'z = tl.{op}(x, axis=0)'\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': patch})\n     # input\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random((shape,), dtype_str=dtype_str, rs=rs)\n-    x_tri = to_triton(x, device=device)\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'max-with-indices': np.max,\n                 'min-with-indices': np.min,\n-                'argmin': np.argmin, 'argmax': np.argmax}[op]\n+                'argmin-tie-break-fast': np.argmin,\n+                'argmin-tie-break-left': np.argmin,\n+                'argmax-tie-break-fast': np.argmax,\n+                'argmax-tie-break-left': np.argmax}[op]\n+    if 'tie-break-left' in op:\n+        x[3:10] = numpy_op(x)\n+    x_tri = to_triton(x, device=device)\n     # numpy result\n     z_dtype_str = 'int32' if op in ('argmin', 'argmax') else dtype_str\n     z_tri_dtype_str = z_dtype_str\n@@ -1439,7 +1452,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n reduce2d_shapes = [(2, 32), (4, 32), (4, 128)]\n # TODO: fix and uncomment\n # , (32, 64), (64, 128)]\n-if 'V100' in torch.cuda.get_device_name(0):\n+if torch.cuda.is_available() and 'V100' in torch.cuda.get_device_name(0):\n     reduce2d_shapes += [(128, 256) and (32, 1024)]\n \n \n@@ -1455,8 +1468,8 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"op, dtype_str, shape, axis\", reduce_configs1 + reduce_configs2)\n-def test_reduce2d(op, dtype_str, shape, axis, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_reduce2d(op, dtype_str, shape, axis, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1477,7 +1490,7 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     rs = RandomState(17)\n     # limit the range of integers so that the sum does not overflow\n     x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     z_dtype_str = get_reduced_dtype(dtype_str, op)\n@@ -1513,17 +1526,135 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n             np.testing.assert_equal(z_ref, z_tri)\n \n \n+scan2d_shapes = [(8, 32), (16, 32), (32, 16), (2, 1024), (1024, 2), (32, 32), (1, 1024)]\n+\n+scan_configs = [\n+    (op, type, shape, axis, num_warps)\n+    for num_warps in [4, 16]\n+    for type in ['int32', 'float32']\n+    for axis in [1, 0]\n+    for shape in scan2d_shapes\n+    for op in ['cumsum', 'cumprod']\n+]\n+\n+\n+@pytest.mark.parametrize(\"op, dtype_str, shape, axis, num_warps\", scan_configs)\n+def test_scan2d(op, dtype_str, shape, axis, num_warps, device):\n+    check_type_supported(dtype_str, device)\n+\n+    # triton kernel\n+    @triton.jit\n+    def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexpr):\n+        range_m = tl.arange(0, BLOCK_M)\n+        range_n = tl.arange(0, BLOCK_N)\n+        x = tl.load(X + range_m[:, None] * BLOCK_N + range_n[None, :])\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z + range_m[:, None] * BLOCK_N + range_n[None, :], z)\n+\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{op}(x, axis={axis})'})\n+    # input\n+    rs = RandomState(17)\n+    x = numpy_random(shape, dtype_str=dtype_str, rs=rs)\n+    z = np.empty_like(x)\n+    x_tri = to_triton(x, device=device)\n+    numpy_op = {'cumsum': np.cumsum, 'cumprod': np.cumprod}[op]\n+    z_dtype_str = dtype_str\n+    z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+    # triton result\n+    z_tri = to_triton(z, device=device)\n+    kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis, num_warps=num_warps)\n+    z_tri = to_numpy(z_tri)\n+    # compare\n+    if dtype_str == 'float32':\n+        if op == 'cumprod':\n+            np.testing.assert_allclose(z_ref, z_tri, rtol=0.01, atol=1e-3)\n+        else:\n+            np.testing.assert_allclose(z_ref, z_tri, rtol=0.01)\n+    else:\n+        np.testing.assert_equal(z_ref, z_tri)\n+\n+\n+scan_layouts = [\n+    BlockedLayout([1, 4], [4, 8], [4, 1], [0, 1]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n+    BlockedLayout([4, 1], [4, 8], [1, 4], [0, 1]),\n+    BlockedLayout([2, 2], [4, 8], [2, 2], [0, 1]),\n+    BlockedLayout([2, 2], [8, 4], [2, 2], [0, 1]),\n+\n+    BlockedLayout([1, 4], [4, 8], [4, 1], [1, 0]),\n+    BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n+    BlockedLayout([4, 1], [4, 8], [1, 4], [1, 0]),\n+    BlockedLayout([2, 2], [4, 8], [2, 2], [1, 0]),\n+    BlockedLayout([2, 2], [8, 4], [2, 2], [1, 0]),\n+]\n+\n+\n+@pytest.mark.parametrize(\"M, N\", [[32, 32], [32, 64], [64, 32]])\n+@pytest.mark.parametrize(\"src_layout\", scan_layouts)\n+@pytest.mark.parametrize(\"axis\", [0, 1])\n+def test_scan_layouts(M, N, src_layout, axis, device):\n+    ir = f\"\"\"\n+    #blocked = {src_layout}\n+    module attributes {{\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32}} {{\n+    tt.func public @kernel_0d1d(%arg0: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}, %arg1: !tt.ptr<i32> {{tt.divisibility = 16 : i32}}) {{\n+      %cst = arith.constant dense<{N}> : tensor<{M}x1xi32, #blocked>\n+      %0 = tt.make_range {{end = {M} : i32, start = 0 : i32}} : tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>\n+      %1 = tt.expand_dims %0 {{axis = 1 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #blocked}}>>) -> tensor<{M}x1xi32, #blocked>\n+      %2 = arith.muli %1, %cst : tensor<{M}x1xi32, #blocked>\n+      %3 = tt.splat %arg0 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n+      %4 = tt.addptr %3, %2 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+      %5 = tt.make_range {{end = {N} : i32, start = 0 : i32}} : tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>\n+      %6 = tt.expand_dims %5 {{axis = 0 : i32}} : (tensor<{N}xi32, #triton_gpu.slice<{{dim = 0, parent = #blocked}}>>) -> tensor<1x{N}xi32, #blocked>\n+      %7 = tt.broadcast %4 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n+      %8 = tt.broadcast %6 : (tensor<1x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n+      %9 = tt.addptr %7, %8 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+      %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #blocked>\n+      %11 = \"tt.scan\"(%10) <{{axis = {axis} : i32}}> ({{\n+      ^bb0(%arg2: i32, %arg3: i32):\n+        %16 = arith.addi %arg2, %arg3 : i32\n+        tt.scan.return %16 : i32\n+      }}) : (tensor<{M}x{N}xi32, #blocked>) -> tensor<{M}x{N}xi32, #blocked>\n+      %12 = tt.splat %arg1 : (!tt.ptr<i32>) -> tensor<{M}x1x!tt.ptr<i32>, #blocked>\n+      %13 = tt.addptr %12, %2 : tensor<{M}x1x!tt.ptr<i32>, #blocked>, tensor<{M}x1xi32, #blocked>\n+      %14 = tt.broadcast %13 : (tensor<{M}x1x!tt.ptr<i32>, #blocked>) -> tensor<{M}x{N}x!tt.ptr<i32>, #blocked>\n+      %15 = tt.addptr %14, %8 : tensor<{M}x{N}x!tt.ptr<i32>, #blocked>, tensor<{M}x{N}xi32, #blocked>\n+      tt.store %15, %11 {{cache = 1 : i32, evict = 1 : i32}} : tensor<{M}x{N}xi32, #blocked>\n+      tt.return\n+    }}\n+    }}\n+    \"\"\"\n+\n+    import tempfile\n+    with tempfile.NamedTemporaryFile(mode='w', suffix='.ttgir') as f:\n+        f.write(ir)\n+        f.flush()\n+        kernel = triton.compile(f.name)\n+    rs = RandomState(17)\n+    x = rs.randint(-100, 100, (M, N)).astype('int32')\n+\n+    z = np.zeros((M, N)).astype('int32')\n+    x_tri = torch.tensor(x, device=device)\n+    z_tri = torch.tensor(z, device=device)\n+\n+    kernel[(1, 1, 1)](x_tri, z_tri)\n+\n+    z_ref = np.cumsum(x, axis=axis)\n+\n+    np.testing.assert_equal(z_ref, z_tri.cpu().numpy())\n+\n+\n layouts = [\n     BlockedLayout([1, 4], [8, 4], [4, 1], [1, 0]),\n     BlockedLayout([1, 4], [8, 4], [4, 1], [0, 1]),\n-    MmaLayout(version=(2, 0), warps_per_cta=[4, 1])\n+    MmaLayout(version=(2, 0), warps_per_cta=[4, 1]),\n+    MmaLayout(version=(2, 0), warps_per_cta=[2, 2])\n ]\n \n \n @pytest.mark.parametrize(\"M, N\", [[128, 16], [128, 128], [32, 128]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"axis\", [0, 1])\n-def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n+def test_reduce_layouts(M, N, src_layout, axis, device):\n     rdims_2d = f\"1x{N}\" if axis == 0 else f\"{M}x1\"\n     rdims_1d = f\"{N}\" if axis == 0 else f\"{M}\"\n     store_range = \"%7\" if axis == 0 else \"%1\"\n@@ -1595,7 +1726,7 @@ def test_reduce_layouts(M, N, src_layout, axis, device='cuda'):\n \n @pytest.mark.parametrize(\"M\", [32, 64, 128, 256])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_store_op(M, src_layout, device='cuda'):\n+def test_store_op(M, src_layout, device):\n     ir = f\"\"\"\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n@@ -1644,7 +1775,7 @@ def test_store_op(M, src_layout, device='cuda'):\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n @pytest.mark.parametrize(\"src_dim\", [0, 1])\n @pytest.mark.parametrize(\"dst_dim\", [0, 1])\n-def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device='cuda'):\n+def test_convert1d(M, src_layout, dst_layout, src_dim, dst_dim, device):\n     ir = f\"\"\"\n     #dst = {dst_layout}\n     #src = {src_layout}\n@@ -1701,7 +1832,19 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n \n @pytest.mark.parametrize(\"M, N\", [[128, 128], [256, 128], [256, 256], [128, 256]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_chain_reduce(M, N, src_layout, device='cuda'):\n+@pytest.mark.parametrize(\"op\", [\"sum\", \"max\"])\n+@pytest.mark.parametrize(\"first_axis\", [0, 1])\n+def test_chain_reduce(M, N, src_layout, op, device, first_axis):\n+    op_str = \"\"\n+    if op == \"sum\":\n+        op_str = f\"\"\"\n+        %13 = arith.addi %arg2, %arg3 : i32\n+        tt.reduce.return %13 : i32\"\"\"\n+    elif op == \"max\":\n+        op_str = f\"\"\"\n+        %13 = \"triton_gpu.cmpi\"(%arg2, %arg3) <{{predicate = 4 : i64}}> : (i32, i32) -> i1\n+        %14 = arith.select %13, %arg2, %arg3 : i32\n+        tt.reduce.return %14 : i32\"\"\"\n     ir = f\"\"\"\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n@@ -1720,14 +1863,12 @@ def test_chain_reduce(M, N, src_layout, device='cuda'):\n         %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #src>\n         %11 = \"tt.reduce\"(%10) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n-        %13 = arith.addi %arg2, %arg3 : i32\n-        tt.reduce.return %13 : i32\n-        }}) {{axis = 1 : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n+        {op_str}\n+        }}) {{axis = {first_axis} : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>\n         %12 = \"tt.reduce\"(%11) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n-        %13 = arith.addi %arg2, %arg3 : i32\n-        tt.reduce.return %13 : i32\n-        }}) {{axis = 0 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> i32\n+        {op_str}\n+        }}) {{axis = 0 : i32}} : (tensor<{M if first_axis == 1 else N}xi32, #triton_gpu.slice<{{dim = {first_axis}, parent = #src}}>>) -> i32\n         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n         tt.return\n     }}\n@@ -1748,12 +1889,15 @@ def test_chain_reduce(M, N, src_layout, device='cuda'):\n     z_tri = torch.tensor(z, device=device)\n \n     pgm = kernel[(1, 1, 1)](x_tri, z_tri)\n-    z_ref = np.sum(x)\n+    if op == \"sum\":\n+        z_ref = np.sum(x)\n+    elif op == \"max\":\n+        z_ref = np.max(x)\n \n     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n \n \n-def test_generic_reduction(device='cuda'):\n+def test_generic_reduction(device):\n \n     @triton.jit\n     def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n@@ -1789,8 +1933,8 @@ def var_mean_kernel(X, out_mean, out_var, BLOCK: tl.constexpr):\n                           for dtype in ['float16', 'float32']\n                              for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n-def test_permute(dtype_str, shape, perm, device='cuda'):\n-    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n+def test_permute(dtype_str, shape, perm, device):\n+    check_type_supported(dtype_str, device)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -1860,7 +2004,9 @@ def kernel(X, stride_xm, stride_xn,\n                                                       ('float16', 'float16'),\n                                                       ('float16', 'float32'),\n                                                       ('float32', 'float32')]])\n-def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device='cuda'):\n+def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, in_dtype, out_dtype, device):\n+    check_cuda_only(device)\n+\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         pytest.skip(\"Only test tl.dot() on devices with sm >= 70\")\n@@ -2018,10 +2164,50 @@ def kernel(X, stride_xm, stride_xk,\n         assert 'mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16' in ptx\n \n \n+@pytest.mark.parametrize('in_dtype', ['float32'])\n+def test_dot_mulbroadcastred(in_dtype, device):\n+    @triton.jit\n+    def kernel(Z, X, Y,\n+               M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n+               BM: tl.constexpr, BN: tl.constexpr, BK: tl.constexpr):\n+        pidn = tl.program_id(1)\n+        pidm = tl.program_id(0)\n+        offm = tl.arange(0, BM)[:, None]\n+        offn = tl.arange(0, BN)[None, :]\n+        offak = tl.arange(0, BK)[None, :]\n+        offbk = tl.arange(0, BK)[:, None]\n+        acc = tl.full((BM, BN), 0.0, tl.float32)\n+        for ridx5 in range(0, K // BK):\n+            x = tl.load(X + ((pidm * K * BM) + (offm * K) + (ridx5 * BK) + offak))\n+            y = tl.load(Y + ((pidn * BN) + (offbk * N) + (ridx5 * N * BK) + offn))\n+            x = tl.expand_dims(x, axis=2)\n+            y = tl.expand_dims(y, axis=0)\n+            t = tl.sum(x * y, axis=1)\n+            acc = t + acc\n+        tl.store(Z + ((pidm * BM * N) + (pidn * BN) + (offm * N) + offn), acc)\n+    M, N, K = 256, 192, 160\n+    BM, BN, BK = 128, 32, 32\n+    rs = RandomState(17)\n+    x = numpy_random((M, K), dtype_str=in_dtype, rs=rs)\n+    y = numpy_random((K, N), dtype_str=in_dtype, rs=rs)\n+    x = x * 0.1\n+    y = y * 0.1\n+    z = numpy_random((M, N), dtype_str=in_dtype, rs=rs)\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    z_tri = to_triton(z, device=device)\n+    grid = M // BM, N // BN\n+    h = kernel[grid](z_tri, x_tri, y_tri, M, N, K, BM, BN, BK)\n+    z_ref = np.matmul(x, y)\n+    np.testing.assert_allclose(z_ref, to_numpy(z_tri), atol=0.01)\n+    assert \"tt.dot\" in h.asm['ttir']\n+    assert \"triton_gpu.async_wait {num = 2 : i32}\" in h.asm['ttgir']\n+\n+\n @pytest.mark.parametrize(\"dtype_str\", int_dtypes + float_dtypes + ['bfloat16'])\n-def test_full(dtype_str):\n+def test_full(dtype_str, device):\n     dtype = getattr(torch, dtype_str)\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     @triton.jit\n     def kernel_static(out):\n@@ -2036,9 +2222,9 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n         tl.store(out_ptr, a)\n \n     kernel_static_patched = patch_kernel(kernel_static, {'GENERATE_TEST_HERE': f\"tl.full((128,), 2, tl.{dtype_str})\"})\n-    out_static = torch.zeros((128), dtype=dtype, device=\"cuda\")\n+    out_static = torch.zeros((128), dtype=dtype, device=device)\n     kernel_static_patched[(1,)](out_static)\n-    out_dynamic = torch.zeros((128), dtype=dtype, device=\"cuda\")\n+    out_dynamic = torch.zeros((128), dtype=dtype, device=device)\n     kernel_dynamic[(1,)](out_dynamic, 2, getattr(triton.language, dtype_str))\n     assert torch.all(out_static == 2)\n     assert torch.all(out_dynamic == 2)\n@@ -2050,43 +2236,42 @@ def kernel_dynamic(out, val, dtype: tl.constexpr):\n                           ('float(\"nan\")', \"f32\"), ('float(\"-nan\")', \"f32\"),\n                           (0., \"f32\"),\n                           (5, \"i32\"), (2**40, \"i64\"),])\n-def test_constexpr(literal, dtype_str):\n+def test_constexpr(literal, dtype_str, device):\n     @triton.jit\n     def kernel(out_ptr):\n         val = GENERATE_TEST_HERE\n         tl.store(out_ptr.to(tl.pointer_type(val.dtype)), val)\n \n     kernel_patched = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{literal}\"})\n-    out = torch.zeros((1,), dtype=torch.float32, device=\"cuda\")\n+    out = torch.zeros((1,), dtype=torch.float32, device=device)\n     h = kernel_patched[(1,)](out)\n     assert re.search(r\"arith.constant .* : \" + dtype_str, h.asm[\"ttir\"]) is not None\n \n-# TODO: uncomment once DotOperandEncoding::getElemsPerThread is implemented\n-# @pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n-# def test_dot_without_load(dtype_str):\n-#     @triton.jit\n-#     def _kernel(out):\n-#         a = GENERATE_TEST_HERE\n-#         b = GENERATE_TEST_HERE\n-#         c = tl.dot(a, b)\n-#         out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-#         tl.store(out_ptr, c)\n-\n-#     kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n-#     a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n-#     b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n-#     out_ref = torch.matmul(a, b)\n-#     out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=\"cuda\")\n-#     kernel[(1,)](out)\n-#     assert torch.all(out == out_ref)\n+\n+@pytest.mark.parametrize(\"dtype_str\", ['float32', 'float16'])\n+def test_dot_without_load(dtype_str, device):\n+    @triton.jit\n+    def _kernel(out):\n+        a = GENERATE_TEST_HERE\n+        b = GENERATE_TEST_HERE\n+        c = tl.dot(a, b)\n+        out_ptr = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+        tl.store(out_ptr, c)\n+    kernel = patch_kernel(_kernel, {'GENERATE_TEST_HERE': f\"tl.full((32, 32), 1.0, tl.{dtype_str})\"})\n+    a = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+    b = torch.ones((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+    out_ref = torch.matmul(a, b)\n+    out = torch.zeros((32, 32), dtype=getattr(torch, dtype_str), device=device)\n+    kernel[(1,)](out)\n+    assert torch.all(out == out_ref)\n \n # ---------------\n # test arange\n # ---------------\n \n \n @pytest.mark.parametrize(\"start\", [0, 1, 7, 16])\n-def test_arange(start, device='cuda'):\n+def test_arange(start, device):\n     BLOCK = 128\n     z_tri = torch.empty(BLOCK, dtype=torch.int32, device=device)\n \n@@ -2106,9 +2291,9 @@ def _kernel(z, BLOCK: tl.constexpr,\n \n \n @pytest.mark.parametrize(\"dtype_str, size, size_diff\", [(dtype_str, size, size_diff) for dtype_str in torch_dtypes for size in [128, 512] for size_diff in [0, 1, 2, 3, 4]])\n-def test_masked_load(dtype_str, size, size_diff, device='cuda'):\n+def test_masked_load(dtype_str, size, size_diff, device):\n     dtype = getattr(torch, dtype_str)\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     input_size = size - size_diff\n     output_size = size\n@@ -2141,8 +2326,8 @@ def _kernel(in_ptr, out_ptr, in_size: tl.constexpr, out_size: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n-def test_masked_load_shared_memory(dtype, device='cuda'):\n-    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+def test_masked_load_shared_memory(dtype, device):\n+    check_type_supported(dtype, device)  # bfloat16 on cc < 80 will not be tested\n \n     M = 32\n     N = 32\n@@ -2190,9 +2375,9 @@ def _kernel(in1_ptr, in2_ptr, output_ptr,\n \n \n @pytest.mark.parametrize(\"cache\", [\"\", \".ca\", \".cg\"])\n-def test_load_cache_modifier(cache):\n-    src = torch.empty(128, device='cuda')\n-    dst = torch.empty(128, device='cuda')\n+def test_load_cache_modifier(cache, device):\n+    src = torch.empty(128, device=device)\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst, src, CACHE: tl.constexpr):\n@@ -2214,9 +2399,9 @@ def _kernel(dst, src, CACHE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"N\", [16, 10, 11, 1024])\n-def test_vectorization(N):\n-    src = torch.empty(1024, device='cuda')\n-    dst = torch.empty(1024, device='cuda')\n+def test_vectorization(N, device):\n+    src = torch.empty(1024, device=device)\n+    dst = torch.empty(1024, device=device)\n \n     @triton.jit\n     def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n@@ -2233,10 +2418,10 @@ def _kernel(dst, src, N, BLOCK_SIZE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"has_hints\", [False, True])\n-def test_vectorization_hints(has_hints):\n-    src = torch.empty(1024, device='cuda')\n-    dst = torch.empty(1024, device='cuda')\n-    off = torch.zeros(1, device='cuda', dtype=torch.int32)\n+def test_vectorization_hints(has_hints, device):\n+    src = torch.empty(1024, device=device)\n+    dst = torch.empty(1024, device=device)\n+    off = torch.zeros(1, device=device, dtype=torch.int32)\n \n     @triton.jit\n     def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n@@ -2257,6 +2442,46 @@ def _kernel(dst, src, off, N, BLOCK_SIZE: tl.constexpr, HINT: tl.constexpr):\n # test store\n # ---------------\n \n+\n+@pytest.mark.parametrize(\"cache\", [\"\", \".wb\", \".cg\", \".cs\", \".wt\"])\n+def test_store_cache_modifier(cache):\n+    src = torch.empty(128, device='cuda')\n+    dst = torch.empty(128, device='cuda')\n+\n+    @triton.jit\n+    def _kernel(dst, src, CACHE: tl.constexpr):\n+        offsets = tl.arange(0, 128)\n+        x = tl.load(src + offsets)\n+        tl.store(dst + offsets, x, cache_modifier=CACHE)\n+\n+    pgm = _kernel[(1,)](dst, src, CACHE=cache)\n+    ptx = pgm.asm['ptx']\n+    if cache == '':\n+        assert 'st.global.wb' not in ptx\n+        assert 'st.global.cg' not in ptx\n+        assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' not in ptx\n+    if cache == '.wb':\n+        assert 'st.global.wb' in ptx\n+        assert 'st.global.cg' not in ptx\n+        assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' not in ptx\n+    if cache == '.cg':\n+        assert 'st.global.wb' not in ptx\n+        assert 'st.global.cg' in ptx\n+        assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' not in ptx\n+    if cache == '.cs':\n+        assert 'st.global.wb' not in ptx\n+        assert 'st.global.cg' not in ptx\n+        assert 'st.global.cs' in ptx\n+        assert 'st.global.wt' not in ptx\n+    if cache == '.wt':\n+        assert 'st.global.wb' not in ptx\n+        assert 'st.global.cg' not in ptx\n+        assert 'st.global.cs' not in ptx\n+        assert 'st.global.wt' in ptx\n+\n # ---------------\n # test if\n # ---------------\n@@ -2280,10 +2505,10 @@ def _impl(value=10):\n     return value\n \n \n-def test_default():\n+def test_default(device):\n     value = 5\n-    ret0 = torch.zeros(1, dtype=torch.int32, device='cuda')\n-    ret1 = torch.zeros(1, dtype=torch.int32, device='cuda')\n+    ret0 = torch.zeros(1, dtype=torch.int32, device=device)\n+    ret1 = torch.zeros(1, dtype=torch.int32, device=device)\n \n     @triton.jit\n     def _kernel(ret0, ret1, value):\n@@ -2299,7 +2524,7 @@ def _kernel(ret0, ret1, value):\n # ----------------\n \n \n-def test_noop(device='cuda'):\n+def test_noop(device):\n     @triton.jit\n     def kernel(x):\n         pass\n@@ -2326,7 +2551,7 @@ def kernel(x):\n     (2**31, 'i64'), (2**32 - 1, 'i64'), (2**32, 'i64'), (2**63 - 1, 'i64'),\n     (-2**63, 'i64'), (2**63, 'u64'), (2**64 - 1, 'u64')\n ])\n-def test_value_specialization(value: int, value_type: str, device='cuda') -> None:\n+def test_value_specialization(value: int, value_type: str, device) -> None:\n     spec_type = None\n \n     def cache_hook(*args, **kwargs):\n@@ -2338,7 +2563,7 @@ def cache_hook(*args, **kwargs):\n     def kernel(VALUE, X):\n         pass\n \n-    x = torch.tensor([3.14159], device='cuda')\n+    x = torch.tensor([3.14159], device=device)\n     pgm = kernel[(1, )](value, x)\n \n     JITFunction.cache_hook = None\n@@ -2353,13 +2578,13 @@ def kernel(VALUE, X):\n     \"value, overflow\",\n     [(2**64 - 1, False), (2**64, True), (-2**63, False), (-2**63 - 1, True)]\n )\n-def test_value_specialization_overflow(value: int, overflow: bool, device='cuda') -> None:\n+def test_value_specialization_overflow(value: int, overflow: bool, device) -> None:\n \n     @triton.jit\n     def kernel(VALUE, X):\n         pass\n \n-    x = torch.tensor([3.14159], device='cuda')\n+    x = torch.tensor([3.14159], device=device)\n \n     if overflow:\n         with pytest.raises(OverflowError):\n@@ -2375,7 +2600,7 @@ def kernel(VALUE, X):\n @pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>', '<<', '>>', '&', '^', '|'])\n @pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n @pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n-def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n+def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr, device):\n \n     @triton.jit\n     def kernel(Z, X, Y):\n@@ -2396,34 +2621,34 @@ def kernel(Z, X, Y):\n         y = numpy_random((1,), dtype_str=\"float32\")\n     kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n     z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n-    x_tri = to_triton(x)\n-    y_tri = to_triton(y)\n-    z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n+    x_tri = to_triton(x, device=device)\n+    y_tri = to_triton(y, device=device)\n+    z_tri = to_triton(np.empty((1,), dtype=z.dtype), device=device)\n     kernel[(1,)](z_tri, x_tri, y_tri)\n     np.testing.assert_allclose(z, to_numpy(z_tri))\n \n \n-def test_constexpr_shape():\n+def test_constexpr_shape(device):\n \n     @triton.jit\n     def kernel(X):\n         off = tl.arange(0, 128 + 128)\n         tl.store(X + off, off)\n \n-    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32), device=device)\n     kernel[(1,)](x_tri)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))\n \n \n-def test_constexpr_scalar_shape():\n+def test_constexpr_scalar_shape(device):\n \n     @triton.jit\n     def kernel(X, s):\n         off = tl.arange(0, 256)\n         val = off % (256 // s)\n         tl.store(X + off, val)\n \n-    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32), device=device)\n     kernel[(1,)](x_tri, 32)\n     np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256) % 8)\n \n@@ -2457,7 +2682,7 @@ def vecmul_kernel(ptr, n_elements, rep, type: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"type\", [\"inline\", \"noinline\"])\n-def test_call(type):\n+def test_call(type, device):\n \n     @triton.jit\n     def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n@@ -2466,7 +2691,7 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n \n     size = 1024\n     rand_val = numpy_random((size,), dtype_str=\"float32\")\n-    rand_val_tri = to_triton(rand_val, device='cuda')\n+    rand_val_tri = to_triton(rand_val, device=device)\n     err_msg = \"\"\n     try:\n         kernel[(size // 128,)](rand_val_tri, size, 3, 5, type)\n@@ -2484,8 +2709,8 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n-def test_if(if_type):\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+def test_if(if_type, device):\n \n     @triton.jit\n     def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr, StaticVaue: tl.constexpr):\n@@ -2509,16 +2734,17 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n             else:\n                 tl.store(Ret, tl.load(XFalse))\n \n-    cond = torch.ones(1, dtype=torch.int32, device='cuda')\n-    x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n-    x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n-    ret = torch.empty(1, dtype=torch.float32, device='cuda')\n+    cond = torch.ones(1, dtype=torch.int32, device=device)\n+    x_true = torch.tensor([3.14], dtype=torch.float32, device=device)\n+    x_false = torch.tensor([1.51], dtype=torch.float32, device=device)\n+    ret = torch.empty(1, dtype=torch.float32, device=device)\n+\n     kernel[(1,)](cond, x_true, x_false, ret, if_type, True, 1)\n     assert torch.equal(ret, x_true)\n \n \n-def test_num_warps_pow2():\n-    dst = torch.empty(128, device='cuda')\n+def test_num_warps_pow2(device):\n+    dst = torch.empty(128, device=device)\n \n     @triton.jit\n     def _kernel(dst):\n@@ -2542,7 +2768,7 @@ def _kernel(dst):\n                           ('float32', 'math.pow', tl.math.libdevice_path()),\n                           ('float64', 'math.pow_dtype', tl.math.libdevice_path()),\n                           ('float64', 'math.norm4d', '')])\n-def test_math_tensor(dtype_str, expr, lib_path):\n+def test_math_tensor(dtype_str, expr, lib_path, device):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -2583,9 +2809,9 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.{expr}(x, x, x, x)'})\n         y_ref = np.sqrt(4 * np.power(x, 2))\n \n-    x_tri = to_triton(x)\n+    x_tri = to_triton(x, device=device)\n     # triton result\n-    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     if expr == 'math.ffs':\n@@ -2598,7 +2824,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n                          [('float32', 'math.pow', ''),\n                           ('float64', 'math.pow_dtype', ''),\n                           ('float64', 'math.pow', tl.math.libdevice_path())])\n-def test_math_scalar(dtype_str, expr, lib_path):\n+def test_math_scalar(dtype_str, expr, lib_path, device):\n \n     @triton.jit\n     def kernel(X, Y, BLOCK: tl.constexpr):\n@@ -2623,8 +2849,8 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n         y_ref[:] = np.power(x, 0.5)\n \n     # triton result\n-    x_tri = to_triton(x)[0].item()\n-    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device='cuda')\n+    x_tri = to_triton(x, device=device)[0].item()\n+    y_tri = to_triton(numpy_random((shape[0],), dtype_str=dtype_str, rs=rs), device=device)\n     kernel[(1,)](x_tri, y_tri, BLOCK=shape[0], extern_libs={'libdevice': lib_path})\n     # compare\n     np.testing.assert_allclose(y_ref, to_numpy(y_tri), rtol=0.01)\n@@ -2637,7 +2863,7 @@ def kernel(X, Y, BLOCK: tl.constexpr):\n @pytest.mark.parametrize(\"lo, hi, iv\", [(2**35, 2**35 + 20, 1), (2**35, 2**35 + 20, 2), (2**35, 2**35 + 20, 3),\n                                         (15, -16, -1), (15, -16, -2), (15, -16, -3),\n                                         (-18, -22, -1), (22, 18, -1)])\n-def test_for_iv(lo, hi, iv):\n+def test_for_iv(lo, hi, iv, device):\n \n     @triton.jit\n     def kernel(Out, lo, hi, iv: tl.constexpr):\n@@ -2649,12 +2875,12 @@ def kernel(Out, lo, hi, iv: tl.constexpr):\n \n     lo = 2**35\n     hi = 2**35 + 20\n-    out = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int64), device=device)\n     kernel[(1,)](out, lo, hi, iv)\n     assert out[0] == sum(range(lo, hi, iv))\n \n \n-def test_if_else():\n+def test_if_else(device):\n \n     @triton.jit\n     def kernel(Cond, TrueVal, FalseVal, Out):\n@@ -2664,10 +2890,10 @@ def kernel(Cond, TrueVal, FalseVal, Out):\n             val = tl.load(FalseVal)\n         tl.store(Out, val)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    true_val = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    false_val = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n-    cond = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    true_val = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    false_val = to_triton(np.full((1,), 2, dtype=np.int32), device=device)\n+    cond = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     # True\n     cond[0] = True\n     kernel[(1,)](cond, true_val, false_val, out)\n@@ -2679,7 +2905,7 @@ def kernel(Cond, TrueVal, FalseVal, Out):\n \n \n @pytest.mark.parametrize(\"mode\", [\"dynamic\", \"static\"])\n-def test_if_return(mode):\n+def test_if_return(mode, device):\n \n     @triton.jit\n     def kernel(ExitEarly, Out, cond: tl.constexpr, mode: tl.constexpr):\n@@ -2693,8 +2919,8 @@ def kernel(ExitEarly, Out, cond: tl.constexpr, mode: tl.constexpr):\n                 return\n         tl.store(Out, 1)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    exit_early = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    exit_early = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     # exit early path taken\n     exit_early[0] = 1\n     kernel[(1,)](exit_early, out, True, mode)\n@@ -2739,7 +2965,7 @@ def add_fn_static_cond(x, cond: tl.constexpr):\n @pytest.mark.parametrize(\"call_type\", [\"attribute\", \"attribute_jit\",\n                                        \"jit\", \"jit_if\", \"jit_ifexp\", \"jit_expr\",\n                                        \"jit_static_cond\", \"jit_noinline\", \"jit_extern\"])\n-def test_if_call(call_type):\n+def test_if_call(call_type, device):\n     @triton.jit\n     def kernel(Out, call_type: tl.constexpr):\n         pid = tl.program_id(0)\n@@ -2798,15 +3024,15 @@ def kernel(Out, call_type: tl.constexpr):\n \n         tl.store(Out, o)\n \n-    out = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n     kernel[(1,)](out, call_type)\n     assert to_numpy(out)[0] == 1\n \n \n @pytest.mark.parametrize(\"_cond1\", [True, False])\n @pytest.mark.parametrize(\"_cond2\", [True, False])\n @pytest.mark.parametrize(\"_cond3\", [True, False])\n-def test_nested_if_else_return(_cond1, _cond2, _cond3):\n+def test_nested_if_else_return(_cond1, _cond2, _cond3, device):\n \n     @triton.jit\n     def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n@@ -2823,13 +3049,13 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n                 val = tl.load(Val3)\n         tl.store(Out, val)\n \n-    out = to_triton(np.full((1,), -1, dtype=np.int32), device='cuda')\n-    cond1 = to_triton(np.full((1,), _cond1, dtype=np.int32), device='cuda')\n-    cond2 = to_triton(np.full((1,), _cond2, dtype=np.int32), device='cuda')\n-    cond3 = to_triton(np.full((1,), _cond3, dtype=np.int32), device='cuda')\n-    val1 = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    val2 = to_triton(np.full((1,), 2, dtype=np.int32), device='cuda')\n-    val3 = to_triton(np.full((1,), 3, dtype=np.int32), device='cuda')\n+    out = to_triton(np.full((1,), -1, dtype=np.int32), device=device)\n+    cond1 = to_triton(np.full((1,), _cond1, dtype=np.int32), device=device)\n+    cond2 = to_triton(np.full((1,), _cond2, dtype=np.int32), device=device)\n+    cond3 = to_triton(np.full((1,), _cond3, dtype=np.int32), device=device)\n+    val1 = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    val2 = to_triton(np.full((1,), 2, dtype=np.int32), device=device)\n+    val3 = to_triton(np.full((1,), 3, dtype=np.int32), device=device)\n     kernel[(1,)](cond1, cond2, cond3, val1, val2, val3, out)\n     targets = {\n         (True, True, True): val1[0],\n@@ -2844,29 +3070,33 @@ def kernel(Cond1, Cond2, Cond3, Val1, Val2, Val3, Out):\n     assert out[0] == targets[(_cond1, _cond2, _cond3)]\n \n \n-def test_while():\n+def test_while(device):\n \n     @triton.jit\n-    def kernel(InitI, Bound, CutOff, OutI, OutJ):\n+    def kernel(InitI, Bound, CutOff, OutI, OutInitI, OutJ):\n         init_i = tl.load(InitI)\n         curr_i = init_i\n         j = 0\n-        while curr_i == init_i and j < tl.load(Bound):\n+        # Check that init_i is not updated by the loop\n+        while j < tl.load(Bound):\n             curr_i = curr_i + (j == tl.load(CutOff))\n             j += 1\n+            tl.store(OutInitI, init_i)\n         tl.store(OutI, curr_i)\n         tl.store(OutJ, j)\n \n-    out_i = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    out_j = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-    init_i = to_triton(np.full((1,), 1, dtype=np.int32), device='cuda')\n-    bound = to_triton(np.full((1,), 10, dtype=np.int32), device='cuda')\n-    cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device='cuda')\n-    kernel[(1,)](init_i, bound, cut_off, out_i, out_j)\n+    out_i = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    out_j = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+    init_i = to_triton(np.full((1,), 1, dtype=np.int32), device=device)\n+    out_init_i = to_triton(np.full((1,), 0, dtype=np.int32), device=device)\n+    bound = to_triton(np.full((1,), 10, dtype=np.int32), device=device)\n+    cut_off = to_triton(np.full((1,), 5, dtype=np.int32), device=device)\n+    kernel[(1,)](init_i, bound, cut_off, out_i, out_init_i, out_j)\n+    assert out_init_i[0] == init_i[0]\n     assert out_i[0] == init_i[0] + 1\n-    assert out_j[0] == cut_off[0] + 1\n+    assert out_j[0] == bound[0]\n \n-# def test_for_if():\n+# def test_for_if(device):\n \n #     @triton.jit\n #     def kernel(bound, cutoff, M, N):\n@@ -2880,8 +3110,8 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n #         tl.store(M, m)\n #         tl.store(N, n)\n \n-#     m = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n-#     n = to_triton(np.zeros((1,), dtype=np.int32), device='cuda')\n+#     m = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n+#     n = to_triton(np.zeros((1,), dtype=np.int32), device=device)\n #     kernel[(1,)](10, 7, m, n)\n #     print(m[0])\n #     print(n[0])\n@@ -2891,7 +3121,8 @@ def kernel(InitI, Bound, CutOff, OutI, OutJ):\n # -----------------------\n \n \n-def test_globaltimer():\n+def test_globaltimer(device):\n+    check_cuda_only(device)\n \n     @triton.jit\n     def kernel(Out1, Out2):\n@@ -2902,21 +3133,22 @@ def kernel(Out1, Out2):\n         end = tl.extra.cuda.globaltimer()\n         tl.store(Out2, end - start)\n \n-    out1 = to_triton(np.zeros((128,), dtype=np.int64), device='cuda')\n-    out2 = to_triton(np.zeros((1,), dtype=np.int64), device='cuda')\n+    out1 = to_triton(np.zeros((128,), dtype=np.int64), device=device)\n+    out2 = to_triton(np.zeros((1,), dtype=np.int64), device=device)\n     h = kernel[(1,)](out1, out2)\n     assert out2[0] > 0\n     # 2 inlined globaltimers + one extra in the wrapper extern function\n     assert h.asm[\"ptx\"].count(\"%globaltimer\") == 3\n \n \n-def test_smid():\n+def test_smid(device):\n+    check_cuda_only(device)\n \n     @triton.jit\n     def kernel(Out):\n         tl.store(Out + tl.program_id(0), tl.extra.cuda.smid())\n \n-    out = to_triton(np.zeros((1024,), dtype=np.int32), device='cuda')\n+    out = to_triton(np.zeros((1024,), dtype=np.int32), device=device)\n     h = kernel[(out.shape[0],)](out)\n     assert out.sort()[0].unique().shape[0] > 0\n     assert h.asm[\"ptx\"].count(\"%smid\") == 2\n@@ -2954,7 +3186,7 @@ def kernel(Out):\n @pytest.mark.parametrize(\"src_layout\", layouts)\n @pytest.mark.parametrize(\"interm_layout\", intermediate_layouts)\n @pytest.mark.parametrize(\"dst_layout\", layouts)\n-def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='cuda'):\n+def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device):\n     if str(src_layout) == str(dst_layout):\n         pytest.skip()\n     if 'mma' in str(src_layout) and 'mma' in str(dst_layout):\n@@ -3006,7 +3238,7 @@ def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='\n }\n \"\"\"\n \n-    x = to_triton(numpy_random(shape, dtype_str=dtype))\n+    x = to_triton(numpy_random(shape, dtype_str=dtype), device=device)\n     z = torch.empty_like(x)\n \n     # write the IR to a temporary file using mkstemp\n@@ -3020,23 +3252,23 @@ def test_convert2d(dtype, shape, src_layout, interm_layout, dst_layout, device='\n     assert torch.equal(z, x)\n \n \n-def test_load_scalar_with_mask():\n+def test_load_scalar_with_mask(device):\n     @triton.jit\n     def kernel(Input, Index, Out, N: int):\n         index = tl.load(Index)\n         scalar = tl.load(Input + index, mask=index < N, other=0)\n         tl.store(Out, scalar, mask=index < N)\n-    Index = torch.tensor([0], dtype=torch.int32, device='cuda')\n-    Input = torch.tensor([0], dtype=torch.int32, device='cuda')\n-    Out = torch.empty_like(Index, device='cuda')\n+    Index = torch.tensor([0], dtype=torch.int32, device=device)\n+    Input = torch.tensor([0], dtype=torch.int32, device=device)\n+    Out = torch.empty_like(Index, device=device)\n     kernel[(1,)](Input, Index, Out, Index.numel())\n     assert Out.data[0] == 0\n \n \n # This test is used to test our own PTX codegen for float16 and int16 conversions\n # maybe delete it later after ptxas has been fixed\n @pytest.mark.parametrize(\"dtype_str\", ['float16', 'int16'])\n-def test_ptx_cast(dtype_str):\n+def test_ptx_cast(dtype_str, device):\n     @triton.jit\n     def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):\n         xoffset = tl.program_id(0) * XBLOCK\n@@ -3066,7 +3298,7 @@ def kernel(in_ptr0, out_ptr2, xnumel, rnumel, dtype: tl.constexpr, XBLOCK: tl.co\n         triton_dtype = tl.float32\n \n     s0 = 4\n-    buf11 = -torch.ones((6 * s0, 197, 197), device='cuda', dtype=torch_dtype)\n-    buf14 = -torch.ones((s0, 6, 197, 197), device='cuda', dtype=torch_dtype)\n+    buf11 = -torch.ones((6 * s0, 197, 197), device=device, dtype=torch_dtype)\n+    buf14 = -torch.ones((s0, 6, 197, 197), device=device, dtype=torch_dtype)\n     kernel[(4728,)](buf11, buf14, 1182 * s0, 197, triton_dtype, 1, 256, num_warps=2)\n     assert buf14.to(torch.float32).mean() == -2.0"}, {"filename": "python/test/unit/language/test_random.py", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -115,7 +115,7 @@ def random_raw(self):\n                          [(size, seed) for size in ['10', '4,53', '10000']\n                           for seed in [0, 42, 124, 54, 0xffffffff, 0xdeadbeefcafeb0ba]]\n                          )\n-def test_randint(size, seed, device='cuda'):\n+def test_randint(size, seed, device):\n     size = list(map(int, size.split(',')))\n \n     @triton.jit\n@@ -141,7 +141,7 @@ def kernel(X, N, seed):\n                          [(size, seed) for size in [1000000]\n                           for seed in [0, 42, 124, 54]]\n                          )\n-def test_rand(size, seed, device='cuda'):\n+def test_rand(size, seed, device):\n     @triton.jit\n     def kernel(X, N, seed):\n         offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n@@ -162,7 +162,7 @@ def kernel(X, N, seed):\n                          [(size, seed) for size in [1000000]\n                           for seed in [0, 42, 124, 54]]\n                          )\n-def test_randn(size, seed, device='cuda'):\n+def test_randn(size, seed, device):\n     @triton.jit\n     def kernel(X, N, seed):\n         offset = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)\n@@ -179,7 +179,7 @@ def kernel(X, N, seed):\n \n # tl.rand() should never produce >=1.0\n \n-def test_rand_limits():\n+def test_rand_limits(device):\n     @triton.jit\n     def kernel(input, output, n: tl.constexpr):\n         idx = tl.arange(0, n)\n@@ -190,8 +190,8 @@ def kernel(input, output, n: tl.constexpr):\n     min_max_int32 = torch.tensor([\n         torch.iinfo(torch.int32).min,\n         torch.iinfo(torch.int32).max,\n-    ], dtype=torch.int32, device='cuda')\n-    output = torch.empty(2, dtype=torch.float32, device='cuda')\n+    ], dtype=torch.int32, device=device)\n+    output = torch.empty(2, dtype=torch.float32, device=device)\n     kernel[(1,)](min_max_int32, output, 2)\n \n     assert output[0] == output[1]"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 9, "deletions": 3, "changes": 12, "file_content_changes": "@@ -67,6 +67,7 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 128, 32, 1, 4, 2, 384, 128, 640, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n+                (128, 256, 64, 1, 8, 3, 1024, 1024, 1024, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n         # n-stage\n@@ -90,7 +91,8 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 16, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n                 (128, 128, 32, 8, 4, 2, 1024, 1024, 1024, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8\", \"float16\")] for AT in [False, True] for BT in [False, True]\n+            ] for ADTYPE, BDTYPE in [(\"float8\", \"float16\"), (\"float16\", \"float32\"), (\"float32\", \"float16\"),\n+                                     (\"bfloat16\", \"float32\"), (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n         ]\n     ),\n )\n@@ -124,13 +126,17 @@ def get_input(n, m, t, dtype):\n             return f8_to_f16(x)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n         return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n+\n     # allocate/transpose inputs\n     a = get_input(M, K, AT, ADTYPE)\n     b = get_input(K, N, BT, BDTYPE)\n     # run test\n-    th_c = torch.matmul(a, b)\n+    th_c = torch.matmul(a.to(torch.float32), b.to(torch.float32))\n     try:\n         tt_c = triton.ops.matmul(a, b)\n-        torch.testing.assert_allclose(th_c, tt_c, atol=1e-2, rtol=0)\n+        atol, rtol = 1e-2, 0\n+        if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:\n+            atol, rtol = 3.5e-2, 0\n+        torch.testing.assert_allclose(th_c, tt_c, atol=atol, rtol=rtol)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}, {"filename": "python/triton/common/backend.py", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -85,10 +85,10 @@ def register_backend(device_type: str, backend_cls: type):\n \n def get_backend(device_type: str):\n     if device_type not in _backends:\n-        device_backend_package_name = f\"triton.third_party.{device_type}\"\n-        if importlib.util.find_spec(device_backend_package_name):\n+        device_backend_package_name = f\"...third_party.{device_type}\"\n+        if importlib.util.find_spec(device_backend_package_name, package=__spec__.name):\n             try:\n-                importlib.import_module(device_backend_package_name)\n+                importlib.import_module(device_backend_package_name, package=__spec__.name)\n             except Exception:\n                 return None\n         else:"}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -563,8 +563,11 @@ def visit_If(self, node):\n             cond = cond.to(language.int1, _builder=self.builder)\n             contains_return = ContainsReturnChecker(self.gscope).visit(node)\n             if self.scf_stack and contains_return:\n-                raise UnsupportedLanguageConstruct(None, node,\n-                                                   \"Cannot have `return` statements inside `while` or `for` statements in triton\")\n+                raise UnsupportedLanguageConstruct(\n+                    None, node,\n+                    \"Cannot have `return` statements inside `while` or `for` statements in triton \"\n+                    \"(note that this also applies to `return` statements that are inside functions \"\n+                    \"transitively called from within `while`/`for` statements)\")\n             elif self.scf_stack or not contains_return:\n                 self.visit_if_scf(cond, node)\n             else:\n@@ -682,10 +685,6 @@ def visit_While(self, node):\n                     yields.append(loop_defs[name])\n             self.builder.create_yield_op([y.handle for y in yields])\n \n-        # update global uses in while_op\n-        for i, name in enumerate(names):\n-            after_block.replace_use_in_block_with(init_args[i].handle, after_block.arg(i))\n-\n         # WhileOp defines new values, update the symbol table (lscope, local_defs)\n         for i, name in enumerate(names):\n             new_def = language.core.tensor(while_op.get_result(i), ret_types[i])"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -11,7 +11,6 @@\n from pathlib import Path\n from typing import Any, Tuple\n \n-# import triton\n from .._C.libtriton.triton import (add_external_libs, compile_ptx_to_cubin,\n                                    get_shared_memory_size, ir,\n                                    translate_llvmir_to_hsaco, translate_llvmir_to_ptx,"}, {"filename": "python/triton/interpreter/memory_map.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -2,7 +2,7 @@\n \n import dataclasses\n \n-from triton.interpreter import torch_wrapper\n+from . import torch_wrapper\n \n torch = torch_wrapper.torch\n "}, {"filename": "python/triton/interpreter/tl_lang.py", "status": "modified", "additions": 12, "deletions": 1, "changes": 13, "file_content_changes": "@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-# import triton\n from ..language import core as lcore\n from . import torch_wrapper\n from .core import ExecutionContext\n@@ -624,3 +623,15 @@ def sum(self, input, axis=None):\n     @_tensor_operation\n     def xor_sum(self, input, axis):\n         raise NotImplementedError()\n+\n+    @_tensor_operation\n+    def cumsum(self, input, axis=None):\n+        if axis is None:\n+            return torch.cumsum(input)\n+        return torch.cumsum(input, dim=axis)\n+\n+    @_tensor_operation\n+    def cumprod(self, input, axis=None):\n+        if axis is None:\n+            return torch.cumprod(input)\n+        return torch.cumprod(input, dim=axis)"}, {"filename": "python/triton/language/__init__.py", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "file_content_changes": "@@ -13,11 +13,13 @@\n     zeros_like,\n )\n from .core import (\n+    TRITON_MAX_TENSOR_NUMEL,\n     abs,\n     advance,\n     arange,\n     argmin,\n     argmax,\n+    associative_scan,\n     atomic_add,\n     atomic_and,\n     atomic_cas,\n@@ -33,6 +35,8 @@\n     cat,\n     constexpr,\n     cos,\n+    cumprod,\n+    cumsum,\n     debug_barrier,\n     device_assert,\n     device_print,\n@@ -103,11 +107,13 @@\n \n \n __all__ = [\n+    \"TRITON_MAX_TENSOR_NUMEL\",\n     \"abs\",\n     \"advance\",\n     \"arange\",\n     \"argmin\",\n     \"argmax\",\n+    \"associative_scan\",\n     \"atomic_add\",\n     \"atomic_and\",\n     \"atomic_cas\",\n@@ -125,6 +131,8 @@\n     \"cdiv\",\n     \"constexpr\",\n     \"cos\",\n+    \"cumprod\",\n+    \"cumsum\",\n     \"debug_barrier\",\n     \"device_assert\",\n     \"device_print\","}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 161, "deletions": 44, "changes": 205, "file_content_changes": "@@ -6,7 +6,6 @@\n from typing import Callable, List, Sequence, TypeVar\n \n from .._C.libtriton.triton import ir\n-# import triton\n from ..runtime.jit import jit\n from . import semantic\n \n@@ -1255,15 +1254,21 @@ def abs(x, _builder=None):\n # Reductions\n # -----------------------\n \n-def _add_reduction_docstr(name: str) -> Callable[[T], T]:\n+def _add_reduction_docstr(name: str, return_indices_arg: str = None, tie_break_arg: str = None) -> Callable[[T], T]:\n \n     def _decorator(func: T) -> T:\n         docstr = \"\"\"\n     Returns the {name} of all elements in the :code:`input` tensor along the provided :code:`axis`\n \n     :param input: the input values\n-    :param axis: the dimension along which the reduction should be done\n-    \"\"\"\n+    :param axis: the dimension along which the reduction should be done\"\"\"\n+        if return_indices_arg is not None:\n+            docstr += f\"\"\"\n+    :param {return_indices_arg}: if true, return index corresponding to the {name} value\"\"\"\n+        if tie_break_arg is not None:\n+            docstr += f\"\"\"\n+    :param {tie_break_arg}: if true, return the left-most indices in case of ties for values that aren't NaN\"\"\"\n+\n         func.__doc__ = docstr.format(name=name)\n         return func\n \n@@ -1374,65 +1379,93 @@ def maximum(x, y):\n \n \n @jit\n-def _max_combine(a, b):\n-    return maximum(a, b)\n+def _argmax_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    gt = value1 > value2 or tie\n+    v_ret = where(gt, value1, value2)\n+    i_ret = where(gt, index1, index2)\n+    return v_ret, i_ret\n \n \n @jit\n-def _argmax_combine(value1, index1, value2, index2):\n-    gt = value1 > value2\n-    value_ret = where(gt, value1, value2)\n-    index_ret = where(gt, index1, index2)\n-    return value_ret, index_ret\n+def _argmax_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, True)\n \n \n @jit\n-@_add_reduction_docstr(\"maximum\")\n-def max(input, axis=None, return_indices=False):\n+def _argmax_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmax_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@_add_reduction_docstr(\"maximum\",\n+                       return_indices_arg=\"return_indices\",\n+                       tie_break_arg=\"return_indices_tie_break_left\")\n+def max(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n     input = _promote_reduction_input(input)\n     if return_indices:\n-        return _reduce_with_indices(input, axis, _argmax_combine)\n+        if return_indices_tie_break_left:\n+            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_left)\n+        else:\n+            return _reduce_with_indices(input, axis, _argmax_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, _max_combine)\n+        return reduce(input, axis, maximum)\n \n \n @jit\n-@_add_reduction_docstr(\"maximum index\")\n-def argmax(input, axis):\n-    (_, ret) = max(input, axis, return_indices=True)\n+@_add_reduction_docstr(\"maximum index\", tie_break_arg=\"tie_break_left\")\n+def argmax(input, axis, tie_break_left=True):\n+    (_, ret) = max(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n     return ret\n \n # min and argmin\n \n \n @jit\n-def _min_combine(a, b):\n-    # TODO: minimum/maximum doesn't get lowered to fmin/fmax...\n-    return minimum(a, b)\n-\n-\n-@jit\n-def _argmin_combine(value1, index1, value2, index2):\n-    lt = value1 < value2\n+def _argmin_combine(value1, index1, value2, index2, tie_break_left):\n+    if tie_break_left:\n+        tie = value1 == value2 and index1 < index2\n+    else:\n+        tie = False\n+    lt = value1 < value2 or tie\n     value_ret = where(lt, value1, value2)\n     index_ret = where(lt, index1, index2)\n     return value_ret, index_ret\n \n \n @jit\n-@_add_reduction_docstr(\"minimum\")\n-def min(input, axis=None, return_indices=False):\n+def _argmin_combine_tie_break_left(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, True)\n+\n+\n+@jit\n+def _argmin_combine_tie_break_fast(value1, index1, value2, index2):\n+    return _argmin_combine(value1, index1, value2, index2, False)\n+\n+\n+@jit\n+@_add_reduction_docstr(\"minimum\",\n+                       return_indices_arg=\"return_indices\",\n+                       tie_break_arg=\"return_indices_tie_break_left\")\n+def min(input, axis=None, return_indices=False, return_indices_tie_break_left=True):\n     input = _promote_reduction_input(input)\n     if return_indices:\n-        return _reduce_with_indices(input, axis, _argmin_combine)\n+        if return_indices_tie_break_left:\n+            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_left)\n+        else:\n+            return _reduce_with_indices(input, axis, _argmin_combine_tie_break_fast)\n     else:\n-        return reduce(input, axis, _min_combine)\n+        return reduce(input, axis, minimum)\n \n \n @jit\n-@_add_reduction_docstr(\"minimum index\")\n-def argmin(input, axis):\n-    _, ret = min(input, axis, return_indices=True)\n+@_add_reduction_docstr(\"minimum index\",\n+                       tie_break_arg=\"tie_break_left\")\n+def argmin(input, axis, tie_break_left=True):\n+    _, ret = min(input, axis, return_indices=True, return_indices_tie_break_left=tie_break_left)\n     return ret\n \n \n@@ -1469,6 +1502,81 @@ def xor_sum(input, axis=None, _builder=None, _generator=None):\n                   _builder=_builder, _generator=_generator)\n \n \n+# -----------------------\n+# Scans\n+# -----------------------\n+\n+def _add_scan_docstr(name: str, return_indices_arg: str = None, tie_break_arg: str = None) -> Callable[[T], T]:\n+\n+    def _decorator(func: T) -> T:\n+        docstr = \"\"\"\n+    Returns the {name} of all elements in the :code:`input` tensor along the provided :code:`axis`\n+\n+    :param input: the input values\n+    :param axis: the dimension along which the scan should be done\"\"\"\n+        func.__doc__ = docstr.format(name=name)\n+        return func\n+\n+    return _decorator\n+\n+\n+@builtin\n+def associative_scan(input, axis, combine_fn, _builder=None, _generator=None):\n+    \"\"\"Applies the combine_fn to each elements with a carry in :code:`input` tensors along the provided :code:`axis` and update the carry\n+\n+    :param input: the input tensor, or tuple of tensors\n+    :param axis: the dimension along which the reduction should be done\n+    :param combine_fn: a function to combine two groups of scalar tensors (must be marked with @triton.jit)\n+\n+    \"\"\"\n+    if isinstance(input, tensor):\n+        return associative_scan((input,), axis, combine_fn,\n+                                _builder=_builder, _generator=_generator)[0]\n+\n+    def make_combine_region(scan_op):\n+        in_scalar_tys = [t.type.scalar for t in input]\n+        prototype = function_type(in_scalar_tys, in_scalar_tys * 2)\n+\n+        region = scan_op.get_region(0)\n+        with _insertion_guard(_builder):\n+            param_types = [ty.to_ir(_builder) for ty in prototype.param_types]\n+            block = _builder.create_block_with_parent(region, param_types)\n+            args = [tensor(block.arg(i), ty)\n+                    for i, ty in enumerate(prototype.param_types)]\n+            results = _generator.call_JitFunction(combine_fn, args, kwargs={})\n+            if isinstance(results, tensor):\n+                handles = [results.handle]\n+            else:\n+                handles = [r.handle for r in results]\n+            _builder.create_scan_ret(*handles)\n+    axis = _constexpr_to_value(axis)\n+    return semantic.associative_scan(input, axis, make_combine_region, _builder)\n+\n+# cumsum\n+\n+\n+@jit\n+@_add_scan_docstr(\"cumsum\")\n+def cumsum(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = _promote_reduction_input(input)\n+    return associative_scan(input, axis, _sum_combine)\n+\n+# cumprod\n+\n+\n+@jit\n+def _prod_combine(a, b):\n+    return a * b\n+\n+\n+@jit\n+@_add_scan_docstr(\"cumprod\")\n+def cumprod(input, axis=0):\n+    # todo rename this to a generic function name\n+    input = _promote_reduction_input(input)\n+    return associative_scan(input, axis, _prod_combine)\n+\n # -----------------------\n # Compiler Hint Ops\n # -----------------------\n@@ -1521,15 +1629,15 @@ def max_contiguous(input, values, _builder=None):\n @builtin\n def static_print(*values, sep: str = \" \", end: str = \"\\n\", file=None, flush=False, _builder=None):\n     '''\n-    Print the values at compile time. The parameters are the same as the Python builtin :code:`print`.\n+    Print the values at compile time.  The parameters are the same as the builtin :code:`print`.\n \n-    Calling the Python builtin :code:`print` inside your kernel is the same as calling this.\n+    NOTE: Calling the Python builtin :code:`print` is not the same as calling this, it instead maps to :code:`device_print`,\n+    which has special requirements for the arguments.\n \n     .. highlight:: python\n     .. code-block:: python\n \n         tl.static_print(f\"{BLOCK_SIZE=}\")\n-        print(f\"{BLOCK_SIZE=}\")\n     '''\n     pass\n \n@@ -1551,13 +1659,18 @@ def static_assert(cond, msg=\"\", _builder=None):\n @builtin\n def device_print(prefix, *args, _builder=None):\n     '''\n-    Print the values at runtime from the device.  String formatting does not work, so you should\n-    provide the values you want to print as arguments.\n+    Print the values at runtime from the device.  String formatting does not work for runtime values, so you should\n+    provide the values you want to print as arguments.  The first value must be a string, all following values must\n+    be scalars or tensors.\n+\n+    Calling the Python builtin :code:`print` is the same as calling this function, and the requirements for the arguments will match\n+    this function (not the normal requirements for :code:`print`).\n \n     .. highlight:: python\n     .. code-block:: python\n \n         tl.device_print(\"pid\", pid)\n+        print(\"pid\", pid)\n \n     :param prefix: a prefix to print before the values. This is required to be a string literal.\n     :param args: the values to print. They can be any tensor or scalar.\n@@ -1605,12 +1718,16 @@ def device_assert(cond, msg=\"\", _builder=None):\n     while hasattr(module, \"__name__\"):\n         frame = frame.f_back\n         module = inspect.getmodule(frame)\n-    func_name = frame.f_code.co_name\n-    file_name = frame.f_back.f_code.co_filename\n-    # TODO: The line number currently indicates the line\n-    # where the triton function is called but not where the\n-    # device_assert is called. Need to enhance this.\n-    lineno = frame.f_back.f_lineno\n+    lineno = 0\n+    func_name = 'unknown'\n+    file_name = 'unknown'\n+    if frame is not None:\n+        func_name = frame.f_code.co_name\n+        file_name = frame.f_back.f_code.co_filename\n+        # TODO: The line number currently indicates the line\n+        # where the triton function is called but not where the\n+        # device_assert is called. Need to enhance this.\n+        lineno = frame.f_back.f_lineno\n     return semantic.device_assert(_to_tensor(cond, _builder), msg, file_name, func_name, lineno, _builder)\n \n "}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 46, "deletions": 4, "changes": 50, "file_content_changes": "@@ -3,8 +3,8 @@\n from functools import wraps\n from typing import List, Optional, Sequence, Tuple, TypeVar\n \n+from .._C.libtriton.triton import ir\n from . import core as tl\n-from triton._C.libtriton.triton import ir\n \n T = TypeVar('T')\n \n@@ -775,7 +775,7 @@ def cast(input: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n-def _str_to_cache_modifier(cache_modifier):\n+def _str_to_load_cache_modifier(cache_modifier):\n     cache = ir.CACHE_MODIFIER.NONE  # default\n     if cache_modifier:\n         if cache_modifier == \".ca\":\n@@ -787,6 +787,22 @@ def _str_to_cache_modifier(cache_modifier):\n     return cache\n \n \n+def _str_to_store_cache_modifier(cache_modifier):\n+    cache = ir.CACHE_MODIFIER.NONE  # default\n+    if cache_modifier:\n+        if cache_modifier == \".wb\":\n+            cache = ir.CACHE_MODIFIER.WB\n+        elif cache_modifier == \".cg\":\n+            cache = ir.CACHE_MODIFIER.CG\n+        elif cache_modifier == \".cs\":\n+            cache = ir.CACHE_MODIFIER.CS\n+        elif cache_modifier == \".wt\":\n+            cache = ir.CACHE_MODIFIER.WT\n+        else:\n+            raise ValueError(f\"Cache modifier {cache_modifier} not supported\")\n+    return cache\n+\n+\n def _str_to_eviction_policy(eviction_policy):\n     eviction = ir.EVICTION_POLICY.NORMAL  # default\n     if eviction_policy:\n@@ -929,7 +945,7 @@ def load(ptr: tl.tensor,\n          is_volatile: bool,\n          builder: ir.builder) -> tl.tensor:\n     # Cache, eviction and padding options\n-    cache = _str_to_cache_modifier(cache_modifier)\n+    cache = _str_to_load_cache_modifier(cache_modifier)\n     eviction = _str_to_eviction_policy(eviction_policy)\n     padding = _str_to_padding_option(padding_option)\n \n@@ -1018,7 +1034,7 @@ def store(ptr: tl.tensor,\n           eviction_policy: str,\n           builder: ir.builder) -> tl.tensor:\n     # Cache and eviction options\n-    cache = _str_to_cache_modifier(cache_modifier)\n+    cache = _str_to_store_cache_modifier(cache_modifier)\n     eviction = _str_to_eviction_policy(eviction_policy)\n \n     if ptr.type.is_ptr() and ptr.type.element_ty.is_block():\n@@ -1305,6 +1321,32 @@ def wrap_tensor(x, scalar_ty):\n     )\n \n \n+# ===----------------------------------------------------------------------===\n+#                               Associative Scan\n+# ===----------------------------------------------------------------------===\n+\n+\n+def associative_scan(\n+    inputs: Sequence[tl.tensor], axis: int, region_builder_fn, builder: ir.builder\n+) -> Tuple[tl.tensor, ...]:\n+    if len(inputs) != 1:\n+        raise ValueError(\"Current implementation only support single tensor input\")\n+    shape = inputs[0].type.shape\n+\n+    def wrap_tensor(x, scalar_ty):\n+        res_ty = tl.block_type(scalar_ty, shape)\n+        return tl.tensor(x, res_ty)\n+\n+    scan_op = builder.create_scan([t.handle for t in inputs], axis)\n+    region_builder_fn(scan_op)\n+    scan_op.verify()\n+\n+    return tuple(\n+        wrap_tensor(scan_op.get_result(i), inputs[i].type.scalar)\n+        for i in range(len(inputs))\n+    )\n+\n+\n # ===----------------------------------------------------------------------===\n #                               Math\n # ===----------------------------------------------------------------------==="}, {"filename": "python/triton/ops/blocksparse/matmul.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -3,9 +3,6 @@\n from ... import cdiv, heuristics, jit\n from ... import language as tl\n \n-# import triton\n-# import language as tl\n-\n # ********************************************************\n # --------------------------------------------------------\n # Sparse = Dense x Dense (SDD)"}, {"filename": "python/triton/ops/blocksparse/softmax.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,7 +1,5 @@\n import torch\n \n-# import triton\n-# import language as tl\n from ... import jit\n from ... import language as tl\n from ... import next_power_of_2"}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "file_content_changes": "@@ -1,7 +1,5 @@\n import torch\n \n-# import triton\n-# import language as tl\n from .. import heuristics, jit\n from .. import language as tl\n from .. import next_power_of_2"}, {"filename": "python/triton/ops/flash_attention.py", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -10,9 +10,6 @@\n from .. import cdiv, jit\n from .. import language as tl\n \n-# import triton\n-# import language as tl\n-\n \n @jit\n def _fwd_kernel("}, {"filename": "python/triton/ops/matmul.py", "status": "modified", "additions": 20, "deletions": 4, "changes": 24, "file_content_changes": "@@ -4,8 +4,21 @@\n from .. import language as tl\n from .matmul_perf_model import early_config_prune, estimate_matmul_time\n \n-# import triton\n-# import language as tl\n+_ordered_datatypes = [torch.float16, torch.bfloat16, torch.float32]\n+\n+\n+def get_higher_dtype(a, b):\n+    if a is b:\n+        return a\n+\n+    assert a in _ordered_datatypes\n+    assert b in _ordered_datatypes\n+\n+    for d in _ordered_datatypes:\n+        if a is d:\n+            return b\n+        if b is d:\n+            return a\n \n \n def init_to_zero(name):\n@@ -100,6 +113,8 @@ def _kernel(A, B, C, M, N, K,\n             k_remaining = K - k * (BLOCK_K * SPLIT_K)\n             a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.)\n             b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.)\n+        a = a.to(C.dtype.element_ty)\n+        b = b.to(C.dtype.element_ty)\n         acc += tl.dot(a, b, out_dtype=dot_out_dtype)\n         A += BLOCK_K * SPLIT_K * stride_ak\n         B += BLOCK_K * SPLIT_K * stride_bk\n@@ -134,9 +149,10 @@ def _call(a, b, dot_out_dtype):\n         M, K = a.shape\n         _, N = b.shape\n         # allocates output\n-        c = torch.empty((M, N), device=device, dtype=a.dtype)\n+        c_dtype = get_higher_dtype(a.dtype, b.dtype)\n+        c = torch.empty((M, N), device=device, dtype=c_dtype)\n         if dot_out_dtype is None:\n-            if a.dtype in [torch.float16, torch.float32, torch.bfloat16]:\n+            if c_dtype in [torch.float16, torch.float32, torch.bfloat16]:\n                 dot_out_dtype = tl.float32\n             else:\n                 dot_out_dtype = tl.int32"}, {"filename": "python/triton/ops/matmul_perf_model.py", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -2,7 +2,6 @@\n \n import torch\n \n-# import triton\n from .. import cdiv\n from .._C.libtriton.triton import runtime\n from ..runtime import driver"}, {"filename": "python/triton/runtime/autotuner.py", "status": "modified", "additions": 14, "deletions": 6, "changes": 20, "file_content_changes": "@@ -25,7 +25,7 @@ def __reduce__(self):\n \n \n class Autotuner(KernelInterface):\n-    def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None):\n+    def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None, warmup=25, rep=100):\n         '''\n         :param prune_configs_by: a dict of functions that are used to prune configs, fields:\n             'perf_model': performance model used to predicate running time with different configs, returns running time\n@@ -58,6 +58,8 @@ def _hook(args):\n         self.perf_model, self.configs_top_k = perf_model, top_k\n         self.early_config_prune = early_config_prune\n         self.fn = fn\n+        self.warmup = warmup\n+        self.rep = rep\n \n     def _bench(self, *args, config, **meta):\n         # check for conflicts, i.e. meta-parameters both provided\n@@ -70,14 +72,15 @@ def _bench(self, *args, config, **meta):\n             )\n         # augment meta-parameters with tunable ones\n         current = dict(meta, **config.kwargs)\n+        full_nargs = {**self.nargs, **current}\n \n         def kernel_call():\n             if config.pre_hook:\n-                config.pre_hook(self.nargs)\n+                config.pre_hook(full_nargs)\n             self.hook(args)\n             self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n         try:\n-            return do_bench(kernel_call, quantiles=(0.5, 0.2, 0.8))\n+            return do_bench(kernel_call, warmup=self.warmup, rep=self.rep, quantiles=(0.5, 0.2, 0.8))\n         except OutOfResources:\n             return [float('inf'), float('inf'), float('inf')]\n \n@@ -106,7 +109,8 @@ def run(self, *args, **kwargs):\n             config = self.configs[0]\n         self.best_config = config\n         if config.pre_hook is not None:\n-            config.pre_hook(self.nargs)\n+            full_nargs = {**self.nargs, **kwargs, **self.best_config.kwargs}\n+            config.pre_hook(full_nargs)\n         return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n \n     def prune_configs(self, kwargs):\n@@ -171,7 +175,7 @@ def __str__(self):\n         return ', '.join(res)\n \n \n-def autotune(configs, key, prune_configs_by=None, reset_to_zero=None):\n+def autotune(configs, key, prune_configs_by=None, reset_to_zero=None, warmup=25, rep=100):\n     \"\"\"\n     Decorator for auto-tuning a :code:`triton.jit`'d function.\n \n@@ -202,9 +206,13 @@ def kernel(x_ptr, x_size, **META):\n         'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It takes configs:List[Config] as its input, and returns pruned configs.\n     :param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs.\n     :type reset_to_zero: list[str]\n+    :param warmup: Warmup time (in ms) to pass to benchmarking, defaults to 25.\n+    :type warmup: int\n+    :param rep: Repetition time (in ms) to pass to benchmarking, defaults to 100.\n+    :type rep: int\n     \"\"\"\n     def decorator(fn):\n-        return Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, prune_configs_by)\n+        return Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, prune_configs_by, warmup, rep)\n \n     return decorator\n "}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -11,8 +11,6 @@\n from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n                     overload)\n \n-# import triton\n-# from .. import compile, CompiledKernel\n from ..common.backend import get_backend\n \n TRITON_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n@@ -71,7 +69,7 @@ def visit_Attribute(self, node):\n         lhs = self.visit(node.value)\n         while isinstance(lhs, ast.Attribute):\n             lhs = self.visit(lhs.value)\n-        if lhs is None or lhs.__name__ == \"triton\":\n+        if lhs is None or (getattr(lhs, \"__name__\", \"\") == \"triton\" or getattr(lhs, \"__name__\", \"\").endswith(\".triton\")):\n             return None\n         return getattr(lhs, node.attr)\n \n@@ -81,7 +79,7 @@ def visit_Call(self, node):\n             return\n         if inspect.isbuiltin(func):\n             return\n-        if func.__module__ and func.__module__.startswith('triton.'):\n+        if func.__module__ and (func.__module__.startswith('triton.') or '.triton.' in func.__module__):\n             return\n         assert isinstance(func, JITFunction), f\"Function \\\"{func.__name__}\\\" is being called from a Triton function but is not a Triton function itself. Decorate it with @triton.jit to fix this\"\n         if func.hash is None:"}, {"filename": "test/Conversion/invalid.mlir", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "file_content_changes": "@@ -0,0 +1,41 @@\n+// RUN: triton-opt %s -split-input-file -verify-diagnostics\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=2}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf32, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{element types of operands A and B must match}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf32, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf16>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{mismatching encoding between A and B operands}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf16> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#mma0 = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[1,1]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, kWidth=2}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  tt.func @convert_dot(%A: tensor<16x16xf16, #dot_operand_a>, %B: tensor<16x16xf16, #dot_operand_b>, %C: tensor<16x16xf32, #mma0>) {\n+    // expected-error@+1 {{mismatching kWidth between A and B operands}}\n+    %D = tt.dot %A, %B, %C {allowTF32 = true, transA = false, transB = false} :\n+        tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    tt.return\n+  }\n+}"}, {"filename": "test/Conversion/triton_ops.mlir", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "file_content_changes": "@@ -187,3 +187,19 @@ tt.func @print_no_arg(%arg0: !tt.ptr<f32>) {\n   tt.store %arg0, %0 {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.return\n }\n+\n+// CHECK-LABEL: scan_op\n+tt.func @scan_op(%ptr: tensor<1x2x4x!tt.ptr<f32>>, %v : tensor<1x2x4xf32>) {\n+  // CHECK: tt.scan\n+  // CHECK-SAME: axis = 1\n+  // CHECK: tt.scan.return\n+  // CHECK-NEXT: (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n+  %a = \"tt.scan\"(%v) <{axis = 1 : i32}>({\n+  ^bb0(%arg0: f32, %arg1: f32):\n+    %add = arith.addf %arg0, %arg1 : f32\n+    tt.scan.return %add : f32\n+  }) : (tensor<1x2x4xf32>) -> tensor<1x2x4xf32>\n+  tt.store %ptr, %a : tensor<1x2x4xf32>\n+  tt.return\n+\n+}"}, {"filename": "test/Conversion/triton_to_tritongpu.mlir", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "file_content_changes": "@@ -67,3 +67,24 @@ tt.func @reduce_ops(%ptr: !tt.ptr<f32> {tt.divisibility = 16 : i32}) {\n \n   tt.return\n }\n+\n+\n+// -----\n+\n+tt.func public @select_op(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i1) attributes {noinline = false} {\n+  // CHECK-LABEL: select_op\n+  %cst = arith.constant dense<0.000000e+00> : tensor<128xf32>\n+  %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32>\n+  %1 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+  %2 = tt.addptr %1, %0 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n+  %3 = tt.load %2 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128xf32>\n+\n+  // CHECK: %[[splat:.*]] = tt.splat %arg2 : (i1) -> tensor<128xi1, #blocked>\n+  // CHECK-NEXT: %{{.*}} = \"triton_gpu.select\"(%[[splat]], %{{.*}}, %{{.*}}) : (tensor<128xi1, #blocked>, tensor<128xf32, #blocked>, tensor<128xf32, #blocked>) -> tensor<128xf32, #blocked>\n+  %4 = arith.select %arg2, %cst, %3 : tensor<128xf32>\n+\n+  %5 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<128x!tt.ptr<f32>>\n+  %6 = tt.addptr %5, %0 : tensor<128x!tt.ptr<f32>>, tensor<128xi32>\n+  tt.store %6, %4 {cache = 1 : i32, evict = 1 : i32} : tensor<128xf32>\n+  tt.return\n+}"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 64, "deletions": 0, "changes": 64, "file_content_changes": "@@ -1159,3 +1159,67 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n       tt.return\n   }\n }\n+\n+// -----\n+\n+#mma = #triton_gpu.mma<{versionMajor=2, warpsPerCTA=[2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, kWidth=1}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, kWidth=1}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+  // CHECK-LABEL: matmul_tf32_cst_b\n+  tt.func @matmul_tf32_cst_b(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n+  %a: tensor<32x16xf32, #dot_operand_a>, %c: tensor<32x32xf32, #mma>) {\n+  // CHECK: %[[CST:.+]] = llvm.mlir.constant(1.000000e+00 : f32) : f32\n+  // CHECK: %[[BC:.+]] = llvm.bitcast %[[CST]] : f32 to i32\n+  // CHECK: %[[SI:.+]] = llvm.mlir.undef : !llvm.struct<(i32, i32, i32, i32, i32, i32, i32, i32)>\n+  // CHECK: llvm.insertvalue %[[BC]], %[[SI]][0] : !llvm.struct<(i32, i32, i32, i32, i32, i32, i32, i32)>\n+    %b_mat = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #dot_operand_b>\n+    %28 = tt.dot %a, %b_mat, %c {allowTF32 = true, transA = false, transB = false} : tensor<32x16xf32, #dot_operand_a> * tensor<16x32xf32, #dot_operand_b> -> tensor<32x32xf32, #mma>\n+    %38 = triton_gpu.convert_layout %28 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x32x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %38 : tensor<32x32xf32, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  // CHECK-LABEL: matmul_f16_cst_operands\n+  tt.func public @matmul_f16_cst_operands(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+  // CHECK: %[[C1f:.+]] = llvm.mlir.constant(1.000000e+00 : f16) : f16\n+  // CHECK: %[[Ci16:.+]] = llvm.bitcast %[[C1f]] : f16 to i16\n+  // CHECK: %[[U:.+]] = llvm.mlir.undef : vector<2xi16>\n+  // CHECK: %[[C0:.+]] = llvm.mlir.constant(0 : i32) : i32\n+  // CHECK: %[[V0:.+]] = llvm.insertelement %[[Ci16]], %[[U]][%[[C0]] : i32] : vector<2xi16>\n+  // CHECK: %[[C1:.+]] = llvm.mlir.constant(1 : i32) : i32\n+  // CHECK: %[[V1:.+]] = llvm.insertelement %[[Ci16]], %[[V0]][%[[C1]] : i32] : vector<2xi16>\n+  // CHECK: %[[BC:.+]] = llvm.bitcast %[[V1]] : vector<2xi16> to i32\n+  // CHECK: %[[SU:.+]] = llvm.mlir.undef : !llvm.struct<(i32, i32, i32, i32, i32, i32, i32, i32)>\n+  // CHECK: llvm.insertvalue %[[BC]], %[[SU]][0] : !llvm.struct<(i32, i32, i32, i32, i32, i32, i32, i32)>\n+    %cst_0 = arith.constant dense<1.000000e+00> : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>>\n+    %cst_1 = arith.constant dense<1.000000e+00> : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>>\n+    %cst_2 = arith.constant dense<32> : tensor<32x1xi32, #blocked>\n+    %0 = tt.dot %cst_0, %cst_1, %cst {allowTF32 = true} : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<32x32xf32, #mma>\n+    %1 = triton_gpu.convert_layout %0 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    %2 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>\n+    %3 = tt.expand_dims %2 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<32x1xi32, #blocked>\n+    %4 = arith.muli %3, %cst_2 : tensor<32x1xi32, #blocked>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<32x1x!tt.ptr<f16>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<32x1x!tt.ptr<f16>, #blocked>, tensor<32x1xi32, #blocked>\n+    %7 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+    %8 = tt.expand_dims %7 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<1x32xi32, #blocked>\n+    %9 = tt.broadcast %6 : (tensor<32x1x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked>\n+    %10 = tt.broadcast %8 : (tensor<1x32xi32, #blocked>) -> tensor<32x32xi32, #blocked>\n+    %11 = tt.addptr %9, %10 : tensor<32x32x!tt.ptr<f16>, #blocked>, tensor<32x32xi32, #blocked>\n+    %12 = arith.truncf %1 : tensor<32x32xf32, #blocked> to tensor<32x32xf16, #blocked>\n+    tt.store %11, %12 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf16, #blocked>\n+    tt.return\n+  }\n+}"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 298, "deletions": 1, "changes": 299, "file_content_changes": "@@ -1122,7 +1122,7 @@ module attributes {\"triton_gpu.num-warps\" = 2 : i32} {\n \n // -----\n \n-// Check if the SimplifyReduceCvt handles convert_layout lifted from the for loop.\n+// Check if SimplifyReduceCvt handles convert_layout lifted from the for loop.\n // CHECK-LABEL: reduce_cvt2\n // Match the reduction\n // CHECK: tt.reduce\n@@ -1202,3 +1202,300 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     tt.return\n   }\n }\n+\n+// -----\n+\n+// Ensure that RematerializeForward doesn't apply when a convert has multiple uses\n+// CHECK-LABEL: loop_convert_multi_uses\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @loop_convert_multi_uses(%arg0: i32 {tt.divisibility = 16 : i32}, %arg1: i32, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg8: i32 {tt.divisibility = 16 : i32}, %arg9: i32, %arg10: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg11: i32 {tt.divisibility = 16 : i32}, %arg12: i32, %arg13: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg14: i32 {tt.divisibility = 16 : i32}, %arg15: i32 {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0xFF800000> : tensor<16xf32, #blocked>\n+    %c1_i32 = arith.constant 1 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %cst_0 = arith.constant dense<0.000000e+00> : tensor<16xf32, #blocked>\n+    %cst_1 = arith.constant dense<1> : tensor<16xi32, #blocked>\n+    %cst_2 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked1>\n+    %cst_3 = arith.constant dense<1> : tensor<16x1xi32, #blocked1>\n+    %c16_i32 = arith.constant 16 : i32\n+    %0 = tt.get_program_id x : i32\n+    %1 = tt.get_program_id y : i32\n+    %2 = arith.divsi %1, %arg0 : i32\n+    %3 = arith.remsi %1, %arg0 : i32\n+    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked>\n+    %5 = arith.muli %0, %c16_i32 : i32\n+    %6 = tt.splat %5 : (i32) -> tensor<16xi32, #blocked>\n+    %7 = arith.addi %6, %4 : tensor<16xi32, #blocked>\n+    %8 = arith.muli %2, %arg3 : i32\n+    %9 = arith.muli %3, %arg4 : i32\n+    %10 = arith.addi %8, %9 : i32\n+    %11 = triton_gpu.convert_layout %7 : (tensor<16xi32, #blocked>) -> tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %12 = tt.expand_dims %11 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16x1xi32, #blocked2>\n+    %13 = triton_gpu.convert_layout %12 : (tensor<16x1xi32, #blocked2>) -> tensor<16x1xi32, #blocked1>\n+    %14 = tt.splat %arg6 : (i32) -> tensor<16x1xi32, #blocked1>\n+    %15 = arith.muli %13, %14 : tensor<16x1xi32, #blocked1>\n+    %16 = triton_gpu.convert_layout %4 : (tensor<16xi32, #blocked>) -> tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %17 = tt.expand_dims %16 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x16xi32, #blocked3>\n+    %18 = tt.broadcast %15 : (tensor<16x1xi32, #blocked1>) -> tensor<16x16xi32, #blocked1>\n+    %19 = tt.broadcast %17 : (tensor<1x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked3>\n+    %20 = triton_gpu.convert_layout %19 : (tensor<16x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked1>\n+    %21 = arith.addi %18, %20 : tensor<16x16xi32, #blocked1>\n+    %22 = tt.splat %arg2 : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #blocked1>\n+    %23 = \"triton_gpu.cmpi\"(%13, %cst_3) <{predicate = 2 : i64}> : (tensor<16x1xi32, #blocked1>, tensor<16x1xi32, #blocked1>) -> tensor<16x1xi1, #blocked1>\n+    %24 = tt.broadcast %23 : (tensor<16x1xi1, #blocked1>) -> tensor<16x16xi1, #blocked1>\n+    %25 = arith.truncf %cst_2 : tensor<16x16xf32, #blocked1> to tensor<16x16xf16, #blocked1>\n+    %26 = arith.muli %2, %arg11 : i32\n+    %27 = arith.muli %3, %arg12 : i32\n+    %28 = arith.addi %26, %27 : i32\n+    %29 = tt.splat %arg10 : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>, #blocked>\n+    %30 = \"triton_gpu.cmpi\"(%7, %cst_1) <{predicate = 2 : i64}> : (tensor<16xi32, #blocked>, tensor<16xi32, #blocked>) -> tensor<16xi1, #blocked>\n+    %31 = arith.muli %2, %arg8 : i32\n+    %32 = arith.muli %3, %arg9 : i32\n+    %33 = arith.addi %31, %32 : i32\n+    %34 = tt.splat %arg7 : (!tt.ptr<f32>) -> tensor<16x!tt.ptr<f32>, #blocked>\n+    %35:3 = scf.for %arg17 = %c0_i32 to %arg1 step %c1_i32 iter_args(%arg18 = %cst_2, %arg19 = %cst_0, %arg20 = %cst) -> (tensor<16x16xf32, #blocked1>, tensor<16xf32, #blocked>, tensor<16xf32, #blocked>)  : i32 {\n+      %60 = arith.muli %arg17, %arg5 : i32\n+      %61 = arith.addi %10, %60 : i32\n+      %62 = tt.splat %61 : (i32) -> tensor<16x16xi32, #blocked1>\n+      %63 = arith.addi %62, %21 : tensor<16x16xi32, #blocked1>\n+      %64 = tt.addptr %22, %63 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi32, #blocked1>\n+      %65 = triton_gpu.convert_layout %64 : (tensor<16x16x!tt.ptr<f16>, #blocked1>) -> tensor<16x16x!tt.ptr<f16>, #blocked4>\n+      %66 = triton_gpu.convert_layout %24 : (tensor<16x16xi1, #blocked1>) -> tensor<16x16xi1, #blocked4>\n+      %67 = triton_gpu.convert_layout %25 : (tensor<16x16xf16, #blocked1>) -> tensor<16x16xf16, #blocked4>\n+      %68 = tt.load %65, %66, %67 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blocked4>\n+      %69 = triton_gpu.convert_layout %68 : (tensor<16x16xf16, #blocked4>) -> tensor<16x16xf16, #blocked1>\n+      %70 = arith.addi %28, %arg17 : i32\n+      %71 = tt.splat %70 : (i32) -> tensor<16xi32, #blocked>\n+      %72 = arith.addi %71, %7 : tensor<16xi32, #blocked>\n+      %73 = tt.addptr %29, %72 : tensor<16x!tt.ptr<f32>, #blocked>, tensor<16xi32, #blocked>\n+      %74 = triton_gpu.convert_layout %73 : (tensor<16x!tt.ptr<f32>, #blocked>) -> tensor<16x!tt.ptr<f32>, #blocked>\n+      %75 = triton_gpu.convert_layout %30 : (tensor<16xi1, #blocked>) -> tensor<16xi1, #blocked>\n+      %76 = triton_gpu.convert_layout %cst_0 : (tensor<16xf32, #blocked>) -> tensor<16xf32, #blocked>\n+      %77 = tt.load %74, %75, %76 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16xf32, #blocked>\n+      %78 = arith.addi %33, %arg17 : i32\n+      %79 = tt.splat %78 : (i32) -> tensor<16xi32, #blocked>\n+      %80 = arith.addi %79, %7 : tensor<16xi32, #blocked>\n+      %81 = tt.addptr %34, %80 : tensor<16x!tt.ptr<f32>, #blocked>, tensor<16xi32, #blocked>\n+      %82 = triton_gpu.convert_layout %81 : (tensor<16x!tt.ptr<f32>, #blocked>) -> tensor<16x!tt.ptr<f32>, #blocked>\n+      %83 = triton_gpu.convert_layout %30 : (tensor<16xi1, #blocked>) -> tensor<16xi1, #blocked>\n+      %84 = triton_gpu.convert_layout %cst_0 : (tensor<16xf32, #blocked>) -> tensor<16xf32, #blocked>\n+      %85 = tt.load %82, %83, %84 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16xf32, #blocked>\n+      %86 = \"triton_gpu.cmpf\"(%arg20, %85) <{predicate = 2 : i64}> : (tensor<16xf32, #blocked>, tensor<16xf32, #blocked>) -> tensor<16xi1, #blocked>\n+      %87 = \"triton_gpu.select\"(%86, %arg20, %85) : (tensor<16xi1, #blocked>, tensor<16xf32, #blocked>, tensor<16xf32, #blocked>) -> tensor<16xf32, #blocked>\n+      %88 = arith.subf %arg20, %87 : tensor<16xf32, #blocked>\n+      %89 = math.exp %88 : tensor<16xf32, #blocked>\n+      %90 = arith.subf %85, %87 : tensor<16xf32, #blocked>\n+      %91 = math.exp %90 : tensor<16xf32, #blocked>\n+      %92 = arith.mulf %89, %arg19 : tensor<16xf32, #blocked>\n+      %93 = arith.mulf %91, %77 : tensor<16xf32, #blocked>\n+      %94 = arith.addf %92, %93 : tensor<16xf32, #blocked>\n+      %95 = arith.divf %91, %94 : tensor<16xf32, #blocked>\n+      %96 = arith.divf %arg19, %94 : tensor<16xf32, #blocked>\n+      %97 = arith.mulf %96, %89 : tensor<16xf32, #blocked>\n+      %98 = triton_gpu.convert_layout %97 : (tensor<16xf32, #blocked>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %99 = tt.expand_dims %98 {axis = 1 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16x1xf32, #blocked2>\n+      %100 = triton_gpu.convert_layout %99 : (tensor<16x1xf32, #blocked2>) -> tensor<16x1xf32, #blocked1>\n+      %101 = tt.broadcast %100 : (tensor<16x1xf32, #blocked1>) -> tensor<16x16xf32, #blocked1>\n+      %102 = arith.mulf %arg18, %101 : tensor<16x16xf32, #blocked1>\n+      %103 = triton_gpu.convert_layout %95 : (tensor<16xf32, #blocked>) -> tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+      %104 = tt.expand_dims %103 {axis = 1 : i32} : (tensor<16xf32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16x1xf32, #blocked2>\n+      %105 = triton_gpu.convert_layout %104 : (tensor<16x1xf32, #blocked2>) -> tensor<16x1xf32, #blocked1>\n+      %106 = tt.broadcast %105 : (tensor<16x1xf32, #blocked1>) -> tensor<16x16xf32, #blocked1>\n+      %107 = arith.extf %69 : tensor<16x16xf16, #blocked1> to tensor<16x16xf32, #blocked1>\n+      %108 = arith.mulf %107, %106 : tensor<16x16xf32, #blocked1>\n+      %109 = arith.addf %102, %108 : tensor<16x16xf32, #blocked1>\n+      scf.yield %109, %94, %87 : tensor<16x16xf32, #blocked1>, tensor<16xf32, #blocked>, tensor<16xf32, #blocked>\n+    }\n+    %36 = arith.muli %2, %arg14 : i32\n+    %37 = arith.muli %3, %arg15 : i32\n+    %38 = arith.addi %36, %37 : i32\n+    %39 = triton_gpu.convert_layout %7 : (tensor<16xi32, #blocked>) -> tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %40 = tt.expand_dims %39 {axis = 1 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<16x1xi32, #blocked2>\n+    %41 = triton_gpu.convert_layout %40 : (tensor<16x1xi32, #blocked2>) -> tensor<16x1xi32, #blocked1>\n+    %42 = tt.splat %arg16 : (i32) -> tensor<16x1xi32, #blocked1>\n+    %43 = arith.muli %41, %42 : tensor<16x1xi32, #blocked1>\n+    %44 = triton_gpu.convert_layout %4 : (tensor<16xi32, #blocked>) -> tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %45 = tt.expand_dims %44 {axis = 0 : i32} : (tensor<16xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x16xi32, #blocked3>\n+    %46 = tt.broadcast %43 : (tensor<16x1xi32, #blocked1>) -> tensor<16x16xi32, #blocked1>\n+    %47 = tt.broadcast %45 : (tensor<1x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked3>\n+    %48 = triton_gpu.convert_layout %47 : (tensor<16x16xi32, #blocked3>) -> tensor<16x16xi32, #blocked1>\n+    %49 = arith.addi %46, %48 : tensor<16x16xi32, #blocked1>\n+    %50 = tt.splat %38 : (i32) -> tensor<16x16xi32, #blocked1>\n+    %51 = arith.addi %50, %49 : tensor<16x16xi32, #blocked1>\n+    %52 = tt.splat %arg13 : (!tt.ptr<f16>) -> tensor<16x16x!tt.ptr<f16>, #blocked1>\n+    %53 = tt.addptr %52, %51 : tensor<16x16x!tt.ptr<f16>, #blocked1>, tensor<16x16xi32, #blocked1>\n+    %54 = \"triton_gpu.cmpi\"(%41, %cst_3) <{predicate = 2 : i64}> : (tensor<16x1xi32, #blocked1>, tensor<16x1xi32, #blocked1>) -> tensor<16x1xi1, #blocked1>\n+    %55 = tt.broadcast %54 : (tensor<16x1xi1, #blocked1>) -> tensor<16x16xi1, #blocked1>\n+    %56 = arith.truncf %35#0 : tensor<16x16xf32, #blocked1> to tensor<16x16xf16, #blocked1>\n+    %57 = triton_gpu.convert_layout %53 : (tensor<16x16x!tt.ptr<f16>, #blocked1>) -> tensor<16x16x!tt.ptr<f16>, #blocked4>\n+    %58 = triton_gpu.convert_layout %56 : (tensor<16x16xf16, #blocked1>) -> tensor<16x16xf16, #blocked4>\n+    %59 = triton_gpu.convert_layout %55 : (tensor<16x16xi1, #blocked1>) -> tensor<16x16xi1, #blocked4>\n+    tt.store %57, %58, %59 {cache = 1 : i32, evict = 1 : i32} : tensor<16x16xf16, #blocked4>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+// Check if MoveConvertOutOfLoop hangs because of adding additional conversions\n+// CHECK-LABEL: loop_print\n+// CHECK-NOT: triton_gpu.convert_layout\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @loop_print(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}, %arg6: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %c32_i32 = arith.constant 32 : i32\n+    %c31_i32 = arith.constant 31 : i32\n+    %c0_i32 = arith.constant 0 : i32\n+    %c1_i32 = arith.constant 1 : i32\n+    %cst = arith.constant dense<32> : tensor<32x128xi32, #blocked>\n+    %cst_0 = arith.constant dense<32> : tensor<128x32xi32, #blocked1>\n+    %cst_1 = arith.constant 0.000000e+00 : f32\n+    %0 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #blocked2>\n+    %1 = triton_gpu.convert_layout %0 : (tensor<128xi32, #blocked2>) -> tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+    %2 = tt.expand_dims %1 {axis = 1 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<128x1xi32, #blocked1>\n+    %3 = tt.splat %arg6 : (i32) -> tensor<128x1xi32, #blocked1>\n+    %4 = arith.muli %2, %3 : tensor<128x1xi32, #blocked1>\n+    %5 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked2>\n+    %6 = triton_gpu.convert_layout %5 : (tensor<32xi32, #blocked2>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %7 = tt.expand_dims %6 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x32xi32, #blocked3>\n+    %8 = tt.broadcast %4 : (tensor<128x1xi32, #blocked1>) -> tensor<128x32xi32, #blocked1>\n+    %9 = tt.broadcast %7 : (tensor<1x32xi32, #blocked3>) -> tensor<128x32xi32, #blocked3>\n+    %10 = triton_gpu.convert_layout %9 : (tensor<128x32xi32, #blocked3>) -> tensor<128x32xi32, #blocked1>\n+    %11 = arith.addi %8, %10 : tensor<128x32xi32, #blocked1>\n+    %12 = triton_gpu.convert_layout %5 : (tensor<32xi32, #blocked2>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>\n+    %13 = tt.expand_dims %12 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<32x1xi32, #blocked1>\n+    %14 = triton_gpu.convert_layout %13 : (tensor<32x1xi32, #blocked1>) -> tensor<32x1xi32, #blocked>\n+    %15 = triton_gpu.convert_layout %0 : (tensor<128xi32, #blocked2>) -> tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %16 = tt.expand_dims %15 {axis = 0 : i32} : (tensor<128xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x128xi32, #blocked3>\n+    %17 = tt.broadcast %14 : (tensor<32x1xi32, #blocked>) -> tensor<32x128xi32, #blocked>\n+    %18 = tt.broadcast %16 : (tensor<1x128xi32, #blocked3>) -> tensor<32x128xi32, #blocked3>\n+    %19 = triton_gpu.convert_layout %18 : (tensor<32x128xi32, #blocked3>) -> tensor<32x128xi32, #blocked>\n+    %20 = arith.addi %17, %19 : tensor<32x128xi32, #blocked>\n+    %21 = arith.addi %arg5, %c31_i32 : i32\n+    %22 = arith.divsi %21, %c32_i32 : i32\n+    %23 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #blocked1>\n+    %24 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #blocked>\n+    %25:3 = scf.for %arg7 = %c0_i32 to %22 step %c1_i32 iter_args(%arg8 = %cst_1, %arg9 = %11, %arg10 = %20) -> (f32, tensor<128x32xi32, #blocked1>, tensor<32x128xi32, #blocked>)  : i32 {\n+      tt.print \"a_offsets: \" : %arg9 : tensor<128x32xi32, #blocked1>\n+      %27 = tt.addptr %23, %arg9 : tensor<128x32x!tt.ptr<f16>, #blocked1>, tensor<128x32xi32, #blocked1>\n+      %28 = triton_gpu.convert_layout %27 : (tensor<128x32x!tt.ptr<f16>, #blocked1>) -> tensor<128x32x!tt.ptr<f16>, #blocked4>\n+      %29 = tt.load %28 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #blocked4>\n+      %30 = triton_gpu.convert_layout %29 : (tensor<128x32xf16, #blocked4>) -> tensor<128x32xf16, #blocked1>\n+      %31 = tt.addptr %24, %arg10 : tensor<32x128x!tt.ptr<f16>, #blocked>, tensor<32x128xi32, #blocked>\n+      %32 = triton_gpu.convert_layout %31 : (tensor<32x128x!tt.ptr<f16>, #blocked>) -> tensor<32x128x!tt.ptr<f16>, #blocked5>\n+      %33 = tt.load %32 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #blocked5>\n+      %34 = triton_gpu.convert_layout %33 : (tensor<32x128xf16, #blocked5>) -> tensor<32x128xf16, #blocked>\n+      %35 = \"tt.reduce\"(%30) <{axis = 0 : i32}> ({\n+      ^bb0(%arg11: f16, %arg12: f16):\n+        %46 = arith.addf %arg11, %arg12 : f16\n+        tt.reduce.return %46 : f16\n+      }) : (tensor<128x32xf16, #blocked1>) -> tensor<32xf16, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>\n+      %36 = triton_gpu.convert_layout %35 : (tensor<32xf16, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<32xf16, #blocked2>\n+      %37 = \"tt.reduce\"(%36) <{axis = 0 : i32}> ({\n+      ^bb0(%arg11: f16, %arg12: f16):\n+        %46 = arith.addf %arg11, %arg12 : f16\n+        tt.reduce.return %46 : f16\n+      }) : (tensor<32xf16, #blocked2>) -> f16\n+      %38 = \"tt.reduce\"(%34) <{axis = 0 : i32}> ({\n+      ^bb0(%arg11: f16, %arg12: f16):\n+        %46 = arith.addf %arg11, %arg12 : f16\n+        tt.reduce.return %46 : f16\n+      }) : (tensor<32x128xf16, #blocked>) -> tensor<128xf16, #triton_gpu.slice<{dim = 0, parent = #blocked}>>\n+      %39 = triton_gpu.convert_layout %38 : (tensor<128xf16, #triton_gpu.slice<{dim = 0, parent = #blocked}>>) -> tensor<128xf16, #blocked2>\n+      %40 = \"tt.reduce\"(%39) <{axis = 0 : i32}> ({\n+      ^bb0(%arg11: f16, %arg12: f16):\n+        %46 = arith.addf %arg11, %arg12 : f16\n+        tt.reduce.return %46 : f16\n+      }) : (tensor<128xf16, #blocked2>) -> f16\n+      %41 = arith.addf %37, %40 : f16\n+      %42 = arith.extf %41 : f16 to f32\n+      %43 = arith.addf %arg8, %42 : f32\n+      %44 = arith.addi %arg9, %cst_0 : tensor<128x32xi32, #blocked1>\n+      %45 = arith.addi %arg10, %cst : tensor<32x128xi32, #blocked>\n+      scf.yield %43, %44, %45 : f32, tensor<128x32xi32, #blocked1>, tensor<32x128xi32, #blocked>\n+    }\n+    %26 = arith.truncf %25#0 : f32 to f16\n+    tt.store %arg2, %26 {cache = 1 : i32, evict = 1 : i32} : f16\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+// Check if SimplifyReduceCvt handles the cvt,reduce->reduce,cvt conversion but not the general push forward conversion\n+// CHECK-LABEL: reduce_cvt3\n+// CHECK: tt.dot\n+// CHECK-NEXT: tt.reduce\n+// CHECK: triton_gpu.convert_layout\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked1 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n+#blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n+#blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blocked5 = #triton_gpu.blocked<{sizePerThread = [2, 2], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [0, 1]}>\n+#shared1 = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @reduce_cvt3(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i32> {tt.divisibility = 16 : i32}) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #blocked>\n+    %cst_0 = arith.constant dense<32> : tensor<32x1xi32, #blocked>\n+    %0 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked1>\n+    %1 = triton_gpu.convert_layout %0 : (tensor<32xi32, #blocked1>) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>\n+    %2 = tt.expand_dims %1 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked2}>>) -> tensor<32x1xi32, #blocked2>\n+    %3 = triton_gpu.convert_layout %2 : (tensor<32x1xi32, #blocked2>) -> tensor<32x1xi32, #blocked>\n+    %4 = arith.muli %3, %cst_0 : tensor<32x1xi32, #blocked>\n+    %5 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<32x1x!tt.ptr<f16>, #blocked>\n+    %6 = tt.addptr %5, %4 : tensor<32x1x!tt.ptr<f16>, #blocked>, tensor<32x1xi32, #blocked>\n+    %7 = triton_gpu.convert_layout %0 : (tensor<32xi32, #blocked1>) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>\n+    %8 = tt.expand_dims %7 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #blocked3}>>) -> tensor<1x32xi32, #blocked3>\n+    %9 = tt.broadcast %6 : (tensor<32x1x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked>\n+    %10 = tt.broadcast %8 : (tensor<1x32xi32, #blocked3>) -> tensor<32x32xi32, #blocked3>\n+    %11 = triton_gpu.convert_layout %10 : (tensor<32x32xi32, #blocked3>) -> tensor<32x32xi32, #blocked>\n+    %12 = tt.addptr %9, %11 : tensor<32x32x!tt.ptr<f16>, #blocked>, tensor<32x32xi32, #blocked>\n+    %13 = tt.splat %arg1 : (!tt.ptr<f16>) -> tensor<32x1x!tt.ptr<f16>, #blocked>\n+    %14 = tt.addptr %13, %4 : tensor<32x1x!tt.ptr<f16>, #blocked>, tensor<32x1xi32, #blocked>\n+    %15 = tt.broadcast %14 : (tensor<32x1x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked>\n+    %16 = tt.addptr %15, %11 : tensor<32x32x!tt.ptr<f16>, #blocked>, tensor<32x32xi32, #blocked>\n+    %17 = triton_gpu.convert_layout %12 : (tensor<32x32x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked4>\n+    %18 = tt.load %17 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf16, #blocked4>\n+    %19 = triton_gpu.convert_layout %18 : (tensor<32x32xf16, #blocked4>) -> tensor<32x32xf16, #blocked>\n+    %20 = triton_gpu.convert_layout %16 : (tensor<32x32x!tt.ptr<f16>, #blocked>) -> tensor<32x32x!tt.ptr<f16>, #blocked4>\n+    %21 = tt.load %20 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf16, #blocked4>\n+    %22 = triton_gpu.convert_layout %21 : (tensor<32x32xf16, #blocked4>) -> tensor<32x32xf16, #blocked>\n+    %23 = triton_gpu.convert_layout %22 : (tensor<32x32xf16, #blocked>) -> tensor<32x32xf16, #shared>\n+    %24 = tt.trans %23 : (tensor<32x32xf16, #shared>) -> tensor<32x32xf16, #shared1>\n+    %25 = triton_gpu.convert_layout %24 : (tensor<32x32xf16, #shared1>) -> tensor<32x32xf16, #blocked>\n+    %26 = triton_gpu.convert_layout %19 : (tensor<32x32xf16, #blocked>) -> tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked5}>>\n+    %27 = triton_gpu.convert_layout %25 : (tensor<32x32xf16, #blocked>) -> tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked5}>>\n+    %28 = triton_gpu.convert_layout %cst : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #blocked5>\n+    %29 = tt.dot %26, %27, %28 {allowTF32 = true} : tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #blocked5}>> * tensor<32x32xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #blocked5}>> -> tensor<32x32xf32, #blocked5>\n+    %30 = triton_gpu.convert_layout %29 : (tensor<32x32xf32, #blocked5>) -> tensor<32x32xf32, #blocked>\n+    %31:2 = \"tt.reduce\"(%30, %11) <{axis = 1 : i32}> ({\n+    ^bb0(%arg3: f32, %arg4: i32, %arg5: f32, %arg6: i32):\n+      %37 = \"triton_gpu.cmpf\"(%arg3, %arg5) <{predicate = 1 : i64}> : (f32, f32) -> i1\n+      %38 = \"triton_gpu.cmpi\"(%arg4, %arg6) <{predicate = 2 : i64}> : (i32, i32) -> i1\n+      %39 = arith.andi %37, %38 : i1\n+      %40 = \"triton_gpu.cmpf\"(%arg3, %arg5) <{predicate = 2 : i64}> : (f32, f32) -> i1\n+      %41 = arith.ori %40, %39 : i1\n+      %42 = arith.select %41, %arg3, %arg5 : f32\n+      %43 = arith.select %41, %arg4, %arg6 : i32\n+      tt.reduce.return %42, %43 : f32, i32\n+    }) : (tensor<32x32xf32, #blocked>, tensor<32x32xi32, #blocked>) -> (tensor<32xf32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>, tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>)\n+    %32 = triton_gpu.convert_layout %31#1 : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked}>>) -> tensor<32xi32, #blocked1>\n+    %33 = tt.splat %arg2 : (!tt.ptr<i32>) -> tensor<32x!tt.ptr<i32>, #blocked1>\n+    %34 = tt.addptr %33, %0 : tensor<32x!tt.ptr<i32>, #blocked1>, tensor<32xi32, #blocked1>\n+    %35 = triton_gpu.convert_layout %34 : (tensor<32x!tt.ptr<i32>, #blocked1>) -> tensor<32x!tt.ptr<i32>, #blocked1>\n+    %36 = triton_gpu.convert_layout %32 : (tensor<32xi32, #blocked1>) -> tensor<32xi32, #blocked1>\n+    tt.store %35, %36 {cache = 1 : i32, evict = 1 : i32} : tensor<32xi32, #blocked1>\n+    tt.return\n+  }\n+}"}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 101, "deletions": 4, "changes": 105, "file_content_changes": "@@ -15,10 +15,12 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // CHECK: tt.func @push_elementwise1\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n-// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n-// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]]\n-// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n-// CHECK: %[[C:.*]] = tt.dot %[[AF16]]\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %{{.*}} : {{.*}} tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>>\n+// CHECK: %[[C:.*]] = tt.dot %[[AF16]], %[[BCVT]]\n+// CHECK-SAME: tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<16x16xf32, #mma>\n // CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n tt.func @push_elementwise1(\n                    %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n@@ -125,4 +127,99 @@ tt.func @push_elementwise5(\n   tt.return %newc : tensor<16x16xf32, #Cv1>\n }\n \n+// CHECK: tt.func @succeeds_if_arg_is_not_convert_layout\n+// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n+// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]]\n+// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n+// CHECK: %[[C:.*]] = tt.dot %[[AF16]]\n+// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n+tt.func @succeeds_if_arg_is_not_convert_layout(\n+                   %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n+  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n+  %dotai8 = triton_gpu.convert_layout %ai8 : (tensor<16x16xi8, #ALR>) -> tensor<16x16xi8, #Av2>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n+  %dotaf8 = tt.bitcast %dotai8 : tensor<16x16xi8, #Av2> -> tensor<16x16xf8E5M2, #Av2>\n+  %dota = tt.fp_to_fp %dotaf8 : tensor<16x16xf8E5M2, #Av2> -> tensor<16x16xf16, #Av2>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n+  tt.return %newc : tensor<16x16xf32, #Cv2>\n+}\n+\n+}\n+\n+// -----\n+\n+#blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+// CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+// CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+// CHECK: #[[MMA:.*]] = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+\n+// CHECK: tt.func @push_convert_both_operands\n+// CHECK: %[[ALOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BA]]>\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] : (tensor<16x16xf16, #[[BA]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BLOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BB]]>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %[[BLOAD]] : (tensor<16x16xf16, #[[BB]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[AEXT:.*]] = arith.extf %[[ACVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BEXT:.*]] = arith.extf %[[BCVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: tt.dot %[[AEXT]], %[[BEXT]], %{{.*}} {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> -> tensor<16x16xf32, #mma>\n+tt.func @push_convert_both_operands(\n+                   %pa: tensor<16x16x!tt.ptr<f16>, #blockedA> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #blockedB> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #mma>) -> tensor<16x16xf32, #mma>{\n+  %a = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedA>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedB>\n+  %ae = arith.extf %a : tensor<16x16xf16, #blockedA> to tensor<16x16xf32, #blockedA>\n+  %be = arith.extf %b : tensor<16x16xf16, #blockedB> to tensor<16x16xf32, #blockedB>\n+  %al = triton_gpu.convert_layout %ae : (tensor<16x16xf32, #blockedA>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+  %bl = triton_gpu.convert_layout %be : (tensor<16x16xf32, #blockedB>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+  %r = tt.dot %al, %bl, %c {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<16x16xf32, #mma>\n+  tt.return %r : tensor<16x16xf32, #mma>\n+}\n+\n+}\n+\n+// -----\n+\n+#blockedA = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+#blockedB = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n+// CHECK: #[[BA:.*]] = #triton_gpu.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>\n+// CHECK: #[[BB:.*]] = #triton_gpu.blocked<{sizePerThread = [2, 1], threadsPerWarp = [8, 4], warpsPerCTA = [1, 4], order = [0, 1]}>\n+// CHECK: #[[MMA:.*]] = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [1, 4]}>\n+\n+// CHECK: tt.func @update_kwidth_slice\n+// CHECK: %[[CST:.+]] = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[ALOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BA]]>\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] : (tensor<16x16xf16, #[[BA]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BLOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BB]]>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %[[BLOAD]] : (tensor<16x16xf16, #[[BB]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[AEXT:.*]] = arith.extf %[[ACVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BEXT:.*]] = arith.extf %[[BCVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[ADD:.+]] = arith.addf %[[BEXT]], %[[CST]] : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: tt.dot %[[AEXT]], %[[ADD]], %{{.*}} {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> -> tensor<16x16xf32, #mma>\n+tt.func @update_kwidth_slice(\n+                   %pa: tensor<16x16x!tt.ptr<f16>, #blockedA> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %pb: tensor<16x16x!tt.ptr<f16>, #blockedB> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n+                   %c: tensor<16x16xf32, #mma>) -> tensor<16x16xf32, #mma>{\n+  %cst = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blockedB>\n+  %a = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedA>\n+  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedB>\n+  %ae = arith.extf %a : tensor<16x16xf16, #blockedA> to tensor<16x16xf32, #blockedA>\n+  %be = arith.extf %b : tensor<16x16xf16, #blockedB> to tensor<16x16xf32, #blockedB>\n+  %add = arith.addf %be, %cst : tensor<16x16xf32, #blockedB>\n+  %al = triton_gpu.convert_layout %ae : (tensor<16x16xf32, #blockedA>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+  %bl = triton_gpu.convert_layout %add : (tensor<16x16xf32, #blockedB>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+  %r = tt.dot %al, %bl, %c {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<16x16xf32, #mma>\n+  tt.return %r : tensor<16x16xf32, #mma>\n+}\n+\n }"}, {"filename": "test/TritonGPU/loop-pipeline.mlir", "status": "modified", "additions": 65, "deletions": 0, "changes": 65, "file_content_changes": "@@ -432,3 +432,68 @@ tt.func @cross_iter_dep(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32},\n   }\n   tt.return %119#0 : tensor<32x32xf32, #C>\n }\n+\n+// CHECK: tt.func @dep_arg_two_uses\n+// CHECK: tt.expand_dims\n+// CHECK: tt.expand_dims\n+// CHECK: tt.expand_dims %arg5\n+// CHECK-NEXT: tt.expand_dims %arg5\n+// CHECK: %[[PTR0:.*]] = tt.splat %arg6\n+// CHECK: %[[PTR1:.*]] = tt.addptr %[[PTR0]]\n+// CHECK-NEXT: tt.load %[[PTR1]]\n+tt.func @dep_arg_two_uses(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32},\n+                          %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32},\n+                          %arg2: !tt.ptr<f16> {tt.divisibility = 16 : i32}) -> tensor<128x128xf32, #C> {\n+  %23 = arith.constant 100 : index\n+  %c64 = arith.constant 64 : i64\n+  %56 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+  %57 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+  %58 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #BL}>>\n+  %83 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+  %85 = tt.splat %c64 : (i64) -> tensor<1x32xi64, #AL>\n+  %86 = tt.splat %c64 : (i64) -> tensor<1x32xi64, #AL>\n+  %68 = tt.splat %arg0 : (!tt.ptr<f16>) -> tensor<128x32x!tt.ptr<f16>, #AL>\n+  %c32_index = arith.constant 32 : index\n+  %c32_i32 = arith.index_cast %c32_index : index to i32\n+  %80 = tt.splat %arg2 : (!tt.ptr<f16>) -> tensor<32x128x!tt.ptr<f16>, #BL>\n+  %cst_6 = arith.constant dense<0.000000e+00> : tensor<32x128xf32, #BL>\n+  %88 = arith.truncf %cst_6 : tensor<32x128xf32, #BL> to tensor<32x128xf16, #BL>\n+  %cst = arith.constant dense<0.000000e+00> : tensor<128x128xf32, #C>\n+  %90 = tt.splat %c64 : (i64) -> tensor<32x128xi64, #BL>\n+  %92 = tt.addptr %arg1, %c32_i32 : !tt.ptr<i32>, i32\n+  %c0_index = arith.constant 0 : index\n+  %91:5 = scf.for %arg19 = %c0_index to %23 step %c32_index iter_args(%arg20 = %68, %arg21 = %83, %arg22 = %92, %arg23 = %cst, %arg24 = %80) -> (tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>, !tt.ptr<i32>, tensor<128x128xf32, #C>, tensor<32x128x!tt.ptr<f16>, #BL>)   {\n+    %1750 = arith.subi %23, %arg19 : index\n+    %175 = arith.index_cast %1750 : index to i32\n+    %176 = tt.splat %175 : (i32) -> tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+    %177 = tt.splat %175 : (i32) -> tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #BL}>>\n+    %178 = \"triton_gpu.cmpi\"(%57, %176) <{predicate = 2 : i64}> : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>, tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>) -> tensor<32xi1, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+    %179 = \"triton_gpu.cmpi\"(%58, %177) <{predicate = 2 : i64}> : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #BL}>>, tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #BL}>>) -> tensor<32xi1, #triton_gpu.slice<{dim = 1, parent = #BL}>>\n+    %180 = tt.expand_dims %178 {axis = 0 : i32} : (tensor<32xi1, #triton_gpu.slice<{dim = 0, parent = #AL}>>) -> tensor<1x32xi1, #AL>\n+    %181 = tt.expand_dims %179 {axis = 1 : i32} : (tensor<32xi1, #triton_gpu.slice<{dim = 1, parent = #BL}>>) -> tensor<32x1xi1, #BL>\n+    %182 = tt.expand_dims %arg21 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>) -> tensor<1x32xi32, #AL>\n+    %183 = tt.expand_dims %arg21 {axis = 0 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>) -> tensor<1x32xi32, #AL>\n+    %184 = arith.extsi %182 : tensor<1x32xi32, #AL> to tensor<1x32xi64, #AL>\n+    %185 = arith.extsi %183 : tensor<1x32xi32, #AL> to tensor<1x32xi64, #AL>\n+    %186 = arith.muli %184, %85 : tensor<1x32xi64, #AL>\n+    %187 = arith.muli %185, %86 : tensor<1x32xi64, #AL>\n+    %188 = tt.broadcast %186 : (tensor<1x32xi64, #AL>) -> tensor<128x32xi64, #AL>\n+    %189 = tt.broadcast %187 : (tensor<1x32xi64, #AL>) -> tensor<128x32xi64, #AL>\n+    %190 = tt.addptr %arg20, %188 : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi64, #AL>\n+    %191 = tt.addptr %arg20, %189 : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<128x32xi64, #AL>\n+    %192 = tt.broadcast %180 : (tensor<1x32xi1, #AL>) -> tensor<128x32xi1, #AL>\n+    %193 = tt.load %191, %192 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<128x32xf16, #AL>\n+    %194 = tt.splat %arg22 : (!tt.ptr<i32>) -> tensor<32x!tt.ptr<i32>, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+    %195 = tt.addptr %194, %56 : tensor<32x!tt.ptr<i32>, #triton_gpu.slice<{dim = 0, parent = #AL}>>, tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+    %196 = tt.load %195 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>\n+    %197 = tt.addptr %arg22, %c32_i32 : !tt.ptr<i32>, i32\n+    %198 = tt.broadcast %181 : (tensor<32x1xi1, #BL>) -> tensor<32x128xi1, #BL>\n+    %199 = tt.load %arg24, %198, %88 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x128xf16, #BL>\n+    %200 = triton_gpu.convert_layout %193 : (tensor<128x32xf16, #AL>) -> tensor<128x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 2}>>\n+    %201 = triton_gpu.convert_layout %199 : (tensor<32x128xf16, #BL>) -> tensor<32x128xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 2}>>\n+    %202 = tt.dot %200, %201, %arg23 {allowTF32 = true} : tensor<128x32xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #C, kWidth = 2}>> * tensor<32x128xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #C, kWidth = 2}>> -> tensor<128x128xf32, #C>\n+    %203 = tt.addptr %arg24, %90 : tensor<32x128x!tt.ptr<f16>, #BL>, tensor<32x128xi64, #BL>\n+    scf.yield %190, %196, %197, %202, %203 : tensor<128x32x!tt.ptr<f16>, #AL>, tensor<32xi32, #triton_gpu.slice<{dim = 0, parent = #AL}>>, !tt.ptr<i32>, tensor<128x128xf32, #C>, tensor<32x128x!tt.ptr<f16>, #BL>\n+  }\n+  tt.return %91#3 : tensor<128x128xf32, #C>\n+}"}, {"filename": "test/TritonGPU/reorder-instructions.mlir", "status": "added", "additions": 49, "deletions": 0, "changes": 49, "file_content_changes": "@@ -0,0 +1,49 @@\n+// RUN: triton-opt %s -split-input-file -tritongpu-reorder-instructions | FileCheck %s\n+\n+// check that we don't hoist convert_layout above its operand definition.\n+// CHECK-LABEL: convert_cannot_hoist\n+//       CHECK:   %[[CVTS:.+]] = triton_gpu.convert_layout\n+//       CHECK:   triton_gpu.convert_layout %[[CVTS]]\n+//       CHECK:   tt.dot\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [0, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @convert_cannot_hoist(%arg0: tensor<32x32x!tt.ptr<f32>, #blocked>) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+    %cst_0 = arith.constant dense<1.230000e+02> : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+    %9 = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %10 = triton_gpu.convert_layout %9 : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n+    %11 = triton_gpu.convert_layout %10 : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+    %12 = tt.dot %11, %cst_0, %cst {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<32x32xf32, #mma>\n+    %13 = triton_gpu.convert_layout %12 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    tt.store %arg0, %13 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf32, #blocked>\n+    tt.return\n+  }\n+}\n+\n+// -----\n+\n+// CHECK-LABEL: sink_convert_idx_1\n+//       CHECK: triton_gpu.convert_layout %{{.*}} : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+//       CHECK: triton_gpu.convert_layout %{{.*}} : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+//       CHECK: tt.dot\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>\n+#mma = #triton_gpu.mma<{versionMajor = 2, versionMinor = 0, warpsPerCTA = [2, 2]}>\n+#shared = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [0, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n+  tt.func public @sink_convert_idx_1(%arg0: tensor<32x32x!tt.ptr<f32>, #blocked>) attributes {noinline = false} {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x32xf32, #mma>\n+    %B = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %BS = triton_gpu.convert_layout %B : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n+    %BD = triton_gpu.convert_layout %BS : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+    %cst_0 = arith.constant dense<1.230000e+02> : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n+    %A = tt.load %arg0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x32xf32, #blocked>\n+    %AS = triton_gpu.convert_layout %A : (tensor<32x32xf32, #blocked>) -> tensor<32x32xf32, #shared>\n+    %AD = triton_gpu.convert_layout %AS : (tensor<32x32xf32, #shared>) -> tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n+    %12 = tt.dot %AD, %BD, %cst {allowTF32 = true} : tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<32x32xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<32x32xf32, #mma>\n+    %13 = triton_gpu.convert_layout %12 : (tensor<32x32xf32, #mma>) -> tensor<32x32xf32, #blocked>\n+    tt.store %arg0, %13 {cache = 1 : i32, evict = 1 : i32} : tensor<32x32xf32, #blocked>\n+    tt.return\n+  }\n+}"}]