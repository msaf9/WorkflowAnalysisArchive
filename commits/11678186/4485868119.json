[{"filename": ".pre-commit-config.yaml", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "file_content_changes": "@@ -0,0 +1,43 @@\n+repos:\n+  - repo: local\n+    hooks:\n+      - id: isort\n+        name: isort\n+        entry: isort\n+        args: [\"./python\"]\n+        language: python\n+        types: [python]\n+        additional_dependencies:\n+          - isort==5.10.1\n+\n+      - id: autopep8\n+        name: autopep8\n+        entry: autopep8\n+        args: [\"-a\", \"-r\", \"-i\", \"./python\"]\n+        language: python\n+        types: [python]\n+        additional_dependencies:\n+          - autopep8==1.6.0\n+\n+      - id: clang-format\n+        name: clang-format\n+        entry: python -m clang_format\n+        args:\n+          [\n+            \"-regex\",\n+            \".*\\\\.(cpp|hpp|h|cc)\",\n+            \"-not\",\n+            \"-path\",\n+            \"./python/triton/*\",\n+            \"-not\",\n+            \"-path\",\n+            \"./python/build/*\",\n+            \"-not\",\n+            \"-path\",\n+            \"./include/triton/external/*\",\n+            \"-i\",\n+          ]\n+        language: python\n+        types: [c, c++]\n+        additional_dependencies:\n+          - clang-format==14.0.6"}, {"filename": "CONTRIBUTING.md", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "file_content_changes": "@@ -0,0 +1,53 @@\n+# Triton Programming Language Contributor's Guide\n+\n+First of all, thank you for considering contributing to the Triton programming language! We appreciate the time and effort you're willing to put into improving and expanding our project. In order to maintain a high standard of code and a welcoming atmosphere for collaboration, we kindly ask you to follow the guidelines outlined below.\n+\n+## General Guidelines\n+\n+1. **Quality Contributions:** We value meaningful contributions that aim to improve the project and help it grow. Please refrain from submitting low-effort pull requests (PR) -- such as minor formatting/typo fixes -- solely for the purpose of appearing in the commit history. Maintainers have limited bandwidth, and may decline to review such work.\n+\n+2. **Code Formatting:** Our Continuous Integration (CI) pipeline uses autopep8, isort, and clang-format to check code formatting. To avoid failing the CI workflow due to formatting issues, please utilize the provided `.pre-commit-config.yaml` pre-commit configuration file.\n+\n+3. **Unit Testing:** When contributing new functionalities, please also include appropriate tests. We aim to continuously improve and expand our CI pipeline to ensure the robustness and reliability of the project. PRs that add a large amount of untested code will be rejected. \n+\n+4. **Respectful Communication:** In all discussions related to PRs or other contributions, please maintain a courteous and civil tone. We strive to foster a collaborative environment that is inclusive and respectful to all contributors.\n+\n+\n+## Request for Comments (RFCs)\n+\n+RFCs are a crucial aspect of the collaborative development process, as they provide a structured way to propose and discuss significant changes or additions to the project. RFCs may encompass modifications to the language itself, extensive changes in the compiler backend, or other substantial updates that impact the Triton ecosystem.\n+\n+To ensure that RFCs are clear and easy to understand, consider the following guidelines when creating one:\n+\n+### Purpose\n+\n+The purpose of an RFC is to:\n+\n+- Clearly communicate your proposal to the Triton community\n+- Collect feedback from maintainers and other contributors\n+- Provide a platform for discussing and refining ideas\n+- Reach a consensus on the best approach for implementing the proposed changes\n+\n+### Structure\n+\n+A well-structured RFC should include:\n+\n+1. **Title:** A concise and descriptive title that reflects the main topic of the proposal.\n+\n+2. **Summary:** A brief overview of the proposed changes, including the motivation behind them and their intended impact on the project.\n+\n+3. **Detailed Design:** A thorough description of the proposed changes, including:\n+   - Technical details and implementation approach\n+   - Any new or modified components, functions, or data structures\n+   - Any potential challenges or limitations, as well as proposed solutions\n+\n+4. **Examples and Use Cases:** Provide examples of how the proposed changes would be used in real-world scenarios, as well as any use cases that demonstrate the benefits of the changes.\n+\n+5. **Performance Impact:** Discuss the expected performance impact of the proposed changes, including any potential bottlenecks or performance improvements.\n+\n+6. **Timeline and Milestones:** Outline a proposed timeline for implementing the changes, including any milestones or intermediate steps.\n+\n+\n+## New backends\n+\n+Due to limited resources, we need to prioritize the number of targets we support. We are committed to providing upstream support for Nvidia and AMD GPUs. However, if you wish to contribute support for other backends, please start your project in a fork. If your backend proves to be useful and meets our performance requirements, we will discuss the possibility of upstreaming it.\n\\ No newline at end of file"}, {"filename": "README.md", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "file_content_changes": "@@ -52,11 +52,9 @@ Version 2.0 is out! New features include:\n \n # Contributing\n \n-Community contributions are more than welcome, whether it be to fix bugs or to add new features. Feel free to open GitHub issues about your contribution ideas, and we will review them. Please do not submit PRs that simply fix simple typos in our documentation -- unless they can lead to confusion.\n+Community contributions are more than welcome, whether it be to fix bugs or to add new features. For more detailed instructions, please visit our [contributor's guide](CONTRIBUTING.md).\n \n-Note 1: A more detailed contributor's guide containing general guidelines is coming soon!\n-\n-Note 2: If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n+If you\u2019re interested in joining our team and working on Triton & GPU kernels, [we\u2019re hiring](https://openai.com/jobs/#acceleration)!\n \n # Compatibility\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -514,6 +514,10 @@ section 9.7.13.4.1 for more details.\n     SmallVector<int> getMMAv1ShapePerWarp() const;\n     int getMMAv1Vec() const;\n     int getMMAv1NumOuter(ArrayRef<int64_t> shape) const;\n+    //\n+    SmallVector<int64_t> getMMAv2Rep(ArrayRef<int64_t> shape,\n+                                     int bitwidth) const;\n+\n   }];\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/CMakeLists.txt", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "file_content_changes": "@@ -1,5 +1,11 @@\n add_mlir_conversion_library(TritonGPUToLLVM\n+    ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp\n+    ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp\n     ConvertLayoutOpToLLVM.cpp\n+    DotOpToLLVM/FMA.cpp\n+    DotOpToLLVM/MMAv1.cpp\n+    DotOpToLLVM/MMAv2.cpp\n     DotOpToLLVM.cpp\n     ElementwiseOpToLLVM.cpp\n     LoadStoreOpToLLVM.cpp\n@@ -10,7 +16,6 @@ add_mlir_conversion_library(TritonGPUToLLVM\n     Utility.cpp\n     TypeConverter.cpp\n     ViewOpToLLVM.cpp\n-    DotOpHelpers.cpp\n \n     ADDITIONAL_HEADER_DIRS\n     ${PROJECT_SOURCE_DIR}/include/triton/Conversion/TritonGPUToLLVM"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 49, "deletions": 38, "changes": 87, "file_content_changes": "@@ -1,12 +1,8 @@\n #include \"ConvertLayoutOpToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStridesFromShapeAndOrder;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getContigPerThread;\n using ::mlir::triton::gpu::getElemsPerThread;\n@@ -16,6 +12,41 @@ using ::mlir::triton::gpu::getSizePerThread;\n using ::mlir::triton::gpu::isaDistributedLayout;\n using ::mlir::triton::gpu::SharedEncodingAttr;\n \n+// Forward declarations\n+\n+namespace SharedToDotOperandMMAv1 {\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+SmallVector<CoordTy> getMNCoords(Value thread,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ArrayRef<unsigned int> wpt,\n+                                 const MmaEncodingAttr &mmaLayout,\n+                                 ArrayRef<int64_t> shape, bool isARow,\n+                                 bool isBRow, bool isAVec4, bool isBVec4);\n+\n+Value convertLayout(int opIdx, Value tensor, const SharedMemoryObject &smemObj,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter, Type resultTy);\n+\n+} // namespace SharedToDotOperandMMAv1\n+\n+namespace SharedToDotOperandMMAv2 {\n+Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,\n+                    Location loc, Value tensor,\n+                    DotOperandEncodingAttr bEncoding,\n+                    const SharedMemoryObject &smemObj,\n+                    TritonGPUToLLVMTypeConverter *typeConverter, Value thread);\n+}\n+\n+namespace SharedToDotOperandFMA {\n+Value convertLayout(int opIdx, Value B, Value llB, BlockedEncodingAttr dLayout,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter);\n+}\n+\n struct ConvertLayoutOpConversion\n     : public ConvertTritonGPUOpToLLVMPattern<triton::gpu::ConvertLayoutOp> {\n public:\n@@ -139,10 +170,10 @@ struct ConvertLayoutOpConversion\n       } else if (mmaLayout.isVolta()) {\n         auto [isARow, isBRow, isAVec4, isBVec4, _] =\n             mmaLayout.decodeVoltaLayoutStates();\n-        auto coords = DotOpMmaV1ConversionHelper::getMNCoords(\n+        auto coords = SharedToDotOperandMMAv1::getMNCoords(\n             threadId, rewriter, mmaLayout.getWarpsPerCTA(), mmaLayout, shape,\n             isARow, isBRow, isAVec4, isBVec4);\n-        return DotOpMmaV1ConversionHelper::getCoord(elemId, coords);\n+        return coords[elemId];\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n@@ -518,15 +549,10 @@ struct ConvertLayoutOpConversion\n                        .dyn_cast_or_null<BlockedEncodingAttr>()) {\n       auto dotOpLayout =\n           dstTensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-      DotOpFMAConversionHelper helper(blockedLayout);\n       auto thread = getThreadId(rewriter, loc);\n-      if (dotOpLayout.getOpIdx() == 0) { // $a\n-        res = helper.loadA(src, adaptor.getSrc(), blockedLayout, thread, loc,\n-                           getTypeConverter(), rewriter);\n-      } else { // $b\n-        res = helper.loadB(src, adaptor.getSrc(), blockedLayout, thread, loc,\n-                           getTypeConverter(), rewriter);\n-      }\n+      res = SharedToDotOperandFMA::convertLayout(\n+          dotOpLayout.getOpIdx(), src, adaptor.getSrc(), blockedLayout, thread,\n+          loc, getTypeConverter(), rewriter);\n     } else {\n       assert(false && \"Unsupported dot operand layout found\");\n     }\n@@ -622,19 +648,12 @@ struct ConvertLayoutOpConversion\n     Value res;\n \n     if (!isOuter && mmaLayout.isAmpere() && isHMMA) { // tensor core v2\n-      MMA16816ConversionHelper mmaHelper(src.getType(), mmaLayout,\n-                                         getThreadId(rewriter, loc), rewriter,\n-                                         getTypeConverter(), op.getLoc());\n-\n-      if (dotOperandLayout.getOpIdx() == 0) {\n-        // operand $a\n-        res = mmaHelper.loadA(src, smemObj);\n-      } else if (dotOperandLayout.getOpIdx() == 1) {\n-        // operand $b\n-        res = mmaHelper.loadB(src, smemObj);\n-      }\n+\n+      res = SharedToDotOperandMMAv2::convertLayout(\n+          dotOperandLayout.getOpIdx(), rewriter, loc, src, dotOperandLayout,\n+          smemObj, getTypeConverter(), tid_val());\n+\n     } else if (!isOuter && mmaLayout.isVolta() && isHMMA) { // tensor core v1\n-      DotOpMmaV1ConversionHelper helper(mmaLayout);\n       bool isMMAv1Row = dotOperandLayout.getMMAv1IsRow();\n       auto srcSharedLayout = src.getType()\n                                  .cast<RankedTensorType>()\n@@ -648,23 +667,15 @@ struct ConvertLayoutOpConversion\n         return Value();\n       }\n \n-      if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n-        // TODO[Superjomn]: transA is not available here.\n-        bool transA = false;\n-        res = helper.loadA(src, smemObj, getThreadId(rewriter, loc), loc,\n-                           getTypeConverter(), rewriter, dst.getType());\n-      } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n-        // TODO[Superjomn]: transB is not available here.\n-        bool transB = false;\n-        res = helper.loadB(src, smemObj, getThreadId(rewriter, loc), loc,\n-                           getTypeConverter(), rewriter, dst.getType());\n-      }\n+      res = SharedToDotOperandMMAv1::convertLayout(\n+          dotOperandLayout.getOpIdx(), src, smemObj, getThreadId(rewriter, loc),\n+          loc, getTypeConverter(), rewriter, dst.getType());\n     } else {\n       assert(false && \"Unsupported mma layout found\");\n     }\n     return res;\n   }\n-};\n+}; // namespace triton::gpu::ConvertLayoutOp>\n \n void populateConvertLayoutOpToLLVMPatterns(\n     TritonGPUToLLVMTypeConverter &typeConverter, RewritePatternSet &patterns,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandFMA.cpp", "status": "added", "additions": 227, "deletions": 0, "changes": 227, "file_content_changes": "@@ -0,0 +1,227 @@\n+#include \"ConvertLayoutOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using ValueTable = std::map<std::pair<int, int>, Value>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+SmallVector<Value>\n+getThreadIds(Value threadId, ArrayRef<unsigned int> shapePerCTA,\n+             ArrayRef<unsigned int> sizePerThread, ArrayRef<unsigned int> order,\n+             ConversionPatternRewriter &rewriter, Location loc) {\n+  int dim = order.size();\n+  SmallVector<Value> threadIds(dim);\n+  for (unsigned k = 0; k < dim - 1; k++) {\n+    Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n+    Value rem = urem(threadId, dimK);\n+    threadId = udiv(threadId, dimK);\n+    threadIds[order[k]] = rem;\n+  }\n+  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n+  threadIds[order[dim - 1]] = urem(threadId, dimK);\n+  return threadIds;\n+}\n+\n+int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n+  auto order = layout.getOrder();\n+  auto shapePerCTA = getShapePerCTA(layout);\n+\n+  int mShapePerCTA =\n+      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int nShapePerCTA =\n+      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  return isM ? mShapePerCTA : nShapePerCTA;\n+}\n+\n+// Get sizePerThread for M or N axis.\n+int getSizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n+  auto order = layout.getOrder();\n+  auto sizePerThread = getSizePerThread(layout);\n+\n+  int mSizePerThread =\n+      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  int nSizePerThread =\n+      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  return isM ? mSizePerThread : nSizePerThread;\n+}\n+\n+Value getStructFromValueTable(ArrayRef<Value> vals,\n+                              ConversionPatternRewriter &rewriter, Location loc,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              Type elemTy) {\n+  SmallVector<Type> elemTypes(vals.size(), elemTy);\n+  SmallVector<Value> elems;\n+  elems.reserve(vals.size());\n+  for (auto &val : vals) {\n+    elems.push_back(val);\n+  }\n+  MLIRContext *ctx = elemTy.getContext();\n+  Type structTy = struct_ty(elemTypes);\n+  return typeConverter->packLLElements(loc, elems, rewriter, structTy);\n+}\n+\n+ValueTable getValueTableFromStruct(Value val, int K, int n0, int shapePerCTA,\n+                                   int sizePerThread,\n+                                   ConversionPatternRewriter &rewriter,\n+                                   Location loc,\n+                                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                                   Type type) {\n+  ValueTable res;\n+  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n+  int index = 0;\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        res[{m + mm, k}] = elems[index++];\n+      }\n+  }\n+  return res;\n+}\n+\n+Value loadAFMA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n+               Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n+               ConversionPatternRewriter &rewriter) {\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto aShape = aTensorTy.getShape();\n+\n+  auto aOrder = aLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isARow = aOrder[0] == 1;\n+\n+  auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n+  Value strideAM = aSmem.strides[0];\n+  Value strideAK = aSmem.strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+  int aNumPtr = 8;\n+  int K = aShape[1];\n+  int M = aShape[0];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value mContig = i32_val(sizePerThread[order[1]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdM = threadIds[0];\n+\n+  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n+  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n+  SmallVector<Value> aOff(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i) {\n+    aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n+  }\n+  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy, 3);\n+  SmallVector<Value> aPtrs(aNumPtr);\n+  for (int i = 0; i < aNumPtr; ++i)\n+    aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n+\n+  SmallVector<Value> vas;\n+\n+  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n+  int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n+        Value offset =\n+            add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n+        Value pa = gep(ptrTy, aPtrs[0], offset);\n+        Value va = load(pa);\n+        vas.emplace_back(va);\n+      }\n+\n+  return getStructFromValueTable(vas, rewriter, loc, typeConverter, elemTy);\n+}\n+\n+Value loadBFMA(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n+               Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n+               ConversionPatternRewriter &rewriter) {\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto bShape = bTensorTy.getShape();\n+\n+  auto bOrder = bLayout.getOrder();\n+  auto order = dLayout.getOrder();\n+\n+  bool isBRow = bOrder[0] == 1;\n+\n+  auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n+  Value strideBN = bSmem.strides[1];\n+  Value strideBK = bSmem.strides[0];\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int bNumPtr = 8;\n+  int K = bShape[0];\n+  int N = bShape[1];\n+\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+  auto sizePerThread = getSizePerThread(dLayout);\n+\n+  Value _0 = i32_val(0);\n+\n+  Value nContig = i32_val(sizePerThread[order[0]]);\n+\n+  // threadId in blocked layout\n+  auto threadIds =\n+      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n+  Value threadIdN = threadIds[1];\n+\n+  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n+  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n+  SmallVector<Value> bOff(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i) {\n+    bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n+  }\n+  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n+\n+  Type ptrTy = ptr_ty(elemTy, 3);\n+  SmallVector<Value> bPtrs(bNumPtr);\n+  for (int i = 0; i < bNumPtr; ++i)\n+    bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n+\n+  SmallVector<Value> vbs;\n+\n+  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n+  int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n+\n+  for (unsigned k = 0; k < K; ++k)\n+    for (unsigned n = 0; n < N; n += nShapePerCTA)\n+      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+        Value offset =\n+            add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n+        Value pb = gep(ptrTy, bPtrs[0], offset);\n+        Value vb = load(pb);\n+        vbs.emplace_back(vb);\n+      }\n+\n+  return getStructFromValueTable(vbs, rewriter, loc, typeConverter, elemTy);\n+}\n+\n+namespace SharedToDotOperandFMA {\n+Value convertLayout(int opIdx, Value val, Value llVal,\n+                    BlockedEncodingAttr dLayout, Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter) {\n+  if (opIdx == 0)\n+    return loadAFMA(val, llVal, dLayout, thread, loc, typeConverter, rewriter);\n+  else\n+    return loadBFMA(val, llVal, dLayout, thread, loc, typeConverter, rewriter);\n+}\n+} // namespace SharedToDotOperandFMA\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv1.cpp", "status": "added", "additions": 460, "deletions": 0, "changes": 460, "file_content_changes": "@@ -0,0 +1,460 @@\n+#include \"ConvertLayoutOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Compute the offset of the matrix to load.\n+// Returns offsetAM, offsetAK, offsetBN, offsetBK.\n+// NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n+// the same time in the usage in convert_layout[shared->dot_op], we leave\n+// the noexist info to be 0 and only use the desired argument from the\n+// composed result. In this way we want to retain the original code\n+// structure in convert_mma884 method for easier debugging.\n+static std::tuple<Value, Value, Value, Value>\n+computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n+               ArrayRef<int> spw, ArrayRef<int> rep,\n+               ConversionPatternRewriter &rewriter, Location loc,\n+               Type resultTy) {\n+  auto *ctx = rewriter.getContext();\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+\n+  Value _1 = i32_val(1);\n+  Value _3 = i32_val(3);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+\n+  Value lane = urem(threadId, _32);\n+  Value warp = udiv(threadId, _32);\n+\n+  // warp offset\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+  Value warpMOff = mul(warp0, i32_val(spw[0]));\n+  Value warpNOff = mul(warp1, i32_val(spw[1]));\n+  // Quad offset\n+  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n+  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n+  // Pair offset\n+  Value pairMOff = udiv(urem(lane, _16), _4);\n+  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n+  pairMOff = mul(pairMOff, _4);\n+  Value pairNOff = udiv(urem(lane, _16), _4);\n+  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n+  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n+  pairNOff = mul(pairNOff, _4);\n+  // scale\n+  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n+  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n+  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n+  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n+  // Quad pair offset\n+  Value laneMOff = add(pairMOff, quadMOff);\n+  Value laneNOff = add(pairNOff, quadNOff);\n+  // A offset\n+  Value offsetAM = add(warpMOff, laneMOff);\n+  Value offsetAK = and_(lane, _3);\n+  // B offset\n+  Value offsetBN = add(warpNOff, laneNOff);\n+  Value offsetBK = and_(lane, _3);\n+  // i indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  if (isARow) {\n+    offsetAM = add(offsetAM, urem(threadId, _4));\n+    offsetAK = i32_val(0);\n+  }\n+  if (!isBRow) {\n+    offsetBN = add(offsetBN, urem(threadId, _4));\n+    offsetBK = i32_val(0);\n+  }\n+\n+  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n+}\n+\n+static Value loadA(Value tensor, const SharedMemoryObject &smemObj,\n+                   Value thread, Location loc,\n+                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                   ConversionPatternRewriter &rewriter, Type resultTy) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+  Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n+  bool isARow = order[0] != 0;\n+  auto resultEncoding = resultTy.cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .cast<DotOperandEncodingAttr>();\n+  auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n+      thread, isARow, false, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n+      resultEncoding.getMMAv1Rep(), rewriter, loc, resultTy);\n+\n+  int vecA = sharedLayout.getVec();\n+\n+  auto strides = smemObj.strides;\n+  Value strideAM = isARow ? strides[0] : i32_val(1);\n+  Value strideAK = isARow ? i32_val(1) : strides[1];\n+  Value strideA0 = isARow ? strideAK : strideAM;\n+  Value strideA1 = isARow ? strideAM : strideAK;\n+\n+  int strideRepM = wpt[0] * fpw[0] * 8;\n+  int strideRepK = 1;\n+\n+  // swizzling\n+  int perPhaseA = sharedLayout.getPerPhase();\n+  int maxPhaseA = sharedLayout.getMaxPhase();\n+  int stepA0 = isARow ? strideRepK : strideRepM;\n+  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n+  int NK = shape[1];\n+\n+  // pre-compute pointer lanes\n+  Value offA0 = isARow ? offsetAK : offsetAM;\n+  Value offA1 = isARow ? offsetAM : offsetAK;\n+  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n+  offA0 = add(offA0, cSwizzleOffset);\n+  SmallVector<Value> offA(numPtrA);\n+  for (int i = 0; i < numPtrA; i++) {\n+    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n+    offA0I = udiv(offA0I, i32_val(vecA));\n+    offA0I = xor_(offA0I, phaseA);\n+    offA0I = mul(offA0I, i32_val(vecA));\n+    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n+  }\n+\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  Type elemPtrTy = ptr_ty(f16_ty, 3);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+    elemPtrTy = ptr_ty(i16_ty, 3);\n+  }\n+\n+  // prepare arguments\n+  SmallVector<Value> ptrA(numPtrA);\n+\n+  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n+  for (int i = 0; i < numPtrA; i++)\n+    ptrA[i] = gep(ptr_ty(f16_ty, 3), smemBase, offA[i]);\n+\n+  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+  auto loadA = [&](int m, int k) {\n+    int offidx = (isARow ? k / 4 : m) % numPtrA;\n+    Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n+\n+    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n+    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n+    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n+                       mul(i32_val(stepAK), strideAK));\n+    Value pa = gep(elemPtrTy, thePtrA, offset);\n+    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n+    Value ha = load(bitcast(pa, aPtrTy));\n+    // record lds that needs to be moved\n+    Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n+    Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n+    ld(has, m, k, ha00, ha01);\n+\n+    if (vecA > 4) {\n+      Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n+      Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n+      if (isARow)\n+        ld(has, m, k + 4, ha10, ha11);\n+      else\n+        ld(has, m + 1, k, ha10, ha11);\n+    }\n+  };\n+\n+  bool isARow_ = resultEncoding.getMMAv1IsRow();\n+  bool isAVec4 = resultEncoding.getMMAv1IsVec4();\n+  unsigned numM = resultEncoding.getMMAv1NumOuter(shape);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      if (!has.count({m, k}))\n+        loadA(m, k);\n+\n+  SmallVector<Value> elems;\n+  elems.reserve(has.size() * 2);\n+  for (auto item : has) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n+  return res;\n+}\n+\n+static Value loadB(Value tensor, const SharedMemoryObject &smemObj,\n+                   Value thread, Location loc,\n+                   TritonGPUToLLVMTypeConverter *typeConverter,\n+                   ConversionPatternRewriter &rewriter, Type resultTy) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+  auto wpt = resultTy.cast<RankedTensorType>()\n+                 .getEncoding()\n+                 .cast<DotOperandEncodingAttr>()\n+                 .getParent()\n+                 .cast<MmaEncodingAttr>()\n+                 .getWarpsPerCTA();\n+  // smem\n+  auto strides = smemObj.strides;\n+\n+  auto *ctx = rewriter.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+\n+  auto shape = tensorTy.getShape();\n+  auto order = sharedLayout.getOrder();\n+\n+  Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+  bool isBRow = order[0] != 0; // is row-major in shared memory layout\n+  // isBRow_ indicates whether B is row-major in DotOperand layout\n+  auto resultEncoding = resultTy.cast<RankedTensorType>()\n+                            .getEncoding()\n+                            .cast<DotOperandEncodingAttr>();\n+\n+  int vecB = sharedLayout.getVec();\n+  Value strideBN = isBRow ? i32_val(1) : strides[1];\n+  Value strideBK = isBRow ? strides[0] : i32_val(1);\n+  Value strideB0 = isBRow ? strideBN : strideBK;\n+  Value strideB1 = isBRow ? strideBK : strideBN;\n+  int strideRepN = wpt[1] * fpw[1] * 8;\n+  int strideRepK = 1;\n+\n+  auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n+      thread, false, isBRow, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n+      resultEncoding.getMMAv1Rep(), rewriter, loc, resultTy);\n+\n+  // swizzling\n+  int perPhaseB = sharedLayout.getPerPhase();\n+  int maxPhaseB = sharedLayout.getMaxPhase();\n+  int stepB0 = isBRow ? strideRepN : strideRepK;\n+  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n+  int NK = shape[0];\n+\n+  Value offB0 = isBRow ? offsetBN : offsetBK;\n+  Value offB1 = isBRow ? offsetBK : offsetBN;\n+  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n+  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+\n+  offB0 = add(offB0, cSwizzleOffset);\n+  SmallVector<Value> offB(numPtrB);\n+  for (int i = 0; i < numPtrB; ++i) {\n+    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n+    offB0I = udiv(offB0I, i32_val(vecB));\n+    offB0I = xor_(offB0I, phaseB);\n+    offB0I = mul(offB0I, i32_val(vecB));\n+    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n+  }\n+\n+  Type elemPtrTy = ptr_ty(f16_ty, 3);\n+  Type elemX2Ty = vec_ty(f16_ty, 2);\n+  if (tensorTy.getElementType().isBF16()) {\n+    elemPtrTy = ptr_ty(i16_ty, 3);\n+    elemX2Ty = vec_ty(i16_ty, 2);\n+  }\n+\n+  SmallVector<Value> ptrB(numPtrB);\n+  ValueTable hbs;\n+  for (int i = 0; i < numPtrB; ++i)\n+    ptrB[i] = gep(ptr_ty(f16_ty, 3), smem, offB[i]);\n+\n+  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n+    vals[{m, k}] = {val0, val1};\n+  };\n+\n+  auto loadB = [&](int n, int K) {\n+    int offidx = (isBRow ? n : K / 4) % numPtrB;\n+    Value thePtrB = ptrB[offidx];\n+\n+    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n+    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n+    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n+                       mul(i32_val(stepBK), strideBK));\n+    Value pb = gep(elemPtrTy, thePtrB, offset);\n+\n+    Value hb =\n+        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n+    // record lds that needs to be moved\n+    Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n+    Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n+    ld(hbs, n, K, hb00, hb01);\n+    if (vecB > 4) {\n+      Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n+      Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n+      if (isBRow)\n+        ld(hbs, n + 1, K, hb10, hb11);\n+      else\n+        ld(hbs, n, K + 4, hb10, hb11);\n+    }\n+  };\n+\n+  bool isBRow_ = resultEncoding.getMMAv1IsRow();\n+  assert(isBRow == isBRow_ && \"B need smem isRow\");\n+  bool isBVec4 = resultEncoding.getMMAv1IsVec4();\n+  unsigned numN = resultEncoding.getMMAv1NumOuter(shape);\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned n = 0; n < numN / 2; ++n) {\n+      if (!hbs.count({n, k}))\n+        loadB(n, k);\n+    }\n+\n+  SmallVector<Value> elems;\n+  for (auto &item : hbs) { // has is a map, the key should be ordered.\n+    elems.push_back(item.second.first);\n+    elems.push_back(item.second.second);\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n+  return res;\n+}\n+\n+namespace SharedToDotOperandMMAv1 {\n+using CoordTy = SmallVector<Value>;\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+SmallVector<CoordTy> getMNCoords(Value thread,\n+                                 ConversionPatternRewriter &rewriter,\n+                                 ArrayRef<unsigned int> wpt,\n+                                 const MmaEncodingAttr &mmaLayout,\n+                                 ArrayRef<int64_t> shape, bool isARow,\n+                                 bool isBRow, bool isAVec4, bool isBVec4) {\n+  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n+\n+  auto *ctx = thread.getContext();\n+  auto loc = UnknownLoc::get(ctx);\n+  Value _1 = i32_val(1);\n+  Value _2 = i32_val(2);\n+  Value _4 = i32_val(4);\n+  Value _16 = i32_val(16);\n+  Value _32 = i32_val(32);\n+  Value _fpw0 = i32_val(fpw[0]);\n+  Value _fpw1 = i32_val(fpw[1]);\n+\n+  // A info\n+  auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout);\n+  auto aRep = aEncoding.getMMAv1Rep();\n+  auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n+  // B info\n+  auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout);\n+  auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n+  auto bRep = bEncoding.getMMAv1Rep();\n+\n+  SmallVector<int, 2> rep({aRep[0], bRep[1]});\n+  SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n+  SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+  Value lane = urem(thread, _32);\n+  Value warp = udiv(thread, _32);\n+\n+  Value warp0 = urem(warp, i32_val(wpt[0]));\n+  Value warp12 = udiv(warp, i32_val(wpt[0]));\n+  Value warp1 = urem(warp12, i32_val(wpt[1]));\n+\n+  // warp offset\n+  Value offWarpM = mul(warp0, i32_val(spw[0]));\n+  Value offWarpN = mul(warp1, i32_val(spw[1]));\n+  // quad offset\n+  Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n+  Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n+  // pair offset\n+  Value offPairM = udiv(urem(lane, _16), _4);\n+  offPairM = urem(offPairM, _fpw0);\n+  offPairM = mul(offPairM, _4);\n+  Value offPairN = udiv(urem(lane, _16), _4);\n+  offPairN = udiv(offPairN, _fpw0);\n+  offPairN = urem(offPairN, _fpw1);\n+  offPairN = mul(offPairN, _4);\n+\n+  // sclare\n+  offPairM = mul(offPairM, i32_val(rep[0] / 2));\n+  offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n+  offPairN = mul(offPairN, i32_val(rep[1] / 2));\n+  offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n+\n+  // quad pair offset\n+  Value offLaneM = add(offPairM, offQuadM);\n+  Value offLaneN = add(offPairN, offQuadN);\n+  // a, b offset\n+  Value offsetAM = add(offWarpM, offLaneM);\n+  Value offsetBN = add(offWarpN, offLaneN);\n+  // m indices\n+  Value offsetCM = add(and_(lane, _1), offsetAM);\n+  SmallVector<Value> idxM;\n+  for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n+    for (unsigned mm = 0; mm < rep[0]; ++mm)\n+      idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n+\n+  // n indices\n+  Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n+  SmallVector<Value> idxN;\n+  for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n+    for (int nn = 0; nn < rep[1]; ++nn) {\n+      idxN.push_back(add(\n+          offsetCN, i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1])));\n+      idxN.push_back(\n+          add(offsetCN,\n+              i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n+    }\n+  }\n+\n+  SmallVector<SmallVector<Value>> axes({idxM, idxN});\n+\n+  // product the axis M and axis N to get coords, ported from\n+  // generator::init_idx method from triton2.0\n+\n+  // TODO[Superjomn]: check the order.\n+  SmallVector<CoordTy> coords;\n+  for (Value x1 : axes[1]) {   // N\n+    for (Value x0 : axes[0]) { // M\n+      SmallVector<Value, 2> idx(2);\n+      idx[0] = x0; // M\n+      idx[1] = x1; // N\n+      coords.push_back(std::move(idx));\n+    }\n+  }\n+\n+  return coords; // {M,N} in row-major\n+}\n+\n+Value convertLayout(int opIdx, Value tensor, const SharedMemoryObject &smemObj,\n+                    Value thread, Location loc,\n+                    TritonGPUToLLVMTypeConverter *typeConverter,\n+                    ConversionPatternRewriter &rewriter, Type resultTy) {\n+  if (opIdx == 0)\n+    return loadA(tensor, smemObj, thread, loc, typeConverter, rewriter,\n+                 resultTy);\n+  else {\n+    assert(opIdx == 1);\n+    return loadB(tensor, smemObj, thread, loc, typeConverter, rewriter,\n+                 resultTy);\n+  }\n+}\n+\n+} // namespace SharedToDotOperandMMAv1\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "added", "additions": 685, "deletions": 0, "changes": 685, "file_content_changes": "@@ -0,0 +1,685 @@\n+#include \"ConvertLayoutOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n+using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n+using ::mlir::LLVM::getStridesFromShapeAndOrder;\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::getContigPerThread;\n+using ::mlir::triton::gpu::getElemsPerThread;\n+using ::mlir::triton::gpu::getOrder;\n+using ::mlir::triton::gpu::getShapePerCTA;\n+using ::mlir::triton::gpu::getSizePerThread;\n+using ::mlir::triton::gpu::isaDistributedLayout;\n+using ::mlir::triton::gpu::SharedEncodingAttr;\n+\n+// Data loader for mma.16816 instruction.\n+class MMA16816SmemLoader {\n+public:\n+  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n+                     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+                     ArrayRef<int> instrShape, ArrayRef<int> matShape,\n+                     int perPhase, int maxPhase, int elemBytes,\n+                     ConversionPatternRewriter &rewriter,\n+                     TritonGPUToLLVMTypeConverter *typeConverter,\n+                     const Location &loc);\n+\n+  // lane = thread % 32\n+  // warpOff = (thread/32) % wpt(0)\n+  llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane,\n+                                          Value cSwizzleOffset) {\n+    if (canUseLdmatrix)\n+      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n+    else if (elemBytes == 4 && needTrans)\n+      return computeB32MatOffs(warpOff, lane, cSwizzleOffset);\n+    else if (elemBytes == 1 && needTrans)\n+      return computeB8MatOffs(warpOff, lane, cSwizzleOffset);\n+    else\n+      llvm::report_fatal_error(\"Invalid smem load config\");\n+\n+    return {};\n+  }\n+\n+  int getNumPtrs() const { return numPtrs; }\n+\n+  // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n+  // mapped to.\n+  SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                            Value cSwizzleOffset);\n+\n+  // Compute 32-bit matrix offsets.\n+  SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane,\n+                                       Value cSwizzleOffset);\n+\n+  // compute 8-bit matrix offset.\n+  SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane,\n+                                      Value cSwizzleOffset);\n+\n+  // Load 4 matrices and returns 4 vec<2> elements.\n+  std::tuple<Value, Value, Value, Value>\n+  loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n+         Type matTy, Type shemPtrTy) const;\n+\n+private:\n+  SmallVector<uint32_t> order;\n+  int kOrder;\n+  SmallVector<int64_t> tileShape;\n+  SmallVector<int> instrShape;\n+  SmallVector<int> matShape;\n+  int perPhase;\n+  int maxPhase;\n+  int elemBytes;\n+  ConversionPatternRewriter &rewriter;\n+  const Location &loc;\n+  MLIRContext *ctx{};\n+\n+  int cMatShape;\n+  int sMatShape;\n+\n+  Value sStride;\n+\n+  bool needTrans;\n+  bool canUseLdmatrix;\n+\n+  int numPtrs;\n+\n+  int pLoadStrideInMat;\n+  int sMatStride;\n+\n+  int matArrStride;\n+  int warpOffStride;\n+};\n+\n+SmallVector<Value>\n+MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n+                                           Value cSwizzleOffset) {\n+  // 4x4 matrices\n+  Value c = urem(lane, i32_val(8));\n+  Value s = udiv(lane, i32_val(8)); // sub-warp-id\n+\n+  // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n+  // warp\n+  Value s0 = urem(s, i32_val(2));\n+  Value s1 = udiv(s, i32_val(2));\n+\n+  // We use different orders for a and b for better performance.\n+  Value kMatArr = kOrder == 1 ? s1 : s0;\n+  Value nkMatArr = kOrder == 1 ? s0 : s1;\n+\n+  // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n+  // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n+  //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n+  //   |0 0 1 1 2 2|\n+  //\n+  // for B(kOrder=0) is\n+  //   |0 0|  -> 0,1,2 are the warpids\n+  //   |1 1|\n+  //   |2 2|\n+  //   |0 0|\n+  //   |1 1|\n+  //   |2 2|\n+  // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n+  // address (s0,s1) annotates.\n+\n+  Value matOff[2];\n+  matOff[kOrder ^ 1] =\n+      add(mul(warpId, i32_val(warpOffStride)),   // warp offset\n+          mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n+  matOff[kOrder] = kMatArr;\n+\n+  // Physical offset (before swizzling)\n+  Value cMatOff = matOff[order[0]];\n+  Value sMatOff = matOff[order[1]];\n+  Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+  cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+  // row offset inside a matrix, each matrix has 8 rows.\n+  Value sOffInMat = c;\n+\n+  SmallVector<Value> offs(numPtrs);\n+  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+  for (int i = 0; i < numPtrs; ++i) {\n+    Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n+    cMatOffI = xor_(cMatOffI, phase);\n+    offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n+  }\n+\n+  return offs;\n+}\n+\n+SmallVector<Value> MMA16816SmemLoader::computeB32MatOffs(Value warpOff,\n+                                                         Value lane,\n+                                                         Value cSwizzleOffset) {\n+  assert(needTrans && \"Only used in transpose mode.\");\n+  // Load tf32 matrices with lds32\n+  Value cOffInMat = udiv(lane, i32_val(4));\n+  Value sOffInMat = urem(lane, i32_val(4));\n+\n+  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n+  SmallVector<Value> offs(numPtrs);\n+\n+  for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n+    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+    if (kMatArrInt > 0) // we don't need pointers for k\n+      continue;\n+    Value kMatArr = i32_val(kMatArrInt);\n+    Value nkMatArr = i32_val(nkMatArrInt);\n+\n+    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                        mul(nkMatArr, i32_val(matArrStride)));\n+    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n+    cMatOff = add(cMatOff, cSwizzleMatOff);\n+\n+    Value sMatOff = kMatArr;\n+    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n+    // FIXME: (kOrder == 1?) is really dirty hack\n+    for (int i = 0; i < numPtrs / 2; ++i) {\n+      Value cMatOffI =\n+          add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n+      cMatOffI = xor_(cMatOffI, phase);\n+      Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+      cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+      sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+      offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n+    }\n+  }\n+  return offs;\n+}\n+\n+SmallVector<Value> MMA16816SmemLoader::computeB8MatOffs(Value warpOff,\n+                                                        Value lane,\n+                                                        Value cSwizzleOffset) {\n+  assert(needTrans && \"Only used in transpose mode.\");\n+  Value cOffInMat = udiv(lane, i32_val(4));\n+  Value sOffInMat =\n+      mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n+\n+  SmallVector<Value> offs(numPtrs);\n+  for (int mat = 0; mat < 4; ++mat) {\n+    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n+    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n+    if (kMatArrInt > 0) // we don't need pointers for k\n+      continue;\n+    Value kMatArr = i32_val(kMatArrInt);\n+    Value nkMatArr = i32_val(nkMatArrInt);\n+\n+    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n+                        mul(nkMatArr, i32_val(matArrStride)));\n+    Value sMatOff = kMatArr;\n+\n+    for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n+      for (int elemOff = 0; elemOff < 4; ++elemOff) {\n+        int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n+        Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n+                                              (kOrder == 1 ? 1 : 2)));\n+        Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n+\n+        // disable swizzling ...\n+\n+        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n+        Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n+        // To prevent out-of-bound access when tile is too small.\n+        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n+        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n+        offs[ptrOff] = add(cOff, mul(sOff, sStride));\n+      }\n+    }\n+  }\n+  return offs;\n+}\n+\n+std::tuple<Value, Value, Value, Value>\n+MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n+                           ArrayRef<Value> ptrs, Type matTy,\n+                           Type shemPtrTy) const {\n+  assert(mat0 % 2 == 0 && mat1 % 2 == 0 && \"smem matrix load must be aligned\");\n+  int matIdx[2] = {mat0, mat1};\n+\n+  int ptrIdx{-1};\n+\n+  if (canUseLdmatrix)\n+    ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n+  else if (elemBytes == 4 && needTrans)\n+    ptrIdx = matIdx[order[0]];\n+  else if (elemBytes == 1 && needTrans)\n+    ptrIdx = matIdx[order[0]] * 4;\n+  else\n+    llvm::report_fatal_error(\"unsupported mma type found\");\n+\n+  // The main difference with the original triton code is we removed the\n+  // prefetch-related logic here for the upstream optimizer phase should\n+  // take care with it, and that is transparent in dot conversion.\n+  auto getPtr = [&](int idx) { return ptrs[idx]; };\n+\n+  Value ptr = getPtr(ptrIdx);\n+\n+  // The struct should have exactly the same element types.\n+  auto resTy = matTy.cast<LLVM::LLVMStructType>();\n+  Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+\n+  // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n+  // instructions to pack & unpack sub-word integers. A workaround is to\n+  // store the results of ldmatrix in i32\n+  if (auto vecElemTy = elemTy.dyn_cast<VectorType>()) {\n+    Type elemElemTy = vecElemTy.getElementType();\n+    if (auto intTy = elemElemTy.dyn_cast<IntegerType>()) {\n+      if (intTy.getWidth() <= 16) {\n+        elemTy = rewriter.getI32Type();\n+        resTy =\n+            LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, elemTy));\n+      }\n+    }\n+  }\n+\n+  if (canUseLdmatrix) {\n+    Value sOffset =\n+        mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n+    Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n+\n+    PTXBuilder builder;\n+    // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n+    // thread.\n+    auto resArgs = builder.newListOperand(4, \"=r\");\n+    auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n+\n+    auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n+                        ->o(\"trans\", needTrans /*predicate*/)\n+                        .o(\"shared.b16\");\n+    ldmatrix(resArgs, addrArg);\n+\n+    // The result type is 4xi32, each i32 is composed of 2xf16\n+    // elements (adjacent two columns in a row) or a single f32 element.\n+    Value resV4 = builder.launch(rewriter, loc, resTy);\n+    return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n+            extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n+  } else if (elemBytes == 4 && needTrans) { // Use lds.32 to load tf32 matrices\n+    Value ptr2 = getPtr(ptrIdx + 1);\n+    assert(sMatStride == 1);\n+    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+    int sOffsetArrElem = sMatStride * sMatShape;\n+    Value sOffsetArrElemVal =\n+        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+    Value elems[4];\n+    if (kOrder == 1) {\n+      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+      elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+      elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n+    } else {\n+      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n+      elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n+      elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n+      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n+    }\n+    std::array<Value, 4> retElems;\n+    retElems.fill(undef(elemTy));\n+    for (auto i = 0; i < 4; ++i) {\n+      retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n+    }\n+    return {retElems[0], retElems[1], retElems[2], retElems[3]};\n+  } else if (elemBytes == 1 && needTrans) { // work with int8\n+    // Can't use i32 here. Use LLVM's VectorType\n+    elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n+    std::array<std::array<Value, 4>, 2> ptrs;\n+    ptrs[0] = {\n+        getPtr(ptrIdx),\n+        getPtr(ptrIdx + 1),\n+        getPtr(ptrIdx + 2),\n+        getPtr(ptrIdx + 3),\n+    };\n+\n+    ptrs[1] = {\n+        getPtr(ptrIdx + 4),\n+        getPtr(ptrIdx + 5),\n+        getPtr(ptrIdx + 6),\n+        getPtr(ptrIdx + 7),\n+    };\n+\n+    assert(sMatStride == 1);\n+    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n+    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n+    int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n+    Value sOffsetArrElemVal =\n+        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n+\n+    std::array<Value, 4> i8v4Elems;\n+    i8v4Elems.fill(undef(elemTy));\n+\n+    Value i8Elems[4][4];\n+    if (kOrder == 1) {\n+      for (int i = 0; i < 2; ++i)\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n+\n+      for (int i = 2; i < 4; ++i)\n+        for (int j = 0; j < 4; ++j)\n+          i8Elems[i][j] =\n+              load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n+\n+      for (int m = 0; m < 4; ++m) {\n+        for (int e = 0; e < 4; ++e)\n+          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                        i8Elems[m][e], i32_val(e));\n+      }\n+    } else { // k first\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n+      for (int j = 0; j < 4; ++j)\n+        i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n+\n+      for (int m = 0; m < 4; ++m) {\n+        for (int e = 0; e < 4; ++e)\n+          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n+                                        i8Elems[m][e], i32_val(e));\n+      }\n+    }\n+\n+    return {bitcast(i8v4Elems[0], i32_ty), bitcast(i8v4Elems[1], i32_ty),\n+            bitcast(i8v4Elems[2], i32_ty), bitcast(i8v4Elems[3], i32_ty)};\n+  }\n+\n+  assert(false && \"Invalid smem load\");\n+  return {Value{}, Value{}, Value{}, Value{}};\n+}\n+\n+MMA16816SmemLoader::MMA16816SmemLoader(\n+    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n+    ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n+    ArrayRef<int> instrShape, ArrayRef<int> matShape, int perPhase,\n+    int maxPhase, int elemBytes, ConversionPatternRewriter &rewriter,\n+    TritonGPUToLLVMTypeConverter *typeConverter, const Location &loc)\n+    : order(order.begin(), order.end()), kOrder(kOrder),\n+      tileShape(tileShape.begin(), tileShape.end()),\n+      instrShape(instrShape.begin(), instrShape.end()),\n+      matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n+      maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n+      ctx(rewriter.getContext()) {\n+  cMatShape = matShape[order[0]];\n+  sMatShape = matShape[order[1]];\n+\n+  sStride = smemStrides[order[1]];\n+\n+  // rule: k must be the fast-changing axis.\n+  needTrans = kOrder != order[0];\n+  canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n+\n+  if (canUseLdmatrix) {\n+    // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n+    // otherwise [wptx1], and each warp will perform a mma.\n+    numPtrs =\n+        tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n+  } else {\n+    numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n+  }\n+  numPtrs = std::max<int>(numPtrs, 2);\n+\n+  // Special rule for i8/u8, 4 ptrs for each matrix\n+  if (!canUseLdmatrix && elemBytes == 1)\n+    numPtrs *= 4;\n+\n+  int loadStrideInMat[2];\n+  loadStrideInMat[kOrder] =\n+      2; // instrShape[kOrder] / matShape[kOrder], always 2\n+  loadStrideInMat[kOrder ^ 1] =\n+      wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n+\n+  pLoadStrideInMat = loadStrideInMat[order[0]];\n+  sMatStride =\n+      loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n+\n+  // Each matArr contains warpOffStride matrices.\n+  matArrStride = kOrder == 1 ? 1 : wpt;\n+  warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n+}\n+\n+Type getShemPtrTy(Type argType) {\n+  MLIRContext *ctx = argType.getContext();\n+  if (argType.isF16())\n+    return ptr_ty(type::f16Ty(ctx), 3);\n+  else if (argType.isBF16())\n+    return ptr_ty(type::i16Ty(ctx), 3);\n+  else if (argType.isF32())\n+    return ptr_ty(type::f32Ty(ctx), 3);\n+  else if (argType.isInteger(8))\n+    return ptr_ty(type::i8Ty(ctx), 3);\n+  else\n+    llvm::report_fatal_error(\"mma16816 data type not supported\");\n+}\n+\n+Type getMatType(Type argType) {\n+  MLIRContext *ctx = argType.getContext();\n+  // floating point types\n+  Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n+  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n+  Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n+  Type fp16x2Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n+  // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n+  Type bf16x2Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n+  Type fp32Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n+  // integer types\n+  Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n+  Type i8x4Pack4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n+\n+  if (argType.isF16())\n+    return fp16x2Pack4Ty;\n+  else if (argType.isBF16())\n+    return bf16x2Pack4Ty;\n+  else if (argType.isF32())\n+    return fp32Pack4Ty;\n+  else if (argType.isInteger(8))\n+    return i8x4Pack4Ty;\n+  else\n+    llvm::report_fatal_error(\"mma16816 data type not supported\");\n+}\n+\n+Value composeValuesToDotOperandLayoutStruct(\n+    const ValueTable &vals, int n0, int n1,\n+    TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+    ConversionPatternRewriter &rewriter) {\n+  std::vector<Value> elems;\n+  for (int m = 0; m < n0; ++m)\n+    for (int k = 0; k < n1; ++k) {\n+      elems.push_back(vals.at({2 * m, 2 * k}));\n+      elems.push_back(vals.at({2 * m, 2 * k + 1}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k}));\n+      elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n+    }\n+\n+  assert(!elems.empty());\n+\n+  Type elemTy = elems[0].getType();\n+  MLIRContext *ctx = elemTy.getContext();\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(elems.size(), elemTy));\n+  auto result = typeConverter->packLLElements(loc, elems, rewriter, structTy);\n+  return result;\n+}\n+\n+std::function<void(int, int)>\n+getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n+                MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n+                SmallVector<int> instrShape, SmallVector<int> matShape,\n+                Value warpId, Value lane, ValueTable &vals, bool isA,\n+                TritonGPUToLLVMTypeConverter *typeConverter,\n+                ConversionPatternRewriter &rewriter, Location loc) {\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  Type eltTy = tensorTy.getElementType();\n+  // We assumes that the input operand of Dot should be from shared layout.\n+  // TODO(Superjomn) Consider other layouts if needed later.\n+  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n+  const int perPhase = sharedLayout.getPerPhase();\n+  const int maxPhase = sharedLayout.getMaxPhase();\n+  const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n+  auto order = sharedLayout.getOrder();\n+\n+  // the original register_lds2, but discard the prefetch logic.\n+  auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n+    vals[{mn, k}] = val;\n+  };\n+\n+  // (a, b) is the coordinate.\n+  auto load = [=, &rewriter, &vals, &ld2](int a, int b) {\n+    MMA16816SmemLoader loader(\n+        wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n+        tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n+        maxPhase, elemBytes, rewriter, typeConverter, loc);\n+    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n+    SmallVector<Value> offs =\n+        loader.computeOffsets(warpId, lane, cSwizzleOffset);\n+    const int numPtrs = loader.getNumPtrs();\n+    SmallVector<Value> ptrs(numPtrs);\n+\n+    Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n+\n+    Type smemPtrTy = getShemPtrTy(eltTy);\n+    for (int i = 0; i < numPtrs; ++i) {\n+      ptrs[i] =\n+          bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n+    }\n+\n+    auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n+        (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n+        ptrs, getMatType(eltTy), getShemPtrTy(eltTy));\n+\n+    if (isA) {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha1);\n+      ld2(vals, a, b + 1, ha2);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    } else {\n+      ld2(vals, a, b, ha0);\n+      ld2(vals, a + 1, b, ha2);\n+      ld2(vals, a, b + 1, ha1);\n+      ld2(vals, a + 1, b + 1, ha3);\n+    }\n+  };\n+\n+  return load;\n+}\n+\n+Value loadA(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n+            DotOperandEncodingAttr aEncoding, const SharedMemoryObject &smemObj,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n+  int bitwidth = aTensorTy.getElementTypeBitWidth();\n+  auto mmaLayout = aEncoding.getParent().cast<MmaEncodingAttr>();\n+\n+  SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n+                             aTensorTy.getShape().end());\n+\n+  ValueTable ha;\n+  std::function<void(int, int)> loadFn;\n+  int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n+  int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n+\n+  auto numRep = aEncoding.getMMAv2Rep(aTensorTy.getShape(), bitwidth);\n+  int numRepM = numRep[0], numRepK = numRep[1];\n+\n+  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n+    int wpt0 = mmaLayout.getWarpsPerCTA()[0];\n+    Value warp = udiv(thread, i32_val(32));\n+    Value lane = urem(thread, i32_val(32));\n+    Value warpM = urem(urem(warp, i32_val(wpt0)), i32_val(shape[0] / 16));\n+    // load from smem\n+    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+    int wpt = std::min<int>(wpt0, shape[0] / 16);\n+    loadFn = getLoadMatrixFn(\n+        tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n+        {mmaInstrM, mmaInstrK} /*instrShape*/,\n+        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/, lane /*laneId*/,\n+        ha /*vals*/, true /*isA*/, typeConverter /* typeConverter */,\n+        rewriter /*rewriter*/, loc /*loc*/);\n+  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n+    // load from registers, used in gemm fuse\n+    // TODO(Superjomn) Port the logic.\n+    assert(false && \"Loading A from register is not supported yet.\");\n+  } else {\n+    assert(false && \"A's layout is not supported.\");\n+  }\n+\n+  // step1. Perform loading.\n+  for (int m = 0; m < numRepM; ++m)\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * m, 2 * k);\n+\n+  // step2. Format the values to LLVM::Struct to passing to mma codegen.\n+  return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK,\n+                                               typeConverter, loc, rewriter);\n+}\n+\n+Value loadB(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n+            DotOperandEncodingAttr bEncoding, const SharedMemoryObject &smemObj,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  ValueTable hb;\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  int bitwidth = tensorTy.getElementTypeBitWidth();\n+  auto mmaLayout = bEncoding.getParent().cast<MmaEncodingAttr>();\n+\n+  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n+                             tensorTy.getShape().end());\n+\n+  // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n+  bool transB = false;\n+  if (transB) {\n+    std::swap(shape[0], shape[1]);\n+  }\n+\n+  int mmaInstrM = 16, mmaInstrN = 8, mmaInstrK = 4 * 64 / bitwidth;\n+  int matShapeM = 8, matShapeN = 8, matShapeK = 2 * 64 / bitwidth;\n+\n+  auto numRep = bEncoding.getMMAv2Rep(tensorTy.getShape(), bitwidth);\n+  int numRepK = numRep[0];\n+  int numRepN = numRep[1];\n+\n+  int wpt0 = mmaLayout.getWarpsPerCTA()[0];\n+  int wpt1 = mmaLayout.getWarpsPerCTA()[1];\n+  Value warp = udiv(thread, i32_val(32));\n+  Value lane = urem(thread, i32_val(32));\n+  Value warpMN = udiv(warp, i32_val(wpt0));\n+  Value warpN = urem(urem(warpMN, i32_val(wpt1)), i32_val(shape[1] / 8));\n+  // we use ldmatrix.x4 so each warp processes 16x16 elements.\n+  int wpt = std::min<int>(wpt1, shape[1] / 16);\n+  auto loadFn = getLoadMatrixFn(\n+      tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n+      {mmaInstrK, mmaInstrN} /*instrShape*/,\n+      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/, lane /*laneId*/,\n+      hb /*vals*/, false /*isA*/, typeConverter /* typeConverter */,\n+      rewriter /*rewriter*/, loc /*loc*/);\n+\n+  for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n+    for (int k = 0; k < numRepK; ++k)\n+      loadFn(2 * n, 2 * k);\n+  }\n+\n+  Value result = composeValuesToDotOperandLayoutStruct(\n+      hb, std::max(numRepN / 2, 1), numRepK, typeConverter, loc, rewriter);\n+  return result;\n+}\n+\n+namespace SharedToDotOperandMMAv2 {\n+Value convertLayout(int opIdx, ConversionPatternRewriter &rewriter,\n+                    Location loc, Value tensor, DotOperandEncodingAttr encoding,\n+                    const SharedMemoryObject &smemObj,\n+                    TritonGPUToLLVMTypeConverter *typeConverter, Value thread) {\n+  if (opIdx == 0)\n+    return loadA(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                 thread);\n+  else {\n+    assert(opIdx == 1);\n+    return loadB(rewriter, loc, tensor, encoding, smemObj, typeConverter,\n+                 thread);\n+  }\n+}\n+} // namespace SharedToDotOperandMMAv2\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.cpp", "status": "removed", "additions": 0, "deletions": 1440, "changes": 1440, "file_content_changes": "@@ -1,1440 +0,0 @@\n-#include \"DotOpHelpers.h\"\n-#include \"TypeConverter.h\"\n-\n-namespace mlir {\n-namespace LLVM {\n-\n-Value DotOpMmaV1ConversionHelper::loadA(\n-    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter,\n-    ConversionPatternRewriter &rewriter, Type resultTy) const {\n-  auto *ctx = rewriter.getContext();\n-  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto shape = tensorTy.getShape();\n-  auto order = sharedLayout.getOrder();\n-\n-  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-  Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n-  bool isARow = order[0] != 0;\n-  auto resultEncoding = resultTy.cast<RankedTensorType>()\n-                            .getEncoding()\n-                            .cast<DotOperandEncodingAttr>();\n-  auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n-      thread, isARow, false, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n-      resultEncoding.getMMAv1Rep(), rewriter, loc);\n-\n-  int vecA = sharedLayout.getVec();\n-\n-  auto strides = smemObj.strides;\n-  Value strideAM = isARow ? strides[0] : i32_val(1);\n-  Value strideAK = isARow ? i32_val(1) : strides[1];\n-  Value strideA0 = isARow ? strideAK : strideAM;\n-  Value strideA1 = isARow ? strideAM : strideAK;\n-\n-  int strideRepM = wpt[0] * fpw[0] * 8;\n-  int strideRepK = 1;\n-\n-  // swizzling\n-  int perPhaseA = sharedLayout.getPerPhase();\n-  int maxPhaseA = sharedLayout.getMaxPhase();\n-  int stepA0 = isARow ? strideRepK : strideRepM;\n-  int numPtrA = std::max(2 * perPhaseA * maxPhaseA / stepA0, 1);\n-  int NK = shape[1];\n-\n-  // pre-compute pointer lanes\n-  Value offA0 = isARow ? offsetAK : offsetAM;\n-  Value offA1 = isARow ? offsetAM : offsetAK;\n-  Value phaseA = urem(udiv(offA1, i32_val(perPhaseA)), i32_val(maxPhaseA));\n-  offA0 = add(offA0, cSwizzleOffset);\n-  SmallVector<Value> offA(numPtrA);\n-  for (int i = 0; i < numPtrA; i++) {\n-    Value offA0I = add(offA0, i32_val(i * (isARow ? 4 : strideRepM)));\n-    offA0I = udiv(offA0I, i32_val(vecA));\n-    offA0I = xor_(offA0I, phaseA);\n-    offA0I = mul(offA0I, i32_val(vecA));\n-    offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n-  }\n-\n-  Type elemX2Ty = vec_ty(f16_ty, 2);\n-  Type elemPtrTy = ptr_ty(f16_ty, 3);\n-  if (tensorTy.getElementType().isBF16()) {\n-    elemX2Ty = vec_ty(i16_ty, 2);\n-    elemPtrTy = ptr_ty(i16_ty, 3);\n-  }\n-\n-  // prepare arguments\n-  SmallVector<Value> ptrA(numPtrA);\n-\n-  std::map<std::pair<int, int>, std::pair<Value, Value>> has;\n-  for (int i = 0; i < numPtrA; i++)\n-    ptrA[i] = gep(ptr_ty(f16_ty, 3), smemBase, offA[i]);\n-\n-  auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n-    vals[{m, k}] = {val0, val1};\n-  };\n-  auto loadA = [&](int m, int k) {\n-    int offidx = (isARow ? k / 4 : m) % numPtrA;\n-    Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n-\n-    int stepAM = isARow ? m : m / numPtrA * numPtrA;\n-    int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n-    Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n-                       mul(i32_val(stepAK), strideAK));\n-    Value pa = gep(elemPtrTy, thePtrA, offset);\n-    Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n-    Value ha = load(bitcast(pa, aPtrTy));\n-    // record lds that needs to be moved\n-    Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n-    Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n-    ld(has, m, k, ha00, ha01);\n-\n-    if (vecA > 4) {\n-      Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n-      Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n-      if (isARow)\n-        ld(has, m, k + 4, ha10, ha11);\n-      else\n-        ld(has, m + 1, k, ha10, ha11);\n-    }\n-  };\n-\n-  bool isARow_ = resultEncoding.getMMAv1IsRow();\n-  bool isAVec4 = resultEncoding.getMMAv1IsVec4();\n-  unsigned numM = resultEncoding.getMMAv1NumOuter(shape);\n-  for (unsigned k = 0; k < NK; k += 4)\n-    for (unsigned m = 0; m < numM / 2; ++m)\n-      if (!has.count({m, k}))\n-        loadA(m, k);\n-\n-  SmallVector<Value> elems;\n-  elems.reserve(has.size() * 2);\n-  for (auto item : has) { // has is a map, the key should be ordered.\n-    elems.push_back(item.second.first);\n-    elems.push_back(item.second.second);\n-  }\n-\n-  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n-  return res;\n-}\n-\n-Value DotOpMmaV1ConversionHelper::loadB(\n-    Value tensor, const SharedMemoryObject &smemObj, Value thread, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter,\n-    ConversionPatternRewriter &rewriter, Type resultTy) const {\n-  // smem\n-  auto strides = smemObj.strides;\n-\n-  auto *ctx = rewriter.getContext();\n-  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-\n-  auto shape = tensorTy.getShape();\n-  auto order = sharedLayout.getOrder();\n-\n-  Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-  bool isBRow = order[0] != 0; // is row-major in shared memory layout\n-  // isBRow_ indicates whether B is row-major in DotOperand layout\n-  auto resultEncoding = resultTy.cast<RankedTensorType>()\n-                            .getEncoding()\n-                            .cast<DotOperandEncodingAttr>();\n-\n-  int vecB = sharedLayout.getVec();\n-  Value strideBN = isBRow ? i32_val(1) : strides[1];\n-  Value strideBK = isBRow ? strides[0] : i32_val(1);\n-  Value strideB0 = isBRow ? strideBN : strideBK;\n-  Value strideB1 = isBRow ? strideBK : strideBN;\n-  int strideRepN = wpt[1] * fpw[1] * 8;\n-  int strideRepK = 1;\n-\n-  auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n-      thread, false, isBRow, fpw, resultEncoding.getMMAv1ShapePerWarp(),\n-      resultEncoding.getMMAv1Rep(), rewriter, loc);\n-\n-  // swizzling\n-  int perPhaseB = sharedLayout.getPerPhase();\n-  int maxPhaseB = sharedLayout.getMaxPhase();\n-  int stepB0 = isBRow ? strideRepN : strideRepK;\n-  int numPtrB = std::max(2 * perPhaseB * maxPhaseB / stepB0, 1);\n-  int NK = shape[0];\n-\n-  Value offB0 = isBRow ? offsetBN : offsetBK;\n-  Value offB1 = isBRow ? offsetBK : offsetBN;\n-  Value phaseB = urem(udiv(offB1, i32_val(perPhaseB)), i32_val(maxPhaseB));\n-  Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-\n-  offB0 = add(offB0, cSwizzleOffset);\n-  SmallVector<Value> offB(numPtrB);\n-  for (int i = 0; i < numPtrB; ++i) {\n-    Value offB0I = add(offB0, i32_val(i * (isBRow ? strideRepN : 4)));\n-    offB0I = udiv(offB0I, i32_val(vecB));\n-    offB0I = xor_(offB0I, phaseB);\n-    offB0I = mul(offB0I, i32_val(vecB));\n-    offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n-  }\n-\n-  Type elemPtrTy = ptr_ty(f16_ty, 3);\n-  Type elemX2Ty = vec_ty(f16_ty, 2);\n-  if (tensorTy.getElementType().isBF16()) {\n-    elemPtrTy = ptr_ty(i16_ty, 3);\n-    elemX2Ty = vec_ty(i16_ty, 2);\n-  }\n-\n-  SmallVector<Value> ptrB(numPtrB);\n-  ValueTable hbs;\n-  for (int i = 0; i < numPtrB; ++i)\n-    ptrB[i] = gep(ptr_ty(f16_ty, 3), smem, offB[i]);\n-\n-  auto ld = [&](decltype(hbs) &vals, int m, int k, Value val0, Value val1) {\n-    vals[{m, k}] = {val0, val1};\n-  };\n-\n-  auto loadB = [&](int n, int K) {\n-    int offidx = (isBRow ? n : K / 4) % numPtrB;\n-    Value thePtrB = ptrB[offidx];\n-\n-    int stepBN = isBRow ? n / numPtrB * numPtrB : n;\n-    int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n-    Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n-                       mul(i32_val(stepBK), strideBK));\n-    Value pb = gep(elemPtrTy, thePtrB, offset);\n-\n-    Value hb =\n-        load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n-    // record lds that needs to be moved\n-    Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n-    Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n-    ld(hbs, n, K, hb00, hb01);\n-    if (vecB > 4) {\n-      Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n-      Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n-      if (isBRow)\n-        ld(hbs, n + 1, K, hb10, hb11);\n-      else\n-        ld(hbs, n, K + 4, hb10, hb11);\n-    }\n-  };\n-\n-  bool isBRow_ = resultEncoding.getMMAv1IsRow();\n-  assert(isBRow == isBRow_ && \"B need smem isRow\");\n-  bool isBVec4 = resultEncoding.getMMAv1IsVec4();\n-  unsigned numN = resultEncoding.getMMAv1NumOuter(shape);\n-  for (unsigned k = 0; k < NK; k += 4)\n-    for (unsigned n = 0; n < numN / 2; ++n) {\n-      if (!hbs.count({n, k}))\n-        loadB(n, k);\n-    }\n-\n-  SmallVector<Value> elems;\n-  for (auto &item : hbs) { // has is a map, the key should be ordered.\n-    elems.push_back(item.second.first);\n-    elems.push_back(item.second.second);\n-  }\n-\n-  Value res = typeConverter->packLLElements(loc, elems, rewriter, resultTy);\n-  return res;\n-}\n-\n-std::tuple<Value, Value, Value, Value>\n-DotOpMmaV1ConversionHelper::computeOffsets(Value threadId, bool isARow,\n-                                           bool isBRow, ArrayRef<int> fpw,\n-                                           ArrayRef<int> spw, ArrayRef<int> rep,\n-                                           ConversionPatternRewriter &rewriter,\n-                                           Location loc) const {\n-  auto *ctx = rewriter.getContext();\n-  Value _1 = i32_val(1);\n-  Value _3 = i32_val(3);\n-  Value _4 = i32_val(4);\n-  Value _16 = i32_val(16);\n-  Value _32 = i32_val(32);\n-\n-  Value lane = urem(threadId, _32);\n-  Value warp = udiv(threadId, _32);\n-\n-  // warp offset\n-  Value warp0 = urem(warp, i32_val(wpt[0]));\n-  Value warp12 = udiv(warp, i32_val(wpt[0]));\n-  Value warp1 = urem(warp12, i32_val(wpt[1]));\n-  Value warpMOff = mul(warp0, i32_val(spw[0]));\n-  Value warpNOff = mul(warp1, i32_val(spw[1]));\n-  // Quad offset\n-  Value quadMOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[0]));\n-  Value quadNOff = mul(udiv(and_(lane, _16), _4), i32_val(fpw[1]));\n-  // Pair offset\n-  Value pairMOff = udiv(urem(lane, _16), _4);\n-  pairMOff = urem(pairMOff, i32_val(fpw[0]));\n-  pairMOff = mul(pairMOff, _4);\n-  Value pairNOff = udiv(urem(lane, _16), _4);\n-  pairNOff = udiv(pairNOff, i32_val(fpw[0]));\n-  pairNOff = urem(pairNOff, i32_val(fpw[1]));\n-  pairNOff = mul(pairNOff, _4);\n-  // scale\n-  pairMOff = mul(pairMOff, i32_val(rep[0] / 2));\n-  quadMOff = mul(quadMOff, i32_val(rep[0] / 2));\n-  pairNOff = mul(pairNOff, i32_val(rep[1] / 2));\n-  quadNOff = mul(quadNOff, i32_val(rep[1] / 2));\n-  // Quad pair offset\n-  Value laneMOff = add(pairMOff, quadMOff);\n-  Value laneNOff = add(pairNOff, quadNOff);\n-  // A offset\n-  Value offsetAM = add(warpMOff, laneMOff);\n-  Value offsetAK = and_(lane, _3);\n-  // B offset\n-  Value offsetBN = add(warpNOff, laneNOff);\n-  Value offsetBK = and_(lane, _3);\n-  // i indices\n-  Value offsetCM = add(and_(lane, _1), offsetAM);\n-  if (isARow) {\n-    offsetAM = add(offsetAM, urem(threadId, _4));\n-    offsetAK = i32_val(0);\n-  }\n-  if (!isBRow) {\n-    offsetBN = add(offsetBN, urem(threadId, _4));\n-    offsetBK = i32_val(0);\n-  }\n-\n-  return std::make_tuple(offsetAM, offsetAK, offsetBN, offsetBK);\n-}\n-\n-DotOpMmaV1ConversionHelper::ValueTable\n-DotOpMmaV1ConversionHelper::extractLoadedOperand(\n-    Value llStruct, int NK, ConversionPatternRewriter &rewriter,\n-    TritonGPUToLLVMTypeConverter *typeConverter, Type type) const {\n-  ValueTable rcds;\n-  SmallVector<Value> elems = typeConverter->unpackLLElements(\n-      llStruct.getLoc(), llStruct, rewriter, type);\n-\n-  int offset = 0;\n-  for (int i = 0; offset < elems.size(); ++i) {\n-    for (int k = 0; k < NK; k += 4) {\n-      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n-      offset += 2;\n-    }\n-  }\n-\n-  return rcds;\n-}\n-\n-// TODO: Mostly a duplicate of TritonGPUToLLVMBase::emitBaseIndexforMMaLayoutV1\n-SmallVector<DotOpMmaV1ConversionHelper::CoordTy>\n-DotOpMmaV1ConversionHelper::getMNCoords(Value thread,\n-                                        ConversionPatternRewriter &rewriter,\n-                                        ArrayRef<unsigned int> wpt,\n-                                        const MmaEncodingAttr &mmaLayout,\n-                                        ArrayRef<int64_t> shape, bool isARow,\n-                                        bool isBRow, bool isAVec4,\n-                                        bool isBVec4) {\n-\n-  auto *ctx = thread.getContext();\n-  auto loc = UnknownLoc::get(ctx);\n-  Value _1 = i32_val(1);\n-  Value _2 = i32_val(2);\n-  Value _4 = i32_val(4);\n-  Value _16 = i32_val(16);\n-  Value _32 = i32_val(32);\n-  Value _fpw0 = i32_val(fpw[0]);\n-  Value _fpw1 = i32_val(fpw[1]);\n-\n-  // A info\n-  auto aEncoding = DotOperandEncodingAttr::get(ctx, 0, mmaLayout);\n-  auto aRep = aEncoding.getMMAv1Rep();\n-  auto aSpw = aEncoding.getMMAv1ShapePerWarp();\n-  // B info\n-  auto bEncoding = DotOperandEncodingAttr::get(ctx, 1, mmaLayout);\n-  auto bSpw = bEncoding.getMMAv1ShapePerWarp();\n-  auto bRep = bEncoding.getMMAv1Rep();\n-\n-  SmallVector<int, 2> rep({aRep[0], bRep[1]});\n-  SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n-  SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n-\n-  Value lane = urem(thread, _32);\n-  Value warp = udiv(thread, _32);\n-\n-  Value warp0 = urem(warp, i32_val(wpt[0]));\n-  Value warp12 = udiv(warp, i32_val(wpt[0]));\n-  Value warp1 = urem(warp12, i32_val(wpt[1]));\n-\n-  // warp offset\n-  Value offWarpM = mul(warp0, i32_val(spw[0]));\n-  Value offWarpN = mul(warp1, i32_val(spw[1]));\n-  // quad offset\n-  Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n-  Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n-  // pair offset\n-  Value offPairM = udiv(urem(lane, _16), _4);\n-  offPairM = urem(offPairM, _fpw0);\n-  offPairM = mul(offPairM, _4);\n-  Value offPairN = udiv(urem(lane, _16), _4);\n-  offPairN = udiv(offPairN, _fpw0);\n-  offPairN = urem(offPairN, _fpw1);\n-  offPairN = mul(offPairN, _4);\n-\n-  // sclare\n-  offPairM = mul(offPairM, i32_val(rep[0] / 2));\n-  offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n-  offPairN = mul(offPairN, i32_val(rep[1] / 2));\n-  offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n-\n-  // quad pair offset\n-  Value offLaneM = add(offPairM, offQuadM);\n-  Value offLaneN = add(offPairN, offQuadN);\n-  // a, b offset\n-  Value offsetAM = add(offWarpM, offLaneM);\n-  Value offsetBN = add(offWarpN, offLaneN);\n-  // m indices\n-  Value offsetCM = add(and_(lane, _1), offsetAM);\n-  SmallVector<Value> idxM;\n-  for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n-    for (unsigned mm = 0; mm < rep[0]; ++mm)\n-      idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n-\n-  // n indices\n-  Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n-  SmallVector<Value> idxN;\n-  for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n-    for (int nn = 0; nn < rep[1]; ++nn) {\n-      idxN.push_back(add(\n-          offsetCN, i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1])));\n-      idxN.push_back(\n-          add(offsetCN,\n-              i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n-    }\n-  }\n-\n-  SmallVector<SmallVector<Value>> axes({idxM, idxN});\n-\n-  // product the axis M and axis N to get coords, ported from\n-  // generator::init_idx method from triton2.0\n-\n-  // TODO[Superjomn]: check the order.\n-  SmallVector<CoordTy> coords;\n-  for (Value x1 : axes[1]) {   // N\n-    for (Value x0 : axes[0]) { // M\n-      SmallVector<Value, 2> idx(2);\n-      idx[0] = x0; // M\n-      idx[1] = x1; // N\n-      coords.push_back(std::move(idx));\n-    }\n-  }\n-\n-  return coords; // {M,N} in row-major\n-}\n-\n-std::tuple<int, int>\n-DotOpMmaV2ConversionHelper::getRepMN(const RankedTensorType &tensorTy) {\n-  auto mmaLayout = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-  auto wpt = mmaLayout.getWarpsPerCTA();\n-\n-  int M = tensorTy.getShape()[0];\n-  int N = tensorTy.getShape()[1];\n-  auto [instrM, instrN] = getInstrShapeMN();\n-  int repM = std::max<int>(M / (wpt[0] * instrM), 1);\n-  int repN = std::max<int>(N / (wpt[1] * instrN), 1);\n-  return {repM, repN};\n-}\n-\n-Type DotOpMmaV2ConversionHelper::getShemPtrTy() const {\n-  switch (mmaType) {\n-  case TensorCoreType::FP32_FP16_FP16_FP32:\n-    return ptr_ty(type::f16Ty(ctx), 3);\n-  case TensorCoreType::FP32_BF16_BF16_FP32:\n-    return ptr_ty(type::i16Ty(ctx), 3);\n-  case TensorCoreType::FP32_TF32_TF32_FP32:\n-    return ptr_ty(type::f32Ty(ctx), 3);\n-  case TensorCoreType::FP16_FP16_FP16_FP16:\n-    return ptr_ty(type::f16Ty(ctx), 3);\n-  case TensorCoreType::INT32_INT8_INT8_INT32:\n-    return ptr_ty(type::i8Ty(ctx), 3);\n-  default:\n-    llvm::report_fatal_error(\"mma16816 data type not supported\");\n-  }\n-  return Type{};\n-}\n-\n-Type DotOpMmaV2ConversionHelper::getMatType() const {\n-  // floating point types\n-  Type fp32x1Ty = vec_ty(type::f32Ty(ctx), 1);\n-  Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n-  Type i16x2Ty = vec_ty(type::i16Ty(ctx), 2);\n-  Type fp16x2Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp16x2Ty));\n-  // LLVM 14.0 does not support bf16 type, so we use i16 instead.\n-  Type bf16x2Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i16x2Ty));\n-  Type fp32Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32x1Ty));\n-  // integer types\n-  Type i8x4Ty = vec_ty(type::i8Ty(ctx), 4);\n-  Type i8x4Pack4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i8x4Ty));\n-\n-  switch (mmaType) {\n-  case TensorCoreType::FP32_FP16_FP16_FP32:\n-    return fp16x2Pack4Ty;\n-  case TensorCoreType::FP16_FP16_FP16_FP16:\n-    return fp16x2Pack4Ty;\n-  case TensorCoreType::FP32_BF16_BF16_FP32:\n-    return bf16x2Pack4Ty;\n-  case TensorCoreType::FP32_TF32_TF32_FP32:\n-    return fp32Pack4Ty;\n-  case TensorCoreType::INT32_INT8_INT8_INT32:\n-    return i8x4Pack4Ty;\n-  default:\n-    llvm::report_fatal_error(\"Unsupported mma type found\");\n-  }\n-\n-  return Type{};\n-}\n-\n-Type DotOpMmaV2ConversionHelper::getLoadElemTy() {\n-  switch (mmaType) {\n-  case TensorCoreType::FP32_FP16_FP16_FP32:\n-    return vec_ty(type::f16Ty(ctx), 2);\n-  case TensorCoreType::FP16_FP16_FP16_FP16:\n-    return vec_ty(type::f16Ty(ctx), 2);\n-  case TensorCoreType::FP32_BF16_BF16_FP32:\n-    return vec_ty(type::bf16Ty(ctx), 2);\n-  case TensorCoreType::FP32_TF32_TF32_FP32:\n-    return type::f32Ty(ctx);\n-  case TensorCoreType::INT32_INT8_INT8_INT32:\n-    return type::i32Ty(ctx);\n-  default:\n-    llvm::report_fatal_error(\"Unsupported mma type found\");\n-  }\n-\n-  return Type{};\n-}\n-\n-Type DotOpMmaV2ConversionHelper::getMmaRetType() const {\n-  Type fp32Ty = type::f32Ty(ctx);\n-  Type fp16Ty = type::f16Ty(ctx);\n-  Type i32Ty = type::i32Ty(ctx);\n-  Type fp32x4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n-  Type i32x4Ty =\n-      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n-  Type fp16x2Pack2Ty = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(2, vec_ty(fp16Ty, 2)));\n-  switch (mmaType) {\n-  case TensorCoreType::FP32_FP16_FP16_FP32:\n-    return fp32x4Ty;\n-  case TensorCoreType::FP16_FP16_FP16_FP16:\n-    return fp16x2Pack2Ty;\n-  case TensorCoreType::FP32_BF16_BF16_FP32:\n-    return fp32x4Ty;\n-  case TensorCoreType::FP32_TF32_TF32_FP32:\n-    return fp32x4Ty;\n-  case TensorCoreType::INT32_INT8_INT8_INT32:\n-    return i32x4Ty;\n-  default:\n-    llvm::report_fatal_error(\"Unsupported mma type found\");\n-  }\n-\n-  return Type{};\n-}\n-\n-int DotOpMmaV2ConversionHelper::getMmaRetSize() const {\n-  switch (mmaType) {\n-  case TensorCoreType::FP32_FP16_FP16_FP32:\n-    return 4;\n-  case TensorCoreType::FP16_FP16_FP16_FP16:\n-    return 2;\n-  case TensorCoreType::FP32_BF16_BF16_FP32:\n-    return 4;\n-  case TensorCoreType::FP32_TF32_TF32_FP32:\n-    return 4;\n-  case TensorCoreType::INT32_INT8_INT8_INT32:\n-    return 4;\n-  default:\n-    llvm::report_fatal_error(\"Unsupported mma type found\");\n-  }\n-\n-  return 4;\n-}\n-\n-DotOpMmaV2ConversionHelper::TensorCoreType\n-DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(Type operandTy) {\n-  auto tensorTy = operandTy.cast<RankedTensorType>();\n-  auto elemTy = tensorTy.getElementType();\n-  if (elemTy.isF16())\n-    return TensorCoreType::FP32_FP16_FP16_FP32;\n-  if (elemTy.isF32())\n-    return TensorCoreType::FP32_TF32_TF32_FP32;\n-  if (elemTy.isBF16())\n-    return TensorCoreType::FP32_BF16_BF16_FP32;\n-  if (elemTy.isInteger(8))\n-    return TensorCoreType::INT32_INT8_INT8_INT32;\n-  return TensorCoreType::NOT_APPLICABLE;\n-}\n-\n-DotOpMmaV2ConversionHelper::TensorCoreType\n-DotOpMmaV2ConversionHelper::getMmaType(triton::DotOp op) {\n-  Value A = op.getA();\n-  Value B = op.getB();\n-  auto aTy = A.getType().cast<RankedTensorType>();\n-  auto bTy = B.getType().cast<RankedTensorType>();\n-  // d = a*b + c\n-  auto dTy = op.getD().getType().cast<RankedTensorType>();\n-\n-  if (dTy.getElementType().isF32()) {\n-    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n-      return TensorCoreType::FP32_FP16_FP16_FP32;\n-    if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n-      return TensorCoreType::FP32_BF16_BF16_FP32;\n-    if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n-        op.getAllowTF32())\n-      return TensorCoreType::FP32_TF32_TF32_FP32;\n-  } else if (dTy.getElementType().isInteger(32)) {\n-    if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n-      return TensorCoreType::INT32_INT8_INT8_INT32;\n-  } else if (dTy.getElementType().isF16()) {\n-    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n-      return TensorCoreType::FP16_FP16_FP16_FP16;\n-  }\n-\n-  return TensorCoreType::NOT_APPLICABLE;\n-}\n-\n-SmallVector<Value>\n-MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n-                                           Value cSwizzleOffset) {\n-  // 4x4 matrices\n-  Value c = urem(lane, i32_val(8));\n-  Value s = udiv(lane, i32_val(8)); // sub-warp-id\n-\n-  // Decompose s => s_0, s_1, that is the coordinate in 2x2 matrices in a\n-  // warp\n-  Value s0 = urem(s, i32_val(2));\n-  Value s1 = udiv(s, i32_val(2));\n-\n-  // We use different orders for a and b for better performance.\n-  Value kMatArr = kOrder == 1 ? s1 : s0;\n-  Value nkMatArr = kOrder == 1 ? s0 : s1;\n-\n-  // matrix coordinate inside a CTA, the matrix layout is [2x2wpt] for A and\n-  // [2wptx2] for B. e.g. Setting wpt=3, The data layout for A(kOrder=1) is\n-  //   |0 0 1 1 2 2| -> 0,1,2 are the warpids\n-  //   |0 0 1 1 2 2|\n-  //\n-  // for B(kOrder=0) is\n-  //   |0 0|  -> 0,1,2 are the warpids\n-  //   |1 1|\n-  //   |2 2|\n-  //   |0 0|\n-  //   |1 1|\n-  //   |2 2|\n-  // Note, for each warp, it handles a 2x2 matrices, that is the coordinate\n-  // address (s0,s1) annotates.\n-\n-  Value matOff[2];\n-  matOff[kOrder ^ 1] =\n-      add(mul(warpId, i32_val(warpOffStride)),   // warp offset\n-          mul(nkMatArr, i32_val(matArrStride))); // matrix offset inside a warp\n-  matOff[kOrder] = kMatArr;\n-\n-  // Physical offset (before swizzling)\n-  Value cMatOff = matOff[order[0]];\n-  Value sMatOff = matOff[order[1]];\n-  Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n-  cMatOff = add(cMatOff, cSwizzleMatOff);\n-\n-  // row offset inside a matrix, each matrix has 8 rows.\n-  Value sOffInMat = c;\n-\n-  SmallVector<Value> offs(numPtrs);\n-  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-  Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-  for (int i = 0; i < numPtrs; ++i) {\n-    Value cMatOffI = add(cMatOff, i32_val(i * pLoadStrideInMat));\n-    cMatOffI = xor_(cMatOffI, phase);\n-    offs[i] = add(mul(cMatOffI, i32_val(cMatShape)), mul(sOff, sStride));\n-  }\n-\n-  return offs;\n-}\n-\n-SmallVector<Value> MMA16816SmemLoader::computeB32MatOffs(Value warpOff,\n-                                                         Value lane,\n-                                                         Value cSwizzleOffset) {\n-  assert(needTrans && \"Only used in transpose mode.\");\n-  // Load tf32 matrices with lds32\n-  Value cOffInMat = udiv(lane, i32_val(4));\n-  Value sOffInMat = urem(lane, i32_val(4));\n-\n-  Value phase = urem(udiv(sOffInMat, i32_val(perPhase)), i32_val(maxPhase));\n-  SmallVector<Value> offs(numPtrs);\n-\n-  for (int mat = 0; mat < 4; ++mat) { // Load 4 mats each time\n-    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n-    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n-    if (kMatArrInt > 0) // we don't need pointers for k\n-      continue;\n-    Value kMatArr = i32_val(kMatArrInt);\n-    Value nkMatArr = i32_val(nkMatArrInt);\n-\n-    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n-                        mul(nkMatArr, i32_val(matArrStride)));\n-    Value cSwizzleMatOff = udiv(cSwizzleOffset, i32_val(cMatShape));\n-    cMatOff = add(cMatOff, cSwizzleMatOff);\n-\n-    Value sMatOff = kMatArr;\n-    Value sOff = add(sOffInMat, mul(sMatOff, i32_val(sMatShape)));\n-    // FIXME: (kOrder == 1?) is really dirty hack\n-    for (int i = 0; i < numPtrs / 2; ++i) {\n-      Value cMatOffI =\n-          add(cMatOff, i32_val(i * pLoadStrideInMat * (kOrder == 1 ? 1 : 2)));\n-      cMatOffI = xor_(cMatOffI, phase);\n-      Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n-      cOff = urem(cOff, i32_val(tileShape[order[0]]));\n-      sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-      offs[2 * i + nkMatArrInt] = add(cOff, mul(sOff, sStride));\n-    }\n-  }\n-  return offs;\n-}\n-\n-SmallVector<Value> MMA16816SmemLoader::computeB8MatOffs(Value warpOff,\n-                                                        Value lane,\n-                                                        Value cSwizzleOffset) {\n-  assert(needTrans && \"Only used in transpose mode.\");\n-  Value cOffInMat = udiv(lane, i32_val(4));\n-  Value sOffInMat =\n-      mul(urem(lane, i32_val(4)), i32_val(4)); // each thread load 4 cols\n-\n-  SmallVector<Value> offs(numPtrs);\n-  for (int mat = 0; mat < 4; ++mat) {\n-    int kMatArrInt = kOrder == 1 ? mat / 2 : mat % 2;\n-    int nkMatArrInt = kOrder == 1 ? mat % 2 : mat / 2;\n-    if (kMatArrInt > 0) // we don't need pointers for k\n-      continue;\n-    Value kMatArr = i32_val(kMatArrInt);\n-    Value nkMatArr = i32_val(nkMatArrInt);\n-\n-    Value cMatOff = add(mul(warpOff, i32_val(warpOffStride)),\n-                        mul(nkMatArr, i32_val(matArrStride)));\n-    Value sMatOff = kMatArr;\n-\n-    for (int loadx4Off = 0; loadx4Off < numPtrs / 8; ++loadx4Off) {\n-      for (int elemOff = 0; elemOff < 4; ++elemOff) {\n-        int ptrOff = loadx4Off * 8 + nkMatArrInt * 4 + elemOff;\n-        Value cMatOffI = add(cMatOff, i32_val(loadx4Off * pLoadStrideInMat *\n-                                              (kOrder == 1 ? 1 : 2)));\n-        Value sOffInMatElem = add(sOffInMat, i32_val(elemOff));\n-\n-        // disable swizzling ...\n-\n-        Value cOff = add(cOffInMat, mul(cMatOffI, i32_val(cMatShape)));\n-        Value sOff = add(sOffInMatElem, mul(sMatOff, i32_val(sMatShape)));\n-        // To prevent out-of-bound access when tile is too small.\n-        cOff = urem(cOff, i32_val(tileShape[order[0]]));\n-        sOff = urem(sOff, i32_val(tileShape[order[1]]));\n-        offs[ptrOff] = add(cOff, mul(sOff, sStride));\n-      }\n-    }\n-  }\n-  return offs;\n-}\n-\n-std::tuple<Value, Value, Value, Value>\n-MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> offs,\n-                           ArrayRef<Value> ptrs, Type matTy,\n-                           Type shemPtrTy) const {\n-  assert(mat0 % 2 == 0 && mat1 % 2 == 0 && \"smem matrix load must be aligned\");\n-  int matIdx[2] = {mat0, mat1};\n-\n-  int ptrIdx{-1};\n-\n-  if (canUseLdmatrix)\n-    ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n-  else if (elemBytes == 4 && needTrans)\n-    ptrIdx = matIdx[order[0]];\n-  else if (elemBytes == 1 && needTrans)\n-    ptrIdx = matIdx[order[0]] * 4;\n-  else\n-    llvm::report_fatal_error(\"unsupported mma type found\");\n-\n-  // The main difference with the original triton code is we removed the\n-  // prefetch-related logic here for the upstream optimizer phase should\n-  // take care with it, and that is transparent in dot conversion.\n-  auto getPtr = [&](int idx) { return ptrs[idx]; };\n-\n-  Value ptr = getPtr(ptrIdx);\n-\n-  // The struct should have exactly the same element types.\n-  auto resTy = matTy.cast<LLVM::LLVMStructType>();\n-  Type elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n-\n-  // For some reasons, LLVM's NVPTX backend inserts unnecessary (?) integer\n-  // instructions to pack & unpack sub-word integers. A workaround is to\n-  // store the results of ldmatrix in i32\n-  if (auto vecElemTy = elemTy.dyn_cast<VectorType>()) {\n-    Type elemElemTy = vecElemTy.getElementType();\n-    if (auto intTy = elemElemTy.dyn_cast<IntegerType>()) {\n-      if (intTy.getWidth() <= 16) {\n-        elemTy = rewriter.getI32Type();\n-        resTy =\n-            LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, elemTy));\n-      }\n-    }\n-  }\n-\n-  if (canUseLdmatrix) {\n-    Value sOffset =\n-        mul(i32_val(matIdx[order[1]] * sMatStride * sMatShape), sStride);\n-    Value sOffsetPtr = gep(shemPtrTy, ptr, sOffset);\n-\n-    PTXBuilder builder;\n-    // ldmatrix.m8n8.x4 returns 4x2xfp16(that is 4xb32) elements for a\n-    // thread.\n-    auto resArgs = builder.newListOperand(4, \"=r\");\n-    auto addrArg = builder.newAddrOperand(sOffsetPtr, \"r\");\n-\n-    auto ldmatrix = builder.create(\"ldmatrix.sync.aligned.m8n8.x4\")\n-                        ->o(\"trans\", needTrans /*predicate*/)\n-                        .o(\"shared.b16\");\n-    ldmatrix(resArgs, addrArg);\n-\n-    // The result type is 4xi32, each i32 is composed of 2xf16\n-    // elements (adjacent two columns in a row) or a single f32 element.\n-    Value resV4 = builder.launch(rewriter, loc, resTy);\n-    return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n-            extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n-  } else if (elemBytes == 4 && needTrans) { // Use lds.32 to load tf32 matrices\n-    Value ptr2 = getPtr(ptrIdx + 1);\n-    assert(sMatStride == 1);\n-    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n-    int sOffsetArrElem = sMatStride * sMatShape;\n-    Value sOffsetArrElemVal =\n-        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n-\n-    Value elems[4];\n-    if (kOrder == 1) {\n-      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n-      elems[1] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n-      elems[2] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n-      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n-    } else {\n-      elems[0] = load(gep(shemPtrTy, ptr, sOffsetElemVal));\n-      elems[2] = load(gep(shemPtrTy, ptr2, sOffsetElemVal));\n-      elems[1] = load(gep(shemPtrTy, ptr, sOffsetArrElemVal));\n-      elems[3] = load(gep(shemPtrTy, ptr2, sOffsetArrElemVal));\n-    }\n-    std::array<Value, 4> retElems;\n-    retElems.fill(undef(elemTy));\n-    for (auto i = 0; i < 4; ++i) {\n-      retElems[i] = insert_element(elemTy, retElems[i], elems[i], i32_val(0));\n-    }\n-    return {retElems[0], retElems[1], retElems[2], retElems[3]};\n-  } else if (elemBytes == 1 && needTrans) { // work with int8\n-    // Can't use i32 here. Use LLVM's VectorType\n-    elemTy = matTy.cast<LLVM::LLVMStructType>().getBody()[0];\n-    std::array<std::array<Value, 4>, 2> ptrs;\n-    ptrs[0] = {\n-        getPtr(ptrIdx),\n-        getPtr(ptrIdx + 1),\n-        getPtr(ptrIdx + 2),\n-        getPtr(ptrIdx + 3),\n-    };\n-\n-    ptrs[1] = {\n-        getPtr(ptrIdx + 4),\n-        getPtr(ptrIdx + 5),\n-        getPtr(ptrIdx + 6),\n-        getPtr(ptrIdx + 7),\n-    };\n-\n-    assert(sMatStride == 1);\n-    int sOffsetElem = matIdx[order[1]] * (sMatStride * sMatShape);\n-    Value sOffsetElemVal = mul(i32_val(sOffsetElem), sStride);\n-    int sOffsetArrElem = 1 * (sMatStride * sMatShape);\n-    Value sOffsetArrElemVal =\n-        add(sOffsetElemVal, mul(i32_val(sOffsetArrElem), sStride));\n-\n-    std::array<Value, 4> i8v4Elems;\n-    i8v4Elems.fill(undef(elemTy));\n-\n-    Value i8Elems[4][4];\n-    if (kOrder == 1) {\n-      for (int i = 0; i < 2; ++i)\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[i][j] = load(gep(shemPtrTy, ptrs[i][j], sOffsetElemVal));\n-\n-      for (int i = 2; i < 4; ++i)\n-        for (int j = 0; j < 4; ++j)\n-          i8Elems[i][j] =\n-              load(gep(shemPtrTy, ptrs[i - 2][j], sOffsetArrElemVal));\n-\n-      for (int m = 0; m < 4; ++m) {\n-        for (int e = 0; e < 4; ++e)\n-          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n-                                        i8Elems[m][e], i32_val(e));\n-      }\n-    } else { // k first\n-      for (int j = 0; j < 4; ++j)\n-        i8Elems[0][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetElemVal));\n-      for (int j = 0; j < 4; ++j)\n-        i8Elems[2][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetElemVal));\n-      for (int j = 0; j < 4; ++j)\n-        i8Elems[1][j] = load(gep(shemPtrTy, ptrs[0][j], sOffsetArrElemVal));\n-      for (int j = 0; j < 4; ++j)\n-        i8Elems[3][j] = load(gep(shemPtrTy, ptrs[1][j], sOffsetArrElemVal));\n-\n-      for (int m = 0; m < 4; ++m) {\n-        for (int e = 0; e < 4; ++e)\n-          i8v4Elems[m] = insert_element(i8v4Elems[m].getType(), i8v4Elems[m],\n-                                        i8Elems[m][e], i32_val(e));\n-      }\n-    }\n-\n-    return {bitcast(i8v4Elems[0], i32_ty), bitcast(i8v4Elems[1], i32_ty),\n-            bitcast(i8v4Elems[2], i32_ty), bitcast(i8v4Elems[3], i32_ty)};\n-  }\n-\n-  assert(false && \"Invalid smem load\");\n-  return {Value{}, Value{}, Value{}, Value{}};\n-}\n-\n-MMA16816SmemLoader::MMA16816SmemLoader(\n-    int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n-    ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n-    ArrayRef<int> instrShape, ArrayRef<int> matShape, int perPhase,\n-    int maxPhase, int elemBytes, ConversionPatternRewriter &rewriter,\n-    TypeConverter *typeConverter, const Location &loc)\n-    : order(order.begin(), order.end()), kOrder(kOrder),\n-      tileShape(tileShape.begin(), tileShape.end()),\n-      instrShape(instrShape.begin(), instrShape.end()),\n-      matShape(matShape.begin(), matShape.end()), perPhase(perPhase),\n-      maxPhase(maxPhase), elemBytes(elemBytes), rewriter(rewriter), loc(loc),\n-      ctx(rewriter.getContext()) {\n-  cMatShape = matShape[order[0]];\n-  sMatShape = matShape[order[1]];\n-\n-  sStride = smemStrides[order[1]];\n-\n-  // rule: k must be the fast-changing axis.\n-  needTrans = kOrder != order[0];\n-  canUseLdmatrix = elemBytes == 2 || (!needTrans); // b16\n-\n-  if (canUseLdmatrix) {\n-    // Each CTA, the warps is arranged as [1xwpt] if not transposed,\n-    // otherwise [wptx1], and each warp will perform a mma.\n-    numPtrs =\n-        tileShape[order[0]] / (needTrans ? wpt : 1) / instrShape[order[0]];\n-  } else {\n-    numPtrs = tileShape[order[0]] / wpt / matShape[order[0]];\n-  }\n-  numPtrs = std::max<int>(numPtrs, 2);\n-\n-  // Special rule for i8/u8, 4 ptrs for each matrix\n-  if (!canUseLdmatrix && elemBytes == 1)\n-    numPtrs *= 4;\n-\n-  int loadStrideInMat[2];\n-  loadStrideInMat[kOrder] =\n-      2; // instrShape[kOrder] / matShape[kOrder], always 2\n-  loadStrideInMat[kOrder ^ 1] =\n-      wpt * (instrShape[kOrder ^ 1] / matShape[kOrder ^ 1]);\n-\n-  pLoadStrideInMat = loadStrideInMat[order[0]];\n-  sMatStride =\n-      loadStrideInMat[order[1]] / (instrShape[order[1]] / matShape[order[1]]);\n-\n-  // Each matArr contains warpOffStride matrices.\n-  matArrStride = kOrder == 1 ? 1 : wpt;\n-  warpOffStride = instrShape[kOrder ^ 1] / matShape[kOrder ^ 1];\n-}\n-Value MMA16816ConversionHelper::loadA(Value tensor,\n-                                      const SharedMemoryObject &smemObj) const {\n-  auto aTensorTy = tensor.getType().cast<RankedTensorType>();\n-\n-  SmallVector<int64_t> shape(aTensorTy.getShape().begin(),\n-                             aTensorTy.getShape().end());\n-\n-  ValueTable ha;\n-  std::function<void(int, int)> loadFn;\n-  auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(aTensorTy);\n-  auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(aTensorTy);\n-\n-  int numRepM = getNumRepM(aTensorTy, shape[0]);\n-  int numRepK = getNumRepK(aTensorTy, shape[1]);\n-\n-  if (aTensorTy.getEncoding().isa<SharedEncodingAttr>()) {\n-    Value warpM = getWarpM(shape[0]);\n-    // load from smem\n-    // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-    int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[0], shape[0] / 16);\n-    loadFn =\n-        getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 1 /*kOrder*/,\n-                        {mmaInstrM, mmaInstrK} /*instrShape*/,\n-                        {matShapeM, matShapeK} /*matShape*/, warpM /*warpId*/,\n-                        ha /*vals*/, true /*isA*/);\n-  } else if (aTensorTy.getEncoding().isa<BlockedEncodingAttr>()) {\n-    // load from registers, used in gemm fuse\n-    // TODO(Superjomn) Port the logic.\n-    assert(false && \"Loading A from register is not supported yet.\");\n-  } else {\n-    assert(false && \"A's layout is not supported.\");\n-  }\n-\n-  // step1. Perform loading.\n-  for (int m = 0; m < numRepM; ++m)\n-    for (int k = 0; k < numRepK; ++k)\n-      loadFn(2 * m, 2 * k);\n-\n-  // step2. Format the values to LLVM::Struct to passing to mma codegen.\n-  return composeValuesToDotOperandLayoutStruct(ha, numRepM, numRepK);\n-}\n-Value MMA16816ConversionHelper::loadB(Value tensor,\n-                                      const SharedMemoryObject &smemObj) {\n-  ValueTable hb;\n-  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-\n-  SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                             tensorTy.getShape().end());\n-\n-  // TODO[Superjomn]: transB cannot be accessed in ConvertLayoutOp.\n-  bool transB = false;\n-  if (transB) {\n-    std::swap(shape[0], shape[1]);\n-  }\n-\n-  auto [matShapeM, matShapeN, matShapeK] = getMmaMatShape(tensorTy);\n-  auto [mmaInstrM, mmaInstrN, mmaInstrK] = getMmaInstrShape(tensorTy);\n-  int numRepK = getNumRepK(tensorTy, shape[0]);\n-  int numRepN = getNumRepN(tensorTy, shape[1]);\n-\n-  Value warpN = getWarpN(shape[1]);\n-  // we use ldmatrix.x4 so each warp processes 16x16 elements.\n-  int wpt = std::min<int>(mmaLayout.getWarpsPerCTA()[1], shape[1] / 16);\n-  auto loadFn =\n-      getLoadMatrixFn(tensor, smemObj, mmaLayout, wpt /*wpt*/, 0 /*kOrder*/,\n-                      {mmaInstrK, mmaInstrN} /*instrShape*/,\n-                      {matShapeK, matShapeN} /*matShape*/, warpN /*warpId*/,\n-                      hb /*vals*/, false /*isA*/);\n-\n-  for (int n = 0; n < std::max(numRepN / 2, 1); ++n) {\n-    for (int k = 0; k < numRepK; ++k)\n-      loadFn(2 * n, 2 * k);\n-  }\n-\n-  Value result = composeValuesToDotOperandLayoutStruct(\n-      hb, std::max(numRepN / 2, 1), numRepK);\n-  return result;\n-}\n-Value MMA16816ConversionHelper::loadC(Value tensor, Value llTensor) const {\n-  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-  auto [repM, repN] = DotOpMmaV2ConversionHelper::getRepMN(tensorTy);\n-  size_t fcSize = 4 * repM * repN;\n-\n-  assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n-         \"Currently, we only support $c with a mma layout.\");\n-  // Load a normal C tensor with mma layout, that should be a\n-  // LLVM::struct with fcSize elements.\n-  auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n-  assert(structTy.getBody().size() == fcSize &&\n-         \"DotOp's $c operand should pass the same number of values as $d in \"\n-         \"mma layout.\");\n-\n-  auto numMmaRets = helper.getMmaRetSize();\n-  assert(numMmaRets == 4 || numMmaRets == 2);\n-  if (numMmaRets == 4) {\n-    return llTensor;\n-  } else if (numMmaRets == 2) {\n-    auto cPack = SmallVector<Value>();\n-    auto cElemTy = tensorTy.getElementType();\n-    int numCPackedElem = 4 / numMmaRets;\n-    Type cPackTy = vec_ty(cElemTy, numCPackedElem);\n-    for (int i = 0; i < fcSize; i += numCPackedElem) {\n-      Value pack = rewriter.create<LLVM::UndefOp>(loc, cPackTy);\n-      for (int j = 0; j < numCPackedElem; ++j) {\n-        pack = insert_element(\n-            cPackTy, pack, extract_val(cElemTy, llTensor, i + j), i32_val(j));\n-      }\n-      cPack.push_back(pack);\n-    }\n-\n-    Type structTy = LLVM::LLVMStructType::getLiteral(\n-        ctx, SmallVector<Type>(cPack.size(), cPackTy));\n-    Value result =\n-        typeConverter->packLLElements(loc, cPack, rewriter, structTy);\n-    return result;\n-  }\n-  return llTensor;\n-}\n-LogicalResult MMA16816ConversionHelper::convertDot(Value a, Value b, Value c,\n-                                                   Value d, Value loadedA,\n-                                                   Value loadedB, Value loadedC,\n-                                                   DotOp op,\n-                                                   DotOpAdaptor adaptor) const {\n-  helper.deduceMmaType(op);\n-\n-  auto aTensorTy = a.getType().cast<RankedTensorType>();\n-  auto bTensorTy = b.getType().cast<RankedTensorType>();\n-  auto dTensorTy = d.getType().cast<RankedTensorType>();\n-\n-  SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n-                              aTensorTy.getShape().end());\n-\n-  auto dShape = dTensorTy.getShape();\n-\n-  // shape / shape_per_cta\n-  int numRepM = getNumRepM(aTensorTy, dShape[0]);\n-  int numRepN = getNumRepN(aTensorTy, dShape[1]);\n-  int numRepK = getNumRepK(aTensorTy, aShape[1]);\n-\n-  ValueTable ha =\n-      getValuesFromDotOperandLayoutStruct(loadedA, numRepM, numRepK, aTensorTy);\n-  ValueTable hb = getValuesFromDotOperandLayoutStruct(\n-      loadedB, std::max(numRepN / 2, 1), numRepK, bTensorTy);\n-  auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n-  auto numMmaRets = helper.getMmaRetSize();\n-  int numCPackedElem = 4 / numMmaRets;\n-\n-  auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n-    unsigned colsPerThread = numRepN * 2;\n-    PTXBuilder builder;\n-    auto &mma = *builder.create(helper.getMmaInstr().str());\n-    // using =r for float32 works but leads to less readable ptx.\n-    bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n-    bool isAccF16 = dTensorTy.getElementType().isF16();\n-    auto retArgs =\n-        builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n-    auto aArgs = builder.newListOperand({\n-        {ha[{m, k}], \"r\"},\n-        {ha[{m + 1, k}], \"r\"},\n-        {ha[{m, k + 1}], \"r\"},\n-        {ha[{m + 1, k + 1}], \"r\"},\n-    });\n-    auto bArgs =\n-        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n-    auto cArgs = builder.newListOperand();\n-    for (int i = 0; i < numMmaRets; ++i) {\n-      cArgs->listAppend(builder.newOperand(\n-          fc[(m * colsPerThread + 4 * n) / numCPackedElem + i],\n-          std::to_string(i)));\n-      // reuse the output registers\n-    }\n-\n-    mma(retArgs, aArgs, bArgs, cArgs);\n-    Value mmaOut = builder.launch(rewriter, loc, helper.getMmaRetType());\n-\n-    Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n-    for (int i = 0; i < numMmaRets; ++i) {\n-      fc[(m * colsPerThread + 4 * n) / numCPackedElem + i] =\n-          extract_val(elemTy, mmaOut, i);\n-    }\n-  };\n-\n-  for (int k = 0; k < numRepK; ++k)\n-    for (int m = 0; m < numRepM; ++m)\n-      for (int n = 0; n < numRepN; ++n)\n-        callMma(2 * m, n, 2 * k);\n-\n-  Type resElemTy = dTensorTy.getElementType();\n-\n-  // replace with new packed result\n-  Type structTy = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(fc.size() * numCPackedElem, resElemTy));\n-  SmallVector<Value> results(fc.size() * numCPackedElem);\n-  for (int i = 0; i < fc.size(); ++i) {\n-    for (int j = 0; j < numCPackedElem; ++j) {\n-      results[i * numCPackedElem + j] =\n-          numCPackedElem > 1\n-              ? bitcast(extract_element(fc[i], i32_val(j)), resElemTy)\n-              : bitcast(fc[i], resElemTy);\n-    }\n-  }\n-  Value res = typeConverter->packLLElements(loc, results, rewriter, structTy);\n-  rewriter.replaceOp(op, res);\n-\n-  return success();\n-}\n-std::function<void(int, int)> MMA16816ConversionHelper::getLoadMatrixFn(\n-    Value tensor, const SharedMemoryObject &smemObj, MmaEncodingAttr mmaLayout,\n-    int wpt, uint32_t kOrder, SmallVector<int> instrShape,\n-    SmallVector<int> matShape, Value warpId,\n-    MMA16816ConversionHelper::ValueTable &vals, bool isA) const {\n-  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n-  // We assumes that the input operand of Dot should be from shared layout.\n-  // TODO(Superjomn) Consider other layouts if needed later.\n-  auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  const int perPhase = sharedLayout.getPerPhase();\n-  const int maxPhase = sharedLayout.getMaxPhase();\n-  const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n-  auto order = sharedLayout.getOrder();\n-\n-  // the original register_lds2, but discard the prefetch logic.\n-  auto ld2 = [](ValueTable &vals, int mn, int k, Value val) {\n-    vals[{mn, k}] = val;\n-  };\n-\n-  // (a, b) is the coordinate.\n-  auto load = [=, &vals, &ld2](int a, int b) {\n-    MMA16816SmemLoader loader(\n-        wpt, sharedLayout.getOrder(), kOrder, smemObj.strides,\n-        tensorTy.getShape() /*tileShape*/, instrShape, matShape, perPhase,\n-        maxPhase, elemBytes, rewriter, typeConverter, loc);\n-    Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n-    SmallVector<Value> offs =\n-        loader.computeOffsets(warpId, lane, cSwizzleOffset);\n-    const int numPtrs = loader.getNumPtrs();\n-    SmallVector<Value> ptrs(numPtrs);\n-\n-    Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-\n-    Type smemPtrTy = helper.getShemPtrTy();\n-    for (int i = 0; i < numPtrs; ++i) {\n-      ptrs[i] =\n-          bitcast(gep(smemPtrTy, smemBase, ValueRange({offs[i]})), smemPtrTy);\n-    }\n-\n-    auto [ha0, ha1, ha2, ha3] = loader.loadX4(\n-        (kOrder == 1) ? a : b /*mat0*/, (kOrder == 1) ? b : a /*mat1*/, offs,\n-        ptrs, helper.getMatType(), helper.getShemPtrTy());\n-\n-    if (isA) {\n-      ld2(vals, a, b, ha0);\n-      ld2(vals, a + 1, b, ha1);\n-      ld2(vals, a, b + 1, ha2);\n-      ld2(vals, a + 1, b + 1, ha3);\n-    } else {\n-      ld2(vals, a, b, ha0);\n-      ld2(vals, a + 1, b, ha2);\n-      ld2(vals, a, b + 1, ha1);\n-      ld2(vals, a + 1, b + 1, ha3);\n-    }\n-  };\n-\n-  return load;\n-}\n-Value MMA16816ConversionHelper::composeValuesToDotOperandLayoutStruct(\n-    const MMA16816ConversionHelper::ValueTable &vals, int n0, int n1) const {\n-  std::vector<Value> elems;\n-  for (int m = 0; m < n0; ++m)\n-    for (int k = 0; k < n1; ++k) {\n-      elems.push_back(vals.at({2 * m, 2 * k}));\n-      elems.push_back(vals.at({2 * m, 2 * k + 1}));\n-      elems.push_back(vals.at({2 * m + 1, 2 * k}));\n-      elems.push_back(vals.at({2 * m + 1, 2 * k + 1}));\n-    }\n-\n-  assert(!elems.empty());\n-\n-  Type elemTy = elems[0].getType();\n-  Type structTy = LLVM::LLVMStructType::getLiteral(\n-      ctx, SmallVector<Type>(elems.size(), elemTy));\n-  auto result = typeConverter->packLLElements(loc, elems, rewriter, structTy);\n-  return result;\n-}\n-MMA16816ConversionHelper::ValueTable\n-MMA16816ConversionHelper::getValuesFromDotOperandLayoutStruct(Value value,\n-                                                              int n0, int n1,\n-                                                              Type type) const {\n-  auto elems = typeConverter->unpackLLElements(loc, value, rewriter, type);\n-\n-  int offset{};\n-  ValueTable vals;\n-  for (int i = 0; i < n0; ++i) {\n-    for (int j = 0; j < n1; j++) {\n-      vals[{2 * i, 2 * j}] = elems[offset++];\n-      vals[{2 * i, 2 * j + 1}] = elems[offset++];\n-      vals[{2 * i + 1, 2 * j}] = elems[offset++];\n-      vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n-    }\n-  }\n-  return vals;\n-}\n-SmallVector<Value> DotOpFMAConversionHelper::getThreadIds(\n-    Value threadId, ArrayRef<unsigned int> shapePerCTA,\n-    ArrayRef<unsigned int> sizePerThread, ArrayRef<unsigned int> order,\n-    ConversionPatternRewriter &rewriter, Location loc) const {\n-  int dim = order.size();\n-  SmallVector<Value> threadIds(dim);\n-  for (unsigned k = 0; k < dim - 1; k++) {\n-    Value dimK = i32_val(shapePerCTA[order[k]] / sizePerThread[order[k]]);\n-    Value rem = urem(threadId, dimK);\n-    threadId = udiv(threadId, dimK);\n-    threadIds[order[k]] = rem;\n-  }\n-  Value dimK = i32_val(shapePerCTA[order[dim - 1]]);\n-  threadIds[order[dim - 1]] = urem(threadId, dimK);\n-  return threadIds;\n-}\n-Value DotOpFMAConversionHelper::loadA(\n-    Value A, Value llA, BlockedEncodingAttr dLayout, Value thread, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter,\n-    ConversionPatternRewriter &rewriter) const {\n-  auto aTensorTy = A.getType().cast<RankedTensorType>();\n-  auto aLayout = aTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto aShape = aTensorTy.getShape();\n-\n-  auto aOrder = aLayout.getOrder();\n-  auto order = dLayout.getOrder();\n-\n-  bool isARow = aOrder[0] == 1;\n-\n-  auto aSmem = getSharedMemoryObjectFromStruct(loc, llA, rewriter);\n-  Value strideAM = aSmem.strides[0];\n-  Value strideAK = aSmem.strides[1];\n-  Value strideA0 = isARow ? strideAK : strideAM;\n-  Value strideA1 = isARow ? strideAM : strideAK;\n-  int aNumPtr = 8;\n-  int K = aShape[1];\n-  int M = aShape[0];\n-\n-  auto shapePerCTA = getShapePerCTA(dLayout);\n-  auto sizePerThread = getSizePerThread(dLayout);\n-\n-  Value _0 = i32_val(0);\n-\n-  Value mContig = i32_val(sizePerThread[order[1]]);\n-\n-  // threadId in blocked layout\n-  auto threadIds =\n-      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n-  Value threadIdM = threadIds[0];\n-\n-  Value offA0 = isARow ? _0 : mul(threadIdM, mContig);\n-  Value offA1 = isARow ? mul(threadIdM, mContig) : _0;\n-  SmallVector<Value> aOff(aNumPtr);\n-  for (int i = 0; i < aNumPtr; ++i) {\n-    aOff[i] = add(mul(offA0, strideA0), mul(offA1, strideA1));\n-  }\n-  auto elemTy = A.getType().cast<RankedTensorType>().getElementType();\n-\n-  Type ptrTy = ptr_ty(elemTy, 3);\n-  SmallVector<Value> aPtrs(aNumPtr);\n-  for (int i = 0; i < aNumPtr; ++i)\n-    aPtrs[i] = gep(ptrTy, aSmem.base, aOff[i]);\n-\n-  SmallVector<Value> vas;\n-\n-  int mShapePerCTA = getShapePerCTAForMN(dLayout, true /*isM*/);\n-  int mSizePerThread = getSizePerThreadForMN(dLayout, true /*isM*/);\n-\n-  for (unsigned k = 0; k < K; ++k)\n-    for (unsigned m = 0; m < M; m += mShapePerCTA)\n-      for (unsigned mm = 0; mm < mSizePerThread; ++mm) {\n-        Value offset =\n-            add(mul(i32_val(m + mm), strideAM), mul(i32_val(k), strideAK));\n-        Value pa = gep(ptrTy, aPtrs[0], offset);\n-        Value va = load(pa);\n-        vas.emplace_back(va);\n-      }\n-\n-  return getStructFromValueTable(vas, rewriter, loc, typeConverter, elemTy);\n-}\n-Value DotOpFMAConversionHelper::loadB(\n-    Value B, Value llB, BlockedEncodingAttr dLayout, Value thread, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter,\n-    ConversionPatternRewriter &rewriter) const {\n-  auto bTensorTy = B.getType().cast<RankedTensorType>();\n-  auto bLayout = bTensorTy.getEncoding().cast<SharedEncodingAttr>();\n-  auto bShape = bTensorTy.getShape();\n-\n-  auto bOrder = bLayout.getOrder();\n-  auto order = dLayout.getOrder();\n-\n-  bool isBRow = bOrder[0] == 1;\n-\n-  auto bSmem = getSharedMemoryObjectFromStruct(loc, llB, rewriter);\n-  Value strideBN = bSmem.strides[1];\n-  Value strideBK = bSmem.strides[0];\n-  Value strideB0 = isBRow ? strideBN : strideBK;\n-  Value strideB1 = isBRow ? strideBK : strideBN;\n-  int bNumPtr = 8;\n-  int K = bShape[0];\n-  int N = bShape[1];\n-\n-  auto shapePerCTA = getShapePerCTA(dLayout);\n-  auto sizePerThread = getSizePerThread(dLayout);\n-\n-  Value _0 = i32_val(0);\n-\n-  Value nContig = i32_val(sizePerThread[order[0]]);\n-\n-  // threadId in blocked layout\n-  auto threadIds =\n-      getThreadIds(thread, shapePerCTA, sizePerThread, order, rewriter, loc);\n-  Value threadIdN = threadIds[1];\n-\n-  Value offB0 = isBRow ? mul(threadIdN, nContig) : _0;\n-  Value offB1 = isBRow ? _0 : mul(threadIdN, nContig);\n-  SmallVector<Value> bOff(bNumPtr);\n-  for (int i = 0; i < bNumPtr; ++i) {\n-    bOff[i] = add(mul(offB0, strideB0), mul(offB1, strideB1));\n-  }\n-  auto elemTy = B.getType().cast<RankedTensorType>().getElementType();\n-\n-  Type ptrTy = ptr_ty(elemTy, 3);\n-  SmallVector<Value> bPtrs(bNumPtr);\n-  for (int i = 0; i < bNumPtr; ++i)\n-    bPtrs[i] = gep(ptrTy, bSmem.base, bOff[i]);\n-\n-  SmallVector<Value> vbs;\n-\n-  int nShapePerCTA = getShapePerCTAForMN(dLayout, false /*isM*/);\n-  int nSizePerThread = getSizePerThreadForMN(dLayout, false /*isM*/);\n-\n-  for (unsigned k = 0; k < K; ++k)\n-    for (unsigned n = 0; n < N; n += nShapePerCTA)\n-      for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n-        Value offset =\n-            add(mul(i32_val(n + nn), strideBN), mul(i32_val(k), strideBK));\n-        Value pb = gep(ptrTy, bPtrs[0], offset);\n-        Value vb = load(pb);\n-        vbs.emplace_back(vb);\n-      }\n-\n-  return getStructFromValueTable(vbs, rewriter, loc, typeConverter, elemTy);\n-}\n-DotOpFMAConversionHelper::ValueTable\n-DotOpFMAConversionHelper::getValueTableFromStruct(\n-    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n-    ConversionPatternRewriter &rewriter, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter, Type type) const {\n-  ValueTable res;\n-  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n-  int index = 0;\n-  for (unsigned k = 0; k < K; ++k) {\n-    for (unsigned m = 0; m < n0; m += shapePerCTA)\n-      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n-        res[{m + mm, k}] = elems[index++];\n-      }\n-  }\n-  return res;\n-}\n-Value DotOpFMAConversionHelper::getStructFromValueTable(\n-    ArrayRef<Value> vals, ConversionPatternRewriter &rewriter, Location loc,\n-    TritonGPUToLLVMTypeConverter *typeConverter, Type elemTy) const {\n-  SmallVector<Type> elemTypes(vals.size(), elemTy);\n-  SmallVector<Value> elems;\n-  elems.reserve(vals.size());\n-  for (auto &val : vals) {\n-    elems.push_back(val);\n-  }\n-\n-  Type structTy = struct_ty(elemTypes);\n-  return typeConverter->packLLElements(loc, elems, rewriter, structTy);\n-}\n-int DotOpFMAConversionHelper::getNumElemsPerThread(\n-    ArrayRef<int64_t> shape, DotOperandEncodingAttr dotOpLayout) {\n-  auto blockedLayout = dotOpLayout.getParent().cast<BlockedEncodingAttr>();\n-  auto shapePerCTA = getShapePerCTA(blockedLayout);\n-  auto sizePerThread = getSizePerThread(blockedLayout);\n-\n-  // TODO[Superjomn]: we assume the k axis is fixed for $a and $b here, fix it\n-  // if not.\n-  int K = dotOpLayout.getOpIdx() == 0 ? shape[1] : shape[0];\n-  int otherDim = dotOpLayout.getOpIdx() == 1 ? shape[1] : shape[0];\n-\n-  bool isM = dotOpLayout.getOpIdx() == 0;\n-  int shapePerCTAMN = getShapePerCTAForMN(blockedLayout, isM);\n-  int sizePerThreadMN = getSizePerThreadForMN(blockedLayout, isM);\n-  return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n-}\n-} // namespace LLVM\n-} // namespace mlir"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "removed", "additions": 0, "deletions": 594, "changes": 594, "file_content_changes": "@@ -1,594 +0,0 @@\n-#ifndef TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_OP_HELPERS_H\n-#define TRITON_CONVERSION_TRITONGPU_TO_LLVM_DOT_OP_HELPERS_H\n-\n-#include \"mlir/Analysis/SliceAnalysis.h\"\n-#include \"mlir/Conversion/ArithToLLVM/ArithToLLVM.h\"\n-#include \"mlir/Conversion/GPUToNVVM/GPUToNVVMPass.h\"\n-#include \"mlir/Conversion/LLVMCommon/LoweringOptions.h\"\n-#include \"mlir/Conversion/LLVMCommon/Pattern.h\"\n-#include \"mlir/Conversion/MathToLLVM/MathToLLVM.h\"\n-#include \"mlir/Dialect/Arith/IR/Arith.h\"\n-#include \"mlir/Dialect/GPU/IR/GPUDialect.h\"\n-#include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n-#include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n-#include \"mlir/IR/Matchers.h\"\n-#include \"mlir/IR/TypeUtilities.h\"\n-#include \"mlir/Transforms/DialectConversion.h\"\n-#include \"triton/Analysis/AxisInfo.h\"\n-#include \"triton/Analysis/Utility.h\"\n-#include \"triton/Conversion/MLIRTypes.h\"\n-#include \"triton/Conversion/TritonGPUToLLVM/PTXAsmFormat.h\"\n-#include \"triton/Dialect/Triton/IR/Dialect.h\"\n-#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n-#include \"llvm/Support/Format.h\"\n-#include \"llvm/Support/FormatVariadic.h\"\n-\n-#include \"Utility.h\"\n-\n-class TritonGPUToLLVMTypeConverter;\n-\n-namespace mlir {\n-namespace LLVM {\n-using namespace mlir::triton;\n-using ::mlir::triton::gpu::BlockedEncodingAttr;\n-using ::mlir::triton::gpu::DotOperandEncodingAttr;\n-using ::mlir::triton::gpu::MmaEncodingAttr;\n-using ::mlir::triton::gpu::SharedEncodingAttr;\n-\n-// Helper for conversion of DotOp with mma<version=1>, that is sm<80\n-struct DotOpMmaV1ConversionHelper {\n-  MmaEncodingAttr mmaLayout;\n-  ArrayRef<unsigned> wpt;\n-  static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n-\n-  using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n-\n-  explicit DotOpMmaV1ConversionHelper(MmaEncodingAttr mmaLayout)\n-      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n-\n-  static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n-\n-  static Type getMmaRetType(TensorType operand) {\n-    auto *ctx = operand.getContext();\n-    Type fp32Ty = type::f32Ty(ctx);\n-    // f16*f16+f32->f32\n-    return struct_ty(SmallVector<Type>{8, fp32Ty});\n-  }\n-\n-  static Type getMatType(TensorType operand) {\n-    auto *ctx = operand.getContext();\n-    Type fp16Ty = type::f16Ty(ctx);\n-    Type vecTy = vec_ty(fp16Ty, 2);\n-    return struct_ty(SmallVector<Type>{vecTy});\n-  }\n-\n-  // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, TritonGPUToLLVMTypeConverter *converter,\n-              ConversionPatternRewriter &rewriter, Type resultTy) const;\n-\n-  // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n-              Location loc, TritonGPUToLLVMTypeConverter *converter,\n-              ConversionPatternRewriter &rewriter, Type resultTy) const;\n-\n-  static ArrayRef<unsigned> getOrder() { return mmaOrder; }\n-\n-  // Compute the offset of the matrix to load.\n-  // Returns offsetAM, offsetAK, offsetBN, offsetBK.\n-  // NOTE, the information M(from $a) and N(from $b) couldn't be retrieved at\n-  // the same time in the usage in convert_layout[shared->dot_op], we leave\n-  // the noexist info to be 0 and only use the desired argument from the\n-  // composed result. In this way we want to retain the original code\n-  // structure in convert_mma884 method for easier debugging.\n-  std::tuple<Value, Value, Value, Value>\n-  computeOffsets(Value threadId, bool isARow, bool isBRow, ArrayRef<int> fpw,\n-                 ArrayRef<int> spw, ArrayRef<int> rep,\n-                 ConversionPatternRewriter &rewriter, Location loc) const;\n-\n-  // Extract values belong to $a or $b from a LLVMStruct, the shape is n0xn1.\n-  DotOpMmaV1ConversionHelper::ValueTable extractLoadedOperand(\n-      Value llStruct, int NK, ConversionPatternRewriter &rewriter,\n-      TritonGPUToLLVMTypeConverter *typeConverter, Type type) const;\n-\n-  using CoordTy = SmallVector<Value>;\n-  // Get the coordinates(m,n) of the elements emit by a thread in accumulator.\n-  static SmallVector<CoordTy>\n-  getMNCoords(Value thread, ConversionPatternRewriter &rewriter,\n-              ArrayRef<unsigned> wpt, const MmaEncodingAttr &mmaLayout,\n-              ArrayRef<int64_t> shape, bool isARow, bool isBRow, bool isAVec4,\n-              bool isBVec4);\n-\n-  // \\param elemId the offset of the element in a thread\n-  static CoordTy getCoord(int elemId, ArrayRef<CoordTy> coords) {\n-    return coords[elemId];\n-  }\n-\n-private:\n-  static constexpr unsigned instrShape[] = {16, 16, 4};\n-  static constexpr unsigned mmaOrder[] = {0, 1};\n-};\n-\n-// Helper for conversion of DotOp with mma<version=2>, that is sm>=80\n-struct DotOpMmaV2ConversionHelper {\n-  enum class TensorCoreType : uint8_t {\n-    // floating-point tensor core instr\n-    FP32_FP16_FP16_FP32 = 0, // default\n-    FP32_BF16_BF16_FP32,\n-    FP32_TF32_TF32_FP32,\n-    FP16_FP16_FP16_FP16,\n-    // integer tensor core instr\n-    INT32_INT1_INT1_INT32, // Not implemented\n-    INT32_INT4_INT4_INT32, // Not implemented\n-    INT32_INT8_INT8_INT32, // Not implemented\n-    //\n-    NOT_APPLICABLE,\n-  };\n-\n-  MmaEncodingAttr mmaLayout;\n-  MLIRContext *ctx{};\n-\n-  explicit DotOpMmaV2ConversionHelper(MmaEncodingAttr mmaLayout)\n-      : mmaLayout(mmaLayout) {\n-    ctx = mmaLayout.getContext();\n-  }\n-\n-  void deduceMmaType(DotOp op) const { mmaType = getMmaType(op); }\n-  void deduceMmaType(Type operandTy) const {\n-    mmaType = getTensorCoreTypeFromOperand(operandTy);\n-  }\n-\n-  // Get the M and N of mma instruction shape.\n-  static std::tuple<int, int> getInstrShapeMN() {\n-    // According to DotOpConversionHelper::mmaInstrShape, all the M,N are\n-    // {16,8}\n-    return {16, 8};\n-  }\n-\n-  static std::tuple<int, int> getRepMN(const RankedTensorType &tensorTy);\n-\n-  Type getShemPtrTy() const;\n-\n-  // The type of matrix that loaded by either a ldmatrix or composed lds.\n-  Type getMatType() const;\n-\n-  Type getLoadElemTy();\n-\n-  Type getMmaRetType() const;\n-\n-  int getMmaRetSize() const;\n-\n-  ArrayRef<int> getMmaInstrShape() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrShape.at(mmaType);\n-  }\n-\n-  static ArrayRef<int> getMmaInstrShape(TensorCoreType tensorCoreType) {\n-    assert(tensorCoreType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrShape.at(tensorCoreType);\n-  }\n-\n-  ArrayRef<int> getMmaMatShape() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaMatShape.at(mmaType);\n-  }\n-\n-  // Deduce the TensorCoreType from either $a or $b's type.\n-  // TODO: both the input type ($a or $b) and output type ($c or $d) are\n-  // needed to differentiate TensorCoreType::FP32_FP16_FP16_FP32 from\n-  // TensorCoreType::FP16_FP16_FP16_FP16\n-  static TensorCoreType getTensorCoreTypeFromOperand(Type operandTy);\n-\n-  int getVec() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrVec.at(mmaType);\n-  }\n-\n-  StringRef getMmaInstr() const {\n-    assert(mmaType != TensorCoreType::NOT_APPLICABLE &&\n-           \"Unknown mma type found.\");\n-    return mmaInstrPtx.at(mmaType);\n-  }\n-\n-  static TensorCoreType getMmaType(triton::DotOp op);\n-\n-private:\n-  mutable TensorCoreType mmaType{TensorCoreType::NOT_APPLICABLE};\n-\n-  // Used on nvidia GPUs mma layout .version == 2\n-  // Refer to\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-storage\n-  // for more details.\n-  inline static const std::map<TensorCoreType, llvm::SmallVector<int>>\n-      mmaInstrShape = {\n-          {TensorCoreType::FP32_FP16_FP16_FP32, {16, 8, 16}},\n-          {TensorCoreType::FP32_BF16_BF16_FP32, {16, 8, 16}},\n-          {TensorCoreType::FP32_TF32_TF32_FP32, {16, 8, 8}},\n-          {TensorCoreType::FP16_FP16_FP16_FP16, {16, 8, 16}},\n-\n-          {TensorCoreType::INT32_INT1_INT1_INT32, {16, 8, 256}},\n-          {TensorCoreType::INT32_INT4_INT4_INT32, {16, 8, 64}},\n-          {TensorCoreType::INT32_INT8_INT8_INT32, {16, 8, 32}},\n-  };\n-\n-  // shape of matrices loaded by ldmatrix (m-n-k, for mxk & kxn matrices)\n-  // Refer to\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix\n-  // for more details.\n-  inline static const std::map<TensorCoreType, llvm::SmallVector<int>>\n-      mmaMatShape = {\n-          {TensorCoreType::FP32_FP16_FP16_FP32, {8, 8, 8}},\n-          {TensorCoreType::FP32_BF16_BF16_FP32, {8, 8, 8}},\n-          {TensorCoreType::FP32_TF32_TF32_FP32, {8, 8, 4}},\n-          {TensorCoreType::FP16_FP16_FP16_FP16, {8, 8, 8}},\n-\n-          {TensorCoreType::INT32_INT1_INT1_INT32, {8, 8, 64}},\n-          {TensorCoreType::INT32_INT4_INT4_INT32, {8, 8, 32}},\n-          {TensorCoreType::INT32_INT8_INT8_INT32, {8, 8, 16}},\n-  };\n-\n-  // Supported mma instruction in PTX.\n-  // Refer to\n-  // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-for-mma\n-  // for more details.\n-  inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n-      {TensorCoreType::FP32_FP16_FP16_FP32,\n-       \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n-      {TensorCoreType::FP32_BF16_BF16_FP32,\n-       \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n-      {TensorCoreType::FP32_TF32_TF32_FP32,\n-       \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n-      {TensorCoreType::FP16_FP16_FP16_FP16,\n-       \"mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\"},\n-\n-      {TensorCoreType::INT32_INT1_INT1_INT32,\n-       \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n-      {TensorCoreType::INT32_INT4_INT4_INT32,\n-       \"mma.sync.aligned.m16n8k64.row.col.satfinite.s32.s4.s4.s32\"},\n-      {TensorCoreType::INT32_INT8_INT8_INT32,\n-       \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n-  };\n-\n-  // vector length per ldmatrix (16*8/element_size_in_bits)\n-  inline static const std::map<TensorCoreType, uint8_t> mmaInstrVec = {\n-      {TensorCoreType::FP32_FP16_FP16_FP32, 8},\n-      {TensorCoreType::FP32_BF16_BF16_FP32, 8},\n-      {TensorCoreType::FP32_TF32_TF32_FP32, 4},\n-      {TensorCoreType::FP16_FP16_FP16_FP16, 8},\n-\n-      {TensorCoreType::INT32_INT1_INT1_INT32, 128},\n-      {TensorCoreType::INT32_INT4_INT4_INT32, 32},\n-      {TensorCoreType::INT32_INT8_INT8_INT32, 16},\n-  };\n-};\n-\n-// Data loader for mma.16816 instruction.\n-class MMA16816SmemLoader {\n-public:\n-  MMA16816SmemLoader(int wpt, ArrayRef<uint32_t> order, uint32_t kOrder,\n-                     ArrayRef<Value> smemStrides, ArrayRef<int64_t> tileShape,\n-                     ArrayRef<int> instrShape, ArrayRef<int> matShape,\n-                     int perPhase, int maxPhase, int elemBytes,\n-                     ConversionPatternRewriter &rewriter,\n-                     TypeConverter *typeConverter, const Location &loc);\n-\n-  // lane = thread % 32\n-  // warpOff = (thread/32) % wpt(0)\n-  llvm::SmallVector<Value> computeOffsets(Value warpOff, Value lane,\n-                                          Value cSwizzleOffset) {\n-    if (canUseLdmatrix)\n-      return computeLdmatrixMatOffs(warpOff, lane, cSwizzleOffset);\n-    else if (elemBytes == 4 && needTrans)\n-      return computeB32MatOffs(warpOff, lane, cSwizzleOffset);\n-    else if (elemBytes == 1 && needTrans)\n-      return computeB8MatOffs(warpOff, lane, cSwizzleOffset);\n-    else\n-      llvm::report_fatal_error(\"Invalid smem load config\");\n-\n-    return {};\n-  }\n-\n-  int getNumPtrs() const { return numPtrs; }\n-\n-  // Compute the offset to the matrix this thread(indexed by warpOff and lane)\n-  // mapped to.\n-  SmallVector<Value> computeLdmatrixMatOffs(Value warpId, Value lane,\n-                                            Value cSwizzleOffset);\n-\n-  // Compute 32-bit matrix offsets.\n-  SmallVector<Value> computeB32MatOffs(Value warpOff, Value lane,\n-                                       Value cSwizzleOffset);\n-\n-  // compute 8-bit matrix offset.\n-  SmallVector<Value> computeB8MatOffs(Value warpOff, Value lane,\n-                                      Value cSwizzleOffset);\n-\n-  // Load 4 matrices and returns 4 vec<2> elements.\n-  std::tuple<Value, Value, Value, Value>\n-  loadX4(int mat0, int mat1, ArrayRef<Value> offs, ArrayRef<Value> ptrs,\n-         Type matTy, Type shemPtrTy) const;\n-\n-private:\n-  SmallVector<uint32_t> order;\n-  int kOrder;\n-  SmallVector<int64_t> tileShape;\n-  SmallVector<int> instrShape;\n-  SmallVector<int> matShape;\n-  int perPhase;\n-  int maxPhase;\n-  int elemBytes;\n-  ConversionPatternRewriter &rewriter;\n-  const Location &loc;\n-  MLIRContext *ctx{};\n-\n-  int cMatShape;\n-  int sMatShape;\n-\n-  Value sStride;\n-\n-  bool needTrans;\n-  bool canUseLdmatrix;\n-\n-  int numPtrs;\n-\n-  int pLoadStrideInMat;\n-  int sMatStride;\n-\n-  int matArrStride;\n-  int warpOffStride;\n-};\n-\n-// This class helps to adapt the existing DotOpConversion to the latest\n-// DotOpOperand layout design. It decouples the existing implementation to two\n-// parts:\n-// 1. loading the specific operand matrix(for $a, $b, $c) from smem\n-// 2. passing the loaded value and perform the mma codegen\n-struct MMA16816ConversionHelper {\n-  MmaEncodingAttr mmaLayout;\n-  ArrayRef<unsigned int> wpt;\n-  SmallVector<unsigned int> properWpt;\n-\n-  Value thread, lane, warp;\n-\n-  DotOpMmaV2ConversionHelper helper;\n-  ConversionPatternRewriter &rewriter;\n-  TritonGPUToLLVMTypeConverter *typeConverter;\n-  Location loc;\n-  MLIRContext *ctx{};\n-\n-  using ValueTable = std::map<std::pair<unsigned, unsigned>, Value>;\n-\n-  // dotOperand: type of either one operand of dotOp.\n-  MMA16816ConversionHelper(Type dotOperand, MmaEncodingAttr mmaLayout,\n-                           Value thread, ConversionPatternRewriter &rewriter,\n-                           TritonGPUToLLVMTypeConverter *typeConverter,\n-                           Location loc)\n-      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()), thread(thread),\n-        helper(mmaLayout), rewriter(rewriter), typeConverter(typeConverter),\n-        loc(loc), ctx(mmaLayout.getContext()) {\n-    helper.deduceMmaType(dotOperand);\n-\n-    Value _32 = i32_val(32);\n-    lane = urem(thread, _32);\n-    warp = udiv(thread, _32);\n-  }\n-\n-  MMA16816ConversionHelper(DotOp op, MmaEncodingAttr mmaLayout, Value thread,\n-                           ConversionPatternRewriter &rewriter,\n-                           TritonGPUToLLVMTypeConverter *typeConverter,\n-                           Location loc)\n-      : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()), thread(thread),\n-        helper(mmaLayout), rewriter(rewriter), typeConverter(typeConverter),\n-        loc(loc), ctx(mmaLayout.getContext()) {\n-    helper.deduceMmaType(op);\n-\n-    Value _32 = i32_val(32);\n-    lane = urem(thread, _32);\n-    warp = udiv(thread, _32);\n-  }\n-\n-  // Get a warpId for M axis.\n-  Value getWarpM(int M) const {\n-    auto matInstrShape = helper.getMmaInstrShape();\n-    return urem(urem(warp, i32_val(wpt[0])), i32_val(M / matInstrShape[0]));\n-  }\n-\n-  // Get a warpId for N axis.\n-  Value getWarpN(int N) const {\n-    auto matInstrShape = helper.getMmaInstrShape();\n-    Value warpMN = udiv(warp, i32_val(wpt[0]));\n-    return urem(urem(warpMN, i32_val(wpt[1])), i32_val(N / matInstrShape[1]));\n-  }\n-\n-  // Get the mmaInstrShape deducing either from $a or $b.\n-  std::tuple<int, int, int> getMmaInstrShape(Type operand) const {\n-    helper.deduceMmaType(operand);\n-    auto mmaInstrShape = helper.getMmaInstrShape();\n-    int mmaInstrM = mmaInstrShape[0];\n-    int mmaInstrN = mmaInstrShape[1];\n-    int mmaInstrK = mmaInstrShape[2];\n-    return std::make_tuple(mmaInstrM, mmaInstrN, mmaInstrK);\n-  }\n-\n-  // Get the mmaMatShape deducing either from $a or $b.\n-  std::tuple<int, int, int> getMmaMatShape(Type operand) const {\n-    helper.deduceMmaType(operand);\n-    auto matShape = helper.getMmaMatShape();\n-    int matShapeM = matShape[0];\n-    int matShapeN = matShape[1];\n-    int matShapeK = matShape[2];\n-    return std::make_tuple(matShapeM, matShapeN, matShapeK);\n-  }\n-\n-  // \\param operand is either $a or $b's type.\n-  inline int getNumRepM(Type operand, int M) const {\n-    return getNumRepM(operand, M, wpt[0]);\n-  }\n-\n-  // \\param operand is either $a or $b's type.\n-  inline int getNumRepN(Type operand, int N) const {\n-    return getNumRepN(operand, N, wpt[1]);\n-  }\n-\n-  // \\param operand is either $a or $b's type.\n-  inline int getNumRepK(Type operand, int K) const {\n-    return getNumRepK_(operand, K);\n-  }\n-\n-  static int getNumRepM(Type operand, int M, int wpt) {\n-    auto tensorCoreType =\n-        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrM =\n-        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[0];\n-    return std::max<int>(M / (wpt * mmaInstrM), 1);\n-  }\n-\n-  static int getNumRepN(Type operand, int N, int wpt) {\n-    auto tensorCoreType =\n-        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrN =\n-        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[1];\n-    return std::max<int>(N / (wpt * mmaInstrN), 1);\n-  }\n-\n-  static int getNumRepK_(Type operand, int K) {\n-    auto tensorCoreType =\n-        DotOpMmaV2ConversionHelper::getTensorCoreTypeFromOperand(operand);\n-    int mmaInstrK =\n-        DotOpMmaV2ConversionHelper::getMmaInstrShape(tensorCoreType)[2];\n-    return std::max<int>(K / mmaInstrK, 1);\n-  }\n-\n-  // Get number of elements per thread for $a operand.\n-  static size_t getANumElemsPerThread(RankedTensorType operand, int wpt) {\n-    auto shape = operand.getShape();\n-    int repM = getNumRepM(operand, shape[0], wpt);\n-    int repK = getNumRepK_(operand, shape[1]);\n-    return 4 * repM * repK;\n-  }\n-\n-  // Get number of elements per thread for $b operand.\n-  static size_t getBNumElemsPerThread(RankedTensorType operand, int wpt) {\n-    auto shape = operand.getShape();\n-    int repK = getNumRepK_(operand, shape[0]);\n-    int repN = getNumRepN(operand, shape[1], wpt);\n-    return 4 * std::max(repN / 2, 1) * repK;\n-  }\n-\n-  // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, const SharedMemoryObject &smemObj) const;\n-\n-  // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, const SharedMemoryObject &smemObj);\n-\n-  // Loading $c to registers, returns a Value.\n-  Value loadC(Value tensor, Value llTensor) const;\n-\n-  // Conduct the Dot conversion.\n-  // \\param a, \\param b, \\param c and \\param d are DotOp operands.\n-  // \\param loadedA, \\param loadedB, \\param loadedC, all of them are result of\n-  // loading.\n-  LogicalResult convertDot(Value a, Value b, Value c, Value d, Value loadedA,\n-                           Value loadedB, Value loadedC, DotOp op,\n-                           DotOpAdaptor adaptor) const;\n-\n-private:\n-  std::function<void(int, int)>\n-  getLoadMatrixFn(Value tensor, const SharedMemoryObject &smemObj,\n-                  MmaEncodingAttr mmaLayout, int wpt, uint32_t kOrder,\n-                  SmallVector<int> instrShape, SmallVector<int> matShape,\n-                  Value warpId, ValueTable &vals, bool isA) const;\n-\n-  // Compose a map of Values to a LLVM::Struct.\n-  // The layout is a list of Value with coordinate of (i,j), the order is as\n-  // the follows:\n-  // [\n-  //  (0,0), (0,1), (1,0), (1,1), # i=0, j=0\n-  //  (0,2), (0,3), (1,2), (1,3), # i=0, j=1\n-  //  (0,4), (0,5), (1,4), (1,5), # i=0, j=2\n-  //  ...\n-  //  (2,0), (2,1), (3,0), (3,1), # i=1, j=0\n-  //  (2,2), (2,3), (3,2), (3,3), # i=1, j=1\n-  //  (2,4), (2,5), (3,4), (3,5), # i=1, j=2\n-  //  ...\n-  // ]\n-  // i \\in [0, n0) and j \\in [0, n1)\n-  // There should be \\param n0 * \\param n1 elements in the output Struct.\n-  Value composeValuesToDotOperandLayoutStruct(const ValueTable &vals, int n0,\n-                                              int n1) const;\n-\n-  ValueTable getValuesFromDotOperandLayoutStruct(Value value, int n0, int n1,\n-                                                 Type type) const;\n-};\n-\n-// Helper for conversion of FMA DotOp.\n-struct DotOpFMAConversionHelper {\n-  Attribute layout;\n-  MLIRContext *ctx{};\n-\n-  using ValueTable = std::map<std::pair<int, int>, Value>;\n-\n-  explicit DotOpFMAConversionHelper(Attribute layout)\n-      : layout(layout), ctx(layout.getContext()) {}\n-\n-  SmallVector<Value>\n-  getThreadIds(Value threadId, ArrayRef<unsigned> shapePerCTA,\n-               ArrayRef<unsigned> sizePerThread, ArrayRef<unsigned> order,\n-               ConversionPatternRewriter &rewriter, Location loc) const;\n-\n-  Value loadA(Value A, Value llA, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n-              ConversionPatternRewriter &rewriter) const;\n-\n-  Value loadB(Value B, Value llB, BlockedEncodingAttr dLayout, Value thread,\n-              Location loc, TritonGPUToLLVMTypeConverter *typeConverter,\n-              ConversionPatternRewriter &rewriter) const;\n-\n-  ValueTable getValueTableFromStruct(\n-      Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n-      ConversionPatternRewriter &rewriter, Location loc,\n-      TritonGPUToLLVMTypeConverter *typeConverter, Type type) const;\n-\n-  Value getStructFromValueTable(ArrayRef<Value> vals,\n-                                ConversionPatternRewriter &rewriter,\n-                                Location loc,\n-                                TritonGPUToLLVMTypeConverter *typeConverter,\n-                                Type elemTy) const;\n-\n-  // get number of elements per thread for $a or $b.\n-  static int getNumElemsPerThread(ArrayRef<int64_t> shape,\n-                                  DotOperandEncodingAttr dotOpLayout);\n-\n-  // Get shapePerCTA for M or N axis.\n-  static int getShapePerCTAForMN(BlockedEncodingAttr layout, bool isM) {\n-    auto order = layout.getOrder();\n-    auto shapePerCTA = getShapePerCTA(layout);\n-\n-    int mShapePerCTA =\n-        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int nShapePerCTA =\n-        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    return isM ? mShapePerCTA : nShapePerCTA;\n-  }\n-\n-  // Get sizePerThread for M or N axis.\n-  static int getSizePerThreadForMN(BlockedEncodingAttr layout, bool isM) {\n-    auto order = layout.getOrder();\n-    auto sizePerThread = getSizePerThread(layout);\n-\n-    int mSizePerThread =\n-        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-    int nSizePerThread =\n-        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-    return isM ? mSizePerThread : nSizePerThread;\n-  }\n-};\n-\n-} // namespace LLVM\n-} // namespace mlir\n-\n-#endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 251, "changes": 267, "file_content_changes": "@@ -1,16 +1,24 @@\n #include \"DotOpToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n \n+LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter);\n+\n+LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter);\n+\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter);\n+\n struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n   using ConvertTritonGPUOpToLLVMPattern<\n       triton::DotOp>::ConvertTritonGPUOpToLLVMPattern;\n@@ -33,15 +41,10 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n                                     .getEncoding()\n                                     .dyn_cast<MmaEncodingAttr>();\n     if (!isOuter && mmaLayout && supportMMA(op, mmaLayout.getVersionMajor())) {\n-      if (mmaLayout.isVolta()) {\n-        if (D.getType().cast<RankedTensorType>().getElementType().isF16()) {\n-          llvm_unreachable(\n-              \"out_dtype=float16 for dot is not implemented on V100 yet\");\n-        }\n-        return convertMMA884(op, adaptor, rewriter);\n-      }\n+      if (mmaLayout.isVolta())\n+        return convertMMA884(op, adaptor, getTypeConverter(), rewriter);\n       if (mmaLayout.isAmpere())\n-        return convertMMA16816(op, adaptor, rewriter);\n+        return convertMMA16816(op, adaptor, getTypeConverter(), rewriter);\n \n       llvm::report_fatal_error(\n           \"Unsupported MMA kind found when converting DotOp to LLVM.\");\n@@ -51,249 +54,11 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n             .cast<RankedTensorType>()\n             .getEncoding()\n             .isa<BlockedEncodingAttr>())\n-      return convertFMADot(op, adaptor, rewriter);\n+      return convertFMADot(op, adaptor, getTypeConverter(), rewriter);\n \n     llvm::report_fatal_error(\n         \"Unsupported DotOp found when converting TritonGPU to LLVM.\");\n   }\n-\n-private:\n-  // Convert to mma.m16n8k16\n-  LogicalResult convertMMA16816(triton::DotOp op, OpAdaptor adaptor,\n-                                ConversionPatternRewriter &rewriter) const {\n-    auto loc = op.getLoc();\n-    auto mmaLayout = op.getResult()\n-                         .getType()\n-                         .cast<RankedTensorType>()\n-                         .getEncoding()\n-                         .cast<MmaEncodingAttr>();\n-\n-    Value A = op.getA();\n-    Value B = op.getB();\n-    Value C = op.getC();\n-\n-    MMA16816ConversionHelper mmaHelper(op, mmaLayout,\n-                                       getThreadId(rewriter, loc), rewriter,\n-                                       getTypeConverter(), loc);\n-\n-    auto ATensorTy = A.getType().cast<RankedTensorType>();\n-    auto BTensorTy = B.getType().cast<RankedTensorType>();\n-\n-    assert(ATensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n-           BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n-           \"Both $a and %b should be DotOperand layout.\");\n-\n-    Value loadedA, loadedB, loadedC;\n-    loadedA = adaptor.getA();\n-    loadedB = adaptor.getB();\n-    loadedC = mmaHelper.loadC(op.getC(), adaptor.getC());\n-\n-    return mmaHelper.convertDot(A, B, C, op.getD(), loadedA, loadedB, loadedC,\n-                                op, adaptor);\n-  }\n-  /// Convert to mma.m8n8k4\n-  LogicalResult convertMMA884(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    auto *ctx = op.getContext();\n-    auto loc = op.getLoc();\n-\n-    Value A = op.getA();\n-    Value B = op.getB();\n-    Value D = op.getResult();\n-    auto mmaLayout = D.getType()\n-                         .cast<RankedTensorType>()\n-                         .getEncoding()\n-                         .cast<MmaEncodingAttr>();\n-    auto ALayout = A.getType()\n-                       .cast<RankedTensorType>()\n-                       .getEncoding()\n-                       .cast<DotOperandEncodingAttr>();\n-    auto BLayout = B.getType()\n-                       .cast<RankedTensorType>()\n-                       .getEncoding()\n-                       .cast<DotOperandEncodingAttr>();\n-\n-    auto ATensorTy = A.getType().cast<RankedTensorType>();\n-    auto BTensorTy = B.getType().cast<RankedTensorType>();\n-    auto DTensorTy = D.getType().cast<RankedTensorType>();\n-    auto AShape = ATensorTy.getShape();\n-    auto BShape = BTensorTy.getShape();\n-\n-    bool isARow = ALayout.getMMAv1IsRow();\n-    bool isBRow = BLayout.getMMAv1IsRow();\n-    auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n-        mmaLayout.decodeVoltaLayoutStates();\n-    assert(isARow == isARow_);\n-    assert(isBRow == isBRow_);\n-\n-    DotOpMmaV1ConversionHelper helper(mmaLayout);\n-\n-    unsigned numM = ALayout.getMMAv1NumOuter(AShape);\n-    unsigned numN = BLayout.getMMAv1NumOuter(BShape);\n-    unsigned NK = AShape[1];\n-\n-    auto has = helper.extractLoadedOperand(adaptor.getA(), NK, rewriter,\n-                                           getTypeConverter(), ATensorTy);\n-    auto hbs = helper.extractLoadedOperand(adaptor.getB(), NK, rewriter,\n-                                           getTypeConverter(), BTensorTy);\n-\n-    // Initialize accumulators with external values, the acc holds the\n-    // accumulator value that is shared between the MMA instructions inside a\n-    // DotOp, we can call the order of the values the accumulator-internal\n-    // order.\n-    SmallVector<Value> acc = getTypeConverter()->unpackLLElements(\n-        loc, adaptor.getC(), rewriter, DTensorTy);\n-    size_t resSize = acc.size();\n-\n-    // The resVals holds the final result of the DotOp.\n-    // NOTE The current order of resVals is different from acc, we call it the\n-    // accumulator-external order. and\n-    SmallVector<Value> resVals(resSize);\n-\n-    auto getIdx = [&](int m, int n) {\n-      std::vector<size_t> idx{{\n-          (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n-          (m * 2 + 0) + (n * 4 + 1) * numM,\n-          (m * 2 + 1) + (n * 4 + 0) * numM, // row1\n-          (m * 2 + 1) + (n * 4 + 1) * numM,\n-          (m * 2 + 0) + (n * 4 + 2) * numM, // row2\n-          (m * 2 + 0) + (n * 4 + 3) * numM,\n-          (m * 2 + 1) + (n * 4 + 2) * numM, // row3\n-          (m * 2 + 1) + (n * 4 + 3) * numM,\n-      }};\n-      return idx;\n-    };\n-\n-    auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n-      auto ha = has.at({m, k});\n-      auto hb = hbs.at({n, k});\n-\n-      PTXBuilder builder;\n-      auto idx = getIdx(m, n);\n-\n-      // note: using \"=f\" for float leads to cleaner PTX\n-      bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n-      auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n-      auto *AOprs = builder.newListOperand({\n-          {ha.first, \"r\"},\n-          {ha.second, \"r\"},\n-      });\n-\n-      auto *BOprs = builder.newListOperand({\n-          {hb.first, \"r\"},\n-          {hb.second, \"r\"},\n-      });\n-      auto *COprs = builder.newListOperand();\n-      for (int i = 0; i < 8; ++i)\n-        COprs->listAppend(builder.newOperand(acc[idx[i]], std::to_string(i)));\n-\n-      auto mma = builder.create(\"mma.sync.aligned.m8n8k4\")\n-                     ->o(isARow ? \"row\" : \"col\")\n-                     .o(isBRow ? \"row\" : \"col\")\n-                     .o(\"f32.f16.f16.f32\");\n-\n-      mma(resOprs, AOprs, BOprs, COprs);\n-\n-      Value res =\n-          builder.launch(rewriter, loc, helper.getMmaRetType(ATensorTy));\n-\n-      for (auto i = 0; i < 8; i++) {\n-        Value elem = extract_val(f32_ty, res, i);\n-        acc[idx[i]] = elem;\n-      }\n-    };\n-\n-    for (unsigned k = 0; k < NK; k += 4)\n-      for (unsigned m = 0; m < numM / 2; ++m)\n-        for (unsigned n = 0; n < numN / 2; ++n) {\n-          callMMA(m, n, k);\n-        }\n-\n-    // res holds the same layout of acc\n-    for (size_t i = 0; i < acc.size(); ++i) {\n-      resVals[i] = acc[i];\n-    }\n-\n-    Value res =\n-        getTypeConverter()->packLLElements(loc, resVals, rewriter, DTensorTy);\n-    rewriter.replaceOp(op, res);\n-    return success();\n-  }\n-\n-  LogicalResult convertFMADot(triton::DotOp op, OpAdaptor adaptor,\n-                              ConversionPatternRewriter &rewriter) const {\n-    auto *ctx = rewriter.getContext();\n-    auto loc = op.getLoc();\n-\n-    auto A = op.getA();\n-    auto B = op.getB();\n-    auto C = op.getC();\n-    auto D = op.getResult();\n-\n-    auto aTensorTy = A.getType().cast<RankedTensorType>();\n-    auto bTensorTy = B.getType().cast<RankedTensorType>();\n-    auto dTensorTy = D.getType().cast<RankedTensorType>();\n-\n-    auto aShape = aTensorTy.getShape();\n-    auto bShape = bTensorTy.getShape();\n-\n-    BlockedEncodingAttr dLayout =\n-        dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n-    auto order = dLayout.getOrder();\n-    auto cc = getTypeConverter()->unpackLLElements(loc, adaptor.getC(),\n-                                                   rewriter, dTensorTy);\n-\n-    DotOpFMAConversionHelper helper(dLayout);\n-    Value llA = adaptor.getA();\n-    Value llB = adaptor.getB();\n-\n-    auto sizePerThread = getSizePerThread(dLayout);\n-    auto shapePerCTA = getShapePerCTA(dLayout);\n-\n-    int K = aShape[1];\n-    int M = aShape[0];\n-    int N = bShape[1];\n-\n-    int mShapePerCTA =\n-        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int mSizePerThread =\n-        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-    int nShapePerCTA =\n-        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n-    int nSizePerThread =\n-        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n-\n-    auto has = helper.getValueTableFromStruct(llA, K, M, mShapePerCTA,\n-                                              mSizePerThread, rewriter, loc,\n-                                              getTypeConverter(), aTensorTy);\n-    auto hbs = helper.getValueTableFromStruct(llB, K, N, nShapePerCTA,\n-                                              nSizePerThread, rewriter, loc,\n-                                              getTypeConverter(), bTensorTy);\n-\n-    SmallVector<Value> ret = cc;\n-    bool isCRow = order[0] == 1;\n-\n-    for (unsigned k = 0; k < K; k++) {\n-      for (unsigned m = 0; m < M; m += mShapePerCTA)\n-        for (unsigned n = 0; n < N; n += nShapePerCTA)\n-          for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n-            for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n-              int mIdx = m / mShapePerCTA * mSizePerThread + mm;\n-              int nIdx = n / nShapePerCTA * nSizePerThread + nn;\n-\n-              int z = isCRow ? mIdx * N / nShapePerCTA * mSizePerThread + nIdx\n-                             : nIdx * M / mShapePerCTA * nSizePerThread + mIdx;\n-              ret[z] = rewriter.create<LLVM::FMulAddOp>(\n-                  loc, has[{m + mm, k}], hbs[{n + nn, k}], ret[z]);\n-            }\n-    }\n-\n-    auto res =\n-        getTypeConverter()->packLLElements(loc, ret, rewriter, dTensorTy);\n-    rewriter.replaceOp(op, res);\n-\n-    return success();\n-  }\n };\n \n void populateDotOpToLLVMPatterns(TritonGPUToLLVMTypeConverter &typeConverter,"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/FMA.cpp", "status": "added", "additions": 100, "deletions": 0, "changes": 100, "file_content_changes": "@@ -0,0 +1,100 @@\n+#include \"DotOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTableFMA = std::map<std::pair<int, int>, Value>;\n+\n+static ValueTableFMA getValueTableFromStructFMA(\n+    Value val, int K, int n0, int shapePerCTA, int sizePerThread,\n+    ConversionPatternRewriter &rewriter, Location loc,\n+    TritonGPUToLLVMTypeConverter *typeConverter, Type type) {\n+  ValueTableFMA res;\n+  auto elems = typeConverter->unpackLLElements(loc, val, rewriter, type);\n+  int index = 0;\n+  for (unsigned k = 0; k < K; ++k) {\n+    for (unsigned m = 0; m < n0; m += shapePerCTA)\n+      for (unsigned mm = 0; mm < sizePerThread; ++mm) {\n+        res[{m + mm, k}] = elems[index++];\n+      }\n+  }\n+  return res;\n+}\n+\n+LogicalResult convertFMADot(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter) {\n+  auto *ctx = rewriter.getContext();\n+  auto loc = op.getLoc();\n+\n+  auto A = op.getA();\n+  auto B = op.getB();\n+  auto C = op.getC();\n+  auto D = op.getResult();\n+\n+  auto aTensorTy = A.getType().cast<RankedTensorType>();\n+  auto bTensorTy = B.getType().cast<RankedTensorType>();\n+  auto dTensorTy = D.getType().cast<RankedTensorType>();\n+\n+  auto aShape = aTensorTy.getShape();\n+  auto bShape = bTensorTy.getShape();\n+\n+  BlockedEncodingAttr dLayout =\n+      dTensorTy.getEncoding().cast<BlockedEncodingAttr>();\n+  auto order = dLayout.getOrder();\n+  auto cc =\n+      typeConverter->unpackLLElements(loc, adaptor.getC(), rewriter, dTensorTy);\n+\n+  Value llA = adaptor.getA();\n+  Value llB = adaptor.getB();\n+\n+  auto sizePerThread = getSizePerThread(dLayout);\n+  auto shapePerCTA = getShapePerCTA(dLayout);\n+\n+  int K = aShape[1];\n+  int M = aShape[0];\n+  int N = bShape[1];\n+\n+  int mShapePerCTA =\n+      order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int mSizePerThread =\n+      order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+  int nShapePerCTA =\n+      order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+  int nSizePerThread =\n+      order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+\n+  auto has =\n+      getValueTableFromStructFMA(llA, K, M, mShapePerCTA, mSizePerThread,\n+                                 rewriter, loc, typeConverter, aTensorTy);\n+  auto hbs =\n+      getValueTableFromStructFMA(llB, K, N, nShapePerCTA, nSizePerThread,\n+                                 rewriter, loc, typeConverter, bTensorTy);\n+\n+  SmallVector<Value> ret = cc;\n+  bool isCRow = order[0] == 1;\n+\n+  for (unsigned k = 0; k < K; k++) {\n+    for (unsigned m = 0; m < M; m += mShapePerCTA)\n+      for (unsigned n = 0; n < N; n += nShapePerCTA)\n+        for (unsigned mm = 0; mm < mSizePerThread; ++mm)\n+          for (unsigned nn = 0; nn < nSizePerThread; ++nn) {\n+            int mIdx = m / mShapePerCTA * mSizePerThread + mm;\n+            int nIdx = n / nShapePerCTA * nSizePerThread + nn;\n+\n+            int z = isCRow ? mIdx * N / nShapePerCTA * mSizePerThread + nIdx\n+                           : nIdx * M / mShapePerCTA * nSizePerThread + mIdx;\n+            ret[z] = rewriter.create<LLVM::FMulAddOp>(loc, has[{m + mm, k}],\n+                                                      hbs[{n + nn, k}], ret[z]);\n+          }\n+  }\n+\n+  auto res = typeConverter->packLLElements(loc, ret, rewriter, dTensorTy);\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv1.cpp", "status": "added", "additions": 161, "deletions": 0, "changes": 161, "file_content_changes": "@@ -0,0 +1,161 @@\n+#include \"DotOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTable = std::map<std::pair<int, int>, std::pair<Value, Value>>;\n+\n+static Type getMmaRetType(TensorType operand) {\n+  auto *ctx = operand.getContext();\n+  Type fp32Ty = type::f32Ty(ctx);\n+  // f16*f16+f32->f32\n+  return struct_ty(SmallVector<Type>{8, fp32Ty});\n+}\n+\n+static ValueTable\n+extractLoadedOperand(Value llStruct, int NK,\n+                     ConversionPatternRewriter &rewriter,\n+                     TritonGPUToLLVMTypeConverter *typeConverter, Type type) {\n+  ValueTable rcds;\n+  SmallVector<Value> elems = typeConverter->unpackLLElements(\n+      llStruct.getLoc(), llStruct, rewriter, type);\n+\n+  int offset = 0;\n+  for (int i = 0; offset < elems.size(); ++i) {\n+    for (int k = 0; k < NK; k += 4) {\n+      rcds[{i, k}] = std::make_pair(elems[offset], elems[offset + 1]);\n+      offset += 2;\n+    }\n+  }\n+\n+  return rcds;\n+}\n+\n+LogicalResult convertMMA884(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                            TritonGPUToLLVMTypeConverter *typeConverter,\n+                            ConversionPatternRewriter &rewriter) {\n+  auto *ctx = op.getContext();\n+  auto loc = op.getLoc();\n+\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  Value D = op.getResult();\n+  auto mmaLayout = D.getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+  auto ALayout = A.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n+  auto BLayout = B.getType()\n+                     .cast<RankedTensorType>()\n+                     .getEncoding()\n+                     .cast<DotOperandEncodingAttr>();\n+\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+  auto DTensorTy = D.getType().cast<RankedTensorType>();\n+  auto AShape = ATensorTy.getShape();\n+  auto BShape = BTensorTy.getShape();\n+\n+  bool isARow = ALayout.getMMAv1IsRow();\n+  bool isBRow = BLayout.getMMAv1IsRow();\n+  auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n+      mmaLayout.decodeVoltaLayoutStates();\n+  assert(isARow == isARow_);\n+  assert(isBRow == isBRow_);\n+\n+  unsigned numM = ALayout.getMMAv1NumOuter(AShape);\n+  unsigned numN = BLayout.getMMAv1NumOuter(BShape);\n+  unsigned NK = AShape[1];\n+\n+  auto has = extractLoadedOperand(adaptor.getA(), NK, rewriter, typeConverter,\n+                                  ATensorTy);\n+  auto hbs = extractLoadedOperand(adaptor.getB(), NK, rewriter, typeConverter,\n+                                  BTensorTy);\n+\n+  // Initialize accumulators with external values, the acc holds the\n+  // accumulator value that is shared between the MMA instructions inside a\n+  // DotOp, we can call the order of the values the accumulator-internal\n+  // order.\n+  SmallVector<Value> acc =\n+      typeConverter->unpackLLElements(loc, adaptor.getC(), rewriter, DTensorTy);\n+  size_t resSize = acc.size();\n+\n+  // The resVals holds the final result of the DotOp.\n+  // NOTE The current order of resVals is different from acc, we call it the\n+  // accumulator-external order. and\n+  SmallVector<Value> resVals(resSize);\n+\n+  auto getIdx = [&](int m, int n) {\n+    std::vector<size_t> idx{{\n+        (m * 2 + 0) + (n * 4 + 0) * numM, // row0\n+        (m * 2 + 0) + (n * 4 + 1) * numM,\n+        (m * 2 + 1) + (n * 4 + 0) * numM, // row1\n+        (m * 2 + 1) + (n * 4 + 1) * numM,\n+        (m * 2 + 0) + (n * 4 + 2) * numM, // row2\n+        (m * 2 + 0) + (n * 4 + 3) * numM,\n+        (m * 2 + 1) + (n * 4 + 2) * numM, // row3\n+        (m * 2 + 1) + (n * 4 + 3) * numM,\n+    }};\n+    return idx;\n+  };\n+\n+  auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n+    auto ha = has.at({m, k});\n+    auto hb = hbs.at({n, k});\n+\n+    PTXBuilder builder;\n+    auto idx = getIdx(m, n);\n+\n+    // note: using \"=f\" for float leads to cleaner PTX\n+    bool isIntMMA = DTensorTy.getElementType().isInteger(32);\n+    auto *resOprs = builder.newListOperand(8, isIntMMA ? \"=r\" : \"=f\");\n+    auto *AOprs = builder.newListOperand({\n+        {ha.first, \"r\"},\n+        {ha.second, \"r\"},\n+    });\n+\n+    auto *BOprs = builder.newListOperand({\n+        {hb.first, \"r\"},\n+        {hb.second, \"r\"},\n+    });\n+    auto *COprs = builder.newListOperand();\n+    for (int i = 0; i < 8; ++i)\n+      COprs->listAppend(builder.newOperand(acc[idx[i]], std::to_string(i)));\n+\n+    auto mma = builder.create(\"mma.sync.aligned.m8n8k4\")\n+                   ->o(isARow ? \"row\" : \"col\")\n+                   .o(isBRow ? \"row\" : \"col\")\n+                   .o(\"f32.f16.f16.f32\");\n+\n+    mma(resOprs, AOprs, BOprs, COprs);\n+\n+    Value res = builder.launch(rewriter, loc, getMmaRetType(ATensorTy));\n+\n+    for (auto i = 0; i < 8; i++) {\n+      Value elem = extract_val(f32_ty, res, i);\n+      acc[idx[i]] = elem;\n+    }\n+  };\n+\n+  for (unsigned k = 0; k < NK; k += 4)\n+    for (unsigned m = 0; m < numM / 2; ++m)\n+      for (unsigned n = 0; n < numN / 2; ++n) {\n+        callMMA(m, n, k);\n+      }\n+\n+  // res holds the same layout of acc\n+  for (size_t i = 0; i < acc.size(); ++i) {\n+    resVals[i] = acc[i];\n+  }\n+\n+  Value res = typeConverter->packLLElements(loc, resVals, rewriter, DTensorTy);\n+  rewriter.replaceOp(op, res);\n+  return success();\n+}\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM/MMAv2.cpp", "status": "added", "additions": 291, "deletions": 0, "changes": 291, "file_content_changes": "@@ -0,0 +1,291 @@\n+#include \"DotOpToLLVM.h\"\n+#include \"Utility.h\"\n+\n+using namespace mlir;\n+using namespace mlir::triton;\n+\n+using ::mlir::triton::gpu::DotOperandEncodingAttr;\n+using ::mlir::triton::gpu::MmaEncodingAttr;\n+\n+using ValueTableV2 = std::map<std::pair<unsigned, unsigned>, Value>;\n+\n+Value loadC(Value tensor, Value llTensor,\n+            TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+            ConversionPatternRewriter &rewriter) {\n+  MLIRContext *ctx = tensor.getContext();\n+  auto tensorTy = tensor.getType().cast<RankedTensorType>();\n+  size_t fcSize = triton::gpu::getElemsPerThread(tensor.getType());\n+\n+  assert(tensorTy.getEncoding().isa<MmaEncodingAttr>() &&\n+         \"Currently, we only support $c with a mma layout.\");\n+  // Load a normal C tensor with mma layout, that should be a\n+  // LLVM::struct with fcSize elements.\n+  auto structTy = llTensor.getType().cast<LLVM::LLVMStructType>();\n+  assert(structTy.getBody().size() == fcSize &&\n+         \"DotOp's $c operand should pass the same number of values as $d in \"\n+         \"mma layout.\");\n+\n+  auto numMmaRets = tensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+  assert(numMmaRets == 4 || numMmaRets == 2);\n+  if (numMmaRets == 4) {\n+    return llTensor;\n+  } else if (numMmaRets == 2) {\n+    auto cPack = SmallVector<Value>();\n+    auto cElemTy = tensorTy.getElementType();\n+    int numCPackedElem = 4 / numMmaRets;\n+    Type cPackTy = vec_ty(cElemTy, numCPackedElem);\n+    for (int i = 0; i < fcSize; i += numCPackedElem) {\n+      Value pack = rewriter.create<LLVM::UndefOp>(loc, cPackTy);\n+      for (int j = 0; j < numCPackedElem; ++j) {\n+        pack = insert_element(\n+            cPackTy, pack, extract_val(cElemTy, llTensor, i + j), i32_val(j));\n+      }\n+      cPack.push_back(pack);\n+    }\n+\n+    Type structTy = LLVM::LLVMStructType::getLiteral(\n+        ctx, SmallVector<Type>(cPack.size(), cPackTy));\n+    Value result =\n+        typeConverter->packLLElements(loc, cPack, rewriter, structTy);\n+    return result;\n+  }\n+\n+  return llTensor;\n+}\n+\n+ValueTableV2 getValuesFromDotOperandLayoutStruct(\n+    TritonGPUToLLVMTypeConverter *typeConverter, Location loc,\n+    ConversionPatternRewriter &rewriter, Value value, int n0, int n1,\n+    RankedTensorType type) {\n+\n+  auto elems = typeConverter->unpackLLElements(loc, value, rewriter, type);\n+  int offset{};\n+  ValueTableV2 vals;\n+  for (int i = 0; i < n0; ++i) {\n+    for (int j = 0; j < n1; j++) {\n+      vals[{2 * i, 2 * j}] = elems[offset++];\n+      vals[{2 * i, 2 * j + 1}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j}] = elems[offset++];\n+      vals[{2 * i + 1, 2 * j + 1}] = elems[offset++];\n+    }\n+  }\n+  return vals;\n+}\n+\n+enum class TensorCoreType : uint8_t {\n+  // floating-point tensor core instr\n+  FP32_FP16_FP16_FP32 = 0, // default\n+  FP32_BF16_BF16_FP32,\n+  FP32_TF32_TF32_FP32,\n+  FP16_FP16_FP16_FP16,\n+  // integer tensor core instr\n+  INT32_INT1_INT1_INT32, // Not implemented\n+  INT32_INT4_INT4_INT32, // Not implemented\n+  INT32_INT8_INT8_INT32, // Not implemented\n+  //\n+  NOT_APPLICABLE,\n+};\n+\n+Type getMmaRetType(TensorCoreType mmaType, MLIRContext *ctx) {\n+  Type fp32Ty = type::f32Ty(ctx);\n+  Type fp16Ty = type::f16Ty(ctx);\n+  Type i32Ty = type::i32Ty(ctx);\n+  Type fp32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, fp32Ty));\n+  Type i32x4Ty =\n+      LLVM::LLVMStructType::getLiteral(ctx, SmallVector<Type>(4, i32Ty));\n+  Type fp16x2Pack2Ty = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(2, vec_ty(fp16Ty, 2)));\n+  switch (mmaType) {\n+  case TensorCoreType::FP32_FP16_FP16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_BF16_BF16_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP32_TF32_TF32_FP32:\n+    return fp32x4Ty;\n+  case TensorCoreType::FP16_FP16_FP16_FP16:\n+    return fp16x2Pack2Ty;\n+  case TensorCoreType::INT32_INT8_INT8_INT32:\n+    return i32x4Ty;\n+  default:\n+    llvm::report_fatal_error(\"Unsupported mma type found\");\n+  }\n+\n+  return Type{};\n+}\n+\n+TensorCoreType getMmaType(triton::DotOp op) {\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  auto aTy = A.getType().cast<RankedTensorType>();\n+  auto bTy = B.getType().cast<RankedTensorType>();\n+  // d = a*b + c\n+  auto dTy = op.getD().getType().cast<RankedTensorType>();\n+\n+  if (dTy.getElementType().isF32()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP32_FP16_FP16_FP32;\n+    if (aTy.getElementType().isBF16() && bTy.getElementType().isBF16())\n+      return TensorCoreType::FP32_BF16_BF16_FP32;\n+    if (aTy.getElementType().isF32() && bTy.getElementType().isF32() &&\n+        op.getAllowTF32())\n+      return TensorCoreType::FP32_TF32_TF32_FP32;\n+  } else if (dTy.getElementType().isInteger(32)) {\n+    if (aTy.getElementType().isInteger(8) && bTy.getElementType().isInteger(8))\n+      return TensorCoreType::INT32_INT8_INT8_INT32;\n+  } else if (dTy.getElementType().isF16()) {\n+    if (aTy.getElementType().isF16() && bTy.getElementType().isF16())\n+      return TensorCoreType::FP16_FP16_FP16_FP16;\n+  }\n+\n+  return TensorCoreType::NOT_APPLICABLE;\n+}\n+\n+inline static const std::map<TensorCoreType, std::string> mmaInstrPtx = {\n+    {TensorCoreType::FP32_FP16_FP16_FP32,\n+     \"mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32\"},\n+    {TensorCoreType::FP32_BF16_BF16_FP32,\n+     \"mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32\"},\n+    {TensorCoreType::FP32_TF32_TF32_FP32,\n+     \"mma.sync.aligned.m16n8k8.row.col.f32.tf32.tf32.f32\"},\n+\n+    {TensorCoreType::INT32_INT1_INT1_INT32,\n+     \"mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc\"},\n+    {TensorCoreType::INT32_INT4_INT4_INT32,\n+     \"mma.sync.aligned.m16n8k64.row.col.satfinite.s32.s4.s4.s32\"},\n+    {TensorCoreType::INT32_INT8_INT8_INT32,\n+     \"mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32\"},\n+\n+    {TensorCoreType::FP16_FP16_FP16_FP16,\n+     \"mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16\"},\n+};\n+\n+LogicalResult convertDot(TritonGPUToLLVMTypeConverter *typeConverter,\n+                         ConversionPatternRewriter &rewriter, Location loc,\n+                         Value a, Value b, Value c, Value d, Value loadedA,\n+                         Value loadedB, Value loadedC, DotOp op,\n+                         DotOpAdaptor adaptor) {\n+  MLIRContext *ctx = c.getContext();\n+  auto aTensorTy = a.getType().cast<RankedTensorType>();\n+  auto bTensorTy = b.getType().cast<RankedTensorType>();\n+  auto dTensorTy = d.getType().cast<RankedTensorType>();\n+\n+  SmallVector<int64_t> aShape(aTensorTy.getShape().begin(),\n+                              aTensorTy.getShape().end());\n+  auto dShape = dTensorTy.getShape();\n+  int bitwidth = aTensorTy.getElementType().getIntOrFloatBitWidth();\n+  auto repA =\n+      aTensorTy.getEncoding().cast<DotOperandEncodingAttr>().getMMAv2Rep(\n+          aTensorTy.getShape(), bitwidth);\n+  auto repB =\n+      bTensorTy.getEncoding().cast<DotOperandEncodingAttr>().getMMAv2Rep(\n+          bTensorTy.getShape(), bitwidth);\n+\n+  assert(repA[1] == repB[0]);\n+  int repM = repA[0], repN = repB[1], repK = repA[1];\n+\n+  // shape / shape_per_cta\n+  auto ha = getValuesFromDotOperandLayoutStruct(typeConverter, loc, rewriter,\n+                                                loadedA, repM, repK, aTensorTy);\n+  auto hb = getValuesFromDotOperandLayoutStruct(typeConverter, loc, rewriter,\n+                                                loadedB, std::max(repN / 2, 1),\n+                                                repK, bTensorTy);\n+  auto fc = typeConverter->unpackLLElements(loc, loadedC, rewriter, dTensorTy);\n+  auto numMmaRets = dTensorTy.getElementType().getIntOrFloatBitWidth() / 8;\n+  int numCPackedElem = 4 / numMmaRets;\n+\n+  auto mmaType = getMmaType(op);\n+\n+  auto callMma = [&](unsigned m, unsigned n, unsigned k) {\n+    unsigned colsPerThread = repN * 2;\n+    PTXBuilder builder;\n+    auto &mma = *builder.create(mmaInstrPtx.at(mmaType));\n+    // using =r for float32 works but leads to less readable ptx.\n+    bool isIntMMA = dTensorTy.getElementType().isInteger(32);\n+    bool isAccF16 = dTensorTy.getElementType().isF16();\n+    auto retArgs =\n+        builder.newListOperand(numMmaRets, isIntMMA || isAccF16 ? \"=r\" : \"=f\");\n+    auto aArgs = builder.newListOperand({\n+        {ha[{m, k}], \"r\"},\n+        {ha[{m + 1, k}], \"r\"},\n+        {ha[{m, k + 1}], \"r\"},\n+        {ha[{m + 1, k + 1}], \"r\"},\n+    });\n+    auto bArgs =\n+        builder.newListOperand({{hb[{n, k}], \"r\"}, {hb[{n, k + 1}], \"r\"}});\n+    auto cArgs = builder.newListOperand();\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      cArgs->listAppend(builder.newOperand(\n+          fc[(m * colsPerThread + 4 * n) / numCPackedElem + i],\n+          std::to_string(i)));\n+      // reuse the output registers\n+    }\n+\n+    mma(retArgs, aArgs, bArgs, cArgs);\n+    Value mmaOut =\n+        builder.launch(rewriter, loc, getMmaRetType(mmaType, op.getContext()));\n+\n+    Type elemTy = mmaOut.getType().cast<LLVM::LLVMStructType>().getBody()[0];\n+    for (int i = 0; i < numMmaRets; ++i) {\n+      fc[(m * colsPerThread + 4 * n) / numCPackedElem + i] =\n+          extract_val(elemTy, mmaOut, i);\n+    }\n+  };\n+\n+  for (int k = 0; k < repK; ++k)\n+    for (int m = 0; m < repM; ++m)\n+      for (int n = 0; n < repN; ++n)\n+        callMma(2 * m, n, 2 * k);\n+\n+  Type resElemTy = dTensorTy.getElementType();\n+\n+  // replace with new packed result\n+  Type structTy = LLVM::LLVMStructType::getLiteral(\n+      ctx, SmallVector<Type>(fc.size() * numCPackedElem, resElemTy));\n+  SmallVector<Value> results(fc.size() * numCPackedElem);\n+  for (int i = 0; i < fc.size(); ++i) {\n+    for (int j = 0; j < numCPackedElem; ++j) {\n+      results[i * numCPackedElem + j] =\n+          numCPackedElem > 1\n+              ? bitcast(extract_element(fc[i], i32_val(j)), resElemTy)\n+              : bitcast(fc[i], resElemTy);\n+    }\n+  }\n+  Value res = typeConverter->packLLElements(loc, results, rewriter, structTy);\n+\n+  rewriter.replaceOp(op, res);\n+\n+  return success();\n+}\n+\n+// Convert to mma.m16n8k16\n+LogicalResult convertMMA16816(triton::DotOp op, triton::DotOp::Adaptor adaptor,\n+                              TritonGPUToLLVMTypeConverter *typeConverter,\n+                              ConversionPatternRewriter &rewriter) {\n+  auto loc = op.getLoc();\n+  auto mmaLayout = op.getResult()\n+                       .getType()\n+                       .cast<RankedTensorType>()\n+                       .getEncoding()\n+                       .cast<MmaEncodingAttr>();\n+\n+  Value A = op.getA();\n+  Value B = op.getB();\n+  Value C = op.getC();\n+\n+  auto ATensorTy = A.getType().cast<RankedTensorType>();\n+  auto BTensorTy = B.getType().cast<RankedTensorType>();\n+\n+  assert(ATensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+         BTensorTy.getEncoding().isa<DotOperandEncodingAttr>() &&\n+         \"Both $a and %b should be DotOperand layout.\");\n+\n+  Value loadedA, loadedB, loadedC;\n+  loadedA = adaptor.getA();\n+  loadedB = adaptor.getB();\n+  loadedC =\n+      loadC(op.getC(), adaptor.getC(), typeConverter, op.getLoc(), rewriter);\n+\n+  return convertDot(typeConverter, rewriter, op.getLoc(), A, B, C, op.getD(),\n+                    loadedA, loadedB, loadedC, op, adaptor);\n+}\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 56, "deletions": 51, "changes": 107, "file_content_changes": "@@ -122,21 +122,21 @@ struct FpToFpOpConversion\n   convertFp8E4M3x4ToBf16x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                                          \\n\"\n-                   \".reg .b32 a<2>, sign<2>, nosign<2>, b<2>;  \\n\"\n-                   \"prmt.b32 a0, 0, $2, 0x5040;                \\n\"\n-                   \"prmt.b32 a1, 0, $2, 0x7060;                \\n\"\n-                   \"and.b32 sign0, a0, 0x80008000;             \\n\"\n-                   \"and.b32 sign1, a1, 0x80008000;             \\n\"\n-                   \"and.b32 nosign0, a0, 0x7fff7fff;           \\n\"\n-                   \"and.b32 nosign1, a1, 0x7fff7fff;           \\n\"\n-                   \"shr.b32 nosign0, nosign0, 4;               \\n\"\n-                   \"shr.b32 nosign1, nosign1, 4;               \\n\"\n-                   \"add.u32 nosign0, nosign0, 0x38003800;      \\n\"\n-                   \"add.u32 nosign1, nosign1, 0x38003800;      \\n\"\n-                   \"or.b32 $0, sign0, nosign0;                 \\n\"\n-                   \"or.b32 $1, sign1, nosign1;                 \\n\"\n-                   \"}\";\n+    auto *ptxAsm = // WARN: subnormal (0bs0000xxx) are not handled\n+        \"{                                      \\n\"\n+        \".reg .b32 a<2>, b<2>;                  \\n\" // if input = 0xf1f2f3f4\n+        \"prmt.b32 a0, 0, $2, 0x5040;            \\n\" // a0 = 0xf300f400\n+        \"prmt.b32 a1, 0, $2, 0x7060;            \\n\" // a1 = 0xf100f200\n+        \"and.b32 b0, a0, 0x7fff7fff;            \\n\" // b0 = a0 & 0x7fff7fff\n+        \"and.b32 b1, a1, 0x7fff7fff;            \\n\" // (strip sign)\n+        \"shr.b32 b0, b0, 4;                     \\n\" // b0 >>= 4\n+        \"shr.b32 b1, b1, 4;                     \\n\" // shift into fp16 position\n+        \"add.u32 b0, b0, 0x3c003c00;            \\n\" // b0.exp += 2**7-2**3\n+                                                    // exponent compensate = 120\n+        \"add.u32 b1, b1, 0x3c003c00;            \\n\" // b1 += 120<<7 | 120<<7<<16\n+        \"lop3.b32 $0, b0, 0x80008000, a0, 0xf8; \\n\" // out0 = b0|(0x80008000&a0)\n+        \"lop3.b32 $1, b1, 0x80008000, a1, 0xf8; \\n\" // (restore sign)\n+        \"}\";\n     return convertFp8x4ToBf16x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   };\n \n@@ -292,42 +292,47 @@ struct FpToFpOpConversion\n   convertBf16x4ToFp8E4M3x4(Location loc, ConversionPatternRewriter &rewriter,\n                            const Value &v0, const Value &v1, const Value &v2,\n                            const Value &v3) {\n-    auto *ptxAsm = \"{                                            \\n\"\n-                   \".reg .u32 sign, sign<2>, nosign, nosign<2>;  \\n\"\n-                   \".reg .u32 fp8_min, fp8_max, rn_, zero;       \\n\"\n-                   \"mov.u32 fp8_min, 0x38003800;                 \\n\"\n-                   \"mov.u32 fp8_max, 0x3ff03ff0;                 \\n\"\n-                   \"mov.u32 rn_, 0x80008;                        \\n\"\n-                   \"mov.u32 zero, 0;                             \\n\"\n-                   \"and.b32 sign0, $1, 0x80008000;               \\n\"\n-                   \"and.b32 sign1, $2, 0x80008000;               \\n\"\n-                   \"prmt.b32 sign, sign0, sign1, 0x7531;         \\n\"\n-                   \"and.b32 nosign0, $1, 0x7fff7fff;             \\n\"\n-                   \"and.b32 nosign1, $2, 0x7fff7fff;             \\n\"\n-                   \".reg .u32 nosign_0_<2>, nosign_1_<2>;        \\n\"\n-                   \"and.b32 nosign_0_0, nosign0, 0xffff0000;     \\n\"\n-                   \"max.u32 nosign_0_0, nosign_0_0, 0x38000000;  \\n\"\n-                   \"min.u32 nosign_0_0, nosign_0_0, 0x3ff00000;  \\n\"\n-                   \"and.b32 nosign_0_1, nosign0, 0x0000ffff;     \\n\"\n-                   \"max.u32 nosign_0_1, nosign_0_1, 0x3800;      \\n\"\n-                   \"min.u32 nosign_0_1, nosign_0_1, 0x3ff0;      \\n\"\n-                   \"or.b32 nosign0, nosign_0_0, nosign_0_1;      \\n\"\n-                   \"and.b32 nosign_1_0, nosign1, 0xffff0000;     \\n\"\n-                   \"max.u32 nosign_1_0, nosign_1_0, 0x38000000;  \\n\"\n-                   \"min.u32 nosign_1_0, nosign_1_0, 0x3ff00000;  \\n\"\n-                   \"and.b32 nosign_1_1, nosign1, 0x0000ffff;     \\n\"\n-                   \"max.u32 nosign_1_1, nosign_1_1, 0x3800;      \\n\"\n-                   \"min.u32 nosign_1_1, nosign_1_1, 0x3ff0;      \\n\"\n-                   \"or.b32 nosign1, nosign_1_0, nosign_1_1;      \\n\"\n-                   \"add.u32 nosign0, nosign0, rn_;               \\n\"\n-                   \"add.u32 nosign1, nosign1, rn_;               \\n\"\n-                   \"sub.u32 nosign0, nosign0, 0x38003800;        \\n\"\n-                   \"sub.u32 nosign1, nosign1, 0x38003800;        \\n\"\n-                   \"shr.u32 nosign0, nosign0, 4;                 \\n\"\n-                   \"shr.u32 nosign1, nosign1, 4;                 \\n\"\n-                   \"prmt.b32 nosign, nosign0, nosign1, 0x6420;   \\n\"\n-                   \"or.b32 $0, nosign, sign;                     \\n\"\n-                   \"}\";\n+    auto *ptxAsm = // bf16 is clamped firstly to fp8 min/max\n+        \"{                                           \\n\" // bf16=fp8>>4 + 120<<7\n+        \".reg .u32 sign, sign<2>, nosign, nosign<2>; \\n\" // fp8_min = 0b00000000\n+        \".reg .u32 fp8_min, fp8_max, rn_;            \\n\" // fp8_max = 0b11111111\n+        \"mov.u32 fp8_min, 0x3c003c00;                \\n\" // so bf16_min = 0x3c00\n+        \"mov.u32 fp8_max, 0x43f043f0;                \\n\" // so bf16_max = 0x43f0\n+        \"mov.u32 rn_, 0x80008;                       \\n\" // round to nearest\n+        \"and.b32 sign0, $1, 0x80008000;              \\n\" // sign0=in0&0x80008000\n+        \"and.b32 sign1, $2, 0x80008000;              \\n\" // (store sign)\n+        \"prmt.b32 sign, sign0, sign1, 0x7531;        \\n\"\n+        \"and.b32 nosign0, $1, 0x7fff7fff;            \\n\" // nosign0=in0&0x7fff7fff\n+        \"and.b32 nosign1, $2, 0x7fff7fff;            \\n\" // (strip sign)\n+\n+        // nosign = clamp(nosign, min, max)\n+        \".reg .u32 nosign_0_<2>, nosign_1_<2>;       \\n\"\n+        \"and.b32 nosign_0_0, nosign0, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_0_0, nosign_0_0, 0x3c000000; \\n\"\n+        \"min.u32 nosign_0_0, nosign_0_0, 0x43f00000; \\n\"\n+        \"and.b32 nosign_0_1, nosign0, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_0_1, nosign_0_1, 0x3c00;     \\n\"\n+        \"min.u32 nosign_0_1, nosign_0_1, 0x43f0;     \\n\"\n+        \"or.b32 nosign0, nosign_0_0, nosign_0_1;     \\n\"\n+        \"and.b32 nosign_1_0, nosign1, 0xffff0000;    \\n\"\n+        \"max.u32 nosign_1_0, nosign_1_0, 0x3c000000; \\n\"\n+        \"min.u32 nosign_1_0, nosign_1_0, 0x43f00000; \\n\"\n+        \"and.b32 nosign_1_1, nosign1, 0x0000ffff;    \\n\"\n+        \"max.u32 nosign_1_1, nosign_1_1, 0x3c00;     \\n\"\n+        \"min.u32 nosign_1_1, nosign_1_1, 0x43f0;     \\n\"\n+        \"or.b32 nosign1, nosign_1_0, nosign_1_1;     \\n\"\n+\n+        \"add.u32 nosign0, nosign0, rn_;              \\n\" // nosign0 += rn_\n+        \"add.u32 nosign1, nosign1, rn_;              \\n\" // (round to nearest)\n+        \"sub.u32 nosign0, nosign0, 0x3c003c00;       \\n\" // nosign0-=0x3c003c00\n+        \"sub.u32 nosign1, nosign1, 0x3c003c00;       \\n\" // (compensate offset)\n+        \"shr.u32 nosign0, nosign0, 4;                \\n\" // nosign0 >>= 4\n+        \"shr.u32 nosign1, nosign1, 4;                \\n\" // shift into to fp8e4\n+        \"prmt.b32 nosign, nosign0, nosign1, 0x6420;  \\n\" // nosign0 = 0x00f100f2\n+                                                         // nosign1 = 0x00f300f4\n+                                                         // nosign = 0xf3f4f1f2\n+        \"or.b32 $0, nosign, sign;                    \\n\" // restore sign\n+        \"}\";\n     return convertBf16x4ToFp8x4(loc, rewriter, ptxAsm, v0, v1, v2, v3);\n   };\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,5 +1,4 @@\n #include \"TritonGPUToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n #include \"mlir/Dialect/ControlFlow/IR/ControlFlowOps.h\"\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "file_content_changes": "@@ -9,7 +9,6 @@\n \n #include \"TypeConverter.h\"\n //\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n #include \"triton/Analysis/AxisInfo.h\"\n@@ -701,7 +700,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto shape = type.getShape();\n \n     auto wpt = mmaLayout.getWarpsPerCTA();\n-    auto fpw = LLVM::DotOpMmaV1ConversionHelper::fpw;\n+    static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n     auto [isARow, isBRow, isAVec4, isBVec4, _] =\n         mmaLayout.decodeVoltaLayoutStates();\n \n@@ -787,7 +786,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n     auto bRep = bEncoding.getMMAv1Rep();\n \n     auto wpt = mmaLayout.getWarpsPerCTA();\n-    auto fpw = LLVM::DotOpMmaV1ConversionHelper::fpw;\n+    static constexpr std::array<int, 3> fpw{{2, 2, 1}};\n     SmallVector<int, 2> rep({aRep[0], bRep[1]});\n     SmallVector<int, 2> spw({aSpw[0], bSpw[1]});\n     SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.cpp", "status": "modified", "additions": 38, "deletions": 66, "changes": 104, "file_content_changes": "@@ -1,15 +1,11 @@\n #include \"TypeConverter.h\"\n-#include \"DotOpHelpers.h\"\n #include \"Utility.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"triton/Conversion/MLIRTypes.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::DotOperandEncodingAttr;\n using ::mlir::triton::gpu::getElemsPerThread;\n@@ -82,25 +78,47 @@ SmallVector<Value> TritonGPUToLLVMTypeConverter::unpackLLElements(\n   return results;\n }\n \n-llvm::Optional<Type>\n-TritonGPUToLLVMTypeConverter::convertTritonTensorType(RankedTensorType type) {\n+Type TritonGPUToLLVMTypeConverter::getElementTypeForStruct(\n+    RankedTensorType type) {\n+  auto ctx = type.getContext();\n+  Attribute layout = type.getEncoding();\n+  Type elemTy = convertType(type.getElementType());\n+  auto dotOpLayout = layout.dyn_cast<DotOperandEncodingAttr>();\n+  if (!dotOpLayout)\n+    return elemTy;\n+  auto mmaParent = dotOpLayout.getParent().dyn_cast<MmaEncodingAttr>();\n+  if (!mmaParent)\n+    return elemTy;\n+  if (mmaParent.isAmpere()) {\n+    int bitwidth = elemTy.getIntOrFloatBitWidth();\n+    // sub-word integer types need to be packed for perf reasons\n+    if (elemTy.isa<IntegerType>() && bitwidth < 32)\n+      return IntegerType::get(ctx, 32);\n+    // TODO: unify everything to use packed integer-types\n+    // otherwise, vector types are ok\n+    const llvm::DenseMap<int, Type> elemTyMap = {\n+        {32, vec_ty(elemTy, 1)},\n+        {16, vec_ty(elemTy, 2)},\n+        {8, vec_ty(elemTy, 4)},\n+    };\n+    return elemTyMap.lookup(bitwidth);\n+  } else {\n+    assert(mmaParent.isVolta());\n+    return vec_ty(elemTy, 2);\n+  }\n+}\n+\n+Type TritonGPUToLLVMTypeConverter::convertTritonTensorType(\n+    RankedTensorType type) {\n   auto ctx = type.getContext();\n   Attribute layout = type.getEncoding();\n   SmallVector<int64_t> shape(type.getShape().begin(), type.getShape().end());\n+  Type eltType = getElementTypeForStruct(type);\n \n-  if (layout &&\n-      (layout.isa<BlockedEncodingAttr>() || layout.isa<SliceEncodingAttr>() ||\n-       layout.isa<MmaEncodingAttr>())) {\n-    unsigned numElementsPerThread = getElemsPerThread(type);\n-    SmallVector<Type, 4> types(numElementsPerThread,\n-                               convertType(type.getElementType()));\n-    return LLVM::LLVMStructType::getLiteral(ctx, types);\n-  } else if (auto shared_layout =\n-                 layout.dyn_cast_or_null<SharedEncodingAttr>()) {\n+  if (auto shared_layout = layout.dyn_cast<SharedEncodingAttr>()) {\n     SmallVector<Type, 4> types;\n     // base ptr\n-    auto ptrType =\n-        LLVM::LLVMPointerType::get(convertType(type.getElementType()), 3);\n+    auto ptrType = LLVM::LLVMPointerType::get(eltType, 3);\n     types.push_back(ptrType);\n     // shape dims\n     auto rank = type.getRank();\n@@ -109,55 +127,9 @@ TritonGPUToLLVMTypeConverter::convertTritonTensorType(RankedTensorType type) {\n       types.push_back(IntegerType::get(ctx, 32));\n     }\n     return LLVM::LLVMStructType::getLiteral(ctx, types);\n-  } else if (auto dotOpLayout =\n-                 layout.dyn_cast_or_null<DotOperandEncodingAttr>()) {\n-    if (dotOpLayout.getParent()\n-            .isa<BlockedEncodingAttr>()) { // for parent is blocked layout\n-      int numElemsPerThread =\n-          DotOpFMAConversionHelper::getNumElemsPerThread(shape, dotOpLayout);\n-\n-      return LLVM::LLVMStructType::getLiteral(\n-          ctx, SmallVector<Type>(numElemsPerThread, type::f32Ty(ctx)));\n-    } else { // for parent is MMA layout\n-      auto mmaLayout = dotOpLayout.getParent().cast<MmaEncodingAttr>();\n-      auto wpt = mmaLayout.getWarpsPerCTA();\n-      Type elemTy = convertType(type.getElementType());\n-      if (mmaLayout.isAmpere()) {\n-        const llvm::DenseMap<int, Type> targetTyMap = {\n-            {32, vec_ty(elemTy, 1)},\n-            {16, vec_ty(elemTy, 2)},\n-            {8, vec_ty(elemTy, 4)},\n-        };\n-        Type targetTy;\n-        if (targetTyMap.count(elemTy.getIntOrFloatBitWidth())) {\n-          targetTy = targetTyMap.lookup(elemTy.getIntOrFloatBitWidth());\n-          // <2xi16>/<4xi8> => i32\n-          // We are doing this because NVPTX inserts extra integer instrs to\n-          // pack & unpack vectors of sub-word integers\n-          // Note: this needs to be synced with\n-          //       DotOpMmaV2ConversionHelper::loadX4\n-          if (elemTy.isa<IntegerType>() &&\n-              (elemTy.getIntOrFloatBitWidth() == 8 ||\n-               elemTy.getIntOrFloatBitWidth() == 16))\n-            targetTy = IntegerType::get(ctx, 32);\n-        } else {\n-          assert(false && \"Unsupported element type\");\n-        }\n-        auto elems = getElemsPerThread(type);\n-        return struct_ty(SmallVector<Type>(elems, targetTy));\n-      }\n-\n-      if (mmaLayout.isVolta()) {\n-        int elems = getElemsPerThread(type);\n-        Type x2Ty = vec_ty(elemTy, 2);\n-        return struct_ty(SmallVector<Type>(elems, x2Ty));\n-      }\n-    }\n-\n-    llvm::errs() << \"Unexpected dot operand layout detected in \"\n-                    \"TritonToLLVMTypeConverter\";\n-    return std::nullopt;\n   }\n \n-  return std::nullopt;\n+  unsigned numElementsPerThread = getElemsPerThread(type);\n+  SmallVector<Type, 4> types(numElementsPerThread, eltType);\n+  return LLVM::LLVMStructType::getLiteral(ctx, types);\n }\n\\ No newline at end of file"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -15,6 +15,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n   TritonGPUToLLVMTypeConverter(MLIRContext *ctx, LowerToLLVMOptions &option,\n                                const DataLayoutAnalysis *analysis = nullptr);\n \n+  Type getElementTypeForStruct(RankedTensorType type);\n   Type convertTritonPointerType(triton::PointerType type);\n \n   Value packLLElements(Location loc, ValueRange resultVals,\n@@ -24,7 +25,7 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n                                       ConversionPatternRewriter &rewriter,\n                                       Type type);\n \n-  llvm::Optional<Type> convertTritonTensorType(RankedTensorType type);\n+  Type convertTritonTensorType(RankedTensorType type);\n };\n \n #endif"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "file_content_changes": "@@ -1,14 +1,9 @@\n #include \"ViewOpToLLVM.h\"\n-#include \"DotOpHelpers.h\"\n \n using namespace mlir;\n using namespace mlir::triton;\n \n-using ::mlir::LLVM::DotOpFMAConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n-using ::mlir::LLVM::DotOpMmaV2ConversionHelper;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n-using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::getElemsPerThread;\n \n struct SplatOpConversion"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 48, "deletions": 18, "changes": 66, "file_content_changes": "@@ -411,30 +411,36 @@ unsigned SharedEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n   return 0;\n }\n \n+SmallVector<int64_t>\n+DotOperandEncodingAttr::getMMAv2Rep(ArrayRef<int64_t> shape,\n+                                    int bitwidth) const {\n+  auto mmaParent = getParent().cast<MmaEncodingAttr>();\n+  SmallVector<int> shapePerWarp = {16, 8, 4 * 64 / bitwidth};\n+  auto warpsPerCTA = getParent().cast<MmaEncodingAttr>().getWarpsPerCTA();\n+  assert(mmaParent.isAmpere());\n+  if (getOpIdx() == 0)\n+    return {std::max<int64_t>(1, shape[0] / (shapePerWarp[0] * warpsPerCTA[0])),\n+            shape[1] / shapePerWarp[2]};\n+  else {\n+    assert(getOpIdx() == 1);\n+    return {\n+        shape[0] / shapePerWarp[2],\n+        std::max<int64_t>(1, shape[1] / (shapePerWarp[1] * warpsPerCTA[1]))};\n+  }\n+}\n+\n unsigned DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n                                                    Type eltTy) const {\n   if (auto mmaParent = getParent().dyn_cast<MmaEncodingAttr>()) {\n     int warpsPerCTAM = mmaParent.getWarpsPerCTA()[0];\n     int warpsPerCTAN = mmaParent.getWarpsPerCTA()[1];\n     // A100\n     if (mmaParent.isAmpere()) {\n-      int bitwidth = eltTy.getIntOrFloatBitWidth();\n-      int shapePerWarpM = 16;\n-      int shapePerWarpN = 8;\n-      int shapePerWarpK = 4 * 64 / bitwidth;\n-      int shapePerCTAM = shapePerWarpM * warpsPerCTAM;\n-      int shapePerCTAN = shapePerWarpN * warpsPerCTAN;\n-\n-      if (getOpIdx() == 0) {\n-        int repM = std::max<int>(1, shape[0] / shapePerCTAM);\n-        int repK = std::max<int>(1, shape[1] / shapePerWarpK);\n-        return 4 * repM * repK;\n-      }\n-      if (getOpIdx() == 1) {\n-        int repN = std::max<int>(1, shape[1] / shapePerCTAN);\n-        int repK = std::max<int>(1, shape[0] / shapePerWarpK);\n-        return 4 * std::max(repN / 2, 1) * repK;\n-      }\n+      auto rep = getMMAv2Rep(shape, eltTy.getIntOrFloatBitWidth());\n+      if (getOpIdx() == 0)\n+        return 4 * rep[0] * rep[1];\n+      if (getOpIdx() == 1)\n+        return 4 * rep[0] * std::max<int>(rep[1] / 2, 1);\n     }\n     // V100\n     if (mmaParent.isVolta()) {\n@@ -497,7 +503,31 @@ unsigned DotOperandEncodingAttr::getElemsPerThread(ArrayRef<int64_t> shape,\n       }\n     }\n   }\n-  llvm_unreachable(\"unknown mma version\");\n+  if (auto blockedLayout = getParent().dyn_cast<BlockedEncodingAttr>()) {\n+    auto shapePerCTA = getShapePerCTA(blockedLayout);\n+    auto order = blockedLayout.getOrder();\n+    auto sizePerThread = getSizePerThread(blockedLayout);\n+\n+    int K = getOpIdx() == 0 ? shape[1] : shape[0];\n+    int otherDim = getOpIdx() == 1 ? shape[1] : shape[0];\n+\n+    bool isM = getOpIdx() == 0;\n+\n+    int mSizePerThread =\n+        order[0] == 1 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    int nSizePerThread =\n+        order[0] == 0 ? sizePerThread[order[1]] : sizePerThread[order[0]];\n+    int sizePerThreadMN = isM ? mSizePerThread : nSizePerThread;\n+\n+    int mShapePerCTA =\n+        order[0] == 1 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+    int nShapePerCTA =\n+        order[0] == 0 ? shapePerCTA[order[1]] : shapePerCTA[order[0]];\n+    int shapePerCTAMN = isM ? mShapePerCTA : nShapePerCTA;\n+\n+    return K * std::max<int>(otherDim / shapePerCTAMN, 1) * sizePerThreadMN;\n+  }\n+  llvm_unreachable(\"unknown dot operand parent layout\");\n   return 0;\n }\n "}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 9, "deletions": 13, "changes": 22, "file_content_changes": "@@ -971,7 +971,8 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n \n \n @pytest.mark.parametrize(\"in_dtype\", [tl.float8e4])\n-def test_f16_to_f8_rounding(in_dtype):\n+@pytest.mark.parametrize(\"out_dtype\", [torch.float16, torch.bfloat16])\n+def test_f16_to_f8_rounding(in_dtype, out_dtype):\n     \"\"\"Takes all float16s, converts them to float8 and back to float16. Checks that the absolute\n     error is the minimum over all float8.\n     Or the same explanation a bit mathier:\n@@ -984,28 +985,22 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n         output = input\n         tl.store(output_ptr + offsets, output, mask=mask)\n \n-    # torch.view with a dtype isn't supported in triton's torch yet so use numpy's view\n-    f16_input_np = (\n-        np.array(\n-            range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=np.int16,\n-        )\n-        .view(np.float16)\n-    )\n-    f16_input = torch.tensor(f16_input_np, dtype=torch.float16, device='cuda')\n+    i16_input = torch.tensor(range(-int(2 ** (16 - 1)), int(2 ** (16 - 1))), dtype=torch.int16, device='cuda')\n+    f16_input = i16_input.view(out_dtype)\n     n_elements = f16_input.numel()\n     f8_output_tensor = torch.empty_like(f16_input, dtype=torch.int8)\n     f8_output = triton.reinterpret(f8_output_tensor, in_dtype)\n     grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n     copy_kernel[grid](f16_input, f8_output, n_elements, BLOCK_SIZE=1024)\n \n-    f16_output = torch.empty_like(f16_input, dtype=torch.float16)\n+    f16_output = torch.empty_like(f16_input, dtype=out_dtype)\n     copy_kernel[grid](f8_output, f16_output, n_elements, BLOCK_SIZE=1024)\n \n     abs_error = torch.abs(f16_input - f16_output)\n \n     all_f8_vals_tensor = torch.tensor(range(2 ** 8), dtype=torch.uint8, device='cuda')\n     all_f8_vals = triton.reinterpret(all_f8_vals_tensor, in_dtype)\n-    all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=torch.float16)\n+    all_f8_vals_in_f16 = torch.empty_like(all_f8_vals_tensor, dtype=out_dtype)\n     copy_kernel[grid](all_f8_vals, all_f8_vals_in_f16, n_elements=256, BLOCK_SIZE=1024)\n \n     all_finite_f8_vals_in_f16 = all_f8_vals_in_f16[\n@@ -1023,9 +1018,10 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n     # WARN: only normalized numbers are handled\n     f8_normal_min = 1 << in_dtype.fp_mantissa_width  # 0b00001000 for float8e4\n     f8_normal_max = 0b01111110\n-    f16_min, f16_max = convert_float_to_float32(torch.tensor([f8_normal_min, f8_normal_max], dtype=torch.int8), in_dtype)\n+    f16_min, f16_max, f16_max_minus_1 = convert_float_to_float32(torch.tensor([f8_normal_min, f8_normal_max, f8_normal_max - 1], dtype=torch.int8), in_dtype)\n+    thres_error = f16_max - f16_max_minus_1\n     mismatch = torch.logical_and(\n-        abs_error != min_error, torch.logical_and(torch.isfinite(f16_input), torch.logical_and(torch.abs(f16_input) <= f16_max, torch.abs(f16_input) >= f16_min))\n+        torch.logical_or(abs_error != min_error, abs_error > thres_error), torch.logical_and(torch.isfinite(f16_input), torch.logical_and(torch.abs(f16_input) <= f16_max, torch.abs(f16_input) >= f16_min))\n     )\n     assert torch.all(\n         torch.logical_not(mismatch)"}]