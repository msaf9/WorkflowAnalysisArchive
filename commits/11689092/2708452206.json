[{"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 63, "deletions": 23, "changes": 86, "file_content_changes": "@@ -86,7 +86,7 @@ Value* geper::operator()(Value *ptr, Value* off, const std::string& name){\n // types\n #define void_ty              builder_->getVoidTy()\n #define f16_ty               builder_->getHalfTy()\n-#define bf16_ty              builder_->getBFloatTy()\n+#define bf16_ty              builder_->getInt16Ty()\n #define f32_ty               builder_->getFloatTy()\n #define i1_ty                builder_->getInt1Ty()\n #define i8_ty                builder_->getInt8Ty()\n@@ -178,7 +178,7 @@ Type *generator::cvt(ir::type *ty) {\n     case ir::type::VoidTyID:      return Type::getVoidTy(*ctx_);\n     case ir::type::FP8TyID:       return Type::getInt8Ty(*ctx_);\n     case ir::type::FP16TyID:      return Type::getHalfTy(*ctx_);\n-    case ir::type::BF16TyID:      return Type::getBFloatTy(*ctx_);\n+    case ir::type::BF16TyID:      return Type::getInt16Ty(*ctx_); // use int16 as storage type\n     case ir::type::FP32TyID:      return Type::getFloatTy(*ctx_);\n     case ir::type::FP64TyID:      return Type::getDoubleTy(*ctx_);\n     case ir::type::LabelTyID:     return Type::getLabelTy(*ctx_);\n@@ -378,8 +378,8 @@ void generator::visit_launch_inst(ir::launch_inst *launch) {\n  */\n void generator::visit_binary_operator(ir::binary_operator*x) {\n   using ll = llvm::Instruction::BinaryOps;\n+  using tt = ir::binary_op_t;\n   auto cvt = [](ir::binary_op_t op){\n-    using tt = ir::binary_op_t;\n     switch(op) {\n       case tt::Add: return ll::Add;\n       case tt::FAdd: return ll::FAdd;\n@@ -406,20 +406,51 @@ void generator::visit_binary_operator(ir::binary_operator*x) {\n   for(indices_t idx: idxs_.at(x)){\n     Value *lhs = vals_[x->get_operand(0)][idx];\n     Value *rhs = vals_[x->get_operand(1)][idx];\n-    auto op = cvt(x->get_op());\n-    if(op == ll::Add)\n-       vals_[x][idx] = add(lhs, rhs);\n-     else if(op == ll::Mul)\n-       vals_[x][idx] = mul(lhs, rhs);\n-     else if(op == ll::FDiv && !x->get_fdiv_ieee_rounding() &&\n-             x->get_type()->get_scalar_ty()->is_fp32_ty()){\n-       InlineAsm *ptx = InlineAsm::get(FunctionType::get(f32_ty, {f32_ty, f32_ty}, false),\n-                                      \" div.full.f32 $0, $1, $2;\", \"=r,r,r\", false);\n-       vals_[x][idx] = builder_->CreateCall(ptx, {lhs, rhs});\n+    // manually select bf16 bin op\n+    if (x->get_operand(0)->get_type()->get_scalar_ty()->is_bf16_ty()) {\n+      assert(x->get_operand(1)->get_type()->get_scalar_ty()->is_bf16_ty());\n+      if (x->get_op() == tt::FAdd) {  // a + b = a * 1.0 + b\n+        InlineAsm *bf16_add_asm =\n+            InlineAsm::get(FunctionType::get(bf16_ty, {bf16_ty, bf16_ty}, false),\n+                           \"{ .reg .b16 c;         \\n\\t\"\n+                           \"   mov.b16 c, 0x3f80U; \\n\\t\" // 1.0\n+                           \"   fma.rn.bf16 $0, $1, c, $2; } \\n\\t\",\n+                           \"=h,h,h\", false);\n+        vals_[x][idx] = builder_->CreateCall(bf16_add_asm, {lhs, rhs});\n+      } else if (x->get_op() == tt::FSub) {  // a - b = b * (-1.0) + a\n+        InlineAsm *bf16_sub_asm =\n+            InlineAsm::get(FunctionType::get(bf16_ty, {bf16_ty, bf16_ty}, false),\n+                           \" { .reg .b16 c;         \\n\\t\"\n+                           \"    mov.b16 c, 0xbf80U; \\n\\t\" // -1.0\n+                           \"    fma.rn.bf16 $0, $2, c, $1;} \\n\\t\",\n+                           \"=h,h,h\", false);\n+        vals_[x][idx] = builder_->CreateCall(bf16_sub_asm, {lhs, rhs});\n+      } else if (x->get_op() == tt::FMul) {  // a * b = a*b + 0\n+        InlineAsm *bf16_mul_asm =\n+          InlineAsm::get(FunctionType::get(bf16_ty, {bf16_ty, bf16_ty}, false),\n+                           \" { .reg .b16 c;        \\n\\t\"\n+                           \"    mov.b16 c, 0x8000U; \\n\\t\" // 0.0\n+                           \"    fma.rn.bf16 $0, $1, $2, c;} \\n\\t\",\n+                           \"=h,h,h\", false);\n+        vals_[x][idx] = builder_->CreateCall(bf16_mul_asm, {lhs, rhs});\n+      } else\n+        throw std::runtime_error(\"invalid bin op for bf16\");\n+    } else {  // not bf16\n+      auto op = cvt(x->get_op());\n+      if(op == ll::Add)\n+        vals_[x][idx] = add(lhs, rhs);\n+      else if(op == ll::Mul)\n+        vals_[x][idx] = mul(lhs, rhs);\n+      else if(op == ll::FDiv && !x->get_fdiv_ieee_rounding() &&\n+              x->get_type()->get_scalar_ty()->is_fp32_ty()){\n+        InlineAsm *ptx = InlineAsm::get(FunctionType::get(f32_ty, {f32_ty, f32_ty}, false),\n+                                        \" div.full.f32 $0, $1, $2;\", \"=r,r,r\", false);\n+        vals_[x][idx] = builder_->CreateCall(ptx, {lhs, rhs});\n \n-     }\n-     else\n-       vals_[x][idx] = bin_op(op, lhs, rhs);\n+      }\n+      else\n+        vals_[x][idx] = bin_op(op, lhs, rhs);\n+    }\n   }\n }\n \n@@ -970,8 +1001,6 @@ void generator::visit_store_inst(ir::store_inst * x){\n   has_l2_evict_policy = false;\n   auto idxs    = idxs_.at(val_op);\n   Type *ty = cvt(val_op->get_type()->get_scalar_ty());\n-  if (ty->isBFloatTy()) // llvm11-nvptx cannot select bf16 store\n-    ty = f16_ty;\n   if(ty->isIntegerTy(1))\n     ty = builder_->getInt8Ty();\n   for(size_t i = 0; i < idxs.size(); i += vec){\n@@ -2830,9 +2859,6 @@ void generator::visit_layout_convert(ir::value *out, ir::value *in){\n   // pointer to temporary shared memory\n   Type *ty = cvt(out->get_type()->get_scalar_ty());\n \n-  if (ty->isBFloatTy()) // llvm11-nvptx cannot select bf16 store\n-    ty = f16_ty;\n-\n   // Orders\n   analysis::distributed_layout* in_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(in));\n   analysis::distributed_layout* out_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(out));\n@@ -3229,8 +3255,22 @@ void generator::visit_constant_int(ir::constant_int *x){\n \n void generator::visit_constant_fp(ir::constant_fp *x){\n   Type *ty = cvt(x->get_type()->get_scalar_ty());\n-  for(indices_t idx: idxs_.at(x))\n-    vals_[x][idx] = ConstantFP::get(ty, x->get_value());\n+  for(indices_t idx: idxs_.at(x)) {\n+    // manually select bf16 constant\n+    if (x->get_type()->get_scalar_ty()->is_bf16_ty()) {\n+      // highest 16 bits of fp32\n+      float fp32_value = x->get_value();\n+      uint16_t bf16_raw = (*reinterpret_cast<uint32_t*>(&fp32_value) \n+                            & 0xffff0000) >> 16;\n+      std::stringstream const_str;\n+      const_str << \"0x\" << std::hex << bf16_raw << \"U\"; // unsigned\n+      InlineAsm *bf16_const = InlineAsm::get(FunctionType::get(bf16_ty, {}, false),\n+                                             \" mov.b16 $0, \" + const_str.str() + \";\",\n+                                             \"=h\", false);\n+      vals_[x][idx] = builder_->CreateCall(bf16_const, {});\n+    } else\n+      vals_[x][idx] = ConstantFP::get(ty, x->get_value());\n+  }\n }\n \n void generator::visit_alloc_const(ir::alloc_const *alloc) {"}, {"filename": "lib/ir/constant.cc", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -18,6 +18,8 @@ constant *constant::get_null_value(type *ty) {\n     return constant_int::get(ty, 0);\n   case type::FP16TyID:\n     return constant_fp::get(type::get_fp16_ty(ctx), 0);\n+  case type::BF16TyID:\n+    return constant_fp::get(type::get_bf16_ty(ctx), 0);\n   case type::FP32TyID:\n     return constant_fp::get(type::get_fp32_ty(ctx), 0);\n   case type::FP64TyID:"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 78, "deletions": 27, "changes": 105, "file_content_changes": "@@ -33,27 +33,37 @@ def numpy_random(shape, dtype_str, rs: Optional[RandomState] = None, low=None, h\n         shape = (shape, )\n     if rs is None:\n         rs = RandomState(seed=17)\n-    dtype = getattr(np, dtype_str)\n     if dtype_str in int_dtypes + uint_dtypes:\n         iinfo = np.iinfo(getattr(np, dtype_str))\n         low = iinfo.min if low is None else max(low, iinfo.min)\n         high = iinfo.max if high is None else min(high, iinfo.max)\n+        dtype = getattr(np, dtype_str)\n         x = rs.randint(low, high, shape, dtype=dtype)\n         x[x == 0] = 1  # Hack. Never return zero so tests of division don't error out.\n         return x\n     elif dtype_str in float_dtypes:\n-        return rs.normal(0, 1, shape).astype(dtype)\n+        return rs.normal(0, 1, shape).astype(dtype_str)\n+    elif dtype_str == 'bfloat16':\n+        return (rs.normal(0, 1, shape).astype('float32').view('uint32')\n+                & np.uint32(0xffff0000)).view('float32')\n     else:\n         raise RuntimeError(f'Unknown dtype {dtype_str}')\n \n \n-def to_triton(x: np.ndarray, device='cuda') -> Union[TensorWrapper, torch.Tensor]:\n+def to_triton(x: np.ndarray, device='cuda', dst_type=None) -> Union[TensorWrapper, torch.Tensor]:\n+    '''\n+    Note: We need dst_type becasue the type of x can be different from dst_type.\n+          For example: x is of type `float32`, dst_type is `bfloat16`.\n+          If dst_type is None, we infer dst_type from x.\n+    '''\n     t = x.dtype.name\n     if t in uint_dtypes:\n         signed_type_name = t.lstrip('u')  # e.g. \"uint16\" -> \"int16\"\n         x_signed = x.astype(getattr(np, signed_type_name))\n         return reinterpret(torch.tensor(x_signed, device=device), getattr(tl, t))\n     else:\n+        if t == 'float32' and dst_type == 'bfloat16':\n+            return torch.tensor(x, device=device).bfloat16()\n         return torch.tensor(x, device=device)\n \n \n@@ -72,6 +82,8 @@ def to_numpy(x):\n     if isinstance(x, TensorWrapper):\n         return x.base.cpu().numpy().astype(getattr(np, torch_dtype_name(x.dtype)))\n     elif isinstance(x, torch.Tensor):\n+        if x.dtype is torch.bfloat16:\n+            return x.cpu().float().numpy()\n         return x.cpu().numpy()\n     else:\n         raise ValueError(f\"Not a triton-compatible tensor: {x}\")\n@@ -84,19 +96,30 @@ def patch_kernel(template, to_replace):\n     return kernel\n \n \n-@pytest.mark.parametrize(\"dtype_x\", [dtype_x for dtype_x in dtypes])\n+def check_type_supported(dtype):\n+    '''\n+    skip test if dtype is not supported on the current device\n+    '''\n+    cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+    if cc < 80 and (dtype is tl.bfloat16 or dtype == \"bfloat16\" or dtype is torch.bfloat16):\n+        pytest.skip(\"bfloat16 is only supported on NVGPU with cc >= 80\")\n+\n+\n+@pytest.mark.parametrize(\"dtype_x\", [dtype_x for dtype_x in dtypes] + [\"bfloat16\"])\n def test_empty_kernel(dtype_x, device='cuda'):\n     SIZE = 128\n \n     @triton.jit\n     def kernel(X, SIZE: tl.constexpr):\n         pass\n-    x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device)\n+    check_type_supported(dtype_x)\n+    x = to_triton(numpy_random(SIZE, dtype_str=dtype_x), device=device, dst_type=dtype_x)\n     kernel[(1, )](x, SIZE=SIZE, num_warps=4)\n \n \n # generic test functions\n def _test_unary(dtype_x, expr, numpy_expr=None, device='cuda'):\n+    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -115,8 +138,8 @@ def kernel(Z, X, SIZE: tl.constexpr):\n     # reference result\n     z_ref = eval(expr if numpy_expr is None else numpy_expr)\n     # triton result\n-    x_tri = to_triton(x, device=device)\n-    z_tri = to_triton(np.empty_like(z_ref), device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype_x)\n+    z_tri = to_triton(np.empty_like(z_ref), device=device, dst_type=dtype_x)\n     kernel[(1, )](z_tri, x_tri, SIZE=SIZE, num_warps=4)\n     # compare\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n@@ -154,6 +177,8 @@ def _binary_op_dtype_override(a: str, b: str) -> Optional[np.dtype]:\n \n \n def _test_binary(dtype_x, dtype_y, expr, numpy_expr=None, mode_x='real', mode_y='real', device='cuda', y_low=None, y_high=None):\n+    check_type_supported(dtype_x)  # early return if dtype_x is not supported\n+    check_type_supported(dtype_y)\n     SIZE = 128\n     # define the kernel / launch-grid\n \n@@ -180,8 +205,8 @@ def kernel(Z, X, Y, SIZE: tl.constexpr):\n     if dtype_z is not None:\n         z_ref = z_ref.astype(dtype_z)\n     # triton result\n-    x_tri = to_triton(x, device=device)\n-    y_tri = to_triton(y, device=device)\n+    x_tri = to_triton(x, device=device, dst_type=dtype_x)\n+    y_tri = to_triton(y, device=device, dst_type=dtype_y)\n     z_tri = to_triton(np.empty(SIZE, dtype=z_ref.dtype), device=device)\n     kernel[(1, )](z_tri, x_tri, y_tri, SIZE=SIZE, num_warps=4)\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), err_msg=expr, rtol=0.01)\n@@ -193,15 +218,20 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n     # remainders than stock LLVM. We currently don't expect to match it\n     # bit-for-bit.\n     return (dtype_x, dtype_y) in [\n+        ('int32', 'bfloat16'),\n         ('int32', 'float16'),\n         ('int32', 'float32'),\n+        ('int64', 'bfloat16'),\n         ('int64', 'float16'),\n         ('int64', 'float32'),\n         ('int64', 'float64'),\n+        ('uint16', 'bfloat16'),\n         ('uint16', 'float16'),\n         ('uint16', 'float32'),\n+        ('uint32', 'bfloat16'),\n         ('uint32', 'float16'),\n         ('uint32', 'float32'),\n+        ('uint64', 'bfloat16'),\n         ('uint64', 'float16'),\n         ('uint64', 'float32'),\n         ('uint64', 'float64'),\n@@ -215,15 +245,15 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n     (dtype_x, dtype_y, op)\n     for op in ['+', '-', '*', '/', '%']\n-    for dtype_x in dtypes\n-    for dtype_y in dtypes\n+    for dtype_x in dtypes + ['bfloat16']\n+    for dtype_y in dtypes + ['bfloat16']\n ])\n def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n     expr = f' x {op} y'\n     if op == '%' and dtype_x in int_dtypes + uint_dtypes and dtype_y in int_dtypes + uint_dtypes:\n         # LLVM has 'numpy.fmod', not 'numpy.remainder', semantics on integer remainders.\n         numpy_expr = 'np.fmod(x, y)'\n-    elif op in ('/', '%') and dtype_x in ('int16', 'float16') and dtype_y in ('int16', 'float16'):\n+    elif op in ('/', '%') and dtype_x in ('int16', 'float16', 'bfloat16') and dtype_y in ('int16', 'float16', 'bfloat16'):\n         # Triton promotes 16-bit floating-point / and % to 32-bit because there\n         # are no native div or FRem operations on float16. Since we have to\n         # convert anyway, we may as well take the accuracy bump.\n@@ -266,8 +296,8 @@ def test_floordiv(dtype_x, dtype_y, device='cuda'):\n @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n     (dtype_x, dtype_y, op)\n     for op in ['&', '|', '^']\n-    for dtype_x in dtypes\n-    for dtype_y in dtypes\n+    for dtype_x in dtypes + ['bfloat16']\n+    for dtype_y in dtypes + ['bfloat16']\n ])\n def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n     expr = f'x {op} y'\n@@ -337,7 +367,7 @@ def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n # test unary ops\n # ---------------\n @pytest.mark.parametrize(\"dtype_x, expr\", [\n-    (dtype_x, ' -x') for dtype_x in dtypes\n+    (dtype_x, ' -x') for dtype_x in dtypes + ['bfloat16']\n ] + [\n     (dtype_x, ' ~x') for dtype_x in int_dtypes\n ])\n@@ -732,9 +762,10 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n @pytest.mark.parametrize(\"op, dtype_str, shape\",\n                          [(op, dtype, shape)\n                           for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-                          for dtype in dtypes\n+                          for dtype in dtypes + ['bfloat16']\n                           for shape in [32, 64, 128, 512]])\n def test_reduce1d(op, dtype_str, shape, device='cuda'):\n+    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -752,9 +783,18 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     # numpy result\n     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n-    z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+    z_tri_dtype_str = z_dtype_str\n+    if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+        z_dtype_str = 'float32'\n+        z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n+        # trunc mantissa for a fair comparison of accuracy\n+        z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+        z_tri_dtype_str = 'bfloat16'\n+    else:\n+        z_ref = numpy_op(x).astype(getattr(np, z_dtype_str))\n     # triton result\n-    z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs), device=device)\n+    z_tri = to_triton(numpy_random((1,), dtype_str=z_dtype_str, rs=rs),\n+                      device=device, dst_type=z_tri_dtype_str)\n     kernel[(1,)](x_tri, z_tri, BLOCK=shape)\n     z_tri = to_numpy(z_tri)\n     # compare\n@@ -770,7 +810,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n \n reduce_configs1 = [\n-    (op, dtype, (1, 1024), axis) for dtype in dtypes\n+    (op, dtype, (1, 1024), axis) for dtype in dtypes + ['bfloat16']\n     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n     for axis in [1]\n ]\n@@ -805,11 +845,19 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n     numpy_op = {'sum': np.sum, 'max': np.max, 'min': np.min,\n                 'argmin': np.argmin, 'argmax': np.argmax}[op]\n     z_dtype_str = 'int32' if op == 'argmin' or op == 'argmax' else dtype_str\n+    z_tri_dtype_str = z_dtype_str\n     # numpy result\n-    z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+    if op not in ['argmin', 'argmax'] and dtype_str == 'bfloat16':\n+        z_dtype_str = 'float32'\n+        z_tri_dtype_str = 'bfloat16'\n+        z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n+        # trunc mantissa for a fair comparison of accuracy\n+        z_ref = (z_ref.view('uint32') & np.uint32(0xffff0000)).view('float32')\n+    else:\n+        z_ref = numpy_op(x, axis=axis).astype(getattr(np, z_dtype_str))\n     # triton result\n     z_tri = to_triton(numpy_random((shape[1 - axis],), dtype_str=z_dtype_str, rs=rs),\n-                      device=device)\n+                      device=device, dst_type=z_tri_dtype_str)\n     kernel[(1,)](x_tri, z_tri, BLOCK_M=shape[0], BLOCK_N=shape[1], AXIS=axis)\n     z_tri = to_numpy(z_tri)\n     # compare\n@@ -834,10 +882,11 @@ def kernel(X, Z, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, AXIS: tl.constexp\n \n @pytest.mark.parametrize(\"dtype_str, shape, perm\",\n                          [(dtype, shape, perm)\n-                          for dtype in ['float16', 'float32']\n+                          for dtype in ['bfloat16', 'float16', 'float32']\n                              for shape in [(64, 64), (128, 128)]\n                              for perm in [(1, 0)]])\n def test_permute(dtype_str, shape, perm, device='cuda'):\n+    check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n \n     # triton kernel\n     @triton.jit\n@@ -852,16 +901,16 @@ def kernel(X, stride_xm, stride_xn,\n     # input\n     x = numpy_random(shape, dtype_str=dtype_str)\n     # triton result\n-    z_tri = to_triton(np.empty_like(x), device=device)\n-    z_tri_contiguous = to_triton(np.empty_like(x), device=device)\n-    x_tri = to_triton(x, device=device)\n+    z_tri = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n+    z_tri_contiguous = to_triton(np.empty_like(x), device=device, dst_type=dtype_str)\n+    x_tri = to_triton(x, device=device, dst_type=dtype_str)\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          z_tri, z_tri.stride(1), z_tri.stride(0),\n                          BLOCK_M=shape[0], BLOCK_N=shape[1])\n     pgm_contiguous = kernel[(1, 1)](x_tri, x_tri.stride(1), x_tri.stride(0),\n                                     z_tri_contiguous, z_tri_contiguous.stride(0), z_tri_contiguous.stride(1),\n                                     BLOCK_M=shape[0], BLOCK_N=shape[1])\n-    # torch result\n+    # numpy result\n     z_ref = x.transpose(*perm)\n     # compare\n     triton.testing.assert_almost_equal(z_tri, z_ref)\n@@ -1038,8 +1087,10 @@ def _kernel(z, BLOCK: tl.constexpr,\n # Testing masked loads with an intermate copy to shared memory run.\n \n \n-@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.float32])\n+@pytest.mark.parametrize(\"dtype\", [torch.bfloat16, torch.float16, torch.float32])\n def test_masked_load_shared_memory(dtype, device='cuda'):\n+    check_type_supported(dtype)  # bfloat16 on cc < 80 will not be tested\n+\n     M = 32\n     N = 32\n     K = 16"}, {"filename": "python/test/unit/operators/test_cross_entropy.py", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "file_content_changes": "@@ -2,18 +2,22 @@\n import torch\n \n import triton\n+import triton._C.libtriton.triton as _triton\n \n \n @pytest.mark.parametrize(\"M, N, dtype, mode\",\n                          [\n                              (M, N, dtype, mode) for M in [1024, 821]\n                              for N in [512, 857, 1871, 2089, 8573, 31000]\n-                             for dtype in ['float16', 'float32']\n+                             for dtype in ['bfloat16', 'float16', 'float32']\n                              for mode in ['forward', 'backward']\n                          ]\n                          )\n def test_op(M, N, dtype, mode):\n-    dtype = {'float16': torch.float16, 'float32': torch.float32}[dtype]\n+    cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n+    if cc < 80 and dtype == \"bfloat16\":\n+        pytest.skip(\"Only test bfloat16 on devices with sm >= 80\")\n+    dtype = {'bfloat16': torch.bfloat16, 'float16': torch.float16, 'float32': torch.float32}[dtype]\n     # create inputs\n     x = torch.randn(M, N, dtype=dtype, device='cuda', requires_grad=True)\n     idx = 4 + torch.ones(M, dtype=torch.int64, device='cuda')"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 30, "deletions": 9, "changes": 39, "file_content_changes": "@@ -58,14 +58,22 @@ def computation_type_impl(a_ty: tl.dtype, b_ty: tl.dtype, div_or_mod: bool) -> t\n         return tl.float32\n     # 3 ) if one operand is half, the other is implicitly converted to half\n     #     unless we're doing / or %, which do not exist natively in PTX for fp16.\n+    #     Supported PTX op: add, sub, mul, fma, neg, abs, min, max, tanh, ex2, setp\n     if a_ty.is_fp16() or b_ty.is_fp16():\n         if div_or_mod:\n             return tl.float32\n         else:\n             return tl.float16\n+    # 4) return bf16 only if both operands are of bf16\n+    if a_ty.is_bf16() or b_ty.is_bf16():\n+        if div_or_mod:\n+            return tl.float32\n+        if a_ty.is_bf16() and b_ty.is_bf16():\n+            return tl.bfloat16\n+        return tl.float32\n     if not a_ty.is_int() or not b_ty.is_int():\n         assert False\n-    # 4 ) both operands are integer and undergo\n+    # 5 ) both operands are integer and undergo\n     #    integer promotion\n     if div_or_mod and a_ty.int_signedness != b_ty.int_signedness:\n         raise ValueError(\"Cannot use /, #, or % with \" + a_ty.__repr__() + \" and \" + b_ty.__repr__() + \" because they have different signedness;\"\n@@ -768,16 +776,25 @@ def atomic_cas(ptr: tl.tensor,\n                cmp: tl.tensor,\n                val: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    # TODO: type checking\n+    element_ty = ptr.type.scalar.element_ty\n+    if element_ty.primitive_bitwidth not in [16, 32, 64]:\n+        raise ValueError(\"atomic_cas only supports elements with width {16, 32, 64}\")\n     return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.type)\n \n \n def atom_red_typechecking_impl(ptr: tl.tensor,\n                                val: tl.tensor,\n                                mask: tl.tensor,\n+                               op: str,\n                                builder: ir.builder) -> Tuple[tl.tensor, tl.tensor, tl.tensor]:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n+\n+    element_ty = ptr.type.scalar.element_ty\n+    if element_ty is tl.float16 and op != 'add':\n+        raise ValueError(\"atomic_\" + op + \" does not support fp16\")\n+    if element_ty in [tl.int1, tl.int8, tl.int16, tl.bfloat16]:\n+        raise ValueError(\"atomic_\" + op + \" does not support \" + element_ty)\n     if ptr.type.is_block():\n         if mask:\n             mask = broadcast_impl_shape(mask, ptr.type.get_block_shapes(), builder)\n@@ -798,7 +815,7 @@ def atomic_max(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'max', builder)\n     sca_ty = val.type.scalar\n     # direct call to atomic_max for integers\n     if sca_ty.is_int():\n@@ -830,7 +847,7 @@ def atomic_min(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'min', builder)\n     sca_ty = val.type.scalar\n     # direct call to atomic_min for integers\n     if sca_ty.is_int():\n@@ -870,7 +887,7 @@ def atomic_add(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'add', builder)\n     sca_ty = val.type.scalar\n     op = ir.ATOMIC_OP.FADD if sca_ty.is_floating() else ir.ATOMIC_OP.ADD\n     return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.type)\n@@ -880,31 +897,31 @@ def atomic_and(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'and', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_or(ptr: tl.tensor,\n               val: tl.tensor,\n               mask: tl.tensor,\n               builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'or', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xor(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n                builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xor', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.type)\n \n \n def atomic_xchg(ptr: tl.tensor,\n                 val: tl.tensor,\n                 mask: tl.tensor,\n                 builder: ir.builder) -> tl.tensor:\n-    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, builder)\n+    ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xchg', builder)\n     return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.type)\n \n # ===----------------------------------------------------------------------===//\n@@ -978,6 +995,10 @@ def reduce_impl(input: tl.tensor, axis: int, builder: ir.builder, name: str,\n     if scalar_ty.is_int() and scalar_ty.int_bitwidth <= 32:\n         input = cast(input, tl.int32, builder)\n \n+    # hardware doesn't support FMAX, FMIN, CMP for bfloat16\n+    if scalar_ty is tl.bfloat16:\n+        input = cast(input, tl.float32, builder)\n+\n     # choose the right unsigned operation\n     if scalar_ty.is_int_unsigned():\n         int_op_to_unit = {"}, {"filename": "python/triton/ops/cross_entropy.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -65,7 +65,7 @@ def _backward(PROBS, IDX, DPROBS, N, BLOCK: tl.constexpr):\n     # write result in-place in PROBS\n     dout = tl.load(DPROBS + row)\n     din = (probs - delta) * dout\n-    tl.store(PROBS, din.to(tl.float16), mask=cols < N)\n+    tl.store(PROBS, din.to(PROBS.dtype.element_ty), mask=cols < N)\n \n \n class _cross_entropy(torch.autograd.Function):"}]