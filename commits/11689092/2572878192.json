[{"filename": "include/triton/codegen/analysis/layout.h", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -258,7 +258,8 @@ class shared_layout: public data_layout {\n                 const std::vector<unsigned>& shapes,\n                 const std::vector<ir::value *> &values_,\n                 ir::type *ty,\n-                analysis::align* align, target *tgt);\n+                analysis::align* align, target *tgt,\n+                bool is_tmp = false);\n   void accept(layout_visitor* vst) { vst->visit_layout_shared(this); }\n   // accessors\n   size_t get_size()                         { return size_; }\n@@ -276,6 +277,7 @@ class shared_layout: public data_layout {\n   int  get_mma_strided()                    { return mma_strided_; }\n   bool allow_swizzle() const                { return allow_swizzle_; }\n   data_layout* get_arg_layout()             { return arg_layout_; }\n+  bool is_tmp() const                       { return is_tmp_; }\n \n private:\n   size_t size_;\n@@ -290,6 +292,7 @@ class shared_layout: public data_layout {\n   int mma_strided_;\n   bool allow_swizzle_ = true;\n   target *tgt_;\n+  bool is_tmp_;\n };\n \n "}, {"filename": "include/triton/codegen/transform/coalesce.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "file_content_changes": "@@ -32,11 +32,12 @@ class coalesce {\n   ir::value* rematerialize(ir::value *v, ir::builder& builder, std::map<ir::value*, ir::value*>& seen);\n \n public:\n-  coalesce(analysis::align* align, triton::codegen::analysis::layouts *layouts);\n+  coalesce(analysis::align* align, triton::codegen::analysis::layouts *layouts, bool has_sm80);\n   triton::ir::value *simplify(ir::instruction* i, triton::ir::builder &builder);\n   void run(ir::module &mod);\n \n private:\n+  bool has_sm80_;\n   analysis::align* align_;\n   analysis::layouts* layout_;\n };"}, {"filename": "include/triton/codegen/transform/cts.h", "status": "modified", "additions": 11, "deletions": 3, "changes": 14, "file_content_changes": "@@ -15,18 +15,26 @@ namespace ir {\n }\n \n namespace codegen{\n+\n+namespace analysis{\n+class layouts;\n+}\n+\n namespace transform{\n \n class cts {\n private:\n-  void add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder, bool to_shared);\n+  bool is_shmem_op(ir::instruction* i, int op);\n+  bool is_shmem_res(ir::value* i);\n+void add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder, bool to_shared, std::map<ir::value*,ir::value*>& copies);\n \n public:\n-  cts(bool use_async = false): use_async_(use_async) {}\n+  cts(analysis::layouts* layouts, bool has_sm80 = false): layouts_(layouts), has_sm80_(has_sm80) {}\n   void run(ir::module &mod);\n \n private:\n-  bool use_async_;\n+  bool has_sm80_;\n+  analysis::layouts* layouts_;\n };\n \n }"}, {"filename": "include/triton/ir/builder.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -142,9 +142,9 @@ class builder{\n   value *create_or(value *lhs, value *rhs);\n   // Input/Output\n   value *create_load(value *arg, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY eviction, bool is_volatile);\n-  value *create_store(value *ptr, value *val);\n+  value *create_store(value *ptr, value *val, store_inst::EVICTION_POLICY eviction);\n   value *create_masked_load(value *arg, value *mask, value *false_value, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY eviction, bool is_volatile);\n-  value *create_masked_store(value *ptr, value *val, value *mask);\n+  value *create_masked_store(value *ptr, value *val, value *mask, store_inst::EVICTION_POLICY eviction);\n   // Struct instructions\n   value *create_insert_value(value* val, value *elt, size_t idx);\n   value *create_extract_value(value* val, size_t idx);\n@@ -176,7 +176,7 @@ class builder{\n   value *create_cos(value* arg);\n   value *create_sin(value* arg);\n   value *create_log(value* arg);\n-  value *create_dot(value *A, value *B, value *C, bool allow_tf32);\n+  value *create_dot(value *A, value *B, value *C, bool trans_a, bool trans_b, bool allow_tf32);\n   value *create_trans(value *A, const std::vector<int> &perm = {});\n   value *create_sqrt(value *A);\n   value *create_reduce(value *A, reduce_inst::op_t op, unsigned axis);"}, {"filename": "include/triton/ir/function.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -112,7 +112,7 @@ class function: public global_object{\n   static function *create(function_type *ty, linkage_types_t linkage,\n                           const std::string &name, module *mod);\n   // blocks\n-  const blocks_t &blocks() { return blocks_; }\n+        blocks_t &blocks() { return blocks_; }\n   const blocks_t &blocks() const { return blocks_; }\n   void insert_block(basic_block* block, basic_block *next = nullptr);\n "}, {"filename": "include/triton/ir/instructions.h", "status": "modified", "additions": 30, "deletions": 18, "changes": 48, "file_content_changes": "@@ -435,13 +435,31 @@ class getelementptr_inst: public instruction {\n //===----------------------------------------------------------------------===//\n \n class io_inst: public instruction {\n+public:\n+\n+  enum EVICTION_POLICY : uint32_t {\n+    NORMAL=0,\n+    EVICT_FIRST,\n+    EVICT_LAST,\n+  };\n+\n protected:\n-  io_inst(type *ty, value_id_t id, unsigned num_ops,\n+  io_inst(type *ty, value_id_t id, unsigned num_ops, EVICTION_POLICY eviction,\n           const std::string &name = \"\", instruction *next = nullptr);\n \n+  std::string get_eviction_policy_repr() const {\n+    if (eviction_ == EVICT_FIRST) return \".L1::evict_first\";\n+    if (eviction_ == EVICT_LAST) return \".L2::evict_last\";\n+    return \"\";\n+  }\n+\n public:\n   // accessors\n   value *get_pointer_operand() { return get_operand(0); }\n+  EVICTION_POLICY get_eviction_policy() const { return eviction_; }\n+\n+protected:\n+  EVICTION_POLICY eviction_;\n };\n \n // load\n@@ -453,14 +471,8 @@ class load_inst: public io_inst {\n     CG,\n   };\n \n-  enum EVICTION_POLICY : uint32_t {\n-    NORMAL=0,\n-    EVICT_FIRST,\n-    EVICT_LAST,\n-  };\n \n   CACHE_MODIFIER get_cache_modifier() const { return cache_; }\n-  EVICTION_POLICY get_eviction_policy() const { return eviction_; }\n   bool get_is_volatile() const { return is_volatile_; }\n \n protected:\n@@ -472,12 +484,6 @@ class load_inst: public io_inst {\n     if (cache_ == CG) return \".cg\";\n     return \"\"; \n   }\n-  std::string get_eviction_policy_repr() const {\n-    if (eviction_ == EVICT_FIRST) return \".L1::evict_first\";\n-    if (eviction_ == EVICT_LAST) return \".L2::evict_last\";\n-    return \"\";\n-  }\n-  EVICTION_POLICY eviction_;\n   CACHE_MODIFIER cache_;\n \n   std::string get_volatile_repr() {\n@@ -553,7 +559,7 @@ class masked_load_async_inst: public load_inst {\n // store\n class store_inst: public io_inst {\n protected:\n-  store_inst(value *ptr, value_id_t id, unsigned num_ops,\n+  store_inst(value *ptr, value_id_t id, unsigned num_ops, EVICTION_POLICY eviction,\n             const std::string &name = \"\", instruction *next = nullptr);\n \n public:\n@@ -564,11 +570,11 @@ class store_inst: public io_inst {\n class unmasked_store_inst: public store_inst{\n private:\n   std::string repr_impl() const { return \"unmasked_store\"; }\n-  unmasked_store_inst(value *ptr, value *v, const std::string &name, instruction *next);\n+  unmasked_store_inst(value *ptr, value *v, EVICTION_POLICY eviction, const std::string &name, instruction *next);\n \n public:\n   // factory method\n-  static unmasked_store_inst* create(value* ptr, value *v,\n+  static unmasked_store_inst* create(value* ptr, value *v, EVICTION_POLICY eviction,\n                                     const std::string &name = \"\",\n                                     instruction *next = nullptr);\n   _TRITON_DEFINE_CLONE(unmasked_store_inst)\n@@ -578,14 +584,14 @@ class unmasked_store_inst: public store_inst{\n class masked_store_inst: public store_inst{\n private:\n   std::string repr_impl() const { return \"masked_store\"; }\n-  masked_store_inst(value *ptr, value *v, value *mask,\n+  masked_store_inst(value *ptr, value *v, value *mask, EVICTION_POLICY eviction,\n                     const std::string &name, instruction *next);\n \n public:\n   // accessors\n   value *get_mask_operand() { return get_operand(2); }\n   // factory method\n-  static masked_store_inst* create(value *ptr, value *v, value *mask,\n+  static masked_store_inst* create(value *ptr, value *v, value *mask, EVICTION_POLICY eviction,\n                                    const std::string &name = \"\",\n                                    instruction *next = nullptr);\n   _TRITON_DEFINE_CLONE(masked_store_inst)\n@@ -755,6 +761,8 @@ class get_num_programs_inst: public builtin_inst {\n class atomic_inst: public io_inst {\n public:\n   using io_inst::io_inst;\n+  atomic_inst(type *ty, value_id_t id, unsigned num_ops, const std::string &name, instruction *next):\n+    io_inst(ty, id, num_ops, NORMAL, name, next) {}\n };\n \n class atomic_rmw_inst: public atomic_inst {\n@@ -856,6 +864,8 @@ class dot_inst: public builtin_inst {\n   bool is_prefetched() const { return is_prefetched_; }\n   void set_prefetched(bool is_prefetched) { is_prefetched_ = is_prefetched; }\n   bool allow_tf32() const { return allow_tf32_; }\n+  bool is_trans_a() const { return AT_ == Trans; }\n+  bool is_trans_b() const { return BT_ == Trans; }\n \n public:\n   static instruction *create(value *A, value *B, value *C, bool AT, bool BT, bool allow_tf32, const std::string &name = \"\", instruction *next = nullptr);\n@@ -872,6 +882,8 @@ class dot_inst: public builtin_inst {\n   DataType C_type_ = DataType::FP32;\n   DataType A_type_ = DataType::FP16;\n   DataType B_type_ = DataType::FP16;\n+  TransT AT_;\n+  TransT BT_;\n };\n \n //class outer_inst: public builtin_inst {"}, {"filename": "include/triton/ir/utils.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -22,6 +22,7 @@ class cfg {\n };\n \n void for_each_instruction(ir::module& mod, const std::function<void(triton::ir::instruction*)> &fn);\n+void for_each_instruction_backward(module &mod, const std::function<void (instruction *)> &do_work);\n void for_each_value(ir::module& mod, const std::function<void(triton::ir::value *)> &fn);\n \n }"}, {"filename": "lib/codegen/analysis/allocation.cc", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -92,8 +92,10 @@ void allocation::run(ir::module &mod) {\n   }\n   // Save maximum size of induced memory space\n   allocated_size_ = 0;\n-  for(shared_layout* x: V)\n+  for(shared_layout* x: V){\n     allocated_size_ = std::max<size_t>(allocated_size_, starts[x] + x->get_size());\n+    // std::cout << \"start: \" << starts[x] << \" | end: \" << starts[x] + x->get_size() << std::endl;\n+  }\n }\n \n }"}, {"filename": "lib/codegen/analysis/layout.cc", "status": "modified", "additions": 39, "deletions": 20, "changes": 59, "file_content_changes": "@@ -212,11 +212,9 @@ mma_layout::mma_layout(size_t num_warps,\n     order_ = {0, 1};\n   }\n   else{\n-    // fpw_ = {1, 1, 1};\n     spw_ = mma_instr_shape_.at(tensor_core_type_); // e.g., {16, 8, 16} for f32.f16.f16.f32\n     contig_per_thread_ = {1, 2};\n     order_ = {1, 0};\n-    // rep_ = {2,  2, 1};\n   }\n \n   /* warps per tile */\n@@ -233,24 +231,45 @@ mma_layout::mma_layout(size_t num_warps,\n     }while(wpt_nm1 != wpt_);\n   } else {\n     bool changed = false;\n-    do {\n-      changed = false;\n-      if (wpt_[0] * wpt_[1] * wpt_[2] >= num_warps)\n-        break;\n-      if (shape_[0] / spw_[0] / wpt_[0] >= shape_[1] / (spw_[1]*2) / wpt_[1]) {\n-        if (wpt_[0] < shape_[0] / spw_[0]) {\n-          wpt_[0] *= 2;\n-          changed = true;\n-        }\n-      } else {\n-        if (wpt_[1] < shape_[1] / (spw_[1]*2)) {\n-          wpt_[1] *= 2;\n-          changed = true;\n+    // try to have a warp own entire rows of the output\n+    // this makes it easier to fuse multiple mmas by fusing\n+    // registers\n+    bool one_warp_per_row = false;\n+    for(ir::value* v: values)\n+    for(ir::user* u: v->get_users()){\n+      auto* dot = dynamic_cast<ir::dot_inst*>(u);\n+      auto* cts = dynamic_cast<ir::copy_to_shared_inst*>(u);\n+      if((dot && dot->get_operand(2)!=v) || !layout_a->to_shared() || cts)\n+        one_warp_per_row = shape[0] / spw_[0] >= num_warps;\n+    }\n+    // std::cout << one_warp_per_row << std::endl;\n+\n+    if(one_warp_per_row){\n+      wpt_[1] = 1;\n+      wpt_[0] = num_warps;\n+    }\n+    else{\n+      do {\n+        changed = false;\n+        if (wpt_[0] * wpt_[1] * wpt_[2] >= num_warps)\n+          break;\n+        if (shape_[0] / spw_[0] / wpt_[0] >= shape_[1] / (spw_[1]*2) / wpt_[1]) {\n+          if (wpt_[0] < shape_[0] / spw_[0]) {\n+            wpt_[0] *= 2;\n+            changed = true;\n+          }\n+        } else {\n+          if (wpt_[1] < shape_[1] / (spw_[1]*2)) {\n+            wpt_[1] *= 2;\n+            changed = true;\n+          }\n         }\n-      }\n-    } while (changed);\n+      } while(changed);\n+    }\n   }\n \n+  // std::cout << wpt_[0] << \" \" << wpt_[1] << std::endl;\n+\n   /* shape per block */\n   shape_per_cta_ = {spw_[0]*wpt_[0], spw_[1]*wpt_[1], 1};\n }\n@@ -430,8 +449,8 @@ shared_layout::shared_layout(data_layout *arg,\n                                  const std::vector<unsigned>& shape,\n                                  const std::vector<ir::value *> &values,\n                                  ir::type *ty,\n-                                 analysis::align* align, target *tgt)\n-    : data_layout(SHARED, axes, shape, values, align), ty_(ty), tgt_(tgt) {\n+                                 analysis::align* align, target *tgt, bool is_tmp)\n+    : data_layout(SHARED, axes, shape, values, align), ty_(ty), tgt_(tgt), is_tmp_(is_tmp){\n \n   size_ = 0;\n   arg_layout_ = arg;\n@@ -619,7 +638,7 @@ void layouts::create_tmp_layout(size_t id, data_layout *arg,\n                                 ir::instruction *i, bool is_index) {\n   ir::type *ty = is_index ? ir::type::get_int32_ty(i->get_type()->get_context())\n                           : i->get_type()->get_scalar_ty();\n-  layouts_[id] = new shared_layout(arg, axes, shape, {i}, ty, align_, tgt_);\n+  layouts_[id] = new shared_layout(arg, axes, shape, {i}, ty, align_, tgt_, true);\n   if (is_index) {\n     tmp_index_[i] = id;\n   } else {"}, {"filename": "lib/codegen/analysis/liveness.cc", "status": "modified", "additions": 90, "deletions": 25, "changes": 115, "file_content_changes": "@@ -14,43 +14,108 @@ namespace analysis{\n void liveness::run(ir::module &mod) {\n   intervals_.clear();\n \n+  std::map<ir::value*, std::set<shared_layout*>> layouts_map;\n+  for(auto &x: layouts_->get_all()){\n+    shared_layout* layout = x.second->to_shared();\n+    if(!layout || layout->is_tmp())\n+      continue;\n+    for(ir::value* v:layout->get_values()){\n+      layouts_map[v].insert(layout);\n+    }\n+  }\n+\n+\n+\n+  std::map<ir::user*, std::set<shared_layout*>> live_in;\n+  while(true){\n+    bool changed = false;\n+    ir::instruction* last_inst = nullptr;\n+    ir::for_each_instruction_backward(mod, [&](ir::instruction* i){\n+      // gen\n+      std::set<shared_layout*> gen;\n+      for(ir::value* v: i->ops())\n+      for(shared_layout* layout: layouts_map[v])\n+        gen.insert(layout);\n+      // kill\n+      std::set<shared_layout*> kill;\n+      for(shared_layout* layout: layouts_map[i])\n+        kill.insert(layout);\n+      // temporaries are handled separately\n+      if(layouts_->has_tmp(i)){\n+        gen.insert(layouts_->get(layouts_->tmp(i))->to_shared());\n+        kill.insert(layouts_->get(layouts_->tmp(i))->to_shared());\n+      }\n+      if(layouts_->has_tmp_index(i)){\n+        gen.insert(layouts_->get(layouts_->tmp_index(i))->to_shared());\n+        kill.insert(layouts_->get(layouts_->tmp_index(i))->to_shared());\n+      }\n+      // live-out\n+      std::set<shared_layout*> live_out;\n+      std::vector<ir::instruction*> succs = {last_inst};\n+      if(i == i->get_parent()->get_inst_list().back())\n+        for(ir::basic_block* succ: i->get_parent()->get_successors())\n+          succs.push_back(succ->get_inst_list().front());\n+      for(ir::instruction* succ: succs)\n+      for(shared_layout* layout: live_in[succ])\n+      if(!layout->is_tmp())\n+        live_out.insert(layout);\n+\n+      // new sets\n+      std::set<shared_layout*> live_out_minus_kill;\n+      std::set_difference(live_out.begin(), live_out.end(), kill.begin(), kill.end(), \n+                          std::inserter(live_out_minus_kill, live_out_minus_kill.end()));\n+      std::set<shared_layout*> new_live_in;\n+      std::set_union(gen.begin(), gen.end(), live_out_minus_kill.begin(), live_out_minus_kill.end(),\n+                      std::inserter(new_live_in, new_live_in.end()));\n+      \n+      changed = changed || (new_live_in != live_in[i]);\n+      live_in[i] = new_live_in;\n+      last_inst = i;\n+    });\n+    if(!changed)\n+      break;\n+  }\n+    \n+  // ir::for_each_instruction(mod, [&](ir::instruction* i){\n+  //   i->print(std::cout);\n+  //   std::cout << \" live_in: \" << live_in[i].size() << std::endl;\n+  // });\n+\n+\n+\n   // Assigns index to each instruction\n   std::map<ir::value*, slot_index> indices;\n-  for(ir::function *fn: mod.get_function_list()){\n-    slot_index index = 0;\n-    for(ir::basic_block *block: fn->blocks())\n-    for(ir::instruction *instr: block->get_inst_list()){\n+  slot_index index = 0;\n+  ir::for_each_instruction(mod, [&](ir::instruction* instr){\n       index += 1;\n       indices.insert({instr, index});\n-    }\n+  });\n+  \n+\n+  for(auto &x: layouts_->get_all()){\n+    shared_layout* layout = x.second->to_shared();\n+    if(layout)\n+      intervals_[layout] = segment{INT32_MAX, 0};\n   }\n \n-  // create live intervals\n+  for(auto& x: live_in)\n+  for(shared_layout* layout: x.second)\n+    intervals_[layout].start = std::min<int>(intervals_[layout].start, indices[x.first]);\n+\n+  for(auto& x: live_in)\n+  for(shared_layout* layout: x.second){\n+    intervals_[layout].end = std::max<int>(intervals_[layout].end, indices[x.first] + 1);\n+  }\n+\n+  \n   for(auto &x: layouts_->get_all()) {\n     shared_layout* layout = x.second->to_shared();\n     if(!layout)\n       continue;\n-    // users\n-    std::set<ir::user*> users;\n-    for(ir::value *v: layout->get_values()){\n-      for(ir::user *u: v->get_users())\n-        users.insert(u);\n-    }\n-    // compute intervals\n-    unsigned start = INT32_MAX;\n-    for(ir::value *v: layout->get_values())\n-      if(indices.find(v) != indices.end())\n-        start = std::min(start, indices.at(v));\n-    unsigned end = 0;\n-    for(ir::user *u: users)\n-      if(indices.find(u) != indices.end())\n-        end = std::max(end, indices.at(u));\n-    if(end == 0)\n-      end = start + 1;\n-    intervals_[layout] = segment{start, end};\n+    // std::cout << intervals_[layout].start << \" \" << intervals_[layout].end << std::endl;\n   }\n \n-\n+  \n \n }\n "}, {"filename": "lib/codegen/analysis/swizzle.cc", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -28,12 +28,15 @@ void swizzle::run(ir::module &) {\n       }\n       auto ord = layout->get_order();\n       scanline_layout* in_layout = dynamic_cast<scanline_layout*>(layout->get_arg_layout());\n-      if(!in_layout)\n-        continue;\n+      int per_phase = 1;\n       int dtsize = layout->get_type()->get_scalar_ty()->get_primitive_size_in_bits() / 8;\n+      if(in_layout)\n+        per_phase = std::max<int>(128 / (in_layout->mts(ord[0])*in_layout->nts(ord[0])*dtsize), 1);\n+      else\n+        per_phase = 1;\n       if(tgt_->as_nvidia() && tgt_->as_nvidia()->sm() < 80){\n         int inner = mma_dot_a ? 0 : 1;\n-        per_phase_[layout] = std::max<int>(128 / (in_layout->mts(ord[0])*in_layout->nts(ord[0])*dtsize), 1);\n+        per_phase_[layout] = per_phase;\n         max_phase_[layout] = (ord[inner] == 1 ? 8 : 4) / per_phase_[layout];\n         if(mma_dot_a)\n           vec_[layout] = 2*layouts_->get(mma_dot_a)->to_mma()->rep(0);\n@@ -46,7 +49,7 @@ void swizzle::run(ir::module &) {\n           max_phase_[layout] = 1;\n           vec_[layout] = 1;\n         } else {\n-          per_phase_[layout] = std::max<int>(128 / (in_layout->mts(ord[0])*in_layout->nts(ord[0])*dtsize), 1);\n+          per_phase_[layout] = per_phase;\n           max_phase_[layout] = layout->get_mma_strided() / per_phase_[layout];\n           vec_[layout]       = layout->get_mma_vec();\n         }"}, {"filename": "lib/codegen/pass.cc", "status": "modified", "additions": 10, "deletions": 4, "changes": 14, "file_content_changes": "@@ -31,27 +31,28 @@ std::unique_ptr<llvm::Module> add_passes_to_emit_bin(ir::module &ir, llvm::LLVMC\n   std::string name = ir.get_function_list()[0]->get_name();\n   std::unique_ptr<llvm::Module> llvm(new llvm::Module(name, ctx));\n   // optimizations\n-  bool cts_use_async = target->as_nvidia() && target->as_nvidia()->sm() >= 80;\n+  bool has_sm80 = target->as_nvidia() && target->as_nvidia()->sm() >= 80;\n   // create passes\n   codegen::analysis::align align;\n   codegen::transform::inliner inliner;\n   codegen::analysis::axes axes;\n-  codegen::transform::cts cts(cts_use_async);\n-  codegen::transform::pipeline pipeline(cts_use_async, num_stages);\n+  codegen::transform::pipeline pipeline(has_sm80, num_stages);\n   codegen::transform::disassociate disassociate;\n   codegen::analysis::layouts layouts(&axes, &align, num_warps, target);\n+  codegen::transform::cts cts(&layouts, has_sm80);\n   codegen::analysis::liveness liveness(&layouts);\n   codegen::analysis::swizzle swizzle(&layouts, target);\n   codegen::analysis::allocation allocation(&liveness);\n   codegen::transform::dce dce;\n   codegen::transform::peephole peephole(target, &layouts);\n-  codegen::transform::coalesce coalesce(&align, &layouts);\n+  codegen::transform::coalesce coalesce(&align, &layouts, has_sm80);\n   codegen::transform::prefetch prefetch_s(target);\n   codegen::transform::membar barriers(&liveness, &layouts, &allocation, &prefetch_s, target);\n   codegen::generator isel(&axes, &layouts, &align, &allocation, &swizzle, target, num_warps);\n   // run passes\n   inliner.run(ir);\n   dce.run(ir);\n+  // ir.print(std::cout);\n   peephole.run(ir);\n   dce.run(ir);\n   pipeline.run(ir);\n@@ -84,10 +85,15 @@ std::unique_ptr<llvm::Module> add_passes_to_emit_bin(ir::module &ir, llvm::LLVMC\n   axes.run(ir);\n   layouts.run(ir);\n   swizzle.run(ir);\n+  // std::cout << \"---\" << std::endl;\n+  // ir.print(std::cout);\n+  // std::cout << \"---\" << std::endl;\n+  // ir.print(std::cout);\n   liveness.run(ir);\n   allocation.run(ir);\n   prefetch_s.run(ir);\n   barriers.run(ir);\n+  // exit(1);\n   // ir.print(std::cout);\n   isel.visit(ir, *llvm);\n   shared_static = allocation.allocated_size();"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 305, "deletions": 110, "changes": 415, "file_content_changes": "@@ -744,11 +744,13 @@ void generator::visit_load_inst(ir::load_inst* x){\n   BasicBlock *current = builder_->GetInsertBlock();\n   Module *module = current->getModule();\n   Value *tid = tgt_->get_local_id(module, *builder_, 0);\n+  Value *lane = urem(tid, i32(32));\n   ir::value *op = x->get_pointer_operand();\n   ir::masked_load_inst *mx = dynamic_cast<ir::masked_load_inst*>(x);\n   Type* ty  = cvt(op->get_type()->get_scalar_ty()->get_pointer_element_ty());\n   // compute vector width\n   size_t vec = 1;\n+  bool is_mma_first_row = false;\n   if(op->get_type()->is_block_ty()){\n     auto   ord = ords_.at(op);\n     size_t aln = alignment_->get(op, ord[0]);\n@@ -757,11 +759,15 @@ void generator::visit_load_inst(ir::load_inst* x){\n       max_eq = std::max<size_t>(max_eq, 1);\n       aln = std::min(aln, max_eq);\n     }\n-    auto layout = layouts_->get(x)->to_scanline();\n-    if(layout){\n-      size_t nts = layout->nts(ord[0]);\n-      vec = std::min(nts, aln);\n-    }\n+    analysis::distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(x));\n+    assert(layout);\n+\n+    vec = std::min<size_t>(layout->contig_per_thread(ord[0]), aln);\n+    // TODO: generalize\n+    is_mma_first_row = (ord.size() >= 1) && layout->to_mma() && \n+                       (a_axes_->get(x, ord[0]) == layouts_->get(x)->get_axis(1));\n+    if(is_mma_first_row)\n+      vec = std::min<size_t>(2, aln);\n   }\n   // code generation\n   auto idxs = idxs_.at(x);\n@@ -795,8 +801,8 @@ void generator::visit_load_inst(ir::load_inst* x){\n     int tot_width = nbits*vec;\n     int width = std::min(tot_width, max_word_width);\n     int n_words = std::max(1, tot_width / width);\n-    bool has_evict_policy = (x->get_eviction_policy() != ir::load_inst::NORMAL) && tgt_->as_nvidia()->sm() >= 80;\n-    has_evict_policy = false; // currently disable until supported in `store`\n+    bool has_l2_evict_policy = (x->get_eviction_policy() != ir::load_inst::NORMAL) && tgt_->as_nvidia()->sm() >= 80;\n+    // has_evict_policy = false; // currently disable until supported in `store`\n     // -----\n     // create inline asm string\n     // -----\n@@ -810,7 +816,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     if (x->get_cache_modifier() == ir::load_inst::CG) asm_oss << \".cg\";\n     if (x->get_eviction_policy() == ir::load_inst::EVICT_FIRST) asm_oss << \".L1::evict_first\";\n     if (x->get_eviction_policy() == ir::load_inst::EVICT_LAST) asm_oss << \".L1::evict_last\";\n-    if (has_evict_policy) asm_oss << \".L2::cache_hint\";\n+    if (has_l2_evict_policy) asm_oss << \".L2::cache_hint\";\n     if(n_words > 1)\n       asm_oss << \".v\" << n_words; // vector width\n     asm_oss << \".b\" << width; // word size\n@@ -822,7 +828,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     asm_oss << \"}\";\n     asm_oss << \", [ $\" << n_words + 1; // load\n     asm_oss << \" + \" << in_off << \"]\"; // constant offset\n-    if (has_evict_policy) asm_oss << \", $\" << n_words + 2;\n+    if (has_l2_evict_policy) asm_oss << \", $\" << n_words + 2;\n     asm_oss << \";\";\n     bool has_other = other && (other != UndefValue::get(other->getType()));\n     std::vector<Value *> others;\n@@ -844,7 +850,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n       if(ConstantInt* cst = dyn_cast<ConstantInt>(v))\n         asm_oss << \"0x\" << std::hex << cst->getSExtValue();\n       else{\n-        asm_oss << \"$\" << n_words + has_evict_policy +  2 + ii;\n+        asm_oss << \"$\" << n_words + has_l2_evict_policy +  2 + ii;\n         others.push_back(v);\n       }\n       asm_oss.flags(flags);\n@@ -859,7 +865,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     std::vector<Type*> arg_tys = {pred->getType(), ptr->getType()};\n     for(Value *v: others)\n         arg_tys.push_back(v->getType());\n-    if (has_evict_policy) \n+    if (has_l2_evict_policy) \n       arg_tys.push_back(i64_ty);\n     FunctionType *asm_ty = FunctionType::get(ret_ty, arg_tys, false);\n     // ---\n@@ -875,7 +881,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n       asm_cstrt += \",\";\n       asm_cstrt += (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n     }\n-    if (has_evict_policy) \n+    if (has_l2_evict_policy) \n       asm_cstrt += \",l\";\n     // ---\n     // finally call inline ASM\n@@ -884,7 +890,7 @@ void generator::visit_load_inst(ir::load_inst* x){\n     std::vector<Value*> args = {pred, ptr};\n     for(Value *v: others)\n         args.push_back(v);\n-    if (has_evict_policy)\n+    if (has_l2_evict_policy)\n       args.push_back(policies_.at(x->get_eviction_policy()));\n   \n     \n@@ -935,6 +941,9 @@ void generator::visit_store_inst(ir::store_inst * x){\n   // operands\n   ir::value *ptr_op = x->get_pointer_operand();\n   ir::value *val_op = x->get_value_operand();\n+  ir::value *msk_op = nullptr;\n+  if(auto* msk_st = dynamic_cast<ir::masked_store_inst*>(x))\n+    msk_op = msk_st->get_mask_operand();\n   // vector size\n   size_t vec = 1;\n   if(val_op->get_type()->is_block_ty()){\n@@ -946,36 +955,107 @@ void generator::visit_store_inst(ir::store_inst * x){\n       max_eq = std::max<size_t>(max_eq, 1);\n       aln = std::min(aln, max_eq);\n     }\n-    vec  = std::min(nts, aln);\n-  }\n+    analysis::distributed_layout* layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(ptr_op));\n+    assert(layout);\n+    // vec  = std::min(nts, aln);\n+    vec = std::min<size_t>(layout->contig_per_thread(ord[0]), aln);\n+    // TODO: generalize\n+    bool is_mma_first_row = (ord.size() >= 1) && layout->to_mma() && \n+                       (a_axes_->get(ptr_op, ord[0]) == layouts_->get(ptr_op)->get_axis(1));\n+    if(is_mma_first_row)\n+      vec = std::min<size_t>(2, aln);\n+  }\n+  bool has_l2_evict_policy = (x->get_eviction_policy() != ir::load_inst::NORMAL) && tgt_->as_nvidia()->sm() >= 80;\n   auto idxs    = idxs_.at(val_op);\n   Type *ty = cvt(val_op->get_type()->get_scalar_ty());\n   if (ty->isBFloatTy()) // llvm11-nvptx cannot select bf16 store\n     ty = f16_ty;\n+  if(ty->isIntegerTy(1))\n+    ty = builder_->getInt8Ty();\n   for(size_t i = 0; i < idxs.size(); i += vec){\n-    auto idx = idxs[i];\n-    // pointer\n+    indices_t idx = idxs[i];\n+    // pointers\n     Value *ptr = vals_[ptr_op][idx];\n-    // vectorize\n-    Type *v_ty = vec_ty(ty, vec);\n-    ptr = bit_cast(ptr, v_ty->getPointerTo(1));\n-    // value\n-    Value* val = UndefValue::get(v_ty);\n-    for(size_t ii = 0; ii < vec; ii++)\n-      val = insert_elt(val, bit_cast(vals_.at(val_op)[idxs[i + ii]], ty), ii);\n-    if(mx){\n-      Value *msk = vals_[mx->get_mask_operand()][idx];\n-      Instruction *no_op = intrinsic(Intrinsic::donothing, {}, {});\n-      builder_->SetInsertPoint(no_op->getParent());\n-      Instruction* dummy = builder_->CreateRet(nullptr);\n-      Instruction *term = llvm::SplitBlockAndInsertIfThen(msk, no_op, false);\n-      dummy->removeFromParent();\n-      builder_->SetInsertPoint(term);\n-      store(val, ptr);\n-      builder_->SetInsertPoint(no_op);\n+    size_t dtsize = std::max<int>(1, val_op->get_type()->get_scalar_ty()->get_primitive_size_in_bits() / 8);\n+    GetElementPtrInst *in_gep = dyn_cast<GetElementPtrInst>(ptr);\n+    size_t in_off;\n+    if(in_gep){\n+        ConstantInt* cst = dyn_cast<ConstantInt>(in_gep->idx_begin());\n+        in_off = cst ? cst->getValue().getSExtValue()*dtsize : 0;\n+        ptr = cst ? in_gep->getPointerOperand() : in_gep;\n     }\n-    else\n-      store(val, ptr);\n+    else{\n+        in_off = 0;\n+    }\n+    // mask\n+    Value *pred = msk_op ? vals_[msk_op][idx] : builder_->getTrue();\n+    size_t nbits = dtsize*8;\n+    // pack sub-words (< 32/64bits) into words\n+    // each load has width min(nbits*vec, 32/64)\n+    // and there are (nbits * vec)/width of them\n+    int max_word_width = std::max<int>(32, nbits);\n+    int tot_width = nbits*vec;\n+    int width = std::min(tot_width, max_word_width);\n+    int n_words = std::max(1, tot_width / width);\n+    // -----\n+    // create inline asm string\n+    // -----\n+    std::ostringstream asm_oss;\n+    asm_oss << \"@$0\"; // predicate\n+    asm_oss << \" st.global\";\n+    if (has_l2_evict_policy) asm_oss << \".L2::cache_hint\";\n+    if(n_words > 1)\n+      asm_oss << \".v\" << n_words; // vector width\n+    asm_oss << \".b\" << width; // word size\n+    asm_oss << \" [ $1 + \" << in_off << \"]\";\n+    asm_oss << \" , {\";\n+    for(int i = 0; i < n_words; i++){ // return values\n+      if(i > 0) asm_oss << \",\";\n+      asm_oss << \"$\" << 2 + i;\n+    }\n+    asm_oss << \"}\";\n+    if (has_l2_evict_policy) asm_oss << \", $\" << n_words + 2;\n+    asm_oss << \";\";\n+    // ----\n+    // create inline ASM signature\n+    // ---\n+    Type* val_arg_ty = IntegerType::get(*ctx_, width);\n+    std::vector<Type*> arg_tys = {pred->getType(), ptr->getType()};\n+    for(int ii = 0; ii < n_words; ii++)\n+      arg_tys.push_back(val_arg_ty);\n+    if (has_l2_evict_policy) \n+      arg_tys.push_back(i64_ty);\n+    FunctionType *asm_ty = FunctionType::get(builder_->getVoidTy(), arg_tys, false);\n+    // ---\n+    // create inline ASM constraints\n+    // ---\n+    std::string asm_cstrt = \"b,l\";\n+    for(int ii = 0; ii < n_words; ii++){\n+      asm_cstrt += \",\";\n+      asm_cstrt += (width == 64) ? \"l\" : ((width == 32) ? \"r\" : \"c\");\n+    }\n+    if (has_l2_evict_policy) \n+      asm_cstrt += \",l\";\n+    // ---\n+    // finally call inline ASM\n+    // ---\n+    InlineAsm *_asm = InlineAsm::get(asm_ty, asm_oss.str(), asm_cstrt, true);\n+    std::vector<Value*> args = {pred, ptr};\n+    for(unsigned int ii = 0; ii < n_words; ii++){\n+      size_t n_subw = width / nbits;\n+      Value* curr = UndefValue::get(vec_ty(ty, n_subw));\n+      for(unsigned int jj = 0; jj < n_subw; jj++){\n+        Value* new_elt = vals_[val_op][idxs[i + ii*n_subw + jj]];\n+        if(new_elt->getType()->isIntegerTy(1))\n+          new_elt = builder_->CreateSExt(new_elt, builder_->getInt8Ty());\n+        new_elt = bit_cast(new_elt, ty);\n+        curr = builder_->CreateInsertElement(curr, new_elt, jj);\n+      }\n+      args.push_back(bit_cast(curr, val_arg_ty));\n+    }\n+    if (has_l2_evict_policy)\n+      args.push_back(policies_.at(x->get_eviction_policy()));\n+    call(_asm, args);\n   }\n }\n void generator::visit_unmasked_store_inst(ir::unmasked_store_inst* x) {\n@@ -1098,6 +1178,7 @@ void generator::visit_exp_inst(ir::exp_inst* x){\n   InlineAsm *ex2 = InlineAsm::get(fn_ty, \"ex2.approx.f32 $0, $0;\", \"=f,0\", false);\n   for(auto idx: idxs_.at(x)){\n     Value *ex2arg = fmul(vals_[x->get_operand(0)][idx], log2e);\n+    // Value *ex2arg = vals_[x->get_operand(0)][idx];\n     vals_[x][idx] = call(ex2, std::vector<llvm::Value*>{ex2arg});\n   }\n }\n@@ -1291,6 +1372,18 @@ void generator::visit_mma884(ir::dot_inst* C, ir::value *A, ir::value *B, ir::va\n   // order\n   auto ord_a = layouts_->get(A)->get_order();\n   auto ord_b = layouts_->get(B)->get_order();\n+  bool is_a_trans = C->is_trans_a();\n+  // is_a_trans = false;\n+  if(C->is_trans_a()){\n+    std::swap(ord_a[0], ord_a[1]);\n+    std::swap(shape_a[0], shape_a[1]);\n+    std::swap(offset_a_m_, offset_a_k_);\n+  }\n+  // std::cout << \"visiting\" << std::endl;\n+  // if(C->is_trans_b()){\n+  //   std::swap(ord_b[0], ord_b[1]);\n+    // std::swap(shape_b[0], shape_b[1]);\n+  // }\n   // layouts\n   analysis::mma_layout*    layout_c = layouts_->get(C)->to_mma();\n   analysis::shared_layout* layout_a = layouts_->get(A)->to_shared();\n@@ -1322,6 +1415,12 @@ void generator::visit_mma884(ir::dot_inst* C, ir::value *A, ir::value *B, ir::va\n   int step_b0   = is_b_row ? stride_rep_n : stride_rep_k;\n   int num_ptr_b = std::max(2 * per_phase_b * max_phase_b / step_b0, 1);\n \n+\n+  // max_phase_a = 4;\n+  // vec_a = 8;\n+  // std::cout << per_phase_a << \" \" << max_phase_a << \" \" << step_a0 << \" \" << num_ptr_a << \" \" << stride_am << \" \" << stride_ak << \" \" << stride_a0 << \" \" << stride_a1 << std::endl;\n+  // std::cout << vec_a << \" \" << vec_b << std::endl;\n+\n   /* --------------------------------- */\n   /* --- pre-compute pointer lanes --- */\n   /* --------------------------------- */\n@@ -1916,12 +2015,17 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n   auto shape_a = A->get_type()->get_block_shapes();\n   auto shape_b = B->get_type()->get_block_shapes();\n   auto ord_a = layouts_->get(A)->get_order();\n+  if(C->is_trans_a()){\n+    std::swap(ord_a[0], ord_a[1]);\n+    std::swap(shape_a[0], shape_a[1]);\n+  }\n   auto ord_b = layouts_->get(B)->get_order();\n+  if(C->is_trans_b()){\n+    std::swap(ord_b[0], ord_b[1]);\n+    std::swap(shape_b[0], shape_b[1]);\n+  }\n+  NK = shape_a[1];\n   analysis::mma_layout* layout = layouts_->get(C)->to_mma();\n-  analysis::shared_layout* layout_a = (analysis::shared_layout*)layouts_->get(C->get_operand(0));\n-  analysis::shared_layout* layout_b = (analysis::shared_layout*)layouts_->get(C->get_operand(1));\n-  bool is_a_row = ord_a[0] == 1;\n-  bool is_b_row = ord_b[0] == 1;\n \n   std::vector<int> mma_instr_shape = layout->get_mma_instr_shape();\n   const int mma_instr_m = mma_instr_shape[0];\n@@ -1933,10 +2037,6 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n   const int mat_shape_n = mat_shape[1];\n   const int mat_shape_k = mat_shape[2];\n \n-  const int per_phase_a = swizzle_->get_per_phase(layout_a);\n-  const int max_phase_a = swizzle_->get_max_phase(layout_a);\n-  const int per_phase_b = swizzle_->get_per_phase(layout_b);\n-  const int max_phase_b = swizzle_->get_max_phase(layout_b);\n \n   const int num_rep_m = shapes[0] / layout->shape_per_cta(0);\n   const int num_rep_n = shapes[1] / layout->shape_per_cta(1);\n@@ -2001,7 +2101,12 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n \n   BasicBlock* CurrBB = builder_->GetInsertBlock();\n   BasicBlock* FirstBB = &CurrBB->getParent()->getEntryBlock();\n-  if(FirstBB != CurrBB)\n+\n+  // if true, this will move pointer declarations to the entry basic block\n+  // not prefetched cases tend to be more limited in resource usage\n+  // so we don't pre-compute ptrs to save registers\n+  bool licm_ptrs = C->is_prefetched() && (FirstBB != CurrBB);\n+  if(licm_ptrs)\n     builder_->SetInsertPoint(FirstBB->getTerminator());\n \n   Value* thread = tgt_->get_local_id(mod_, *builder_, 0);\n@@ -2015,47 +2120,137 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n   size_t dtsize_a = A->get_type()->get_scalar_ty()->get_primitive_size_in_bits() / 8;\n   size_t dtsize_b = B->get_type()->get_scalar_ty()->get_primitive_size_in_bits() / 8;\n \n+  ir::phi_node* phiA = dynamic_cast<ir::phi_node*>(A);\n+  ir::phi_node* phiB = dynamic_cast<ir::phi_node*>(B);\n+  auto register_lds2 =\n+    [&](std::map<std::pair<unsigned, unsigned>, Value*>& vals, int mn, int k, int inc, Value* val, bool is_prefetch) {\n+      if (k < 2 && is_prefetch) {\n+        ir::basic_block* inc_block = phiA->get_incoming_block(inc);\n+        lazy_phi_incs_.push_back(std::make_tuple((PHINode*)vals[{mn, k}], val, inc_block));\n+      } else\n+        vals[{mn, k}] = val;\n+  };\n+\n   // | -> k (row-major), since we have ldmatrix.trans, we only need to change stride\n   // v (s0_0(0), s1_0(2), | *num_rep_k\n   // m  s0_1(1), s1_1(3)) |  (stride in num of matrices(mat_stride_ak): 2)\n   // -----------\n   //   *num_rep_m (stride in num of matrices(mat_stride_am): 2*layout->wpt(0))\n-  mma16816_smem_loader a_loader(layout->wpt(0), ord_a, /*k_order*/1, shape_a, \n-                                {mma_instr_m, mma_instr_k}, {mat_shape_m, mat_shape_k}, \n-                                per_phase_a, max_phase_a, dtsize_a, builder_, add, mul, gep);\n-  std::vector<Value*> off_a = a_loader.compute_offs(warp_m, lane);\n-  int num_ptr_a = a_loader.get_num_ptr();\n+  std::function<void(int,int,int,bool)> load_a;\n+  analysis::shared_layout* layout_a = layouts_->get(C->get_operand(0))->to_shared();\n+  bool is_a_shared = layout_a != nullptr;\n+  if(is_a_shared) {\n+    const int per_phase_a = swizzle_->get_per_phase(layout_a);\n+    const int max_phase_a = swizzle_->get_max_phase(layout_a);\n+    mma16816_smem_loader a_loader(layout->wpt(0), ord_a, /*k_order*/1, shape_a, \n+                                  {mma_instr_m, mma_instr_k}, {mat_shape_m, mat_shape_k}, \n+                                  per_phase_a, max_phase_a, dtsize_a, builder_, add, mul, gep);\n+    std::vector<Value*> off_a = a_loader.compute_offs(warp_m, lane);\n+    int num_ptr_a = a_loader.get_num_ptr();\n+    // pointers\n+    std::vector<Value*> ptrs_a(num_ptr_a);\n+    if(licm_ptrs)\n+      builder_->SetInsertPoint(CurrBB);\n+    for(int i = 0; i < num_ptr_a; i++)\n+      ptrs_a[i] = bit_cast(gep(shmems_[A], {off_a[i]}), smem_ptr_ty);\n+    if(licm_ptrs)\n+      builder_->SetInsertPoint(FirstBB->getTerminator());\n+    // loading function\n+    load_a = [&,a_loader,ptrs_a,off_a](int m, int k, int inc, bool is_prefetch) mutable {\n+      auto [ha0, ha1, ha2, ha3] = a_loader.load_x4(m, k, inc, is_prefetch, phiA, shared_pre_ptr_[layout_a],\n+                                                  shared_next_ptr_[layout_a], off_a, ptrs_a, \n+                                                  ldmatrix_ty, smem_ptr_ty, prefetch_latch_to_bb_);\n+      register_lds2(ha, m,   k,   inc, ha0, is_prefetch);\n+      register_lds2(ha, m+1, k,   inc, ha1, is_prefetch);\n+      register_lds2(ha, m,   k+1, inc, ha2, is_prefetch);\n+      register_lds2(ha, m+1, k+1, inc, ha3, is_prefetch);\n+    };\n+  }\n+  else {\n+    load_a = [&](int m, int k, int inc, bool is_prefetch) {\n+      distributed_axis ax_n = axes_.at(a_axes_->get(A, 1));\n+      int ldm = ax_n.values.size();\n+      if(ldm != num_rep_k*4)\n+        throw std::runtime_error(\"Internal compiler error when trying to fuse matmuls!\");\n+      // std::cout << m << \" \" << k << std::endl;\n+      // std::cout << idxs_[A].size() << std::endl;\n+      // std::cout << (m+1)*ldm + k*2 + 3 << std::endl;\n+      // int ldm = num_rep_k*4;\n+      Value* ha0 = UndefValue::get(fp16x2_ty);\n+      Value* ha1 = UndefValue::get(fp16x2_ty);\n+      Value* ha2 = UndefValue::get(fp16x2_ty);\n+      Value* ha3 = UndefValue::get(fp16x2_ty);\n+      ha0 = builder_->CreateInsertElement(ha0, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 0]], i32(0));\n+      ha0 = builder_->CreateInsertElement(ha0, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 1]], i32(1));\n+      ha1 = builder_->CreateInsertElement(ha1, vals_[A][idxs_[A][(m+1)*ldm + k*2 + 0]], i32(0));\n+      ha1 = builder_->CreateInsertElement(ha1, vals_[A][idxs_[A][(m+1)*ldm + k*2 + 1]], i32(1));\n+      ha2 = builder_->CreateInsertElement(ha2, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 2]], i32(0));\n+      ha2 = builder_->CreateInsertElement(ha2, vals_[A][idxs_[A][(m+0)*ldm + k*2 + 3]], i32(1));\n+      ha3 = builder_->CreateInsertElement(ha3, vals_[A][idxs_[A][(m+1)*ldm + k*2 + 2]], i32(0));\n+      ha3 = builder_->CreateInsertElement(ha3, vals_[A][idxs_[A][(m+1)*ldm + k*2 + 3]], i32(1));\n+      ha[{m, k}] = ha0;\n+      ha[{m+1, k}] = ha1;\n+      ha[{m, k+1}] = ha2;\n+      ha[{m+1, k+1}] = ha3;\n+    };\n+  }\n+\n \n   // | -> n (col-major)\n   // v (s0_0(0), | (stride: wpt(1)) | s1_0(2)  | *num_rep_n\n   // k  s0_1(1), |                  | s1_1(3)) | (stride in num of matrices(mat_stride_bn): wpt(1))\n   // -----------\n   //   *num_rep_k (stride in num of matrices(mat_stride_bk): 2)\n-  mma16816_smem_loader b_loader(layout->wpt(1), ord_b, /*k_order*/0, shape_b,\n-                                {mma_instr_k, mma_instr_n}, {mat_shape_k, mat_shape_n},\n+  analysis::shared_layout* layout_b = layouts_->get(C->get_operand(1))->to_shared();\n+  const int per_phase_b = swizzle_->get_per_phase(layout_b);\n+  const int max_phase_b = swizzle_->get_max_phase(layout_b);\n+  std::vector<int> mma_instr_b{mma_instr_k, mma_instr_n};\n+  std::vector<int> mat_shape_b{mat_shape_k, mat_shape_n};\n+  int k_order_b = 0;\n+  // if(C->is_trans_b()){\n+    // std::swap(mma_instr_b[0], mma_instr_b[1]);\n+    // std::swap(mat_shape_b[0], mat_shape_b[1]);\n+    // k_order_b = k_order_b ^ 1;\n+    // std::swap(ord_b[0], ord_b[1]);\n+    // std::swap(shape_b[0], shape_b[1]);\n+  // }\n+\n+  mma16816_smem_loader b_loader(layout->wpt(1), ord_b, k_order_b, shape_b,\n+                                mma_instr_b, mat_shape_b,\n                                 per_phase_b, max_phase_b, dtsize_b, builder_, add, mul, gep);\n   std::vector<Value*> off_b = b_loader.compute_offs(warp_n, lane);\n-  int num_ptr_b = b_loader.get_num_ptr();\n \n-  builder_->SetInsertPoint(CurrBB);\n-  // A pointer\n-  std::vector<Value*> ptrs_a(num_ptr_a);\n-  for(int i = 0; i < num_ptr_a; i++)\n-    ptrs_a[i] = bit_cast(gep(shmems_[A], {off_a[i]}), smem_ptr_ty);\n-  // B pointer\n+  if(licm_ptrs)\n+    builder_->SetInsertPoint(CurrBB);\n+  // pointers\n+  int num_ptr_b = b_loader.get_num_ptr();\n   std::vector<Value*> ptrs_b(num_ptr_b);\n   for(int i = 0; i < num_ptr_b; i++)\n     ptrs_b[i] = bit_cast(gep(shmems_[B], {off_b[i]}), smem_ptr_ty);\n \n-  InlineAsm *mma_fn = InlineAsm::get(mma_ty, layout->get_ptx_instr() +\n+    \n+  // loading function\n+  std::function<void(int,int,int,bool)> load_b;\n+  load_b = [&](int n, int k, int inc, bool is_prefetch) {\n+      auto [hb0, hb1, hb2, hb3] = b_loader.load_x4(k, n, inc, is_prefetch, phiB, shared_pre_ptr_[layout_b],\n+                                                   shared_next_ptr_[layout_b], off_b, ptrs_b, \n+                                                   ldmatrix_ty, smem_ptr_ty, prefetch_latch_to_bb_);\n+      register_lds2(hb, n,   k,   inc, hb0, is_prefetch);\n+      register_lds2(hb, n+1, k,   inc, hb2, is_prefetch);\n+      register_lds2(hb, n,   k+1, inc, hb1, is_prefetch);\n+      register_lds2(hb, n+1, k+1, inc, hb3, is_prefetch);\n+  };\n+\n+\n+\n+  // create mma & unpack result, m, n, k are offsets in mat\n+  auto call_mma = [&](unsigned m, unsigned n, unsigned k) {\n+      InlineAsm *mma_fn = InlineAsm::get(mma_ty, layout->get_ptx_instr() +\n                                              \" {$0, $1, $2, $3},\"\n                                              \" {$4, $5, $6, $7},\"\n                                              \" {$8, $9},\"\n                                              \" {$10, $11, $12, $13};\",\n                                              \"=r,=r,=r,=r,r,r,r,r,r,r,0,1,2,3\", true);\n-\n-  // create mma & unpack result, m, n, k are offsets in mat\n-  auto call_mma = [&](unsigned m, unsigned n, unsigned k) {\n       unsigned cols_per_thread = num_rep_n * 2;\n       std::vector<size_t> idx = {\n         (m + 0)*cols_per_thread + (n*2 + 0),\n@@ -2072,39 +2267,6 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n       fc[idx[2]] = extract_val(nc, std::vector<unsigned>{2});\n       fc[idx[3]] = extract_val(nc, std::vector<unsigned>{3});\n   };\n-\n-  ir::phi_node* phiA = dynamic_cast<ir::phi_node*>(A);\n-  ir::phi_node* phiB = dynamic_cast<ir::phi_node*>(B);\n-\n-  auto register_lds2 =\n-    [&](std::map<std::pair<unsigned, unsigned>, Value*>& vals, int mn, int k, int inc, Value* val, bool is_prefetch) {\n-      if (k < 2 && is_prefetch) {\n-        ir::basic_block* inc_block = phiA->get_incoming_block(inc);\n-        lazy_phi_incs_.push_back(std::make_tuple((PHINode*)vals[{mn, k}], val, inc_block));\n-      } else\n-        vals[{mn, k}] = val;\n-  };\n-\n-  auto load_a = [&](int m, int k, int inc, bool is_prefetch) {\n-      auto [ha0, ha1, ha2, ha3] = a_loader.load_x4(m, k, inc, is_prefetch, phiA, shared_pre_ptr_[layout_a],\n-                                                   shared_next_ptr_[layout_a], off_a, ptrs_a, \n-                                                   ldmatrix_ty, smem_ptr_ty, prefetch_latch_to_bb_);\n-      register_lds2(ha, m,   k,   inc, ha0, is_prefetch);\n-      register_lds2(ha, m+1, k,   inc, ha1, is_prefetch);\n-      register_lds2(ha, m,   k+1, inc, ha2, is_prefetch);\n-      register_lds2(ha, m+1, k+1, inc, ha3, is_prefetch);\n-  };\n-\n-  auto load_b = [&](int n, int k, int inc, bool is_prefetch) {\n-      auto [hb0, hb1, hb2, hb3] = b_loader.load_x4(k, n, inc, is_prefetch, phiB, shared_pre_ptr_[layout_b],\n-                                                   shared_next_ptr_[layout_b], off_b, ptrs_b, \n-                                                   ldmatrix_ty, smem_ptr_ty, prefetch_latch_to_bb_);\n-      register_lds2(hb, n,   k,   inc, hb0, is_prefetch);\n-      register_lds2(hb, n+1, k,   inc, hb2, is_prefetch);\n-      register_lds2(hb, n,   k+1, inc, hb1, is_prefetch);\n-      register_lds2(hb, n+1, k+1, inc, hb3, is_prefetch);\n-  };\n-\n   if (C->is_prefetched()) {\n       // create phis\n       builder_->SetInsertPoint(CurrBB->getFirstNonPHI());\n@@ -2163,6 +2325,7 @@ void generator::visit_mma16816(ir::dot_inst* C, ir::value *A, ir::value *B, ir::\n       i = 0;\n     vals_[C][idx] = fcs.at(key)[i++];\n   };\n+\n }\n \n /**\n@@ -2384,7 +2547,7 @@ void generator::visit_reducend_inst_fast(ir::reduce_inst* x, acc_fn_t do_acc, Va\n   } else if (layout->to_mma()) {\n     shuffle_width = 4;\n     warps_per_inner = layout->to_mma()->wpt(1);\n-    col_per_thread = 16;\n+    col_per_thread = axes_.at(a_axes_->get(arg, 1)).values.size();\n     warp_j = axes_.at(a_axes_->get(arg, 1)).thread_id;\n   } \n   assert(warp_j != nullptr);\n@@ -2403,7 +2566,8 @@ void generator::visit_reducend_inst_fast(ir::reduce_inst* x, acc_fn_t do_acc, Va\n   Value* is_warp0 = icmp_eq(warp, i32(0));\n   Value* is_thread0 = icmp_eq(thread, i32(0));\n   Value* lane_j = urem(lane, i32(shuffle_width));\n-  add_barrier();\n+  if(warps_per_inner > 1)\n+    add_barrier();\n   // compute partial sum for each warp, and store to shared memory\n   for(size_t i = 0; i < n_elts/col_per_thread; i++){\n     std::pair<Value*, Value*> acc;\n@@ -2425,13 +2589,21 @@ void generator::visit_reducend_inst_fast(ir::reduce_inst* x, acc_fn_t do_acc, Va\n     // store partial result to shared memory\n     auto x_idxs = idxs_[x][i];\n     Value* x_idx = x_idxs.empty() ? builder_->getInt32(0) : x_idxs[0];\n-    Value* st_off = add(mul(x_idx, i32(warps_per_inner)), warp_j);\n-    call(st_shared, {icmp_eq(lane_j, i32(0)), gep(base, st_off), acc.first});\n-    if (with_index) {\n-      call(st_shared_index,\n-           {icmp_eq(lane_j, i32(0)), gep(index_base, st_off), acc.second});\n+    // single warp on the reduce dimension -- no need to use shmem\n+    if(warps_per_inner==1){\n+      vals_[x][idxs_[x][i]] = with_index ? acc.second : acc.first;\n+    }\n+    else{\n+      Value* st_off = add(mul(x_idx, i32(warps_per_inner)), warp_j);\n+      call(st_shared, {icmp_eq(lane_j, i32(0)), gep(base, st_off), acc.first});\n+      if (with_index) {\n+        call(st_shared_index,\n+            {icmp_eq(lane_j, i32(0)), gep(index_base, st_off), acc.second});\n+      }\n     }\n   }\n+  if(warps_per_inner==1)\n+    return;\n   add_barrier();\n   // at this point, partial accumulator synchronized in shared memory\n   // Just need to reduce `warp_per_inner` numbers in shared memory\n@@ -2559,6 +2731,7 @@ void generator::visit_reduce_inst(ir::reduce_inst* x) {\n     case ir::reduce_inst::FMAX: return max_num(x, y);\n     case ir::reduce_inst::FMIN: return min_num(x, y);\n     case ir::reduce_inst::XOR: return xor_(x, y);\n+\n     default: throw std::runtime_error(\"unreachable\");\n     }\n   };\n@@ -2639,7 +2812,9 @@ void generator::visit_layout_convert(ir::value *out, ir::value *in){\n   analysis::distributed_layout* in_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(in));\n   analysis::distributed_layout* out_layout = dynamic_cast<analysis::distributed_layout*>(layouts_->get(out));\n   Value *base;\n-  base = gep(shmem_, i32(alloc_->offset(layouts_->get(layouts_->tmp(out)))));\n+  int off = alloc_->offset(layouts_->get(layouts_->tmp(out)));\n+   // std::cout << off << std::endl;\n+  base = gep(shmem_, i32(off));\n   base = bit_cast(base, ptr_ty(ty, 3));\n   std::vector<int> n_reps;\n   for(int i = 0; i < shape.size(); i++){\n@@ -2821,15 +2996,26 @@ void generator::visit_copy_to_shared_inst(ir::copy_to_shared_inst* cts) {\n   //\n   int mts_0 = in_layout->shape_per_cta(in_order[0]) / in_layout->contig_per_thread(in_order[0]);\n   int mts_1 = in_layout->shape_per_cta(in_order[1]) / in_layout->contig_per_thread(in_order[1]);\n+  if(in_layout->to_mma()){\n+    mts_0 = 4 * in_layout->to_mma()->wpt(in_order[0]);\n+    mts_1 = 8 * in_layout->to_mma()->wpt(in_order[1]);\n+    per_phase = 1;\n+    max_phase = 8;\n+  }\n \n   int in_ld = in_layout->get_shape()[in_order[0]] / mts_0;\n-  int n_shared_1 = std::max<int>(per_phase*max_phase / mts_1, 1);\n   int n_shared_0 = std::max<int>(in_vec    / out_vec, 1);\n+  int n_shared_1 = std::max<int>(per_phase*max_phase / mts_1, 1);\n+  if(in_layout->to_mma()){\n+    n_shared_0 = 8;\n+    n_shared_1 = 1;\n+  }\n \n   BasicBlock* CurrBB = builder_->GetInsertBlock();\n   BasicBlock* FirstBB = &CurrBB->getParent()->getEntryBlock();\n   auto shapes = cts->get_type()->get_block_shapes();\n \n+\n   // store to shared\n   Value *current = nullptr;\n   std::map<std::pair<int, int>, Value*> ptrs;\n@@ -2844,9 +3030,7 @@ void generator::visit_copy_to_shared_inst(ir::copy_to_shared_inst* cts) {\n       // input ptr info\n       int id_0 = id % (in_ld/min_vec);\n       int id_1 = id / (in_ld/min_vec);\n-      int off_0 = id_0 / n_shared_0 * n_shared_0 * mts_0;\n-      int off_1 = id_1 / n_shared_1 * n_shared_1 * mts_1;\n-      int off = (off_1*shapes[in_order[0]] + off_0);\n+      // std::cout << id_0 << \" \" << id_1 << \" \" << in_ld << \" \" << std::endl;\n       std::pair<int, int> key = {id_1  % n_shared_1, id_0 % n_shared_0};\n       if(ptrs.find(key) == ptrs.end()){\n         if(FirstBB->getTerminator())\n@@ -2865,6 +3049,13 @@ void generator::visit_copy_to_shared_inst(ir::copy_to_shared_inst* cts) {\n         builder_->SetInsertPoint(CurrBB);\n         ptrs[key] = gep(shmems_.at(cts), {off});\n       }\n+      int off_0 = id_0 / n_shared_0 * n_shared_0 * mts_0;\n+      int off_1 = id_1 / n_shared_1 * n_shared_1 * mts_1;\n+      if(in_layout->to_mma()){\n+        off_0 = id_0/n_shared_0*n_shared_0*8;\n+        off_1 = id_1/n_shared_1*n_shared_1*8;\n+      }\n+      int off = (off_1*shapes[in_order[0]] + off_0);\n       Value* ptr = gep(ptrs[key], {i32(off)});\n       ptr = bit_cast(ptr, current->getType()->getPointerTo(3));\n       // asm\n@@ -3069,7 +3260,7 @@ void generator::visit_function(ir::function* fn) {\n   if(tgt_->as_nvidia()->sm() >= 80)\n   for(ir::load_inst::EVICTION_POLICY evict: {ir::load_inst::EVICT_FIRST, ir::load_inst::EVICT_LAST}){\n     std::string policy = (evict == ir::load_inst::EVICT_FIRST) ? \"evict_first\" : \"evict_last\";\n-    std::string asm_str = \"createpolicy.fractional.L2::\" + policy + \".b64 $0;\";\n+    std::string asm_str = \"createpolicy.fractional.L2::\" + policy + \".b64 $0, 1.0;\";\n     InlineAsm* iasm = InlineAsm::get(FunctionType::get(i64_ty, {}), asm_str, \"=l\", false);\n     policies_[evict] = call(iasm);\n   }\n@@ -3299,7 +3490,6 @@ void generator::visit_basic_block(ir::basic_block * block) {\n   BasicBlock *parent = bbs_[block];\n   builder_->SetInsertPoint(parent);\n   for(ir::instruction *i: block->get_inst_list()){\n-    // i->print(std::cout);\n     visit_value(i);\n     // std::cout << \"done\" << std::endl;\n   }\n@@ -3324,7 +3514,10 @@ void generator::init_idx(ir::value *v) {\n   std::vector<distributed_axis> axes(rank);\n   std::vector<int> ord(rank);\n   // compute axes\n+  // std::cout << \"axes\" << std::endl;\n   for(size_t d = 0; d < shapes.size(); d++){\n+    // std::cout << d << \" \" << shapes[d] << std::endl;\n+    // std::cout << a_axes_->get(v, d) << std::endl;\n     if(shapes[d] > 1){\n       unsigned x = a_axes_->get(v, d);\n       axes[d] = axes_.at(x);\n@@ -3334,6 +3527,7 @@ void generator::init_idx(ir::value *v) {\n       axes[d].values = {i32(0)};\n     }\n   }\n+  // std::cout << \"axes ok\" << std::endl;\n   // compute order\n   analysis::data_layout* layout = layouts_->get(v);\n   std::iota(ord.begin(), ord.end(), 0);\n@@ -3480,6 +3674,7 @@ void generator::finalize_phi_node(ir::phi_node *x) {\n     for(indices_t idx: idxs_.at(x)){\n       PHINode *phi = (PHINode*)vals_[x][idx];\n       Value *inc = vals_[x->get_incoming_value(n)][idx];\n+      // x->print(std::cout);\n       phi->addIncoming(inc, block);\n     }\n   }"}, {"filename": "lib/codegen/transform/coalesce.cc", "status": "modified", "additions": 10, "deletions": 6, "changes": 16, "file_content_changes": "@@ -12,8 +12,8 @@ namespace triton {\n namespace codegen{\n namespace transform{\n \n-coalesce::coalesce(analysis::align* align, analysis::layouts *layouts)\n-  : align_(align), layout_(layouts) { }\n+coalesce::coalesce(analysis::align* align, analysis::layouts *layouts, bool has_sm80)\n+  : align_(align), layout_(layouts), has_sm80_(has_sm80) { }\n \n \n // simplify layout conversions using the following simple rules:\n@@ -64,15 +64,18 @@ void coalesce::run(ir::module &mod) {\n     if(op->get_type()->is_block_ty())\n     if(op->get_type()->get_tile_rank() == 2)\n     if(invalidated.find(layout_->get(op)) == invalidated.end())\n-    if(layout_->get(op)->to_mma()){\n+    if(layout_->get(op)->to_mma())\n+    if(dynamic_cast<ir::io_inst*>(i)->get_eviction_policy()==ir::io_inst::NORMAL){\n       ir::instruction* new_op = ir::cvt_layout_inst::create(op);\n       builder.set_insert_point(i);\n       builder.insert(new_op);\n       i->replace_uses_of_with(op, new_op);\n     }\n     // coalesce before copy_to_shared\n-    // It's dirty, but the backend is being rewritten from scratch. :)\n-    if(dynamic_cast<ir::copy_to_shared_inst*>(i))\n+    // only necessary for sm < 80 as Ampere+ can handle reduction\n+    // on MMA layout\n+    if(!has_sm80_)\n+    if(dynamic_cast<ir::copy_to_shared_inst*>(i) || dynamic_cast<ir::reduce_inst*>(i))\n     if(ir::value* op = i->get_operand(0))\n     if(op->get_type()->is_block_ty())\n     if(op->get_type()->get_tile_rank() == 2)\n@@ -89,7 +92,8 @@ void coalesce::run(ir::module &mod) {\n     if(auto x = dynamic_cast<ir::load_inst*>(i))\n     if(x->get_type()->is_block_ty())\n     if(x->get_type()->get_tile_rank()==2)\n-    if(layout_->get(x)->to_mma()){\n+    if(layout_->get(x)->to_mma())\n+    if(!has_sm80_ || dynamic_cast<ir::io_inst*>(i)->get_eviction_policy()==ir::io_inst::NORMAL){\n         builder.set_insert_point_after(x);\n         ir::instruction* new_x = ir::cvt_layout_inst::create(x);\n         builder.insert(new_x);"}, {"filename": "lib/codegen/transform/cts.cc", "status": "modified", "additions": 43, "deletions": 22, "changes": 65, "file_content_changes": "@@ -1,26 +1,28 @@\n+#include \"triton/codegen/analysis/layout.h\"\n #include \"triton/codegen/transform/cts.h\"\n #include \"triton/ir/module.h\"\n #include \"triton/ir/function.h\"\n #include \"triton/ir/basic_block.h\"\n #include \"triton/ir/instructions.h\"\n+#include \"triton/ir/utils.h\"\n #include <iostream>\n \n namespace triton {\n namespace codegen{\n namespace transform{\n \n \n-inline bool is_shmem_op(ir::instruction* i, int op) {\n+bool cts::is_shmem_op(ir::instruction* i, int op) {\n   if(i->get_id() == ir::INST_DOT)\n-    return op==0 || op==1;\n+    return op == 0 || op == 1;\n   if(i->get_id() == ir::INST_COPY_FROM_SHARED)\n     return op==0;\n   if(i->get_id() == ir::INST_TRANS)\n     return op==0;\n   return false;\n }\n \n-inline bool is_shmem_res(ir::value* v){\n+bool cts::is_shmem_res(ir::value* v){\n   ir::instruction* i = dynamic_cast<ir::instruction*>(v);\n   if(!i)\n     return false;\n@@ -35,7 +37,7 @@ inline bool is_shmem_res(ir::value* v){\n \n \n // run pass on module\n-void cts::add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder, bool to_shared) {\n+void cts::add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder, bool to_shared, std::map<ir::value*, ir::value*>& copies) {\n   auto *i = dynamic_cast<ir::instruction*>(x);\n   // not an instruction\n   if(!i) {\n@@ -51,7 +53,7 @@ void cts::add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder,\n   // phi node\n   if(auto* phi = dynamic_cast<ir::phi_node*>(x)) {\n     for(unsigned i = 0; i < phi->get_num_incoming(); ++i)\n-      add_copy(phi, phi->get_incoming_value(i), builder, to_shared);\n+      add_copy(phi, phi->get_incoming_value(i), builder, to_shared, copies);\n     return;\n   }\n   // already in shared memory\n@@ -65,30 +67,49 @@ void cts::add_copy(ir::instruction *parent, ir::value *x, ir::builder &builder,\n   }\n   else\n     copy = builder.create_copy_from_shared(x);\n-  parent->replace_uses_of_with(x, copy);\n+  copies.insert({x, copy});\n+  parent->replace_uses_of_with(x, copies.at(x));\n }\n \n void cts::run(ir::module &mod) {\n+  // Precompute where copies should be added\n+  std::set<ir::value*> shmem_ops;\n+  std::set<ir::value*> shmem_res;\n+  ir::for_each_instruction(mod, [&](ir::instruction* i) {\n+    if(i->get_id() == ir::INST_DOT){\n+      ir::dot_inst* dot = dynamic_cast<ir::dot_inst*>(i);\n+      ir::value* lhs = i->get_operand(0);\n+      ir::type* ty = lhs->get_type()->get_scalar_ty();\n+      analysis::mma_layout* mma_lhs = layouts_->get(lhs)->to_mma();\n+      // TODO: V100\n+      bool is_lhs_shmem = !(mma_lhs && has_sm80_ && ty->get_primitive_size_in_bits() == 16 && !dot->is_trans_a());\n+      if(is_lhs_shmem)\n+        shmem_ops.insert(lhs);\n+      shmem_ops.insert(i->get_operand(1));\n+    }\n+    if(i->get_id() == ir::INST_COPY_FROM_SHARED)\n+      shmem_ops.insert(i->get_operand(0));\n+    if(i->get_id() == ir::INST_TRANS)\n+      shmem_ops.insert(i->get_operand(0));\n+    if(i->get_id() == ir::INST_TRANS ||\n+       i->get_id() == ir::INST_COPY_TO_SHARED ||\n+       i->get_id() == ir::INST_MASKED_LOAD_ASYNC)\n+      shmem_res.insert(i);\n+  });\n+\n   // Add shared copies\n+  std::map<ir::value*, ir::value*> copies;\n   ir::builder &builder = mod.get_builder();\n-  for(ir::function* fn: mod.get_function_list()){\n-    for(ir::basic_block* block: fn->blocks())\n-    for(ir::instruction* i: block->get_inst_list()){\n-      size_t num_op = i->get_num_operands();\n+  ir::for_each_instruction(mod, [&](ir::instruction* i) {\n+    size_t num_op = i->get_num_operands();\n+    for(size_t k = 0; k < num_op; k++){\n+      ir::value* op = i->get_operand(k);\n       // copy to shared operands\n-      for(size_t k = 0; k < num_op; k++)\n-        if(is_shmem_op(i, k)){\n-          add_copy(i, i->get_operand(k), builder, true);\n-        }\n-      // copy from shared operands\n-      for(size_t k = 0; k < num_op; k++)\n-        if(!dynamic_cast<ir::phi_node*>(i) &&\n-           !is_shmem_op(i,k) &&\n-           is_shmem_res(i->get_operand(k))){\n-          add_copy(i, i->get_operand(k), builder, false);\n-        }\n+      bool is_shmem_op = shmem_ops.find(op) != shmem_ops.end();\n+      if(is_shmem_op)\n+        add_copy(i, op, builder, true, copies);\n     }\n-  }\n+  });\n }\n \n "}, {"filename": "lib/codegen/transform/peephole.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -87,7 +87,7 @@ bool peephole::rewrite_dot(ir::instruction *value, ir::builder& builder){\n     ir::value *a = dot->get_operand(0);\n     ir::value *b = dot->get_operand(1);\n     builder.set_insert_point(add);\n-    ir::value * new_dot = builder.insert(ir::dot_inst::create_nn(a, b, other, dot->allow_tf32(), dot->get_name()));\n+    ir::value * new_dot = builder.insert(ir::dot_inst::create(a, b, other, dot->is_trans_a(), dot->is_trans_b(), dot->allow_tf32(), dot->get_name()));\n     add->replace_all_uses_with(new_dot);\n     return true;\n   }"}, {"filename": "lib/ir/basic_block.cc", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "file_content_changes": "@@ -26,7 +26,10 @@ void basic_block::replace_phi_uses_with(basic_block* before, basic_block* after)\n     auto* curr_phi = dynamic_cast<ir::phi_node*>(i);\n     if(!curr_phi)\n       break;\n-    curr_phi->replace_uses_of_with(before, after);\n+    // curr_phi->replace_uses_of_with(before, after);\n+    for (size_t idx = 0; idx < curr_phi->get_num_incoming(); ++idx)\n+    if (curr_phi->get_incoming_block(idx) == before)\n+      curr_phi->set_incoming_block(idx, after);\n   }\n }\n "}, {"filename": "lib/ir/builder.cc", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -299,16 +299,16 @@ value *builder::create_load(value *ptr, load_inst::CACHE_MODIFIER cache, load_in\n   return insert(unmasked_load_inst::create(ptr, cache, eviction, is_volatile));\n }\n \n-value *builder::create_store(value *ptr, value *val){\n-  return insert(unmasked_store_inst::create(ptr, val));\n+value *builder::create_store(value *ptr, value *val, store_inst::EVICTION_POLICY eviction){\n+  return insert(unmasked_store_inst::create(ptr, val, eviction));\n }\n \n value *builder::create_masked_load(value *ptr, value *mask, value *false_value, load_inst::CACHE_MODIFIER cache, load_inst::EVICTION_POLICY eviction, bool is_volatile){\n   return insert(masked_load_inst::create(ptr, mask, false_value, cache, eviction, is_volatile));\n }\n \n-value *builder::create_masked_store(value *ptr, value *val, value *mask){\n-  return insert(masked_store_inst::create(ptr, val, mask));\n+value *builder::create_masked_store(value *ptr, value *val, value *mask, store_inst::EVICTION_POLICY eviction){\n+  return insert(masked_store_inst::create(ptr, val, mask, eviction));\n }\n \n //===----------------------------------------------------------------------===//\n@@ -412,8 +412,8 @@ value *builder::create_log(value *arg){\n   return insert(log_inst::create(arg));\n }\n \n-value *builder::create_dot(value *A, value *B, value *C, bool allow_tf32) {\n-  return insert(dot_inst::create_nn(A, B, C, allow_tf32));\n+value *builder::create_dot(value *A, value *B, value *C, bool trans_a, bool trans_b, bool allow_tf32) {\n+  return insert(dot_inst::create(A, B, C, trans_a, trans_b, allow_tf32));\n }\n \n value *builder::create_trans(value *A, const std::vector<int>& perm) {"}, {"filename": "lib/ir/instructions.cc", "status": "modified", "additions": 16, "deletions": 14, "changes": 30, "file_content_changes": "@@ -69,6 +69,7 @@ void phi_node::set_incoming_block(unsigned i, basic_block *block){\n \n // Add incoming\n void phi_node::add_incoming(value *v, basic_block *block){\n+  assert(v && \"PHI node got a null value!!\");\n   resize_ops(get_num_operands() + 1);\n   blocks_.resize(get_num_operands() + 1);\n   set_incoming_value(get_num_operands() - 1, v);\n@@ -494,13 +495,13 @@ getelementptr_inst *getelementptr_inst::create(value *ptr, const std::vector<val\n //===----------------------------------------------------------------------===//\n \n // io_inst\n-io_inst::io_inst(type *ty, value_id_t id, unsigned num_ops, const std::string &name, instruction *next)\n-  : instruction(ty, id, num_ops, name, next)\n+io_inst::io_inst(type *ty, value_id_t id, unsigned num_ops, EVICTION_POLICY eviction, const std::string &name, instruction *next)\n+  : instruction(ty, id, num_ops, name, next), eviction_(eviction)\n { }\n \n // load_inst\n load_inst::load_inst(value *ptr, value_id_t id, unsigned num_ops, load_inst::CACHE_MODIFIER cache, EVICTION_POLICY eviction, bool is_volatile, const std::string &name, instruction *next)\n-  : io_inst(get_pointee_type(ptr->get_type()), id, num_ops, name, next), cache_(cache), eviction_(eviction), is_volatile_(is_volatile)\n+  : io_inst(get_pointee_type(ptr->get_type()), id, num_ops, eviction, name, next), cache_(cache), is_volatile_(is_volatile)\n { }\n \n // load\n@@ -557,34 +558,35 @@ masked_load_async_inst* masked_load_async_inst::create(value *ptr, value *mask,\n \n // store\n \n-store_inst::store_inst(value *ptr, value_id_t id, unsigned num_ops, const std::string &name, instruction *next)\n-  : io_inst(type::get_void_ty(ptr->get_type()->get_context()), id, num_ops, name, next)\n+store_inst::store_inst(value *ptr, value_id_t id, unsigned num_ops, EVICTION_POLICY eviction, const std::string &name, instruction *next)\n+  : io_inst(type::get_void_ty(ptr->get_type()->get_context()), id, num_ops, eviction, name, next)\n { }\n \n // unmasked_store\n-unmasked_store_inst::unmasked_store_inst(value *ptr, value *val,\n+unmasked_store_inst::unmasked_store_inst(value *ptr, value *val, EVICTION_POLICY eviction,\n                                          const std::string &name, instruction *next)\n-    : store_inst(ptr, INST_UNMASKED_STORE, 2, name, next)  {\n+    : store_inst(ptr, INST_UNMASKED_STORE, 2, eviction, name, next)  {\n   set_operand(0, ptr);\n   set_operand(1, val);\n }\n \n-unmasked_store_inst* unmasked_store_inst::create(value *ptr, value *val,\n+unmasked_store_inst* unmasked_store_inst::create(value *ptr, value *val, EVICTION_POLICY eviction,\n                                                  const std::string &name, instruction *next) {\n-  return new unmasked_store_inst(ptr, val, name, next);\n+  return new unmasked_store_inst(ptr, val, eviction, name, next);\n }\n \n // masked store\n-masked_store_inst::masked_store_inst(value *ptr, value *val, value *mask,\n+masked_store_inst::masked_store_inst(value *ptr, value *val, value *mask, EVICTION_POLICY eviction,\n                                      const std::string &name, instruction *next)\n-  : store_inst(ptr, INST_MASKED_STORE, 3, name, next) {\n+  : store_inst(ptr, INST_MASKED_STORE, 3, eviction, name, next) {\n   set_operand(0, ptr);\n   set_operand(1, val);\n   set_operand(2, mask);\n }\n \n-masked_store_inst* masked_store_inst::create(value *ptr, value *val, value *mask, const std::string &name, instruction *next)  {\n-  return new masked_store_inst(ptr, val, mask, name, next);\n+masked_store_inst* masked_store_inst::create(value *ptr, value *val, value *mask, EVICTION_POLICY eviction, \n+                                             const std::string &name, instruction *next)  {\n+  return new masked_store_inst(ptr, val, mask, eviction, name, next);\n }\n \n //===----------------------------------------------------------------------===//\n@@ -679,7 +681,7 @@ instruction* downcast_inst::create(value *arg, const std::string &name, instruct\n \n dot_inst::dot_inst(value *A, value *B, value *C, TransT AT, TransT BT, bool allow_tf32,\n                          const std::string &name, instruction *next)\n-    : builtin_inst(C->get_type(), INST_DOT, 3, name, next) {\n+    : builtin_inst(C->get_type(), INST_DOT, 3, name, next), AT_(AT), BT_(BT){\n   set_operand(0, A);\n   set_operand(1, B);\n   set_operand(2, C);"}, {"filename": "lib/ir/utils.cc", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -43,6 +43,15 @@ std::vector<basic_block*> cfg::reverse_post_order(function* fn) {\n   return result;\n }\n \n+void for_each_instruction_backward(module &mod, const std::function<void (instruction *)> &do_work) {\n+  for(ir::function *fn: mod.get_function_list())\n+  for(ir::basic_block *block: cfg::post_order(fn)){\n+    auto inst_list = block->get_inst_list();\n+    for(auto it = inst_list.rbegin(); it != inst_list.rend() ; it++)\n+      do_work(*it);\n+  }\n+}\n+\n void for_each_instruction(module &mod, const std::function<void (instruction *)> &do_work) {\n   for(ir::function *fn: mod.get_function_list())\n   for(ir::basic_block *block: cfg::reverse_post_order(fn))"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 46, "deletions": 11, "changes": 57, "file_content_changes": "@@ -840,10 +840,10 @@ def kernel(X, stride_xm, stride_xn,\n \n @pytest.mark.parametrize(\"epilogue, allow_tf32, dtype\",\n                          [(epilogue, allow_tf32, dtype)\n-                          for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols']\n+                          for epilogue in ['none', 'trans', 'add-matrix', 'add-rows', 'add-cols', 'softmax', 'chain-dot']\n                           for allow_tf32 in [True, False]\n-                          for dtype in ['float32', 'int8']\n-                          if not (allow_tf32 and (dtype == 'int8'))])\n+                          for dtype in ['float16']\n+                          if not (allow_tf32 and (dtype in ['float16']))])\n def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n     cc = _triton.runtime.cc(_triton.runtime.backend.CUDA, torch.cuda.current_device())\n     if cc < 80:\n@@ -852,21 +852,30 @@ def test_dot(epilogue, allow_tf32, dtype, device='cuda'):\n         elif dtype == 'float32' and allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n \n+    M, N, K = 128, 128, 64\n+    num_warps = 8\n+    trans_a, trans_b = False, False\n+\n     # triton kernel\n     @triton.jit\n     def kernel(X, stride_xm, stride_xk,\n                Y, stride_yk, stride_yn,\n+               W, stride_wn, stride_wl,\n                Z, stride_zm, stride_zn,\n                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n                ADD_MATRIX: tl.constexpr, ADD_ROWS: tl.constexpr, ADD_COLS: tl.constexpr,\n-               ALLOW_TF32: tl.constexpr):\n+               ALLOW_TF32: tl.constexpr,\n+               DO_SOFTMAX: tl.constexpr, CHAIN_DOT: tl.constexpr,\n+               TRANS_A: tl.constexpr, TRANS_B: tl.constexpr):\n         off_m = tl.arange(0, BLOCK_M)\n         off_n = tl.arange(0, BLOCK_N)\n+        off_l = tl.arange(0, BLOCK_N)\n         off_k = tl.arange(0, BLOCK_K)\n         Xs = X + off_m[:, None] * stride_xm + off_k[None, :] * stride_xk\n         Ys = Y + off_k[:, None] * stride_yk + off_n[None, :] * stride_yn\n+        Ws = W + off_n[:, None] * stride_wn + off_l[None, :] * stride_wl\n         Zs = Z + off_m[:, None] * stride_zm + off_n[None, :] * stride_zn\n-        z = tl.dot(tl.load(Xs), tl.load(Ys), allow_tf32=ALLOW_TF32)\n+        z = tl.dot(tl.load(Xs), tl.load(Ys), trans_a=TRANS_A, trans_b=TRANS_B, allow_tf32=ALLOW_TF32)\n         if ADD_MATRIX:\n             z += tl.load(Zs)\n         if ADD_ROWS:\n@@ -875,39 +884,65 @@ def kernel(X, stride_xm, stride_xk,\n         if ADD_COLS:\n             ZCs = Z + off_n * stride_zn\n             z += tl.load(ZCs)[None, :]\n+        if DO_SOFTMAX:\n+            max = tl.max(z, 1)\n+            z = z - max[:, None]\n+            num = tl.exp(z)\n+            den = tl.sum(num, 1)\n+            z = num / den[:, None]\n+        if CHAIN_DOT:\n+            # tl.store(Zs, z)\n+            # tl.debug_barrier()\n+            z = tl.dot(z.to(tl.float16), tl.load(Ws), trans_a=TRANS_A)\n         tl.store(Zs, z)\n     # input\n-    M, N, K = 64, 64, 32\n     rs = RandomState(17)\n-    x = numpy_random((M, K), dtype_str=dtype, rs=rs)\n-    y = numpy_random((K, N), dtype_str=dtype, rs=rs)\n+    x = numpy_random((K, M) if trans_a else (M, K), dtype_str=dtype, rs=rs) * .1\n+    y = numpy_random((N, K) if trans_b else (K, N), dtype_str=dtype, rs=rs) * .1\n+    w = numpy_random((N, N), dtype_str=dtype, rs=rs) * .1\n     if allow_tf32:\n         x = (x.view('uint32') & np.uint32(0xffffe000)).view('float32')\n         y = (y.view('uint32') & np.uint32(0xffffe000)).view('float32')\n+        w = (w.view('uint32') & np.uint32(0xffffe000)).view('float32')\n     x_tri = to_triton(x, device=device)\n     y_tri = to_triton(y, device=device)\n+    w_tri = to_triton(w, device=device)\n     # triton result\n-    z = numpy_random((M, N), dtype_str=dtype, rs=rs)\n+    z = 1 + numpy_random((M, N), dtype_str=dtype, rs=rs) * .1\n     z_tri = to_triton(z, device=device)\n     if epilogue == 'trans':\n         z_tri = torch.as_strided(z_tri, (M, N), z_tri.stride()[::-1])\n     pgm = kernel[(1, 1)](x_tri, x_tri.stride(0), x_tri.stride(1),\n                          y_tri, y_tri.stride(0), y_tri.stride(1),\n+                         w_tri, w_tri.stride(0), w_tri.stride(1),\n                          z_tri, z_tri.stride(0), z_tri.stride(1),\n+                         TRANS_A=trans_a, TRANS_B=trans_b,\n                          BLOCK_M=M, BLOCK_K=K, BLOCK_N=N,\n                          ADD_MATRIX=epilogue == 'add-matrix',\n                          ADD_ROWS=epilogue == 'add-rows',\n                          ADD_COLS=epilogue == 'add-cols',\n-                         ALLOW_TF32=allow_tf32)\n+                         DO_SOFTMAX=epilogue == 'softmax',\n+                         CHAIN_DOT=epilogue == 'chain-dot',\n+                         ALLOW_TF32=allow_tf32,\n+                         num_warps=num_warps)\n     # torch result\n-    z_ref = np.matmul(x, y)\n+    x_ref = x.T if trans_a else x\n+    y_ref = y.T if trans_b else y\n+    z_ref = np.matmul(x_ref, y_ref)\n     if epilogue == 'add-matrix':\n         z_ref += z\n     if epilogue == 'add-rows':\n         z_ref += z[:, 0][:, None]\n     if epilogue == 'add-cols':\n         z_ref += z[0, :][None, :]\n+    if epilogue == 'softmax':\n+        num = np.exp(z_ref - np.max(z_ref, axis=-1, keepdims=True))\n+        denom = np.sum(num, axis=-1, keepdims=True)\n+        z_ref = num / denom\n+    if epilogue == 'chain-dot':\n+        z_ref = np.matmul(z_ref.T if trans_a else z_ref, w)\n     # compare\n+    # print(z_ref[:,0], z_tri[:,0])\n     np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n     # make sure ld/st are vectorized\n     ptx = pgm.asm['ptx']"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -211,7 +211,7 @@ def _try_remove_trivial_phi(self, phi: triton.language.tensor) -> triton.languag\n             return phi\n         v = unique_handles.pop()\n         phi.handle.replace_all_uses_with(v)\n-        phi.handle.erase_from_parent()\n+        # phi.handle.erase_from_parent()\n         # TODO: remove trivial phis recursively\n         return triton.language.tensor(v, phi.type)\n "}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -732,7 +732,7 @@ def reshape(input, shape, _builder=None):\n \n \n @builtin\n-def dot(input, other, allow_tf32=True, _builder=None):\n+def dot(input, other, trans_a=False, trans_b=False, allow_tf32=True, _builder=None):\n     \"\"\"\n     Returns the matrix product of two blocks.\n \n@@ -744,7 +744,7 @@ def dot(input, other, allow_tf32=True, _builder=None):\n     :type other: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}\n     \"\"\"\n     allow_tf32 = _constexpr_to_value(allow_tf32)\n-    return semantic.dot(input, other, allow_tf32, _builder)\n+    return semantic.dot(input, other, trans_a, trans_b, allow_tf32, _builder)\n \n \n # -----------------------\n@@ -782,7 +782,7 @@ def load(pointer, mask=None, other=None, cache_modifier=\"\", eviction_policy=\"\",\n \n \n @builtin\n-def store(pointer, value, mask=None, _builder=None):\n+def store(pointer, value, eviction_policy=\"\", mask=None, _builder=None):\n     \"\"\"\n     Stores :code:`value` tensor of elements in memory, element-wise, at the memory locations specified by :code:`pointer`.\n \n@@ -799,7 +799,7 @@ def store(pointer, value, mask=None, _builder=None):\n     value = _to_tensor(value, _builder)\n     if mask is not None:\n         mask = _to_tensor(mask, _builder)\n-    return semantic.store(pointer, value, mask, _builder)\n+    return semantic.store(pointer, value, mask, eviction_policy, _builder)\n \n \n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 33, "deletions": 22, "changes": 55, "file_content_changes": "@@ -648,6 +648,18 @@ def cast(input: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n+def _parse_eviction_policy(eviction_policy):\n+    eviction = ir.EVICTION_POLICY.NORMAL  # default\n+    if eviction_policy:\n+        if eviction_policy == \"evict_last\":\n+            eviction = ir.EVICTION_POLICY.EVICT_LAST\n+        elif eviction_policy == \"evict_first\":\n+            eviction = ir.EVICTION_POLICY.EVICT_FIRST\n+        else:\n+            raise ValueError(f\"Eviction policy {eviction_policy} not supported\")\n+    return eviction\n+\n+\n def load(ptr: tl.tensor,\n          mask: Optional[tl.tensor],\n          other: Optional[tl.tensor],\n@@ -684,14 +696,7 @@ def load(ptr: tl.tensor,\n             raise ValueError(f\"Cache modifier {cache_modifier} not supported\")\n \n     # eviction policy\n-    eviction = ir.EVICTION_POLICY.NORMAL  # default\n-    if eviction_policy:\n-        if eviction_policy == \"evict_last\":\n-            eviction = ir.EVICTION_POLICY.EVICT_LAST\n-        elif eviction_policy == \"evict_first\":\n-            eviction = ir.EVICTION_POLICY.EVICT_FIRST\n-        else:\n-            raise ValueError(f\"Eviction policy {eviction_policy} not supported\")\n+    eviction = _parse_eviction_policy(eviction_policy)\n \n     if ptr.type.is_block():\n         shape = ptr.type.get_block_shapes()\n@@ -721,6 +726,7 @@ def load(ptr: tl.tensor,\n def store(ptr: tl.tensor,\n           val: tl.tensor,\n           mask: Optional[tl.tensor],\n+          eviction_policy: str,\n           builder: ir.builder) -> tl.tensor:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n@@ -735,14 +741,15 @@ def store(ptr: tl.tensor,\n         elt_ty_ptr = tl.int8\n         ptr_ty = tl.pointer_type(elt_ty_ptr, ptr_ty.address_space)\n         ptr = cast(ptr, ptr_ty, builder)\n-\n+    # eviction policy\n+    eviction = _parse_eviction_policy(eviction_policy)\n     # cast to target data-type\n     val = cast(val, elt_ty, builder)\n     if not mask:\n-        return tl.tensor(builder.create_store(ptr.handle, val.handle), tl.void)\n+        return tl.tensor(builder.create_store(ptr.handle, val.handle, eviction), tl.void)\n     if not mask.type.scalar.is_bool():\n         raise ValueError(\"Mask must have boolean scalar type\")\n-    return tl.tensor(builder.create_masked_store(ptr.handle, val.handle, mask.handle), tl.void)\n+    return tl.tensor(builder.create_masked_store(ptr.handle, val.handle, mask.handle, eviction), tl.void)\n \n #########\n # atomic\n@@ -897,27 +904,31 @@ def atomic_xchg(ptr: tl.tensor,\n # ===----------------------------------------------------------------------===//\n \n \n-def dot(lhs: tl.tensor,\n-        rhs: tl.tensor,\n+def dot(a: tl.tensor,\n+        b: tl.tensor,\n+        trans_a: bool,\n+        trans_b: bool,\n         allow_tf32: bool,\n         builder: ir.builder) -> tl.tensor:\n-    assert lhs.type.is_block() and rhs.type.is_block()\n-    assert len(lhs.shape) == 2 and len(rhs.shape) == 2\n-    assert lhs.shape[-1] == rhs.shape[0]\n-    assert lhs.shape[0] >= 16 and lhs.shape[1] >= 16 and rhs.shape[1] >= 16,\\\n+    in_a = 1 if not trans_a else 0\n+    in_b = 1 if trans_b else 0\n+    assert a.type.is_block() and b.type.is_block()\n+    assert len(a.shape) == 2 and len(b.shape) == 2\n+    assert a.shape[in_a] == b.shape[in_b]\n+    assert a.shape[0] >= 16 and a.shape[1] >= 16 and b.shape[1] >= 16,\\\n         \"small blocks not supported!\"\n-    if lhs.type.scalar.is_int():\n+    if a.type.scalar.is_int():\n         _0 = builder.get_int32(0)\n         ret_scalar_ty = tl.int32\n     else:\n         _0 = builder.get_float32(0)\n         ret_scalar_ty = tl.float32\n-    M = lhs.type.shape[0]\n-    N = rhs.type.shape[1]\n+    M = a.type.shape[in_a ^ 1]\n+    N = b.type.shape[in_b ^ 1]\n     _0 = builder.create_splat(_0, [M, N])\n     ret_ty = tl.block_type(ret_scalar_ty, [M, N])\n-    return tl.tensor(builder.create_dot(lhs.handle, rhs.handle, _0, allow_tf32),\n-                     ret_ty)\n+    ret = builder.create_dot(a.handle, b.handle, _0, trans_a, trans_b, allow_tf32)\n+    return tl.tensor(ret, ret_ty)\n \n \n # ===----------------------------------------------------------------------===//"}, {"filename": "python/tutorials/06-fused-attention.py", "status": "added", "additions": 198, "deletions": 0, "changes": 198, "file_content_changes": "@@ -0,0 +1,198 @@\n+import pytest\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+@triton.jit\n+def _fwd_kernel(\n+    Q, K, V,\n+    TMP, L, M,  # NOTE: TMP is a scratchpad buffer to workaround a compiler bug\n+    Out,\n+    stride_qz, stride_qh, stride_qm, stride_qk,\n+    stride_kz, stride_kh, stride_kk, stride_kn,\n+    stride_vz, stride_vh, stride_vk, stride_vn,\n+    stride_oz, stride_oh, stride_om, stride_on,\n+    Z, H, N_CTX,\n+    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    start_qm = tl.program_id(0)\n+    off_hz = tl.program_id(1)\n+    # initialize offsets\n+    offs_m = start_qm * BLOCK_M + tl.arange(0, BLOCK_M)\n+    offs_n = tl.arange(0, BLOCK_N)\n+    offs_d = tl.arange(0, BLOCK_DMODEL)\n+    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk\n+    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk\n+    # Initialize pointers to Q, K, V\n+    q_ptrs = Q + off_q\n+    k_ptrs = K + off_k\n+    v_ptrs = V + off_v\n+    # initialize pointer to m and l\n+    t_ptrs = TMP + off_hz * N_CTX + offs_m\n+\n+    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n+\n+    q = tl.load(q_ptrs)\n+    for start_n in range(0, start_qm + 1):\n+        # -- compute qk ----\n+        k = tl.load(k_ptrs)\n+        qk = tl.dot(q, k)\n+        qk += tl.where(offs_m[:, None] >= (start_n * BLOCK_N + offs_n[None, :]), 0, float(\"-inf\"))\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        p = tl.exp(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        alpha = tl.exp(m_i - m_i_new)\n+        beta = tl.exp(m_ij - m_i_new)\n+        l_i_new = alpha * l_i + beta * l_ij\n+        # -- update output accumulator --\n+        # scale p\n+        p_scale = beta / l_i_new\n+        p = p * p_scale[:, None]\n+        p = p.to(tl.float16)\n+        # scale acc\n+        acc_scale = l_i / l_i_new * alpha\n+        tl.store(t_ptrs, acc_scale)\n+        acc_scale = tl.load(t_ptrs)  # BUG: have to store and immediately load\n+        acc = acc * acc_scale[:, None]\n+        # update acc\n+        v = tl.load(v_ptrs)\n+        acc += tl.dot(p, v)\n+        k_ptrs += BLOCK_N * stride_kn\n+        v_ptrs += BLOCK_N * stride_vk\n+        # r_ptrs += BLOCK_N\n+        l_i = l_i_new\n+        m_i = m_i_new\n+\n+    start_qm = tl.program_id(0)\n+    offs_m = start_qm * BLOCK_M + tl.arange(0, BLOCK_M)\n+    # write back l and m\n+    l_ptrs = L + off_hz * N_CTX + offs_m\n+    m_ptrs = M + off_hz * N_CTX + offs_m\n+    tl.store(l_ptrs, l_i)\n+    tl.store(m_ptrs, m_i)\n+    # initialize pointers to output\n+    offs_n = tl.arange(0, BLOCK_DMODEL)\n+    off_out = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n+    out_ptrs = Out + off_out\n+    tl.store(out_ptrs, acc)\n+\n+\n+class _attention(torch.autograd.Function):\n+\n+    @staticmethod\n+    def forward(ctx, q, k, v):\n+        BLOCK = 128\n+        # shape constraints\n+        Lq, Lk = q.shape[-1], k.shape[-2]\n+        assert Lq == Lk\n+        o = torch.empty_like(q)\n+        grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])\n+        tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        _fwd_kernel[grid](\n+            q, k, v,\n+            tmp, L, m,\n+            o,\n+            q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n+            k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n+            v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n+            o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n+            q.shape[0], q.shape[1], q.shape[2],\n+            BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n+            BLOCK_DMODEL=64, num_warps=4,\n+            num_stages=1,\n+        )\n+        ctx.save_for_backward(q, k, v, o, L, m)\n+        ctx.BLOCK = BLOCK\n+        ctx.grid = grid\n+        return o\n+\n+\n+attention = _attention.apply\n+\n+\n+@pytest.mark.parametrize('Z, H, N_CTX, D_MODEL', [(2, 3, 1024, 64)])\n+def test_op(Z, H, N_CTX, D_MODEL, dtype=torch.float16):\n+    torch.manual_seed(20)\n+    q = .5 * torch.randn((Z, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n+    k = .5 * torch.randn((Z, H, D_MODEL, N_CTX), dtype=dtype, device=\"cuda\", requires_grad=True)\n+    v = .5 * torch.randn((Z, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n+    # triton implementation\n+    tri_out = attention(q, k, v)\n+    # reference implementation\n+    M = torch.tril(torch.ones((N_CTX, N_CTX), device=\"cuda\"))\n+    ref_qk = torch.matmul(q, k)\n+    for z in range(Z):\n+        for h in range(H):\n+            ref_qk[:, :, M == 0] = float(\"-inf\")\n+    ref_qk = torch.softmax(ref_qk, dim=-1)\n+    ref_out = torch.matmul(ref_qk, v)\n+    # compare\n+    triton.testing.assert_almost_equal(ref_out, tri_out)\n+\n+\n+try:\n+    from flash_attn.flash_attn_interface import flash_attn_func\n+    HAS_FLASH = True\n+except BaseException:\n+    HAS_FLASH = False\n+\n+BATCH, N_HEADS, N_CTX, D_HEAD = 4, 64, 2048, 64\n+# vary batch size for fixed heads / seq\n+batch_bench = triton.testing.Benchmark(\n+    x_names=['BATCH'],\n+    x_vals=[2**i for i in range(0, 8)],\n+    line_arg='provider',\n+    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+    styles=[('red', '-'), ('blue', '-')],\n+    ylabel='ms',\n+    plot_name=f'fused-attention-seq{N_CTX}-head{N_HEADS}-d{D_HEAD}',\n+    args={'H': N_HEADS, 'N_CTX': N_CTX, 'D_MODEL': D_HEAD, 'dtype': torch.float16}\n+)\n+# vary seq length for fixed head and batch=4\n+seq_bench = triton.testing.Benchmark(\n+    x_names=['N_CTX'],\n+    x_vals=[2**i for i in range(10, 16)],\n+    line_arg='provider',\n+    line_vals=['triton'] + (['flash'] if HAS_FLASH else []),\n+    line_names=['Triton'] + (['Flash'] if HAS_FLASH else []),\n+    styles=[('red', '-'), ('blue', '-')],\n+    ylabel='ms',\n+    plot_name=f'fused-attention-batch{BATCH}-head{N_HEADS}-d{D_HEAD}',\n+    args={'H': D_HEAD, 'BATCH': BATCH, 'D_MODEL': D_HEAD, 'dtype': torch.float16}\n+)\n+\n+\n+@triton.testing.perf_report([batch_bench, seq_bench])\n+def bench_flash_attention(BATCH, H, N_CTX, D_MODEL, provider, dtype=torch.float16, device=\"cuda\"):\n+    warmup = 25\n+    rep = 500\n+    if provider == \"triton\":\n+        q = torch.randn((BATCH, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        k = torch.randn((BATCH, H, D_MODEL, N_CTX), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        v = torch.randn((BATCH, H, N_CTX, D_MODEL), dtype=dtype, device=\"cuda\", requires_grad=True)\n+        fn = lambda: attention(q, k, v)\n+        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        return ms\n+    if provider == \"flash\":\n+        lengths = torch.full((BATCH,), fill_value=N_CTX, device=device)\n+        cu_seqlens = torch.zeros((BATCH + 1,), device=device, dtype=torch.int32)\n+        cu_seqlens[1:] = lengths.cumsum(0)\n+        qkv = torch.randn((BATCH * N_CTX, 3, H, D_MODEL), dtype=dtype, device=device, requires_grad=True)\n+        fn = lambda: flash_attn_func(qkv, cu_seqlens, 0., N_CTX, causal=True)\n+        ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n+        return ms\n+\n+\n+bench_flash_attention.run(save_path='.', print_data=True)"}]