[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "file_content_changes": "@@ -756,6 +756,25 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n         assert to_numpy(z_tri) == z_ref\n \n \n+def test_store_bool():\n+    \"\"\"Tests that boolean True is stored as 1\"\"\"\n+    @triton.jit\n+    def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n+        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        input = tl.load(input_ptr + offsets, mask=mask)\n+        output = input\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+    src = torch.tensor([True, False], dtype=torch.bool, device='cuda')\n+    n_elements = src.numel()\n+    dst = torch.empty_like(src)\n+    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)\n+    copy_kernel[grid](src, dst, n_elements, BLOCK_SIZE=1024)\n+\n+    assert (to_numpy(src).view('uint8') == to_numpy(dst).view('uint8')).all()\n+\n+\n @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n def test_f8_xf16_roundtrip(dtype):\n     \"\"\"Tests that converting an f8 to f16 and back to f8 doesn't change its value\"\"\""}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -761,8 +761,10 @@ def store(ptr: tl.tensor,\n     elt_ty = ptr_ty.element_ty\n     # treat bool* as tl.int8*\n     if elt_ty == tl.int1:\n-        elt_ty_ptr = tl.int8\n-        ptr_ty = tl.pointer_type(elt_ty_ptr, ptr_ty.address_space)\n+        # convert to bool first and then store as int8\n+        val = cast(val, tl.int1, builder)\n+        elt_ty = tl.int8\n+        ptr_ty = tl.pointer_type(elt_ty, ptr_ty.address_space)\n         ptr = cast(ptr, ptr_ty, builder)\n     # eviction policy\n     eviction = _parse_eviction_policy(eviction_policy)"}]