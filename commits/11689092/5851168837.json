[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -99,7 +99,7 @@ jobs:\n         if: ${{ env.BACKEND == 'CUDA' && env.ENABLE_TMA == '0' && env.ENABLE_MMA_V3 == '0'}}\n         run: |\n           cd python/test/unit\n-          python3 -m pytest -n 8 --ignore=runtime\n+          python3 -m pytest -n 8 --ignore=runtime --ignore=hopper\n           # run runtime tests serially to avoid race condition with cache handling.\n           python3 -m pytest runtime/\n "}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 26, "deletions": 28, "changes": 54, "file_content_changes": "@@ -58,14 +58,10 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 256, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 16, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 32, 1, 8, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                # split-k\n-                (64, 64, 16, 2, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 4, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 8, 4, 2, None, None, None, AT, BT, DTYPE, DTYPE),\n                 # variable input\n                 (128, 128, 32, 1, 4, 2, 256, 384, 160, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 256, AT, BT, DTYPE, DTYPE),\n-                (128, 128, 32, 1, 4, 2, 107, 233, 311, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 128, AT, BT, DTYPE, DTYPE),\n+                (128, 128, 32, 1, 4, 2, 107, 233, 83, AT, BT, DTYPE, DTYPE),\n                 (128, 256, 64, 1, 8, 3, 256, 512, 160, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True]\n         ],\n@@ -77,9 +73,6 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 64, 16, 1, 4, STAGES, 256, 128, 80, AT, BT, DTYPE, DTYPE),\n                 (256, 128, 32, 1, 8, STAGES, 512, 256, 160, AT, BT, DTYPE, DTYPE),\n                 (128, 128, 32, 1, 4, STAGES, 256, 256, 160, AT, BT, DTYPE, DTYPE),\n-                # split-k\n-                (64, 64, 16, 8, 4, STAGES, 128, 128, 768, AT, BT, DTYPE, DTYPE),\n-                (64, 64, 16, 8, 4, STAGES, 128, 128, 32, AT, BT, DTYPE, DTYPE),\n             ] for DTYPE in [\"float16\", \"bfloat16\", \"float32\"] for AT in [False, True] for BT in [False, True] for STAGES in [4]\n         ],\n         # mixed-precision\n@@ -88,7 +81,6 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (32, 32, 32, 1, 1, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n-                (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n             ] for ADTYPE, BDTYPE in [(\"float8e4\", \"float8e5\"),\n                                      (\"float8e4\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n@@ -131,35 +123,44 @@ def test_op(BLOCK_M, BLOCK_N, BLOCK_K, SPLIT_K, NWARP, NSTAGE, M, N, K, AT, BT,\n     M = BLOCK_M if M is None else M\n     N = BLOCK_N if N is None else N\n     K = BLOCK_K * SPLIT_K if K is None else K\n-    a_fp8 = \"float8\" in ADTYPE\n-    b_fp8 = \"float8\" in BDTYPE\n \n     def maybe_upcast(x, dtype, is_float8):\n         if is_float8:\n             return f8_to_f16(x, dtype)\n         return x\n \n-    def init_input(n, m, t, dtype, is_float8):\n-        if t:\n-            return init_input(m, n, False, dtype, is_float8).t()\n-        if is_float8:\n-            return torch.randint(20, 50, (n, m), device=\"cuda\", dtype=torch.int8)\n+    def init_input(m, n, dtype):\n+        if 'float8' in dtype:\n+            ewidth = {'float8e4b15': 4, 'float8e4': 4, 'float8e5': 5}[dtype]\n+            sign = torch.randint(2, size=(m, n), device=\"cuda\", dtype=torch.int8) * 128\n+            val = torch.randint(2**3 - 1, size=(m, n), device=\"cuda\", dtype=torch.int8) << 7 - ewidth\n+            return sign | val\n         if dtype == \"int8\":\n-            return torch.randint(-128, 127, (n, m), device=\"cuda\", dtype=torch.int8)\n+            return torch.randint(-128, 127, (m, n), device=\"cuda\", dtype=torch.int8)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n-        return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n+        exponents = torch.randint(-10, 0, size=(m, n))\n+        ret = (2. ** exponents).to(dtype).to(\"cuda\")\n+        return ret\n \n     # allocate/transpose inputs\n-    a = init_input(M, K, AT, ADTYPE, a_fp8)\n-    b = init_input(K, N, BT, BDTYPE, b_fp8)\n+    a = init_input(M, K, ADTYPE)\n+    b = init_input(K, N, BDTYPE)\n+    a = a if not AT else a.T.contiguous().T\n+    b = b if not BT else b.T.contiguous().T\n     # run test\n-    th_a = maybe_upcast(a, ADTYPE, a_fp8).to(torch.float32)\n+    a_fp8 = \"float8\" in ADTYPE\n+    b_fp8 = \"float8\" in BDTYPE\n+    th_a = maybe_upcast(a, ADTYPE, a_fp8)\n     if AT and a_fp8:\n         th_a = th_a.view(th_a.shape[::-1]).T\n-    th_b = maybe_upcast(b, BDTYPE, b_fp8).to(torch.float32)\n+    th_b = maybe_upcast(b, BDTYPE, b_fp8)\n     if BT and b_fp8:\n         th_b = th_b.view(th_b.shape[::-1]).T\n-    th_c = torch.matmul(th_a, th_b)\n+    if th_a.is_floating_point():\n+        ab_dtype = th_a.dtype if th_a.element_size() > th_b.element_size() else th_b.dtype\n+    else:\n+        ab_dtype = torch.float32\n+    th_c = torch.matmul(th_a.to(ab_dtype), th_b.to(ab_dtype))\n     if ADTYPE == \"int8\" or BDTYPE == \"int8\":\n         th_c = th_c.to(torch.int8)\n     try:\n@@ -168,9 +169,6 @@ def init_input(n, m, t, dtype, is_float8):\n         if b_fp8:\n             b = triton.reinterpret(b, getattr(tl, BDTYPE))\n         tt_c = triton.ops.matmul(a, b)\n-        atol, rtol = 1e-2, 0\n-        if ADTYPE == torch.bfloat16 or BDTYPE == torch.bfloat16:\n-            atol, rtol = 3.5e-2, 0\n-        torch.testing.assert_allclose(th_c, tt_c, atol=atol, rtol=rtol)\n+        torch.testing.assert_allclose(th_c, tt_c, atol=0, rtol=0)\n     except triton.OutOfResources as e:\n         pytest.skip(str(e))"}]