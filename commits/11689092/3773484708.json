[{"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "file_content_changes": "@@ -4,11 +4,13 @@\n using namespace mlir;\n using namespace mlir::triton;\n \n+using ::mlir::LLVM::DotOpFMAConversionHelper;\n using ::mlir::LLVM::DotOpMmaV1ConversionHelper;\n using ::mlir::LLVM::DotOpMmaV2ConversionHelper;\n using ::mlir::LLVM::getElementsFromStruct;\n using ::mlir::LLVM::getSharedMemoryObjectFromStruct;\n using ::mlir::LLVM::getStructFromElements;\n+using ::mlir::LLVM::MMA16816ConversionHelper;\n using ::mlir::triton::gpu::getElemsPerThread;\n \n struct SplatOpConversion\n@@ -38,6 +40,11 @@ struct SplatOpConversion\n           LLVM::LLVMStructType::getLiteral(rewriter.getContext(), elemTypes);\n \n       return getStructFromElements(loc, elems, rewriter, structTy);\n+    } else if (auto dotLayout =\n+                   tensorTy.getEncoding()\n+                       .dyn_cast<triton::gpu::DotOperandEncodingAttr>()) {\n+      return convertSplatLikeOpWithDotOperandLayout(\n+          dotLayout, resType, elemType, constVal, typeConverter, rewriter, loc);\n     } else if (auto mmaLayout =\n                    tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>()) {\n       return convertSplatLikeOpWithMmaLayout(\n@@ -48,6 +55,38 @@ struct SplatOpConversion\n     return {};\n   }\n \n+  static Value convertSplatLikeOpWithDotOperandLayout(\n+      const triton::gpu::DotOperandEncodingAttr &layout, Type resType,\n+      Type elemType, Value constVal, TypeConverter *typeConverter,\n+      ConversionPatternRewriter &rewriter, Location loc) {\n+    auto tensorTy = resType.cast<RankedTensorType>();\n+    auto shape = tensorTy.getShape();\n+    auto parent = layout.getParent();\n+    int numElems{};\n+    if (auto mmaLayout = parent.dyn_cast<MmaEncodingAttr>()) {\n+      if (mmaLayout.isAmpere()) {\n+        numElems = layout.getOpIdx() == 0\n+                       ? MMA16816ConversionHelper::getANumElemsPerThread(\n+                             tensorTy, mmaLayout.getWarpsPerCTA()[0])\n+                       : MMA16816ConversionHelper::getBNumElemsPerThread(\n+                             tensorTy, mmaLayout.getWarpsPerCTA()[1]);\n+      } else if (mmaLayout.isVolta()) {\n+        DotOpMmaV1ConversionHelper helper(mmaLayout);\n+        numElems = layout.getOpIdx() == 0\n+                       ? helper.numElemsPerThreadA(shape, {0, 1})\n+                       : helper.numElemsPerThreadB(shape, {0, 1});\n+      }\n+    } else if (auto blockedLayout = parent.dyn_cast<BlockedEncodingAttr>()) {\n+      numElems = DotOpFMAConversionHelper::getNumElemsPerThread(shape, layout);\n+    } else {\n+      assert(false && \"Unsupported layout found\");\n+    }\n+    auto structTy = LLVM::LLVMStructType::getLiteral(\n+        rewriter.getContext(), SmallVector<Type>(numElems, elemType));\n+    return getStructFromElements(loc, SmallVector<Value>(numElems, constVal),\n+                                 rewriter, structTy);\n+  }\n+\n   static Value convertSplatLikeOpWithMmaLayout(\n       const MmaEncodingAttr &layout, Type resType, Type elemType,\n       Value constVal, TypeConverter *typeConverter,"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 14, "deletions": 14, "changes": 28, "file_content_changes": "@@ -1227,20 +1227,20 @@ def kernel(X, stride_xm, stride_xk,\n     elif dtype == 'int8':\n         assert 'mma.sync.aligned.m16n8k32.row.col.satfinite.s32.s8.s8.s32' in ptx\n \n-# FIXME: Unsupported layout found in ConvertSplatLikeOp\n-# def test_dot_without_load():\n-#    @triton.jit\n-#    def kernel(out):\n-#        pid = tl.program_id(axis=0)\n-#        a = tl.zeros((32, 32), tl.float32)\n-#        b = tl.zeros((32, 32), tl.float32)\n-#        c = tl.zeros((32, 32), tl.float32)\n-#        c = tl.dot(a, b)\n-#        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n-#        tl.store(pout, c)\n-#\n-#    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n-#    kernel[(1,)](out)\n+\n+def test_dot_without_load():\n+    @triton.jit\n+    def kernel(out):\n+        pid = tl.program_id(axis=0)\n+        a = tl.zeros((32, 32), tl.float32)\n+        b = tl.zeros((32, 32), tl.float32)\n+        c = tl.zeros((32, 32), tl.float32)\n+        c = tl.dot(a, b)\n+        pout = out + tl.arange(0, 32)[:, None] * 32 + tl.arange(0, 32)[None, :]\n+        tl.store(pout, c)\n+\n+    out = torch.ones((32, 32), dtype=torch.float32, device=\"cuda\")\n+    kernel[(1,)](out)\n \n # ---------------\n # test arange"}]