[{"filename": "lib/ir/builder.cc", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -1,4 +1,3 @@\n-#include <bits/types/clock_t.h>\n #include <string>\n #include <algorithm>\n #include <iostream>"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 37, "deletions": 21, "changes": 58, "file_content_changes": "@@ -1032,28 +1032,44 @@ def kernel(VALUE, X):\n             kernel[(1, )](value, x)\n     else:\n         kernel[(1, )](value, x)\n-# -------------------------\n-# test dynamic parallelism\n-# -------------------------\n \n \n-@triton.jit\n-def mult(x, alpha):\n-    tl.store(x + tl.program_id(0), alpha)\n+# ----------------\n+# test constexpr\n+# ----------------\n \n+@pytest.mark.parametrize(\"op\", ['+', '-', '*', '/', '%', '<', '>'])\n+@pytest.mark.parametrize(\"is_lhs_constexpr\", [False, True])\n+@pytest.mark.parametrize(\"is_rhs_constexpr\", [True, False])\n+def test_bin_op_constexpr(op, is_lhs_constexpr, is_rhs_constexpr):\n \n-@triton.jit\n-def stub(X, alpha, grid_0, grid_1, grid_2):\n-    tl.launch(mult, [X, alpha], [grid_0, grid_1, grid_2])\n-\n-\n-# def test_dyn_par(cond=True, device='cuda'):\n-#     n_pids = 10\n-#     # pids = torch.arange(n_pids, device=device)\n-#     # alpha = 2.0\n-#     # x_ref = pids * alpha\n-#     x_tri = torch.full((10,), fill_value=-1., device=device)\n-#     # cond = torch.tensor([cond], device=device)\n-#     stub[(1,)](x_tri, 3.14, n_pids, 1, 1)\n-#     print(x_tri)\n-#     # triton.testing.assert_almost_equal(x_ref, x_tri)\n+    @triton.jit\n+    def kernel(Z, X, Y):\n+        x = tl.load(X)\n+        y = tl.load(Y)\n+        z = GENERATE_TEST_HERE\n+        tl.store(Z, z)\n+\n+    x_str = \"3.14\" if is_lhs_constexpr else \"x\"\n+    y_str = \"4.13\" if is_rhs_constexpr else \"y\"\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f\"{x_str} {op} {y_str}\"})\n+    x = numpy_random((1,), dtype_str=\"float32\")\n+    y = numpy_random((1,), dtype_str=\"float32\")\n+    z = np.array(eval(f\"{x_str} {op} {y_str}\"))\n+    x_tri = to_triton(x)\n+    y_tri = to_triton(y)\n+    z_tri = to_triton(np.empty((1,), dtype=z.dtype))\n+    kernel[(1,)](z_tri, x_tri, y_tri)\n+    np.testing.assert_allclose(z, to_numpy(z_tri))\n+\n+\n+def test_constexpr_shape():\n+\n+    @triton.jit\n+    def kernel(X):\n+        off = tl.arange(0, 128 + 128)\n+        tl.store(X + off, off)\n+\n+    x_tri = to_triton(np.empty((256, ), dtype=np.int32))\n+    kernel[(1,)](x_tri)\n+    np.testing.assert_equal(to_numpy(x_tri), np.arange(0, 256))"}, {"filename": "python/triton/code_gen.py", "status": "modified", "additions": 29, "deletions": 14, "changes": 43, "file_content_changes": "@@ -390,12 +390,14 @@ def visit_Tuple(self, node):\n         return tuple(args)\n \n     def visit_BinOp(self, node):\n+        # visit operand\n         lhs = self.visit(node.left)\n         rhs = self.visit(node.right)\n-        if isinstance(lhs, triton.language.constexpr):\n-            lhs = lhs.value\n-        if isinstance(rhs, triton.language.constexpr):\n-            rhs = rhs.value\n+        is_lhs_constexpr = isinstance(lhs, triton.language.constexpr)\n+        is_rhs_constexpr = isinstance(rhs, triton.language.constexpr)\n+        lhs = lhs.value if is_lhs_constexpr else lhs\n+        rhs = rhs.value if is_rhs_constexpr else rhs\n+        # get function name\n         fn = {\n             ast.Add: '__add__',\n             ast.Sub: '__sub__',\n@@ -410,6 +412,10 @@ def visit_BinOp(self, node):\n             ast.BitOr: '__or__',\n             ast.BitXor: '__xor__',\n         }[type(node.op)]\n+        # return a new constexpr if both arg are constexprs\n+        if is_lhs_constexpr and is_rhs_constexpr:\n+            return triton.language.constexpr(getattr(lhs, fn)(rhs))\n+        # call operator\n         if is_triton_tensor(lhs):\n             return getattr(lhs, fn)(rhs, _builder=self.builder)\n         elif is_triton_tensor(rhs):\n@@ -468,14 +474,16 @@ def visit_Compare(self, node):\n         assert len(node.ops) == 1\n         lhs = self.visit(node.left)\n         rhs = self.visit(node.comparators[0])\n-        if isinstance(lhs, triton.language.constexpr):\n-            lhs = lhs.value\n-        if isinstance(rhs, triton.language.constexpr):\n-            rhs = rhs.value\n+        is_lhs_constexpr = isinstance(lhs, triton.language.constexpr)\n+        is_rhs_constexpr = isinstance(rhs, triton.language.constexpr)\n+        lhs = lhs.value if is_lhs_constexpr else lhs\n+        rhs = rhs.value if is_rhs_constexpr else rhs\n+        # handle `is`` and `is not``\n         if type(node.ops[0]) == ast.Is:\n             return triton.language.constexpr(lhs is rhs)\n         if type(node.ops[0]) == ast.IsNot:\n             return triton.language.constexpr(lhs is not rhs)\n+        # function name\n         fn = {\n             ast.Eq: '__eq__',\n             ast.NotEq: '__ne__',\n@@ -484,29 +492,32 @@ def visit_Compare(self, node):\n             ast.Gt: '__gt__',\n             ast.GtE: '__ge__',\n         }[type(node.ops[0])]\n+        # return a new constexpr if both arg are constexprs\n+        if is_lhs_constexpr and is_rhs_constexpr:\n+            return triton.language.constexpr(getattr(lhs, fn)(rhs))\n+        # call operator\n         if is_triton_tensor(lhs):\n             return getattr(lhs, fn)(rhs, _builder=self.builder)\n         elif is_triton_tensor(rhs):\n             fn = fn[:2] + 'r' + fn[2:]\n             return getattr(rhs, fn)(lhs, _builder=self.builder)\n         else:\n-            return getattr(lhs, fn)(rhs)\n+            assert False\n \n     def visit_UnaryOp(self, node):\n         op = self.visit(node.operand)\n         if type(node.op) == ast.Not:\n             assert isinstance(op, triton.language.constexpr), \"`not` only supported for constexpr at the moment\"\n             return triton.language.constexpr(not op)\n-        if isinstance(op, triton.language.constexpr):\n-            op = op.value\n         fn = {\n             ast.USub: '__neg__',\n             ast.UAdd: '__pos__',\n             ast.Invert: '__invert__',\n         }[type(node.op)]\n-        if is_triton_tensor(op):\n-            return getattr(op, fn)(_builder=self.builder)\n-        return getattr(op, fn)()\n+        if isinstance(op, triton.language.constexpr):\n+            return triton.language.constexpr(getattr(op.value, fn)())\n+        assert is_triton_tensor(op)\n+        return getattr(op, fn)(_builder=self.builder)\n \n     def visit_While(self, node):\n         current_bb = self.builder.get_insert_block()\n@@ -656,6 +667,10 @@ def visit_Call(self, node):\n             args = [arg.value if isinstance(arg, triton.language.constexpr) else arg\n                     for arg in args]\n             ret = fn(*args, **kws)\n+            if isinstance(ret, (bool, int, float)):\n+                ret = triton.language.core.constexpr(ret)\n+            else:\n+                ret = triton.language.core._to_tensor(ret, self.builder)\n         # special case: dynamic parallelism\n         # in this case the core primitive returns a proxy\n         # if isinstance(ret, triton.language.core.LaunchProxy):"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 6, "deletions": 62, "changes": 68, "file_content_changes": "@@ -337,68 +337,6 @@ def __init__(self, value):\n     def __repr__(self) -> str:\n         return f\"constexpr[{self.value}]\"\n \n-    def __add__(self, other):\n-        return self.value + other.value\n-\n-    def __radd__(self, other):\n-        return other.value + self.value\n-\n-    def __sub__(self, other):\n-        return self.value - other.value\n-\n-    def __rsub__(self, other):\n-        return other.value - self.value\n-\n-    def __mul__(self, other):\n-        return self.value * other.value\n-\n-    def __rmul__(self, other):\n-        return other.value * self.value\n-\n-    def __truediv__(self, other):\n-        return self.value / other.value\n-\n-    def __rtruediv__(self, other):\n-        return other.value / self.value\n-\n-    def __floordiv__(self, other):\n-        return self.value // other.value\n-\n-    def __rfloordiv__(self, other):\n-        return other.value // self.value\n-\n-    #\n-\n-    def __gt__(self, other):\n-        return self.value > other.value\n-\n-    def __rgt__(self, other):\n-        return other.value > self.value\n-\n-    def __ge__(self, other):\n-        return self.value >= other.value\n-\n-    def __rge__(self, other):\n-        return other.value >= self.value\n-\n-    def __lt__(self, other):\n-        return self.value < other.value\n-\n-    def __rlt__(self, other):\n-        return other.value < self.value\n-\n-    def __le__(self, other):\n-        return self.value <= other.value\n-\n-    def __rle__(self, other):\n-        return other.value <= self.value\n-\n-    def __eq__(self, other):\n-        return self.value == other.value\n-\n-    def __ne__(self, other):\n-        return self.value != other.value\n-\n     def __bool__(self):\n         return bool(self.value)\n \n@@ -496,6 +434,11 @@ def __mod__(self, other, _builder=None):\n         other = _to_tensor(other, _builder)\n         return semantic.mod(self, other, _builder)\n \n+    @builtin\n+    def __rmod__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n+        return semantic.mod(other, self, _builder)\n+\n     # unary operators\n     @builtin\n     def __neg__(self, _builder=None):\n@@ -564,6 +507,7 @@ def __lt__(self, other, _builder=None):\n \n     @builtin\n     def __rlt__(self, other, _builder=None):\n+        other = _to_tensor(other, _builder)\n         return semantic.less_than(other, self, _builder)\n \n     # <="}]