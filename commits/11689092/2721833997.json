[{"filename": "python/src/triton.cc", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -878,6 +878,7 @@ void init_triton_ir(py::module &&m) {\n       .def(\"create_int_cast\", &ir::builder::create_int_cast, ret::reference)\n       .def(\"create_downcast\", &ir::builder::create_downcast, ret::reference)\n       .def(\"create_int_to_ptr\", &ir::builder::create_int_to_ptr, ret::reference)\n+      .def(\"create_ptr_to_int\", &ir::builder::create_ptr_to_int, ret::reference)\n       // phi\n       .def(\"create_phi\", &ir::builder::create_phi, ret::reference)\n       // Binary instructions"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 54, "deletions": 7, "changes": 61, "file_content_changes": "@@ -17,6 +17,7 @@\n uint_dtypes = ['uint8', 'uint16', 'uint32', 'uint64']\n float_dtypes = ['float16', 'float32', 'float64']\n dtypes = int_dtypes + uint_dtypes + float_dtypes\n+dtypes_with_bfloat16 = dtypes + ['bfloat16']\n \n \n def _bitwidth(dtype: str) -> int:\n@@ -46,6 +47,8 @@ def numpy_random(shape, dtype_str, rs: Optional[RandomState] = None, low=None, h\n     elif dtype_str == 'bfloat16':\n         return (rs.normal(0, 1, shape).astype('float32').view('uint32')\n                 & np.uint32(0xffff0000)).view('float32')\n+    elif dtype_str in ['bool', 'int1', 'bool_']:\n+        return rs.normal(0, 1, shape) > 0.0\n     else:\n         raise RuntimeError(f'Unknown dtype {dtype_str}')\n \n@@ -245,8 +248,8 @@ def _mod_operation_ill_conditioned(dtype_x, dtype_y) -> bool:\n @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n     (dtype_x, dtype_y, op)\n     for op in ['+', '-', '*', '/', '%']\n-    for dtype_x in dtypes + ['bfloat16']\n-    for dtype_y in dtypes + ['bfloat16']\n+    for dtype_x in dtypes_with_bfloat16\n+    for dtype_y in dtypes_with_bfloat16\n ])\n def test_bin_op(dtype_x, dtype_y, op, device='cuda'):\n     expr = f' x {op} y'\n@@ -296,8 +299,8 @@ def test_floordiv(dtype_x, dtype_y, device='cuda'):\n @pytest.mark.parametrize(\"dtype_x, dtype_y, op\", [\n     (dtype_x, dtype_y, op)\n     for op in ['&', '|', '^']\n-    for dtype_x in dtypes + ['bfloat16']\n-    for dtype_y in dtypes + ['bfloat16']\n+    for dtype_x in dtypes + dtypes_with_bfloat16\n+    for dtype_y in dtypes + dtypes_with_bfloat16\n ])\n def test_bitwise_op(dtype_x, dtype_y, op, device='cuda'):\n     expr = f'x {op} y'\n@@ -363,11 +366,55 @@ def test_compare_op(dtype_x, dtype_y, op, mode_x, mode_y, device='cuda'):\n     _test_binary(dtype_x, dtype_y, expr, numpy_expr, mode_x=mode_x, mode_y=mode_y, device=device)\n \n \n+# ---------------\n+# test where\n+# ---------------\n+@pytest.mark.parametrize(\"dtype\", dtypes_with_bfloat16 + [\"*int32\"])\n+def test_where(dtype):\n+    select_ptrs = False\n+    if dtype == \"*int32\":\n+        dtype = \"int64\"\n+        select_ptrs = True\n+    check_type_supported(dtype)\n+\n+    @triton.jit\n+    def where_kernel(cond_ptr, a_ptr, b_ptr, output_ptr, n_elements,\n+                     BLOCK_SIZE: tl.constexpr,\n+                     TEST_POINTERS: tl.constexpr):\n+        offsets = tl.program_id(axis=0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+        mask = offsets < n_elements\n+        decide = tl.load(cond_ptr + offsets, mask=mask)\n+        if TEST_POINTERS:\n+            a = tl.load(a_ptr + offsets, mask=mask).to(tl.pi32_t)\n+            b = tl.load(b_ptr + offsets, mask=mask).to(tl.pi32_t)\n+        else:\n+            a = tl.load(a_ptr + offsets, mask=mask)\n+            b = tl.load(b_ptr + offsets, mask=mask)\n+        output = tl.where(decide, a, b)\n+        tl.store(output_ptr + offsets, output, mask=mask)\n+\n+    SIZE = 1_000\n+    rs = RandomState(17)\n+    cond = numpy_random(SIZE, 'bool', rs)\n+    x = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+    y = numpy_random(SIZE, dtype_str=dtype, rs=rs)\n+    z = np.where(cond, x, y)\n+\n+    cond_tri = to_triton(cond, device='cuda')\n+    x_tri = to_triton(x, device='cuda', dst_type=dtype)\n+    y_tri = to_triton(y, device='cuda', dst_type=dtype)\n+    z_tri = to_triton(np.empty(SIZE, dtype=z.dtype), device='cuda', dst_type=dtype)\n+\n+    grid = lambda meta: (triton.cdiv(SIZE, meta['BLOCK_SIZE']),)\n+    where_kernel[grid](cond_tri, x_tri, y_tri, z_tri, SIZE, BLOCK_SIZE=1024, TEST_POINTERS=select_ptrs)\n+    assert (z == to_numpy(z_tri)).all()\n+\n+\n # ---------------\n # test unary ops\n # ---------------\n @pytest.mark.parametrize(\"dtype_x, expr\", [\n-    (dtype_x, ' -x') for dtype_x in dtypes + ['bfloat16']\n+    (dtype_x, ' -x') for dtype_x in dtypes_with_bfloat16\n ] + [\n     (dtype_x, ' ~x') for dtype_x in int_dtypes\n ])\n@@ -762,7 +809,7 @@ def copy_kernel(input_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):\n @pytest.mark.parametrize(\"op, dtype_str, shape\",\n                          [(op, dtype, shape)\n                           for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n-                          for dtype in dtypes + ['bfloat16']\n+                          for dtype in dtypes_with_bfloat16\n                           for shape in [32, 64, 128, 512]])\n def test_reduce1d(op, dtype_str, shape, device='cuda'):\n     check_type_supported(dtype_str)  # bfloat16 on cc < 80 will not be tested\n@@ -810,7 +857,7 @@ def kernel(X, Z, BLOCK: tl.constexpr):\n \n \n reduce_configs1 = [\n-    (op, dtype, (1, 1024), axis) for dtype in dtypes + ['bfloat16']\n+    (op, dtype, (1, 1024), axis) for dtype in dtypes_with_bfloat16\n     for op in ['min', 'max', 'argmin', 'argmax', 'sum']\n     for axis in [1]\n ]"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 3, "deletions": 11, "changes": 14, "file_content_changes": "@@ -637,7 +637,7 @@ def cast(input: tl.tensor,\n     if src_sca_ty.is_ptr() and dst_sca_ty.is_int():\n         bitwidth = dst_sca_ty.int_bitwidth\n         if bitwidth == 64:\n-            return tl.tensor(builder.create_cast(ir.PtrToInt, input.handle, dst_ty.to_ir(builder)),\n+            return tl.tensor(builder.create_ptr_to_int(input.handle, dst_ty.to_ir(builder)),\n                              dst_ty)\n         if bitwidth == 1:\n             return not_equal(cast(input, tl.int64, builder),\n@@ -969,16 +969,8 @@ def where(condition: tl.tensor,\n         x = broadcast_impl_shape(x, condition.type.get_block_shapes(), builder)\n         y = broadcast_impl_shape(y, condition.type.get_block_shapes(), builder)\n \n-    # TODO: we need to check x's and y's shape?\n-    x_ty = x.type.scalar\n-    y_ty = y.type.scalar\n-    ty = computation_type_impl(x_ty, y_ty, div_or_mod=False)\n-    x = cast(x, ty, builder)\n-    y = cast(y, ty, builder)\n-    if x.type.is_block():\n-        ret_ty = tl.block_type(ty, x.type.shape)\n-    else:\n-        ret_ty = ty\n+    x, y = binary_op_type_checking_impl(x, y, builder, True, True)\n+    ret_ty = x.type\n     return tl.tensor(builder.create_select(condition.handle, x.handle, y.handle), ret_ty)\n \n "}]