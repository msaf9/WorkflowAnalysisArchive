[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 34, "deletions": 6, "changes": 40, "file_content_changes": "@@ -376,31 +376,59 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n   );\n \n   let builders = [\n-    // specific for MMAV1(Volta)\n+     // Specially for MMAV1(Volta)\n+    AttrBuilder<(ins \"int\":$versionMajor,\n+                     \"int\":$numWarps,\n+                     \"int\":$id), [{\n+      assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n+      SmallVector<unsigned> wpt({static_cast<unsigned>(numWarps), 1});\n+      int versionMinor = 0;\n+\n+      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n+      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n+        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n+\n+      return $_get(context, versionMajor, versionMinor, wpt);\n+    }]>,\n+\n+    // Specially for MMAV1(Volta)\n     AttrBuilder<(ins \"int\":$versionMajor,\n                      \"ArrayRef<unsigned>\":$warpsPerCTA,\n                      \"ArrayRef<int64_t>\":$shapeA,\n                      \"ArrayRef<int64_t>\":$shapeB,\n                      \"bool\":$isARow,\n-                     \"bool\":$isBRow), [{\n-      assert(versionMajor == 1 && \"Only MMAv1 has multiple versionMinor.\");\n+                     \"bool\":$isBRow,\n+                     \"int\":$id), [{\n+      assert(versionMajor == 1 && \"This builder is specially for versionMajor==1\");\n       bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n       bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n       // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n       int versionMinor = (isARow * (1<<0)) |\\\n                          (isBRow * (1<<1)) |\\\n                          (isAVec4 * (1<<2)) |\\\n                          (isBVec4 * (1<<3));\n+\n+      assert(id < (1<<numBitsToHoldMmaV1ID) && \"MMAv1 ID exceeds the maximum\");\n+      for (int i = 0; i < numBitsToHoldMmaV1ID; ++i)\n+        versionMinor |= static_cast<bool>((1<<i) & id) * (1<<(4+i));\n+\n       return $_get(context, versionMajor, versionMinor, warpsPerCTA);\n     }]>\n-\n   ];\n \n   let extraClassDeclaration = extraBaseClassDeclaration # [{\n     bool isVolta() const;\n     bool isAmpere() const;\n-    // Get [isARow, isBRow, isAVec4, isBVec4] from versionMinor\n-    std::tuple<bool, bool, bool, bool> decodeVoltaLayoutStates() const;\n+    // Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n+    std::tuple<bool, bool, bool, bool, int> decodeVoltaLayoutStates() const;\n+    // Number of bits in versionMinor to hold the ID of the MMA encoding instance.\n+    // Here 5 bits can hold 32 IDs in a single module.\n+    static constexpr int numBitsToHoldMmaV1ID{5};\n+\n+    // Here is a temporary flag that indicates whether we need to update the warpsPerCTA for MMAv1, since the current backend cannot support the updated wpt.\n+    // The mmav1's wpt-related logic is separated into multiple files, so a global flag is added here for universal coordination.\n+    // TODO[Superjomn]: Remove this flag once the MMAv1 backend is ready.\n+    static constexpr bool _mmaV1UpdateWpt{false};\n   }];\n \n }"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -21,6 +21,8 @@ std::unique_ptr<Pass> createTritonGPUCombineOpsPass(int computeCapability = 80);\n \n std::unique_ptr<Pass> createTritonGPUVerifier();\n \n+std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass();\n+\n /// Generate the code for registering passes.\n #define GEN_PASS_REGISTRATION\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\""}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Passes.td", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -108,4 +108,16 @@ def TritonGPUCanonicalizeLoops: Pass<\"tritongpu-canonicalize-loops\", \"mlir::Modu\n   let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n }\n \n+def UpdateMmaForVolta : Pass<\"tritongpu-update-mma-for-volta\", \"mlir::ModuleOp\"> {\n+  let summary = \"Update mma encodings for Volta\";\n+\n+  let description = [{\n+    This helps to update the mma encodings for Volta.\n+  }];\n+\n+  let constructor = \"mlir::createTritonGPUUpdateMmaForVoltaPass()\";\n+\n+  let dependentDialects = [\"mlir::triton::gpu::TritonGPUDialect\"];\n+}\n+\n #endif"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -600,15 +600,20 @@ bool MmaEncodingAttr::isVolta() const { return getVersionMajor() == 1; }\n \n bool MmaEncodingAttr::isAmpere() const { return getVersionMajor() == 2; }\n \n-// Get [isARow, isBRow, isAVec4, isBVec4] from versionMinor\n-std::tuple<bool, bool, bool, bool>\n+// Get [isARow, isBRow, isAVec4, isBVec4, id] from versionMinor\n+std::tuple<bool, bool, bool, bool, int>\n MmaEncodingAttr::decodeVoltaLayoutStates() const {\n   unsigned versionMinor = getVersionMinor();\n   bool isARow = versionMinor & (1 << 0);\n   bool isBRow = versionMinor & (1 << 1);\n   bool isAVec4 = versionMinor & (1 << 2);\n   bool isBVec4 = versionMinor & (1 << 3);\n-  return std::make_tuple(isARow, isBRow, isAVec4, isBVec4);\n+\n+  int id = 0;\n+  for (int i = numBitsToHoldMmaV1ID - 1; i >= 0; --i)\n+    id = (id << 1) + static_cast<bool>(versionMinor & (1 << (4 + i)));\n+\n+  return std::make_tuple(isARow, isBRow, isAVec4, isBVec4, id);\n }\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/CMakeLists.txt", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "file_content_changes": "@@ -11,6 +11,8 @@ add_mlir_dialect_library(TritonGPUTransforms\n   ReorderInstructions.cpp\n   DecomposeConversions.cpp\n   TritonGPUConversion.cpp\n+  UpdateMmaForVolta.cpp\n+  Utility.cpp\n \n   DEPENDS\n   TritonGPUTransformsIncGen"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 36, "deletions": 285, "changes": 321, "file_content_changes": "@@ -1,3 +1,4 @@\n+#include \"Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/SCF.h\"\n #include \"mlir/IR/BlockAndValueMapping.h\"\n@@ -26,6 +27,7 @@ using triton::DotOp;\n using triton::gpu::ConvertLayoutOp;\n using triton::gpu::DotOperandEncodingAttr;\n using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SliceEncodingAttr;\n \n // -----------------------------------------------------------------------------\n //\n@@ -885,24 +887,31 @@ SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n \n SmallVector<unsigned, 2> warpsPerTileV1(const ArrayRef<int64_t> shape,\n                                         int numWarps) {\n-  SmallVector<unsigned, 2> ret = {1, 1};\n-  SmallVector<int64_t, 2> shapePerWarp =\n-      mmaVersionToShapePerWarp(1 /*version*/);\n-  bool changed = false;\n-  do {\n-    changed = false;\n-    int pre = ret[0];\n-    if (ret[0] * ret[1] < numWarps) {\n-      ret[0] = std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n-      changed = pre != ret[0];\n-    }\n-    if (ret[0] * ret[1] < numWarps) {\n-      pre = ret[1];\n-      ret[1] = std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n-      changed = pre != ret[1];\n-    }\n-  } while (changed);\n-  return ret;\n+  if (!MmaEncodingAttr::_mmaV1UpdateWpt) {\n+    SmallVector<unsigned, 2> ret = {1, 1};\n+    SmallVector<int64_t, 2> shapePerWarp =\n+        mmaVersionToShapePerWarp(1 /*version*/);\n+    bool changed = false;\n+    do {\n+      changed = false;\n+      int pre = ret[0];\n+      if (ret[0] * ret[1] < numWarps) {\n+        ret[0] =\n+            std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n+        changed = pre != ret[0];\n+      }\n+      if (ret[0] * ret[1] < numWarps) {\n+        pre = ret[1];\n+        ret[1] =\n+            std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n+        changed = pre != ret[1];\n+      }\n+    } while (changed);\n+    return ret;\n+  } else {\n+    // Set a default value and ensure product of wpt equals numWarps\n+    return {static_cast<unsigned>(numWarps), 1};\n+  }\n }\n \n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n@@ -1039,6 +1048,7 @@ class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n \n class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n+  mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n \n public:\n   BlockedToMMA(mlir::MLIRContext *context, int computeCapability)\n@@ -1097,13 +1107,13 @@ class BlockedToMMA : public mlir::RewritePattern {\n         getWarpsPerTile(dotOp, retShape, versionMajor, numWarps);\n     triton::gpu::MmaEncodingAttr mmaEnc;\n     if (versionMajor == 1) {\n-      auto shapeA = AType.getShape();\n-      auto shapeB = BType.getShape();\n-      bool isARow = AOrder[0] != 0;\n-      bool isBRow = BOrder[0] != 0;\n-      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-          oldRetType.getContext(), versionMajor, warpsPerTile, shapeA, shapeB,\n-          isARow, isBRow);\n+      if (MmaEncodingAttr::_mmaV1UpdateWpt)\n+        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+            oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n+      else\n+        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+            dotOp->getContext(), versionMajor, 0 /*versionMinor*/,\n+            warpsPerTileV1(retShape, numWarps));\n     } else if (versionMajor == 2) {\n       mmaEnc = triton::gpu::MmaEncodingAttr::get(\n           oldRetType.getContext(), versionMajor, 0 /*versionMinor*/,\n@@ -1159,102 +1169,6 @@ class BlockedToMMA : public mlir::RewritePattern {\n   }\n };\n \n-class FixupLoop : public mlir::RewritePattern {\n-\n-public:\n-  explicit FixupLoop(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-    auto forOp = cast<scf::ForOp>(op);\n-\n-    // Rewrite init argument\n-    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n-    bool shouldRematerialize = false;\n-    for (size_t i = 0; i < newInitArgs.size(); i++) {\n-      auto initArg = newInitArgs[i];\n-      auto regionArg = forOp.getRegionIterArgs()[i];\n-      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n-          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n-        shouldRematerialize = true;\n-        break;\n-      }\n-    }\n-    if (!shouldRematerialize)\n-      return failure();\n-\n-    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n-        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n-        forOp.getStep(), newInitArgs);\n-    newForOp->moveBefore(forOp);\n-    rewriter.setInsertionPointToStart(newForOp.getBody());\n-    BlockAndValueMapping mapping;\n-    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n-      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n-    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n-\n-    for (Operation &op : forOp.getBody()->getOperations()) {\n-      rewriter.clone(op, mapping);\n-    }\n-    rewriter.replaceOp(forOp, newForOp.getResults());\n-    return success();\n-  }\n-};\n-\n-// This pattern collects the wrong Mma those need to update and create the right\n-// ones for each.\n-class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n-  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n-\n-public:\n-  CollectMmaToUpdateForVolta(\n-      mlir::MLIRContext *ctx,\n-      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n-      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n-        mmaToUpdate(mmaToUpdate) {}\n-\n-  mlir::LogicalResult\n-  matchAndRewrite(mlir::Operation *op,\n-                  mlir::PatternRewriter &rewriter) const override {\n-\n-    auto dotOp = cast<triton::DotOp>(op);\n-    auto *ctx = dotOp->getContext();\n-    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n-    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n-    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n-    if (!DT.getEncoding())\n-      return failure();\n-    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n-    if (!(mmaLayout && mmaLayout.isVolta()))\n-      return failure();\n-\n-    // Has processed.\n-    if (mmaToUpdate.count(mmaLayout))\n-      return failure();\n-\n-    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n-    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n-    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    auto [isARow_, isBRow_, isAVec4, isBVec4] =\n-        mmaLayout.decodeVoltaLayoutStates();\n-    if (isARow_ == isARow && isBRow_ == isBRow) {\n-      return failure(); // No need to update\n-    }\n-\n-    auto newMmaLayout = MmaEncodingAttr::get(\n-        ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n-        AT.getShape(), BT.getShape(), isARow, isBRow);\n-\n-    // Collect the wrong MMA Layouts, and mark need to update.\n-    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n-\n-    return failure();\n-  }\n-};\n-\n // Convert + trans + convert\n // x = convert_layout distributed -> #shared_x\n // y = trans x -> #shared_y\n@@ -1359,145 +1273,6 @@ class ConvertDotConvert : public mlir::RewritePattern {\n   }\n };\n \n-// Correct the versionMinor field in MmaEncodingAttr for Volta.\n-class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n-  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n-  enum class Kind {\n-    kUnk,\n-    kCvtToMma,\n-    kCvtToDotOp,\n-    kDot,\n-    kConstant,\n-  };\n-  mutable Kind rewriteKind{Kind::kUnk};\n-\n-public:\n-  UpdateMMAVersionMinorForVolta(\n-      mlir::MLIRContext *ctx, llvm::StringRef opName,\n-      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n-      : RewritePattern(opName, 1 /*benefit*/, ctx), mmaToUpdate(mmaToUpdate) {}\n-\n-  LogicalResult match(Operation *op) const override {\n-    MmaEncodingAttr mma;\n-    if (mmaToUpdate.empty())\n-      return failure();\n-    if (op->getNumResults() != 1)\n-      return failure();\n-    auto tensorTy = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n-    if (!tensorTy)\n-      return failure();\n-\n-    // ConvertLayoutOp\n-    if (auto cvt = llvm::dyn_cast<ConvertLayoutOp>(op)) {\n-      // cvt X -> dot_operand\n-      if (auto dotOperand =\n-              tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>()) {\n-        mma = dotOperand.getParent().dyn_cast<MmaEncodingAttr>();\n-        rewriteKind = Kind::kCvtToDotOp;\n-        if (mma && mmaToUpdate.count(mma))\n-          return success();\n-      }\n-      if ((mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())) {\n-        // cvt X -> mma\n-        rewriteKind = Kind::kCvtToMma;\n-        if (mma && mmaToUpdate.count(mma))\n-          return success();\n-      }\n-    } else if (auto dot = llvm::dyn_cast<DotOp>(op)) {\n-      // DotOp\n-      mma = dot.d()\n-                .getType()\n-                .cast<RankedTensorType>()\n-                .getEncoding()\n-                .dyn_cast<MmaEncodingAttr>();\n-      rewriteKind = Kind::kDot;\n-    } else if (auto constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n-      // ConstantOp\n-      mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n-      rewriteKind = Kind::kConstant;\n-    }\n-\n-    return success(mma && mmaToUpdate.count(mma));\n-  }\n-\n-  void rewrite(Operation *op, PatternRewriter &rewriter) const override {\n-    switch (rewriteKind) {\n-    case Kind::kDot:\n-      rewriteDot(op, rewriter);\n-      break;\n-    case Kind::kConstant:\n-      rewriteConstant(op, rewriter);\n-      break;\n-    case Kind::kCvtToDotOp:\n-      rewriteCvtDotOp(op, rewriter);\n-      break;\n-    case Kind::kCvtToMma:\n-      rewriteCvtToMma(op, rewriter);\n-      break;\n-    default:\n-      llvm::report_fatal_error(\"Not supported rewrite kind\");\n-    }\n-  }\n-\n-private:\n-  void rewriteCvtDotOp(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n-    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n-    auto dotOperand = tensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n-    MmaEncodingAttr newMma =\n-        mmaToUpdate.lookup(dotOperand.getParent().cast<MmaEncodingAttr>());\n-    auto newDotOperand = DotOperandEncodingAttr::get(\n-        ctx, dotOperand.getOpIdx(), newMma, dotOperand.getIsMMAv1Row());\n-    auto newTensorTy = RankedTensorType::get(\n-        tensorTy.getShape(), tensorTy.getElementType(), newDotOperand);\n-    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n-                                                 cvt.getOperand());\n-  }\n-\n-  void rewriteDot(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto dot = llvm::cast<DotOp>(op);\n-    auto tensorTy = dot.d().getType().cast<RankedTensorType>();\n-    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto newMma = mmaToUpdate.lookup(mma);\n-    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n-                                             tensorTy.getElementType(), newMma);\n-    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dot.a(), dot.b(),\n-                                       dot.c(), dot.allowTF32());\n-  }\n-\n-  void rewriteCvtToMma(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n-    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n-    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto newMma = mmaToUpdate.lookup(mma);\n-    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n-                                             tensorTy.getElementType(), newMma);\n-    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n-                                                 cvt.getOperand());\n-  }\n-\n-  void rewriteConstant(Operation *op, PatternRewriter &rewriter) const {\n-    auto *ctx = op->getContext();\n-    auto constant = llvm::cast<arith::ConstantOp>(op);\n-    auto tensorTy = constant.getResult().getType().dyn_cast<RankedTensorType>();\n-    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n-    auto newMma = mmaToUpdate.lookup(mma);\n-    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n-                                             tensorTy.getElementType(), newMma);\n-    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n-      auto newRet =\n-          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n-      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newTensorTy, newRet);\n-      return;\n-    }\n-\n-    assert(false && \"Not supported ConstantOp value type\");\n-  }\n-};\n-\n } // namespace\n \n #define GEN_PASS_CLASSES\n@@ -1534,31 +1309,7 @@ class TritonGPUCombineOpsPass\n       signalPassFailure();\n     }\n \n-    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n-    {\n-      mlir::RewritePatternSet patterns(context);\n-      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n-      if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n-        signalPassFailure();\n-    }\n-    {\n-      mlir::RewritePatternSet patterns(context);\n-      patterns.add<UpdateMMAVersionMinorForVolta>(\n-          context, DotOp::getOperationName(), mmaToUpdate);\n-      patterns.add<UpdateMMAVersionMinorForVolta>(\n-          context, ConvertLayoutOp::getOperationName(), mmaToUpdate);\n-      patterns.add<UpdateMMAVersionMinorForVolta>(\n-          context, arith::ConstantOp::getOperationName(), mmaToUpdate);\n-      mlir::GreedyRewriteConfig config;\n-      config.useTopDownTraversal = true;\n-\n-      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n-        signalPassFailure();\n-    }\n-\n-    mlir::RewritePatternSet loopFixup(context);\n-    loopFixup.add<FixupLoop>(context);\n-    if (applyPatternsAndFoldGreedily(m, std::move(loopFixup)).failed()) {\n+    if (fixupLoops(m).failed()) {\n       signalPassFailure();\n     }\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "added", "additions": 354, "deletions": 0, "changes": 354, "file_content_changes": "@@ -0,0 +1,354 @@\n+#include \"Utility.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+#include \"triton/Analysis/Utility.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+\n+namespace mlir {\n+namespace {\n+using triton::DotOp;\n+using triton::gpu::ConvertLayoutOp;\n+using triton::gpu::DotOperandEncodingAttr;\n+using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SliceEncodingAttr;\n+\n+// This pattern collects the wrong Mma those need to update and create the right\n+// ones for each.\n+// TODO[Superjomn]: RewirtePattern is not needed here, Rewrite this to a method\n+class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n+  // Holds the mapping from old(wrong) mmaEncodingAttr to the new(correct)\n+  // mmaEncodingAttr.\n+  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  CollectMmaToUpdateForVolta(\n+      mlir::MLIRContext *ctx,\n+      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+\n+    auto dotOp = cast<triton::DotOp>(op);\n+    auto *ctx = dotOp->getContext();\n+    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n+    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n+    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n+    if (!DT.getEncoding())\n+      return failure();\n+    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    if (!(mmaLayout && mmaLayout.isVolta()))\n+      return failure();\n+\n+    // Has processed.\n+    if (mmaToUpdate.count(mmaLayout))\n+      return failure();\n+\n+    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n+    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n+    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto [isARow_, isBRow_, isAVec4, isBVec4, mmaId] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+\n+    // The wpt of MMAv1 is also determined by isARow, isBRow and shape, and it\n+    // could only be set here for those states might be updated by previous\n+    // patterns in the Combine Pass.\n+    if (isARow_ == isARow && isBRow_ == isBRow) {\n+      auto tgtWpt =\n+          getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n+                         product(mmaLayout.getWarpsPerCTA()));\n+      // Check if the wpt should be updated.\n+      if (tgtWpt == mmaLayout.getWarpsPerCTA() ||\n+          !MmaEncodingAttr::_mmaV1UpdateWpt)\n+        return failure();\n+    }\n+\n+    MmaEncodingAttr newMmaLayout;\n+    {\n+      // Create a temporary MMA layout to obtain the isAVec4 and isBVec4\n+      auto tmpMmaLayout = MmaEncodingAttr::get(\n+          ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n+          AT.getShape(), BT.getShape(), isARow, isBRow, mmaId);\n+      auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n+          tmpMmaLayout.decodeVoltaLayoutStates();\n+\n+      // Recalculate the wpt, for here we could get the latest information, the\n+      // wpt should be updated.\n+      auto updatedWpt =\n+          getWarpsPerCTA(DT.getShape(), isARow_, isBRow_, isAVec4_, isBVec4_,\n+                         product(mmaLayout.getWarpsPerCTA()));\n+      auto newWpt = MmaEncodingAttr::_mmaV1UpdateWpt\n+                        ? updatedWpt\n+                        : mmaLayout.getWarpsPerCTA();\n+      newMmaLayout = MmaEncodingAttr::get(ctx, mmaLayout.getVersionMajor(),\n+                                          newWpt, AT.getShape(), BT.getShape(),\n+                                          isARow, isBRow, mmaId);\n+    }\n+\n+    // Collect the wrong MMA Layouts, and mark need to update.\n+    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n+\n+    return failure();\n+  }\n+\n+  // Get the wpt for MMAv1 using more information.\n+  // Reference the original logic here\n+  // https://github.com/openai/triton/blob/0e4691e6dd91e001a8d33b71badf8b3314325459/lib/codegen/analysis/layout.cc#L223\n+  SmallVector<unsigned, 2> getWarpsPerCTA(ArrayRef<int64_t> shape, bool isARow,\n+                                          bool isBRow, bool isAVec4,\n+                                          bool isBVec4, int numWarps) const {\n+    // TODO[Superjomn]: Share code with\n+    // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n+    // rep,spw and fpw.\n+    SmallVector<unsigned, 2> wpt({1, 1});\n+    SmallVector<unsigned, 2> wpt_nm1;\n+\n+    SmallVector<int, 2> rep(2), spw(2);\n+    std::array<int, 3> fpw{{2, 2, 1}};\n+    int packSize0 = (isARow || isAVec4) ? 1 : 2;\n+    rep[0] = 2 * packSize0;\n+    spw[0] = fpw[0] * 4 * rep[0];\n+\n+    int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n+    rep[1] = 2 * packSize1;\n+    spw[1] = fpw[1] * 4 * rep[1];\n+\n+    do {\n+      wpt_nm1 = wpt;\n+      if (wpt[0] * wpt[1] < numWarps)\n+        wpt[0] = std::clamp<int>(wpt[0] * 2, 1, shape[0] / spw[0]);\n+      if (wpt[0] * wpt[1] < numWarps)\n+        wpt[1] = std::clamp<int>(wpt[1] * 2, 1, shape[1] / spw[1]);\n+    } while (wpt_nm1 != wpt);\n+\n+    return wpt;\n+  }\n+};\n+\n+class UpdateMMAForMMAv1 : public mlir::RewritePattern {\n+  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  UpdateMMAForMMAv1(\n+      MLIRContext *context,\n+      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : RewritePattern(MatchAnyOpTypeTag{}, 1, context),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    // Nothing to update\n+    if (mmaToUpdate.empty())\n+      return failure();\n+\n+    if (auto dotOp = llvm::dyn_cast<DotOp>(op))\n+      return rewriteDotOp(op, rewriter);\n+    else if (auto cvtOp = llvm::dyn_cast<ConvertLayoutOp>(op))\n+      return rewriteCvtOp(op, rewriter);\n+    else if (auto expandDimsOp = llvm::dyn_cast<triton::ExpandDimsOp>(op))\n+      return rewriteExpandDimsOp(op, rewriter);\n+    else if (auto constOp = llvm::dyn_cast<arith::ConstantOp>(op))\n+      return rewriteConstantOp(op, rewriter);\n+    else\n+      return rewriteElementwiseOp(op, rewriter);\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteDotOp(Operation *op,\n+                             mlir::PatternRewriter &rewriter) const {\n+    auto dotOp = llvm::cast<DotOp>(op);\n+    auto tensorTy = dotOp->getResult(0).getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return failure();\n+\n+    auto mma = dotOp.d()\n+                   .getType()\n+                   .cast<RankedTensorType>()\n+                   .getEncoding()\n+                   .dyn_cast<MmaEncodingAttr>();\n+    if (!mma || !mmaToUpdate.count(mma))\n+      return failure();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dotOp.a(), dotOp.b(),\n+                                       dotOp.c(), dotOp.allowTF32());\n+    return success();\n+  }\n+\n+  LogicalResult rewriteCvtOp(Operation *op,\n+                             mlir::PatternRewriter &rewriter) const {\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    if (!needUpdate(cvt.getResult().getType()))\n+      return failure();\n+    auto tensorTy = cvt.result().getType().dyn_cast<RankedTensorType>();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    auto newOp = rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                              cvt.getOperand());\n+    return success();\n+  }\n+\n+  LogicalResult rewriteExpandDimsOp(Operation *op,\n+                                    mlir::PatternRewriter &rewriter) const {\n+    auto expandDims = llvm::cast<triton::ExpandDimsOp>(op);\n+    auto srcTy = expandDims.src().getType();\n+    auto resTy = expandDims.getResult().getType();\n+\n+    // the result type need to update\n+    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n+      rewriter.replaceOpWithNewOp<triton::ExpandDimsOp>(op, expandDims.src(),\n+                                                        expandDims.axis());\n+      return success();\n+    }\n+\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteConstantOp(Operation *op,\n+                                  mlir::PatternRewriter &rewriter) const {\n+    auto constant = llvm::cast<arith::ConstantOp>(op);\n+    auto resTy = constant.getResult().getType();\n+    if (!needUpdate(resTy))\n+      return failure();\n+\n+    auto tensorTy = constant.getResult().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    if ((!mma))\n+      return failure();\n+\n+    auto newTensorTy = getUpdatedType(tensorTy);\n+    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n+      auto newRet =\n+          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n+      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newTensorTy, newRet);\n+      return success();\n+    }\n+\n+    return failure();\n+  }\n+\n+  LogicalResult rewriteElementwiseOp(Operation *op,\n+                                     mlir::PatternRewriter &rewriter) const {\n+    if (op->getNumOperands() != 1 || op->getNumResults() != 1)\n+      return failure();\n+\n+    auto srcTy = op->getOperand(0).getType();\n+    auto resTy = op->getResult(0).getType();\n+    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n+      op->getResult(0).setType(\n+          getUpdatedType(resTy.dyn_cast<RankedTensorType>()));\n+      return success();\n+    }\n+    return failure();\n+  }\n+\n+  RankedTensorType getUpdatedType(RankedTensorType type) const {\n+    if (!needUpdate(type))\n+      return type;\n+    auto encoding = type.getEncoding();\n+    if (auto mma = encoding.dyn_cast<MmaEncodingAttr>()) {\n+      auto newMma = mmaToUpdate.lookup(mma);\n+      return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                   newMma);\n+    } else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n+      if (auto mma = slice.getParent().dyn_cast<MmaEncodingAttr>()) {\n+        auto newMma = mmaToUpdate.lookup(mma);\n+        auto newSlice =\n+            SliceEncodingAttr::get(slice.getContext(), slice.getDim(), newMma);\n+        return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                     newSlice);\n+      }\n+    } else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n+      if (auto mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>()) {\n+        auto newMma = mmaToUpdate.lookup(mma);\n+        auto newDotOp =\n+            DotOperandEncodingAttr::get(dotOp.getContext(), dotOp.getOpIdx(),\n+                                        newMma, dotOp.getIsMMAv1Row());\n+        return RankedTensorType::get(type.getShape(), type.getElementType(),\n+                                     newDotOp);\n+      }\n+    }\n+    return type;\n+  }\n+\n+  // Tell if this type contains a wrong MMA encoding and need to update.\n+  bool needUpdate(Type type) const {\n+    auto tensorTy = type.dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return false;\n+    return needUpdate(tensorTy);\n+  }\n+\n+  // Tell if this type contains a wrong MMA encoding and need to update.\n+  bool needUpdate(RankedTensorType type) const {\n+    auto encoding = type.getEncoding();\n+    if (!encoding)\n+      return false;\n+\n+    MmaEncodingAttr mma;\n+    if ((mma = encoding.dyn_cast<MmaEncodingAttr>())) {\n+    } else if (auto slice = encoding.dyn_cast<SliceEncodingAttr>()) {\n+      mma = slice.getParent().dyn_cast<MmaEncodingAttr>();\n+    } else if (auto dotOp = encoding.dyn_cast<DotOperandEncodingAttr>()) {\n+      mma = dotOp.getParent().dyn_cast<MmaEncodingAttr>();\n+    }\n+\n+    return mma && mmaToUpdate.count(mma);\n+  }\n+};\n+\n+} // namespace\n+\n+#define GEN_PASS_CLASSES\n+#include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n+\n+class UpdateMmaForVoltaPass\n+    : public UpdateMmaForVoltaBase<UpdateMmaForVoltaPass> {\n+public:\n+  UpdateMmaForVoltaPass() = default;\n+  void runOnOperation() override {\n+    MLIRContext *context = &getContext();\n+    ModuleOp m = getOperation();\n+\n+    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n+\n+      GreedyRewriteConfig config;\n+      config.enableRegionSimplification =\n+          false; // The pattern doesn't modify the IR\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+    }\n+\n+    if (!mmaToUpdate.empty()) {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<UpdateMMAForMMAv1>(context, mmaToUpdate);\n+\n+      mlir::GreedyRewriteConfig config;\n+      // Make sure the slice and dot_operand layouts' parent mma are updated\n+      // before updating DotOp or it will get a mismatch parent-encoding.\n+      config.useTopDownTraversal = true;\n+\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+\n+      if (fixupLoops(m).failed())\n+        signalPassFailure();\n+    }\n+  }\n+};\n+\n+std::unique_ptr<Pass> createTritonGPUUpdateMmaForVoltaPass() {\n+  return std::make_unique<UpdateMmaForVoltaPass>();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "added", "additions": 63, "deletions": 0, "changes": 63, "file_content_changes": "@@ -0,0 +1,63 @@\n+#include \"Utility.h\"\n+#include \"mlir/Dialect/SCF/SCF.h\"\n+#include \"mlir/IR/BlockAndValueMapping.h\"\n+#include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n+\n+namespace mlir {\n+\n+namespace {\n+\n+class FixupLoop : public mlir::RewritePattern {\n+\n+public:\n+  explicit FixupLoop(mlir::MLIRContext *context)\n+      : mlir::RewritePattern(scf::ForOp::getOperationName(), 2, context) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+    auto forOp = cast<scf::ForOp>(op);\n+\n+    // Rewrite init argument\n+    SmallVector<Value, 4> newInitArgs = forOp.getInitArgs();\n+    bool shouldRematerialize = false;\n+    for (size_t i = 0; i < newInitArgs.size(); i++) {\n+      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n+          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n+        shouldRematerialize = true;\n+        break;\n+      }\n+    }\n+    if (!shouldRematerialize)\n+      return failure();\n+\n+    scf::ForOp newForOp = rewriter.create<scf::ForOp>(\n+        forOp.getLoc(), forOp.getLowerBound(), forOp.getUpperBound(),\n+        forOp.getStep(), newInitArgs);\n+    newForOp->moveBefore(forOp);\n+    rewriter.setInsertionPointToStart(newForOp.getBody());\n+    BlockAndValueMapping mapping;\n+    for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n+      mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n+\n+    for (Operation &op : forOp.getBody()->getOperations()) {\n+      rewriter.clone(op, mapping);\n+    }\n+    rewriter.replaceOp(forOp, newForOp.getResults());\n+    return success();\n+  }\n+};\n+\n+} // namespace\n+\n+LogicalResult fixupLoops(ModuleOp mod) {\n+  auto *ctx = mod.getContext();\n+  mlir::RewritePatternSet patterns(ctx);\n+  patterns.add<FixupLoop>(ctx);\n+  if (applyPatternsAndFoldGreedily(mod, std::move(patterns)).failed())\n+    return failure();\n+  return success();\n+}\n+\n+} // namespace mlir"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.h", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "file_content_changes": "@@ -0,0 +1,12 @@\n+#ifndef TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#define TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#include \"mlir/IR/Matchers.h\"\n+#include \"mlir/IR/PatternMatch.h\"\n+\n+namespace mlir {\n+\n+LogicalResult fixupLoops(ModuleOp mod);\n+\n+} // namespace mlir\n+\n+#endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1347,6 +1347,10 @@ void init_triton_ir(py::module &&m) {\n              self.addPass(\n                  mlir::createTritonGPUCombineOpsPass(computeCapability));\n            })\n+      .def(\"add_tritongpu_update_mma_for_volta_pass\",\n+           [](mlir::PassManager &self) {\n+             self.addPass(mlir::createTritonGPUUpdateMmaForVoltaPass());\n+           })\n       .def(\"add_tritongpu_reorder_instructions_pass\",\n            [](mlir::PassManager &self) {\n              self.addPass(mlir::createTritonGPUReorderInstructionsPass());"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -900,6 +900,9 @@ def ttir_to_ttgir(mod, num_warps, num_stages, compute_capability):\n     pm.add_tritongpu_combine_pass(compute_capability)\n     pm.add_licm_pass()\n     pm.add_tritongpu_combine_pass(compute_capability)\n+    if compute_capability // 10 == 7:\n+        # The update_mma_for_volta pass helps to compute some information for MMA encoding specifically for MMAv1\n+        pm.add_tritongpu_update_mma_for_volta_pass()\n     pm.add_cse_pass()\n     pm.add_tritongpu_decompose_conversions_pass()\n     pm.add_cse_pass()"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 0, "deletions": 29, "changes": 29, "file_content_changes": "@@ -183,32 +183,3 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   return\n }\n-\n-\n-// -----\n-\n-// check the UpdateMMAVersionMinorForVolta pattern\n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n-#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n-#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[1,1]}>\n-// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n-// and the pattern should update the versionMinor.\n-#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n-#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n-// It creates a new MMA layout to fit with $a and $b's dot_operand\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 11, warpsPerCTA = [1, 1]}>\n-module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK-LABEL: dot_mmav1\n-  func @dot_mmav1(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) -> tensor<16x16xf32, #blocked0> {\n-    %C = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked0>\n-    %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_a>\n-    %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_b>\n-    %CC = triton_gpu.convert_layout %C : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #mma0>\n-\n-    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<16x16xf32, [[new_mma]]>\n-    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n-    %res = triton_gpu.convert_layout %D : (tensor<16x16xf32, #mma0>) -> tensor<16x16xf32, #blocked0>\n-\n-    return %res : tensor<16x16xf32, #blocked0>\n-  }\n-}"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "added", "additions": 73, "deletions": 0, "changes": 73, "file_content_changes": "@@ -0,0 +1,73 @@\n+// RUN: triton-opt %s -split-input-file -tritongpu-combine -tritongpu-update-mma-for-volta 2>&1 | FileCheck %s\n+\n+// -----\n+\n+// check the UpdateMMAVersionMinorForVolta pattern\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 4], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[4,4]}>\n+// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n+// and the pattern should update the versionMinor.\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+// It creates a new MMA layout to fit with $a and $b's dot_operand, and get the right warpsPerCTA\n+// The ID of this MMA instance should be 0.\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n+    %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n+\n+    return %res : tensor<64x64xf32, #blocked0>\n+  }\n+}\n+\n+\n+// -----\n+// Check id in multiple MMA layout instances\n+\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [4, 4], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[4,4]}>\n+// mma id=1, with all other boolean flags be false, should get a versionMinor of 16(= 1 * 1<<4)\n+#mma1 = #triton_gpu.mma<{versionMajor=1, versionMinor=16, warpsPerCTA=[4,4]}>\n+\n+// Will still get two MMA layouts\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+// CHECK: [[new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 4]}>\n+\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+#dot_operand_a1 = #triton_gpu.dot_op<{opIdx=0, parent=#mma1, isMMAv1Row=true}>\n+#dot_operand_b1 = #triton_gpu.dot_op<{opIdx=1, parent=#mma1, isMMAv1Row=false}>\n+\n+module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma0>\n+\n+    %AA1 = triton_gpu.convert_layout %A : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_a1>\n+    %BB1 = triton_gpu.convert_layout %B : (tensor<64x64xf16, #blocked0>) -> tensor<64x64xf16, #dot_operand_b1>\n+    %CC1 = triton_gpu.convert_layout %C : (tensor<64x64xf32, #blocked0>) -> tensor<64x64xf32, #mma1>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma]]>\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, {{.*}} {allowTF32 = true} : tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma1]], isMMAv1Row = true}>> * tensor<64x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma1]], isMMAv1Row = true}>> -> tensor<64x64xf32, [[new_mma1]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<64x64xf32, #mma0>\n+    %D1 = tt.dot %AA1, %BB1, %CC1 {allowTF32 = true} : tensor<64x64xf16, #dot_operand_a1> * tensor<64x64xf16, #dot_operand_b1> -> tensor<64x64xf32, #mma1>\n+    %res = triton_gpu.convert_layout %D : (tensor<64x64xf32, #mma0>) -> tensor<64x64xf32, #blocked0>\n+    %res1 = triton_gpu.convert_layout %D1 : (tensor<64x64xf32, #mma1>) -> tensor<64x64xf32, #blocked0>\n+    %sum = arith.addf %res, %res1 : tensor<64x64xf32, #blocked0>\n+\n+    return %sum : tensor<64x64xf32, #blocked0>\n+  }\n+}"}, {"filename": "unittest/Dialect/TritonGPU/CMakeLists.txt", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "file_content_changes": "@@ -1,6 +1,5 @@\n-\n add_triton_ut(\n \tNAME TestSwizzling\n \tSRCS SwizzleTest.cpp\n \tLIBS TritonGPUIR  ${dialect_libs} ${conversion_libs}\n-)\n\\ No newline at end of file\n+)"}]