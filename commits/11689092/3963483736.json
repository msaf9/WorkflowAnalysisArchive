[{"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -35,7 +35,9 @@ SmallVector<unsigned> getContigPerThread(Attribute layout);\n \n SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout);\n \n-SmallVector<unsigned> getShapePerCTA(const Attribute &layout);\n+SmallVector<unsigned>\n+getShapePerCTA(const Attribute &layout,\n+               ArrayRef<int64_t> tensorShape = ArrayRef<int64_t>());\n \n SmallVector<unsigned> getOrder(const Attribute &layout);\n "}, {"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 1, "deletions": 10, "changes": 11, "file_content_changes": "@@ -95,9 +95,6 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n           bool is_row = order[0] != 0;\n           bool is_vec4 = opIdx == 0 ? !is_row && (shape[order[0]] <= 16) :\n               is_row && (shape[order[0]] <= 16);\n-          // TODO[Superjomn]: Support the case when is_vec4=false later\n-          // Currently, we only support ld.v2, for the mma layout varies with different ld vector width.\n-          is_vec4 = true;\n           int pack_size = opIdx == 0 ? ((is_row || is_vec4) ? 1 : 2) :\n                                        ((is_row && !is_vec4) ? 2 : 1);\n           int rep = 2 * pack_size;\n@@ -135,8 +132,6 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n \n         // ---- not implemented ----\n         llvm_unreachable(\"unsupported swizzling for provided MMA version\");\n-\n-\n     }]>\n   ];\n \n@@ -403,6 +398,7 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n       bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n       bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n       // 4-bits to encode 4 booleans: [isARow, isBRow, isAVec4, isBVec4]\n+      // 3-bits to encode the MMA ID to make each unique\n       int versionMinor = (isARow * (1<<0)) |\\\n                          (isBRow * (1<<1)) |\\\n                          (isAVec4 * (1<<2)) |\\\n@@ -424,11 +420,6 @@ For example, the matrix L corresponding to blockTileSize=[32,16] is:\n     // Number of bits in versionMinor to hold the ID of the MMA encoding instance.\n     // Here 5 bits can hold 32 IDs in a single module.\n     static constexpr int numBitsToHoldMmaV1ID{5};\n-\n-    // Here is a temporary flag that indicates whether we need to update the warpsPerCTA for MMAv1, since the current backend cannot support the updated wpt.\n-    // The mmav1's wpt-related logic is separated into multiple files, so a global flag is added here for universal coordination.\n-    // TODO[Superjomn]: Remove this flag once the MMAv1 backend is ready.\n-    static constexpr bool _mmaV1UpdateWpt{false};\n   }];\n \n }"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "file_content_changes": "@@ -75,8 +75,10 @@ getScratchConfigForCvtLayout(triton::gpu::ConvertLayoutOp op, unsigned &inVec,\n   inVec = outOrd[0] == 0 ? 1 : inOrd[0] == 0 ? 1 : srcContigPerThread;\n   outVec = outOrd[0] == 0 ? 1 : dstContigPerThread;\n \n-  auto srcShapePerCTA = getShapePerCTA(srcLayout);\n-  auto dstShapePerCTA = getShapePerCTA(dstLayout);\n+  auto srcShape = srcTy.getShape();\n+  auto dstShape = dstTy.getShape();\n+  auto srcShapePerCTA = getShapePerCTA(srcLayout, srcShape);\n+  auto dstShapePerCTA = getShapePerCTA(dstLayout, dstShape);\n \n   unsigned rank = dstTy.getRank();\n   SmallVector<unsigned> paddedRepShape(rank);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM.cpp", "status": "modified", "additions": 152, "deletions": 54, "changes": 206, "file_content_changes": "@@ -120,27 +120,7 @@ struct ConvertLayoutOpConversion\n         mmaColIdx[0] = add(mmaThreadIdInGrpM2, colWarpOffset);\n         mmaColIdx[1] = add(mmaThreadIdInGrpM2P1, colWarpOffset);\n       } else if (mmaLayout.isVolta()) {\n-        multiDimWarpId[0] = urem(multiDimWarpId[0], idx_val(shape[0] / 16));\n-        multiDimWarpId[1] = urem(multiDimWarpId[1], idx_val(shape[1] / 16));\n-        Value laneIdDiv16 = udiv(laneId, _16);\n-        Value laneIdRem16 = urem(laneId, _16);\n-        Value laneIdRem2 = urem(laneId, _2);\n-        Value laneIdRem16Div8 = udiv(laneIdRem16, _8);\n-        Value laneIdRem16Div4 = udiv(laneIdRem16, _4);\n-        Value laneIdRem16Div4Rem2 = urem(laneIdRem16Div4, _2);\n-        Value laneIdRem4Div2 = udiv(urem(laneId, _4), _2);\n-        Value rowWarpOffset = mul(multiDimWarpId[0], _16);\n-        Value colWarpOffset = mul(multiDimWarpId[1], _16);\n-        mmaRowIdx[0] =\n-            add(add(mul(laneIdDiv16, _8), mul(laneIdRem16Div4Rem2, _4)),\n-                laneIdRem2);\n-        mmaRowIdx[0] = add(mmaRowIdx[0], rowWarpOffset);\n-        mmaRowIdx[1] = add(mmaRowIdx[0], _2);\n-        mmaColIdx[0] = add(mul(laneIdRem16Div8, _4), mul(laneIdRem4Div2, _2));\n-        mmaColIdx[0] = add(mmaColIdx[0], colWarpOffset);\n-        mmaColIdx[1] = add(mmaColIdx[0], _1);\n-        mmaColIdx[2] = add(mmaColIdx[0], _8);\n-        mmaColIdx[3] = add(mmaColIdx[0], idx_val(9));\n+        // Volta doesn't follow the pattern here.\"\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n@@ -155,26 +135,12 @@ struct ConvertLayoutOpConversion\n         multiDimOffset[1] = add(\n             multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n       } else if (mmaLayout.isVolta()) {\n-        // the order of elements in a thread:\n-        //   c0, c1, ...  c4, c5\n-        //   c2, c3, ...  c6, c7\n-        if (elemId < 2) {\n-          multiDimOffset[0] = mmaRowIdx[0];\n-          multiDimOffset[1] = mmaColIdx[elemId % 2];\n-        } else if (elemId >= 2 && elemId < 4) {\n-          multiDimOffset[0] = mmaRowIdx[1];\n-          multiDimOffset[1] = mmaColIdx[elemId % 2];\n-        } else if (elemId >= 4 && elemId < 6) {\n-          multiDimOffset[0] = mmaRowIdx[0];\n-          multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n-        } else if (elemId >= 6) {\n-          multiDimOffset[0] = mmaRowIdx[1];\n-          multiDimOffset[1] = mmaColIdx[elemId % 2 + 2];\n-        }\n-        multiDimOffset[0] = add(\n-            multiDimOffset[0], idx_val(multiDimCTAInRepId[0] * shapePerCTA[0]));\n-        multiDimOffset[1] = add(\n-            multiDimOffset[1], idx_val(multiDimCTAInRepId[1] * shapePerCTA[1]));\n+        auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n+            mmaLayout.decodeVoltaLayoutStates();\n+        auto coords = DotOpMmaV1ConversionHelper::getMNCoords(\n+            threadId, rewriter, mmaLayout.getWarpsPerCTA(), shape, isARow,\n+            isBRow, isAVec4, isBVec4);\n+        return DotOpMmaV1ConversionHelper::getCoord(elemId, coords);\n       } else {\n         llvm_unreachable(\"Unexpected MMALayout version\");\n       }\n@@ -200,7 +166,7 @@ struct ConvertLayoutOpConversion\n     auto sizePerThread = getSizePerThread(layout);\n     auto accumSizePerThread = product<unsigned>(sizePerThread);\n     SmallVector<unsigned> numCTAs(rank);\n-    auto shapePerCTA = getShapePerCTA(layout);\n+    auto shapePerCTA = getShapePerCTA(layout, type.getShape());\n     auto order = getOrder(layout);\n     for (unsigned d = 0; d < rank; ++d) {\n       numCTAs[d] = ceil<unsigned>(type.getShape()[d], shapePerCTA[d]);\n@@ -269,6 +235,109 @@ struct ConvertLayoutOpConversion\n     }\n   }\n \n+  // The MMAV1's result is quite different from the exising \"Replica\" structure,\n+  // add a new simple but clear implementation for it to avoid modificating the\n+  // logic of the exising one.\n+  void processReplicaForMMAV1(Location loc, ConversionPatternRewriter &rewriter,\n+                              bool stNotRd, RankedTensorType type,\n+                              ArrayRef<unsigned> multiDimRepId, unsigned vec,\n+                              ArrayRef<unsigned> paddedRepShape,\n+                              ArrayRef<unsigned> outOrd,\n+                              SmallVector<Value> &vals, Value smemBase,\n+                              ArrayRef<int64_t> shape,\n+                              bool isDestMma = false) const {\n+    unsigned accumNumCTAsEachRep = 1;\n+    auto layout = type.getEncoding();\n+    MmaEncodingAttr mma = layout.dyn_cast<MmaEncodingAttr>();\n+    auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>();\n+    if (sliceLayout)\n+      mma = sliceLayout.getParent().cast<MmaEncodingAttr>();\n+\n+    auto order = getOrder(layout);\n+    auto rank = type.getRank();\n+    int accumSizePerThread = vals.size();\n+\n+    SmallVector<unsigned> numCTAs(rank, 1);\n+    SmallVector<unsigned> numCTAsEachRep(rank, 1);\n+    SmallVector<unsigned> shapePerCTA = getShapePerCTA(layout, shape);\n+    auto elemTy = type.getElementType();\n+\n+    int ctaId = 0;\n+\n+    auto multiDimCTAInRepId =\n+        getMultiDimIndex<unsigned>(ctaId, numCTAsEachRep, order);\n+    SmallVector<unsigned> multiDimCTAId(rank);\n+    for (const auto &it : llvm::enumerate(multiDimCTAInRepId)) {\n+      auto d = it.index();\n+      multiDimCTAId[d] = multiDimRepId[d] * numCTAsEachRep[d] + it.value();\n+    }\n+\n+    std::vector<std::pair<SmallVector<Value>, Value>> coord2valT(\n+        accumSizePerThread);\n+    bool needTrans = outOrd[0] != 0;\n+    if (sliceLayout || isDestMma)\n+      needTrans = false;\n+\n+    vec = needTrans ? 2 : 1;\n+    {\n+      // We need to transpose the coordinates and values here to enable vec=2\n+      // when store to smem.\n+      std::vector<std::pair<SmallVector<Value>, Value>> coord2val(\n+          accumSizePerThread);\n+      for (unsigned elemId = 0; elemId < accumSizePerThread; ++elemId) {\n+        // TODO[Superjomn]: Move the coordinate computation out of loop, it is\n+        // duplicate in Volta.\n+        SmallVector<Value> multiDimOffset =\n+            getMultiDimOffset(layout, loc, rewriter, elemId, type.getShape(),\n+                              multiDimCTAInRepId, shapePerCTA);\n+        coord2val[elemId] = std::make_pair(multiDimOffset, vals[elemId]);\n+      }\n+\n+      if (needTrans) {\n+        auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n+            mma.decodeVoltaLayoutStates();\n+        DotOpMmaV1ConversionHelper helper(mma);\n+        // do transpose\n+        int numM = helper.getElemsM(mma.getWarpsPerCTA()[0], shape[0], isARow,\n+                                    isAVec4);\n+        int numN = accumSizePerThread / numM;\n+\n+        for (int r = 0; r < numM; r++) {\n+          for (int c = 0; c < numN; c++) {\n+            coord2valT[r * numN + c] = std::move(coord2val[c * numM + r]);\n+          }\n+        }\n+      } else {\n+        coord2valT = std::move(coord2val);\n+      }\n+    }\n+\n+    // Now the coord2valT has the transposed and contiguous elements(with\n+    // vec=2), the original vals is not needed.\n+    for (unsigned elemId = 0; elemId < accumSizePerThread; elemId += vec) {\n+      auto coord = coord2valT[elemId].first;\n+      Value offset = linearize(rewriter, loc, coord, paddedRepShape, outOrd);\n+      auto elemPtrTy = ptr_ty(elemTy, 3);\n+      Value ptr = gep(elemPtrTy, smemBase, offset);\n+      auto vecTy = vec_ty(elemTy, vec);\n+      ptr = bitcast(ptr, ptr_ty(vecTy, 3));\n+      if (stNotRd) {\n+        Value valVec = undef(vecTy);\n+        for (unsigned v = 0; v < vec; ++v) {\n+          auto currVal = coord2valT[elemId + v].second;\n+          valVec = insert_element(vecTy, valVec, currVal, idx_val(v));\n+        }\n+        store(valVec, ptr);\n+      } else {\n+        Value valVec = load(ptr);\n+        for (unsigned v = 0; v < vec; ++v) {\n+          Value currVal = extract_element(elemTy, valVec, idx_val(v));\n+          vals[elemId + v] = currVal;\n+        }\n+      }\n+    }\n+  }\n+\n   // blocked/mma -> blocked/mma.\n   // Data padding in shared memory to avoid bank conflict.\n   LogicalResult\n@@ -293,8 +362,26 @@ struct ConvertLayoutOpConversion\n     SmallVector<unsigned> outNumCTAsEachRep(rank);\n     SmallVector<unsigned> inNumCTAs(rank);\n     SmallVector<unsigned> outNumCTAs(rank);\n-    auto srcShapePerCTA = getShapePerCTA(srcLayout);\n-    auto dstShapePerCTA = getShapePerCTA(dstLayout);\n+    auto srcShapePerCTA = getShapePerCTA(srcLayout, srcTy.getShape());\n+    auto dstShapePerCTA = getShapePerCTA(dstLayout, shape);\n+\n+    // For Volta, all the coords for a CTA are calculated.\n+    bool isSrcMmaV1{}, isDstMmaV1{};\n+    if (auto mmaLayout = srcLayout.dyn_cast<MmaEncodingAttr>()) {\n+      isSrcMmaV1 = mmaLayout.isVolta();\n+    }\n+    if (auto sliceLayout = srcLayout.dyn_cast<SliceEncodingAttr>()) {\n+      isSrcMmaV1 = sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n+                   sliceLayout.getParent().cast<MmaEncodingAttr>().isVolta();\n+    }\n+    if (auto mmaLayout = dstLayout.dyn_cast<MmaEncodingAttr>()) {\n+      isDstMmaV1 = mmaLayout.isVolta();\n+    }\n+    if (auto sliceLayout = dstLayout.dyn_cast<SliceEncodingAttr>()) {\n+      isDstMmaV1 = sliceLayout.getParent().isa<MmaEncodingAttr>() &&\n+                   sliceLayout.getParent().cast<MmaEncodingAttr>().isVolta();\n+    }\n+\n     for (unsigned d = 0; d < rank; ++d) {\n       unsigned inPerCTA = std::min<unsigned>(shape[d], srcShapePerCTA[d]);\n       unsigned outPerCTA = std::min<unsigned>(shape[d], dstShapePerCTA[d]);\n@@ -326,20 +413,31 @@ struct ConvertLayoutOpConversion\n       if (srcLayout.isa<BlockedEncodingAttr>() ||\n           srcLayout.isa<SliceEncodingAttr>() ||\n           srcLayout.isa<MmaEncodingAttr>()) {\n-        processReplica(loc, rewriter, /*stNotRd*/ true, srcTy, inNumCTAsEachRep,\n-                       multiDimRepId, inVec, paddedRepShape, outOrd, vals,\n-                       smemBase);\n+        if (isSrcMmaV1)\n+          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ true, srcTy,\n+                                 multiDimRepId, inVec, paddedRepShape, outOrd,\n+                                 vals, smemBase, shape);\n+        else\n+          processReplica(loc, rewriter, /*stNotRd*/ true, srcTy,\n+                         inNumCTAsEachRep, multiDimRepId, inVec, paddedRepShape,\n+                         outOrd, vals, smemBase);\n       } else {\n         assert(0 && \"ConvertLayout with input layout not implemented\");\n         return failure();\n       }\n+\n       barrier();\n       if (dstLayout.isa<BlockedEncodingAttr>() ||\n           dstLayout.isa<SliceEncodingAttr>() ||\n           dstLayout.isa<MmaEncodingAttr>()) {\n-        processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n-                       outNumCTAsEachRep, multiDimRepId, outVec, paddedRepShape,\n-                       outOrd, outVals, smemBase);\n+        if (isDstMmaV1)\n+          processReplicaForMMAV1(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                                 multiDimRepId, outVec, paddedRepShape, outOrd,\n+                                 outVals, smemBase, shape, /*isDestMma=*/true);\n+        else\n+          processReplica(loc, rewriter, /*stNotRd*/ false, dstTy,\n+                         outNumCTAsEachRep, multiDimRepId, outVec,\n+                         paddedRepShape, outOrd, outVals, smemBase);\n       } else {\n         assert(0 && \"ConvertLayout with output layout not implemented\");\n         return failure();\n@@ -540,13 +638,13 @@ struct ConvertLayoutOpConversion\n       if (dotOperandLayout.getOpIdx() == 0) { // operand $a\n         // TODO[Superjomn]: transA is not available here.\n         bool transA = false;\n-        res = helper.loadA(src, transA, smemObj, getThreadId(rewriter, loc),\n-                           loc, rewriter);\n+        res = helper.loadA(src, smemObj, getThreadId(rewriter, loc), loc,\n+                           rewriter);\n       } else if (dotOperandLayout.getOpIdx() == 1) { // operand $b\n         // TODO[Superjomn]: transB is not available here.\n         bool transB = false;\n-        res = helper.loadB(src, transB, smemObj, getThreadId(rewriter, loc),\n-                           loc, rewriter);\n+        res = helper.loadB(src, smemObj, getThreadId(rewriter, loc), loc,\n+                           rewriter);\n       }\n     } else {\n       assert(false && \"Unsupported mma layout found\");"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 214, "deletions": 82, "changes": 296, "file_content_changes": "@@ -47,40 +47,47 @@ struct DotOpMmaV1ConversionHelper {\n       : mmaLayout(mmaLayout), wpt(mmaLayout.getWarpsPerCTA()) {}\n \n   // Help to share some variables across multiple functions for A.\n+  // TODO[Superjomn]: refactor and restrict this to only use in DotOp\n+  // conversion.\n   struct AParam {\n     SmallVector<int> rep;\n     SmallVector<int> spw;\n+    bool isAVec4{};\n+    int vec{}; // This could only used in DotOp, not in\n+               // loadA/loadB/TypeConverter\n \n-    // TODO[Superjomn]: Support the case when isAVec4=false later\n-    // Currently, we only support ld.v2, for the mma layout varies with\n-    // different ld vector width.\n-    // bool isAVec4 = !isARow && shapeTransed[orderTransed[0]] <= 16;\n-    const bool isAVec4{true};\n+    AParam(bool isARow, bool isAVec4) : isAVec4(isAVec4) { build(isARow); }\n \n-    explicit AParam(bool isARow) {\n+  private:\n+    void build(bool isARow) {\n       int packSize0 = (isARow || isAVec4) ? 1 : 2;\n       int repM = 2 * packSize0;\n       int repK = 1;\n       int spwM = fpw[0] * 4 * repM;\n       rep.assign({repM, 0, repK});\n       spw.assign({spwM, 0, 1});\n+      vec = 2 * rep[0];\n     }\n   };\n \n   // Help to share some variables across multiple functions for A.\n+  // TODO[Superjomn]: refactor and restrict this to only use in DotOp\n+  // conversion.\n   struct BParam {\n     SmallVector<int> rep;\n     SmallVector<int> spw;\n-    // TODO[Superjomn]: Support the case when isBVec4=false later\n-    // Currently, we only support ld.v2, for the mma layout varies with\n-    // different ld vector width.\n-    // bool isBVec4 = isBRow && shapeTransed[orderTransed[0]] <= 16;\n-    const bool isBVec4{true};\n+    bool isBVec4{};\n+    int vec{}; // This could only used in DotOp, not in\n+               // loadA/loadB/TypeConverter\n \n-    explicit BParam(bool isBRow) {\n+    BParam(bool isBRow, bool isBVec4) : isBVec4(isBVec4) { build(isBRow); }\n+\n+  private:\n+    void build(bool isBRow) {\n       int packSize1 = (isBRow && !isBVec4) ? 2 : 1;\n       rep.assign({0, 2 * packSize1, 1});\n       spw.assign({0, fpw[1] * 4 * rep[1], 1});\n+      vec = 2 * rep[1];\n     }\n   };\n \n@@ -93,91 +100,107 @@ struct DotOpMmaV1ConversionHelper {\n \n   static ArrayRef<unsigned> getMmaInstrShape() { return instrShape; }\n \n-  static Type getMatType(TensorType operand) {\n-    auto *ctx = operand.getContext();\n-    Type fp16Ty = type::f16Ty(ctx);\n-    Type vecTy = vec_ty(fp16Ty, 2);\n-    return struct_ty(SmallVector<Type>{vecTy});\n-  }\n-\n   static Type getMmaRetType(TensorType operand) {\n     auto *ctx = operand.getContext();\n     Type fp32Ty = type::f32Ty(ctx);\n     // f16*f16+f32->f32\n     return struct_ty(SmallVector<Type>{8, fp32Ty});\n   }\n \n+  static Type getMatType(TensorType operand) {\n+    auto *ctx = operand.getContext();\n+    Type fp16Ty = type::f16Ty(ctx);\n+    Type vecTy = vec_ty(fp16Ty, 2);\n+    return struct_ty(SmallVector<Type>{vecTy});\n+  }\n+\n   // Get the number of fp16x2 elements for $a.\n-  // \\param shapeTransed: A's shape or reordered shape if transpose needed.\n-  // \\param orderTransed: the order or reordered order if transpose needed.\n-  unsigned getNumM(ArrayRef<int64_t> shapeTransed, bool isARow) const {\n-    AParam param(isARow);\n+  unsigned getNumM(int M, bool isARow, bool isAVec4) const {\n+    AParam param(isARow, isAVec4);\n \n-    unsigned numM = param.rep[0] * shapeTransed[0] / (param.spw[0] * wpt[0]);\n+    unsigned numM = param.rep[0] * M / (param.spw[0] * wpt[0]);\n     return numM;\n   }\n \n   // Get the number of fp16x2 elements for $b.\n-  // \\param shapeTransed: B' shape or reordered shape if transpose needed.\n-  // \\param orderTransed: the order or reordered order if transpose needed.\n-  unsigned getNumN(ArrayRef<int64_t> shapeTransed, bool isBRow) const {\n-    BParam param(isBRow);\n+  unsigned getNumN(int N, bool isBRow, bool isBVec4) const {\n+    BParam param(isBRow, isBVec4);\n \n-    unsigned numN = param.rep[1] * shapeTransed[1] / (param.spw[1] * wpt[1]);\n+    unsigned numN = param.rep[1] * N / (param.spw[1] * wpt[1]);\n     return numN;\n   }\n \n-  int numElemsPerThreadA(ArrayRef<int64_t> shapeTransed,\n-                         ArrayRef<unsigned> orderTransed) const {\n-    int numM = getNumM(shapeTransed, orderTransed[0] == 1);\n-    int NK = shapeTransed[1];\n+  int numElemsPerThreadA(ArrayRef<int64_t> shape, bool isARow, bool isAVec4,\n+                         int vec) const {\n+    int numM = getNumM(shape[0], isARow, isAVec4);\n+    int NK = shape[1];\n+    // Here we mimic the logic in loadA, the result cannot be calculated\n+    // directly.\n+    llvm::DenseSet<std::pair<int, int>> visited;\n+    auto ld = [&](int m, int k) {\n+      visited.insert({m, k});\n+      if (vec > 4) {\n+        if (isARow)\n+          visited.insert({m, k + 4});\n+        else\n+          visited.insert({m + 1, k});\n+      }\n+    };\n+\n+    for (unsigned k = 0; k < NK; k += 4)\n+      for (unsigned m = 0; m < numM / 2; ++m)\n+        if (!visited.count({m, k}))\n+          ld(m, k);\n \n-    // NOTE: We couldn't get the vec from the shared layout.\n-    // int vecA = sharedLayout.getVec();\n-    // TODO[Superjomn]: Consider the case when vecA > 4\n-    bool vecGt4 = false;\n-    int elemsPerLd = vecGt4 ? 4 : 2;\n-    return (numM / 2) * (NK / 4) * elemsPerLd;\n+    return visited.size() * 2;\n   }\n \n-  int numElemsPerThreadB(ArrayRef<int64_t> shapeTransed,\n-                         ArrayRef<unsigned> orderTransed) const {\n-    unsigned numN = getNumN(shapeTransed, orderTransed[0] == 1);\n-    int NK = shapeTransed[0];\n-    // NOTE: We couldn't get the vec from the shared layout.\n-    // int vecB = sharedLayout.getVec();\n-    // TODO[Superjomn]: Consider the case when vecA > 4\n-    bool vecGt4 = false;\n-    int elemsPerLd = vecGt4 ? 4 : 2;\n-    return (numN / 2) * (NK / 4) * elemsPerLd;\n+  int numElemsPerThreadB(ArrayRef<int64_t> shape, bool isBRow, bool isBVec4,\n+                         int vec) const {\n+    unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n+    int NK = shape[0];\n+    // Here we mimic the logic in loadA, the result cannot be calculated\n+    // directly.\n+    llvm::DenseSet<std::pair<int, int>> visited;\n+    int elemsPerLd = vec > 4 ? 4 : 2;\n+    auto ld = [&](int n, int k) {\n+      visited.insert({n, k});\n+      if (vec > 4) {\n+        if (isBRow)\n+          visited.insert({n + 1, k});\n+        else\n+          visited.insert({n, k + 4});\n+      }\n+    };\n+\n+    for (unsigned k = 0; k < NK; k += 4)\n+      for (unsigned n = 0; n < numN / 2; ++n) {\n+        if (!visited.count({n, k}))\n+          ld(n, k);\n+      }\n+\n+    return visited.size() * 2;\n   }\n \n   // Loading $a from smem to registers, returns a LLVM::Struct.\n-  Value loadA(Value tensor, bool transA, const SharedMemoryObject &smemObj,\n-              Value thread, Location loc,\n-              ConversionPatternRewriter &rewriter) const {\n+  Value loadA(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const {\n     auto *ctx = rewriter.getContext();\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n-    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                               tensorTy.getShape().end());\n-    SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n-                                sharedLayout.getOrder().end());\n+    auto shape = tensorTy.getShape();\n+    auto order = sharedLayout.getOrder();\n \n     Value cSwizzleOffset = smemObj.getCSwizzleOffset(order[0]);\n     Value smemBase = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n \n     bool isARow = order[0] != 0;\n-    AParam param(isARow);\n+    auto [isARow_, _0, isAVec4, _1, _2] = mmaLayout.decodeVoltaLayoutStates();\n \n-    auto [offsetAM, offsetAK, _0, _1] = computeOffsets(\n-        thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n+    AParam param(isARow_, isAVec4);\n \n-    if (transA) {\n-      std::swap(shape[0], shape[1]);\n-      std::swap(offsetAM, offsetAK);\n-      std::swap(order[0], order[1]);\n-    }\n+    auto [offsetAM, offsetAK, _3, _4] = computeOffsets(\n+        thread, isARow, false, fpw, param.spw, param.rep, rewriter, loc);\n \n     int vecA = sharedLayout.getVec();\n \n@@ -254,10 +277,11 @@ struct DotOpMmaV1ConversionHelper {\n       }\n     };\n \n-    unsigned numM = getNumM(shape, order[0] == 1);\n+    unsigned numM = getNumM(shape[0], isARow, isAVec4);\n     for (unsigned k = 0; k < NK; k += 4)\n       for (unsigned m = 0; m < numM / 2; ++m)\n-        loadA(m, k);\n+        if (!has.count({m, k}))\n+          loadA(m, k);\n \n     SmallVector<Value> elems;\n     elems.reserve(has.size() * 2);\n@@ -272,24 +296,25 @@ struct DotOpMmaV1ConversionHelper {\n   }\n \n   // Loading $b from smem to registers, returns a LLVM::Struct.\n-  Value loadB(Value tensor, bool transB, const SharedMemoryObject &smemObj,\n-              Value thread, Location loc,\n-              ConversionPatternRewriter &rewriter) const {\n+  Value loadB(Value tensor, const SharedMemoryObject &smemObj, Value thread,\n+              Location loc, ConversionPatternRewriter &rewriter) const {\n     // smem\n     auto strides = smemObj.strides;\n \n     auto *ctx = rewriter.getContext();\n     auto tensorTy = tensor.getType().cast<RankedTensorType>();\n     auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n \n-    SmallVector<int64_t> shape(tensorTy.getShape().begin(),\n-                               tensorTy.getShape().end());\n-    SmallVector<unsigned> order(sharedLayout.getOrder().begin(),\n-                                sharedLayout.getOrder().end());\n+    auto shape = tensorTy.getShape();\n+    auto order = sharedLayout.getOrder();\n \n     Value smem = smemObj.getBaseBeforeSwizzle(order[0], loc, rewriter);\n-    bool isBRow = order[0] != 0;\n-    BParam param(isBRow);\n+    bool isBRow = order[0] != 0; // is row-major in shared memory layout\n+    // isBRow_ indicates whether B is row-major in DotOperand layout\n+    auto [_0, isBRow_, _1, isBVec4, _2] = mmaLayout.decodeVoltaLayoutStates();\n+    assert(isBRow == isBRow_ && \"B need smem isRow\");\n+\n+    BParam param(isBRow_, isBVec4);\n \n     int vecB = sharedLayout.getVec();\n     Value strideBN = isBRow ? i32_val(1) : strides[1];\n@@ -299,13 +324,8 @@ struct DotOpMmaV1ConversionHelper {\n     int strideRepN = wpt[1] * fpw[1] * 8;\n     int strideRepK = 1;\n \n-    auto [_0, _1, offsetBN, offsetBK] = computeOffsets(\n+    auto [_3, _4, offsetBN, offsetBK] = computeOffsets(\n         thread, false, isBRow, fpw, param.spw, param.rep, rewriter, loc);\n-    if (transB) {\n-      std::swap(order[0], order[1]);\n-      std::swap(shape[0], shape[1]);\n-      std::swap(offsetBK, offsetBN);\n-    }\n \n     // swizzling\n     int perPhaseB = sharedLayout.getPerPhase();\n@@ -371,7 +391,7 @@ struct DotOpMmaV1ConversionHelper {\n       }\n     };\n \n-    unsigned numN = getNumN(shape, order[0] == 1);\n+    unsigned numN = getNumN(shape[1], isBRow, isBVec4);\n     for (unsigned k = 0; k < NK; k += 4)\n       for (unsigned n = 0; n < numN / 2; ++n) {\n         if (!hbs.count({n, k}))\n@@ -383,6 +403,7 @@ struct DotOpMmaV1ConversionHelper {\n       elems.push_back(item.second.first);\n       elems.push_back(item.second.second);\n     }\n+\n     Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n     Value res = getStructFromElements(loc, elems, rewriter, resTy);\n     return res;\n@@ -475,6 +496,117 @@ struct DotOpMmaV1ConversionHelper {\n     return rcds;\n   }\n \n+  // Get the number of elements of this thread in M axis. The N axis could be\n+  // further deduced with the accSize / elemsM. \\param wpt: the wpt in M axis\n+  // \\param M: the shape in M axis\n+  int getElemsM(int wpt, int M, bool isARow, bool isAVec4) {\n+    DotOpMmaV1ConversionHelper::AParam param(isARow, isAVec4);\n+    int shapePerCTAM = param.spw[0] * wpt;\n+    return M / shapePerCTAM * param.rep[0];\n+  }\n+\n+  using CoordTy = SmallVector<Value, 2>;\n+  // Get the coordinates(m,n) of the elements emit by a thread in accumulator.\n+  static SmallVector<CoordTy>\n+  getMNCoords(Value thread, ConversionPatternRewriter &rewriter,\n+              ArrayRef<unsigned> wpt, ArrayRef<int64_t> shape, bool isARow,\n+              bool isBRow, bool isAVec4, bool isBVec4) {\n+\n+    auto *ctx = thread.getContext();\n+    auto loc = UnknownLoc::get(ctx);\n+    Value _1 = i32_val(1);\n+    Value _2 = i32_val(2);\n+    Value _4 = i32_val(4);\n+    Value _16 = i32_val(16);\n+    Value _32 = i32_val(32);\n+    Value _fpw0 = i32_val(fpw[0]);\n+    Value _fpw1 = i32_val(fpw[1]);\n+\n+    DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n+    DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n+\n+    SmallVector<int, 2> rep({aParam.rep[0], bParam.rep[1]});\n+    SmallVector<int, 2> spw({aParam.spw[0], bParam.spw[1]});\n+    SmallVector<unsigned, 2> shapePerCTA({spw[0] * wpt[0], spw[1] * wpt[1]});\n+\n+    Value lane = urem(thread, _32);\n+    Value warp = udiv(thread, _32);\n+\n+    Value warp0 = urem(warp, i32_val(wpt[0]));\n+    Value warp12 = udiv(warp, i32_val(wpt[0]));\n+    Value warp1 = urem(warp12, i32_val(wpt[1]));\n+\n+    // warp offset\n+    Value offWarpM = mul(warp0, i32_val(spw[0]));\n+    Value offWarpN = mul(warp1, i32_val(spw[1]));\n+    // quad offset\n+    Value offQuadM = mul(udiv(and_(lane, _16), _4), _fpw0);\n+    Value offQuadN = mul(udiv(and_(lane, _16), _4), _fpw1);\n+    // pair offset\n+    Value offPairM = udiv(urem(lane, _16), _4);\n+    offPairM = urem(offPairM, _fpw0);\n+    offPairM = mul(offPairM, _4);\n+    Value offPairN = udiv(urem(lane, _16), _4);\n+    offPairN = udiv(offPairN, _fpw0);\n+    offPairN = urem(offPairN, _fpw1);\n+    offPairN = mul(offPairN, _4);\n+\n+    // sclare\n+    offPairM = mul(offPairM, i32_val(rep[0] / 2));\n+    offQuadM = mul(offQuadM, i32_val(rep[0] / 2));\n+    offPairN = mul(offPairN, i32_val(rep[1] / 2));\n+    offQuadN = mul(offQuadN, i32_val(rep[1] / 2));\n+\n+    // quad pair offset\n+    Value offLaneM = add(offPairM, offQuadM);\n+    Value offLaneN = add(offPairN, offQuadN);\n+    // a, b offset\n+    Value offsetAM = add(offWarpM, offLaneM);\n+    Value offsetBN = add(offWarpN, offLaneN);\n+    // m indices\n+    Value offsetCM = add(and_(lane, _1), offsetAM);\n+    SmallVector<Value> idxM;\n+    for (unsigned m = 0; m < shape[0]; m += shapePerCTA[0])\n+      for (unsigned mm = 0; mm < rep[0]; ++mm)\n+        idxM.push_back(add(offsetCM, i32_val(m + mm * 2)));\n+\n+    // n indices\n+    Value offsetCN = add((and_(lane, _2)), (add(offWarpN, offPairN)));\n+    SmallVector<Value> idxN;\n+    for (int n = 0; n < shape[1]; n += shapePerCTA[1]) {\n+      for (int nn = 0; nn < rep[1]; ++nn) {\n+        idxN.push_back(add(offsetCN, i32_val(n + nn / 2 * 4 +\n+                                             (nn % 2) * 2 * fpw[1] * rep[1])));\n+        idxN.push_back(\n+            add(offsetCN,\n+                i32_val(n + nn / 2 * 4 + (nn % 2) * 2 * fpw[1] * rep[1] + 1)));\n+      }\n+    }\n+\n+    SmallVector<SmallVector<Value>> axes({idxM, idxN});\n+\n+    // product the axis M and axis N to get coords, ported from\n+    // generator::init_idx method from triton2.0\n+\n+    // TODO[Superjomn]: check the order.\n+    SmallVector<CoordTy> coords;\n+    for (Value x1 : axes[1]) {   // N\n+      for (Value x0 : axes[0]) { // M\n+        SmallVector<Value, 2> idx(2);\n+        idx[0] = x0; // M\n+        idx[1] = x1; // N\n+        coords.push_back(std::move(idx));\n+      }\n+    }\n+\n+    return coords; // {M,N} in row-major\n+  }\n+\n+  // \\param elemId the offset of the element in a thread\n+  static CoordTy getCoord(int elemId, ArrayRef<CoordTy> coords) {\n+    return coords[elemId];\n+  }\n+\n private:\n   static constexpr unsigned instrShape[] = {16, 16, 4};\n   static constexpr unsigned mmaOrder[] = {0, 1};"}, {"filename": "lib/Conversion/TritonGPUToLLVM/DotOpToLLVM.cpp", "status": "modified", "additions": 11, "deletions": 17, "changes": 28, "file_content_changes": "@@ -120,11 +120,15 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n \n     bool isARow = ALayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n     bool isBRow = BLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto [isARow_, isBRow_, isAVec4_, isBVec4_, mmaId] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+    assert(isARow == isARow_);\n+    assert(isBRow == isBRow_);\n \n     DotOpMmaV1ConversionHelper helper(mmaLayout);\n \n-    unsigned numM = helper.getNumM(AShape, isARow);\n-    unsigned numN = helper.getNumN(BShape, isBRow);\n+    unsigned numM = helper.getNumM(AShape[0], isARow, isAVec4_);\n+    unsigned numN = helper.getNumN(BShape[1], isBRow, isBVec4_);\n     unsigned NK = AShape[1];\n \n     auto has = helper.extractLoadedOperand(adaptor.a(), NK, rewriter);\n@@ -156,20 +160,6 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       return idx;\n     };\n \n-    { // convert the acc's value from accumuator-external order to\n-      // accumulator-internal order.\n-      SmallVector<Value> accInit(acc.size());\n-\n-      for (unsigned m = 0; m < numM / 2; ++m)\n-        for (unsigned n = 0; n < numN / 2; ++n) {\n-          auto idx = getIdx(m, n);\n-          for (unsigned i = 0; i < 8; ++i)\n-            accInit[idx[i]] = acc[(m * numN / 2 + n) * 8 + i];\n-        }\n-\n-      acc = accInit;\n-    }\n-\n     auto callMMA = [&](unsigned m, unsigned n, unsigned k) {\n       auto ha = has.at({m, k});\n       auto hb = hbs.at({n, k});\n@@ -206,7 +196,6 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n       for (auto i = 0; i < 8; i++) {\n         Value elem = extract_val(f32_ty, res, i32_arr_attr(i));\n         acc[idx[i]] = elem;\n-        resVals[(m * numN / 2 + n) * 8 + i] = elem;\n       }\n     };\n \n@@ -216,6 +205,11 @@ struct DotOpConversion : public ConvertTritonGPUOpToLLVMPattern<triton::DotOp> {\n           callMMA(m, n, k);\n         }\n \n+    // res holds the same layout of acc\n+    for (size_t i = 0; i < acc.size(); ++i) {\n+      resVals[i] = acc[i];\n+    }\n+\n     Type structTy = LLVM::LLVMStructType::getLiteral(\n         ctx, SmallVector<Type>(resSize, type::f32Ty(ctx)));\n     Value res = getStructFromElements(loc, resVals, rewriter, structTy);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVM.cpp", "status": "modified", "additions": 57, "deletions": 1, "changes": 58, "file_content_changes": "@@ -69,10 +69,42 @@ struct BroadcastOpConversion\n     auto srcOffsets = emitOffsetForLayout(srcLayout, srcShape);\n     auto resultOffsets = emitOffsetForLayout(resultLayout, resultShape);\n     SmallVector<Value> srcVals = getElementsFromStruct(loc, src, rewriter);\n+    if (auto srcMma = srcLayout.dyn_cast<MmaEncodingAttr>()) {\n+\n+      // NOTE: This is just an naive fix, but for MMA layout, and 2-d fix should\n+      // be all right.\n+      // TODO[Superjomn]: Replace this with a generic implementation.\n+      if (srcMma.isVolta()) {\n+        assert(srcTy.getElementType().isF16() &&\n+               \"Unexpected data type on Volta\");\n+        int numElemsPerThread = srcMma.getElemsPerThread(resultTy.getShape());\n+        int srcUniqElems = srcVals.size() / 2;\n+        int dup = numElemsPerThread / srcUniqElems;\n+        SmallVector<Value> retVals;\n+        if (srcShape[0] == 1) { // add-cols\n+          for (int i = 0; i < srcUniqElems; ++i)\n+            for (int k = 0; k < dup; ++k)\n+              retVals.push_back(srcVals[i * 2]);\n+\n+        } else { // add-rows\n+          for (int k = 0; k < dup; ++k)\n+            for (int i = 0; i < srcUniqElems; ++i)\n+              retVals.push_back(srcVals[i]);\n+        }\n+\n+        auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n+        Value ret = getStructFromElements(loc, retVals, rewriter, llvmStructTy);\n+\n+        rewriter.replaceOp(op, {ret});\n+        return success();\n+      }\n+    }\n+\n     DenseMap<SmallVector<unsigned>, Value, SmallVectorKeyInfo> srcValues;\n     for (size_t i = 0; i < srcOffsets.size(); i++) {\n       srcValues[srcOffsets[i]] = srcVals[i];\n     }\n+\n     SmallVector<Value> resultVals;\n     for (size_t i = 0; i < resultOffsets.size(); i++) {\n       auto offset = resultOffsets[i];\n@@ -81,6 +113,7 @@ struct BroadcastOpConversion\n           offset[j] = 0;\n       resultVals.push_back(srcValues.lookup(offset));\n     }\n+\n     auto llvmStructTy = getTypeConverter()->convertType(resultTy);\n     Value resultStruct =\n         getStructFromElements(loc, resultVals, rewriter, llvmStructTy);\n@@ -523,6 +556,29 @@ struct AsyncCommitGroupOpConversion\n   }\n };\n \n+namespace mlir {\n+namespace LLVM {\n+\n+void vprintf(StringRef msg, ValueRange args,\n+             ConversionPatternRewriter &rewriter) {\n+  PrintfOpConversion::llPrintf(msg, args, rewriter);\n+}\n+\n+void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n+                   std::string elem_repr, ConversionPatternRewriter &builder) {\n+  std::string fmt = info + \" t-%d \";\n+  std::vector<Value> new_arr({thread});\n+  for (int i = 0; i < arr.size(); ++i) {\n+    fmt += elem_repr + ((i == arr.size() - 1) ? \"\" : \", \");\n+    new_arr.push_back(arr[i]);\n+  }\n+\n+  vprintf(fmt, new_arr, builder);\n+}\n+\n+} // namespace LLVM\n+} // namespace mlir\n+\n void populateTritonGPUToLLVMPatterns(\n     mlir::LLVMTypeConverter &typeConverter, RewritePatternSet &patterns,\n     int numWarps, AxisInfoAnalysis &axisInfoAnalysis,\n@@ -543,4 +599,4 @@ void populateTritonGPUToLLVMPatterns(\n   patterns.add<MakeRangeOpConversion>(typeConverter, indexCacheInfo, benefit);\n   patterns.add<ReturnOpConversion>(typeConverter, benefit);\n   patterns.add<PrintfOpConversion>(typeConverter, benefit);\n-}\n\\ No newline at end of file\n+}"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMBase.h", "status": "modified", "additions": 23, "deletions": 2, "changes": 25, "file_content_changes": "@@ -18,6 +18,20 @@ using ::mlir::LLVM::SharedMemoryObject;\n using ::mlir::triton::gpu::BlockedEncodingAttr;\n using ::mlir::triton::gpu::MmaEncodingAttr;\n using ::mlir::triton::gpu::SliceEncodingAttr;\n+\n+namespace mlir {\n+namespace LLVM {\n+\n+// Helper function for using printf in LLVM conversion.\n+void vprintf(StringRef msg, ValueRange args,\n+             ConversionPatternRewriter &rewriter);\n+\n+void vprintf_array(Value thread, ArrayRef<Value> arr, std::string info,\n+                   std::string elem_repr, ConversionPatternRewriter &builder);\n+\n+} // namespace LLVM\n+} // namespace mlir\n+\n // FuncOpConversion/FuncOpConversionBase is borrowed from\n // https://github.com/llvm/llvm-project/blob/fae656b2dd80246c3c6f01e9c77c49560368752c/mlir/lib/Conversion/FuncToLLVM/FuncToLLVM.cpp#L276\n // since it is not exposed on header files in mlir v14\n@@ -199,6 +213,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         ValueRange{rewriter.create<::mlir::gpu::ThreadIdOp>(\n             loc, rewriter.getIndexType(), ::mlir::gpu::Dimension::x)});\n     Value threadId = cast.getResult(0);\n+\n     return threadId;\n   }\n \n@@ -688,8 +703,10 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n                            ArrayRef<int64_t> shape) const {\n     SmallVector<SmallVector<unsigned>> ret;\n \n-    for (unsigned i = 0; i < shape[0]; i += getShapePerCTA(mmaLayout)[0]) {\n-      for (unsigned j = 0; j < shape[1]; j += getShapePerCTA(mmaLayout)[1]) {\n+    for (unsigned i = 0; i < shape[0];\n+         i += getShapePerCTA(mmaLayout, shape)[0]) {\n+      for (unsigned j = 0; j < shape[1];\n+           j += getShapePerCTA(mmaLayout, shape)[1]) {\n         ret.push_back({i, j});\n         ret.push_back({i, j + 1});\n         ret.push_back({i + 2, j});\n@@ -700,6 +717,7 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n         ret.push_back({i + 2, j + 9});\n       }\n     }\n+\n     return ret;\n   }\n \n@@ -751,6 +769,9 @@ class ConvertTritonGPUOpToLLVMPatternBase {\n   SmallVector<SmallVector<Value>> emitIndicesForDistributedLayout(\n       Location loc, ConversionPatternRewriter &rewriter,\n       const Attribute &layout, ArrayRef<int64_t> shape) const {\n+    if (auto mmaLayout = layout.template dyn_cast<MmaEncodingAttr>()) {\n+      assert(!mmaLayout.isVolta());\n+    }\n \n     // step 1, delinearize threadId to get the base index\n     auto multiDimBase = emitBaseIndexForLayout(loc, rewriter, layout, shape);"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TypeConverter.h", "status": "modified", "additions": 8, "deletions": 12, "changes": 20, "file_content_changes": "@@ -113,24 +113,20 @@ class TritonGPUToLLVMTypeConverter : public LLVMTypeConverter {\n         }\n \n         if (mmaLayout.isVolta()) {\n+          auto [isARow, isBRow, isAVec4, isBVec4, mmaId] =\n+              mmaLayout.decodeVoltaLayoutStates();\n           DotOpMmaV1ConversionHelper helper(mmaLayout);\n-\n-          // TODO[Superjomn]: Both transA and transB are not available here.\n-          bool trans = false;\n-          // TODO[Superjomn]: The order of A and B are not available here.\n-          SmallVector<unsigned> order({1, 0});\n-          if (trans) {\n-            std::swap(shape[0], shape[1]);\n-            std::swap(order[0], order[1]);\n-          }\n-\n           if (dotOpLayout.getOpIdx() == 0) { // $a\n-            int elems = helper.numElemsPerThreadA(shape, order);\n+            DotOpMmaV1ConversionHelper::AParam param(isARow, isAVec4);\n+            int elems =\n+                helper.numElemsPerThreadA(shape, isARow, isAVec4, param.vec);\n             Type x2Ty = vec_ty(elemTy, 2);\n             return struct_ty(SmallVector<Type>(elems, x2Ty));\n           }\n           if (dotOpLayout.getOpIdx() == 1) { // $b\n-            int elems = helper.numElemsPerThreadB(shape, order);\n+            DotOpMmaV1ConversionHelper::BParam param(isBRow, isBVec4);\n+            int elems =\n+                helper.numElemsPerThreadB(shape, isBRow, isBVec4, param.vec);\n             Type x2Ty = vec_ty(elemTy, 2);\n             return struct_ty(SmallVector<Type>(elems, x2Ty));\n           }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ViewOpToLLVM.cpp", "status": "modified", "additions": 16, "deletions": 3, "changes": 19, "file_content_changes": "@@ -61,6 +61,8 @@ struct SplatOpConversion\n       ConversionPatternRewriter &rewriter, Location loc) {\n     auto tensorTy = resType.cast<RankedTensorType>();\n     auto shape = tensorTy.getShape();\n+    auto dotOperand =\n+        tensorTy.getEncoding().cast<triton::gpu::DotOperandEncodingAttr>();\n     auto parent = layout.getParent();\n     Value retVal = constVal;\n     Type retTy = elemType;\n@@ -78,11 +80,21 @@ struct SplatOpConversion\n         matTy = helper.getMatType();\n       } else if (mmaLayout.isVolta()) {\n         DotOpMmaV1ConversionHelper helper(mmaLayout);\n-        numElems = layout.getOpIdx() == 0\n-                       ? helper.numElemsPerThreadA(shape, {0, 1})\n-                       : helper.numElemsPerThreadB(shape, {0, 1});\n+        bool isRow = layout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+        auto [isARow, isBRow, isAVec4, isBVec4, _0] =\n+            mmaLayout.decodeVoltaLayoutStates();\n+        if (layout.getOpIdx() == 0) {\n+          DotOpMmaV1ConversionHelper::AParam aParam(isARow, isAVec4);\n+          numElems =\n+              helper.numElemsPerThreadA(shape, isARow, isAVec4, aParam.vec);\n+        } else {\n+          DotOpMmaV1ConversionHelper::BParam bParam(isBRow, isBVec4);\n+          numElems =\n+              helper.numElemsPerThreadB(shape, isBRow, isBVec4, bParam.vec);\n+        }\n         matTy = helper.getMatType(tensorTy);\n       }\n+\n       auto numPackedElems = matTy.cast<LLVM::LLVMStructType>()\n                                 .getBody()[0]\n                                 .cast<VectorType>()\n@@ -92,6 +104,7 @@ struct SplatOpConversion\n       for (auto i = 0; i < numPackedElems; ++i) {\n         retVal = insert_element(retTy, retVal, constVal, i32_val(i));\n       }\n+\n     } else if (auto blockedLayout = parent.dyn_cast<BlockedEncodingAttr>()) {\n       numElems = DotOpFMAConversionHelper::getNumElemsPerThread(shape, layout);\n     } else {"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 13, "deletions": 9, "changes": 22, "file_content_changes": "@@ -112,9 +112,7 @@ SmallVector<unsigned> getSizePerThread(const Attribute &layout) {\n     if (mmaLayout.isAmpere()) {\n       return {2, 2};\n     } else if (mmaLayout.isVolta()) {\n-      // Note: here the definition of sizePerThread is obscure, which doesn't\n-      // mean vecSize=4 can be supported in the last dimension.\n-      return {2, 4};\n+      return {1, 2};\n     } else {\n       llvm_unreachable(\"Unexpected mma version\");\n     }\n@@ -173,7 +171,8 @@ SmallVector<unsigned> getThreadsPerCTA(const Attribute &layout) {\n   return threads;\n }\n \n-SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n+SmallVector<unsigned> getShapePerCTA(const Attribute &layout,\n+                                     ArrayRef<int64_t> tensorShape) {\n   SmallVector<unsigned> shape;\n   if (auto blockedLayout = layout.dyn_cast<BlockedEncodingAttr>()) {\n     for (unsigned d = 0, n = blockedLayout.getOrder().size(); d < n; ++d)\n@@ -186,23 +185,28 @@ SmallVector<unsigned> getShapePerCTA(const Attribute &layout) {\n     for (unsigned d = 0, n = getOrder(parent).size(); d < n; ++d) {\n       if (d == dim)\n         continue;\n-      shape.push_back(getShapePerCTA(parent)[d]);\n+      shape.push_back(getShapePerCTA(parent, tensorShape)[d]);\n     }\n   } else if (auto mmaLayout = layout.dyn_cast<MmaEncodingAttr>()) {\n     if (mmaLayout.isAmpere())\n       return {16 * mmaLayout.getWarpsPerCTA()[0],\n               8 * mmaLayout.getWarpsPerCTA()[1]};\n-    if (mmaLayout.isVolta())\n-      return {16 * mmaLayout.getWarpsPerCTA()[0],\n-              16 * mmaLayout.getWarpsPerCTA()[1]};\n+    if (mmaLayout.isVolta()) {\n+      assert(!tensorShape.empty() && \"Volta needs the tensorShape\");\n+      if (tensorShape.size() == 1) // must be SliceEncoding\n+        return {static_cast<unsigned>(tensorShape[0]),\n+                static_cast<unsigned>(tensorShape[0])};\n+      return {static_cast<unsigned>(tensorShape[0]),\n+              static_cast<unsigned>(tensorShape[1])};\n+    }\n     assert(0 && \"Unexpected MMA layout version found\");\n   } else if (auto dotLayout = layout.dyn_cast<DotOperandEncodingAttr>()) {\n     auto parentLayout = dotLayout.getParent();\n     assert(parentLayout && \"DotOperandEncodingAttr must have a parent\");\n     if (auto parentMmaLayout = parentLayout.dyn_cast<MmaEncodingAttr>()) {\n       assert(parentMmaLayout.isAmpere() &&\n              \"mmaLayout version = 1 is not implemented yet\");\n-      auto parentShapePerCTA = getShapePerCTA(parentLayout);\n+      auto parentShapePerCTA = getShapePerCTA(parentLayout, tensorShape);\n       auto opIdx = dotLayout.getOpIdx();\n       if (opIdx == 0) {\n         return {parentShapePerCTA[0], 16};"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 4, "deletions": 32, "changes": 36, "file_content_changes": "@@ -887,31 +887,8 @@ SmallVector<int64_t, 2> mmaVersionToShapePerWarp(int version) {\n \n SmallVector<unsigned, 2> warpsPerTileV1(const ArrayRef<int64_t> shape,\n                                         int numWarps) {\n-  if (!MmaEncodingAttr::_mmaV1UpdateWpt) {\n-    SmallVector<unsigned, 2> ret = {1, 1};\n-    SmallVector<int64_t, 2> shapePerWarp =\n-        mmaVersionToShapePerWarp(1 /*version*/);\n-    bool changed = false;\n-    do {\n-      changed = false;\n-      int pre = ret[0];\n-      if (ret[0] * ret[1] < numWarps) {\n-        ret[0] =\n-            std::clamp<unsigned>(ret[0] * 2, 1, shape[0] / shapePerWarp[0]);\n-        changed = pre != ret[0];\n-      }\n-      if (ret[0] * ret[1] < numWarps) {\n-        pre = ret[1];\n-        ret[1] =\n-            std::clamp<unsigned>(ret[1] * 2, 1, shape[1] / shapePerWarp[1]);\n-        changed = pre != ret[1];\n-      }\n-    } while (changed);\n-    return ret;\n-  } else {\n-    // Set a default value and ensure product of wpt equals numWarps\n-    return {static_cast<unsigned>(numWarps), 1};\n-  }\n+  // Set a default value and ensure product of wpt equals numWarps\n+  return {static_cast<unsigned>(numWarps), 1};\n }\n \n SmallVector<unsigned, 2> warpsPerTileV2(triton::DotOp dotOp,\n@@ -1107,13 +1084,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n         getWarpsPerTile(dotOp, retShape, versionMajor, numWarps);\n     triton::gpu::MmaEncodingAttr mmaEnc;\n     if (versionMajor == 1) {\n-      if (MmaEncodingAttr::_mmaV1UpdateWpt)\n-        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-            oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n-      else\n-        mmaEnc = triton::gpu::MmaEncodingAttr::get(\n-            dotOp->getContext(), versionMajor, 0 /*versionMinor*/,\n-            warpsPerTileV1(retShape, numWarps));\n+      mmaEnc = triton::gpu::MmaEncodingAttr::get(\n+          oldRetType.getContext(), versionMajor, numWarps, mmaV1Counter++);\n     } else if (versionMajor == 2) {\n       mmaEnc = triton::gpu::MmaEncodingAttr::get(\n           oldRetType.getContext(), versionMajor, 0 /*versionMinor*/,"}, {"filename": "lib/Dialect/TritonGPU/Transforms/UpdateMmaForVolta.cpp", "status": "modified", "additions": 25, "deletions": 28, "changes": 53, "file_content_changes": "@@ -13,6 +13,7 @@ using triton::DotOp;\n using triton::gpu::ConvertLayoutOp;\n using triton::gpu::DotOperandEncodingAttr;\n using triton::gpu::MmaEncodingAttr;\n+using triton::gpu::SharedEncodingAttr;\n using triton::gpu::SliceEncodingAttr;\n \n // This pattern collects the wrong Mma those need to update and create the right\n@@ -33,12 +34,13 @@ class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-\n     auto dotOp = cast<triton::DotOp>(op);\n     auto *ctx = dotOp->getContext();\n     auto AT = dotOp.a().getType().cast<RankedTensorType>();\n     auto BT = dotOp.b().getType().cast<RankedTensorType>();\n     auto DT = dotOp.d().getType().cast<RankedTensorType>();\n+    auto shapeA = AT.getShape();\n+    auto shapeB = BT.getShape();\n     if (!DT.getEncoding())\n       return failure();\n     auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n@@ -53,42 +55,34 @@ class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n     auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n     bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n     bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n-    auto [isARow_, isBRow_, isAVec4, isBVec4, mmaId] =\n+    auto [isARow_, isBRow_, isAVec4_, isBVec4_, mmaId] =\n         mmaLayout.decodeVoltaLayoutStates();\n \n+    bool isAVec4 = !isARow && (shapeA[isARow] <= 16);\n+    bool isBVec4 = isBRow && (shapeB[isBRow] <= 16);\n+\n     // The wpt of MMAv1 is also determined by isARow, isBRow and shape, and it\n     // could only be set here for those states might be updated by previous\n     // patterns in the Combine Pass.\n-    if (isARow_ == isARow && isBRow_ == isBRow) {\n-      auto tgtWpt =\n-          getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n-                         product(mmaLayout.getWarpsPerCTA()));\n-      // Check if the wpt should be updated.\n-      if (tgtWpt == mmaLayout.getWarpsPerCTA() ||\n-          !MmaEncodingAttr::_mmaV1UpdateWpt)\n+    auto tgtWpt = getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4,\n+                                 isBVec4, product(mmaLayout.getWarpsPerCTA()));\n+    if (isARow == isARow_ && isBRow == isBRow_ && isAVec4 == isAVec4_ &&\n+        isBVec4 == isBVec4_) {\n+      if (tgtWpt == mmaLayout.getWarpsPerCTA())\n         return failure();\n     }\n \n     MmaEncodingAttr newMmaLayout;\n     {\n-      // Create a temporary MMA layout to obtain the isAVec4 and isBVec4\n-      auto tmpMmaLayout = MmaEncodingAttr::get(\n-          ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n-          AT.getShape(), BT.getShape(), isARow, isBRow, mmaId);\n-      auto [isARow_, isBRow_, isAVec4_, isBVec4_, _] =\n-          tmpMmaLayout.decodeVoltaLayoutStates();\n-\n       // Recalculate the wpt, for here we could get the latest information, the\n       // wpt should be updated.\n       auto updatedWpt =\n-          getWarpsPerCTA(DT.getShape(), isARow_, isBRow_, isAVec4_, isBVec4_,\n+          getWarpsPerCTA(DT.getShape(), isARow, isBRow, isAVec4, isBVec4,\n                          product(mmaLayout.getWarpsPerCTA()));\n-      auto newWpt = MmaEncodingAttr::_mmaV1UpdateWpt\n-                        ? updatedWpt\n-                        : mmaLayout.getWarpsPerCTA();\n+\n       newMmaLayout = MmaEncodingAttr::get(ctx, mmaLayout.getVersionMajor(),\n-                                          newWpt, AT.getShape(), BT.getShape(),\n-                                          isARow, isBRow, mmaId);\n+                                          updatedWpt, AT.getShape(),\n+                                          BT.getShape(), isARow, isBRow, mmaId);\n     }\n \n     // Collect the wrong MMA Layouts, and mark need to update.\n@@ -100,14 +94,14 @@ class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n   // Get the wpt for MMAv1 using more information.\n   // Reference the original logic here\n   // https://github.com/openai/triton/blob/0e4691e6dd91e001a8d33b71badf8b3314325459/lib/codegen/analysis/layout.cc#L223\n-  SmallVector<unsigned, 2> getWarpsPerCTA(ArrayRef<int64_t> shape, bool isARow,\n-                                          bool isBRow, bool isAVec4,\n-                                          bool isBVec4, int numWarps) const {\n+  SmallVector<unsigned> getWarpsPerCTA(ArrayRef<int64_t> shape, bool isARow,\n+                                       bool isBRow, bool isAVec4, bool isBVec4,\n+                                       int numWarps) const {\n     // TODO[Superjomn]: Share code with\n     // DotOpMmaV1ConversionHelper::AParam/BParam, since same code to compute the\n     // rep,spw and fpw.\n-    SmallVector<unsigned, 2> wpt({1, 1});\n-    SmallVector<unsigned, 2> wpt_nm1;\n+    SmallVector<unsigned> wpt({1, 1});\n+    SmallVector<unsigned> wpt_nm1;\n \n     SmallVector<int, 2> rep(2), spw(2);\n     std::array<int, 3> fpw{{2, 2, 1}};\n@@ -242,7 +236,10 @@ class UpdateMMAForMMAv1 : public mlir::RewritePattern {\n \n     auto srcTy = op->getOperand(0).getType();\n     auto resTy = op->getResult(0).getType();\n-    if (!needUpdate(srcTy) && needUpdate(resTy)) {\n+\n+    if (needUpdate(resTy)) {\n+      //  The op-inputs' types are not necessary to update, for some\n+      //  replaceOpWithNewOp will help update them.\n       op->getResult(0).setType(\n           getUpdatedType(resTy.dyn_cast<RankedTensorType>()));\n       return success();"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -1108,6 +1108,10 @@ def test_dot(M, N, K, num_warps, col_a, col_b, epilogue, allow_tf32, dtype, devi\n             pytest.skip(\"Only test int8 on devices with sm >= 80\")\n         elif dtype == 'float32' and allow_tf32:\n             pytest.skip(\"Only test tf32 on devices with sm >= 80\")\n+    if capability[0] == 7:\n+        if (M, N, K, num_warps) == (128, 256, 32, 8):\n+            pytest.skip(\"shared memory out of resource\")\n+\n     torch.backends.cuda.matmul.allow_tf32 = allow_tf32\n \n     # triton kernel"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -900,12 +900,13 @@ def ttir_to_ttgir(mod, num_warps, num_stages, compute_capability):\n     pm.add_tritongpu_combine_pass(compute_capability)\n     pm.add_licm_pass()\n     pm.add_tritongpu_combine_pass(compute_capability)\n+    pm.add_cse_pass()\n+    pm.add_tritongpu_decompose_conversions_pass()\n     if compute_capability // 10 == 7:\n         # The update_mma_for_volta pass helps to compute some information for MMA encoding specifically for MMAv1\n+        # NOTE this pass should be placed after all the passes those modifies mma layout\n         pm.add_tritongpu_update_mma_for_volta_pass()\n     pm.add_cse_pass()\n-    pm.add_tritongpu_decompose_conversions_pass()\n-    pm.add_cse_pass()\n     pm.add_symbol_dce_pass()\n     pm.add_tritongpu_reorder_instructions_pass()\n     pm.run(mod)"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 20, "deletions": 21, "changes": 41, "file_content_changes": "@@ -758,12 +758,12 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n \n // -----\n \n-#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#mma = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [2, 1]}>\n+#blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n+#mma = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [2, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n   // CHECK: llvm.mlir.global external @global_smem() {addr_space = 3 : i32} : !llvm.array<0 x i8>\n   // CHECK-LABEL: convert_layout_mmav1_block\n-  func @convert_layout_mmav1_blocked(%arg0: tensor<32x16xf32, #mma>) {\n+  func @convert_layout_mmav1_blocked(%arg0: tensor<32x64xf32, #mma>) {\n     // CHECK: llvm.store\n     // CHECK-SAME: !llvm.ptr<vector<2xf32>, 3>\n     // CHECK: llvm.store\n@@ -775,13 +775,12 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     // CHECK: nvvm.barrier0\n     // CHECK: llvm.load\n     // CHECK-SAME: !llvm.ptr<vector<4xf32>, 3>\n-    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x16xf32, #mma>) -> tensor<32x16xf32, #blocked0>\n+    %0 = triton_gpu.convert_layout %arg0 : (tensor<32x64xf32, #mma>) -> tensor<32x64xf32, #blocked>\n     return\n   }\n }\n \n // -----\n-\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n@@ -868,24 +867,24 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n // -----\n \n #blocked = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n-#shared = #triton_gpu.shared<{vec = 1, perPhase = 1, maxPhase = 1, order = [1, 0]}>\n-#mma = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [2, 2]}>\n+#shared0 = #triton_gpu.shared<{vec = 4, perPhase = 1, maxPhase = 8, order = [1, 0]}>\n+#shared1 = #triton_gpu.shared<{vec = 8, perPhase = 1, maxPhase = 4, order = [1, 0]}>\n+#mma = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [2, 2]}>\n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma, isMMAv1Row=true}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   func @matmul884_kernel_dot_operand_layout(%ptr:!tt.ptr<f32> {tt.divisibility = 16 : i32},\n-  %a:tensor<128x32xf16, #shared>, %b:tensor<32x256xf16, #shared>) {\n-    %cst = arith.constant dense<0.000000e+00> : tensor<128x256xf32, #mma>\n+  %a:tensor<32x64xf16, #shared0>, %b:tensor<64x64xf16, #shared1>) {\n+    %cst = arith.constant dense<0.000000e+00> : tensor<32x64xf32, #mma>\n     // CHECK: ldmatrix.sync.aligned.m8n8.x4.shared.b16\n-    %a_mat = triton_gpu.convert_layout %a : (tensor<128x32xf16, #shared>) -> tensor<128x32xf16, #dot_operand_a>\n-    %b_mat = triton_gpu.convert_layout %b : (tensor<32x256xf16, #shared>) -> tensor<32x256xf16, #dot_operand_b>\n+    %a_mat = triton_gpu.convert_layout %a : (tensor<32x64xf16, #shared0>) -> tensor<32x64xf16, #dot_operand_a>\n+    %b_mat = triton_gpu.convert_layout %b : (tensor<64x64xf16, #shared1>) -> tensor<64x64xf16, #dot_operand_b>\n \n-    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<128x32xf16, #dot_operand_a> * tensor<32x256xf16, #dot_operand_b> -> tensor<128x256xf32, #mma>\n-    // TODO[goostavz]: uncomment the following lines after convert_layout[mma<v1> -> blocked] is ready.\n-    // %38 = triton_gpu.convert_layout %28 : (tensor<128x256xf32, #mma>) -> tensor<128x256xf32, #blocked>\n-    // %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<128x1x!tt.ptr<f32>, #blocked>\n-    // %36 = tt.broadcast %30 : (tensor<128x1x!tt.ptr<f32>, #blocked>) -> tensor<128x256x!tt.ptr<f32>, #blocked>\n-    // tt.store %36, %38 : tensor<128x256xf32, #blocked>\n+    %28 = tt.dot %a_mat, %b_mat, %cst {allowTF32 = true, transA = false, transB = false} : tensor<32x64xf16, #dot_operand_a> * tensor<64x64xf16, #dot_operand_b> -> tensor<32x64xf32, #mma>\n+    %38 = triton_gpu.convert_layout %28 : (tensor<32x64xf32, #mma>) -> tensor<32x64xf32, #blocked>\n+    %30 = tt.splat %ptr : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked>\n+    %36 = tt.broadcast %30 : (tensor<32x1x!tt.ptr<f32>, #blocked>) -> tensor<32x64x!tt.ptr<f32>, #blocked>\n+    tt.store %36, %38 : tensor<32x64xf32, #blocked>\n     return\n   }\n }\n@@ -999,15 +998,15 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n     %v1 = arith.addi %v0, %blockdimz : i32\n     %0 = tt.splat %v1 : (i32) -> tensor<32xi32, #blocked0>\n     tt.store %a, %0 : tensor<32xi32, #blocked0>\n-  \n+\n     return\n   }\n }\n \n // -----\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [2], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n-  // CHECK-LABEL: test_index_cache \n+  // CHECK-LABEL: test_index_cache\n   func @test_index_cache() {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked0>\n@@ -1021,7 +1020,7 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>\n #shared0 = #triton_gpu.shared<{vec = 8, perPhase = 2, maxPhase = 4, order = [1, 0]}>\n module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n-  // CHECK-LABEL: test_base_index_cache \n+  // CHECK-LABEL: test_base_index_cache\n   func @test_base_index_cache(%arg0: tensor<128x32xf32, #blocked0>) {\n     // CHECK: nvvm.read.ptx.sreg.tid.x\n     %0 = triton_gpu.convert_layout %arg0 : (tensor<128x32xf32, #blocked0>) -> tensor<128x32xf32, #shared0>\n@@ -1045,4 +1044,4 @@ module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n     }\n     return\n   }\n-}\n\\ No newline at end of file\n+}"}, {"filename": "test/TritonGPU/update-mma-for-volta.mlir", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "file_content_changes": "@@ -12,7 +12,7 @@\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n // It creates a new MMA layout to fit with $a and $b's dot_operand, and get the right warpsPerCTA\n // The ID of this MMA instance should be 0.\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n   // CHECK-LABEL: dot_mmav1\n   func @dot_mmav1(%A: tensor<64x64xf16, #blocked0>, %B: tensor<64x64xf16, #blocked0>) -> tensor<64x64xf32, #blocked0> {\n@@ -40,8 +40,8 @@ module attributes {\"triton_gpu.num-warps\" = 16 : i32} {\n #mma1 = #triton_gpu.mma<{versionMajor=1, versionMinor=16, warpsPerCTA=[4,4]}>\n \n // Will still get two MMA layouts\n-// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 4]}>\n-// CHECK: [[new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 4]}>\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 3, warpsPerCTA = [4, 2]}>\n+// CHECK: [[new_mma1:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 19, warpsPerCTA = [4, 2]}>\n \n #dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n #dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>"}]