[{"filename": "python/test/unit/hopper/test_persistent_warp_specialized_gemm.py", "status": "modified", "additions": 13, "deletions": 28, "changes": 41, "file_content_changes": "@@ -302,20 +302,7 @@ def test_non_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N, BLOCK_K\n     th_c = torch.matmul(a, b)\n     torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n \n-    # # #############################################Performance Evaluation#############################################\n-    # fn = lambda: call_vintage()\n-    # ms = triton.testing.do_bench(fn, warmup=25, rep=100)\n-    # cur_gpu_perf = round(2. * M * N * K / ms * 1e-9, 2)\n-    # print(' '.join(['Performance of', str(M), str(N), str(K), ':', str(ms), 'ms, ', str(cur_gpu_perf), 'TFLOPS']))\n-\n-\n-@triton.autotune(\n-    configs=[\n-        triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=True),\n-        # triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=False),\n-    ],\n-    key=['M', 'N', 'K'],\n-)\n+\n @triton.jit\n def static_persistent_warp_specialized_matmul_kernel(\n     a_ptr, b_ptr, c_ptr,\n@@ -355,13 +342,6 @@ def static_persistent_warp_specialized_matmul_kernel(\n         tl.store(c_ptrs, accumulator)\n \n \n-@triton.autotune(\n-    configs=[\n-        triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=True),\n-        # triton.Config({}, num_stages=3, num_warps=4, enable_warp_specialization=False),\n-    ],\n-    key=['M', 'N', 'K'],\n-)\n @triton.jit\n def static_persistent_tma_warp_specialized_matmul_kernel(\n     a_ptr, b_ptr, c_ptr,\n@@ -429,6 +409,12 @@ def static_persistent_tma_warp_specialized_matmul_kernel(\n                              [4096, 4096, 256, 256, 128, 64, 1, False, True],\n                              [4096, 4096, 256, 128, 256, 16, 1, False, True],\n                              [4096, 4096, 256, 128, 256, 64, 1, False, True],\n+                             # numCTAs > 1\n+                             [2048, 2048, 64, 128, 128, 64, 2, False, True],\n+                             [2048, 2048, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 128, 256, 128, 64, 4, False, True],\n+                             [4096, 4096, 256, 128, 256, 64, 4, False, True],\n+                             [4096, 4096, 256, 256, 256, 64, 4, False, True],\n                          ]\n                              for use_tma in [False, True]\n                          ])\n@@ -455,23 +441,22 @@ def test_user_defined_persistent_warp_specialized_gemm(M, N, K, BLOCK_M, BLOCK_N\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n-            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs)\n+            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs,\n+            num_warps=4, num_ctas=NUM_CTAS,\n+            enable_warp_specialization=True)\n     else:\n         static_persistent_warp_specialized_matmul_kernel[grid](\n             a, b, c,\n             M, N, K,\n             a.stride(0), a.stride(1),\n             b.stride(0), b.stride(1),\n             c.stride(0), c.stride(1),\n-            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs)\n+            BLOCK_M, BLOCK_N, BLOCK_K, num_SMs,\n+            num_warps=4, num_ctas=NUM_CTAS,\n+            enable_warp_specialization=True)\n \n     th_c = torch.matmul(a, b)\n     torch.testing.assert_allclose(th_c, c, atol=1e-2, rtol=0)\n-    # #############################################Performance Evaluation#############################################\n-    # fn = lambda: call_stylish()\n-    # ms = triton.testing.do_bench(fn, warmup=25, rep=100)\n-    # cur_gpu_perf = round(2. * M * N * K / ms * 1e-9, 2)\n-    # print(' '.join(['Performance of', str(M), str(N), str(K), ':', str(ms), 'ms, ', str(cur_gpu_perf), 'TFLOPS']))\n \n \n @triton.jit"}]