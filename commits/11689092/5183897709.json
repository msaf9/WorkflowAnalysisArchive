[{"filename": "include/triton/Dialect/Triton/IR/TritonAttrDefs.td", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "file_content_changes": "@@ -14,6 +14,17 @@ def TT_CacheModifierAttr : I32EnumAttr<\n     let cppNamespace = \"::mlir::triton\";\n }\n \n+def TT_MemSemanticAttr : I32EnumAttr<\n+    \"MemSemantic\", \"\",\n+    [\n+      I32EnumAttrCase<\"RELAXED\", 1, \"relaxed\">,\n+      I32EnumAttrCase<\"ACQUIRE\", 2, \"acquire\">,\n+      I32EnumAttrCase<\"RELEASE\", 3, \"release\">,\n+      I32EnumAttrCase<\"ACQUIRE_RELEASE\", 4, \"acq_rel\">,\n+    ]> {\n+    let cppNamespace = \"::mlir::triton\";\n+}\n+\n def TT_EvictionPolicyAttr : I32EnumAttr<\n     \"EvictionPolicy\", \"\",\n     ["}, {"filename": "include/triton/Dialect/Triton/IR/TritonOps.td", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -234,7 +234,8 @@ def TT_AtomicRMWOp : TT_Op<\"atomic_rmw\", [SameOperandsAndResultShape,\n     }];\n \n     let arguments = (ins TT_AtomicRMWAttr:$atomic_rmw_op, TT_PtrLike:$ptr,\n-                         TT_Type:$val, Optional<TT_BoolLike>:$mask);\n+                         TT_Type:$val, Optional<TT_BoolLike>:$mask,\n+                         TT_MemSemanticAttr:$sem);\n \n     let results = (outs TT_Type:$result);\n }\n@@ -255,7 +256,8 @@ def TT_AtomicCASOp : TT_Op<\"atomic_cas\", [MemoryEffects<[MemRead]>,\n         return $old\n     }];\n \n-    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$cmp, TT_Type:$val);\n+    let arguments = (ins TT_PtrLike:$ptr, TT_Type:$cmp, TT_Type:$val,\n+                     TT_MemSemanticAttr:$sem);\n \n     let results = (outs TT_Type:$result);\n }\n@@ -444,7 +446,6 @@ class TT_ExternElementwiseOpBase<string mnemonic, list<Trait> traits = []> :\n     let assemblyFormat = \"operands attr-dict `:` functional-type(operands, $result)\";\n }\n \n-\n def TT_PureExternElementwiseOp : TT_ExternElementwiseOpBase<\"pure_extern_elementwise\", [Pure, Elementwise]> {\n     let summary = \"FFI for pure element-wise extern LLVM bitcode functions\";\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 10, "deletions": 13, "changes": 23, "file_content_changes": "@@ -406,11 +406,6 @@ struct AtomicCASOpConversion\n                  : valueTy;\n     auto valueElemNBits = valueElemTy.getIntOrFloatBitWidth();\n     Value mask = getMask(valueTy, rewriter, loc);\n-    PTXBuilder ptxBuilderMemfence;\n-    auto memfence = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n-    memfence();\n-    auto ASMReturnTy = void_ty(ctx);\n-    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n \n     Value atomPtr = getSharedMemoryBase(loc, rewriter, op.getOperation());\n     atomPtr = bitcast(atomPtr, ptr_ty(valueElemTy, 3));\n@@ -424,7 +419,10 @@ struct AtomicCASOpConversion\n     auto *cmpOpr = ptxBuilderAtomicCAS.newOperand(casCmp, \"r\");\n     auto *valOpr = ptxBuilderAtomicCAS.newOperand(casVal, \"r\");\n     auto &atom = *ptxBuilderAtomicCAS.create<PTXInstr>(\"atom\");\n-    atom.global().o(\"cas\").o(\"b32\");\n+    std::string semStr;\n+    llvm::raw_string_ostream os(semStr);\n+    os << op.getSem();\n+    atom.global().o(semStr).o(\"cas\").o(\"b32\");\n     atom(dstOpr, ptrOpr, cmpOpr, valOpr).predicate(mask);\n     auto old = ptxBuilderAtomicCAS.launch(rewriter, loc, valueElemTy);\n     barrier();\n@@ -435,8 +433,8 @@ struct AtomicCASOpConversion\n     auto &st = *ptxBuilderStore.create<PTXInstr>(\"st\");\n     st.shared().o(\"b32\");\n     st(dstOprStore, valOprStore).predicate(mask);\n+    auto ASMReturnTy = void_ty(ctx);\n     ptxBuilderStore.launch(rewriter, loc, ASMReturnTy);\n-    ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n     barrier();\n     Value ret = load(atomPtr);\n     barrier();\n@@ -464,7 +462,7 @@ struct AtomicRMWOpConversion\n                   ConversionPatternRewriter &rewriter) const override {\n     auto loc = op.getLoc();\n     MLIRContext *ctx = rewriter.getContext();\n-\n+    //\n     auto atomicRmwAttr = op.getAtomicRmwOp();\n \n     Value val = op.getVal();\n@@ -565,7 +563,10 @@ struct AtomicRMWOpConversion\n       default:\n         return failure();\n       }\n-      atom.o(rmwOp).o(sTy);\n+      std::string semStr;\n+      llvm::raw_string_ostream os(semStr);\n+      os << op.getSem();\n+      atom.o(semStr).o(rmwOp).o(sTy);\n       if (tensorTy) {\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n         auto retType = vec == 1 ? valueElemTy : vecTy;\n@@ -575,11 +576,7 @@ struct AtomicRMWOpConversion\n               vec == 1 ? ret : extract_element(valueElemTy, ret, i32_val(ii));\n         }\n       } else {\n-        PTXBuilder ptxBuilderMemfence;\n-        auto memfenc = ptxBuilderMemfence.create<PTXInstr>(\"membar\")->o(\"gl\");\n-        memfenc();\n         auto ASMReturnTy = void_ty(ctx);\n-        ptxBuilderMemfence.launch(rewriter, loc, ASMReturnTy);\n         atom(dstOpr, ptrOpr, valOpr).predicate(rmwMask);\n         auto old = ptxBuilderAtomicRMW.launch(rewriter, loc, valueElemTy);\n         if (op->user_begin() == op->user_end()) {"}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "file_content_changes": "@@ -393,7 +393,8 @@ struct TritonAtomicCASPattern\n                   ConversionPatternRewriter &rewriter) const override {\n     addNamedAttrs(rewriter.replaceOpWithNewOp<triton::AtomicCASOp>(\n                       op, typeConverter->convertType(op.getType()),\n-                      adaptor.getPtr(), adaptor.getCmp(), adaptor.getVal()),\n+                      adaptor.getPtr(), adaptor.getCmp(), adaptor.getVal(),\n+                      op.getSem()),\n                   adaptor.getAttributes());\n     return success();\n   }\n@@ -409,7 +410,7 @@ struct TritonAtomicRMWPattern\n     addNamedAttrs(rewriter.replaceOpWithNewOp<triton::AtomicRMWOp>(\n                       op, typeConverter->convertType(op.getType()),\n                       adaptor.getAtomicRmwOp(), adaptor.getPtr(),\n-                      adaptor.getVal(), adaptor.getMask()),\n+                      adaptor.getVal(), adaptor.getMask(), op.getSem()),\n                   adaptor.getAttributes());\n     return success();\n   }"}, {"filename": "python/src/triton.cc", "status": "modified", "additions": 12, "deletions": 5, "changes": 17, "file_content_changes": "@@ -91,6 +91,13 @@ void init_triton_ir(py::module &&m) {\n       .value(\"CG\", mlir::triton::CacheModifier::CG)\n       .export_values();\n \n+  py::enum_<mlir::triton::MemSemantic>(m, \"MEM_SEMANTIC\")\n+      .value(\"ACQUIRE_RELEASE\", mlir::triton::MemSemantic::ACQUIRE_RELEASE)\n+      .value(\"ACQUIRE\", mlir::triton::MemSemantic::ACQUIRE)\n+      .value(\"RELEASE\", mlir::triton::MemSemantic::RELEASE)\n+      .value(\"RELAXED\", mlir::triton::MemSemantic::RELAXED)\n+      .export_values();\n+\n   py::enum_<mlir::triton::EvictionPolicy>(m, \"EVICTION_POLICY\")\n       .value(\"NORMAL\", mlir::triton::EvictionPolicy::NORMAL)\n       .value(\"EVICT_FIRST\", mlir::triton::EvictionPolicy::EVICT_FIRST)\n@@ -1268,7 +1275,7 @@ void init_triton_ir(py::module &&m) {\n       // // atomic\n       .def(\"create_atomic_cas\",\n            [](mlir::OpBuilder &self, mlir::Value &ptr, mlir::Value &cmp,\n-              mlir::Value &val) -> mlir::Value {\n+              mlir::Value &val, mlir::triton::MemSemantic sem) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n              if (auto srcTensorType =\n@@ -1284,12 +1291,12 @@ void init_triton_ir(py::module &&m) {\n                dstType = ptrType.getPointeeType();\n              }\n              return self.create<mlir::triton::AtomicCASOp>(loc, dstType, ptr,\n-                                                           cmp, val);\n+                                                           cmp, val, sem);\n            })\n       .def(\"create_atomic_rmw\",\n            [](mlir::OpBuilder &self, mlir::triton::RMWOp rmwOp,\n-              mlir::Value &ptr, mlir::Value &val,\n-              mlir::Value &mask) -> mlir::Value {\n+              mlir::Value &ptr, mlir::Value &val, mlir::Value &mask,\n+              mlir::triton::MemSemantic sem) -> mlir::Value {\n              auto loc = self.getUnknownLoc();\n              mlir::Type dstType;\n              if (auto srcTensorType =\n@@ -1305,7 +1312,7 @@ void init_triton_ir(py::module &&m) {\n                dstType = ptrType.getPointeeType();\n              }\n              return self.create<mlir::triton::AtomicRMWOp>(loc, dstType, rmwOp,\n-                                                           ptr, val, mask);\n+                                                           ptr, val, mask, sem);\n            })\n       // External\n       .def(\"create_extern_elementwise\","}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 20, "deletions": 13, "changes": 33, "file_content_changes": "@@ -937,15 +937,16 @@ def kernel(X, Y, Z):\n # ---------------\n # test atomics\n # ---------------\n-@pytest.mark.parametrize(\"op, dtype_x_str, mode\", itertools.chain.from_iterable([\n+@pytest.mark.parametrize(\"op, dtype_x_str, mode, sem\", itertools.chain.from_iterable([\n     [\n-        ('add', 'float16', mode),\n-        ('add', 'uint32', mode), ('add', 'int32', mode), ('add', 'float32', mode),\n-        ('max', 'uint32', mode), ('max', 'int32', mode), ('max', 'float32', mode),\n-        ('min', 'uint32', mode), ('min', 'int32', mode), ('min', 'float32', mode),\n+        ('add', 'float16', mode, sem),\n+        ('add', 'uint32', mode, sem), ('add', 'int32', mode, sem), ('add', 'float32', mode, sem),\n+        ('max', 'uint32', mode, sem), ('max', 'int32', mode, sem), ('max', 'float32', mode, sem),\n+        ('min', 'uint32', mode, sem), ('min', 'int32', mode, sem), ('min', 'float32', mode, sem),\n     ]\n-    for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']]))\n-def test_atomic_rmw(op, dtype_x_str, mode, device='cuda'):\n+    for mode in ['all_neg', 'all_pos', 'min_neg', 'max_pos']\n+    for sem in [None, 'acquire', 'release', 'acq_rel', 'relaxed']]))\n+def test_atomic_rmw(op, dtype_x_str, mode, sem, device='cuda'):\n     capability = torch.cuda.get_device_capability()\n     if capability[0] < 7:\n         if dtype_x_str == 'float16':\n@@ -959,7 +960,8 @@ def kernel(X, Z):\n         x = tl.load(X + pid)\n         old = GENERATE_TEST_HERE\n \n-    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x)'})\n+    sem_arg = sem if sem is None else f'\"{sem}\"'\n+    kernel = patch_kernel(kernel, {'GENERATE_TEST_HERE': f'tl.atomic_{op}(Z, x, sem={sem_arg})'})\n     numpy_op = {'add': np.sum, 'max': np.max, 'min': np.min}[op]\n     max_neutral = float('-inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).min\n     min_neutral = float('inf') if dtype_x_str in float_dtypes else np.iinfo(getattr(np, dtype_x_str)).max\n@@ -981,7 +983,7 @@ def kernel(X, Z):\n     x_tri = to_triton(x, device=device)\n \n     z_tri = to_triton(np.array([neutral], dtype=getattr(np, dtype_x_str)), device=device)\n-    kernel[(n_programs, )](x_tri, z_tri)\n+    h = kernel[(n_programs, )](x_tri, z_tri)\n     # torch result\n     z_ref = numpy_op(x).astype(getattr(np, dtype_x_str))\n     # compare\n@@ -990,6 +992,8 @@ def kernel(X, Z):\n         assert z_ref.item() == to_numpy(z_tri).item()\n     else:\n         np.testing.assert_allclose(z_ref, to_numpy(z_tri), rtol=0.01)\n+    sem_str = \"acq_rel\" if sem is None else sem\n+    assert f\"atom.global.gpu.{sem_str}\" in h.asm[\"ptx\"]\n \n \n def test_atomic_rmw_predicate(device=\"cuda\"):\n@@ -1047,7 +1051,8 @@ def kernel(X, SHAPE0: tl.constexpr, SHAPE1: tl.constexpr):\n     assert torch.min(x).item() == 0.0\n \n \n-def test_atomic_cas():\n+@pytest.mark.parametrize(\"sem\", [None, 'acquire', 'release', 'acq_rel', 'relaxed'])\n+def test_atomic_cas(sem):\n     # 1. make sure that atomic_cas changes the original value (Lock)\n     @triton.jit\n     def change_value(Lock):\n@@ -1060,9 +1065,9 @@ def change_value(Lock):\n \n     # 2. only one block enters the critical section\n     @triton.jit\n-    def serialized_add(data, Lock):\n+    def serialized_add(data, Lock, SEM: tl.constexpr):\n         ptrs = data + tl.arange(0, 128)\n-        while tl.atomic_cas(Lock, 0, 1) == 1:\n+        while tl.atomic_cas(Lock, 0, 1, SEM) == 1:\n             pass\n \n         tl.store(ptrs, tl.load(ptrs) + 1.0)\n@@ -1073,8 +1078,10 @@ def serialized_add(data, Lock):\n     Lock = torch.zeros((1,), device='cuda', dtype=torch.int32)\n     data = torch.zeros((128,), device='cuda', dtype=torch.float32)\n     ref = torch.full((128,), 64.0)\n-    serialized_add[(64,)](data, Lock)\n+    h = serialized_add[(64,)](data, Lock, SEM=sem)\n+    sem_str = \"acq_rel\" if sem is None else sem\n     np.testing.assert_allclose(to_numpy(data), to_numpy(ref))\n+    assert f\"atom.global.{sem_str}\" in h.asm[\"ptx\"]\n \n \n # ---------------"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 24, "deletions": 16, "changes": 40, "file_content_changes": "@@ -1092,59 +1092,67 @@ def _decorator(func: T) -> T:\n \n @builtin\n @_add_atomic_docstr(\"compare-and-swap\")\n-def atomic_cas(pointer, cmp, val, _builder=None):\n+def atomic_cas(pointer, cmp, val, sem=None, _builder=None):\n     cmp = _to_tensor(cmp, _builder)\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_cas(pointer, cmp, val, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_cas(pointer, cmp, val, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"exchange\")\n-def atomic_xchg(pointer, val, mask=None, _builder=None):\n+def atomic_xchg(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_xchg(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_xchg(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"add\")\n-def atomic_add(pointer, val, mask=None, _builder=None):\n+def atomic_add(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_add(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_add(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"max\")\n-def atomic_max(pointer, val, mask=None, _builder=None):\n+def atomic_max(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_max(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_max(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"min\")\n-def atomic_min(pointer, val, mask=None, _builder=None):\n+def atomic_min(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_min(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_min(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"logical and\")\n-def atomic_and(pointer, val, mask=None, _builder=None):\n+def atomic_and(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_and(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_and(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"logical or\")\n-def atomic_or(pointer, val, mask=None, _builder=None):\n+def atomic_or(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_or(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_or(pointer, val, mask, sem, _builder)\n \n \n @builtin\n @_add_atomic_docstr(\"logical xor\")\n-def atomic_xor(pointer, val, mask=None, _builder=None):\n+def atomic_xor(pointer, val, mask=None, sem=None, _builder=None):\n     val = _to_tensor(val, _builder)\n-    return semantic.atomic_xor(pointer, val, mask, _builder)\n+    sem = _constexpr_to_value(sem)\n+    return semantic.atomic_xor(pointer, val, mask, sem, _builder)\n \n \n # -----------------------"}, {"filename": "python/triton/language/semantic.py", "status": "modified", "additions": 52, "deletions": 15, "changes": 67, "file_content_changes": "@@ -811,6 +811,22 @@ def _str_to_padding_option(padding_option):\n     return padding\n \n \n+def _str_to_sem(sem_option):\n+    sem = ir.MEM_SEMANTIC.ACQUIRE_RELEASE\n+    if sem_option:\n+        if sem_option == \"acquire\":\n+            sem = ir.MEM_SEMANTIC.ACQUIRE\n+        elif sem_option == \"release\":\n+            sem = ir.MEM_SEMANTIC.RELEASE\n+        elif sem_option == \"acq_rel\":\n+            sem = ir.MEM_SEMANTIC.ACQUIRE_RELEASE\n+        elif sem_option == \"relaxed\":\n+            sem = ir.MEM_SEMANTIC.RELAXED\n+        else:\n+            raise ValueError(f\"Memory semantic {sem_option} not supported\")\n+    return sem\n+\n+\n def _canonicalize_boundary_check(boundary_check, block_shape):\n     if boundary_check:\n         if not hasattr(boundary_check, \"__iter__\"):\n@@ -1021,11 +1037,13 @@ def store(ptr: tl.tensor,\n def atomic_cas(ptr: tl.tensor,\n                cmp: tl.tensor,\n                val: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n+    sem = _str_to_sem(sem)\n     element_ty = ptr.type.scalar.element_ty\n     if element_ty.primitive_bitwidth not in [16, 32, 64]:\n         raise ValueError(\"atomic_cas only supports elements with width {16, 32, 64}\")\n-    return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle), val.type)\n+    return tl.tensor(builder.create_atomic_cas(ptr.handle, cmp.handle, val.handle, sem), val.type)\n \n \n def atom_red_typechecking_impl(ptr: tl.tensor,\n@@ -1035,7 +1053,6 @@ def atom_red_typechecking_impl(ptr: tl.tensor,\n                                builder: ir.builder) -> Tuple[tl.tensor, tl.tensor, tl.tensor]:\n     if not ptr.type.scalar.is_ptr():\n         raise ValueError(\"Pointer argument of store instruction is \" + ptr.type.__repr__())\n-\n     element_ty = ptr.type.scalar.element_ty\n     if element_ty is tl.float16 and op != 'add':\n         raise ValueError(\"atomic_\" + op + \" does not support fp16\")\n@@ -1060,22 +1077,26 @@ def atom_red_typechecking_impl(ptr: tl.tensor,\n def atomic_max(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'max', builder)\n+    sem = _str_to_sem(sem)\n     sca_ty = val.type.scalar\n     # direct call to atomic_max for integers\n     if sca_ty.is_int():\n         if sca_ty.is_int_signed():\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX,\n                                                        ptr.handle,\n                                                        val.handle,\n-                                                       mask.handle),\n+                                                       mask.handle,\n+                                                       sem),\n                              val.type)\n         else:\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMAX,\n                                                        ptr.handle,\n                                                        val.handle,\n-                                                       mask.handle),\n+                                                       mask.handle,\n+                                                       sem),\n                              val.type)\n     # for float\n     # return atomic_smax(i_ptr, i_val) if val >= 0\n@@ -1084,30 +1105,34 @@ def atomic_max(ptr: tl.tensor,\n     i_ptr = bitcast(ptr, tl.pointer_type(tl.int32, 1), builder)\n     pos = greater_equal(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n     neg = less_than(val, tl.tensor(builder.get_fp32(0), sca_ty), builder)\n-    pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle), i_val.type)\n-    neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle), i_val.type)\n+    pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MAX, i_ptr.handle, i_val.handle, and_(mask, pos, builder).handle, sem), i_val.type)\n+    neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN, i_ptr.handle, i_val.handle, and_(mask, neg, builder).handle, sem), i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n \n \n def atomic_min(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'min', builder)\n+    sem = _str_to_sem(sem)\n     sca_ty = val.type.scalar\n     # direct call to atomic_min for integers\n     if sca_ty.is_int():\n         if sca_ty.is_int_signed():\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                        ptr.handle,\n                                                        val.handle,\n-                                                       mask.handle),\n+                                                       mask.handle,\n+                                                       sem),\n                              val.type)\n         else:\n             return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMIN,\n                                                        ptr.handle,\n                                                        val.handle,\n-                                                       mask.handle),\n+                                                       mask.handle,\n+                                                       sem),\n                              val.type)\n     # for float\n     # return atomic_smin(i_ptr, i_val) if val >= 0\n@@ -1119,56 +1144,68 @@ def atomic_min(ptr: tl.tensor,\n     pos_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.MIN,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n-                                                  and_(mask, pos, builder).handle),\n+                                                  and_(mask, pos, builder).handle,\n+                                                  sem),\n                         i_val.type)\n     neg_ret = tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.UMAX,\n                                                   i_ptr.handle,\n                                                   i_val.handle,\n-                                                  and_(mask, neg, builder).handle),\n+                                                  and_(mask, neg, builder).handle,\n+                                                  sem),\n                         i_val.type)\n     return where(pos, pos_ret, neg_ret, builder)\n \n \n def atomic_add(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'add', builder)\n+    sem = _str_to_sem(sem)\n     sca_ty = val.type.scalar\n     op = ir.ATOMIC_OP.FADD if sca_ty.is_floating() else ir.ATOMIC_OP.ADD\n-    return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle), val.type)\n+    return tl.tensor(builder.create_atomic_rmw(op, ptr.handle, val.handle, mask.handle, sem), val.type)\n \n \n def atomic_and(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'and', builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle), val.type)\n+    sem = _str_to_sem(sem)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.AND, ptr.handle, val.handle, mask.handle, sem), val.type)\n \n \n def atomic_or(ptr: tl.tensor,\n               val: tl.tensor,\n               mask: tl.tensor,\n+              sem: str,\n               builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'or', builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle), val.type)\n+    sem = _str_to_sem(sem)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.OR, ptr.handle, val.handle, mask.handle, sem), val.type)\n \n \n def atomic_xor(ptr: tl.tensor,\n                val: tl.tensor,\n                mask: tl.tensor,\n+               sem: str,\n                builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xor', builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle), val.type)\n+    sem = _str_to_sem(sem)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XOR, ptr.handle, val.handle, mask.handle, sem), val.type)\n \n \n def atomic_xchg(ptr: tl.tensor,\n                 val: tl.tensor,\n                 mask: tl.tensor,\n+                sem: str,\n                 builder: ir.builder) -> tl.tensor:\n     ptr, val, mask = atom_red_typechecking_impl(ptr, val, mask, 'xchg', builder)\n-    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle), val.type)\n+    sem = _str_to_sem(sem)\n+    return tl.tensor(builder.create_atomic_rmw(ir.ATOMIC_OP.XCHG, ptr.handle, val.handle, mask.handle, sem), val.type)\n \n # ===----------------------------------------------------------------------===//\n #                               Linear Algebra"}, {"filename": "test/Conversion/tritongpu_to_llvm.mlir", "status": "modified", "additions": 5, "deletions": 6, "changes": 11, "file_content_changes": "@@ -1010,10 +1010,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   // CHECK-LABEL: atomic_add_f32\n   tt.func @atomic_add_f32(%arg0 : tensor<256x!tt.ptr<f32>, #blocked0>, %arg1 : tensor<256xi1, #blocked0>, %arg2 : tensor<256xf32, #blocked0>) {\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n+    // CHECK-SAME: @$3 atom.global.gpu.relaxed.add.f32\n     // CHECK: llvm.inline_asm\n-    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n-    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n+    // CHECK-SAME: @$3 atom.global.gpu.relaxed.add.f32\n+    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32, sem = 1 : i32} : (tensor<256x!tt.ptr<f32>, #blocked0>, tensor<256xf32, #blocked0>, tensor<256xi1, #blocked0>) -> tensor<256xf32, #blocked0>\n     tt.return\n   }\n }\n@@ -1025,9 +1025,8 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n   tt.func @atomic_add_f32_scalar(%arg0 : !tt.ptr<f32>, %arg1 : i1, %arg2 : f32) {\n     // CHECK: llvm.icmp \"eq\"\n     // CHECK: llvm.inline_asm\n-    // CHECK: llvm.inline_asm\n-    // CHECK-SAME: @$3 atom.global.gpu.add.f32\n-    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32} : (!tt.ptr<f32>, f32, i1) -> f32\n+    // CHECK-SAME: @$3 atom.global.gpu.relaxed.add.f32\n+    %0 = \"tt.atomic_rmw\" (%arg0, %arg2, %arg1) {atomic_rmw_op = 5 : i32, sem = 1: i32} : (!tt.ptr<f32>, f32, i1) -> f32\n     tt.return\n   }\n }"}]