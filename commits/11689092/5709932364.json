[{"filename": "include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "file_content_changes": "@@ -83,18 +83,17 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n         if(!mmaEnc)\n           return $_get(context, 1, 1, 1, order);\n \n-\n         int opIdx = dotOpEnc.getOpIdx();\n \n         // number of rows per phase\n-        int perPhase = 128 / (shape[order[0]] * (typeWidthInBit / 8));\n-        perPhase = std::max<int>(perPhase, 1);\n \n         // index of the inner dimension in `order`\n         unsigned inner = (opIdx == 0) ? 0 : 1;\n \n         // ---- begin Volta ----\n         if (mmaEnc.isVolta()) {\n+          int perPhase = 128 / (shape[order[0]] * (typeWidthInBit / 8));\n+          perPhase = std::max<int>(perPhase, 1);\n           bool is_row = order[0] != 0;\n           bool is_vec4 = opIdx == 0 ? !is_row && (shape[order[0]] <= 16) :\n               is_row && (shape[order[0]] <= 16);\n@@ -108,10 +107,11 @@ A_{3, 2}  A_{3, 3}  A_{3, 0}  A_{3, 1} ...   [phase 1] /\n \n         // ---- begin Ampere ----\n         if (mmaEnc.isAmpere()) {\n-          std::vector<size_t> matShape = {8, 8,\n-                                          2 * 64 / typeWidthInBit};\n+          int perPhase = 128 / (shape[order[0]] * 4 / dotOpEnc.getMMAv2kWidth());\n+          perPhase = std::max<int>(perPhase, 1);\n+          std::vector<size_t> matShape = {8, 8, 4 * dotOpEnc.getMMAv2kWidth()};\n           // for now, disable swizzle when using transposed int8 tensor cores\n-          if (typeWidthInBit == 8 && order[0] == inner)\n+          if ((32 / typeWidthInBit != dotOpEnc.getMMAv2kWidth()) && order[0] == inner)\n             return $_get(context, 1, 1, 1, order);\n \n           // --- handle A operand ---"}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 40, "deletions": 20, "changes": 60, "file_content_changes": "@@ -60,6 +60,7 @@ class MMA16816SmemLoader {\n   SmallVector<uint32_t> warpsPerCTA;\n   int kOrder;\n   int kWidth;\n+  int vecWidth;\n   SmallVector<int64_t> tileShape;\n   SmallVector<int> instrShape;\n   SmallVector<int> matShape;\n@@ -178,13 +179,13 @@ MMA16816SmemLoader::computeLdmatrixMatOffs(Value warpId, Value lane,\n // <----------------------------------------->\n // vecWidth\n // <------->\n-//  t0 ... t0  t1 ... t1  t2 ... t2  t3 ... t3   ||  t0 ... t0  t1 ... t1  t2 ... t2  t3 ... t3  /|\\\n+//  *#t0 ... *#t0  t1 ... t1  t2 ... t2  t3 ... t3   ||  *t0 ... *t0  t1 ... t1  t2 ... t2  t3 ... t3  /|\\\n //  t4 ... t4  t5 ... t5  t6 ... t6  t7 ... t7   ||  t4 ... t4  t5 ... t5  t6 ... t6  t7 ... t7   |\n //  t8 ... t8  t9 ... t9 t10 .. t10 t11 .. t11   ||  t8 ... t8  t9 ... t9 t10 .. t10 t11 .. t11   | quad height\n // ...                                                                                            |\n // t28 .. t28 t29 .. t29 t30 .. t30 t31 .. t31   || t28 .. t28 t29 .. t29 t30 .. t30 t31 .. t31  \\|/\n // --------------------------------------------- || --------------------------------------------\n-//  t0 ... t0  t1 ... t1  t2 ... t2  t3 ... t3   ||  t0 ... t0  t1 ... t1  t2 ... t2  t3 ... t3\n+//  *#t0 ... *#t0  t1 ... t1  t2 ... t2  t3 ... t3   ||  t0 ... t0  t1 ... t1  t2 ... t2  t3 ... t3\n //  t4 ... t4  t5 ... t5  t6 ... t6  t7 ... t7   ||  t4 ... t4  t5 ... t5  t6 ... t6  t7 ... t7\n //  t8 ... t8  t9 ... t9 t10 .. t10 t11 .. t11   ||  t8 ... t8  t9 ... t9 t10 .. t10 t11 .. t11\n // ...\n@@ -206,23 +207,21 @@ SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value warpOff,\n \n   SmallVector<Value> offs(numPtrs);\n \n-  int vecWidth = kWidth;\n   int threadsPerQuad[2] = {8, 4};\n   int laneWidth = 4;\n   int laneHeight = 8;\n-  int quadWidth = laneWidth * vecWidth;\n+  int quadWidth = laneWidth * kWidth;\n   int quadHeight = laneHeight;\n   int numQuadI = 2;\n \n   // outer index base\n   Value iBase = udiv(lane, i32_val(laneWidth));\n \n-  for (int rep = 0; rep < numPtrs / (2 * vecWidth); ++rep)\n+  for (int rep = 0; rep < numPtrs / (2 * kWidth); ++rep)\n     for (int quadId = 0; quadId < 2; ++quadId)\n-      for (int elemId = 0; elemId < vecWidth; ++elemId) {\n-        int idx = rep * 2 * vecWidth + quadId * vecWidth + elemId;\n+      for (int elemId = 0; elemId < kWidth; ++elemId) {\n         // inner index base\n-        Value jBase = mul(urem(lane, i32_val(laneWidth)), i32_val(vecWidth));\n+        Value jBase = mul(urem(lane, i32_val(laneWidth)), i32_val(kWidth));\n         jBase = add(jBase, i32_val(elemId));\n         // inner index offset\n         Value jOff = i32_val(0);\n@@ -250,9 +249,17 @@ SmallVector<Value> MMA16816SmemLoader::computeLdsMatOffs(Value warpOff,\n         // To prevent out-of-bound access when tile is too small.\n         Value i = add(iBase, mul(iOff, i32_val(quadHeight)));\n         Value j = add(jBase, mul(jOff, i32_val(quadWidth)));\n-        // wrap around the bounds\n-        // i = urem(i, i32_val(cTileShape));\n-        // j = urem(j, i32_val(sTileShape));\n+        // Compute id of this ptr\n+        int idx = rep * 2 * kWidth;\n+        if (needTrans) {\n+          idx += quadId * vecWidth;\n+          idx += elemId % vecWidth;\n+          idx += elemId / vecWidth * kWidth;\n+        } else {\n+          idx += quadId * kWidth;\n+          idx += elemId;\n+        }\n+\n         if (needTrans) {\n           offs[idx] = add(i, mul(j, stridedSmemOffset));\n         } else {\n@@ -274,7 +281,7 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> ptrs, Type matTy,\n   if (canUseLdmatrix)\n     ptrIdx = matIdx[order[0]] / (instrShape[order[0]] / matShape[order[0]]);\n   else\n-    ptrIdx = matIdx[order[0]] * 4 / elemBytes;\n+    ptrIdx = matIdx[order[0]] * (needTrans ? kWidth : vecWidth);\n \n   // The main difference with the original triton code is we removed the\n   // prefetch-related logic here for the upstream optimizer phase should\n@@ -323,11 +330,8 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> ptrs, Type matTy,\n     return {extract_val(elemTy, resV4, 0), extract_val(elemTy, resV4, 1),\n             extract_val(elemTy, resV4, 2), extract_val(elemTy, resV4, 3)};\n   } else {\n-    if (needTrans && (4 / elemBytes) != kWidth)\n-      llvm_unreachable(\"unimplemented Shared -> DotOperandMmav2 code path\");\n     // base pointers\n     std::array<std::array<Value, 4>, 2> ptrs;\n-    int vecWidth = 4 / elemBytes;\n     for (int i = 0; i < vecWidth; i++)\n       ptrs[0][i] = getPtr(ptrIdx + i);\n     for (int i = 0; i < vecWidth; i++)\n@@ -336,7 +340,8 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> ptrs, Type matTy,\n     int _i0 = matIdx[order[1]] * (stridedLoadMatOffset * stridedMatShape);\n     int _i1 = _i0;\n     if (needTrans)\n-      _i1 += stridedLoadMatOffset * stridedMatShape;\n+      _i1 += (kWidth != vecWidth) ? vecWidth\n+                                  : stridedLoadMatOffset * stridedMatShape;\n     else\n       _i1 += (kOrder == 1 ? 1 : stridedLoadMatOffset) * stridedMatShape;\n     Value i0 = mul(i32_val(_i0), stridedSmemOffset);\n@@ -345,9 +350,11 @@ MMA16816SmemLoader::loadX4(int mat0, int mat1, ArrayRef<Value> ptrs, Type matTy,\n     // load 4 32-bit values from shared memory\n     // (equivalent to ldmatrix.x4)\n     SmallVector<SmallVector<Value>> vptrs(4, SmallVector<Value>(vecWidth));\n+\n     for (int i = 0; i < 4; ++i)\n-      for (int j = 0; j < vecWidth; ++j)\n+      for (int j = 0; j < vecWidth; ++j) {\n         vptrs[i][j] = gep(shemPtrTy, ptrs[i / 2][j], ii[i % 2]);\n+      }\n     // row + trans and col + no-trans are equivalent\n     bool isActualTrans =\n         (needTrans && kOrder == 1) || (!needTrans && kOrder == 0);\n@@ -398,13 +405,14 @@ MMA16816SmemLoader::MMA16816SmemLoader(\n       ctx(rewriter.getContext()) {\n   contiguousMatShape = matShape[order[0]];\n   stridedMatShape = matShape[order[1]];\n-\n   stridedSmemOffset = smemStrides[order[1]];\n+  vecWidth = 4 / elemBytes;\n \n   // rule: k must be the fast-changing axis.\n   needTrans = kOrder != order[0];\n   canUseLdmatrix = elemBytes == 2 || (!needTrans);\n-  canUseLdmatrix = canUseLdmatrix && (kWidth == 4 / elemBytes);\n+  canUseLdmatrix = canUseLdmatrix && (kWidth == vecWidth);\n+  // canUseLdmatrix = false;\n \n   if (canUseLdmatrix) {\n     // Each CTA, the warps is arranged as [1xwarpsPerTile] if not transposed,\n@@ -414,7 +422,7 @@ MMA16816SmemLoader::MMA16816SmemLoader(\n   } else {\n     numPtrs = tileShape[order[0]] / (needTrans ? warpsPerTile : 1) /\n               matShape[order[0]];\n-    numPtrs *= 4 / elemBytes;\n+    numPtrs *= kWidth;\n   }\n   numPtrs = std::max<int>(numPtrs, 2);\n \n@@ -488,9 +496,21 @@ std::function<void(int, int)> getLoadMatrixFn(\n   auto sharedLayout = tensorTy.getEncoding().cast<SharedEncodingAttr>();\n   const int perPhase = sharedLayout.getPerPhase();\n   const int maxPhase = sharedLayout.getMaxPhase();\n+  const int vecPhase = sharedLayout.getVec();\n   const int elemBytes = tensorTy.getElementTypeBitWidth() / 8;\n   auto order = sharedLayout.getOrder();\n \n+  if (tensor.getType()\n+          .cast<RankedTensorType>()\n+          .getElementType()\n+          .isa<mlir::Float8E4M3B11FNUZType>()) {\n+    bool noTrans = (isA ^ order[0] == 0);\n+    assert(noTrans && \"float8e4b15 must have row-col layout\");\n+  }\n+\n+  if (kWidth != (4 / elemBytes))\n+    assert(vecPhase == 1 || vecPhase == 4 * kWidth);\n+\n   // (a, b) is the coordinate.\n   auto load = [=, &rewriter, &vals](int a, int b) {\n     MMA16816SmemLoader loader("}, {"filename": "lib/Conversion/TritonGPUToLLVM/ElementwiseOpToLLVM.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "file_content_changes": "@@ -342,7 +342,6 @@ static SmallVector<Value> reorderValues(const SmallVector<Value> &values,\n     //   ret.push_back(values[i + 14]);\n     //   ret.push_back(values[i + 15]);\n     // }\n-    return values;\n   }\n   llvm_unreachable(\"unimplemented code path\");\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/LoadStoreOpToLLVM.cpp", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "file_content_changes": "@@ -782,7 +782,9 @@ struct InsertSliceAsyncOpConversion\n     // start of the vector and the other pointer moving to the next vector.\n     unsigned inVec = getContiguity(src);\n     unsigned outVec = resSharedLayout.getVec();\n-    unsigned minVec = std::min(outVec, inVec);\n+    unsigned minVec = inVec;\n+    if (outVec > 1)\n+      minVec = std::min(outVec, inVec);\n     unsigned numElems = getTotalElemsPerThread(srcTy);\n     unsigned perPhase = resSharedLayout.getPerPhase();\n     unsigned maxPhase = resSharedLayout.getMaxPhase();"}, {"filename": "lib/Conversion/TritonGPUToLLVM/TritonGPUToLLVMPass.cpp", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "file_content_changes": "@@ -566,7 +566,9 @@ class ConvertTritonGPUToLLVM\n         inVec =\n             std::min<unsigned>(axisInfoAnalysis.getMaskAlignment(mask), inVec);\n       unsigned outVec = resSharedLayout.getVec();\n-      unsigned minVec = std::min(outVec, inVec);\n+      unsigned minVec = inVec;\n+      if (outVec > 1)\n+        minVec = std::min(outVec, inVec);\n       auto maxBitWidth =\n           std::max<unsigned>(128, resElemTy.getIntOrFloatBitWidth());\n       auto vecBitWidth = resElemTy.getIntOrFloatBitWidth() * minVec;\n@@ -577,8 +579,9 @@ class ConvertTritonGPUToLLVM\n       // capability does not support async copy, then we do decompose\n       if (triton::gpu::InsertSliceAsyncOp::getEligibleLoadByteWidth(\n               computeCapability)\n-              .contains(byteWidth))\n+              .contains(byteWidth)) {\n         return;\n+      }\n \n       // load\n       auto tmpTy ="}, {"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 36, "deletions": 19, "changes": 55, "file_content_changes": "@@ -1,3 +1,4 @@\n+#include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"triton/Analysis/Utility.h\"\n@@ -76,6 +77,29 @@ class BlockedToMMA : public mlir::RewritePattern {\n   int computeCapability;\n   mutable int mmaV1Counter{}; // used to generate ID for MMAv1 encoding\n \n+  static bool bwdFilter(Operation *op) {\n+    return op->getNumOperands() == 1 &&\n+           (isa<triton::FpToFpOp, triton::BitcastOp,\n+                triton::gpu::ConvertLayoutOp>(op) ||\n+            op->getDialect()->getTypeID() ==\n+                mlir::TypeID::get<arith::ArithDialect>());\n+  }\n+\n+  // finds the first different value bitwidth in the chain of\n+  // shape-preserving unary ops  that x depends on\n+  static int computeOrigBitWidth(Value x) {\n+    int finalBitWidth = getElementTypeOrSelf(x).getIntOrFloatBitWidth();\n+    int origBitWidth = finalBitWidth;\n+    SetVector<Operation *> slice;\n+    mlir::getBackwardSlice(x, &slice, bwdFilter);\n+    Operation *firstOp = slice.empty() ? nullptr : *slice.begin();\n+    if (firstOp)\n+      if (Value arg = firstOp->getOperand(0))\n+        if (RankedTensorType argTy = arg.getType().dyn_cast<RankedTensorType>())\n+          origBitWidth = argTy.getElementType().getIntOrFloatBitWidth();\n+    return origBitWidth;\n+  }\n+\n public:\n   BlockedToMMA(mlir::MLIRContext *context, int computeCapability)\n       : mlir::RewritePattern(triton::DotOp::getOperationName(), 2, context),\n@@ -87,6 +111,7 @@ class BlockedToMMA : public mlir::RewritePattern {\n     if (computeCapability < 70)\n       return failure();\n     auto dotOp = cast<triton::DotOp>(op);\n+    auto ctx = op->getContext();\n     // TODO: Check data-types and SM compatibility\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n     if (!oldRetType.getEncoding() ||\n@@ -151,36 +176,28 @@ class BlockedToMMA : public mlir::RewritePattern {\n     }\n     auto newRetType =\n         RankedTensorType::get(retShape, oldRetType.getElementType(), mmaEnc);\n-\n     // convert accumulator\n     auto oldAcc = dotOp.getOperand(2);\n     auto newAcc = rewriter.create<triton::gpu::ConvertLayoutOp>(\n         oldAcc.getLoc(), newRetType, oldAcc);\n-    auto oldAOrder = oldAType.getEncoding()\n-                         .cast<triton::gpu::DotOperandEncodingAttr>()\n-                         .getParent()\n-                         .cast<triton::gpu::BlockedEncodingAttr>()\n-                         .getOrder();\n-    auto oldBOrder = oldBType.getEncoding()\n-                         .cast<triton::gpu::DotOperandEncodingAttr>()\n-                         .getParent()\n-                         .cast<triton::gpu::BlockedEncodingAttr>()\n-                         .getOrder();\n-\n+    // convert operands\n+    int minBitwidth = std::min(computeOrigBitWidth(a), computeOrigBitWidth(b));\n+    Type minType = IntegerType::get(ctx, minBitwidth);\n+    // convert A operand\n     auto newAEncoding = triton::gpu::DotOperandEncodingAttr::get(\n         oldAType.getContext(), 0, newRetType.getEncoding(),\n-        oldAType.getElementType());\n-    auto newBEncoding = triton::gpu::DotOperandEncodingAttr::get(\n-        oldBType.getContext(), 1, newRetType.getEncoding(),\n-        oldBType.getElementType());\n-\n+        minBitwidth > 0 ? minType : oldAType.getElementType());\n     auto newAType = RankedTensorType::get(\n         oldAType.getShape(), oldAType.getElementType(), newAEncoding);\n+    a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n+    // convert B operand\n+    auto newBEncoding = triton::gpu::DotOperandEncodingAttr::get(\n+        oldBType.getContext(), 1, newRetType.getEncoding(),\n+        minBitwidth > 0 ? minType : oldBType.getElementType());\n     auto newBType = RankedTensorType::get(\n         oldBType.getShape(), oldBType.getElementType(), newBEncoding);\n-\n-    a = rewriter.create<triton::gpu::ConvertLayoutOp>(a.getLoc(), newAType, a);\n     b = rewriter.create<triton::gpu::ConvertLayoutOp>(b.getLoc(), newBType, b);\n+    // convert dot instruction\n     auto newDot = rewriter.create<triton::DotOp>(\n         dotOp.getLoc(), newRetType, a, b, newAcc, dotOp.getAllowTF32());\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 58, "deletions": 244, "changes": 302, "file_content_changes": "@@ -1,3 +1,4 @@\n+#include \"mlir/IR/TypeUtilities.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n@@ -73,260 +74,77 @@ class ConvertTransConvert : public mlir::RewritePattern {\n   }\n };\n \n-//\n-\n+// convert(layout_preserving_op(x), dot_operand)\n+// -> layout_preserving_op(convert(x, dot_operand))\n class MoveOpAfterLayoutConversion : public mlir::RewritePattern {\n-\n public:\n   MoveOpAfterLayoutConversion(mlir::MLIRContext *context)\n-      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, context) {}\n-\n-  static mlir::LogicalResult\n-  isBlockedToDotOperand(mlir::Operation *op,\n-                        triton::gpu::DotOperandEncodingAttr &retEncoding,\n-                        triton::gpu::BlockedEncodingAttr &srcEncoding) {\n-    auto cvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(op);\n-    if (!cvt)\n-      return failure();\n-    auto srcTy = cvt.getOperand().getType().cast<RankedTensorType>();\n-    auto retTy = cvt.getResult().getType().dyn_cast<RankedTensorType>();\n-    retEncoding =\n-        retTy.getEncoding().dyn_cast<triton::gpu::DotOperandEncodingAttr>();\n-    srcEncoding =\n-        srcTy.getEncoding().dyn_cast<triton::gpu::BlockedEncodingAttr>();\n-    if (!retTy)\n-      return failure();\n-    if (!retEncoding)\n-      return failure();\n-    auto retEncodingParent =\n-        retEncoding.getParent().dyn_cast<triton::gpu::MmaEncodingAttr>();\n-    if (!retEncodingParent || retEncodingParent.isVolta())\n-      return failure();\n-    if (!srcEncoding)\n-      return failure();\n-    return success();\n-  }\n-\n-  static bool isTrans(const triton::gpu::DotOperandEncodingAttr &retEncoding,\n-                      const triton::gpu::BlockedEncodingAttr &srcEncoding) {\n-    int kOrder = retEncoding.getOpIdx() ^ 1;\n-    return kOrder != srcEncoding.getOrder()[0];\n-  }\n-\n-  static bool isDotNT(triton::DotOp dotOp) {\n-    triton::gpu::DotOperandEncodingAttr aRetEncoding;\n-    triton::gpu::DotOperandEncodingAttr bRetEncoding;\n-    triton::gpu::BlockedEncodingAttr aSrcEncoding;\n-    triton::gpu::BlockedEncodingAttr bSrcEncoding;\n-    if (isBlockedToDotOperand(dotOp.getOperand(0).getDefiningOp(), aRetEncoding,\n-                              aSrcEncoding)\n-            .failed())\n-      return false;\n-    if (isBlockedToDotOperand(dotOp.getOperand(1).getDefiningOp(), bRetEncoding,\n-                              bSrcEncoding)\n-            .failed())\n-      return false;\n-    if (!aRetEncoding || !bRetEncoding || !aSrcEncoding || !bSrcEncoding)\n-      return false;\n-    return !isTrans(aRetEncoding, aSrcEncoding) &&\n-           !isTrans(bRetEncoding, bSrcEncoding);\n-  }\n+      : mlir::RewritePattern(triton::gpu::ConvertLayoutOp::getOperationName(),\n+                             1, context) {}\n \n-  mlir::LogicalResult\n+  LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    auto dotOp = cast<triton::DotOp>(op);\n-    // only supports dot NT\n-    if (!isDotNT(dotOp))\n-      return failure();\n-    bool changed = false;\n-    for (Value operand : {dotOp.getOperand(0), dotOp.getOperand(1)}) {\n-      auto cvt = operand.getDefiningOp<triton::gpu::ConvertLayoutOp>();\n-      triton::gpu::DotOperandEncodingAttr retEncoding;\n-      triton::gpu::BlockedEncodingAttr srcEncoding;\n-      bool failed =\n-          isBlockedToDotOperand(cvt, retEncoding, srcEncoding).failed();\n-      assert(!failed);\n-\n-      // don't move things around when cvt operand is a block arg\n-      Operation *argOp = cvt.getOperand().getDefiningOp();\n-      if (!argOp)\n-        continue;\n-      SetVector<Operation *> processed;\n-      SetVector<Attribute> layout;\n-      llvm::MapVector<Value, Attribute> toConvert;\n-      int numCvts = simulateBackwardRematerialization(cvt, processed, layout,\n-                                                      toConvert, retEncoding);\n-      if (numCvts > 1 || toConvert.size() == 1)\n+    auto cvt = cast<triton::gpu::ConvertLayoutOp>(op);\n+    // conversion should be dependent on a load\n+    // and all operations between the load and the conversion\n+    // should be layout preserving\n+    SetVector<Operation *> slice;\n+    getBackwardSlice(op, &slice);\n+    int loadIdx = -1;\n+    bool checkOp = false;\n+    for (int i = 0; i < slice.size(); i++) {\n+      Operation *currOp = *(slice.begin() + i);\n+      if (currOp->getParentRegion() != op->getParentRegion())\n         continue;\n-      bool replaceOperand = true;\n-      for (Operation *op : processed) {\n-        if (op->getNumOperands() != 1)\n-          continue;\n-        auto srcTy = op->getOperand(0).getType().cast<RankedTensorType>();\n-        auto dstTy = op->getResult(0).getType().cast<RankedTensorType>();\n-        // we don't want to push conversions backward if there is a downcast\n-        // since it would result in more shared memory traffic\n-        if (srcTy.getElementType().getIntOrFloatBitWidth() >\n-            dstTy.getElementType().getIntOrFloatBitWidth()) {\n-          replaceOperand = false;\n-          break;\n-        }\n-        // we only push back when the first op in the chain has a load operand\n-        if ((op == processed.back()) &&\n-            !isa<triton::LoadOp>(op->getOperand(0).getDefiningOp())) {\n-          replaceOperand = false;\n-          break;\n-        }\n-        // we don't want to use ldmatrix for 8-bit data that requires trans\n-        // since Nvidia GPUs can't do it efficiently\n-        int kOrder = retEncoding.getOpIdx() ^ 1;\n-        bool isTrans = kOrder != srcEncoding.getOrder()[0];\n-        bool isInt8 = srcTy.getElementType().getIntOrFloatBitWidth() == 8;\n-        if (isTrans && isInt8) {\n-          replaceOperand = false;\n-          break;\n-        }\n+      if (isa<triton::LoadOp>(currOp))\n+        checkOp = true;\n+      else if (checkOp) {\n+        if (!isa<triton::FpToFpOp, triton::BitcastOp>(currOp) &&\n+            currOp->getDialect()->getTypeID() !=\n+                mlir::TypeID::get<arith::ArithDialect>())\n+          return mlir::failure();\n       }\n-      if (!replaceOperand)\n-        continue;\n-      IRMapping mapping;\n-      rematerializeConversionChain(toConvert, rewriter, processed, mapping);\n-      rewriter.replaceOp(cvt, mapping.lookup(cvt->getOperand(0)));\n-      changed = true;\n     }\n-    return mlir::success(changed);\n+    if (!checkOp)\n+      return mlir::failure();\n+\n+    auto cvtTy = cvt.getType().cast<RankedTensorType>();\n+    auto cvtArgOp = cvt.getSrc().getDefiningOp();\n+    if (!cvtArgOp || cvtArgOp->getNumOperands() == 0)\n+      return mlir::failure();\n+    // only consider custom conversions or arith ops\n+    if (!isa<triton::FpToFpOp, triton::BitcastOp>(cvtArgOp) &&\n+        cvtArgOp->getDialect()->getTypeID() !=\n+            mlir::TypeID::get<arith::ArithDialect>())\n+      return mlir::failure();\n+    // only considers conversions to dot operand\n+    if (!cvtTy.getEncoding().isa<triton::gpu::DotOperandEncodingAttr>())\n+      return mlir::failure();\n+    auto argTy = cvtArgOp->getOperand(0).getType().cast<RankedTensorType>();\n+    auto retTy = cvtArgOp->getResult(0).getType().cast<RankedTensorType>();\n+    if (!argTy || !retTy)\n+      return mlir::failure();\n+    Type newRetTy = RankedTensorType::get(\n+        retTy.getShape(), retTy.getElementType(), cvtTy.getEncoding());\n+    Type newCvtTy = RankedTensorType::get(\n+        retTy.getShape(), argTy.getElementType(), cvtTy.getEncoding());\n+    int numArgs = cvtArgOp->getNumOperands();\n+    SmallVector<triton::gpu::ConvertLayoutOp> newCvts(numArgs);\n+    for (int i = 0; i < numArgs; i++)\n+      newCvts[i] = rewriter.create<triton::gpu::ConvertLayoutOp>(\n+          cvt.getLoc(), newCvtTy, cvtArgOp->getOperand(i));\n+    auto newRet = rewriter.clone(*cvtArgOp);\n+    for (int i = 0; i < numArgs; i++)\n+      newRet->setOperand(i, newCvts[i]);\n+    newRet->getResult(0).setType(newRetTy);\n+    rewriter.replaceOp(op, newRet->getResults());\n+    return mlir::success();\n   }\n };\n \n } // namespace\n \n-static bool isConvertToDotEncoding(Operation *op) {\n-  auto convertLayout = llvm::dyn_cast<ConvertLayoutOp>(op);\n-  if (!convertLayout)\n-    return false;\n-  auto tensorType =\n-      convertLayout.getResult().getType().cast<RankedTensorType>();\n-  return tensorType.getEncoding().isa<DotOperandEncodingAttr>();\n-}\n-\n-static ConvertLayoutOp updateConvert(OpBuilder &builder, ConvertLayoutOp cvt,\n-                                     IRMapping &mapping, Type smallestType) {\n-  auto cvtDstTy = cvt.getResult().getType().cast<RankedTensorType>();\n-  auto cvtDstEnc = cvtDstTy.getEncoding().cast<DotOperandEncodingAttr>();\n-  Value operand = cvt.getOperand();\n-  if (mapping.contains(operand))\n-    operand = mapping.lookup(operand);\n-  auto newDstTy = RankedTensorType::get(\n-      cvtDstTy.getShape(), cvtDstTy.getElementType(),\n-      DotOperandEncodingAttr::get(cvtDstEnc.getContext(), cvtDstEnc.getOpIdx(),\n-                                  cvtDstEnc.getParent(), smallestType));\n-  auto newCvt =\n-      builder.create<ConvertLayoutOp>(cvt.getLoc(), newDstTy, operand);\n-  mapping.map(cvt.getResult(), newCvt.getResult());\n-  return newCvt;\n-}\n-\n-// Update kWidth based on the smallestType found in the given convert ops and\n-// propagate the type change.\n-static void\n-updateDotEncodingLayout(SmallVector<ConvertLayoutOp> &convertsToDotEncoding,\n-                        Type smallestType) {\n-  IRMapping mapping;\n-  OpBuilder builder(smallestType.getContext());\n-  SetVector<Operation *> slices(convertsToDotEncoding.begin(),\n-                                convertsToDotEncoding.end());\n-  // Collect all the operations where the type needs to be propagated.\n-  for (auto cvt : convertsToDotEncoding) {\n-    auto forwardFilter = [&](Operation *op) {\n-      if (op == cvt.getOperation())\n-        return true;\n-      for (Value operand : op->getOperands()) {\n-        auto tensorType = operand.getType().dyn_cast<RankedTensorType>();\n-        if (tensorType &&\n-            tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n-          return true;\n-      }\n-      return false;\n-    };\n-    auto backwardFilter = [&](Operation *op) {\n-      for (Value results : op->getResults()) {\n-        auto tensorType = results.getType().dyn_cast<RankedTensorType>();\n-        if (tensorType &&\n-            tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n-          return true;\n-      }\n-      return false;\n-    };\n-    SetVector<Operation *> opSlice =\n-        getSlice(cvt.getOperation(), {backwardFilter}, {forwardFilter});\n-    slices.insert(opSlice.begin(), opSlice.end());\n-  }\n-  // Apply the type change by walking ops in topological order.\n-  slices = mlir::topologicalSort(slices);\n-  for (Operation *op : slices) {\n-    builder.setInsertionPoint(op);\n-    if (isConvertToDotEncoding(op)) {\n-      auto cvt = cast<ConvertLayoutOp>(op);\n-      ConvertLayoutOp newCvt =\n-          updateConvert(builder, cvt, mapping, smallestType);\n-      continue;\n-    }\n-    auto *newOp = cloneWithInferType(builder, op, mapping);\n-    for (auto [result, newResult] :\n-         llvm::zip(op->getResults(), newOp->getResults())) {\n-      result.replaceUsesWithIf(newResult, [&](OpOperand &operand) {\n-        return slices.count(operand.getOwner()) == 0;\n-      });\n-    }\n-  }\n-  for (Operation *op : llvm::reverse(slices))\n-    op->erase();\n-}\n-\n-// Change the layout of dotOperand layout to use the kWidth from the smallest\n-// loaded type. This allows better code generation for mixed-mode matmul.\n-static void optimizeKWidth(triton::FuncOp func) {\n-  SmallVector<ConvertLayoutOp> convertsToDotEncoding;\n-  Type smallestType;\n-  func->walk([&](triton::LoadOp loadOp) {\n-    if (!loadOp.getResult().hasOneUse())\n-      return;\n-    Operation *use = *loadOp.getResult().getUsers().begin();\n-\n-    // Advance to the first conversion as long as the use resides in shared\n-    // memory and it has a single use itself\n-    while (use) {\n-      if (use->getNumResults() != 1 || !use->getResult(0).hasOneUse())\n-        break;\n-      auto tensorType =\n-          use->getResult(0).getType().dyn_cast<RankedTensorType>();\n-      if (!tensorType || !tensorType.getEncoding().isa<SharedEncodingAttr>())\n-        break;\n-      use = *use->getResult(0).getUsers().begin();\n-    }\n-\n-    auto convertLayout = llvm::dyn_cast<ConvertLayoutOp>(use);\n-    if (!convertLayout)\n-      return;\n-    auto tensorType =\n-        convertLayout.getResult().getType().cast<RankedTensorType>();\n-    if (!tensorType.getEncoding().isa<DotOperandEncodingAttr>())\n-      return;\n-    convertsToDotEncoding.push_back(convertLayout);\n-\n-    // Update the smallest type.\n-    auto ty = loadOp.getType().cast<RankedTensorType>();\n-    Type eltTy = ty.getElementType();\n-    if (!smallestType ||\n-        (eltTy.getIntOrFloatBitWidth() < smallestType.getIntOrFloatBitWidth()))\n-      smallestType = eltTy;\n-  });\n-  if (!smallestType)\n-    return;\n-  updateDotEncodingLayout(convertsToDotEncoding, smallestType);\n-}\n-\n #define GEN_PASS_CLASSES\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h.inc\"\n \n@@ -351,10 +169,6 @@ class TritonGPUOptimizeDotOperandsPass\n       signalPassFailure();\n     if (fixupLoops(m).failed())\n       signalPassFailure();\n-\n-    // Change the layout of dotOperand layout to use the kWidth from the\n-    // smallest loaded type.\n-    m->walk([](triton::FuncOp func) { optimizeKWidth(func); });\n   }\n };\n "}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -493,9 +493,7 @@ void LoopPipeliner::createBufferTypes() {\n     SmallVector<int64_t> bufferShape(ty.getShape().begin(),\n                                      ty.getShape().end());\n     bufferShape.insert(bufferShape.begin(), numStages);\n-    unsigned bitWidth = dotOpEnc.getMMAv2kWidth()\n-                            ? 32 / dotOpEnc.getMMAv2kWidth()\n-                            : ty.getElementType().getIntOrFloatBitWidth();\n+    unsigned bitWidth = ty.getElementType().getIntOrFloatBitWidth();\n     auto sharedEnc =\n         ttg::SharedEncodingAttr::get(ty.getContext(), dotOpEnc, ty.getShape(),\n                                      ttg::getOrder(ty.getEncoding()), bitWidth);"}, {"filename": "python/test/unit/operators/test_matmul.py", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "file_content_changes": "@@ -89,13 +89,21 @@ def kernel(Y, X, N, BLOCK_SIZE: tl.constexpr):\n                 (128, 256, 32, 1, 8, 2, None, None, None, AT, BT, ADTYPE, BDTYPE),\n                 (32, 64, 32, 1, 1, 2, 64, 128, 32, AT, BT, ADTYPE, BDTYPE),\n                 (128, 128, 32, 8, 4, 2, 256, 256, 128, AT, BT, ADTYPE, BDTYPE),\n-            ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4\", \"float8e5\"),\n                                      (\"float8e4\", \"float16\"),\n                                      (\"float16\", \"float8e5\"),\n                                      (\"float16\", \"float32\"),\n                                      (\"float32\", \"float16\"),\n                                      (\"bfloat16\", \"float32\"),\n                                      (\"float32\", \"bfloat16\")] for AT in [False, True] for BT in [False, True]\n+        ],\n+        *[\n+            # float8e4b15 only supports row-col layout\n+            [\n+                (128, 128, 32, 1, 4, 2, None, None, None, False, True, ADTYPE, BDTYPE),\n+            ] for ADTYPE, BDTYPE in [(\"float8e4b15\", \"float8e5\"),\n+                                     (\"float8e4b15\", \"float16\"),\n+                                     (\"float16\", \"float8e4b15\")]\n         ]\n     ),\n )\n@@ -132,7 +140,7 @@ def init_input(n, m, t, dtype, is_float8):\n         if t:\n             return init_input(m, n, False, dtype, is_float8).t()\n         if is_float8:\n-            return torch.randint(20, 60, (n, m), device=\"cuda\", dtype=torch.int8)\n+            return torch.randint(20, 50, (n, m), device=\"cuda\", dtype=torch.int8)\n         dtype = {\"float16\": torch.float16, \"bfloat16\": torch.bfloat16, \"float32\": torch.float32}[dtype]\n         return .1 * torch.randn((n, m), device=\"cuda\", dtype=dtype)\n "}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 26, "deletions": 112, "changes": 138, "file_content_changes": "@@ -1,8 +1,12 @@\n // RUN: triton-opt %s -split-input-file -tritongpu-optimize-dot-operands -tritongpu-remove-layout-conversions -canonicalize | FileCheck %s\n \n #Cv2 = #triton_gpu.mma<{versionMajor = 2, warpsPerCTA = [4, 1]}>\n-#Av2 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv2, kWidth=2}>\n-#Bv2 = #triton_gpu.dot_op<{opIdx = 1, parent = #Cv2, kWidth=2}>\n+#Av2k1 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv2, kWidth=1}>\n+#Bv2k1 = #triton_gpu.dot_op<{opIdx = 1, parent = #Cv2, kWidth=1}>\n+#Av2k2 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv2, kWidth=2}>\n+#Bv2k2 = #triton_gpu.dot_op<{opIdx = 1, parent = #Cv2, kWidth=2}>\n+#Av2k4 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv2, kWidth=4}>\n+#Bv2k4 = #triton_gpu.dot_op<{opIdx = 1, parent = #Cv2, kWidth=4}>\n #Cv1 = #triton_gpu.mma<{versionMajor = 1, warpsPerCTA = [4, 1]}>\n #Av1 = #triton_gpu.dot_op<{opIdx = 0, parent = #Cv1}>\n #Bv1 = #triton_gpu.dot_op<{opIdx = 1, parent = #Cv1}>\n@@ -13,7 +17,7 @@\n \n module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n-// CHECK: tt.func @push_elementwise1\n+// CHECK: tt.func @push_elementwise\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n // CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n // CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ACVT]] {{.*}} #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>>\n@@ -22,111 +26,21 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n // CHECK: %[[C:.*]] = tt.dot %[[AF16]], %[[BCVT]]\n // CHECK-SAME: tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 4}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 4}>> -> tensor<16x16xf32, #mma>\n // CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n-tt.func @push_elementwise1(\n+tt.func @push_elementwise(\n                    %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n                    %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n                    %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n   %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n   %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n   %af8 = tt.bitcast %ai8: tensor<16x16xi8, #ALR> -> tensor<16x16xf8E5M2, #ALR>\n   %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #ALR> -> tensor<16x16xf16, #ALR>\n-  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALR>) -> tensor<16x16xf16, #Av2>\n-  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2>\n-  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n+  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALR>) -> tensor<16x16xf16, #Av2k4>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2k4>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2k4> * tensor<16x16xf16, #Bv2k4> -> tensor<16x16xf32, #Cv2>\n   tt.return %newc : tensor<16x16xf32, #Cv2>\n }\n \n \n-// Not modified for row-row\n-// CHECK: tt.func @push_elementwise2\n-// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n-// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ALOAD]]\n-// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n-// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[AF16]]\n-// CHECK: %[[C:.*]] = tt.dot %[[ACVT]]\n-// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n-tt.func @push_elementwise2(\n-                   %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %pb: tensor<16x16x!tt.ptr<f16>, #BLR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n-  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n-  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLR>\n-  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #ALR> -> tensor<16x16xf8E5M2, #ALR>\n-  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #ALR> -> tensor<16x16xf16, #ALR>\n-  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALR>) -> tensor<16x16xf16, #Av2>\n-  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLR>) -> tensor<16x16xf16, #Bv2>\n-  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n-  tt.return %newc : tensor<16x16xf32, #Cv2>\n-}\n-\n-\n-// Not modified for col-row\n-// CHECK: tt.func @push_elementwise3\n-// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n-// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ALOAD]]\n-// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n-// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[AF16]]\n-// CHECK: %[[C:.*]] = tt.dot %[[ACVT]]\n-// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n-tt.func @push_elementwise3(\n-                   %pa: tensor<16x16x!tt.ptr<i8>, #ALC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %pb: tensor<16x16x!tt.ptr<f16>, #BLR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n-  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALC>\n-  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLR>\n-  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #ALC> -> tensor<16x16xf8E5M2, #ALC>\n-  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #ALC> -> tensor<16x16xf16, #ALC>\n-  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALC>) -> tensor<16x16xf16, #Av2>\n-  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLR>) -> tensor<16x16xf16, #Bv2>\n-  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n-  tt.return %newc : tensor<16x16xf32, #Cv2>\n-}\n-\n-// Not modified for col-col\n-// CHECK: tt.func @push_elementwise4\n-// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n-// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ALOAD]]\n-// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n-// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[AF16]]\n-// CHECK: %[[C:.*]] = tt.dot %[[ACVT]]\n-// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma>\n-tt.func @push_elementwise4(\n-                   %pa: tensor<16x16x!tt.ptr<i8>, #ALC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n-  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALC>\n-  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n-  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #ALC> -> tensor<16x16xf8E5M2, #ALC>\n-  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #ALC> -> tensor<16x16xf16, #ALC>\n-  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALC>) -> tensor<16x16xf16, #Av2>\n-  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2>\n-  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n-  tt.return %newc : tensor<16x16xf32, #Cv2>\n-}\n-\n-\n-// Not modified for Volta\n-// CHECK: tt.func @push_elementwise5\n-// CHECK: %[[ALOAD:.*]] = tt.load %arg0\n-// CHECK: %[[AF8E5:.*]] = tt.bitcast %[[ALOAD]]\n-// CHECK: %[[AF16:.*]] = tt.fp_to_fp %[[AF8E5]]\n-// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[AF16]]\n-// CHECK: %[[C:.*]] = tt.dot %[[ACVT]]\n-// CHECK: tt.return %[[C]] : tensor<16x16xf32, #mma1>\n-tt.func @push_elementwise5(\n-                   %pa: tensor<16x16x!tt.ptr<i8>, #ALR> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n-                   %c: tensor<16x16xf32, #Cv1>) -> tensor<16x16xf32, #Cv1>{\n-  %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n-  %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n-  %af8 = tt.bitcast %ai8: tensor<16x16xi8, #ALR> -> tensor<16x16xf8E5M2, #ALR>\n-  %a = tt.fp_to_fp %af8: tensor<16x16xf8E5M2, #ALR> -> tensor<16x16xf16, #ALR>\n-  %dota = triton_gpu.convert_layout %a : (tensor<16x16xf16, #ALR>) -> tensor<16x16xf16, #Av1>\n-  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv1>\n-  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av1> * tensor<16x16xf16, #Bv1> -> tensor<16x16xf32, #Cv1>\n-  tt.return %newc : tensor<16x16xf32, #Cv1>\n-}\n-\n // CHECK: tt.func @succeeds_if_arg_is_not_convert_layout\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n // CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n@@ -139,12 +53,12 @@ tt.func @succeeds_if_arg_is_not_convert_layout(\n                    %pb: tensor<16x16x!tt.ptr<f16>, #BLC> {tt.divisibility=16: i32, tt.contiguity=2 : i32},\n                    %c: tensor<16x16xf32, #Cv2>) -> tensor<16x16xf32, #Cv2>{\n   %ai8 = tt.load %pa {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xi8, #ALR>\n-  %dotai8 = triton_gpu.convert_layout %ai8 : (tensor<16x16xi8, #ALR>) -> tensor<16x16xi8, #Av2>\n+  %dotai8 = triton_gpu.convert_layout %ai8 : (tensor<16x16xi8, #ALR>) -> tensor<16x16xi8, #Av2k4>\n   %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #BLC>\n-  %dotaf8 = tt.bitcast %dotai8 : tensor<16x16xi8, #Av2> -> tensor<16x16xf8E5M2, #Av2>\n-  %dota = tt.fp_to_fp %dotaf8 : tensor<16x16xf8E5M2, #Av2> -> tensor<16x16xf16, #Av2>\n-  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2>\n-  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2> * tensor<16x16xf16, #Bv2> -> tensor<16x16xf32, #Cv2>\n+  %dotaf8 = tt.bitcast %dotai8 : tensor<16x16xi8, #Av2k4> -> tensor<16x16xf8E5M2, #Av2k4>\n+  %dota = tt.fp_to_fp %dotaf8 : tensor<16x16xf8E5M2, #Av2k4> -> tensor<16x16xf16, #Av2k4>\n+  %dotb = triton_gpu.convert_layout %b : (tensor<16x16xf16, #BLC>) -> tensor<16x16xf16, #Bv2k4>\n+  %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av2k4> * tensor<16x16xf16, #Bv2k4> -> tensor<16x16xf32, #Cv2>\n   tt.return %newc : tensor<16x16xf32, #Cv2>\n }\n \n@@ -163,10 +77,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n \n // CHECK: tt.func @push_convert_both_operands\n // CHECK: %[[ALOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BA]]>\n-// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] : (tensor<16x16xf16, #[[BA]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n // CHECK: %[[BLOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BB]]>\n-// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %[[BLOAD]] : (tensor<16x16xf16, #[[BB]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] : (tensor<16x16xf16, #[[BA]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n // CHECK: %[[AEXT:.*]] = arith.extf %[[ACVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %[[BLOAD]] : (tensor<16x16xf16, #[[BB]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n // CHECK: %[[BEXT:.*]] = arith.extf %[[BCVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n // CHECK: tt.dot %[[AEXT]], %[[BEXT]], %{{.*}} {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> -> tensor<16x16xf32, #mma>\n tt.func @push_convert_both_operands(\n@@ -177,9 +91,9 @@ tt.func @push_convert_both_operands(\n   %b = tt.load %pb {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #blockedB>\n   %ae = arith.extf %a : tensor<16x16xf16, #blockedA> to tensor<16x16xf32, #blockedA>\n   %be = arith.extf %b : tensor<16x16xf16, #blockedB> to tensor<16x16xf32, #blockedB>\n-  %al = triton_gpu.convert_layout %ae : (tensor<16x16xf32, #blockedA>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n-  %bl = triton_gpu.convert_layout %be : (tensor<16x16xf32, #blockedB>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n-  %r = tt.dot %al, %bl, %c {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<16x16xf32, #mma>\n+  %al = triton_gpu.convert_layout %ae : (tensor<16x16xf32, #blockedA>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>>\n+  %bl = triton_gpu.convert_layout %be : (tensor<16x16xf32, #blockedB>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>>\n+  %r = tt.dot %al, %bl, %c {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<16x16xf32, #mma>\n   tt.return %r : tensor<16x16xf32, #mma>\n }\n \n@@ -199,10 +113,10 @@ module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n // CHECK: tt.func @update_kwidth_slice\n // CHECK: %[[CST:.+]] = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n // CHECK: %[[ALOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BA]]>\n-// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] : (tensor<16x16xf16, #[[BA]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n // CHECK: %[[BLOAD:.*]] = tt.load %{{.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<16x16xf16, #[[BB]]>\n-// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %[[BLOAD]] : (tensor<16x16xf16, #[[BB]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]] : (tensor<16x16xf16, #[[BA]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n // CHECK: %[[AEXT:.*]] = arith.extf %[[ACVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>>\n+// CHECK: %[[BCVT:.*]] = triton_gpu.convert_layout %[[BLOAD]] : (tensor<16x16xf16, #[[BB]]>) -> tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n // CHECK: %[[BEXT:.*]] = arith.extf %[[BCVT]] : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> to tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n // CHECK: %[[ADD:.+]] = arith.addf %[[BEXT]], %[[CST]] : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>>\n // CHECK: tt.dot %[[AEXT]], %[[ADD]], %{{.*}} {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #[[MMA]], kWidth = 2}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #[[MMA]], kWidth = 2}>> -> tensor<16x16xf32, #mma>\n@@ -216,9 +130,9 @@ tt.func @update_kwidth_slice(\n   %ae = arith.extf %a : tensor<16x16xf16, #blockedA> to tensor<16x16xf32, #blockedA>\n   %be = arith.extf %b : tensor<16x16xf16, #blockedB> to tensor<16x16xf32, #blockedB>\n   %add = arith.addf %be, %cst : tensor<16x16xf32, #blockedB>\n-  %al = triton_gpu.convert_layout %ae : (tensor<16x16xf32, #blockedA>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>>\n-  %bl = triton_gpu.convert_layout %add : (tensor<16x16xf32, #blockedB>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>>\n-  %r = tt.dot %al, %bl, %c {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 1}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 1}>> -> tensor<16x16xf32, #mma>\n+  %al = triton_gpu.convert_layout %ae : (tensor<16x16xf32, #blockedA>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>>\n+  %bl = triton_gpu.convert_layout %add : (tensor<16x16xf32, #blockedB>) -> tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>>\n+  %r = tt.dot %al, %bl, %c {allowTF32 = true} : tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 0, parent = #mma, kWidth = 2}>> * tensor<16x16xf32, #triton_gpu.dot_op<{opIdx = 1, parent = #mma, kWidth = 2}>> -> tensor<16x16xf32, #mma>\n   tt.return %r : tensor<16x16xf32, #mma>\n }\n "}, {"filename": "unittest/Dialect/TritonGPU/SwizzleTest.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "file_content_changes": "@@ -29,8 +29,8 @@ TEST_P(SwizzleDotOperandTestFixture, DotOperands) {\n   ctx.loadDialect<triton::gpu::TritonGPUDialect>();\n   // create encoding\n   auto parent = triton::gpu::MmaEncodingAttr::get(&ctx, 2, 0, {1, 1});\n-  auto encoding =\n-      triton::gpu::DotOperandEncodingAttr::get(&ctx, params.opIdx, parent, 0);\n+  auto encoding = triton::gpu::DotOperandEncodingAttr::get(\n+      &ctx, params.opIdx, parent, 32 / params.typeWidth);\n \n   // create element type\n   Type eltType = IntegerType::get(&ctx, params.typeWidth);"}]