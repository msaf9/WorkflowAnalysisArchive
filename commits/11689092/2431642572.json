[{"filename": "python/triton/code_gen.py", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "file_content_changes": "@@ -63,7 +63,7 @@ def mangle_ty(ty):\n def mangle_fn(name, arg_tys, constants):\n     # doesn't mangle ret type, which must be a function of arg tys\n     mangled_arg_names = '_'.join([mangle_ty(ty) for ty in arg_tys])\n-    key = lambda x: x.cache_key if isinstance(x, JITFunction) else repr(x)\n+    key = lambda x: x.__name__ if isinstance(x, JITFunction) else repr(x)\n     mangled_constants = '_'.join([f'{i}c{key(constants[i])}' for i in sorted(constants)])\n     mangled_constants = mangled_constants.replace('.', '_d_')\n     mangled_constants = mangled_constants.replace(\"'\", '_sq_')\n@@ -971,6 +971,10 @@ def __call__(self, *wargs, grid, num_warps=4, num_stages=2, **kwargs):\n         #         assert arg.is_cuda, \"All tensors must be on GPU!\"\n         # set device (i.e., make sure torch has the context initialized)\n         device = torch.cuda.current_device()\n+        # torch creates new thread for backward pass that may have uninitlialized context\n+        # no way to know if this function should or shouldn't initialize the cuda context\n+        # so we're being conservative here\n+        torch.cuda.set_device(device)\n         if device not in self.cache_key:\n             cc = torch.cuda.get_device_capability(device)\n             cc = str(cc[0]) + '-' + str(cc[1])"}]