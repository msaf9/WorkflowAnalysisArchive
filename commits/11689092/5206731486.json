[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 15, "deletions": 1, "changes": 16, "file_content_changes": "@@ -87,7 +87,7 @@ template <typename Int> Int product(llvm::ArrayRef<Int> arr) {\n \n template <typename Int> Int ceil(Int m, Int n) { return (m + n - 1) / n; }\n \n-// output[i] = input[order[i]]\n+/// output[i] = input[order[i]]\n template <typename T, typename RES_T = T>\n SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   size_t rank = order.size();\n@@ -99,13 +99,27 @@ SmallVector<RES_T> reorder(ArrayRef<T> input, ArrayRef<unsigned> order) {\n   return result;\n }\n \n+/// Get the highest power of 2 divisor of an integer.\n template <typename T> T highestPowOf2Divisor(T n) {\n   if (n == 0) {\n     return (static_cast<T>(1) << (sizeof(T) * 8 - 2));\n   }\n   return (n & (~(n - 1)));\n }\n \n+/// Get the next power of 2 for an integer (or the integer itself if it is a\n+/// power of 2).\n+template <typename T> T nextPowOf2(T n) {\n+  if (n == 0) {\n+    return 1;\n+  }\n+  n--;\n+  for (unsigned i = 1; i < sizeof(T) * 8; i <<= 1) {\n+    n |= n >> i;\n+  }\n+  return n + 1;\n+}\n+\n bool isSingleValue(Value value);\n \n bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -23,6 +23,9 @@ namespace gpu {\n \n unsigned getTotalElemsPerThread(Type type);\n \n+unsigned getTotalElemsPerThread(Attribute layout, ArrayRef<int64_t> shape,\n+                                Type eltTy);\n+\n SmallVector<unsigned> getElemsPerThread(Type type);\n \n SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n@@ -72,6 +75,8 @@ SmallVector<unsigned> getOrder(Attribute layout);\n \n bool isaDistributedLayout(Attribute layout);\n \n+bool expensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n+\n } // namespace gpu\n } // namespace triton\n "}, {"filename": "lib/Conversion/TritonToTritonGPU/TritonToTritonGPUPass.cpp", "status": "modified", "additions": 37, "deletions": 4, "changes": 41, "file_content_changes": "@@ -6,6 +6,7 @@\n #include \"mlir/Dialect/Index/IR/IndexDialect.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Transforms/DialectConversion.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n@@ -309,11 +310,43 @@ struct TritonCatPattern : public OpConversionPattern<triton::CatOp> {\n   LogicalResult\n   matchAndRewrite(triton::CatOp op, OpAdaptor adaptor,\n                   ConversionPatternRewriter &rewriter) const override {\n-    // For now, this behaves like generic, but this will evolve when\n-    // we add support for `can_reorder=False`\n-    Type retType = this->getTypeConverter()->convertType(op.getType());\n+    // The cat op satisfy two conditions:\n+    // 1. output.numel = lhs.numel + rhs.numel\n+    // 2. output.total_elems_per_thread =\n+    // next_power_of_2(lhs.total_elems_per_thread + rhs.total_elems_per_thread)\n+    // For now, this behaves like generic, but this\n+    // will evolve when we add support for `can_reorder=False`.\n+    auto retType = this->getTypeConverter()\n+                       ->convertType(op.getType())\n+                       .cast<RankedTensorType>();\n+    auto retEncoding =\n+        retType.getEncoding().cast<triton::gpu::BlockedEncodingAttr>();\n+    auto lhsType = adaptor.getLhs().getType().cast<RankedTensorType>();\n+    auto rhsType = adaptor.getRhs().getType().cast<RankedTensorType>();\n+    auto lhsTotalElemsPerThread = triton::gpu::getTotalElemsPerThread(lhsType);\n+    auto rhsTotalElemsPerThread = triton::gpu::getTotalElemsPerThread(rhsType);\n+    auto retTotalElemsPerThread = triton::gpu::getTotalElemsPerThread(retType);\n+    auto retShape = retType.getShape();\n+    auto retOrder = retEncoding.getOrder();\n+    auto retSizePerThread = retEncoding.getSizePerThread();\n+    auto retThreadsPerWarp = retEncoding.getThreadsPerWarp();\n+    auto retWarpsPerCTA = retEncoding.getWarpsPerCTA();\n+    // Get new retSizePerThread if ret elems per thread is not enough.\n+    // We have to round it up to the next power of 2 due to triton's tensor size\n+    // constraint.\n+    auto newRetTotalElemsPerThread =\n+        nextPowOf2(lhsTotalElemsPerThread + rhsTotalElemsPerThread);\n+    auto newRetSizePerThread = retSizePerThread.vec();\n+    newRetSizePerThread[retOrder[0]] *=\n+        newRetTotalElemsPerThread / retTotalElemsPerThread;\n+    triton::gpu::BlockedEncodingAttr newRetEncoding =\n+        triton::gpu::BlockedEncodingAttr::get(getContext(), newRetSizePerThread,\n+                                              retThreadsPerWarp, retWarpsPerCTA,\n+                                              retOrder);\n+    auto newRetType = RankedTensorType::get(retShape, retType.getElementType(),\n+                                            newRetEncoding);\n     addNamedAttrs(rewriter.replaceOpWithNewOp<triton::CatOp>(\n-                      op, retType, adaptor.getOperands()),\n+                      op, newRetType, adaptor.getOperands()),\n                   adaptor.getAttributes());\n     return success();\n   }"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "file_content_changes": "@@ -354,6 +354,19 @@ bool isaDistributedLayout(Attribute layout) {\n          layout.isa<SliceEncodingAttr>();\n }\n \n+bool expensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n+  // If the new elements per thread is less than the old one, we will need to do\n+  // convert encoding that goes through shared memory anyway. So we consider it\n+  // as expensive.\n+  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n+  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n+  auto shape = tensorTy.getShape();\n+  auto elemTy = tensorTy.getElementType();\n+  auto newTotalElemsPerThread =\n+      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n+  return newTotalElemsPerThread < totalElemsPerThread;\n+}\n+\n } // namespace gpu\n } // namespace triton\n \n@@ -1127,6 +1140,10 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   }\n   // cvt(cat) -> cat\n   if (auto cat = dyn_cast<triton::CatOp>(arg)) {\n+    auto encoding =\n+        op->getResult(0).getType().cast<RankedTensorType>().getEncoding();\n+    if (triton::gpu::expensiveCat(cat, encoding))\n+      return mlir::failure();\n     rewriter.replaceOpWithNewOp<triton::CatOp>(op, op->getResult(0).getType(),\n                                                cat.getOperands());\n     return mlir::success();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -359,7 +359,7 @@ class RematerializeForward : public mlir::RewritePattern {\n \n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensiveToRemat(op, dstEncoding))\n+      if (expensiveToRemat(op, srcEncoding))\n         return failure();\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "file_content_changes": "@@ -112,6 +112,8 @@ bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n     return expensiveLoadOrStore(op, targetEncoding);\n+  if (isa<triton::CatOp>(op))\n+    return triton::gpu::expensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n@@ -122,10 +124,11 @@ bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   return false;\n }\n \n-bool canFoldConversion(Operation *op) {\n+bool canFoldConversion(Operation *op, Attribute &targetEncoding) {\n+  if (isa<triton::CatOp>(op))\n+    return !triton::gpu::expensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n-             triton::MakeRangeOp, triton::SplatOp, triton::ViewOp,\n-             triton::CatOp>(*op);\n+             triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }\n \n int simulateBackwardRematerialization(\n@@ -173,7 +176,7 @@ int simulateBackwardRematerialization(\n         continue;\n       // If the conversion can be folded into opArgI then\n       // we don't count this conversion as expensive\n-      if (canFoldConversion(opArgI))\n+      if (canFoldConversion(opArgI, newEncoding))\n         continue;\n \n       // We add one expensive conversion for the current operand"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "file_content_changes": "@@ -1147,6 +1147,28 @@ def kernel(X, Z, BITCAST: tl.constexpr):\n         assert to_numpy(z_tri) == z_ref\n \n \n+@pytest.mark.parametrize(\"dtype_str, num_warps\", [(dtype_str, num_warps) for dtype_str in int_dtypes + float_dtypes for num_warps in [4, 8]])\n+def test_cat(dtype_str, num_warps):\n+    check_type_supported(dtype_str)\n+\n+    @triton.jit\n+    def kernel(X, Y, Z, N: tl.constexpr):\n+        offs = tl.arange(0, N)\n+        x = tl.load(X + offs)\n+        y = tl.load(Y + offs)\n+        z = tl.cat(x, y, can_reorder=True)\n+        tl.store(Z + tl.arange(0, 2 * N), z)\n+\n+    x = torch.arange(0, 128, device='cuda').to(getattr(torch, dtype_str))\n+    y = torch.arange(-128, 0, device='cuda').to(getattr(torch, dtype_str))\n+    z_ref = torch.cat([x, y], dim=0).sum()\n+    z = torch.zeros((256,), dtype=getattr(torch, dtype_str), device='cuda')\n+    kernel[(1, )](x, y, z, N=128, num_warps=num_warps)\n+    assert z.sum() == z_ref\n+    # check if there's no duplicate value in z\n+    assert z.unique().size(0) == z.size(0)\n+\n+\n @pytest.mark.parametrize(\"dtype_str\", list(torch_dtypes))\n def test_store_constant(dtype_str):\n     check_type_supported(dtype_str)"}]