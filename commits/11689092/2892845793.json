[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 10, "deletions": 5, "changes": 15, "file_content_changes": "@@ -204,13 +204,16 @@ class _attention(torch.autograd.Function):\n     def forward(ctx, q, k, v, sm_scale):\n         BLOCK = 128\n         # shape constraints\n-        Lq, Lk = q.shape[-1], k.shape[-1]\n-        assert Lq == Lk\n+        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n+        assert Lq == Lk and Lk == Lv\n+        assert Lk in {16, 32, 64, 128}\n         o = torch.empty_like(q)\n         grid = (triton.cdiv(q.shape[2], BLOCK), q.shape[0] * q.shape[1])\n         tmp = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n         m = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)\n+        num_warps = 4 if Lk <= 64 else 8\n+\n         _fwd_kernel[grid](\n             q, k, v, sm_scale,\n             tmp, L, m,\n@@ -221,14 +224,14 @@ def forward(ctx, q, k, v, sm_scale):\n             o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n             q.shape[0], q.shape[1], q.shape[2],\n             BLOCK_M=BLOCK, BLOCK_N=BLOCK,\n-            BLOCK_DMODEL=64, num_warps=4,\n+            BLOCK_DMODEL=Lk, num_warps=num_warps,\n             num_stages=1,\n         )\n         ctx.save_for_backward(q, k, v, o, L, m)\n         ctx.BLOCK = BLOCK\n         ctx.grid = grid\n         ctx.sm_scale = sm_scale\n-        ctx.BLOCK_DMODEL = 64\n+        ctx.BLOCK_DMODEL = Lk\n         return o\n \n     @staticmethod\n@@ -245,6 +248,8 @@ def backward(ctx, do):\n             do_scaled, delta,\n             BLOCK_M=ctx.BLOCK, D_HEAD=ctx.BLOCK_DMODEL,\n         )\n+\n+        num_warps = 4 if ctx.BLOCK_DMODEL <= 64 else 8\n         _bwd_kernel[(ctx.grid[1],)](\n             q, k, v, ctx.sm_scale,\n             o, do_scaled,\n@@ -257,7 +262,7 @@ def backward(ctx, do):\n             q.shape[0], q.shape[1], q.shape[2],\n             ctx.grid[0],\n             BLOCK_M=ctx.BLOCK, BLOCK_N=ctx.BLOCK,\n-            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=8,\n+            BLOCK_DMODEL=ctx.BLOCK_DMODEL, num_warps=num_warps,\n             num_stages=1,\n         )\n         return dq, dk, dv, None"}]