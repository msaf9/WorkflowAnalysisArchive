[{"filename": "include/triton/external/CUDA/cuda.h", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "file_content_changes": "@@ -818,7 +818,7 @@ typedef enum CUcomputemode_enum {\n  * Memory advise values\n  */\n typedef enum CUmem_advise_enum {\n-    CU_MEM_ADVISE_SET_READ_MOSTLY          = 1, /**< Data will mostly be read and only occassionally be written to */\n+    CU_MEM_ADVISE_SET_READ_MOSTLY          = 1, /**< Data will mostly be read and only occasionally be written to */\n     CU_MEM_ADVISE_UNSET_READ_MOSTLY        = 2, /**< Undo the effect of ::CU_MEM_ADVISE_SET_READ_MOSTLY */\n     CU_MEM_ADVISE_SET_PREFERRED_LOCATION   = 3, /**< Set the preferred location for the data as the specified device */\n     CU_MEM_ADVISE_UNSET_PREFERRED_LOCATION = 4, /**< Clear the preferred location for the data */\n@@ -827,7 +827,7 @@ typedef enum CUmem_advise_enum {\n } CUmem_advise;\n \n typedef enum CUmem_range_attribute_enum {\n-    CU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY            = 1, /**< Whether the range will mostly be read and only occassionally be written to */\n+    CU_MEM_RANGE_ATTRIBUTE_READ_MOSTLY            = 1, /**< Whether the range will mostly be read and only occasionally be written to */\n     CU_MEM_RANGE_ATTRIBUTE_PREFERRED_LOCATION     = 2, /**< The preferred location of the range */\n     CU_MEM_RANGE_ATTRIBUTE_ACCESSED_BY            = 3, /**< Memory range has ::CU_MEM_ADVISE_SET_ACCESSED_BY set for specified device */\n     CU_MEM_RANGE_ATTRIBUTE_LAST_PREFETCH_LOCATION = 4  /**< The last location to which the range was prefetched */\n@@ -849,7 +849,7 @@ typedef enum CUjit_option_enum\n      * IN: Specifies minimum number of threads per block to target compilation\n      * for\\n\n      * OUT: Returns the number of threads the compiler actually targeted.\n-     * This restricts the resource utilization fo the compiler (e.g. max\n+     * This restricts the resource utilization of the compiler (e.g. max\n      * registers) such that a block with the given number of threads should be\n      * able to launch based on register limitations. Note, this option does not\n      * currently take into account any other resource limitations, such as\n@@ -974,10 +974,10 @@ typedef enum CUjit_option_enum\n     CU_JIT_FAST_COMPILE,\n \n     /**\n-     * Array of device symbol names that will be relocated to the corresponing\n+     * Array of device symbol names that will be relocated to the corresponding\n      * host addresses stored in ::CU_JIT_GLOBAL_SYMBOL_ADDRESSES.\\n\n      * Must contain ::CU_JIT_GLOBAL_SYMBOL_COUNT entries.\\n\n-     * When loding a device module, driver will relocate all encountered\n+     * When loading a device module, driver will relocate all encountered\n      * unresolved symbols to the host addresses.\\n\n      * It is only allowed to register symbols that correspond to unresolved\n      * global variables.\\n\n@@ -1194,7 +1194,7 @@ typedef enum CUlimit_enum {\n  * Resource types\n  */\n typedef enum CUresourcetype_enum {\n-    CU_RESOURCE_TYPE_ARRAY           = 0x00, /**< Array resoure */\n+    CU_RESOURCE_TYPE_ARRAY           = 0x00, /**< Array resource */\n     CU_RESOURCE_TYPE_MIPMAPPED_ARRAY = 0x01, /**< Mipmapped array resource */\n     CU_RESOURCE_TYPE_LINEAR          = 0x02, /**< Linear resource */\n     CU_RESOURCE_TYPE_PITCH2D         = 0x03  /**< Pitch 2D resource */\n@@ -2914,9 +2914,9 @@ typedef struct CUmemAllocationProp_st {\n     CUmemLocation location;\n     /**\n      * Windows-specific POBJECT_ATTRIBUTES required when\n-     * ::CU_MEM_HANDLE_TYPE_WIN32 is specified.  This object atributes structure\n+     * ::CU_MEM_HANDLE_TYPE_WIN32 is specified.  This object attributes structure\n      * includes security attributes that define\n-     * the scope of which exported allocations may be tranferred to other\n+     * the scope of which exported allocations may be transferred to other\n      * processes.  In all other cases, this field is required to be zero.\n      */\n     void *win32HandleMetaData;\n@@ -3036,7 +3036,7 @@ typedef struct CUmemPoolProps_st {\n     /**\n      * Windows-specific LPSECURITYATTRIBUTES required when\n      * ::CU_MEM_HANDLE_TYPE_WIN32 is specified.  This security attribute defines\n-     * the scope of which exported allocations may be tranferred to other\n+     * the scope of which exported allocations may be transferred to other\n      * processes.  In all other cases, this field is required to be zero.\n      */\n     void *win32SecurityAttributes;\n@@ -3519,7 +3519,7 @@ CUresult CUDAAPI cuDeviceGet(CUdevice *device, int ordinal);\n CUresult CUDAAPI cuDeviceGetCount(int *count);\n \n /**\n- * \\brief Returns an identifer string for the device\n+ * \\brief Returns an identifier string for the device\n  *\n  * Returns an ASCII string identifying the device \\p dev in the NULL-terminated\n  * string pointed to by \\p name. \\p len specifies the maximum length of the\n@@ -3556,7 +3556,7 @@ CUresult CUDAAPI cuDeviceGetName(char *name, int len, CUdevice dev);\n  * Note there is a later version of this API, ::cuDeviceGetUuid_v2. It will\n  * supplant this version in 12.0, which is retained for minor version compatibility.\n  *\n- * Returns 16-octets identifing the device \\p dev in the structure\n+ * Returns 16-octets identifying the device \\p dev in the structure\n  * pointed by the \\p uuid.\n  *\n  * \\param uuid - Returned UUID\n@@ -3586,7 +3586,7 @@ CUresult CUDAAPI cuDeviceGetUuid(CUuuid *uuid, CUdevice dev);\n /**\n  * \\brief Return an UUID for the device (11.4+)\n  *\n- * Returns 16-octets identifing the device \\p dev in the structure\n+ * Returns 16-octets identifying the device \\p dev in the structure\n  * pointed by the \\p uuid. If the device is in MIG mode, returns its\n  * MIG UUID which uniquely identifies the subscribed MIG compute instance.\n  *\n@@ -3867,15 +3867,15 @@ CUresult CUDAAPI cuDeviceGetTexture1DLinearMaxWidth(size_t *maxWidthInElements,\n  *   supports native atomic operations.\n  * - ::CU_DEVICE_ATTRIBUTE_SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO: Ratio of single precision performance\n  *   (in floating-point operations per second) to double precision performance.\n- * - ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS: Device suppports coherently accessing\n+ * - ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS: Device supports coherently accessing\n  *   pageable memory without calling cudaHostRegister on it.\n  * - ::CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS: Device can coherently access managed memory\n  *   concurrently with the CPU.\n  * - ::CU_DEVICE_ATTRIBUTE_COMPUTE_PREEMPTION_SUPPORTED: Device supports Compute Preemption.\n  * - ::CU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM: Device can access host registered\n  *   memory at the same virtual address as the CPU.\n  * -  ::CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN: The maximum per block shared memory size\n- *    suported on this device. This is the maximum value that can be opted into when using the cuFuncSetAttribute() call.\n+ *    supported on this device. This is the maximum value that can be opted into when using the cuFuncSetAttribute() call.\n  *    For more details see ::CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES\n  * - ::CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES: Device accesses pageable memory via the host's\n  *   page tables.\n@@ -4132,7 +4132,7 @@ __CUDA_DEPRECATED CUresult CUDAAPI cuDeviceGetProperties(CUdevprop *prop, CUdevi\n  *\n  * \\deprecated\n  *\n- * This function was deprecated as of CUDA 5.0 and its functionality superceded\n+ * This function was deprecated as of CUDA 5.0 and its functionality superseded\n  * by ::cuDeviceGetAttribute().\n  *\n  * Returns in \\p *major and \\p *minor the major and minor revision numbers that\n@@ -4962,10 +4962,10 @@ CUresult CUDAAPI cuCtxSynchronize(void);\n  *   returned.\n  *\n  * - ::CU_LIMIT_MAX_L2_FETCH_GRANULARITY controls the L2 cache fetch granularity.\n- *   Values can range from 0B to 128B. This is purely a performence hint and\n+ *   Values can range from 0B to 128B. This is purely a performance hint and\n  *   it can be ignored or clamped depending on the platform.\n  *\n- * - ::CU_LIMIT_PERSISTING_L2_CACHE_SIZE controls size in bytes availabe for\n+ * - ::CU_LIMIT_PERSISTING_L2_CACHE_SIZE controls size in bytes available for\n  *   persisting L2 cache. This is purely a performance hint and it can be\n  *   ignored or clamped depending on the platform.\n  *\n@@ -6398,7 +6398,7 @@ CUresult CUDAAPI cuMemHostGetFlags(unsigned int *pFlags, void *p);\n  * ::cuStreamAttachMemAsync will be required to enable access on such devices.\n  *\n  * If the association is later changed via ::cuStreamAttachMemAsync to\n- * a single stream, the default association as specifed during ::cuMemAllocManaged\n+ * a single stream, the default association as specified during ::cuMemAllocManaged\n  * is restored when that stream is destroyed. For __managed__ variables, the\n  * default association is always ::CU_MEM_ATTACH_GLOBAL. Note that destroying a\n  * stream is an asynchronous operation, and as a result, the change to default\n@@ -9616,13 +9616,13 @@ CUresult CUDAAPI cuMemAddressFree(CUdeviceptr ptr, size_t size);\n * \\brief Create a CUDA memory handle representing a memory allocation of a given size described by the given properties\n *\n * This creates a memory allocation on the target device specified through the\n-* \\p prop strcuture. The created allocation will not have any device or host\n+* \\p prop structure. The created allocation will not have any device or host\n * mappings. The generic memory \\p handle for the allocation can be\n * mapped to the address space of calling process via ::cuMemMap. This handle\n * cannot be transmitted directly to other processes (see\n * ::cuMemExportToShareableHandle).  On Windows, the caller must also pass\n * an LPSECURITYATTRIBUTE in \\p prop to be associated with this handle which\n-* limits or allows access to this handle for a recepient process (see\n+* limits or allows access to this handle for a recipient process (see\n * ::CUmemAllocationProp::win32HandleMetaData for more).  The \\p size of this\n * allocation must be a multiple of the the value given via\n * ::cuMemGetAllocationGranularity with the ::CU_MEM_ALLOC_GRANULARITY_MINIMUM\n@@ -9660,7 +9660,7 @@ CUresult CUDAAPI cuMemCreate(CUmemGenericAllocationHandle *handle, size_t size,\n * are unmapped and when all outstanding references to the handle (including it's\n * shareable counterparts) are also released. The generic memory handle can be\n * freed when there are still outstanding mappings made with this handle. Each\n-* time a recepient process imports a shareable handle, it needs to pair it with\n+* time a recipient process imports a shareable handle, it needs to pair it with\n * ::cuMemRelease for the handle to be freed.  If \\p handle is not a valid handle\n * the behavior is undefined. \n *\n@@ -10975,7 +10975,7 @@ CUresult CUDAAPI cuMemAdvise(CUdeviceptr devPtr, size_t count, CUmem_advise advi\n  * a GPU id or CU_DEVICE_CPU depending on whether the last location for prefetch was a GPU or the CPU\n  * respectively. If any page in the memory range was never explicitly prefetched or if all pages were not\n  * prefetched to the same location, CU_DEVICE_INVALID will be returned. Note that this simply returns the\n- * last location that the applicaton requested to prefetch the memory range to. It gives no indication as to\n+ * last location that the application requested to prefetch the memory range to. It gives no indication as to\n  * whether the prefetch operation to that location has completed or even begun.\n  *\n  * \\param data      - A pointers to a memory location where the result\n@@ -13561,7 +13561,7 @@ CUresult CUDAAPI cuLaunchCooperativeKernel(CUfunction f,\n  * All kernels launched must be identical with respect to the compiled code. Note that\n  * any __device__, __constant__ or __managed__ variables present in the module that owns\n  * the kernel launched on each device, are independently instantiated on every device.\n- * It is the application's responsiblity to ensure these variables are initialized and\n+ * It is the application's responsibility to ensure these variables are initialized and\n  * used appropriately.\n  *\n  * The size of the grids as specified in blocks, the size of the blocks themselves"}, {"filename": "include/triton/external/CUDA/nvml.h", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "file_content_changes": "@@ -328,7 +328,7 @@ typedef enum nvmlGpuLevel_enum\n typedef enum nvmlGpuP2PStatus_enum\n {\n     NVML_P2P_STATUS_OK     = 0,\n-    NVML_P2P_STATUS_CHIPSET_NOT_SUPPORED,\n+    NVML_P2P_STATUS_CHIPSET_NOT_SUPPORTED,\n     NVML_P2P_STATUS_GPU_NOT_SUPPORTED,\n     NVML_P2P_STATUS_IOH_TOPOLOGY_NOT_SUPPORTED,\n     NVML_P2P_STATUS_DISABLED_BY_REGKEY,\n@@ -736,7 +736,7 @@ typedef enum nvmlReturn_enum\n     NVML_ERROR_IN_USE = 19,             //!< An operation cannot be performed because the GPU is currently in use\n     NVML_ERROR_MEMORY = 20,             //!< Insufficient memory\n     NVML_ERROR_NO_DATA = 21,            //!<No data\n-    NVML_ERROR_VGPU_ECC_NOT_SUPPORTED = 22,    //!< The requested vgpu operation is not available on target device, becasue ECC is enabled\n+    NVML_ERROR_VGPU_ECC_NOT_SUPPORTED = 22,    //!< The requested vgpu operation is not available on target device, because ECC is enabled\n     NVML_ERROR_UNKNOWN = 999            //!< An internal driver error occurred\n } nvmlReturn_t;\n \n@@ -1463,7 +1463,7 @@ typedef struct nvmlEncoderSessionInfo_st\n  */\n typedef enum nvmlFBCSessionType_enum\n {\n-    NVML_FBC_SESSION_TYPE_UNKNOWN = 0,     //!< Unknwon\n+    NVML_FBC_SESSION_TYPE_UNKNOWN = 0,     //!< Unknown\n     NVML_FBC_SESSION_TYPE_TOSYS,           //!< ToSys\n     NVML_FBC_SESSION_TYPE_CUDA,            //!< Cuda\n     NVML_FBC_SESSION_TYPE_VID,             //!< Vid\n@@ -3678,10 +3678,10 @@ nvmlReturn_t DECLDIR nvmlDeviceGetEncoderStats (nvmlDevice_t device, unsigned in\n  * Retrieves information about active encoder sessions on a target device.\n  *\n  * An array of active encoder sessions is returned in the caller-supplied buffer pointed at by \\a sessionInfos. The\n- * array elememt count is passed in \\a sessionCount, and \\a sessionCount is used to return the number of sessions\n+ * array element count is passed in \\a sessionCount, and \\a sessionCount is used to return the number of sessions\n  * written to the buffer.\n  *\n- * If the supplied buffer is not large enough to accomodate the active session array, the function returns\n+ * If the supplied buffer is not large enough to accommodate the active session array, the function returns\n  * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlEncoderSessionInfo_t array required in \\a sessionCount.\n  * To query the number of active encoder sessions, call this function with *sessionCount = 0.  The code will return\n  * NVML_SUCCESS with number of active encoder sessions updated in *sessionCount.\n@@ -3727,7 +3727,7 @@ nvmlReturn_t DECLDIR nvmlDeviceGetDecoderUtilization(nvmlDevice_t device, unsign\n * For Maxwell &tm; or newer fully supported devices.\n *\n * @param device                            The identifier of the target device\n-* @param fbcStats                          Reference to nvmlFBCStats_t structure contianing NvFBC stats\n+* @param fbcStats                          Reference to nvmlFBCStats_t structure containing NvFBC stats\n *\n * @return\n *         - \\ref NVML_SUCCESS                  if \\a fbcStats is fetched\n@@ -3742,10 +3742,10 @@ nvmlReturn_t DECLDIR nvmlDeviceGetFBCStats(nvmlDevice_t device, nvmlFBCStats_t *\n * Retrieves information about active frame buffer capture sessions on a target device.\n *\n * An array of active encoder sessions is returned in the caller-supplied buffer pointed at by \\a sessionInfo. The\n-* array elememt count is passed in \\a sessionCount, and \\a sessionCount is used to return the number of sessions\n+* array element count is passed in \\a sessionCount, and \\a sessionCount is used to return the number of sessions\n * written to the buffer.\n *\n-* If the supplied buffer is not large enough to accomodate the active session array, the function returns\n+* If the supplied buffer is not large enough to accommodate the active session array, the function returns\n * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlFBCSessionInfo_t array required in \\a sessionCount.\n * To query the number of active FBC sessions, call this function with *sessionCount = 0.  The code will return\n * NVML_SUCCESS with number of active FBC sessions updated in *sessionCount.\n@@ -4208,7 +4208,7 @@ nvmlReturn_t DECLDIR nvmlDeviceGetRetiredPages(nvmlDevice_t device, nvmlPageReti\n  * The address information provided from this API is the hardware address of the page that was retired.  Note\n  * that this does not match the virtual address used in CUDA, but will match the address information in XID 63\n  *\n- * \\note nvmlDeviceGetRetiredPages_v2 adds an additional timestamps paramter to return the time of each page's\n+ * \\note nvmlDeviceGetRetiredPages_v2 adds an additional timestamps parameter to return the time of each page's\n  *       retirement.\n  * \n  * For Kepler &tm; or newer fully supported devices.\n@@ -4476,7 +4476,7 @@ nvmlReturn_t DECLDIR nvmlDeviceSetDriverModel(nvmlDevice_t device, nvmlDriverMod\n  * Set clocks that device will lock to.\n  *\n  * Sets the clocks that the device will be running at to the value in the range of minGpuClockMHz to maxGpuClockMHz.\n- * Setting this will supercede application clock values and take effect regardless if a cuda app is running.\n+ * Setting this will supersede application clock values and take effect regardless if a cuda app is running.\n  * See /ref nvmlDeviceSetApplicationsClocks\n  *\n  * Can be used as a setting to request constant performance.\n@@ -5297,7 +5297,7 @@ nvmlReturn_t DECLDIR nvmlDeviceSetVirtualizationMode(nvmlDevice_t device, nvmlGp\n  * pointed at by \\a vgpuTypeIds. The element count of nvmlVgpuTypeId_t array is passed in \\a vgpuCount, and \\a vgpuCount\n  * is used to return the number of vGPU types written to the buffer.\n  *\n- * If the supplied buffer is not large enough to accomodate the vGPU type array, the function returns\n+ * If the supplied buffer is not large enough to accommodate the vGPU type array, the function returns\n  * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuTypeId_t array required in \\a vgpuCount.\n  * To query the number of vGPU types supported for the GPU, call this function with *vgpuCount = 0.\n  * The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU types are supported.\n@@ -5327,9 +5327,9 @@ nvmlReturn_t DECLDIR nvmlDeviceGetSupportedVgpus(nvmlDevice_t device, unsigned i\n  * can concurrently run on a device.  For example, if only one vGPU type is allowed at a time on a device, then the creatable\n  * list will be restricted to whatever vGPU type is already running on the device.\n  *\n- * If the supplied buffer is not large enough to accomodate the vGPU type array, the function returns\n+ * If the supplied buffer is not large enough to accommodate the vGPU type array, the function returns\n  * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuTypeId_t array required in \\a vgpuCount.\n- * To query the number of vGPU types createable for the GPU, call this function with *vgpuCount = 0.\n+ * To query the number of vGPU types creatable for the GPU, call this function with *vgpuCount = 0.\n  * The code will return NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU types are creatable.\n  *\n  * @param device                   The identifier of the target device\n@@ -5392,7 +5392,7 @@ nvmlReturn_t DECLDIR nvmlVgpuTypeGetName(nvmlVgpuTypeId_t vgpuTypeId, char *vgpu\n  *\n  * @param vgpuTypeId               Handle to vGPU type\n  * @param deviceID                 Device ID and vendor ID of the device contained in single 32 bit value\n- * @param subsystemID              Subsytem ID and subsytem vendor ID of the device contained in single 32 bit value\n+ * @param subsystemID              subsystem ID and subsystem vendor ID of the device contained in single 32 bit value\n  *\n  * @return\n  *         - \\ref NVML_SUCCESS                 successful completion\n@@ -5516,10 +5516,10 @@ nvmlReturn_t DECLDIR nvmlVgpuTypeGetMaxInstances(nvmlDevice_t device, nvmlVgpuTy\n  * Retrieve the active vGPU instances on a device.\n  *\n  * An array of active vGPU instances is returned in the caller-supplied buffer pointed at by \\a vgpuInstances. The\n- * array elememt count is passed in \\a vgpuCount, and \\a vgpuCount is used to return the number of vGPU instances\n+ * array element count is passed in \\a vgpuCount, and \\a vgpuCount is used to return the number of vGPU instances\n  * written to the buffer.\n  *\n- * If the supplied buffer is not large enough to accomodate the vGPU instance array, the function returns\n+ * If the supplied buffer is not large enough to accommodate the vGPU instance array, the function returns\n  * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlVgpuInstance_t array required in \\a vgpuCount.\n  * To query the number of active vGPU instances, call this function with *vgpuCount = 0.  The code will return\n  * NVML_ERROR_INSUFFICIENT_SIZE, or NVML_SUCCESS if no vGPU Types are supported.\n@@ -5702,7 +5702,7 @@ nvmlReturn_t DECLDIR nvmlVgpuInstanceGetFrameRateLimit(nvmlVgpuInstance_t vgpuIn\n  * @param encoderCapacity          Reference to an unsigned int for the encoder capacity\n  *\n  * @return\n- *         - \\ref NVML_SUCCESS                 if \\a encoderCapacity has been retrived\n+ *         - \\ref NVML_SUCCESS                 if \\a encoderCapacity has been retrieved\n  *         - \\ref NVML_ERROR_UNINITIALIZED     if the library has not been successfully initialized\n  *         - \\ref NVML_ERROR_INVALID_ARGUMENT  if \\a vgpuInstance is 0, or \\a encoderQueryType is invalid\n  *         - \\ref NVML_ERROR_NOT_FOUND         if \\a vgpuInstance does not match a valid active vGPU instance on the system\n@@ -5863,10 +5863,10 @@ nvmlReturn_t DECLDIR nvmlVgpuInstanceGetEncoderStats(nvmlVgpuInstance_t vgpuInst\n  * Retrieves information about all active encoder sessions on a vGPU Instance.\n  *\n  * An array of active encoder sessions is returned in the caller-supplied buffer pointed at by \\a sessionInfo. The\n- * array elememt count is passed in \\a sessionCount, and \\a sessionCount is used to return the number of sessions\n+ * array element count is passed in \\a sessionCount, and \\a sessionCount is used to return the number of sessions\n  * written to the buffer.\n  *\n- * If the supplied buffer is not large enough to accomodate the active session array, the function returns\n+ * If the supplied buffer is not large enough to accommodate the active session array, the function returns\n  * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlEncoderSessionInfo_t array required in \\a sessionCount.\n  * To query the number of active encoder sessions, call this function with *sessionCount = 0. The code will return\n  * NVML_SUCCESS with number of active encoder sessions updated in *sessionCount.\n@@ -5896,7 +5896,7 @@ nvmlReturn_t DECLDIR nvmlVgpuInstanceGetEncoderSessions(nvmlVgpuInstance_t vgpuI\n * For Maxwell &tm; or newer fully supported devices.\n *\n * @param vgpuInstance                      Identifier of the target vGPU instance\n-* @param fbcStats                          Reference to nvmlFBCStats_t structure contianing NvFBC stats\n+* @param fbcStats                          Reference to nvmlFBCStats_t structure containing NvFBC stats\n *\n * @return\n *         - \\ref NVML_SUCCESS                  if \\a fbcStats is fetched\n@@ -5914,7 +5914,7 @@ nvmlReturn_t DECLDIR nvmlVgpuInstanceGetFBCStats(nvmlVgpuInstance_t vgpuInstance\n * array element count is passed in \\a sessionCount, and \\a sessionCount is used to return the number of sessions\n * written to the buffer.\n *\n-* If the supplied buffer is not large enough to accomodate the active session array, the function returns\n+* If the supplied buffer is not large enough to accommodate the active session array, the function returns\n * NVML_ERROR_INSUFFICIENT_SIZE, with the element count of nvmlFBCSessionInfo_t array required in \\a sessionCount.\n * To query the number of active FBC sessions, call this function with *sessionCount = 0.  The code will return\n * NVML_SUCCESS with number of active FBC sessions updated in *sessionCount.\n@@ -6094,7 +6094,7 @@ typedef struct nvmlVgpuPgpuMetadata_st\n     unsigned int            version;                                                    //!< Current version of the structure\n     unsigned int            revision;                                                   //!< Current revision of the structure\n     char                    hostDriverVersion[NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE];  //!< Host driver version\n-    unsigned int            pgpuVirtualizationCaps;                                     //!< Pgpu virtualizaion capabilities bitfileld\n+    unsigned int            pgpuVirtualizationCaps;                                     //!< Pgpu virtualization capabilities bitfield\n     unsigned int            reserved[7];                                                //!< Reserved for internal use\n     unsigned int            opaqueDataSize;                                             //!< Size of opaque data field in bytes\n     char                    opaqueData[4];                                              //!< Opaque data\n@@ -6191,7 +6191,7 @@ nvmlReturn_t DECLDIR nvmlDeviceGetVgpuMetadata(nvmlDevice_t device, nvmlVgpuPgpu\n  *\n  * The caller passes in a buffer via \\a compatibilityInfo, into which a compatibility information structure is written. The\n  * structure defines the states in which the vGPU / VM may be booted on the physical GPU. If the vGPU / VM compatibility\n- * with the physical GPU is limited, a limit code indicates the factor limiting compability.\n+ * with the physical GPU is limited, a limit code indicates the factor limiting compatibility.\n  * (see \\ref nvmlVgpuPgpuCompatibilityLimitCode_t for details).\n  *\n  * Note: vGPU compatibility does not take into account dynamic capacity conditions that may limit a system's ability to"}, {"filename": "include/triton/external/half.hpp", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "file_content_changes": "@@ -950,7 +950,7 @@ namespace half_float\n \t\t/// Convert half-precision floating point to integer.\n \t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n \t\t/// \\tparam E `true` for round to even, `false` for round away from zero\n-\t\t/// \\tparam T type to convert to (buitlin integer type with at least 16 bits precision, excluding any implicit sign bits)\n+\t\t/// \\tparam T type to convert to (builtin integer type with at least 16 bits precision, excluding any implicit sign bits)\n \t\t/// \\param value binary representation of half-precision value\n \t\t/// \\return integral value\n \t\ttemplate<std::float_round_style R,bool E,typename T> T half2int_impl(uint16 value)\n@@ -988,13 +988,13 @@ namespace half_float\n \n \t\t/// Convert half-precision floating point to integer.\n \t\t/// \\tparam R rounding mode to use, `std::round_indeterminate` for fastest rounding\n-\t\t/// \\tparam T type to convert to (buitlin integer type with at least 16 bits precision, excluding any implicit sign bits)\n+\t\t/// \\tparam T type to convert to (builtin integer type with at least 16 bits precision, excluding any implicit sign bits)\n \t\t/// \\param value binary representation of half-precision value\n \t\t/// \\return integral value\n \t\ttemplate<std::float_round_style R,typename T> T half2int(uint16 value) { return half2int_impl<R,HALF_ROUND_TIES_TO_EVEN,T>(value); }\n \n \t\t/// Convert half-precision floating point to integer using round-to-nearest-away-from-zero.\n-\t\t/// \\tparam T type to convert to (buitlin integer type with at least 16 bits precision, excluding any implicit sign bits)\n+\t\t/// \\tparam T type to convert to (builtin integer type with at least 16 bits precision, excluding any implicit sign bits)\n \t\t/// \\param value binary representation of half-precision value\n \t\t/// \\return integral value\n \t\ttemplate<typename T> T half2int_up(uint16 value) { return half2int_impl<std::round_to_nearest,0,T>(value); }\n@@ -1053,7 +1053,7 @@ namespace half_float\n \n \t/// Half-precision floating point type.\n \t/// This class implements an IEEE-conformant half-precision floating point type with the usual arithmetic operators and \n-\t/// conversions. It is implicitly convertible to single-precision floating point, which makes artihmetic expressions and \n+\t/// conversions. It is implicitly convertible to single-precision floating point, which makes arithmetic expressions and \n \t/// functions with mixed-type operands to be of the most precise operand type. Additionally all arithmetic operations \n \t/// (and many mathematical functions) are carried out in single-precision internally. All conversions from single- to \n \t/// half-precision are done using the library's default rounding mode, but temporary results inside chained arithmetic \n@@ -1062,7 +1062,7 @@ namespace half_float\n \t/// According to the C++98/03 definition, the half type is not a POD type. But according to C++11's less strict and \n \t/// extended definitions it is both a standard layout type and a trivially copyable type (even if not a POD type), which \n \t/// means it can be standard-conformantly copied using raw binary copies. But in this context some more words about the \n-\t/// actual size of the type. Although the half is representing an IEEE 16-bit type, it does not neccessarily have to be of \n+\t/// actual size of the type. Although the half is representing an IEEE 16-bit type, it does not necessarily have to be of \n \t/// exactly 16-bits size. But on any reasonable implementation the actual binary representation of this type will most \n \t/// probably not ivolve any additional \"magic\" or padding beyond the simple binary representation of the underlying 16-bit \n \t/// IEEE number, even if not strictly guaranteed by the standard. But even then it only has an actual size of 16 bits if \n@@ -2181,7 +2181,7 @@ namespace half_float\n \n \t\t/// Identity.\n \t\t/// \\param arg operand\n-\t\t/// \\return uncahnged operand\n+\t\t/// \\return unchanged operand\n \t\ttemplate<typename T> HALF_CONSTEXPR typename enable<T,T>::type operator+(T arg) { return arg; }\n \n \t\t/// Negation.\n@@ -2620,7 +2620,7 @@ namespace half_float\n \t\t/// Multiply by power of two.\n \t\t/// \\param arg number to modify\n \t\t/// \\param exp power of two to multiply with\n-\t\t/// \\return \\a arg multplied by 2 raised to \\a exp\n+\t\t/// \\return \\a arg multiplied by 2 raised to \\a exp\n //\t\ttemplate<typename T> typename enable<half,T>::type ldexp(T arg, int exp) { return functions::scalbln(arg, exp); }\n \t\tinline half ldexp(half arg, int exp) { return functions::scalbln(arg, exp); }\n \t\tinline half ldexp(expr arg, int exp) { return functions::scalbln(arg, exp); }\n@@ -2636,15 +2636,15 @@ namespace half_float\n \t\t/// Multiply by power of two.\n \t\t/// \\param arg number to modify\n \t\t/// \\param exp power of two to multiply with\n-\t\t/// \\return \\a arg multplied by 2 raised to \\a exp\n+\t\t/// \\return \\a arg multiplied by 2 raised to \\a exp\n //\t\ttemplate<typename T> typename enable<half,T>::type scalbn(T arg, int exp) { return functions::scalbln(arg, exp); }\n \t\tinline half scalbn(half arg, int exp) { return functions::scalbln(arg, exp); }\n \t\tinline half scalbn(expr arg, int exp) { return functions::scalbln(arg, exp); }\n \n \t\t/// Multiply by power of two.\n \t\t/// \\param arg number to modify\n \t\t/// \\param exp power of two to multiply with\n-\t\t/// \\return \\a arg multplied by 2 raised to \\a exp\t\n+\t\t/// \\return \\a arg multiplied by 2 raised to \\a exp\t\n //\t\ttemplate<typename T> typename enable<half,T>::type scalbln(T arg, long exp) { return functions::scalbln(arg, exp); }\n \t\tinline half scalbln(half arg, long exp) { return functions::scalbln(arg, exp); }\n \t\tinline half scalbln(expr arg, long exp) { return functions::scalbln(arg, exp); }"}, {"filename": "lib/codegen/transform/inline.cc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -91,7 +91,7 @@ void inliner::do_inline(ir::function* fn, ir::call_inst* callsite, ir::builder&\n         if(inst_map.find(inst_op) != inst_map.end())\n           new_inst->set_operand(k, inst_map.at(inst_op));\n     }\n-    // handles a ret instruciton.\n+    // handles a ret instruction.\n     // instead of returning we need to branch to after the function call\n     if(ir::return_inst* ret = dynamic_cast<ir::return_inst*>(new_inst)) {\n       if(ir::value* ret_val = ret->get_return_value())"}, {"filename": "python/setup.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -51,7 +51,7 @@ class Package(NamedTuple):\n         Package(\"pybind11\", \"pybind11-2.10.0\", \"https://github.com/pybind/pybind11/archive/refs/tags/v2.10.0.tar.gz\", \"include/pybind11/pybind11.h\", \"PYBIND11_INCLUDE_DIR\", \"\")\n     ]\n     if not use_system_llvm():\n-        # donwload LLVM if no suitable system LLVM is installed\n+        # download LLVM if no suitable system LLVM is installed\n         packages.append(\n             Package(\"llvm\", \"clang+llvm-11.0.1-x86_64-linux-gnu-ubuntu-16.04\", \"https://github.com/llvm/llvm-project/releases/download/llvmorg-11.0.1/clang+llvm-11.0.1-x86_64-linux-gnu-ubuntu-16.04.tar.xz\", \"lib\", \"LLVM_INCLUDE_DIRS\", \"LLVM_LIBRARY_DIR\")\n         )"}, {"filename": "python/src/functions.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -232,7 +232,7 @@ ir::value *store(ir::value *ptr, ir::value *val, std::optional<ir::value *> _mas\n  ----------------------------------------------*/\n std::string dot_docstr = R\"pbdoc(\n     Returns the matrix product of two blocks.\n-    The two blocks must be two dimensionals and have compatible inner dimensions.\n+    The two blocks must be two dimensions and have compatible inner dimensions.\n \n     :param input: The first block to be multiplied.\n     :type input: 2D block of scalar-type in {`float16`, `float32`}"}, {"filename": "python/triton/compiler.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1126,7 +1126,7 @@ def put(self, data, filename, binary=True):\n             os.rename(filepath + \".tmp\", filepath)\n \n \n-# utilties for generating and compiling C wrappers\n+# utilities for generating and compiling C wrappers\n \n \n @functools.lru_cache()"}, {"filename": "python/triton/language/core.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -768,7 +768,7 @@ def dot(input, other, trans_a=False, trans_b=False, allow_tf32=True, _builder=No\n     \"\"\"\n     Returns the matrix product of two blocks.\n \n-    The two blocks must be two dimensionals and have compatible inner dimensions.\n+    The two blocks must be two dimensions and have compatible inner dimensions.\n \n     :param input: The first tensor to be multiplied.\n     :type input: 2D tensor of scalar-type in {:code:`float16`, :code:`bfloat16`, :code:`float32`}"}]