[{"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -128,7 +128,7 @@ def _add(x_ptr, y_ptr, output_ptr, n_elements,\n         1024 * 16: 0.0219,\n         1024 * 64: 0.0791,\n         1024 * 256: 0.243,\n-        1024 * 1024: 0.534,\n+        1024 * 1024: 0.530,\n         1024 * 4096: 0.796,\n         1024 * 16384: 0.905,\n         1024 * 65536: 0.939,"}, {"filename": "python/triton/testing.py", "status": "modified", "additions": 9, "deletions": 2, "changes": 11, "file_content_changes": "@@ -115,7 +115,9 @@ def nvsmi(attrs):\n     return ret\n \n \n-def do_bench(fn, warmup=25, rep=100, grad_to_none=None, percentiles=[0.5, 0.2, 0.8], record_clocks=False):\n+def do_bench(fn, warmup=25, rep=100, grad_to_none=None,\n+             percentiles=(0.5, 0.2, 0.8),\n+             record_clocks=False, fast_flush=False):\n     \"\"\"\n     Benchmark the runtime of the provided function. By default, return the median runtime of :code:`fn` along with\n     the 20-th and 80-th performance percentile.\n@@ -130,6 +132,8 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None, percentiles=[0.5, 0.2, 0\n     :type grad_to_none: torch.tensor, optional\n     :param percentiles: Performance percentile to return in addition to the median.\n     :type percentiles: list[float]\n+    :param fast_flush: Use faster kernel to flush L2 between measurements\n+    :type fast_flush: bool\n     \"\"\"\n \n     # Estimate the runtime of the function\n@@ -151,7 +155,10 @@ def do_bench(fn, warmup=25, rep=100, grad_to_none=None, percentiles=[0.5, 0.2, 0\n     # doesn't contain any input data before the run\n     start_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n     end_event = [torch.cuda.Event(enable_timing=True) for i in range(n_repeat)]\n-    cache = torch.empty(int(256e6), dtype=torch.int8, device='cuda')\n+    if fast_flush:\n+        cache = torch.empty(int(256e6 // 4), dtype=torch.int, device='cuda')\n+    else:\n+        cache = torch.empty(int(256e6), dtype=torch.int8, device='cuda')\n     # Warm-up\n     for _ in range(n_warmup):\n         fn()"}]