[{"filename": ".github/workflows/integration-tests.yml", "status": "modified", "additions": 43, "deletions": 5, "changes": 48, "file_content_changes": "@@ -25,7 +25,7 @@ jobs:\n         id: set-matrix\n         run: |\n           if [ x\"${{ github.repository }}\" == x\"openai/triton\" ]; then\n-            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"gfx908\"]]'\n+            echo '::set-output name=matrix::[[\"self-hosted\", \"A100\"], [\"self-hosted\", \"V100\"], [\"self-hosted\", \"gfx908\"], [\"self-hosted\", \"arc770\"]]'\n           else\n             echo '::set-output name=matrix::[\"ubuntu-latest\"]'\n           fi\n@@ -53,6 +53,11 @@ jobs:\n         run: |\n           echo \"BACKEND=ROCM\" >> \"${GITHUB_ENV}\"\n \n+      - name: Set XPU ENV\n+        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'arc770')}}\n+        run: |\n+          echo \"BACKEND=XPU\" >> \"${GITHUB_ENV}\"\n+\n       - name: Clear cache\n         run: |\n           rm -rf ~/.triton\n@@ -62,13 +67,22 @@ jobs:\n           echo \"PATH=${HOME}/.local/bin:${PATH}\" >> \"${GITHUB_ENV}\"\n \n       - name: Check pre-commit\n-        if: ${{ matrix.runner != 'macos-10.15' }}\n+        if: ${{ matrix.runner != 'macos-10.15' && (matrix.runner[1] != 'arc770') }}\n         run: |\n           python3 -m pip install --upgrade pre-commit\n           python3 -m pre_commit run --all-files\n \n+      - name: Check pre-commit arc770\n+        if: ${{ matrix.runner != 'macos-10.15' && (matrix.runner[1] == 'arc770') }}\n+        run: |\n+          source ${HOME}/triton_vars.sh\n+          source ${HOME}/miniconda3/bin/activate\n+          conda activate triton-xpu-ci\n+          python3 -m pip install --upgrade pre-commit\n+          python3 -m pre_commit run --all-files\n+\n       - name: Install Triton\n-        if: ${{ env.BACKEND != 'ROCM'}}\n+        if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n           cd python\n           python3 -m pip install --upgrade pip\n@@ -84,8 +98,23 @@ jobs:\n           python3 -m pip install torch==1.13.1 --index-url https://download.pytorch.org/whl/rocm5.2\n           python3 -m pip install --no-build-isolation -vvv '.[tests]'\n \n+      - name: Install Triton on XPU\n+        if: ${{ env.BACKEND == 'XPU'}}\n+        run: |\n+          source ${HOME}/triton_vars.sh\n+          source ${HOME}/miniconda3/bin/activate\n+          conda activate triton-xpu-ci\n+          git submodule update --init --recursive\n+          cd python\n+          python3 -m pip install --upgrade pip\n+          python3 -m pip install cmake==3.24\n+          export TRITON_CODEGEN_INTEL_XPU_BACKEND=1\n+          python3 -m pip uninstall -y triton\n+          python3 setup.py build\n+          python3 -m pip install --no-build-isolation -vvv '.[tests]'\n+\n       - name: Run lit tests\n-        if: ${{ env.BACKEND != 'ROCM'}}\n+        if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n           python3 -m pip install lit\n           cd python\n@@ -115,7 +144,7 @@ jobs:\n           path: ~/.triton/artifacts.tar.gz\n \n       - name: Run CXX unittests\n-        if: ${{ env.BACKEND != 'ROCM'}}\n+        if: ${{ env.BACKEND == 'CUDA'}}\n         run: |\n           cd python\n           cd \"build/$(ls build | grep -i cmake)\"\n@@ -127,6 +156,15 @@ jobs:\n           cd python/test/unit/language\n           python3 -m pytest --capture=tee-sys -rfs --verbose \"test_core.py::test_empty_kernel\"\n \n+      - name: Run python tests on XPU\n+        if: ${{ env.BACKEND == 'XPU'}}\n+        run: |\n+          source ${HOME}/triton_vars.sh\n+          source ${HOME}/miniconda3/bin/activate\n+          conda activate triton-xpu-ci\n+          cd python/test/backend/third_party_backends\n+          python3 -m pytest --capture=tee-sys -rfs --verbose --backend xpu\n+\n       - name: Regression tests\n         if: ${{ contains(matrix.runner, 'A100') }}\n         run: |"}, {"filename": ".gitmodules", "status": "added", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -0,0 +1,3 @@\n+[submodule \"third_party/intel_xpu_backend\"]\n+\tpath = third_party/intel_xpu_backend\n+\turl = http://github.com/intel/intel-xpu-backend-for-triton"}, {"filename": "CMakeLists.txt", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "file_content_changes": "@@ -22,6 +22,7 @@ endif()\n # Options\n option(TRITON_BUILD_TUTORIALS \"Build C++ Triton tutorials\" ON)\n option(TRITON_BUILD_PYTHON_MODULE \"Build Python Triton bindings\" OFF)\n+set(TRITON_CODEGEN_BACKENDS \"\" CACHE STRING \"Enable different codegen backends\")\n \n # Ensure Python3 vars are set correctly\n # used conditionally in this file and by lit tests\n@@ -263,6 +264,14 @@ if(TRITON_BUILD_PYTHON_MODULE AND NOT WIN32)\n   target_link_libraries(triton ${CUTLASS_LIBRARIES} ${PYTHON_LDFLAGS})\n endif()\n \n+list(LENGTH TRITON_CODEGEN_BACKENDS CODEGEN_BACKENDS_LEN)\n+if (${CODEGEN_BACKENDS_LEN} GREATER 0)\n+  set(PYTHON_THIRD_PARTY_PATH ${CMAKE_CURRENT_SOURCE_DIR}/python/triton/third_party)\n+  foreach(CODEGEN_BACKEND ${TRITON_CODEGEN_BACKENDS})\n+    add_subdirectory(third_party/${CODEGEN_BACKEND})\n+  endforeach()\n+endif()\n+\n add_subdirectory(test)\n \n add_subdirectory(unittest)"}, {"filename": "python/setup.py", "status": "modified", "additions": 17, "deletions": 3, "changes": 20, "file_content_changes": "@@ -31,6 +31,17 @@ def get_build_type():\n         # TODO: change to release when stable enough\n         return \"TritonRelBuildWithAsserts\"\n \n+\n+def get_codegen_backends():\n+    backends = []\n+    env_prefix = \"TRITON_CODEGEN_\"\n+    for name, _ in os.environ.items():\n+        if name.startswith(env_prefix) and check_env_flag(name):\n+            assert name.count(env_prefix) <= 1\n+            backends.append(name.replace(env_prefix, '').lower())\n+    return backends\n+\n+\n # --- third party packages -----\n \n \n@@ -210,6 +221,11 @@ def build_extension(self, ext):\n         cfg = get_build_type()\n         build_args = [\"--config\", cfg]\n \n+        codegen_backends = get_codegen_backends()\n+        if len(codegen_backends) > 0:\n+            all_codegen_backends = ';'.join(codegen_backends)\n+            cmake_args += [\"-DTRITON_CODEGEN_BACKENDS=\" + all_codegen_backends]\n+\n         if platform.system() == \"Windows\":\n             cmake_args += [f\"-DCMAKE_RUNTIME_OUTPUT_DIRECTORY_{cfg.upper()}={extdir}\"]\n             if sys.maxsize > 2**32:\n@@ -256,9 +272,7 @@ def build_extension(self, ext):\n         \"triton/ops/blocksparse\",\n         \"triton/runtime\",\n         \"triton/runtime/backends\",\n-        \"triton/third_party/cuda/bin\",\n-        \"triton/third_party/cuda/include\",\n-        \"triton/third_party/cuda/lib\",\n+        \"triton/third_party\",\n         \"triton/tools\",\n     ],\n     install_requires=["}, {"filename": "python/test/backend/extension_backend.c", "status": "added", "additions": 42, "deletions": 0, "changes": 42, "file_content_changes": "@@ -0,0 +1,42 @@\n+#include <Python.h>\n+#include <stdio.h>\n+#include <stdlib.h>\n+\n+static PyObject *getDeviceProperties(PyObject *self, PyObject *args) {\n+  // create a struct to hold device properties\n+  return Py_BuildValue(\"{s:i, s:i, s:i, s:i, s:i}\", \"max_shared_mem\", 1024,\n+                       \"multiprocessor_count\", 16, \"sm_clock_rate\", 2100,\n+                       \"mem_clock_rate\", 2300, \"mem_bus_width\", 2400);\n+}\n+\n+static PyObject *loadBinary(PyObject *self, PyObject *args) {\n+  // get allocated registers and spilled registers from the function\n+  int n_regs = 0;\n+  int n_spills = 0;\n+  int mod = 0;\n+  int fun = 0;\n+  return Py_BuildValue(\"(KKii)\", (uint64_t)mod, (uint64_t)fun, n_regs,\n+                       n_spills);\n+}\n+\n+static PyMethodDef ModuleMethods[] = {\n+    {\"load_binary\", loadBinary, METH_VARARGS,\n+     \"Load dummy binary for the extension device\"},\n+    {\"get_device_properties\", getDeviceProperties, METH_VARARGS,\n+     \"Get the properties for the extension device\"},\n+    {NULL, NULL, 0, NULL} // sentinel\n+};\n+\n+static struct PyModuleDef ModuleDef = {PyModuleDef_HEAD_INIT, \"ext_utils\",\n+                                       NULL, // documentation\n+                                       -1,   // size\n+                                       ModuleMethods};\n+\n+PyMODINIT_FUNC PyInit_ext_utils(void) {\n+  PyObject *m = PyModule_Create(&ModuleDef);\n+  if (m == NULL) {\n+    return NULL;\n+  }\n+  PyModule_AddFunctions(m, ModuleMethods);\n+  return m;\n+}"}, {"filename": "python/test/backend/test_device_backend.py", "status": "added", "additions": 262, "deletions": 0, "changes": 262, "file_content_changes": "@@ -0,0 +1,262 @@\n+import functools\n+import hashlib\n+import importlib\n+import os\n+import shutil\n+import subprocess\n+import sysconfig\n+import tempfile\n+from pathlib import Path\n+\n+import setuptools\n+import torch\n+\n+import triton\n+import triton.language as tl\n+from triton.common.backend import BaseBackend, register_backend\n+from triton.common.build import quiet\n+from triton.compiler.make_launcher import make_so_cache_key\n+from triton.runtime.cache import get_cache_manager\n+from triton.runtime.driver import DriverBase\n+from triton.runtime.jit import version_key\n+\n+\n+def build_for_backend(name, src, srcdir):\n+    suffix = sysconfig.get_config_var('EXT_SUFFIX')\n+    so = os.path.join(srcdir, '{name}{suffix}'.format(name=name, suffix=suffix))\n+    # try to avoid setuptools if possible\n+    cc = os.environ.get(\"CC\")\n+    if cc is None:\n+        # TODO: support more things here.\n+        clang = shutil.which(\"clang\")\n+        gcc = shutil.which(\"gcc\")\n+        cc = gcc if gcc is not None else clang\n+        if cc is None:\n+            raise RuntimeError(\"Failed to find C compiler. Please specify via CC environment variable.\")\n+    # This function was renamed and made public in Python 3.10\n+    if hasattr(sysconfig, 'get_default_scheme'):\n+        scheme = sysconfig.get_default_scheme()\n+    else:\n+        scheme = sysconfig._get_default_scheme()\n+    # 'posix_local' is a custom scheme on Debian. However, starting Python 3.10, the default install\n+    # path changes to include 'local'. This change is required to use triton with system-wide python.\n+    if scheme == 'posix_local':\n+        scheme = 'posix_prefix'\n+    py_include_dir = sysconfig.get_paths(scheme=scheme)[\"include\"]\n+\n+    ret = subprocess.check_call([cc, src, f\"-I{py_include_dir}\", f\"-I{srcdir}\", \"-shared\", \"-fPIC\", \"-o\", so])\n+    if ret == 0:\n+        return so\n+    # fallback on setuptools\n+    extra_compile_args = []\n+    library_dirs = []\n+    include_dirs = [srcdir]\n+    libraries = []\n+    # extra arguments\n+    extra_link_args = []\n+    # create extension module\n+    ext = setuptools.Extension(\n+        name=name,\n+        language='c',\n+        sources=[src],\n+        include_dirs=include_dirs,\n+        extra_compile_args=extra_compile_args + ['-O3'],\n+        extra_link_args=extra_link_args,\n+        library_dirs=library_dirs,\n+        libraries=libraries,\n+    )\n+    # build extension module\n+    args = ['build_ext']\n+    args.append('--build-temp=' + srcdir)\n+    args.append('--build-lib=' + srcdir)\n+    args.append('-q')\n+    args = dict(\n+        name=name,\n+        ext_modules=[ext],\n+        script_args=args,\n+    )\n+    with quiet():\n+        setuptools.setup(**args)\n+    return so\n+\n+\n+class ExtensionUtils:\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(ExtensionUtils, cls).__new__(cls)\n+        return cls.instance\n+\n+    def __init__(self):\n+        dirname = os.path.dirname(os.path.realpath(__file__))\n+        src = Path(os.path.join(dirname, \"extension_backend.c\")).read_text()\n+        key = hashlib.md5(src.encode(\"utf-8\")).hexdigest()\n+        cache = get_cache_manager(key)\n+        fname = \"ext_utils.so\"\n+        cache_path = cache.get_file(fname)\n+        if cache_path is None:\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                src_path = os.path.join(tmpdir, \"main.c\")\n+                with open(src_path, \"w\") as f:\n+                    f.write(src)\n+                so = build_for_backend(\"ext_utils\", src_path, tmpdir)\n+                with open(so, \"rb\") as f:\n+                    cache_path = cache.put(f.read(), fname, binary=True)\n+        import importlib.util\n+        spec = importlib.util.spec_from_file_location(\"ext_utils\", cache_path)\n+        mod = importlib.util.module_from_spec(spec)\n+        spec.loader.exec_module(mod)\n+        self.load_binary = mod.load_binary\n+        self.get_device_properties = mod.get_device_properties\n+\n+\n+class ExtensionDriver(DriverBase):\n+    def __new__(cls):\n+        if not hasattr(cls, 'instance'):\n+            cls.instance = super(ExtensionDriver, cls).__new__(cls)\n+        return cls.instance\n+\n+    def __init__(self):\n+        self.utils = ExtensionUtils()\n+\n+\n+class ExtensionBackend(BaseBackend):\n+    stub_so_path = \"\"\n+\n+    def __init__(self, device_type: str) -> None:\n+        super(ExtensionBackend, self).__init__(device_type)\n+        self.driver = ExtensionDriver()\n+\n+    def add_stages(self, arch, extern_libs, stages):\n+        filter_in_stages = [\"ast\", \"ttir\", \"ttgir\"]\n+        filter_out_stages = []\n+        for key, _ in stages.items():\n+            if key not in filter_in_stages:\n+                filter_out_stages.append(key)\n+        for filter_out_key in filter_out_stages:\n+            stages.pop(filter_out_key)\n+\n+    def add_meta_info(self, ir, cur_module, next_module, metadata, asm):\n+        metadata[\"name\"] = \"extension_backend_name\"\n+\n+    def get_driver(self):\n+        return self.driver\n+\n+    def get_stream(self):\n+        return \"\"\n+\n+    @functools.lru_cache(None)\n+    def get_device_properties(self, device):\n+        return self.driver.utils.get_device_properties()\n+\n+    def get_current_device(self):\n+        return torch.device(\"cpu\")\n+\n+    def set_current_device(self, device):\n+        pass\n+\n+    def get_load_binary_fn(self):\n+        return self.driver.utils.load_binary\n+\n+    def get_kernel_bin(self):\n+        return \"ttgir\"\n+\n+    def get_architecture_descriptor(self, **kwargs):\n+        return \"\"\n+\n+    def make_launcher_stub(self, name, signature, constants):\n+        # name of files that are cached\n+        so_cache_key = make_so_cache_key(version_key(), signature, constants)\n+        so_cache_manager = get_cache_manager(so_cache_key)\n+        so_name = f\"{name}.so\"\n+        # retrieve stub from cache if it exists\n+        cache_path = so_cache_manager.get_file(so_name)\n+        if cache_path is None:\n+            with tempfile.TemporaryDirectory() as tmpdir:\n+                src = self._generate_launcher(constants, signature)\n+                src_path = os.path.join(tmpdir, \"main.c\")\n+                with open(src_path, \"w\") as f:\n+                    f.write(src)\n+                so = build_for_backend(name, src_path, tmpdir)\n+                with open(so, \"rb\") as f:\n+                    so_path = so_cache_manager.put(f.read(), so_name, binary=True)\n+                    type(self).stub_so_path = so_path\n+                    return so_path\n+        else:\n+            type(self).stub_so_path = cache_path\n+            return cache_path\n+\n+    def _generate_launcher(self, constants, signature):\n+        # generate glue code\n+        src = \"\"\"\n+        #define __EXTENSION_BACKEND__\n+        #include <Python.h>\n+        #include <stdio.h>\n+\n+        static PyObject* launch_counter(PyObject* self, PyObject* args) {\n+        static int64_t launch_counter = 0;\n+        launch_counter += 1;\n+        return PyLong_FromLong(launch_counter);\n+        }\n+\n+        static PyObject* launch(PyObject* self, PyObject* args) {\n+        if (PyErr_Occurred()) {\n+            return NULL;\n+        }\n+        launch_counter(self, args);\n+        // return None\n+        Py_INCREF(Py_None);\n+        return Py_None;\n+        }\n+\n+        static PyMethodDef ModuleMethods[] = {\n+        {\"launch\", launch, METH_VARARGS, \"Entry point for all kernels with this signature\"},\n+        {\"launch_counter\", launch_counter, METH_VARARGS, \"Entry point to get launch counter\"},\n+        {NULL, NULL, 0, NULL} // sentinel\n+        };\n+\n+        static struct PyModuleDef ModuleDef = {\n+        PyModuleDef_HEAD_INIT,\n+        \\\"__triton_launcher\\\",\n+        NULL, //documentation\n+        -1, //size\n+        ModuleMethods\n+        };\n+\n+        PyMODINIT_FUNC PyInit___triton_launcher(void) {\n+        PyObject *m = PyModule_Create(&ModuleDef);\n+        if(m == NULL) {\n+            return NULL;\n+        }\n+        PyModule_AddFunctions(m, ModuleMethods);\n+        return m;\n+        }\n+        \"\"\"\n+\n+        return src\n+\n+\n+def test_dummy_backend():\n+    register_backend(\"cpu\", ExtensionBackend)\n+\n+    @triton.jit\n+    def kernel(in_ptr0, out_ptr0, xnumel, XBLOCK: tl.constexpr):\n+        xnumel = 10\n+        xoffset = tl.program_id(0) * XBLOCK\n+        xindex = xoffset + tl.arange(0, XBLOCK)[:]\n+        xmask = xindex < xnumel\n+        x0 = xindex\n+        tmp0 = tl.load(in_ptr0 + (x0), xmask)\n+        tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp0, xmask)\n+\n+    inp = torch.randn(10)\n+    out = torch.randn(10)\n+    kernel[(10,)](inp, out, 10, XBLOCK=16)\n+    spec = importlib.util.spec_from_file_location(\"__triton_launcher\", ExtensionBackend.stub_so_path)\n+    mod = importlib.util.module_from_spec(spec)\n+    spec.loader.exec_module(mod)\n+    launch_counter = getattr(mod, \"launch_counter\")\n+\n+    for _ in range(100):\n+        kernel[(10,)](inp, out, 10, XBLOCK=16)\n+\n+    assert launch_counter() > 0"}, {"filename": "python/test/backend/third_party_backends/conftest.py", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "file_content_changes": "@@ -0,0 +1,14 @@\n+# content of conftest.py\n+\n+import pytest\n+\n+\n+def pytest_addoption(parser):\n+    parser.addoption(\n+        \"--backend\", action=\"store\", default=\"\", help=\"Codegen backend\"\n+    )\n+\n+\n+@pytest.fixture\n+def cmdopt(request):\n+    return request.config.getoption(\"--backend\")"}, {"filename": "python/test/backend/third_party_backends/test_xpu_backend.py", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "file_content_changes": "@@ -0,0 +1,33 @@\n+import torch\n+\n+import triton\n+import triton.language as tl\n+\n+\n+def test_xpu_backend(cmdopt):\n+    if cmdopt == \"xpu\":\n+        has_ipex = False\n+        try:\n+            # Import IPEX to provide Intel GPU runtime\n+            import intel_extension_for_pytorch  # type: ignore # noqa: F401\n+            has_ipex = True if hasattr(torch, \"xpu\") else False\n+        except Exception:\n+            has_ipex = False\n+\n+        @triton.jit()\n+        def kernel(x_ptr, y_ptr, out_ptr):\n+            pid = tl.program_id(axis=0)\n+            x = tl.load(x_ptr + pid)\n+            y = tl.load(y_ptr + pid)\n+            out = x + y\n+            tl.store(out_ptr + pid, out)\n+\n+        if has_ipex:\n+            for _ in range(1000):\n+                x = torch.randn((65536,), device=\"xpu\", dtype=torch.float32)\n+                y = torch.randn((65536,), device=\"xpu\", dtype=torch.float32)\n+                z = torch.zeros((65536,), device=\"xpu\", dtype=torch.float32)\n+                kernel[(65536,)](x, y, z, num_warps=32)\n+                assert torch.all(x + y == z)\n+    else:\n+        return"}, {"filename": "python/test/regression/test_performance.py", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -69,7 +69,7 @@ def nvsmi(attrs):\n         (16, 1024, 1024): {'float16': 0.0077, 'float32': 0.0127, 'int8': 0.005},\n         (16, 4096, 4096): {'float16': 0.044, 'float32': 0.0457, 'int8': 0.0259},\n         (16, 8192, 8192): {'float16': 0.07, 'float32': 0.0648, 'int8': 0.0431},\n-        (64, 1024, 1024): {'float16': 0.030, 'float32': 0.0509, 'int8': 0.0169},\n+        (64, 1024, 1024): {'float16': 0.028, 'float32': 0.0509, 'int8': 0.0169},\n         (64, 4096, 4096): {'float16': 0.163, 'float32': 0.162, 'int8': 0.097},\n         (64, 8192, 8192): {'float16': 0.285, 'float32': 0.257, 'int8': 0.174},\n         (1024, 64, 1024): {'float16': 0.033, 'float32': 0.0458, 'int8': 0.017},"}, {"filename": "python/triton/common/backend.py", "status": "added", "additions": 96, "deletions": 0, "changes": 96, "file_content_changes": "@@ -0,0 +1,96 @@\n+\n+import importlib\n+import importlib.util\n+from typing import Dict\n+\n+from triton.runtime.driver import DriverBase\n+\n+\n+class BaseBackend:\n+    def __init__(self, device_type: str) -> None:\n+        self.device_type = device_type\n+\n+    def add_stages(self, arch, extern_libs, stages):\n+        \"\"\"\n+        Custom the arch, extern_libs and stages per backend specific requirement\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def add_meta_info(self, ir, cur_module, next_module, metadata, asm):\n+        \"\"\"\n+        Custom the ir, module, metadata and asm per backend specific requirement\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_load_binary_fn(self):\n+        \"\"\"\n+        Return a callable to load binary\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_driver(self) -> DriverBase:\n+        \"\"\"\n+        Get the backend driver. Please refer to \"DriverBase\" for more details\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_stream(self):\n+        \"\"\"\n+        Get stream for current device\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_device_properties(self, device):\n+        raise NotImplementedError\n+\n+    def get_current_device(self):\n+        \"\"\"\n+        Get current device\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def set_current_device(self, device):\n+        \"\"\"\n+        Set current device as the given device\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_kernel_bin(self):\n+        raise NotImplementedError\n+\n+    def make_launcher_stub(self, name, signature, constants):\n+        \"\"\"\n+        Generate the launcher stub to launch the kernel\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_architecture_descriptor(self, **kwargs):\n+        \"\"\"\n+        Get the architecture descriptor the backend\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    @classmethod\n+    def create_backend(cls, device_type: str):\n+        return cls(device_type)\n+\n+\n+_backends: Dict[str, BaseBackend] = {}\n+\n+\n+def register_backend(device_type: str, backend_cls: type):\n+    if device_type not in _backends:\n+        _backends[device_type] = backend_cls.create_backend(device_type)\n+\n+\n+def get_backend(device_type: str):\n+    if device_type not in _backends:\n+        device_backend_package_name = f\"triton.third_party.{device_type}\"\n+        if importlib.util.find_spec(device_backend_package_name):\n+            try:\n+                importlib.import_module(device_backend_package_name)\n+            except Exception:\n+                return None\n+        else:\n+            return None\n+    return _backends[device_type] if device_type in _backends else None"}, {"filename": "python/triton/compiler/compiler.py", "status": "modified", "additions": 53, "deletions": 13, "changes": 66, "file_content_changes": "@@ -13,6 +13,7 @@\n \n import triton\n import triton._C.libtriton.triton as _triton\n+from ..common.backend import get_backend\n from ..runtime import driver\n # TODO: runtime.errors\n from ..runtime.autotuner import OutOfResources\n@@ -362,8 +363,19 @@ def add_cuda_stages(arch, extern_libs, stages):\n \n \n def compile(fn, **kwargs):\n-    arch = get_architecture_descriptor(kwargs.get(\"cc\", None))\n-    is_cuda = _is_cuda(arch)\n+    # Get device type to decide which backend should be used\n+    device_type = kwargs.get(\"device_type\", \"cuda\")\n+    _device_backend = get_backend(device_type)\n+\n+    if device_type in [\"cuda\", \"hip\"]:\n+        arch = get_architecture_descriptor(kwargs.get(\"cc\", None))\n+    else:\n+        _device_backend = get_backend(device_type)\n+        assert _device_backend\n+        arch = _device_backend.get_architecture_descriptor(**kwargs)\n+\n+    is_cuda = device_type == \"cuda\" and _is_cuda(arch)\n+    is_hip = device_type in [\"cuda\", \"hip\"] and not is_cuda\n     context = _triton.ir.context()\n     asm = dict()\n     constants = kwargs.get(\"constants\", dict())\n@@ -373,6 +385,7 @@ def compile(fn, **kwargs):\n     if extern_libs is None:\n         extern_libs = dict()\n     debug = kwargs.get(\"debug\", False)\n+\n     # build compilation stages\n     stages = dict()\n     stages[\"ast\"] = (lambda path: fn, None)\n@@ -384,8 +397,10 @@ def compile(fn, **kwargs):\n                       lambda src: ttgir_to_llir(src, extern_libs, arch))\n     if is_cuda:\n         add_cuda_stages(arch, extern_libs, stages)\n-    else:\n+    elif is_hip:\n         add_rocm_stages(arch, extern_libs, stages)\n+    else:\n+        _device_backend.add_stages(arch, extern_libs, stages)\n \n     # find out the signature of the function\n     if isinstance(fn, triton.runtime.JITFunction):\n@@ -418,7 +433,11 @@ def compile(fn, **kwargs):\n         first_stage = list(stages.keys()).index(ir)\n \n     # cache manager\n-    so_path = make_stub(name, signature, constants)\n+    if is_cuda or is_hip:\n+        so_path = make_stub(name, signature, constants)\n+    else:\n+        so_path = _device_backend.make_launcher_stub(name, signature, constants)\n+\n     # create cache manager\n     fn_cache_manager = get_cache_manager(make_hash(fn, arch, **kwargs))\n     # determine name and extension type of provided function\n@@ -451,6 +470,9 @@ def compile(fn, **kwargs):\n             assert \"shared\" in kwargs, \"ptx compilation must provide shared memory size\"\n             metadata[\"shared\"] = kwargs[\"shared\"]\n \n+    # Add device type to meta information\n+    metadata[\"device_type\"] = device_type\n+\n     first_stage = list(stages.keys()).index(ext)\n     asm = dict()\n     module = fn\n@@ -493,6 +515,8 @@ def compile(fn, **kwargs):\n         if ir == \"amdgcn\":\n             metadata[\"name\"] = get_kernel_name(next_module[0], pattern='.globl')\n             asm[\"hsaco_path\"] = next_module[1]\n+        if not is_cuda and not is_hip:\n+            _device_backend.add_meta_info(ir, module, next_module, metadata, asm)\n         module = next_module\n     # write-back metadata, if it didn't come from the cache\n     if metadata_path is None:\n@@ -518,10 +542,12 @@ def __init__(self, fn, so_path, metadata, asm):\n         spec.loader.exec_module(mod)\n         self.c_wrapper = getattr(mod, \"launch\")\n         # initialize metadata\n-        self.shared = metadata[\"shared\"]\n+        self.shared = metadata[\"shared\"] if \"shared\" in metadata else 0\n         self.num_warps = metadata[\"num_warps\"]\n         self.num_stages = metadata[\"num_stages\"]\n         self.constants = metadata[\"constants\"]\n+        self.device_type = metadata[\"device_type\"]\n+        self.device_backend = get_backend(self.device_type) if self.device_type not in [\"cuda\", \"hip\"] else None\n         # initialize asm dict\n         self.asm = asm\n         # binaries are lazily initialized\n@@ -534,15 +560,26 @@ def __init__(self, fn, so_path, metadata, asm):\n     def _init_handles(self):\n         if self.cu_module is not None:\n             return\n-        device = triton.runtime.jit.get_current_device()\n-        bin_path = {\n-            driver.HIP: \"hsaco_path\",\n-            driver.CUDA: \"cubin\"\n-        }[driver.backend]\n-        max_shared = driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n+\n+        if self.device_type in [\"cuda\", \"hip\"]:\n+            device = triton.runtime.jit.get_current_device()\n+            bin_path = {\n+                driver.HIP: \"hsaco_path\",\n+                driver.CUDA: \"cubin\"\n+            }[driver.backend]\n+            max_shared = driver.utils.get_device_properties(device)[\"max_shared_mem\"]\n+            fn_load_binary = driver.utils.load_binary\n+        else:\n+            assert self.device_backend\n+            device = self.device_backend.get_current_device()\n+            bin_path = self.device_backend.get_kernel_bin()\n+            max_shared = self.device_backend.get_device_properties(device)[\"max_shared_mem\"]\n+            fn_load_binary = self.device_backend.get_load_binary_fn()\n+\n         if self.shared > max_shared:\n             raise OutOfResources(self.shared, max_shared, \"shared memory\")\n-        mod, func, n_regs, n_spills = driver.utils.load_binary(self.metadata[\"name\"], self.asm[bin_path], self.shared, device)\n+\n+        mod, func, n_regs, n_spills = fn_load_binary(self.metadata[\"name\"], self.asm[bin_path], self.shared, device)\n \n         self.n_spills = n_spills\n         self.n_regs = n_regs\n@@ -559,7 +596,10 @@ def __getitem__(self, grid):\n \n         def runner(*args, stream=None):\n             if stream is None:\n-                stream = triton.runtime.jit.get_cuda_stream()\n+                if self.device_type in [\"cuda\", \"rocm\"]:\n+                    stream = triton.runtime.jit.get_cuda_stream()\n+                else:\n+                    stream = get_backend(self.device_type).get_stream(None)\n             self.c_wrapper(grid[0], grid[1], grid[2], self.num_warps, self.shared, stream, self.cu_function,\n                            CompiledKernel.launch_enter_hook, CompiledKernel.launch_exit_hook, self, *args)\n         return runner"}, {"filename": "python/triton/runtime/jit.py", "status": "modified", "additions": 71, "deletions": 9, "changes": 80, "file_content_changes": "@@ -8,9 +8,13 @@\n import subprocess\n import textwrap\n from collections import defaultdict, namedtuple\n-from typing import Callable, Generic, Iterable, Optional, TypeVar, Union, cast, overload\n+from typing import (Callable, Generic, Iterable, List, Optional, TypeVar, Union, cast,\n+                    overload)\n+\n+import torch\n \n import triton\n+from triton.common.backend import get_backend\n \n \n def get_cuda_stream(idx=None):\n@@ -158,6 +162,22 @@ def _key_of(arg):\n         else:\n             raise TypeError(f'Unsupported type {type(arg)} for {arg}')\n \n+    @staticmethod\n+    def _device_of(arg):\n+        if hasattr(arg, \"device\"):\n+            if hasattr(arg.device, 'type'):\n+                return arg.device.type\n+\n+        return ''\n+\n+    @staticmethod\n+    def _pinned_memory_of(arg):\n+        if hasattr(arg, \"is_pinned\"):\n+            if isinstance(arg.is_pinned, Callable):\n+                return arg.is_pinned()\n+\n+        return False\n+\n     @staticmethod\n     def _spec_of(arg):\n         if hasattr(arg, \"data_ptr\"):\n@@ -261,12 +281,28 @@ def _get_arg_sig_key(self, arg) -> str:\n         else:\n             return f'_key_of({arg})'\n \n+    def _conclude_device_type(self, device_types: List[str], pinned_memory_flags: List[bool]) -> str:\n+        device_types = [device_type for device_type in device_types if device_type != '']\n+        # Return cuda if one of the input tensors is cuda\n+        if 'cuda' in device_types:\n+            return 'hip' if torch.version.hip else 'cuda'\n+\n+        is_cpu = all(device_type == 'cpu' for device_type in device_types)\n+        is_pinned_memory = any(pinned_memory_flag for pinned_memory_flag in pinned_memory_flags)\n+        # Return cuda if all the input tensors are cpu while the memory is pinned\n+        if is_cpu and is_pinned_memory:\n+            return 'cuda'\n+\n+        return device_types[0] if len(device_types) > 0 else 'cuda'\n+\n     def _make_launcher(self):\n         regular_args = [f'{arg}' for i, arg in enumerate(self.arg_names) if i not in self.constexprs]\n         constexpr_args = [f'{arg}' for i, arg in enumerate(self.arg_names) if i in self.constexprs]\n         args = ', '.join(regular_args)\n         # cache key for regular argument type\n         sig_keys = ', '.join([self._get_arg_sig_key(arg) for arg in regular_args])\n+        device_types = '[' + ', '.join([f'_device_of({arg})' for arg in regular_args]) + ']'\n+        pinned_memory_flags = '[' + ', '.join([f'_pinned_memory_of({arg})' for arg in regular_args]) + ']'\n         # cache key for constexpr argument values\n         constexpr_keys = ', '.join(constexpr_args)\n         # cache key for argument specialization\n@@ -280,7 +316,7 @@ def _make_launcher(self):\n         grid_args = ','.join([f'\"{arg}\": {arg}' for arg in self.arg_names])\n \n         src = f\"\"\"\n-def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None):\n+def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False, device=None, device_type=None):\n     sig_key =  {sig_keys},\n     constexpr_key = {f'{constexpr_keys},' if len(constexpr_keys) > 0 else ()}\n     spec_key = {f'{spec_keys},' if len(spec_keys) > 0 else ()}\n@@ -294,11 +330,30 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n     grid_0 = grid[0]\n     grid_1 = grid[1] if grid_size > 1 else 1\n     grid_2 = grid[2] if grid_size > 2 else 1\n+\n+    if device_type is None:\n+        device_types = [_device_type for _device_type in {device_types} if _device_type != '']\n+        device_type = self._conclude_device_type(device_types, {pinned_memory_flags})\n+\n+    device_backend = None\n+    if device_type not in ['cuda', 'hip']:\n+        device_backend = get_backend(device_type)\n+        if device_backend is None:\n+            raise ValueError('Cannot find backend for ' + device_type)\n+\n     if device is None:\n-        device = get_current_device()\n-        set_current_device(device)\n+        if device_type in ['cuda', 'hip']:\n+            device = get_current_device()\n+            set_current_device(device)\n+        else:\n+            device = device_backend.get_current_device()\n+            device_backend.set_current_device(device)\n     if stream is None and not warmup:\n-      stream = get_cuda_stream(device)\n+        if device_type in ['cuda', 'hip']:\n+            stream = get_cuda_stream(device)\n+        else:\n+            stream = device_backend.get_stream()\n+\n     bin = cache[device].get(key, None)\n     if bin is not None:\n       if not warmup:\n@@ -320,16 +375,23 @@ def {self.fn.__name__}({', '.join(self.arg_names)}, grid, num_warps=4, num_stage\n         if callable(arg):\n           raise TypeError(f\"Callable constexpr at index {{i}} is not supported\")\n       if not self._call_hook(key, signature, device, constants, num_warps, num_stages, extern_libs, configs):\n-        bin = triton.compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs, configs=configs, debug=self.debug)\n+        bin = triton.compile(self, signature=signature, device=device, constants=constants, num_warps=num_warps, num_stages=num_stages, extern_libs=extern_libs, configs=configs, debug=self.debug, device_type=device_type)\n         if not warmup:\n             bin.c_wrapper(grid_0, grid_1, grid_2, bin.num_warps, bin.shared, stream, bin.cu_function, triton.compiler.CompiledKernel.launch_enter_hook, triton.compiler.CompiledKernel.launch_exit_hook, bin, *args)\n         self.cache[device][key] = bin\n         return bin\n       return None\n \"\"\"\n-        scope = {\"version_key\": version_key(), \"get_cuda_stream\": get_cuda_stream,\n-                 \"self\": self, \"_spec_of\": self._spec_of, \"_key_of\": self._key_of,\n-                 \"cache\": self.cache, \"triton\": triton,\n+        scope = {\"version_key\": version_key(),\n+                 \"get_cuda_stream\": get_cuda_stream,\n+                 \"self\": self,\n+                 \"_spec_of\": self._spec_of,\n+                 \"_key_of\": self._key_of,\n+                 \"_device_of\": self._device_of,\n+                 \"_pinned_memory_of\": self._pinned_memory_of,\n+                 \"cache\": self.cache,\n+                 \"triton\": triton,\n+                 \"get_backend\": get_backend,\n                  \"get_current_device\": get_current_device,\n                  \"set_current_device\": set_current_device}\n         exec(src, scope)"}, {"filename": "third_party/intel_xpu_backend", "status": "added", "additions": 1, "deletions": 0, "changes": 1, "file_content_changes": "@@ -0,0 +1 @@\n+Subproject commit 0bcc485f82b34d49494bd0264bacc24a20aafb7a"}]