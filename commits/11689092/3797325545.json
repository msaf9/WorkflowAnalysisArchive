[{"filename": "lib/Conversion/TritonGPUToLLVM/DotOpHelpers.h", "status": "modified", "additions": 25, "deletions": 19, "changes": 44, "file_content_changes": "@@ -204,7 +204,12 @@ struct DotOpMmaV1ConversionHelper {\n       offA[i] = add(mul(offA0I, strideA0), mul(offA1, strideA1));\n     }\n \n-    Type f16x2Ty = vec_ty(f16_ty, 2);\n+    Type elemX2Ty = vec_ty(f16_ty, 2);\n+    Type elemPtrTy = ptr_ty(f16_ty);\n+    if (tensorTy.getElementType().isBF16()) {\n+      elemX2Ty = vec_ty(i16_ty, 2);\n+      elemPtrTy = ptr_ty(i16_ty);\n+    }\n \n     // prepare arguments\n     SmallVector<Value> ptrA(numPtrA);\n@@ -213,30 +218,28 @@ struct DotOpMmaV1ConversionHelper {\n     for (int i = 0; i < numPtrA; i++)\n       ptrA[i] = gep(ptr_ty(f16_ty), smemBase, offA[i]);\n \n-    Type f16PtrTy = ptr_ty(f16_ty);\n-\n     auto ld = [&](decltype(has) &vals, int m, int k, Value val0, Value val1) {\n       vals[{m, k}] = {val0, val1};\n     };\n     auto loadA = [&](int m, int k) {\n       int offidx = (isARow ? k / 4 : m) % numPtrA;\n-      Value thePtrA = gep(f16PtrTy, smemBase, offA[offidx]);\n+      Value thePtrA = gep(elemPtrTy, smemBase, offA[offidx]);\n \n       int stepAM = isARow ? m : m / numPtrA * numPtrA;\n       int stepAK = isARow ? k / (numPtrA * vecA) * (numPtrA * vecA) : k;\n       Value offset = add(mul(i32_val(stepAM * strideRepM), strideAM),\n                          mul(i32_val(stepAK), strideAK));\n-      Value pa = gep(f16PtrTy, thePtrA, offset);\n+      Value pa = gep(elemPtrTy, thePtrA, offset);\n       Type aPtrTy = ptr_ty(vec_ty(i32_ty, std::max<int>(vecA / 2, 1)), 3);\n       Value ha = load(bitcast(pa, aPtrTy));\n       // record lds that needs to be moved\n-      Value ha00 = bitcast(extract_element(ha, i32_val(0)), f16x2Ty);\n-      Value ha01 = bitcast(extract_element(ha, i32_val(1)), f16x2Ty);\n+      Value ha00 = bitcast(extract_element(ha, i32_val(0)), elemX2Ty);\n+      Value ha01 = bitcast(extract_element(ha, i32_val(1)), elemX2Ty);\n       ld(has, m, k, ha00, ha01);\n \n       if (vecA > 4) {\n-        Value ha10 = bitcast(extract_element(ha, i32_val(2)), f16x2Ty);\n-        Value ha11 = bitcast(extract_element(ha, i32_val(3)), f16x2Ty);\n+        Value ha10 = bitcast(extract_element(ha, i32_val(2)), elemX2Ty);\n+        Value ha11 = bitcast(extract_element(ha, i32_val(3)), elemX2Ty);\n         if (isARow)\n           ld(has, m, k + 4, ha10, ha11);\n         else\n@@ -256,7 +259,7 @@ struct DotOpMmaV1ConversionHelper {\n       elems.push_back(item.second.second);\n     }\n \n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), f16x2Ty));\n+    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n     Value res = getStructFromElements(loc, elems, rewriter, resTy);\n     return res;\n   }\n@@ -319,8 +322,12 @@ struct DotOpMmaV1ConversionHelper {\n       offB[i] = add(mul(offB0I, strideB0), mul(offB1, strideB1));\n     }\n \n-    Type f16PtrTy = ptr_ty(f16_ty);\n-    Type f16x2Ty = vec_ty(f16_ty, 2);\n+    Type elemPtrTy = ptr_ty(f16_ty);\n+    Type elemX2Ty = vec_ty(f16_ty, 2);\n+    if (tensorTy.getElementType().isBF16()) {\n+      elemPtrTy = ptr_ty(i16_ty);\n+      elemX2Ty = vec_ty(i16_ty, 2);\n+    }\n \n     SmallVector<Value> ptrB(numPtrB);\n     ValueTable hbs;\n@@ -339,17 +346,17 @@ struct DotOpMmaV1ConversionHelper {\n       int stepBK = isBRow ? K : K / (numPtrB * vecB) * (numPtrB * vecB);\n       Value offset = add(mul(i32_val(stepBN * strideRepN), strideBN),\n                          mul(i32_val(stepBK), strideBK));\n-      Value pb = gep(f16PtrTy, thePtrB, offset);\n+      Value pb = gep(elemPtrTy, thePtrB, offset);\n \n       Value hb =\n           load(bitcast(pb, ptr_ty(vec_ty(i32_ty, std::max(vecB / 2, 1)), 3)));\n       // record lds that needs to be moved\n-      Value hb00 = bitcast(extract_element(hb, i32_val(0)), f16x2Ty);\n-      Value hb01 = bitcast(extract_element(hb, i32_val(1)), f16x2Ty);\n+      Value hb00 = bitcast(extract_element(hb, i32_val(0)), elemX2Ty);\n+      Value hb01 = bitcast(extract_element(hb, i32_val(1)), elemX2Ty);\n       ld(hbs, n, K, hb00, hb01);\n       if (vecB > 4) {\n-        Value hb10 = bitcast(extract_element(hb, i32_val(2)), f16x2Ty);\n-        Value hb11 = bitcast(extract_element(hb, i32_val(3)), f16x2Ty);\n+        Value hb10 = bitcast(extract_element(hb, i32_val(2)), elemX2Ty);\n+        Value hb11 = bitcast(extract_element(hb, i32_val(3)), elemX2Ty);\n         if (isBRow)\n           ld(hbs, n + 1, K, hb10, hb11);\n         else\n@@ -369,8 +376,7 @@ struct DotOpMmaV1ConversionHelper {\n       elems.push_back(item.second.first);\n       elems.push_back(item.second.second);\n     }\n-    Type fp16x2Ty = vec_ty(type::f16Ty(ctx), 2);\n-    Type resTy = struct_ty(SmallVector<Type>(elems.size(), fp16x2Ty));\n+    Type resTy = struct_ty(SmallVector<Type>(elems.size(), elemX2Ty));\n     Value res = getStructFromElements(loc, elems, rewriter, resTy);\n     return res;\n   }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Combine.cpp", "status": "modified", "additions": 224, "deletions": 3, "changes": 227, "file_content_changes": "@@ -22,6 +22,10 @@\n using namespace mlir;\n namespace {\n #include \"TritonGPUCombine.inc\"\n+using triton::DotOp;\n+using triton::gpu::ConvertLayoutOp;\n+using triton::gpu::DotOperandEncodingAttr;\n+using triton::gpu::MmaEncodingAttr;\n \n // -----------------------------------------------------------------------------\n //\n@@ -1019,6 +1023,7 @@ class OptimizeConvertToDotOperand : public mlir::RewritePattern {\n         dstDotOperandLayout.getIsMMAv1Row().cast<BoolAttr>().getValue();\n     if ((order[0] == 1 && isMMAv1Row) || (order[0] == 0 && !isMMAv1Row))\n       return failure();\n+\n     auto newIsRow = BoolAttr::get(op->getContext(), !isMMAv1Row);\n     auto newDstEncoding = triton::gpu::DotOperandEncodingAttr::get(\n         op->getContext(), dstDotOperandLayout.getOpIdx(),\n@@ -1060,7 +1065,8 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto dotOp = cast<triton::DotOp>(op);\n     // TODO: Check data-types and SM compatibility\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n-    if (oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n+    if (!oldRetType.getEncoding() ||\n+        oldRetType.getEncoding().isa<triton::gpu::MmaEncodingAttr>())\n       return failure();\n \n     auto AType = dotOp.getOperand(0).getType().cast<RankedTensorType>();\n@@ -1170,7 +1176,8 @@ class FixupLoop : public mlir::RewritePattern {\n     for (size_t i = 0; i < newInitArgs.size(); i++) {\n       auto initArg = newInitArgs[i];\n       auto regionArg = forOp.getRegionIterArgs()[i];\n-      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType()) {\n+      if (newInitArgs[i].getType() != forOp.getRegionIterArgs()[i].getType() ||\n+          newInitArgs[i].getType() != forOp.getResultTypes()[i]) {\n         shouldRematerialize = true;\n         break;\n       }\n@@ -1186,15 +1193,207 @@ class FixupLoop : public mlir::RewritePattern {\n     BlockAndValueMapping mapping;\n     for (const auto &arg : llvm::enumerate(forOp.getRegionIterArgs()))\n       mapping.map(arg.value(), newForOp.getRegionIterArgs()[arg.index()]);\n+    mapping.map(forOp.getInductionVar(), newForOp.getInductionVar());\n \n     for (Operation &op : forOp.getBody()->getOperations()) {\n-      Operation *newOp = rewriter.clone(op, mapping);\n+      rewriter.clone(op, mapping);\n     }\n     rewriter.replaceOp(forOp, newForOp.getResults());\n     return success();\n   }\n };\n \n+// This pattern collects the wrong Mma those need to update and create the right\n+// ones for each.\n+class CollectMmaToUpdateForVolta : public mlir::RewritePattern {\n+  DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+\n+public:\n+  CollectMmaToUpdateForVolta(\n+      mlir::MLIRContext *ctx,\n+      DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : mlir::RewritePattern(triton::DotOp::getOperationName(), 1, ctx),\n+        mmaToUpdate(mmaToUpdate) {}\n+\n+  mlir::LogicalResult\n+  matchAndRewrite(mlir::Operation *op,\n+                  mlir::PatternRewriter &rewriter) const override {\n+\n+    auto dotOp = cast<triton::DotOp>(op);\n+    auto *ctx = dotOp->getContext();\n+    auto AT = dotOp.a().getType().cast<RankedTensorType>();\n+    auto BT = dotOp.b().getType().cast<RankedTensorType>();\n+    auto DT = dotOp.d().getType().cast<RankedTensorType>();\n+    if (!DT.getEncoding())\n+      return failure();\n+    auto mmaLayout = DT.getEncoding().dyn_cast<MmaEncodingAttr>();\n+    if (!(mmaLayout && mmaLayout.isVolta()))\n+      return failure();\n+\n+    // Has processed.\n+    if (mmaToUpdate.count(mmaLayout))\n+      return failure();\n+\n+    auto dotOperandA = AT.getEncoding().cast<DotOperandEncodingAttr>();\n+    auto dotOperandB = BT.getEncoding().cast<DotOperandEncodingAttr>();\n+    bool isARow = dotOperandA.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    bool isBRow = dotOperandB.getIsMMAv1Row().cast<BoolAttr>().getValue();\n+    auto [isARow_, isBRow_, isAVec4, isBVec4] =\n+        mmaLayout.decodeVoltaLayoutStates();\n+    if (isARow_ == isARow && isBRow_ == isBRow) {\n+      return failure(); // No need to update\n+    }\n+\n+    auto newMmaLayout = MmaEncodingAttr::get(\n+        ctx, mmaLayout.getVersionMajor(), mmaLayout.getWarpsPerCTA(),\n+        AT.getShape(), BT.getShape(), isARow, isBRow);\n+\n+    // Collect the wrong MMA Layouts, and mark need to update.\n+    mmaToUpdate.try_emplace(mmaLayout, newMmaLayout);\n+\n+    return failure();\n+  }\n+};\n+\n+// Correct the versionMinor field in MmaEncodingAttr for Volta.\n+class UpdateMMAVersionMinorForVolta : public mlir::RewritePattern {\n+  const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate;\n+  enum class Kind {\n+    kUnk,\n+    kCvtToMma,\n+    kCvtToDotOp,\n+    kDot,\n+    kConstant,\n+  };\n+  mutable Kind rewriteKind{Kind::kUnk};\n+\n+public:\n+  UpdateMMAVersionMinorForVolta(\n+      mlir::MLIRContext *ctx, llvm::StringRef opName,\n+      const DenseMap<MmaEncodingAttr, MmaEncodingAttr> &mmaToUpdate)\n+      : RewritePattern(opName, 1 /*benefit*/, ctx), mmaToUpdate(mmaToUpdate) {}\n+\n+  LogicalResult match(Operation *op) const override {\n+    MmaEncodingAttr mma;\n+    if (mmaToUpdate.empty())\n+      return failure();\n+    if (op->getNumResults() != 1)\n+      return failure();\n+    auto tensorTy = op->getResult(0).getType().dyn_cast<RankedTensorType>();\n+    if (!tensorTy)\n+      return failure();\n+\n+    // ConvertLayoutOp\n+    if (auto cvt = llvm::dyn_cast<ConvertLayoutOp>(op)) {\n+      // cvt X -> dot_operand\n+      if (auto dotOperand =\n+              tensorTy.getEncoding().dyn_cast<DotOperandEncodingAttr>()) {\n+        mma = dotOperand.getParent().dyn_cast<MmaEncodingAttr>();\n+        rewriteKind = Kind::kCvtToDotOp;\n+        if (mma && mmaToUpdate.count(mma))\n+          return success();\n+      }\n+      if ((mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>())) {\n+        // cvt X -> mma\n+        rewriteKind = Kind::kCvtToMma;\n+        if (mma && mmaToUpdate.count(mma))\n+          return success();\n+      }\n+    } else if (auto dot = llvm::dyn_cast<DotOp>(op)) {\n+      // DotOp\n+      mma = dot.d()\n+                .getType()\n+                .cast<RankedTensorType>()\n+                .getEncoding()\n+                .dyn_cast<MmaEncodingAttr>();\n+      rewriteKind = Kind::kDot;\n+    } else if (auto constant = llvm::dyn_cast<arith::ConstantOp>(op)) {\n+      // ConstantOp\n+      mma = tensorTy.getEncoding().dyn_cast<MmaEncodingAttr>();\n+      rewriteKind = Kind::kConstant;\n+    }\n+\n+    return success(mma && mmaToUpdate.count(mma));\n+  }\n+\n+  void rewrite(Operation *op, PatternRewriter &rewriter) const override {\n+    switch (rewriteKind) {\n+    case Kind::kDot:\n+      rewriteDot(op, rewriter);\n+      break;\n+    case Kind::kConstant:\n+      rewriteConstant(op, rewriter);\n+      break;\n+    case Kind::kCvtToDotOp:\n+      rewriteCvtDotOp(op, rewriter);\n+      break;\n+    case Kind::kCvtToMma:\n+      rewriteCvtToMma(op, rewriter);\n+      break;\n+    default:\n+      llvm::report_fatal_error(\"Not supported rewrite kind\");\n+    }\n+  }\n+\n+private:\n+  void rewriteCvtDotOp(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n+    auto dotOperand = tensorTy.getEncoding().cast<DotOperandEncodingAttr>();\n+    MmaEncodingAttr newMma =\n+        mmaToUpdate.lookup(dotOperand.getParent().cast<MmaEncodingAttr>());\n+    auto newDotOperand = DotOperandEncodingAttr::get(\n+        ctx, dotOperand.getOpIdx(), newMma, dotOperand.getIsMMAv1Row());\n+    auto newTensorTy = RankedTensorType::get(\n+        tensorTy.getShape(), tensorTy.getElementType(), newDotOperand);\n+    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                 cvt.getOperand());\n+  }\n+\n+  void rewriteDot(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto dot = llvm::cast<DotOp>(op);\n+    auto tensorTy = dot.d().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto newMma = mmaToUpdate.lookup(mma);\n+    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n+                                             tensorTy.getElementType(), newMma);\n+    rewriter.replaceOpWithNewOp<DotOp>(op, newTensorTy, dot.a(), dot.b(),\n+                                       dot.c(), dot.allowTF32());\n+  }\n+\n+  void rewriteCvtToMma(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto cvt = llvm::cast<ConvertLayoutOp>(op);\n+    auto tensorTy = cvt.result().getType().cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto newMma = mmaToUpdate.lookup(mma);\n+    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n+                                             tensorTy.getElementType(), newMma);\n+    rewriter.replaceOpWithNewOp<ConvertLayoutOp>(op, newTensorTy,\n+                                                 cvt.getOperand());\n+  }\n+\n+  void rewriteConstant(Operation *op, PatternRewriter &rewriter) const {\n+    auto *ctx = op->getContext();\n+    auto constant = llvm::cast<arith::ConstantOp>(op);\n+    auto tensorTy = constant.getResult().getType().dyn_cast<RankedTensorType>();\n+    auto mma = tensorTy.getEncoding().cast<MmaEncodingAttr>();\n+    auto newMma = mmaToUpdate.lookup(mma);\n+    auto newTensorTy = RankedTensorType::get(tensorTy.getShape(),\n+                                             tensorTy.getElementType(), newMma);\n+    if (auto attr = constant.getValue().dyn_cast<SplatElementsAttr>()) {\n+      auto newRet =\n+          SplatElementsAttr::get(newTensorTy, attr.getSplatValue<Attribute>());\n+      rewriter.replaceOpWithNewOp<arith::ConstantOp>(op, newTensorTy, newRet);\n+      return;\n+    }\n+\n+    assert(false && \"Not supported ConstantOp value type\");\n+  }\n+};\n+\n } // namespace\n \n #define GEN_PASS_CLASSES\n@@ -1229,6 +1428,28 @@ class TritonGPUCombineOpsPass\n       signalPassFailure();\n     }\n \n+    llvm::DenseMap<MmaEncodingAttr, MmaEncodingAttr> mmaToUpdate;\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<CollectMmaToUpdateForVolta>(context, mmaToUpdate);\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns)).failed())\n+        signalPassFailure();\n+    }\n+    {\n+      mlir::RewritePatternSet patterns(context);\n+      patterns.add<UpdateMMAVersionMinorForVolta>(\n+          context, DotOp::getOperationName(), mmaToUpdate);\n+      patterns.add<UpdateMMAVersionMinorForVolta>(\n+          context, ConvertLayoutOp::getOperationName(), mmaToUpdate);\n+      patterns.add<UpdateMMAVersionMinorForVolta>(\n+          context, arith::ConstantOp::getOperationName(), mmaToUpdate);\n+      mlir::GreedyRewriteConfig config;\n+      config.useTopDownTraversal = true;\n+\n+      if (applyPatternsAndFoldGreedily(m, std::move(patterns), config).failed())\n+        signalPassFailure();\n+    }\n+\n     mlir::RewritePatternSet loopFixup(context);\n     loopFixup.add<FixupLoop>(context);\n     if (applyPatternsAndFoldGreedily(m, std::move(loopFixup)).failed()) {"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 32, "deletions": 4, "changes": 36, "file_content_changes": "@@ -1,4 +1,4 @@\n-// RUN: triton-opt %s -tritongpu-combine 2>&1 | FileCheck %s\n+// RUN: triton-opt %s -split-input-file -tritongpu-combine 2>&1 | FileCheck %s\n \n #layout0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #layout1 = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n@@ -7,7 +7,6 @@\n // CHECK: [[row_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 4], order = [1, 0]}>\n // CHECK: [[col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n-\n func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -62,9 +61,9 @@ func @remat(%arg0: i32) -> tensor<1024xi32, #layout1> {\n // CHECK-LABEL: transpose\n func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n-  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, %cst, %cst_0 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n+  // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[row_layout]]>\n   // CHECK: [[cvt_val:%.*]] = triton_gpu.convert_layout [[loaded_val]] : (tensor<64x64xf32, [[row_layout]]>) -> tensor<64x64xf32, [[col_layout]]>\n-  // CHECK: tt.store {{.*}}, [[cvt_val]], %cst_1 : tensor<64x64xf32, [[col_layout]]>\n+  // CHECK: tt.store {{.*}}, [[cvt_val]], {{%cst.*}} : tensor<64x64xf32, [[col_layout]]>\n   // CHECK: return\n   %cst = arith.constant dense<0.000000e+00> : tensor<64x64xf32, #blocked1>\n   %cst_0 = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n@@ -184,3 +183,32 @@ func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f3\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   return\n }\n+\n+\n+// -----\n+\n+// check the UpdateMMAVersionMinorForVolta pattern\n+#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [8, 4], warpsPerCTA = [1, 1], order = [1, 0]}>\n+#shared0 = #triton_gpu.shared<{vec = 1, perPhase=2, maxPhase=8 ,order = [1, 0]}>\n+#mma0 = #triton_gpu.mma<{versionMajor=1, versionMinor=0, warpsPerCTA=[1,1]}>\n+// Here, the isMMAv1Row of a and b's dot_operands mismatch #mma0's versionMinor,\n+// and the pattern should update the versionMinor.\n+#dot_operand_a = #triton_gpu.dot_op<{opIdx=0, parent=#mma0, isMMAv1Row=true}>\n+#dot_operand_b = #triton_gpu.dot_op<{opIdx=1, parent=#mma0, isMMAv1Row=false}>\n+// It creates a new MMA layout to fit with $a and $b's dot_operand\n+// CHECK: [[new_mma:#mma.*]] = #triton_gpu.mma<{versionMajor = 1, versionMinor = 11, warpsPerCTA = [1, 1]}>\n+module attributes {\"triton_gpu.num-warps\" = 1 : i32} {\n+  // CHECK-LABEL: dot_mmav1\n+  func @dot_mmav1(%A: tensor<16x16xf16, #blocked0>, %B: tensor<16x16xf16, #blocked0>) -> tensor<16x16xf32, #blocked0> {\n+    %C = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked0>\n+    %AA = triton_gpu.convert_layout %A : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_a>\n+    %BB = triton_gpu.convert_layout %B : (tensor<16x16xf16, #blocked0>) -> tensor<16x16xf16, #dot_operand_b>\n+    %CC = triton_gpu.convert_layout %C : (tensor<16x16xf32, #blocked0>) -> tensor<16x16xf32, #mma0>\n+\n+    // CHECK: {{.*}} = tt.dot {{.*}}, {{.*}}, %cst {allowTF32 = true} : tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 0, parent = [[new_mma]], isMMAv1Row = true}>> * tensor<16x16xf16, #triton_gpu.dot_op<{opIdx = 1, parent = [[new_mma]], isMMAv1Row = true}>> -> tensor<16x16xf32, [[new_mma]]>\n+    %D = tt.dot %AA, %BB, %CC {allowTF32 = true} : tensor<16x16xf16, #dot_operand_a> * tensor<16x16xf16, #dot_operand_b> -> tensor<16x16xf32, #mma0>\n+    %res = triton_gpu.convert_layout %D : (tensor<16x16xf32, #mma0>) -> tensor<16x16xf32, #blocked0>\n+\n+    return %res : tensor<16x16xf32, #blocked0>\n+  }\n+}"}]