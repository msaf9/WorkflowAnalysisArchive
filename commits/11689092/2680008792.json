[{"filename": "python/tutorials/06-fused-attention.py", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "file_content_changes": "@@ -1,7 +1,8 @@\n \"\"\"\n Fused Attention\n ===============\n-This is a Triton implementation of the Flash Attention algorithm (Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf)\n+This is a Triton implementation of the Flash Attention algorithm \n+(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf; Rabe and Staats https://arxiv.org/pdf/2112.05682v2.pdf)\n \"\"\"\n \n import pytest\n@@ -349,5 +350,5 @@ def bench_flash_attention(BATCH, H, N_CTX, D_HEAD, mode, provider, dtype=torch.f\n         ms = triton.testing.do_bench(fn, percentiles=None, warmup=warmup, rep=rep)\n         return ms\n \n-\n-bench_flash_attention.run(save_path='.', print_data=True)\n+# only works on A100 at the moment\n+# bench_flash_attention.run(save_path='.', print_data=True)"}]