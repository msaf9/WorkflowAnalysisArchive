[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "file_content_changes": "@@ -12,19 +12,19 @@ namespace mlir {\n \n class ReduceOpHelper {\n public:\n-  explicit ReduceOpHelper(triton::ReduceOp rop)\n-      : op(rop.getOperation()), axis(rop.getAxis()) {\n-    auto firstTy = rop.getOperands()[0].getType().cast<RankedTensorType>();\n+  explicit ReduceOpHelper(triton::ReduceOp op)\n+      : op(op.getOperation()), axis(op.getAxis()) {\n+    auto firstTy = op.getOperands()[0].getType().cast<RankedTensorType>();\n     srcShape = firstTy.getShape();\n     srcEncoding = firstTy.getEncoding();\n-    srcElementTypes = rop.getElementTypes();\n+    srcElementTypes = op.getElementTypes();\n \n-    for (const auto &t : rop.getInputTypes()) {\n+    for (const auto &t : op.getInputTypes()) {\n       if (t.getShape() != srcShape) {\n-        rop.emitError() << \"shape mismatch\";\n+        op.emitError() << \"shape mismatch\";\n       }\n       if (t.getEncoding() != srcEncoding) {\n-        rop.emitError() << \"encoding mismatch\";\n+        op.emitError() << \"encoding mismatch\";\n       }\n     }\n   }\n@@ -33,6 +33,8 @@ class ReduceOpHelper {\n \n   Attribute getSrcLayout() { return srcEncoding; }\n \n+  triton::ReduceOp getOperation() { return op; }\n+\n   bool isFastReduction();\n \n   unsigned getInterWarpSize();\n@@ -54,7 +56,7 @@ class ReduceOpHelper {\n   bool isSupportedLayout();\n \n private:\n-  Operation *op;\n+  triton::ReduceOp op;\n   ArrayRef<int64_t> srcShape;\n   Attribute srcEncoding;\n   SmallVector<Type> srcElementTypes;"}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "file_content_changes": "@@ -28,8 +28,13 @@ unsigned getTotalElemsPerThread(Attribute layout, ArrayRef<int64_t> shape,\n \n SmallVector<unsigned> getElemsPerThread(Type type);\n \n+// Returns the number of threads per warp that may have access to replicated\n+// elements. If you want non-replicated threads, use\n+// getThreadsPerWarpWithUniqueData.\n SmallVector<unsigned> getThreadsPerWarp(Attribute layout);\n \n+// Returns the number of warps per CTA that may have access to replicated\n+// elements. If you want non-replicated warps, use getWarpsPerCTAWithUniqueData.\n SmallVector<unsigned> getWarpsPerCTA(Attribute layout);\n \n SmallVector<unsigned> getSizePerThread(Attribute layout);"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -5,11 +5,15 @@\n #include \"mlir/IR/Matchers.h\"\n #include \"triton/Dialect/Triton/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Tools/Sys/GetEnv.hpp\"\n #include <deque>\n \n namespace mlir {\n \n bool ReduceOpHelper::isFastReduction() {\n+  // Disable fast reduction only for debugging purpose\n+  if (::triton::tools::getBoolEnv(\"DISABLE_FAST_REDUCTION\"))\n+    return false;\n   return axis == triton::gpu::getOrder(getSrcLayout())[0];\n }\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ConvertLayoutOpToLLVM/SharedToDotOperandMMAv2.cpp", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "file_content_changes": "@@ -559,9 +559,6 @@ Value loadArg(ConversionPatternRewriter &rewriter, Location loc, Value tensor,\n \n   SmallVector<Value> multiDimWarpId =\n       delinearize(rewriter, loc, warp, warpsPerCTA, order);\n-  unsigned lastAxis = order[order.size() - 1];\n-  multiDimWarpId[lastAxis] =\n-      urem(multiDimWarpId[lastAxis], i32_val(warpsPerCTA[lastAxis]));\n   Value warpM = urem(multiDimWarpId[0], i32_val(shape[0] / 16));\n   Value warpN = urem(multiDimWarpId[1], i32_val(shape[1] / 8));\n "}, {"filename": "lib/Conversion/TritonGPUToLLVM/ReduceOpToLLVM.cpp", "status": "modified", "additions": 8, "deletions": 3, "changes": 11, "file_content_changes": "@@ -370,8 +370,10 @@ struct ReduceOpConversion\n     Value warpId = udiv(threadId, warpSize);\n     Value laneId = urem(threadId, warpSize);\n \n-    auto threadsPerWarp = triton::gpu::getThreadsPerWarp(srcLayout);\n-    auto warpsPerCTA = triton::gpu::getWarpsPerCTA(srcLayout);\n+    auto threadsPerWarp =\n+        triton::gpu::getThreadsPerWarpWithUniqueData(srcLayout, srcShape);\n+    auto warpsPerCTA =\n+        triton::gpu::getWarpsPerCTAWithUniqueData(srcLayout, srcShape);\n     auto order = getOrder(srcLayout);\n     SmallVector<Value> multiDimLaneId =\n         delinearize(rewriter, loc, laneId, threadsPerWarp, order);\n@@ -415,8 +417,11 @@ struct ReduceOpConversion\n     //\n     // Each thread needs to process:\n     //   elemsPerThread = sizeInterWarps * s1 * s2 .. Sn / numThreads\n+\n+    auto mod = op.getOperation()->getParentOfType<ModuleOp>();\n     unsigned numThreads =\n-        product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) * 32;\n+        product<unsigned>(triton::gpu::getWarpsPerCTA(srcLayout)) *\n+        triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n     unsigned elemsPerThread = std::max<unsigned>(elems / numThreads, 1);\n     Value readOffset = threadId;\n     for (unsigned round = 0; round < elemsPerThread; ++round) {"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.cpp", "status": "modified", "additions": 28, "deletions": 11, "changes": 39, "file_content_changes": "@@ -79,30 +79,47 @@ SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n   unsigned rank = shape.size();\n   assert(rank == order.size());\n   auto reordered = reorder(shape, order);\n-  auto reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n+  SmallVector<Value> reorderedMultiDim(rank);\n+  if (auto constantOp = linear.getDefiningOp<arith::ConstantOp>()) {\n+    unsigned intVal =\n+        constantOp.getValue().cast<IntegerAttr>().getValue().getSExtValue();\n+    reorderedMultiDim = delinearize(rewriter, loc, intVal, reordered);\n+  } else {\n+    reorderedMultiDim = delinearize(rewriter, loc, linear, reordered);\n+  }\n   SmallVector<Value> multiDim(rank);\n   for (unsigned i = 0; i < rank; ++i) {\n     multiDim[order[i]] = reorderedMultiDim[i];\n   }\n   return multiDim;\n }\n \n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, unsigned linear,\n+                               ArrayRef<unsigned> shape) {\n+  unsigned rank = shape.size();\n+  assert(rank > 0);\n+  SmallVector<Value> multiDim(rank);\n+  unsigned remained = linear;\n+  for (auto &&en : llvm::enumerate(shape)) {\n+    unsigned dimSize = en.value();\n+    multiDim[en.index()] = i32_val(remained % dimSize);\n+    remained = remained / dimSize;\n+  }\n+  return multiDim;\n+}\n+\n SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n                                Location loc, Value linear,\n                                ArrayRef<unsigned> shape) {\n   unsigned rank = shape.size();\n   assert(rank > 0);\n   SmallVector<Value> multiDim(rank);\n-  if (rank == 1) {\n-    multiDim[0] = linear;\n-  } else {\n-    Value remained = linear;\n-    for (auto &&en : llvm::enumerate(shape.drop_back())) {\n-      Value dimSize = i32_val(en.value());\n-      multiDim[en.index()] = urem(remained, dimSize);\n-      remained = udiv(remained, dimSize);\n-    }\n-    multiDim[rank - 1] = remained;\n+  Value remained = linear;\n+  for (auto &&en : llvm::enumerate(shape)) {\n+    Value dimSize = i32_val(en.value());\n+    multiDim[en.index()] = urem(remained, dimSize);\n+    remained = udiv(remained, dimSize);\n   }\n   return multiDim;\n }"}, {"filename": "lib/Conversion/TritonGPUToLLVM/Utility.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -265,6 +265,10 @@ SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n                                ArrayRef<unsigned> shape,\n                                ArrayRef<unsigned> order);\n \n+SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n+                               Location loc, unsigned linear,\n+                               ArrayRef<unsigned> shape);\n+\n SmallVector<Value> delinearize(ConversionPatternRewriter &rewriter,\n                                Location loc, Value linear,\n                                ArrayRef<unsigned> shape);"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -85,6 +85,8 @@ SmallVector<unsigned> getThreadsPerWarp(Attribute layout) {\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parent = sliceLayout.getParent();\n     auto parentThreadsPerWarp = getThreadsPerWarp(parent);\n+    assert(parentThreadsPerWarp.size() == 2 &&\n+           \"getThreadsPerWarp only implemented for 2D slice layout\");\n     SmallVector<unsigned> threadsPerWarp = parentThreadsPerWarp;\n     threadsPerWarp.erase(threadsPerWarp.begin() + sliceLayout.getDim());\n     for (unsigned i = 0; i < threadsPerWarp.size(); i++)\n@@ -129,6 +131,8 @@ SmallVector<unsigned> getWarpsPerCTA(Attribute layout) {\n   if (auto sliceLayout = layout.dyn_cast<SliceEncodingAttr>()) {\n     auto parent = sliceLayout.getParent();\n     auto parentWarpsPerCTA = getWarpsPerCTA(parent);\n+    assert(parentWarpsPerCTA.size() == 2 &&\n+           \"getWarpsPerCTA only implemented for 2D slice layout\");\n     SmallVector<unsigned> warpsPerCTA = parentWarpsPerCTA;\n     warpsPerCTA.erase(warpsPerCTA.begin() + sliceLayout.getDim());\n     for (unsigned i = 0; i < warpsPerCTA.size(); i++)"}, {"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 18, "deletions": 6, "changes": 24, "file_content_changes": "@@ -1683,7 +1683,18 @@ def _welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n \n @pytest.mark.parametrize(\"M, N\", [[128, 128], [256, 128], [256, 256], [128, 256]])\n @pytest.mark.parametrize(\"src_layout\", layouts)\n-def test_chain_reduce(M, N, src_layout, device):\n+@pytest.mark.parametrize(\"op\", [\"sum\", \"max\"])\n+def test_chain_reduce(M, N, src_layout, op, device):\n+    op_str = \"\"\n+    if op == \"sum\":\n+        op_str = f\"\"\"\n+        %13 = arith.addi %arg2, %arg3 : i32\n+        tt.reduce.return %13 : i32\"\"\"\n+    elif op == \"max\":\n+        op_str = f\"\"\"\n+        %13 = \"triton_gpu.cmpi\"(%arg2, %arg3) <{{predicate = 4 : i64}}> : (i32, i32) -> i1\n+        %14 = arith.select %13, %arg2, %arg3 : i32\n+        tt.reduce.return %14 : i32\"\"\"\n     ir = f\"\"\"\n     #src = {src_layout}\n     module attributes {{\"triton_gpu.num-warps\" = 4 : i32}} {{\n@@ -1702,13 +1713,11 @@ def test_chain_reduce(M, N, src_layout, device):\n         %10 = tt.load %9 {{cache = 1 : i32, evict = 1 : i32, isVolatile = false}} : tensor<{M}x{N}xi32, #src>\n         %11 = \"tt.reduce\"(%10) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n-        %13 = arith.addi %arg2, %arg3 : i32\n-        tt.reduce.return %13 : i32\n+        {op_str}\n         }}) {{axis = 1 : i32}} : (tensor<{M}x{N}xi32, #src>) -> tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>\n         %12 = \"tt.reduce\"(%11) ({{\n         ^bb0(%arg2: i32, %arg3: i32):\n-        %13 = arith.addi %arg2, %arg3 : i32\n-        tt.reduce.return %13 : i32\n+        {op_str}\n         }}) {{axis = 0 : i32}} : (tensor<{M}xi32, #triton_gpu.slice<{{dim = 1, parent = #src}}>>) -> i32\n         tt.store %arg1, %12 {{cache = 1 : i32, evict = 1 : i32}} : i32\n         tt.return\n@@ -1730,7 +1739,10 @@ def test_chain_reduce(M, N, src_layout, device):\n     z_tri = torch.tensor(z, device=device)\n \n     pgm = kernel[(1, 1, 1)](x_tri, z_tri)\n-    z_ref = np.sum(x)\n+    if op == \"sum\":\n+        z_ref = np.sum(x)\n+    elif op == \"max\":\n+        z_ref = np.max(x)\n \n     np.testing.assert_allclose(z_ref, z_tri.cpu().numpy(), rtol=0.01, atol=1e-3)\n "}]