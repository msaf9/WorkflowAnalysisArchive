[{"filename": "python/test/unit/language/test_core.py", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "file_content_changes": "@@ -2484,11 +2484,11 @@ def kernel(ptr, n_elements, num1, num2, type: tl.constexpr):\n # -------------\n \n \n-@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and\"])\n+@pytest.mark.parametrize(\"if_type\", [\"if\", \"if_exp\", \"if_and_dynamic\", \"if_and_static\"])\n def test_if(if_type):\n \n     @triton.jit\n-    def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr):\n+    def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr, StaticVaue: tl.constexpr):\n         pid = tl.program_id(0)\n         cond = tl.load(Cond)\n         if IfType == \"if\":\n@@ -2498,17 +2498,22 @@ def kernel(Cond, XTrue, XFalse, Ret, IfType: tl.constexpr, BoolVar: tl.constexpr\n                 tl.store(Ret, tl.load(XFalse))\n         elif IfType == \"if_exp\":\n             tl.store(Ret, tl.load(XTrue)) if pid % 2 else tl.store(Ret, tl.load(XFalse))\n-        elif IfType == \"if_and\":\n+        elif IfType == \"if_and_dynamic\":\n             if BoolVar and pid % 2 == 0:\n                 tl.store(Ret, tl.load(XTrue))\n             else:\n                 tl.store(Ret, tl.load(XFalse))\n+        elif IfType == \"if_and_static\":\n+            if StaticVaue != 0 and StaticVaue != 0:\n+                tl.store(Ret, tl.load(XTrue))\n+            else:\n+                tl.store(Ret, tl.load(XFalse))\n \n     cond = torch.ones(1, dtype=torch.int32, device='cuda')\n     x_true = torch.tensor([3.14], dtype=torch.float32, device='cuda')\n     x_false = torch.tensor([1.51], dtype=torch.float32, device='cuda')\n     ret = torch.empty(1, dtype=torch.float32, device='cuda')\n-    kernel[(1,)](cond, x_true, x_false, ret, if_type, True)\n+    kernel[(1,)](cond, x_true, x_false, ret, if_type, True, 1)\n     assert torch.equal(ret, x_true)\n \n "}, {"filename": "python/triton/compiler/code_generator.py", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "file_content_changes": "@@ -595,12 +595,14 @@ def visit_Pass(self, node):\n     def visit_Compare(self, node):\n         if not (len(node.comparators) == 1 and len(node.ops) == 1):\n             raise UnsupportedLanguageConstruct(None, node, \"simultaneous multiple comparison is not supported\")\n-        lhs = _unwrap_if_constexpr(self.visit(node.left))\n-        rhs = _unwrap_if_constexpr(self.visit(node.comparators[0]))\n+        lhs = self.visit(node.left)\n+        rhs = self.visit(node.comparators[0])\n+        lhs_value = _unwrap_if_constexpr(lhs)\n+        rhs_value = _unwrap_if_constexpr(rhs)\n         if type(node.ops[0]) == ast.Is:\n-            return constexpr(lhs is rhs)\n+            return constexpr(lhs_value is rhs_value)\n         if type(node.ops[0]) == ast.IsNot:\n-            return constexpr(lhs is not rhs)\n+            return constexpr(lhs_value is not rhs_value)\n         method_name = self._method_name_for_comp_op.get(type(node.ops[0]))\n         if method_name is None:\n             raise UnsupportedLanguageConstruct(None, node, \"AST comparison operator '{}' is not (currently) implemented.\".format(node.ops[0].__name__))\n@@ -988,7 +990,7 @@ def execute_static_assert(self, node: ast.Call) -> None:\n         if not (0 < arg_count <= 2) or len(node.keywords):\n             raise TypeError(\"`static_assert` requires one or two positional arguments only\")\n \n-        passed = self.visit(node.args[0])\n+        passed = _unwrap_if_constexpr(self.visit(node.args[0]))\n         if not isinstance(passed, bool):\n             raise NotImplementedError(\"Assertion condition could not be determined at compile-time. Make sure that it depends only on `constexpr` values\")\n         if not passed:"}]