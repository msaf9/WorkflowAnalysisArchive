[{"filename": "include/triton/Analysis/Utility.h", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "file_content_changes": "@@ -69,9 +69,11 @@ bool supportMMA(triton::DotOp op, int version);\n \n bool supportMMA(Value value, int version);\n \n-Type getElementType(Value value);\n+bool isSingleValue(Value value);\n \n-std::string getValueOperandName(Value value, AsmState &state);\n+bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n+\n+Type getElementType(Value value);\n \n template <typename T_OUT, typename T_IN>\n inline SmallVector<T_OUT> convertType(ArrayRef<T_IN> in) {\n@@ -120,10 +122,6 @@ template <typename T> T nextPowOf2(T n) {\n   return n + 1;\n }\n \n-bool isSingleValue(Value value);\n-\n-bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy);\n-\n /// Multi-root DAG topological sort.\n /// Performs a topological sort of the Operation in the `toSort` SetVector.\n /// Returns a topologically sorted SetVector."}, {"filename": "include/triton/Dialect/TritonGPU/IR/Dialect.h", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "file_content_changes": "@@ -75,13 +75,11 @@ SmallVector<unsigned> getOrder(Attribute layout);\n \n bool isaDistributedLayout(Attribute layout);\n \n-bool expensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n+bool isSharedEncoding(Value value);\n \n } // namespace gpu\n } // namespace triton\n \n-bool isSharedEncoding(Value value);\n-\n } // namespace mlir\n \n #endif // TRITON_DIALECT_TRITONGPU_IR_DIALECT_H_"}, {"filename": "include/triton/Dialect/TritonGPU/Transforms/Utility.h", "status": "renamed", "additions": 11, "deletions": 5, "changes": 16, "file_content_changes": "@@ -1,9 +1,13 @@\n-#ifndef TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n-#define TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#ifndef TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#define TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+\n #include \"mlir/IR/Matchers.h\"\n #include \"mlir/IR/PatternMatch.h\"\n #include \"llvm/ADT/MapVector.h\"\n \n+#include \"triton/Dialect/Triton/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+\n namespace mlir {\n \n LogicalResult fixupLoops(ModuleOp mod);\n@@ -12,9 +16,11 @@ LogicalResult fixupLoops(ModuleOp mod);\n LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n                              Attribute &ret);\n \n-bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding);\n+\n+bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding);\n \n-bool expensiveToRemat(Operation *op, Attribute &targetEncoding);\n+bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding);\n \n // skipInit is True when we only consider the operands of the initOp but\n // not the initOp itself.\n@@ -36,4 +42,4 @@ LogicalResult canMoveOutOfLoop(BlockArgument arg,\n \n } // namespace mlir\n \n-#endif // TRITON_LIB_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_\n+#endif // TRITON_DIALECT_TRITONGPU_TRANSFORMS_UTILITY_H_"}, {"filename": "include/triton/Target/AMDGCN/AMDGCNTranslation.h", "status": "removed", "additions": 0, "deletions": 19, "changes": 19, "file_content_changes": "@@ -1,19 +0,0 @@\n-#ifndef TRITON_TARGET_AMDGCNTRANSLATION_H\n-#define TRITON_TARGET_AMDGCNTRANSLATION_H\n-\n-#include <string>\n-#include <tuple>\n-\n-namespace llvm {\n-class Module;\n-} // namespace llvm\n-\n-namespace triton {\n-\n-// Translate LLVM IR to AMDGCN code.\n-std::tuple<std::string, std::string>\n-translateLLVMIRToAMDGCN(llvm::Module &module, std::string cc);\n-\n-} // namespace triton\n-\n-#endif"}, {"filename": "lib/Analysis/Alias.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -38,7 +38,7 @@ void SharedMemoryAliasAnalysis::visitOperation(\n       // insert_slice %src into %dst[%offsets]\n       aliasInfo = AliasInfo(operands[1]->getValue());\n       pessimistic = false;\n-    } else if (isSharedEncoding(result)) {\n+    } else if (triton::gpu::isSharedEncoding(result)) {\n       aliasInfo.insert(result);\n       pessimistic = false;\n     }"}, {"filename": "lib/Analysis/Allocation.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -151,7 +151,7 @@ class AllocationAnalysis {\n     }\n \n     for (Value result : op->getResults()) {\n-      if (isSharedEncoding(result)) {\n+      if (triton::gpu::isSharedEncoding(result)) {\n         // Bytes could be a different value once we support padding or other\n         // allocation policies.\n         auto tensorType = result.getType().dyn_cast<RankedTensorType>();"}, {"filename": "lib/Analysis/Utility.cpp", "status": "modified", "additions": 0, "deletions": 7, "changes": 7, "file_content_changes": "@@ -163,13 +163,6 @@ Type getElementType(Value value) {\n   return type;\n }\n \n-std::string getValueOperandName(Value value, AsmState &state) {\n-  std::string opName;\n-  llvm::raw_string_ostream ss(opName);\n-  value.printAsOperand(ss, state);\n-  return opName;\n-}\n-\n bool isMmaToDotShortcut(RankedTensorType &srcTy, RankedTensorType &dstTy) {\n   // dot_op<opIdx=0, parent=#mma> = #mma\n   // when #mma = MmaEncoding<version=2, warpsPerCTA=[..., 1]>"}, {"filename": "lib/Dialect/TritonGPU/IR/Dialect.cpp", "status": "modified", "additions": 13, "deletions": 24, "changes": 37, "file_content_changes": "@@ -7,6 +7,7 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.cpp.inc\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/TypeSwitch.h\"\n \n using namespace mlir;\n@@ -354,22 +355,6 @@ bool isaDistributedLayout(Attribute layout) {\n          layout.isa<SliceEncodingAttr>();\n }\n \n-bool expensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n-  // If the new elements per thread is less than the old one, we will need to do\n-  // convert encoding that goes through shared memory anyway. So we consider it\n-  // as expensive.\n-  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n-  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n-  auto shape = tensorTy.getShape();\n-  auto elemTy = tensorTy.getElementType();\n-  auto newTotalElemsPerThread =\n-      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n-  return newTotalElemsPerThread < totalElemsPerThread;\n-}\n-\n-} // namespace gpu\n-} // namespace triton\n-\n bool isSharedEncoding(Value value) {\n   auto type = value.getType();\n   if (auto tensorType = type.dyn_cast<RankedTensorType>()) {\n@@ -379,6 +364,9 @@ bool isSharedEncoding(Value value) {\n   return false;\n }\n \n+} // namespace gpu\n+} // namespace triton\n+\n } // namespace mlir\n \n static LogicalResult parseIntAttrValue(AsmParser &parser, Attribute attr,\n@@ -1142,7 +1130,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   if (auto cat = dyn_cast<triton::CatOp>(arg)) {\n     auto encoding =\n         op->getResult(0).getType().cast<RankedTensorType>().getEncoding();\n-    if (triton::gpu::expensiveCat(cat, encoding))\n+    if (isExpensiveCat(cat, encoding))\n       return mlir::failure();\n     rewriter.replaceOpWithNewOp<triton::CatOp>(op, op->getResult(0).getType(),\n                                                cat.getOperands());\n@@ -1151,7 +1139,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(alloc_tensor(x), type2) -> alloc_tensor(x, type2)\n   auto alloc_tensor = dyn_cast<triton::gpu::AllocTensorOp>(arg);\n   if (alloc_tensor) {\n-    if (!isSharedEncoding(op->getResult(0))) {\n+    if (!triton::gpu::isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n     }\n     rewriter.replaceOpWithNewOp<triton::gpu::AllocTensorOp>(\n@@ -1161,7 +1149,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(insert_slice(x), type2) -> insert_slice(cvt(x, type2))\n   auto insert_slice = dyn_cast<triton::gpu::InsertSliceAsyncOp>(arg);\n   if (insert_slice) {\n-    if (!isSharedEncoding(op->getResult(0))) {\n+    if (!triton::gpu::isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n     }\n     auto newType = op->getResult(0).getType().cast<RankedTensorType>();\n@@ -1183,7 +1171,7 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(extract_slice(x), type2) -> extract_slice(cvt(x, type2))\n   auto extract_slice = dyn_cast<triton::gpu::ExtractSliceOp>(arg);\n   if (extract_slice) {\n-    if (!isSharedEncoding(op->getResult(0))) {\n+    if (!triton::gpu::isSharedEncoding(op->getResult(0))) {\n       return mlir::failure();\n     }\n     auto origType =\n@@ -1213,12 +1201,13 @@ LogicalResult ConvertLayoutOp::canonicalize(ConvertLayoutOp op,\n   // cvt(cvt(x, type1), type2) -> cvt(x, type2)\n   if (llvm::isa<triton::gpu::ConvertLayoutOp>(arg)) {\n     if (arg->getOperand(0).getDefiningOp() &&\n-        !isSharedEncoding(arg->getOperand(0)) &&\n-        isSharedEncoding(op.getOperand()) &&\n-        !isSharedEncoding(op.getResult())) {\n+        !triton::gpu::isSharedEncoding(arg->getOperand(0)) &&\n+        triton::gpu::isSharedEncoding(op.getOperand()) &&\n+        !triton::gpu::isSharedEncoding(op.getResult())) {\n       return mlir::failure();\n     }\n-    if (isSharedEncoding(op.getOperand()) && isSharedEncoding(op.getResult())) {\n+    if (triton::gpu::isSharedEncoding(op.getOperand()) &&\n+        triton::gpu::isSharedEncoding(op.getResult())) {\n       return mlir::failure();\n     }\n     auto srcType = op.getOperand().getType().cast<RankedTensorType>();"}, {"filename": "lib/Dialect/TritonGPU/IR/Traits.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -7,7 +7,7 @@ mlir::OpTrait::impl::verifyResultsAreSharedEncoding(Operation *op) {\n     return failure();\n \n   for (auto result : op->getResults())\n-    if (!isSharedEncoding(result))\n+    if (!triton::gpu::isSharedEncoding(result))\n       return op->emitOpError() << \"requires all results to be shared encoding\";\n \n   return success();"}, {"filename": "lib/Dialect/TritonGPU/Transforms/OptimizeDotOperands.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,11 +1,11 @@\n-#include \"Utility.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n #include \"mlir/Transforms/Passes.h\"\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include <memory>\n \n using namespace mlir;"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Pipeline.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -1,4 +1,3 @@\n-#include \"Utility.h\"\n #include \"mlir/Dialect/Tensor/IR/Tensor.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/IR/TypeUtilities.h\"\n@@ -8,6 +7,7 @@\n #include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n #include \"llvm/ADT/MapVector.h\"\n \n //===----------------------------------------------------------------------===//"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Prefetch.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "file_content_changes": "@@ -172,7 +172,7 @@ LogicalResult Prefetcher::initialize() {\n         break;\n       rets.push_back(op->getOperand(0));\n       if (auto cvt = dyn_cast_or_null<triton::gpu::ConvertLayoutOp>(op))\n-        if (isSharedEncoding(cvt.getOperand())) {\n+        if (triton::gpu::isSharedEncoding(cvt.getOperand())) {\n           foundConvertFromShared = true;\n           break;\n         }"}, {"filename": "lib/Dialect/TritonGPU/Transforms/RemoveLayoutConversions.cpp", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "file_content_changes": "@@ -1,4 +1,3 @@\n-#include \"Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n@@ -16,6 +15,7 @@\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/Passes.h\"\n #include \"triton/Dialect/TritonGPU/Transforms/TritonGPUConversion.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n \n #include <memory>\n \n@@ -359,7 +359,7 @@ class RematerializeForward : public mlir::RewritePattern {\n \n     for (Operation *op : cvtSlices) {\n       // don't rematerialize anything expensive\n-      if (expensiveToRemat(op, srcEncoding))\n+      if (isExpensiveToRemat(op, srcEncoding))\n         return failure();\n       // don't rematerialize non-element-wise\n       if (!op->hasTrait<mlir::OpTrait::SameOperandsAndResultEncoding>() &&\n@@ -408,8 +408,8 @@ class RematerializeBackward : public mlir::RewritePattern {\n     if (!op)\n       return mlir::failure();\n     // we don't want to rematerialize any conversion to/from shared\n-    if (isSharedEncoding(cvt->getResults()[0]) ||\n-        isSharedEncoding(cvt->getOperand(0)))\n+    if (triton::gpu::isSharedEncoding(cvt->getResults()[0]) ||\n+        triton::gpu::isSharedEncoding(cvt->getOperand(0)))\n       return mlir::failure();\n     // we don't handle conversions to DotOperandEncodingAttr\n     // this is a heuristics to accommodate fused attention"}, {"filename": "lib/Dialect/TritonGPU/Transforms/Utility.cpp", "status": "modified", "additions": 26, "deletions": 16, "changes": 42, "file_content_changes": "@@ -1,10 +1,10 @@\n-#include \"Utility.h\"\n+#include \"triton/Analysis/Utility.h\"\n #include \"mlir/Analysis/SliceAnalysis.h\"\n #include \"mlir/Dialect/SCF/IR/SCF.h\"\n #include \"mlir/IR/IRMapping.h\"\n #include \"mlir/Transforms/GreedyPatternRewriteDriver.h\"\n-#include \"triton/Analysis/Utility.h\"\n #include \"triton/Dialect/TritonGPU/IR/Dialect.h\"\n+#include \"triton/Dialect/TritonGPU/Transforms/Utility.h\"\n \n namespace mlir {\n \n@@ -88,32 +88,42 @@ LogicalResult invertEncoding(Attribute targetEncoding, Operation *op,\n   return success();\n }\n \n-bool expensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n+bool isExpensiveLoadOrStore(Operation *op, Attribute &targetEncoding) {\n   // Case 1: A size 1 tensor is not expensive since all threads will load the\n   // same\n   if (isSingleValue(op->getOperand(0)))\n     return false;\n   // Case 2: Tensor of pointers has more threads than elements\n   // we can presume a high hit-rate that makes it cheap to load\n   auto ptrType = op->getOperand(0).getType().cast<RankedTensorType>();\n-  IntegerAttr numWarps =\n-      op->getParentOfType<ModuleOp>()->getAttrOfType<IntegerAttr>(\n-          \"triton_gpu.num-warps\");\n-  if (numWarps) {\n-    int sizePerThread = triton::gpu::getTotalElemsPerThread(ptrType);\n-    if (ptrType.getNumElements() < numWarps.getInt() * 32)\n-      return false;\n-  }\n+  auto mod = op->getParentOfType<ModuleOp>();\n+  int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n+  int threadsPerWarp = triton::gpu::TritonGPUDialect::getThreadsPerWarp(mod);\n+  if (ptrType.getNumElements() < numWarps * threadsPerWarp)\n+    return false;\n   return true;\n }\n \n-bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n+bool isExpensiveCat(triton::CatOp cat, Attribute &targetEncoding) {\n+  // If the new elements per thread is less than the old one, we will need to do\n+  // convert encoding that goes through shared memory anyway. So we consider it\n+  // as expensive.\n+  auto tensorTy = cat.getResult().getType().cast<RankedTensorType>();\n+  auto totalElemsPerThread = triton::gpu::getTotalElemsPerThread(tensorTy);\n+  auto shape = tensorTy.getShape();\n+  auto elemTy = tensorTy.getElementType();\n+  auto newTotalElemsPerThread =\n+      triton::gpu::getTotalElemsPerThread(targetEncoding, shape, elemTy);\n+  return newTotalElemsPerThread < totalElemsPerThread;\n+}\n+\n+bool isExpensiveToRemat(Operation *op, Attribute &targetEncoding) {\n   if (!op)\n     return true;\n   if (isa<triton::LoadOp, triton::StoreOp>(op))\n-    return expensiveLoadOrStore(op, targetEncoding);\n+    return isExpensiveLoadOrStore(op, targetEncoding);\n   if (isa<triton::CatOp>(op))\n-    return triton::gpu::expensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   if (isa<tensor::ExtractSliceOp, triton::gpu::AllocTensorOp,\n           triton::gpu::InsertSliceAsyncOp, triton::AtomicRMWOp,\n           triton::AtomicCASOp, triton::DotOp>(op))\n@@ -126,7 +136,7 @@ bool expensiveToRemat(Operation *op, Attribute &targetEncoding) {\n \n bool canFoldConversion(Operation *op, Attribute &targetEncoding) {\n   if (isa<triton::CatOp>(op))\n-    return !triton::gpu::expensiveCat(cast<triton::CatOp>(op), targetEncoding);\n+    return !isExpensiveCat(cast<triton::CatOp>(op), targetEncoding);\n   return isa<triton::gpu::ConvertLayoutOp, arith::ConstantOp,\n              triton::MakeRangeOp, triton::SplatOp, triton::ViewOp>(op);\n }\n@@ -148,7 +158,7 @@ int simulateBackwardRematerialization(\n     queue.pop_back();\n     // If the current operation is expensive to rematerialize,\n     // we stop everything\n-    if (expensiveToRemat(currOp, currLayout))\n+    if (isExpensiveToRemat(currOp, currLayout))\n       break;\n     // A conversion will be removed here (i.e. transferred to operands)\n     numCvts -= 1;"}, {"filename": "test/TritonGPU/combine.mlir", "status": "modified", "additions": 24, "deletions": 5, "changes": 29, "file_content_changes": "@@ -9,6 +9,8 @@\n // CHECK: [[$col_layout:#.*]] = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK: [[$col_layout_novec:#.*]] = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n // CHECK-LABEL: cst\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n tt.func @cst() -> tensor<1024xi32, #layout1> {\n   %cst = arith.constant dense<0> : tensor<1024xi32, #layout0>\n   %1 = triton_gpu.convert_layout %cst : (tensor<1024xi32, #layout0>) -> tensor<1024xi32, #layout1>\n@@ -67,8 +69,6 @@ tt.func @remat_single_value(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   tt.return\n }\n \n-\n-module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   %0 = tt.splat %arg : (!tt.ptr<i32>) -> tensor<16x!tt.ptr<i32>, #layout1>\n   %1 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #layout1>\n@@ -80,7 +80,6 @@ tt.func @remat_fast_load(%arg: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n   tt.store %5, %4 : tensor<16xi32, #layout0>\n   tt.return\n }\n-}\n \n // CHECK-LABEL: if\n tt.func @if(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}) {\n@@ -164,6 +163,8 @@ tt.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility =\n   tt.return\n }\n \n+}\n+\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>\n #slice1dim1 = #triton_gpu.slice<{dim = 1, parent = #blocked1}>\n@@ -173,6 +174,7 @@ tt.func @if_else_both_convert(%arg0: i32, %arg1: !tt.ptr<i32> {tt.divisibility =\n #blocked4 = #triton_gpu.blocked<{sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n // CHECK-LABEL: transpose\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32 {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   // CHECK: [[loaded_val:%.*]] = tt.load {{.*}}, {{%cst.*}}, {{%cst.*}} {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<64x64xf32, [[$row_layout]]>\n@@ -212,8 +214,10 @@ tt.func @transpose(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: i32\n   tt.store %24, %25, %26 : tensor<64x64xf32, #blocked4>\n   tt.return\n }\n+}\n \n // CHECK-LABEL: loop\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   // CHECK: [[loop_ret:%.*]]:2 = scf.for {{.*}} -> (tensor<64x64xf32, [[$row_layout]]>, tensor<64x64x!tt.ptr<f32>, [[$row_layout]]>)\n@@ -266,8 +270,10 @@ tt.func @loop(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32,\n   tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n   tt.return\n }\n+}\n \n // CHECK-LABEL: loop_if\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i32, %arg4: i32) {\n   %cst = arith.constant dense<true> : tensor<64x64xi1, #blocked1>\n   %cst_0 = arith.constant dense<64> : tensor<64x64xi32, #blocked1>\n@@ -318,8 +324,10 @@ tt.func @loop_if(%arg0: !tt.ptr<f32>, %arg1: i32, %arg2: !tt.ptr<f32>, %arg3: i3\n   tt.store %20, %21, %22 : tensor<64x64xf32, #blocked1>\n   tt.return\n }\n+}\n \n // CHECK-LABEL: vecadd\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c256_i32 = arith.constant 256 : i32\n@@ -349,9 +357,11 @@ tt.func @vecadd(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr\n   tt.store %21, %22 : tensor<256xf32, #layout1>\n   tt.return\n }\n+}\n \n // Select has args with different element types\n // CHECK-LABEL: select\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %cst = arith.constant dense<30000> : tensor<1x1xi32, #blocked2>\n@@ -400,9 +410,11 @@ tt.func @select(%arg0: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr\n   }\n   tt.return\n }\n+}\n \n // Make sure the following IR doesn't hang the compiler.\n // CHECK-LABEL: long_func\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg12: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg13: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg14: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg15: !tt.ptr<f64> {tt.divisibility = 16 : i32}, %arg16: i32 {tt.divisibility = 16 : i32}) {\n   %cst = arith.constant dense<1.000000e+00> : tensor<1024xf32, #blocked0>\n   %cst_0 = arith.constant dense<5.000000e-04> : tensor<1024xf32, #blocked0>\n@@ -796,10 +808,12 @@ tt.func public @long_func(%arg0: !tt.ptr<i64> {tt.divisibility = 16 : i32}, %arg\n   tt.store %365, %366 : tensor<1024xf64, #blocked0>\n   tt.return\n }\n+}\n \n // A mnist model from torch inductor.\n // Check if topological sort is working correct and there's no unnecessary convert\n // CHECK-LABEL: mnist\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: i32) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %cst = arith.constant dense<10> : tensor<16x1xi32, #blocked2>\n@@ -884,17 +898,19 @@ tt.func public @mnist(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !\n   tt.store %61, %62, %63 : tensor<16x16xf32, #blocked4>\n   tt.return\n }\n+}\n \n // -----\n \n+// cmpf and cmpi have different operands and result types\n+// CHECK-LABEL: cmp\n #blocked0 = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>\n #blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [8, 1], order = [0, 1]}>\n #blocked2 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [2, 4], order = [0, 1]}>\n #blocked3 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8], order = [0, 1]}>\n #blocked4 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>\n #blocked5 = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [8, 1], order = [1, 0]}>\n-// cmpf and cmpi have different operands and result types\n-// CHECK-LABEL: cmp\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg4: i32 {tt.divisibility = 16 : i32}, %arg5: i32 {tt.divisibility = 16 : i32}) {\n   %c64 = arith.constant 64 : index\n   %c2048 = arith.constant 2048 : index\n@@ -1034,11 +1050,13 @@ tt.func public @cmp(%arg0: !tt.ptr<f16> {tt.divisibility = 16 : i32}, %arg1: !tt\n   }\n   tt.return\n }\n+}\n \n // -----\n \n // Just make sure it doesn't crash on non-tensor types.\n // CHECK-LABEL: if_no_tensor\n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n tt.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32 {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<i64> {tt.divisibility = 16 : i32}) {\n   // CHECK-NOT: triton_gpu.convert_layout\n   %c-1_i64 = arith.constant -1 : i64\n@@ -1062,6 +1080,7 @@ tt.func public @if_no_tensor(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %\n   tt.store %9, %8 {cache = 1 : i32, evict = 1 : i32} : f32\n   tt.return\n }\n+}\n \n // -----\n "}, {"filename": "test/TritonGPU/dot-operands.mlir", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "file_content_changes": "@@ -11,6 +11,8 @@\n #BLR = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>\n #BLC = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [0, 1]}>\n \n+module attributes {\"triton_gpu.num-warps\" = 4 : i32} {\n+\n // CHECK: tt.func @push_elementwise1\n // CHECK: %[[ALOAD:.*]] = tt.load %arg0\n // CHECK: %[[ACVT:.*]] = triton_gpu.convert_layout %[[ALOAD]]\n@@ -122,3 +124,5 @@ tt.func @push_elementwise5(\n   %newc = tt.dot %dota, %dotb, %c {allowTF32 = true, transA = false, transB = false} : tensor<16x16xf16, #Av1> * tensor<16x16xf16, #Bv1> -> tensor<16x16xf32, #Cv1>\n   tt.return %newc : tensor<16x16xf32, #Cv1>\n }\n+\n+}"}, {"filename": "test/lib/Analysis/TestAlias.cpp", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "file_content_changes": "@@ -13,6 +13,13 @@ struct TestAliasPass\n \n   MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(TestAliasPass);\n \n+  static std::string getValueOperandName(Value value, AsmState &state) {\n+    std::string opName;\n+    llvm::raw_string_ostream ss(opName);\n+    value.printAsOperand(ss, state);\n+    return opName;\n+  }\n+\n   static void print(StringRef name, SmallVector<std::string, 4> &vals,\n                     raw_ostream &os) {\n     if (vals.empty())"}]