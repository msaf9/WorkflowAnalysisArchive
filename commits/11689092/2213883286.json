[{"filename": "include/triton/codegen/analysis/align.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "file_content_changes": "@@ -12,6 +12,7 @@ namespace ir {\n   class phi_node;\n   class splat_inst;\n   class cast_inst;\n+  class cmp_inst;\n   class reshape_inst;\n   class broadcast_inst;\n   class binary_operator;\n@@ -35,6 +36,7 @@ class align {\n   std::vector<cst_info> populate_is_constant_reshape(ir::reshape_inst* x);\n   std::vector<cst_info> populate_is_constant_broadcast(ir::broadcast_inst* x);\n   std::vector<cst_info> populate_is_constant_binop(ir::binary_operator* x);\n+  std::vector<cst_info> populate_is_constant_cmp(ir::cmp_inst* x);\n   std::vector<cst_info> populate_is_constant_gep(ir::getelementptr_inst* x);\n   std::vector<cst_info> populate_is_constant_default(ir::value* v);\n   std::vector<cst_info> populate_is_constant(ir::value *v);\n@@ -65,6 +67,7 @@ class align {\n   void run(ir::module &mod);\n   unsigned get(ir::value* v, unsigned ax) const;\n   std::vector<unsigned> contiguous(ir::value* v) const;\n+  std::vector<cst_info> get_cst_info(ir::value* v) const;\n \n private:\n   std::map<ir::value*, std::vector<cst_info>> is_constant_;"}, {"filename": "lib/codegen/analysis/align.cc", "status": "modified", "additions": 38, "deletions": 3, "changes": 41, "file_content_changes": "@@ -129,19 +129,49 @@ std::vector<align::cst_info> align::populate_is_constant_broadcast(ir::broadcast\n   return add_to_cache(x, result, is_constant_);\n }\n \n+std::vector<align::cst_info> align::populate_is_constant_cmp(ir::cmp_inst* x) {\n+  auto x_shapes = get_shapes(x);\n+  std::vector<cst_info> result;\n+  ir::value* lhs_op = x->get_operand(0);\n+  ir::value* rhs_op = x->get_operand(1);\n+  auto lhs = populate_is_constant(lhs_op);\n+  auto rhs = populate_is_constant(rhs_op);\n+  auto lhs_max_contiguous = populate_max_contiguous(lhs_op);\n+  auto rhs_max_contiguous = populate_max_contiguous(rhs_op);\n+  auto lhs_multiple_of = populate_starting_multiple(lhs_op);\n+  auto rhs_multiple_of = populate_starting_multiple(rhs_op);\n+  for(size_t d = 0; d < x_shapes.size(); d++) {\n+    cst_info ax = {1, 0};\n+    // if lhs (resp. rhs) is a range of M value starting at a multiple of N\n+    // and rhs (resp. lhs) is made of M constants that are multiples of N\n+    // then comparisons have M constants\n+    int min_multiple = std::min(lhs_multiple_of[d], rhs_multiple_of[d]);\n+    if(rhs[d].num_cst % lhs_max_contiguous[d] == 0)\n+      ax = {std::min<int>(min_multiple, lhs_max_contiguous[d]), 0};\n+    else if(lhs[d].num_cst % rhs_max_contiguous[d] == 0)\n+      ax = {std::min<int>(min_multiple, rhs_max_contiguous[d]), 0};\n+    result.push_back(ax);\n+  }\n+  return add_to_cache(x, result, is_constant_);\n+}\n+\n+\n std::vector<align::cst_info> align::populate_is_constant_binop(ir::binary_operator* x) {\n   auto x_shapes = get_shapes(x);\n   std::vector<cst_info> result;\n   ir::value* lhs_op = x->get_operand(0);\n   ir::value* rhs_op = x->get_operand(1);\n   auto lhs = populate_is_constant(lhs_op);\n   auto rhs = populate_is_constant(rhs_op);\n-  auto max_contiguous = populate_max_contiguous(lhs_op);\n+  auto lhs_max_contiguous = populate_max_contiguous(lhs_op);\n+  auto rhs_max_contiguous = populate_max_contiguous(rhs_op);\n+  auto lhs_multiple_of = populate_starting_multiple(lhs_op);\n+  auto rhs_multiple_of = populate_starting_multiple(rhs_op);\n   for(size_t d = 0; d < x_shapes.size(); d++) {\n     cst_info ax;\n     if(lhs[d].num_cst==0 && rhs[d].value && x->is_int_div()){\n       // todo might not be entirely true\n-      unsigned num_constants = gcd(max_contiguous[d], rhs[d].value);\n+      unsigned num_constants = gcd(lhs_max_contiguous[d], rhs[d].value);\n       ax = {num_constants, 0};\n     }\n     else\n@@ -184,6 +214,8 @@ std::vector<align::cst_info> align::populate_is_constant(ir::value *v) {\n     return populate_is_constant_broadcast(x);\n   if(auto *x = dynamic_cast<ir::binary_operator*>(v))\n     return populate_is_constant_binop(x);\n+  if(auto *x = dynamic_cast<ir::cmp_inst*>(v))\n+    return populate_is_constant_cmp(x);\n   if(auto *x = dynamic_cast<ir::getelementptr_inst*>(v))\n     return populate_is_constant_gep(x);\n   return populate_is_constant_default(v);\n@@ -511,12 +543,15 @@ std::vector<unsigned> align::contiguous(ir::value* v) const {\n   return max_contiguous_.at(v);\n }\n \n+std::vector<align::cst_info> align::get_cst_info(ir::value* v) const {\n+  return is_constant_.at(v);\n+}\n+\n \n void align::populate(ir::value *v) {\n   populate_is_constant(v);\n   populate_starting_multiple(v);\n   populate_max_contiguous(v);\n-\n }\n \n void align::run(ir::module &mod) {"}, {"filename": "lib/codegen/selection/generator.cc", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "file_content_changes": "@@ -744,6 +744,11 @@ void generator::visit_load_inst(ir::load_inst* x){\n   if(op->get_type()->is_block_ty()){\n     auto   ord = ords_.at(op);\n     size_t aln = alignment_->get(op, ord[0]);\n+    if(mx){\n+      size_t max_eq = alignment_->get_cst_info(mx->get_mask_operand())[ord[0]].num_cst;\n+      max_eq = std::max<size_t>(max_eq, 1);\n+      aln = std::min(aln, max_eq);\n+    }\n     auto layout = layouts_->get(x)->to_scanline();\n     if(layout){\n       size_t nts = layout->nts(ord[0]);\n@@ -912,6 +917,11 @@ void generator::visit_store_inst(ir::store_inst * x){\n     auto ord = ords_.at(x->get_pointer_operand());\n     size_t aln = alignment_->get(ptr_op, ord[0]);\n     size_t nts = axes_.at(a_axes_->get(x->get_pointer_operand(), ord[0])).contiguous;\n+    if(mx){\n+      size_t max_eq = alignment_->get_cst_info(mx->get_mask_operand())[ord[0]].num_cst;\n+      max_eq = std::max<size_t>(max_eq, 1);\n+      aln = std::min(aln, max_eq);\n+    }\n     vec  = std::min(nts, aln);\n   }\n   auto idxs    = idxs_.at(val_op);"}, {"filename": "python/test/unit/runtime/test_comm.py", "status": "removed", "additions": 0, "deletions": 98, "changes": 98, "file_content_changes": "@@ -1,98 +0,0 @@\n-import subprocess\r\n-\r\n-import numpy as np\r\n-import pytest\r\n-import torch\r\n-\r\n-import triton\r\n-import triton.language as tl\r\n-\r\n-\r\n-def get_p2p_matrix():\r\n-    try:\r\n-        stdout = subprocess.check_output([\"nvidia-smi\", \"topo\", \"-p2p\", \"n\"]).decode(\"ascii\")\r\n-    except subprocess.CalledProcessError:\r\n-        return pytest.skip(\"No multi-GPU topology\", allow_module_level=True)\r\n-\r\n-    lines = stdout.split(\"Legend\")[0].split('\\n')[1:]\r\n-    matrix = np.array([line.split('\\t')[1:-1] for line in lines][:-2])\r\n-    if matrix.size <= 1:\r\n-        return pytest.skip(\"No multi-GPU topology\", allow_module_level=True)\r\n-    else:\r\n-        return matrix\r\n-\r\n-\r\n-def get_p2p_devices():\r\n-    matrix = get_p2p_matrix()\r\n-    idx = np.where(matrix == \"OK\")\r\n-    return [f\"cuda:{idx[0][0]}\", f\"cuda:{idx[1][0]}\"] if len(idx[0]) > 0 else []\r\n-\r\n-\r\n-def get_non_p2p_devices():\r\n-    matrix = get_p2p_matrix()\r\n-    idx = np.where(matrix == \"NS\")\r\n-    return [f\"cuda:{idx[0][0]}\", f\"cuda:{idx[1][0]}\"] if len(idx[0]) > 0 else []\r\n-\r\n-\r\n-p2p_devices = get_p2p_devices()\r\n-non_p2p_devices = get_non_p2p_devices()\r\n-\r\n-\r\n-@triton.jit\r\n-def _copy(from_ptr, to_ptr, N, **meta):\r\n-    pid = tl.program_id(0)\r\n-    offsets = pid * meta['BLOCK'] + tl.arange(0, meta['BLOCK'])\r\n-    values = tl.load(from_ptr + offsets, mask=offsets < N)\r\n-    tl.store(to_ptr + offsets, values, mask=offsets < N)\r\n-\r\n-\r\n-@pytest.mark.skipif(not p2p_devices, reason=\"No pair of device with P2P support\")\r\n-@pytest.mark.parametrize(\"device_kernel, device_from, device_to, stream_from, stream_to\",\r\n-                         [(device_kernel, device_from, device_to, stream_from, stream_to)\r\n-                          for device_kernel in p2p_devices\r\n-                          for device_from in p2p_devices\r\n-                          for device_to in p2p_devices\r\n-                          for stream_from in ['default', 'custom']\r\n-                          for stream_to in ['default', 'custom']\r\n-                          ])\r\n-def test_p2p(device_kernel, device_from, device_to, stream_from, stream_to):\r\n-    if device_to == device_from:\r\n-        return pytest.skip()\r\n-\r\n-    torch.cuda.set_device(device_kernel)\r\n-    N = 512\r\n-    grid = lambda meta: (triton.cdiv(N, meta['BLOCK']),)\r\n-\r\n-    with torch.cuda.stream(None if stream_from == 'default' else torch.cuda.Stream(device_from)):\r\n-        x_from = torch.randn(N, dtype=torch.float32, device=device_from)\r\n-    with torch.cuda.stream(None if stream_to == 'default' else torch.cuda.Stream(device_to)):\r\n-        x_to = torch.empty(N, dtype=torch.float32, device=device_to)\r\n-\r\n-    _copy[grid](x_from, x_to, N, BLOCK=1024)\r\n-    assert torch.allclose(x_from, x_to.to(device_from))\r\n-\r\n-\r\n-@pytest.mark.skipif(not non_p2p_devices, reason=\"No pair of device with no P2P support\")\r\n-@pytest.mark.parametrize(\"device_kernel, device_from, device_to, stream_from, stream_to\",\r\n-                         [(device_kernel, device_from, device_to, stream_from, stream_to)\r\n-                          for device_kernel in non_p2p_devices\r\n-                          for device_from in non_p2p_devices\r\n-                          for device_to in non_p2p_devices\r\n-                          for stream_from in ['default', 'custom']\r\n-                          for stream_to in ['default', 'custom']\r\n-                          ])\r\n-def test_non_p2p(device_kernel, device_from, device_to, stream_from, stream_to):\r\n-    if device_to == device_from:\r\n-        return pytest.skip()\r\n-\r\n-    with pytest.raises(RuntimeError):\r\n-        torch.cuda.set_device(device_kernel)\r\n-        N = 512\r\n-        grid = lambda meta: (triton.cdiv(N, meta['BLOCK']),)\r\n-\r\n-        with torch.cuda.stream(None if stream_from == 'default' else torch.cuda.Stream(device_from)):\r\n-            x_from = torch.randn(N, dtype=torch.float32, device=device_from)\r\n-        with torch.cuda.stream(None if stream_to == 'default' else torch.cuda.Stream(device_to)):\r\n-            x_to = torch.empty(N, dtype=torch.float32, device=device_to)\r\n-\r\n-        _copy[grid](x_from, x_to, N, BLOCK=1024)\r"}]