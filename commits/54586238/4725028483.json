[{"filename": "lib/Dialect/TritonGPU/Transforms/AccelerateMatmul.cpp", "status": "modified", "additions": 16, "deletions": 8, "changes": 24, "file_content_changes": "@@ -87,9 +87,23 @@ class BlockedToMMA : public mlir::RewritePattern {\n   mlir::LogicalResult\n   matchAndRewrite(mlir::Operation *op,\n                   mlir::PatternRewriter &rewriter) const override {\n-    if (computeCapability <= 70)\n-      return failure();\n     auto dotOp = cast<triton::DotOp>(op);\n+\n+    // operands\n+    Value a = dotOp.getA();\n+    Value b = dotOp.getB();\n+    auto oldAType = a.getType().cast<RankedTensorType>();\n+    auto oldBType = b.getType().cast<RankedTensorType>();\n+\n+    if (computeCapability < 70) {\n+      if (oldAType.getElementType().isF16()) {\n+        llvm_unreachable(\"Float16 type is not supported with computeCapability \"\n+                         \"< 70 (use Float32 or above)\");\n+      } else {\n+        return failure();\n+      }\n+    }\n+\n     // TODO: Check data-types and SM compatibility\n     auto oldRetType = dotOp.getResult().getType().cast<RankedTensorType>();\n     if (!oldRetType.getEncoding() ||\n@@ -106,12 +120,6 @@ class BlockedToMMA : public mlir::RewritePattern {\n     auto mod = op->getParentOfType<mlir::ModuleOp>();\n     int numWarps = triton::gpu::TritonGPUDialect::getNumWarps(mod);\n \n-    // operands\n-    Value a = dotOp.getA();\n-    Value b = dotOp.getB();\n-    auto oldAType = a.getType().cast<RankedTensorType>();\n-    auto oldBType = b.getType().cast<RankedTensorType>();\n-\n     triton::gpu::MmaEncodingAttr mmaEnc;\n     if (versionMajor == 1) {\n       SetVector<Operation *> aBwdSlices, bBwdSlices;"}]